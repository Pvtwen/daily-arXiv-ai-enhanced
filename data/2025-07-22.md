<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 58]
- [cs.LG](#cs.LG) [Total: 128]
- [stat.ML](#stat.ML) [Total: 12]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Optimizing VO2max Prediction in Gamified Cardiac Assessment: Leveraging Effective Feature Selection and Refined Protocols for Robust Models](https://arxiv.org/abs/2507.14138)
*Vaishnavi C K,Sricharan Vijayarangan,Sri Gayathri G,Danush Adhithya N,Alex Joseph,Preejith SP,Mohanasankar Sivaprakasam*

Main category: eess.SP

TL;DR: 研究通过改进心肺点慢跑测试（CPSJT）协议，结合性别、BMI等特征，利用机器学习模型提高VO2max预测精度。


<details>
  <summary>Details</summary>
Motivation: 当前VO2max测量方法（如CPET）成本高且需专业人员，限制了大规模筛查。早期替代方法（如CPSJT）误差高，需改进。

Method: 优化CPSJT协议，提取性别、BMI等特征，使用线性回归、随机森林和支持向量回归模型进行预测。

Result: 在44名印度参与者中，所有模型表现稳健，RMSE值低（5.78-5.17），测试相关性高。

Conclusion: 改进的CPSJT协议结合机器学习模型，为VO2max预测提供了低成本、高精度的解决方案。

Abstract: VO2max is a critical indicator of cardiopulmonary fitness, reflecting the
maximum amount of oxygen the body can utilize during intense exercise.
Accurately measuring VO2max is essential for assessing cardiovascular health
and predicting outcomes in clinical settings. However, current methods for
VO2max estimation, such as Cardiopulmonary Exercise Testing (CPET), require
expensive equipment and the supervision of trained personnel, limiting
accessibility for large-scale screening. Preliminary efforts have been made to
create a more accessible method, such as the Cardiopulmonary Spot Jog Test
(CPSJT). Unfortunately, these early attempts yielded high error margins,
rendering them unsuitable for widespread use. In our study, we address these
shortcomings by refining the CPSJT protocol to improve prediction accuracy. A
crucial contribution is improved feature extraction which include gender, body
mass index, aerobic duration, and anaerobic duration. This targeted approach
helps in streamlining the model to enhance prediction precision while
minimizing the risk of overfitting. In a cohort of 44 participants from the
Indian population, we assessed the performance of various machine learning
models using these features. With Stratified 5-Fold Cross-Validation, the Root
Mean Squared Error (RMSE) values were 5.78 for Linear Regression, 5.15 for
Random Forest, and 5.17 for Support Vector Regression. All models demonstrated
strong test correlations and low RMSE values, underscoring their robust and
reliable performance.

</details>


### [2] [DIVER-0 : A Fully Channel Equivariant EEG Foundation Model](https://arxiv.org/abs/2507.14141)
*Danny Dongyeop Han,Ahhyun Lucy Lee,Taeyang Lee,Yonghyeon Gwon,Sebin Lee,Seongjin Lee,David Keetae Park,Shinjae Yoo,Jiook Cha,Chun Kee Chung*

Main category: eess.SP

TL;DR: DIVER-0是一种新型EEG基础模型，通过全时空注意力和改进的位置编码技术，解决了现有模型在时空动态建模和通道置换等变性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型在时空动态建模和通道置换等变性方面表现不足，限制了其在不同电极配置下的泛化能力。

Method: 提出DIVER-0模型，采用全时空注意力机制，结合RoPE和二进制注意力偏置，并引入STCPE以保持时空平移和通道置换等变性。

Result: 实验表明，DIVER-0仅用10%的预训练数据即达到竞争性能，且在不同通道置换条件下表现一致。

Conclusion: DIVER-0有效解决了EEG数据的异质性问题，为跨数据集泛化提供了关键设计原则。

Abstract: Electroencephalography (EEG) is a non-invasive technique widely used in
brain-computer interfaces and clinical applications, yet existing EEG
foundation models face limitations in modeling spatio-temporal brain dynamics
and lack channel permutation equivariance, preventing robust generalization
across diverse electrode configurations. To address these challenges, we
propose DIVER-0, a novel EEG foundation model that demonstrates how full
spatio-temporal attention-rather than segregated spatial or temporal
processing-achieves superior performance when properly designed with Rotary
Position Embedding (RoPE) for temporal relationships and binary attention
biases for channel differentiation. We also introduce Sliding Temporal
Conditional Positional Encoding (STCPE), which improves upon existing
conditional positional encoding approaches by maintaining both temporal
translation equivariance and channel permutation equivariance, enabling robust
adaptation to arbitrary electrode configurations unseen during pretraining.
Experimental results demonstrate that DIVER-0 achieves competitive performance
with only 10% of pretraining data while maintaining consistent results across
all channel permutation conditions, validating its effectiveness for
cross-dataset generalization and establishing key design principles for
handling the inherent heterogeneity of neural recording setups.

</details>


### [3] [Recursive KalmanNet: Analyse des capacités de généralisation d'un réseau de neurones récurrent guidé par un filtre de Kalman](https://arxiv.org/abs/2507.14144)
*Cyril Falcon,Hassan Mortada,Mathéo Clavaud,Jean-Philippe Michel*

Main category: eess.SP

TL;DR: Recursive KalmanNet是一种基于卡尔曼滤波的递归神经网络，能够在未知噪声特性的情况下估计动态系统的状态变量和误差协方差。本文探讨了其在分布外场景中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究Recursive KalmanNet在测试数据与训练数据动态特性不同的情况下的泛化能力。

Method: 使用基于卡尔曼滤波的递归神经网络（Recursive KalmanNet）进行状态估计，无需噪声特性先验知识。

Result: 论文探讨了模型在分布外场景中的表现，验证其泛化能力。

Conclusion: Recursive KalmanNet在动态特性不同的测试数据中展现出泛化潜力。

Abstract: The Recursive KalmanNet, recently introduced by the authors, is a recurrent
neural network guided by a Kalman filter, capable of estimating the state
variables and error covariance of stochastic dynamic systems from noisy
measurements, without prior knowledge of the noise characteristics. This paper
explores its generalization capabilities in out-of-distribution scenarios,
where the temporal dynamics of the test measurements differ from those
encountered during training.
  Le Recursive KalmanNet, r\'ecemment introduit par les auteurs, est un
r\'eseau de neurones r\'ecurrent guid\'e par un filtre de Kalman, capable
d'estimer les variables d'\'etat et la covariance des erreurs des syst\`emes
dynamiques stochastiques \`a partir de mesures bruit\'ees, sans connaissance
pr\'ealable des caract\'eristiques des bruits. Cet article explore ses
capacit\'es de g\'en\'eralisation dans des sc\'enarios hors distribution, o\`u
les dynamiques temporelles des mesures de test diff\`erent de celles
rencontr\'ees \`a l'entra\^inement.

</details>


### [4] [Estimating Markers of Driving Stress through Multimodal Physiological Monitoring](https://arxiv.org/abs/2507.14146)
*Kleanthis Avramidis,Emily Zhou,Tiantian Feng,Hossein Hamidi Shishavan,Frederico Marcolino Quintao Severgnini,Danny J. Lohan,Paul Schmalenberg,Ercan M. Dede,Shrikanth Narayanan*

Main category: eess.SP

TL;DR: 研究探讨短期压力事件对驾驶者生理和行为的影响，通过多模态机器学习系统实时估计驾驶压力。


<details>
  <summary>Details</summary>
Motivation: 驾驶压力影响道路安全和驾驶者健康，现有安全系统难以适应多变行为和环境。

Method: 利用驾驶模拟器收集31名成年人的生理信号，设计多模态机器学习系统分析压力。

Result: 模型能敏感捕捉压力动态，并揭示压力与车辆控制行为的关系。

Conclusion: 生理信号与行为线索结合可提升驾驶压力的实时估计能力。

Abstract: Understanding and mitigating driving stress is vital for preventing accidents
and advancing both road safety and driver well-being. While vehicles are
equipped with increasingly sophisticated safety systems, many limits exist in
their ability to account for variable driving behaviors and environmental
contexts. In this study we examine how short-term stressor events impact
drivers' physiology and their behavioral responses behind the wheel. Leveraging
a controlled driving simulation setup, we collected physiological signals from
31 adult participants and designed a multimodal machine learning system to
estimate the presence of stressors. Our analysis explores the model sensitivity
and temporal dynamics against both known and novel emotional inducers, and
examines the relationship between predicted stress and observable patterns of
vehicle control. Overall, this study demonstrates the potential of linking
physiological signals with contextual and behavioral cues in order to improve
real-time estimation of driving stress.

</details>


### [5] [Graph Convolutional Neural Networks to Model the Brain for Insomnia](https://arxiv.org/abs/2507.14147)
*Kevin Monteiro,Sam Nallaperuma-Herzberg,Martina Mason,Steve Niederer*

Main category: eess.SP

TL;DR: 该论文提出了一种基于脑电图（EEG）和图卷积神经网络（GCNN）的方法，用于研究失眠症患者的大脑功能特征，并实现分类任务。


<details>
  <summary>Details</summary>
Motivation: 现有失眠治疗方法副作用多，需改进。脑模型在阿尔茨海默症、癫痫等疾病中有应用，但未用于失眠研究。

Method: 利用长时间连续EEG数据，基于功能连接和空间距离构建脑网络，计算主要脑波频段的功率谱密度，训练GCNN模型进行分类。

Result: 50秒非重叠滑动窗口最适合EEG分割，分类准确率在窗口和受试者层面分别为70%和68%。特定电极通道（C4-P4、F4-C4、C4-A1）的缺失对模型性能影响显著。

Conclusion: 该方法可有效识别失眠相关大脑功能特征，特定脑区功能连接异常与失眠相关。

Abstract: Insomnia affects a vast population of the world and can have a wide range of
causes. Existing treatments for insomnia have been linked with many side
effects like headaches, dizziness, etc. As such, there is a clear need for
improved insomnia treatment. Brain modelling has helped with assessing the
effects of brain pathology on brain network dynamics and with supporting
clinical decisions in the treatment of Alzheimer's disease, epilepsy, etc.
However, such models have not been developed for insomnia. Therefore, this
project attempts to understand the characteristics of the brain of individuals
experiencing insomnia using continuous long-duration EEG data. Brain networks
are derived based on functional connectivity and spatial distance between EEG
channels. The power spectral density of the channels is then computed for the
major brain wave frequency bands. A graph convolutional neural network (GCNN)
model is then trained to capture the functional characteristics associated with
insomnia and configured for the classification task to judge performance.
Results indicated a 50-second non-overlapping sliding window was the most
suitable choice for EEG segmentation. This approach achieved a classification
accuracy of 70% at window level and 68% at subject level. Additionally, the
omission of EEG channels C4-P4, F4-C4 and C4-A1 caused higher degradation in
model performance than the removal of other channels. These channel electrodes
are positioned near brain regions known to exhibit atypical levels of
functional connectivity in individuals with insomnia, which can explain such
results.

</details>


### [6] [Visible Light Indoor Positioning with a Single LED and Distributed Single-Element OIRS: An Iterative Approach with Adaptive Beam Steering](https://arxiv.org/abs/2507.14148)
*Daniele Pugliese,Giovanni Iacovelli,Alessio Fascista,Domenico Striccoli,Oleksandr Romanov,Luigi Alfredo Grieco,Gennaro Boggia*

Main category: eess.SP

TL;DR: 论文研究了在单LED和多OIRS的VLC系统中，通过间接定位框架和ML估计器，结合IWLS算法和自适应波束控制，实现低成本PD的高效定位。


<details>
  <summary>Details</summary>
Motivation: 利用OIRS和VLC系统提升频谱效率、抗LOS阻塞能力和定位能力，解决单LED环境下低成本PD的定位问题。

Method: 采用ML估计器优化LoS/NLoS距离，提出基于噪声方差变换的算法，结合IWLS和自适应波束控制实现定位。

Result: 仿真显示该方法定位精度高，对OIRS失准鲁棒，且迭代次数少。

Conclusion: 所提方法在VLC系统中实现了高效、低成本的PD定位，具有实际应用潜力。

Abstract: The integration of Optical Intelligent Reflective Surfaces (OIRSs) into
Visible Light Communication (VLC) systems is gaining momentum as a valid
alternative to RF technologies, harnessing the existing lighting
infrastructures and the vast unlicensed optical spectrum to enable higher
spectral efficiency, improved resilience to Line-of-Sight (LoS) blockages, and
enhanced positioning capabilities. This paper investigates the problem of
localizing a low-cost Photo Detector (PD) in a VLC-based indoor environment
consisting of only a single Light Emitting Diode (LED) as an active anchor, and
multiple spatially distributed single-element OIRSs. We formulate the problem
within an indirect, computationally efficient localization framework: first,
the optimal Maximum Likelihood (ML) estimators of the LoS and Non-Line-of-Sight
(NLoS) distances are derived, using a suitable OIRS activation strategy to
prevent interferences. To overcome the grid-based optimization required by the
ML NLoS estimator, we devise a novel algorithm based on an unstructured noise
variance transformation, which admits a closed-form solution. The set of
estimated LoS/NLoS distances are then used within a low-complexity localization
algorithm combining an Iterative Weighted Least Squares (IWLS) procedure, whose
weights are set according to the inverse of the Cram\'er-Rao Lower Bound
(CRLB), with an adaptive beam steering strategy that allows the OIRSs network
to dynamically align with the PD, without any prior knowledge of its position.
Accordingly, we derive the CRLB for both LoS/NLoS distance estimation and PD
position estimation. Simulation results demonstrate the effectiveness of our
approach in terms of localization accuracy, robustness against OIRSs
misalignment conditions, and low number of iterations required to attain the
theoretical bounds.

</details>


### [7] [Machine learning-enabled river water quality monitoring using lithography-free 3D-printed sensors](https://arxiv.org/abs/2507.14152)
*Frank Efe Erukainure,Feidra Gjata,Matin Ataei Kachouei,Henry Cox,Md. Azahar Ali*

Main category: eess.SP

TL;DR: 开发了一种无光刻技术的磷酸盐传感器（P-sensor），用于检测河流中低至1 ppb的磷酸盐，响应时间短，并通过神经网络实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 河流水质监测对生态和人类健康至关重要，磷酸盐污染会导致富营养化等问题，需要高效监测工具。

Method: 采用3D打印聚合物图案涂覆磷酸盐选择性膜，构建固态指示电极，结合神经网络预测磷酸盐浓度。

Result: 传感器在0-475 ppm范围内检测1 ppb磷酸盐，响应时间<30秒，神经网络预测误差低，相关性高。

Conclusion: 该传感器为连续水质监测提供了实用工具，有助于改善公共健康和政策制定。

Abstract: River water quality monitoring is important for aquatic life, livestock, and
humans because clean water is critical to meeting food demand during the global
food crisis. Excessive contaminants, including phosphate, deplete dissolved
oxygen and trigger eutrophication, leading to serious health and ecological
problems. Continuous sensors that track phosphate levels can therefore help
prevent eutrophication. In this work we present a lithography-free phosphate
sensor (P-sensor) that detects phosphate in river water at parts-per-billion
levels. The device uses a solid-state indicator electrode formed by 3D-printed
periodic polymer patterns (8 um feature size) coated with a thin phosphate
ion-selective membrane. The P-sensor detects as little as 1 ppb phosphate
across 0 - 475 ppm with a response time under 30 seconds. We validated the
sensor on Rappahannock River water, Virginia (less than 0.8 ppm phosphate) at
sites upstream and downstream of a sewage treatment plant and benchmarked the
results against a commercial phosphate meter. A feed-forward neural network was
trained to predict phosphate levels, achieving a mean-squared error below 1e-3,
zero standard deviation, and a Pearson correlation coefficient of 0.997 for
river samples. These results demonstrate a practical tool for continuous
water-quality monitoring that can inform stakeholders and policymakers and
ultimately improve public health.

</details>


### [8] [Self-DANA: A Resource-Efficient Channel-Adaptive Self-Supervised Approach for ECG Foundation Models](https://arxiv.org/abs/2507.14151)
*Giuliana Monachino,Nicolò La Porta,Beatrice Zanchi,Luigi Fiorillo,Alvise Dei Rossi,Georgiy Farina,Francesca Dalia Faraci*

Main category: eess.SP

TL;DR: 论文提出Self-DANA方法，使自监督架构适应减少的输入通道，提高资源效率和性能。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴设备的普及，研究如何将ECG基础模型（FMs）适应于减少通道的下游场景。

Method: 提出Self-DANA和Random Lead Selection增强技术，预训练模型以适应不同通道配置。

Result: 实验显示Self-DANA显著提高资源效率，减少CPU和GPU内存及时间消耗，性能达到SOTA。

Conclusion: Self-DANA为减少通道的ECG分析提供高效且高性能的解决方案。

Abstract: Foundation Models (FMs) are large-scale machine learning models trained on
extensive, diverse datasets that can be adapted to a wide range of downstream
tasks with minimal fine-tuning. In the last two years, interest in FMs has also
grown for applications in the cardiological field to analyze the
electrocardiogram (ECG) signals. One of the key properties of FMs is their
transferability to a wide range of downstream scenarios. With the spread of
wearable and portable devices, keen interest in learning from reduced-channel
configurations has arisen. However, the adaptation of ECG FMs to downstream
scenarios with fewer available channels still has to be properly investigated.
In this work, we propose Self-DANA, a novel, easy-to-integrate solution that
makes self-supervised architectures adaptable to a reduced number of input
channels, ensuring resource efficiency and high performance. We also introduce
Random Lead Selection, a novel augmentation technique to pre-train models in a
more robust and channel-agnostic way. Our experimental results on five
reduced-channel configurations demonstrate that Self-DANA significantly
enhances resource efficiency while reaching state-of-the-art performance. It
requires up to 69.3% less peak CPU memory, 34.4% less peak GPU memory, about
17% less average epoch CPU time, and about 24% less average epoch GPU time.

</details>


### [9] [A Multi-Modal IoT Node for Energy-Efficient Environmental Monitoring with Edge AI Processing](https://arxiv.org/abs/2507.14165)
*Philip Wiese,Victor Kartsch,Marco Guermandi,Luca Benini*

Main category: eess.SP

TL;DR: 论文介绍了一种紧凑型多模态MCU环境物联网节点，集成了11种传感器，支持边缘计算，实现了节能高效的ML模型处理。


<details>
  <summary>Details</summary>
Motivation: 当前物联网环境监测平台传感器种类有限且计算能力不足，无法支持先进的ML和AI算法在边缘设备上的部署。

Method: 开发了一种集成11种传感器的紧凑型MCU节点，采用GAP9芯片实现实时低功耗边缘处理，并部署了YOLOv5模型进行占用检测。

Result: 实现了42%的能耗节省，单次充电可运行143小时，支持智能室内空气质量监测。

Conclusion: 该平台为预测性室内空气质量监测等创新应用奠定了基础，推动了高效AI驱动的边缘预测技术。

Abstract: The widespread adoption of Internet of Things (IoT) technologies has
significantly advanced environmental monitoring (EM) by enabling cost-effective
and scalable sensing solutions. Concurrently, machine learning (ML) and
artificial intelligence (AI) are introducing powerful tools for the efficient
and accurate analysis of complex environmental data. However, current IoT
platforms for environmental sensing are typically limited to a narrow set of
sensors, preventing a comprehensive assessment of environmental conditions and
lacking sufficient computational capabilities to support the deployment of
advanced ML and AI algorithms on the edge. To overcome these limitations, we
introduce a compact (17x38 mm2), multi-modal, MCU-based environmental IoT node
integrating 11 sensors, including CO2 concentration, volatile organic compounds
(VOCs), light intensity, UV radiation, pressure, temperature, humidity, visual
sensing via an RGB camera, and precise geolocation through a GNSS module. It
features GAP9, a parallel ultra-low-power system-on-chip, enabling real-time,
energy-efficient edge processing of advanced ML models directly on-device. We
implemented a YOLOv5-based occupancy detection pipeline (0.3 M parameters, 42
MOP per inference), demonstrating 42% energy savings over raw data streaming.
Additionally, we present a smart indoor air quality (IAQ) monitoring setup that
combines occupancy detection with adaptive sample rates, achieving operational
times of up to 143 h on a single compact 600 mAh, 3.7 V battery. Our platform
lays the groundwork for innovative applications such as predictive indoor IAQ,
enabling efficient AI-driven on-edge forecasting for energy-efficient and
autonomous, proactive pollution-mitigation control strategies

</details>


### [10] [Boosted Enhanced Quantile Regression Neural Networks with Spatiotemporal Permutation Entropy for Complex System Prognostics](https://arxiv.org/abs/2507.14194)
*David J Poland*

Main category: eess.SP

TL;DR: 提出了一种基于时空排列熵分析和增强分位数回归神经网络（BEQRNNs）的模式预测和系统预后框架，结合熵复杂度测量与高级神经网络架构，显著提升了复杂动态模式的理解和预测能力。


<details>
  <summary>Details</summary>
Motivation: 解决多维系统中复杂动态模式理解的挑战，结合熵复杂度测量与神经网络技术，提升预测精度和不确定性量化能力。

Method: 采用双阶段计算：首先优化时空熵提取，随后集成BEQRNN层进行概率模式预测和不确定性量化。

Result: 在时空模式分类中达到81.17%的准确率，预测范围达200时间步，关键过渡检测精度提升79%，长期预测可靠性提高81.22%。

Conclusion: 该框架在处理复杂多模态熵特征方面表现出色，具有实时预后应用的巨大潜力。

Abstract: This paper presents a novel framework for pattern prediction and system
prognostics centered on Spatiotemporal Permutation Entropy analysis integrated
with Boosted Enhanced Quantile Regression Neural Networks (BEQRNNs). We address
the challenge of understanding complex dynamical patterns in multidimensional
systems through an approach that combines entropy-based complexity measures
with advanced neural architectures. The system leverages dual computational
stages: first implementing spatiotemporal entropy extraction optimized for
multiscale temporal and spatial data streams, followed by an integrated BEQRNN
layer that enables probabilistic pattern prediction with uncertainty
quantification. This architecture achieves 81.17% accuracy in spatiotemporal
pattern classification with prediction horizons up to 200 time steps and
maintains robust performance across diverse regimes. Field testing across
chaotic attractors, reaction-diffusion systems, and industrial datasets shows a
79% increase in critical transition detection accuracy and 81.22% improvement
in long-term prediction reliability. The framework's effectiveness in
processing complex, multimodal entropy features demonstrates significant
potential for real-time prognostic applications.

</details>


### [11] [Surface EMG Profiling in Parkinson's Disease: Advancing Severity Assessment with GCN-SVM](https://arxiv.org/abs/2507.14153)
*Daniel Cieślak,Barbara Szyca,Weronika Bajko,Liwia Florkiewicz,Kinga Grzęda,Mariusz Kaczmarek,Helena Kamieniecka,Hubert Lis,Weronika Matwiejuk,Anna Prus,Michalina Razik,Inga Rozumowicz,Wiktoria Ziembakowska*

Main category: eess.SP

TL;DR: 该研究提出了一种利用表面肌电图（sEMG）评估帕金森病（PD）严重程度的新方法，通过改进的GCN-SVM模型将准确率提升至92%。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的诊断和监测因其复杂症状和渐进性而具有挑战性，需要更客观的评估方法。

Method: 研究使用sEMG分析肱二头肌肌肉活动，比较PD患者和健康对照组的数据，并采用SVM和GCN-SVM模型进行分类。

Result: 初步结果显示SVM准确率为83%，GCN-SVM提升至92%。

Conclusion: 该方法为PD严重程度评估提供了新思路，未来需扩大样本验证并应用于临床。

Abstract: Parkinson's disease (PD) poses challenges in diagnosis and monitoring due to
its progressive nature and complex symptoms. This study introduces a novel
approach utilizing surface electromyography (sEMG) to objectively assess PD
severity, focusing on the biceps brachii muscle. Initial analysis of sEMG data
from five PD patients and five healthy controls revealed significant
neuromuscular differences. A traditional Support Vector Machine (SVM) model
achieved up to 83% accuracy, while enhancements with a Graph Convolutional
Network-Support Vector Machine (GCN-SVM) model increased accuracy to 92%.
Despite the preliminary nature of these results, the study outlines a detailed
experimental methodology for future research with larger cohorts to validate
these findings and integrate the approach into clinical practice. The proposed
approach holds promise for advancing PD severity assessment and improving
patient care in Parkinson's disease management.

</details>


### [12] [UniPhyNet: A Unified Network For Multimodal Physiological Raw Signal Classification](https://arxiv.org/abs/2507.14163)
*Renxiang Qiu,Raghavendra Selvan*

Main category: eess.SP

TL;DR: UniPhyNet是一种新型神经网络架构，用于通过多模态生理数据（EEG、ECG、EDA）分类认知负荷，无需手工特征提取。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖手工特征提取的问题，提供端到端的认知状态监测解决方案。

Method: 结合多尺度并行卷积块、ResNet块和通道块注意力模块，使用双向门控循环单元捕捉时序依赖，通过中间融合处理单模态和多模态信号。

Result: 在CL-Drive数据集上，二元分类准确率从70%提升至80%，三元分类从62%提升至74%，优于基于特征的方法。

Conclusion: UniPhyNet是一种有效的端到端解决方案，适用于实际认知状态监测。

Abstract: We present UniPhyNet, a novel neural network architecture to classify
cognitive load using multimodal physiological data -- specifically EEG, ECG and
EDA signals -- without the explicit need for extracting hand-crafted features.
UniPhyNet integrates multiscale parallel convolutional blocks and ResNet-type
blocks enhanced with channel block attention module to focus on the informative
features while a bidirectional gated recurrent unit is used to capture temporal
dependencies. This architecture processes and combines signals in both unimodal
and multimodal configurations via intermediate fusion of learned feature maps.
On the CL-Drive dataset, UniPhyNet improves raw signal classification accuracy
from 70% to 80% (binary) and 62% to 74% (ternary), outperforming feature-based
models, demonstrating its effectiveness as an end-to-end solution for
real-world cognitive state monitoring.

</details>


### [13] [Extreme Value Theory-based Distributed Interference Prediction for 6G Industrial Sub-networks](https://arxiv.org/abs/2507.14155)
*Pramesh Gautam,Sushmita Sapkota,Carsten Bockelmann,Shashi Raj Pandey,Armin Dekorsy*

Main category: eess.SP

TL;DR: 提出了一种结合统计和机器学习的干扰尾部预测框架，用于超密集子网络中极端干扰事件的预测和资源分配。


<details>
  <summary>Details</summary>
Motivation: 解决超密集子网络中极端和罕见干扰事件的预测问题，以支持超可靠低延迟通信（HRLLC）。

Method: 结合倒置分位数补丁变换器（iQPTransformer）和极值理论（EVT），设计了一种分布式训练方法（split-iQPTransformer）。

Result: 在多种移动模型和流量模式下，显著优于基线方法，实现了超可靠区域中95%分位数以上的块错误率目标。

Conclusion: 该框架能有效处理极端干扰事件，为资源分配提供统计覆盖保证。

Abstract: Interference prediction that accounts for extreme and rare events remains a
key challenge for ultra-densely deployed sub-networks (SNs) requiring
hyper-reliable low-latency communication (HRLLC), particularly under dynamic
mobility, rapidly varying channel statistics, and sporadic traffic. This paper
proposes a novel calibrated interference tail prediction framework, a hybrid
statistical and machine learning (ML) approach that integrates an inverted
quantile patch transformer (iQPTransformer) within extreme value theory (EVT).
It captures interference dynamics and tail behavior while quantifying
uncertainty to provide statistical coverage guarantees. Its effectiveness is
demonstrated by leveraging the estimated interference tail distribution to
design predictive, risk-aware resource allocation. In resource-constrained SN
scenarios, we introduce the split-iQPTransformer, enabling collaborative
training by distributing neural network components between sensor-actuator (SA)
pairs and the SN controller, while maintaining minimal performance disparity
compared to the centralized iQPTransformer. The framework effectively handles
deep fading, random traffic, and time-division duplexing (TDD) misalignments
and is resilient to rare and extreme interference events. Extensive evaluations
are performed under two mobility models and two realistic SN traffic patterns,
using a spatially consistent 3GPP channel model across all scenarios.
Experimental results show consistent achievement of block error rate (BLER)
targets beyond the 95th percentile in the hyper-reliable regime, significantly
outperforming baseline approaches.

</details>


### [14] [A Denoising VAE for Intracardiac Time Series in Ischemic Cardiomyopathy](https://arxiv.org/abs/2507.14164)
*Samuel Ruipérez-Campillo,Alain Ryser,Thomas M. Sutter,Ruibin Feng,Prasanth Ganesan,Brototo Deb,Kelly A. Brennan,Maxime Pedron,Albert J. Rogers,Maarten Z. H. Kolk,Fleur V. Y. Tjong,Sanjiv M. Narayan,Julia E. Vogt*

Main category: eess.SP

TL;DR: 本文提出了一种基于变分自编码器（VAE）的方法，用于提高心室内单相动作电位（MAP）信号的质量，相比传统滤波方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 心脏电生理学中，传统降噪方法难以处理信号中的非线性、非平稳噪声，影响心律失常和心肌病的诊断与治疗。

Method: 利用来自42名缺血性心肌病患者的5706个时间序列数据，构建VAE模型以生成干净信号的表示。

Result: VAE模型在多种噪声类型（包括时变非线性噪声）下表现出优于传统方法的降噪性能。

Conclusion: VAE能有效消除单次心跳中的多种噪声源，可能提升心脏电生理学的治疗效果。

Abstract: In the field of cardiac electrophysiology (EP), effectively reducing noise in
intra-cardiac signals is crucial for the accurate diagnosis and treatment of
arrhythmias and cardiomyopathies. However, traditional noise reduction
techniques fall short in addressing the diverse noise patterns from various
sources, often non-linear and non-stationary, present in these signals. This
work introduces a Variational Autoencoder (VAE) model, aimed at improving the
quality of intra-ventricular monophasic action potential (MAP) signal
recordings. By constructing representations of clean signals from a dataset of
5706 time series from 42 patients diagnosed with ischemic cardiomyopathy, our
approach demonstrates superior denoising performance when compared to
conventional filtering methods commonly employed in clinical settings. We
assess the effectiveness of our VAE model using various metrics, indicating its
superior capability to denoise signals across different noise types, including
time-varying non-linear noise frequently found in clinical settings. These
results reveal that VAEs can eliminate diverse sources of noise in single
beats, outperforming state-of-the-art denoising techniques and potentially
improving treatment efficacy in cardiac EP.

</details>


### [15] [A Comprehensive Benchmark for Electrocardiogram Time-Series](https://arxiv.org/abs/2507.14206)
*Zhijiang Tang,Jiaxin Qi,Yuhua Zheng,Jianqiang Huang*

Main category: eess.SP

TL;DR: 本文深入研究了ECG信号，建立了全面基准，包括分类下游任务、改进评估指标并提出新模型架构，验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽略ECG信号的独特性和专用下游应用，导致对其特性理解不足。

Method: 分类下游任务、改进评估指标并提出新模型架构。

Result: 实验证明基准全面且稳健，新指标和模型架构有效。

Conclusion: 为ECG信号分析研究奠定了坚实基础。

Abstract: Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial
for assessing cardiac health and diagnosing various diseases. Given its
time-series format, ECG data is often incorporated into pre-training datasets
for large-scale time-series model training. However, existing studies often
overlook its unique characteristics and specialized downstream applications,
which differ significantly from other time-series data, leading to an
incomplete understanding of its properties. In this paper, we present an
in-depth investigation of ECG signals and establish a comprehensive benchmark,
which includes (1) categorizing its downstream applications into four distinct
evaluation tasks, (2) identifying limitations in traditional evaluation metrics
for ECG analysis, and introducing a novel metric; (3) benchmarking
state-of-the-art time-series models and proposing a new architecture. Extensive
experiments demonstrate that our proposed benchmark is comprehensive and
robust. The results validate the effectiveness of the proposed metric and model
architecture, which establish a solid foundation for advancing research in ECG
signal analysis.

</details>


### [16] [Automated Vigilance State Classification in Rodents Using Machine Learning and Feature Engineering](https://arxiv.org/abs/2507.14166)
*Sankalp Jajee,Gaurav Kumar,Homayoun Valafar*

Main category: eess.SP

TL;DR: 该研究提出了一种自动化框架，用于分类小啮齿动物的脑电图（EEG）记录，区分三种警觉状态：REM睡眠、慢波睡眠（SWS）和清醒状态，显著提高了分类准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 临床前睡眠研究受限于人工分类的劳动密集性和评分者间差异，影响了通量和可重复性。

Method: 结合信号处理和机器学习，利用时域和频域特征（如频谱功率、最大最小距离和跨频耦合指标）训练XGBoost模型。

Result: 模型在2024年大数据健康科学案例竞赛中验证，整体准确率达91.5%，优于基线方法。

Conclusion: 该框架是自动化睡眠状态分类的重要进展，有助于加速睡眠科学研究和慢性睡眠障碍的干预开发。

Abstract: Preclinical sleep research remains constrained by labor intensive, manual
vigilance state classification and inter rater variability, limiting throughput
and reproducibility. This study presents an automated framework developed by
Team Neural Prognosticators to classify electroencephalogram (EEG) recordings
of small rodents into three critical vigilance states paradoxical sleep (REM),
slow wave sleep (SWS), and wakefulness. The system integrates advanced signal
processing with machine learning, leveraging engineered features from both time
and frequency domains, including spectral power across canonical EEG bands
(delta to gamma), temporal dynamics via Maximum-Minimum Distance, and
cross-frequency coupling metrics. These features capture distinct
neurophysiological signatures such as high frequency desynchronization during
wakefulness, delta oscillations in SWS, and REM specific bursts. Validated
during the 2024 Big Data Health Science Case Competition (University of South
Carolina Big Data Health Science Center, 2024), our XGBoost model achieved
91.5% overall accuracy, 86.8% precision, 81.2% recall, and an F1 score of
83.5%, outperforming all baseline methods. Our approach represents a critical
advancement in automated sleep state classification and a valuable tool for
accelerating discoveries in sleep science and the development of targeted
interventions for chronic sleep disorders. As a publicly available code (BDHSC)
resource is set to contribute significantly to advancements.

</details>


### [17] [Attention-Based Fusion of IQ and FFT Spectrograms with AoA Features for GNSS Jammer Localization](https://arxiv.org/abs/2507.14167)
*Lucas Heublein,Christian Wielenberg,Thorsten Nowak,Tobias Feigl,Christopher Mutschler,Felix Ott*

Main category: eess.SP

TL;DR: 论文提出了一种基于注意力融合框架的新方法，用于检测和分类GNSS干扰信号，并估计干扰源的距离、方位角和仰角，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: GNSS干扰信号威胁定位可靠性，传统AoA方法在多路径环境下精度不足且计算资源需求高。

Method: 结合IQ样本和FFT频谱图，引入22个AoA特征，通过注意力融合框架提升定位精度。

Result: 在动态多路径环境下，新方法性能优于现有技术。

Conclusion: 提出的方法在干扰检测和定位方面表现出色，适用于复杂环境。

Abstract: Jamming devices disrupt signals from the global navigation satellite system
(GNSS) and pose a significant threat by compromising the reliability of
accurate positioning. Consequently, the detection and localization of these
interference signals are essential to achieve situational awareness, mitigating
their impact, and implementing effective counter-measures. Classical Angle of
Arrival (AoA) methods exhibit reduced accuracy in multipath environments due to
signal reflections and scattering, leading to localization errors.
Additionally, AoA-based techniques demand substantial computational resources
for array signal processing. In this paper, we propose a novel approach for
detecting and classifying interference while estimating the distance, azimuth,
and elevation of jamming sources. Our benchmark study evaluates 128 vision
encoder and time-series models to identify the highest-performing methods for
each task. We introduce an attention-based fusion framework that integrates
in-phase and quadrature (IQ) samples with Fast Fourier Transform (FFT)-computed
spectrograms while incorporating 22 AoA features to enhance localization
accuracy. Furthermore, we present a novel dataset of moving jamming devices
recorded in an indoor environment with dynamic multipath conditions and
demonstrate superior performance compared to state-of-the-art methods.

</details>


### [18] [CQI-Based Interference Prediction for Link Adaptation in Industrial Sub-networks](https://arxiv.org/abs/2507.14169)
*Pramesh Gautam,Ravi Sharan B A G,Paolo Baracca,Carsten Bockelmann,Thorsten Wild,Armin Dekorsy*

Main category: eess.SP

TL;DR: 提出了一种新型干扰预测方案，用于改进高密度工业子网络中高可靠低延迟通信的链路自适应（LA）性能。


<details>
  <summary>Details</summary>
Motivation: 在密集部署的工业子网络中，高可靠性和低延迟通信需求对链路自适应提出了更高要求，传统方法难以应对干扰的复杂性和延迟问题。

Method: 通过预测和利用重尾干扰概率密度函数，采用向量离散时间状态空间模型（vDSSM）和稀疏Student-t过程回归（SPTPR）方法，结合改进的无迹卡尔曼滤波器，实现干扰功率的鲁棒估计。

Result: 数值结果表明，该方法复杂度降低10倍以上，同时保持BLER低于1e-6的90百分位目标，性能与仅使用CQI报告的最先进监督技术相当。

Conclusion: 该方案显著提升了链路自适应的准确性和鲁棒性，适用于高密度工业子网络中的高可靠低延迟通信场景。

Abstract: We propose a novel interference prediction scheme to improve link adaptation
(LA) in densely deployed industrial sub-networks (SNs) with high-reliability
and low-latency communication (HRLLC) requirements. The proposed method aims to
improve the LA framework by predicting and leveraging the heavy-tailed
interference probability density function (pdf). Interference is modeled as a
latent vector of available channel quality indicator (CQI), using a vector
discrete-time state-space model (vDSSM) at the SN controller, where the CQI is
subjected to compression, quantization, and delay-induced errors. To robustly
estimate interference power values under these impairments, we employ a
low-complexity, outlier-robust, sparse Student-t process regression (SPTPR)
method. This is integrated into a modified unscented Kalman filter, which
recursively refines predicted interference using CQI, enabling accurate
estimation and compensating protocol feedback delays, crucial for accurate LA.
Numerical results show that the proposed method achieves over 10x lower
complexity compared to a similar non-parametric baseline. It also maintains a
BLER below the 90th percentile target of 1e-6 while delivering performance
comparable to a state-of-the-art supervised technique using only CQI reports.

</details>


### [19] [Enhancing Generalization in PPG-Based Emotion Measurement with a CNN-TCN-LSTM Model](https://arxiv.org/abs/2507.14173)
*Karim Alghoul,Hussein Al Osman,Abdulmotaleb El Saddik*

Main category: eess.SP

TL;DR: 本文提出了一种结合CNN、LSTM和TCN的混合架构，用于提高基于PPG信号的情感识别模型的泛化能力，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于PPG信号的情感识别模型在跨个体泛化方面存在挑战，需要更鲁棒的解决方案。

Method: 采用CNN提取PPG信号特征，LSTM和TCN分别处理特征，最终拼接生成特征表示用于情感分类。

Result: 在PPGE数据集上，混合模型在AUC和F1 Score上优于现有CNN和CNN-LSTM模型。

Conclusion: 混合架构显著提升了情感识别的泛化能力，为PPG信号的情感分析提供了更优方案。

Abstract: Human computer interaction has become integral to modern life, driven by
advancements in machine learning technologies. Affective computing, in
particular, has focused on systems that recognize, interpret, and respond to
human emotions, often using wearable devices, which provide continuous data
streams of physiological signals. Among various physiological signals, the
photoplethysmogram (PPG) has gained prominence due to its ease of acquisition
from widely available devices. However, the generalization of PPG-based emotion
recognition models across individuals remains an unresolved challenge. This
paper introduces a novel hybrid architecture that combines Convolutional Neural
Networks (CNNs), Long Short-Term Memory networks (LSTMs), and Temporal
Convolutional Networks (TCNs) to address this issue. The proposed model
integrates the strengths of these architectures to improve robustness and
generalization. Raw PPG signals are fed into the CNN for feature extraction.
These features are processed separately by LSTM and TCN. The outputs from these
components are concatenated to generate a final feature representation, which
serves as the input for classifying valence and arousal, the primary dimensions
of emotion. Experiments using the Photoplethysmogram Dataset for Emotional
Analysis (PPGE) demonstrate that the proposed hybrid model achieves better
model generalization than standalone CNN and LSTM architectures. Our results
show that the proposed solution outperforms the state-of-the-art CNN
architecture, as well as a CNN-LSTM model, in emotion recognition tasks with
PPG signals. Using metrics such as Area Under the Curve (AUC) and F1 Score, we
highlight the model's effectiveness in handling subject variability.

</details>


### [20] [NeuroHD-RA: Neural-distilled Hyperdimensional Model with Rhythm Alignment](https://arxiv.org/abs/2507.14184)
*ZhengXiao He,Jinghao Wen,Huayu Li,Ao Li*

Main category: eess.SP

TL;DR: 提出了一种结合超维计算（HDC）与可学习神经编码的新型心电图（ECG）疾病检测框架，通过动态编码提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统HDC方法依赖静态随机投影，缺乏任务适应性，因此需要一种更灵活且可解释的ECG检测方法。

Method: 采用基于RR间隔的节律感知可训练编码管道，结合神经蒸馏HDC架构和BinaryLinear投影层，优化交叉熵和代理度量损失。

Result: 在Apnea-ECG和PTB-XL数据集上表现优于传统HDC和经典ML方法，Apnea-ECG上达到73.09%精度和0.626 F1分数。

Conclusion: 该框架为边缘兼容的ECG分类提供了高效、可扩展且可解释的解决方案，适用于个性化健康监测。

Abstract: We present a novel and interpretable framework for electrocardiogram
(ECG)-based disease detection that combines hyperdimensional computing (HDC)
with learnable neural encoding. Unlike conventional HDC approaches that rely on
static, random projections, our method introduces a rhythm-aware and trainable
encoding pipeline based on RR intervals, a physiological signal segmentation
strategy that aligns with cardiac cycles. The core of our design is a
neural-distilled HDC architecture, featuring a learnable RR-block encoder and a
BinaryLinear hyperdimensional projection layer, optimized jointly with
cross-entropy and proxy-based metric loss. This hybrid framework preserves the
symbolic interpretability of HDC while enabling task-adaptive representation
learning. Experiments on Apnea-ECG and PTB-XL demonstrate that our model
significantly outperforms traditional HDC and classical ML baselines, achieving
73.09\% precision and an F1 score of 0.626 on Apnea-ECG, with comparable
robustness on PTB-XL. Our framework offers an efficient and scalable solution
for edge-compatible ECG classification, with strong potential for interpretable
and personalized health monitoring.

</details>


### [21] [Latent Sensor Fusion: Multimedia Learning of Physiological Signals for Resource-Constrained Devices](https://arxiv.org/abs/2507.14185)
*Abdullah Ahmed,Jeremy Gummeson*

Main category: eess.SP

TL;DR: 论文提出了一种基于潜在空间的统一编码器，通过传感器-潜在融合分析多模态生理信号，解决了资源受限设备上的计算挑战。


<details>
  <summary>Details</summary>
Motivation: 利用潜在空间的高效性和隐含的元信息保存能力，开发一种模态无关的统一编码器。

Method: 采用传感器-潜在融合和基于自动编码器的潜在空间融合，结合压缩感知技术。

Result: 实验表明，统一编码器在速度、轻量化和可扩展性上显著优于模态特定方法，且不损失表征精度。

Conclusion: 该方法为资源受限设备上的多模态生理信号分析提供了一种高效解决方案。

Abstract: Latent spaces offer an efficient and effective means of summarizing data
while implicitly preserving meta-information through relational encoding. We
leverage these meta-embeddings to develop a modality-agnostic, unified encoder.
Our method employs sensor-latent fusion to analyze and correlate multimodal
physiological signals. Using a compressed sensing approach with
autoencoder-based latent space fusion, we address the computational challenges
of biosignal analysis on resource-constrained devices. Experimental results
show that our unified encoder is significantly faster, lighter, and more
scalable than modality-specific alternatives, without compromising
representational accuracy.

</details>


### [22] [AI-Based Impedance Encoding-Decoding Method for Online Impedance Network Construction of Wind Farms](https://arxiv.org/abs/2507.14187)
*Xiaojuan Zhang,Tianyu Jiang,Haoxiang Zong,Chen Zhang,Chendan Li,Marta Molinas*

Main category: eess.SP

TL;DR: 提出了一种基于AI的阻抗编码-解码方法，用于在线构建风电场的阻抗网络模型，解决了传统方法中高密度阻抗曲线传输困难的问题。


<details>
  <summary>Details</summary>
Motivation: 风电场的阻抗网络模型构建需要大量高密度阻抗曲线，传输困难，限制了在线应用。

Method: 训练阻抗编码器压缩阻抗曲线，上传压缩数据后训练解码器重构曲线，最后基于节点导纳矩阵方法获取模型。

Result: 验证表明，编码后的阻抗向量能快速传输并准确重构原始阻抗曲线。

Conclusion: 该方法有效解决了阻抗曲线传输问题，为在线构建阻抗网络模型提供了可行方案。

Abstract: The impedance network (IN) model is gaining popularity in the oscillation
analysis of wind farms. However, the construction of such an IN model requires
impedance curves of each wind turbine under their respective operating
conditions, making its online application difficult due to the transmission of
numerous high-density impedance curves. To address this issue, this paper
proposes an AI-based impedance encoding-decoding method to facilitate the
online construction of IN model. First, an impedance encoder is trained to
compress impedance curves by setting the number of neurons much smaller than
that of frequency points. Then, the compressed data of each turbine are
uploaded to the wind farm and an impedance decoder is trained to reconstruct
original impedance curves. At last, based on the nodal admittance matrix (NAM)
method, the IN model of the wind farm can be obtained. The proposed method is
validated via model training and real-time simulations, demonstrating that the
encoded impedance vectors enable fast transmission and accurate reconstruction
of the original impedance curves.

</details>


### [23] [Traffic Signal Phase and Timing Estimation with Large-Scale Floating Car Data](https://arxiv.org/abs/2507.14190)
*Mingcheng Liao,Zebang Feng,Miao Fan,Shengtong Xu,Haoyi Xiong*

Main category: eess.SP

TL;DR: 提出了一种工业级的FCD分析套件，用于准确估计信号相位和时序（SPaT），解决了现有方法在数据局限性和多样性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有FCD方法假设固定信号周期和简单交叉口设计，无法应对周期性信号变化和多样性结构，缺乏通用性。

Method: 开发了一个从数据预处理到SPaT估计的完整框架，包括信号相位估计、TOD时段识别和红绿灯时长确定。

Result: 系统每天处理1500万条FCD记录，支持中国大陆200多万个交通信号，75%以上的估计误差小于5秒。

Conclusion: 该框架在多样条件下表现出稳定性和鲁棒性，为未来研究提供了清洁且去标识化的FCD数据集。

Abstract: Effective modern transportation systems depend critically on accurate Signal
Phase and Timing (SPaT) estimation. However, acquiring ground-truth SPaT
information faces significant hurdles due to communication challenges with
transportation departments and signal installers. As a result, Floating Car
Data (FCD) has become the primary source for large-scale SPaT analyses. Current
FCD approaches often simplify the problem by assuming fixed schedules and basic
intersection designs for specific times and locations. These methods fail to
account for periodic signal changes, diverse intersection structures, and the
inherent limitations of real-world data, thus lacking a comprehensive framework
that is universally applicable. Addressing this limitation, we propose an
industrial-grade FCD analysis suite that manages the entire process, from
initial data preprocessing to final SPaT estimation. Our approach estimates
signal phases, identifies time-of-day (TOD) periods, and determines the
durations of red and green lights. The framework's notable stability and
robustness across diverse conditions, regardless of road geometry, is a key
feature. Furthermore, we provide a cleaned, de-identified FCD dataset and
supporting parameters to facilitate future research. Currently operational
within our navigation platform, the system analyses over 15 million FCD records
daily, supporting over two million traffic signals in mainland China, with more
than 75\% of estimations demonstrating less than five seconds of error.

</details>


### [24] [School Attendance Control System Based on RFID Technology with Raspberry Pi and Arduino: EDURFID](https://arxiv.org/abs/2507.14191)
*Cliver Oliver Turpo Benique*

Main category: eess.SP

TL;DR: EDURFID是一个基于RFID技术的自动化学校考勤系统，专为秘鲁农村教育机构设计，使用开源硬件和Python Django开发，实现了高精度和低成本。


<details>
  <summary>Details</summary>
Motivation: 为秘鲁农村教育机构提供低成本、高效的自动化考勤解决方案，减少行政时间。

Method: 集成Raspberry Pi 5、Arduino UNO R3和RC522 RFID模块，采用Python Django开发Web架构。

Result: 系统RFID读取精度100%，响应时间0.03秒，成本降低94%，验证中节省50分钟行政时间。

Conclusion: EDURFID成功实现了自动化考勤，显著提升了效率和成本效益。

Abstract: This paper presents EDURFID, an automated school attendance control system
based on RFID technology designed for rural educational institutions in Peru.
The system integrates open-source hardware (Raspberry Pi 5, Arduino UNO R3)
with RC522 RFID modules operating at 13.56 MHz, implementing a web architecture
developed in Python Django. The system demonstrates 100% precision in RFID
readings with 0.03-second response time, achieving 94% cost reduction compared
to commercial solutions. Validation at T\'upac Amaru Secondary Educational
Institution showed successful automation of attendance processes, saving 50
daily minutes of administrative time while providing real-time reporting
capabilities.

</details>


### [25] [UWB Radar-based Heart Rate Monitoring: A Transfer Learning Approach](https://arxiv.org/abs/2507.14195)
*Elzbieta Gruzewska,Pooja Rao,Sebastien Baur,Matthew Baugh,Mathias M. J. Bellaiche,Sharanya Srinivas,Octavio Ponce,Matthew Thompson,Pramod Rudrapatna,Michael A. Sanchez,Lawrence Z. Cai,Timothy JA Chico,Robert F. Storey,Emily Maz,Umesh Telang,Shravya Shetty,Mayank Daswani*

Main category: eess.SP

TL;DR: 该研究展示了在FMCW和IR-UWB雷达系统之间进行迁移学习，用于心率监测，显著提升了IR-UWB系统的性能。


<details>
  <summary>Details</summary>
Motivation: 利用雷达技术实现无接触、被动的心率监测，但不同雷达系统缺乏标准化，需大量配对数据集。

Method: 采用新型2D+1D ResNet架构，先在FMCW雷达数据上训练，再迁移到IR-UWB数据上微调。

Result: FMCW雷达模型MAE为0.85 bpm，MAPE为1.42%；迁移后IR-UWB模型MAE降低25%，达到4.1 bpm。

Conclusion: 迁移学习可加速雷达心率监测技术在消费设备中的应用。

Abstract: Radar technology presents untapped potential for continuous, contactless, and
passive heart rate monitoring via consumer electronics like mobile phones.
However the variety of available radar systems and lack of standardization
means that a large new paired dataset collection is required for each radar
system. This study demonstrates transfer learning between frequency-modulated
continuous wave (FMCW) and impulse-radio ultra-wideband (IR-UWB) radar systems,
both increasingly integrated into consumer devices. FMCW radar utilizes a
continuous chirp, while IR-UWB radar employs short pulses. Our mm-wave FMCW
radar operated at 60 GHz with a 5.5 GHz bandwidth (2.7 cm resolution, 3
receiving antennas [Rx]), and our IR-UWB radar at 8 GHz with a 500 MHz
bandwidth (30 cm resolution, 2 Rx). Using a novel 2D+1D ResNet architecture we
achieved a mean absolute error (MAE) of 0.85 bpm and a mean absolute percentage
error (MAPE) of 1.42% for heart rate monitoring with FMCW radar (N=119
participants, an average of 8 hours per participant). This model maintained
performance (under 5 MAE/10% MAPE) across various body positions and heart rate
ranges, with a 98.9% recall. We then fine-tuned a variant of this model,
trained on single-antenna and single-range bin FMCW data, using a small (N=376,
avg 6 minutes per participant) IR-UWB dataset. This transfer learning approach
yielded a model with MAE 4.1 bpm and MAPE 6.3% (97.5% recall), a 25% MAE
reduction over the IR-UWB baseline. This demonstration of transfer learning
between radar systems for heart rate monitoring has the potential to accelerate
its introduction into existing consumer devices.

</details>


### [26] [Explainable Parallel CNN-LSTM Model for Differentiating Ventricular Tachycardia from Supraventricular Tachycardia with Aberrancy in 12-Lead ECGs](https://arxiv.org/abs/2507.14196)
*Zahra Teimouri-Jervekani,Fahimeh Nasimi,Mohammadreza Yazdchi,Ghazal MogharehZadeh,Javad Tezerji,Farzan Niknejad Mazandarani,Maryam Mohebbi*

Main category: eess.SP

TL;DR: 提出了一种轻量级并行深度学习架构，用于高效区分宽复合心动过速（WCT），并通过SHAP增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于心室心动过速（VT）和伴差异性传导的室上性心动过速（SVT-A）在心电图（ECG）信号上的形态相似性，区分WCT具有临床挑战性，误诊风险高。

Method: 采用轻量级并行深度学习架构，通过1D-CNN块提取局部特征，LSTM层捕捉时间依赖性，SHAP提供解释性。

Result: 模型准确率达95.63%，敏感性和特异性均超过95%，计算效率优于现有方法。

Conclusion: 该框架实现了高精度WCT分类，计算开销低，SHAP增强了临床信任，适用于实际ECG分析。

Abstract: Background and Objective: Differentiating wide complex tachycardia (WCT) is
clinically critical yet challenging due to morphological similarities in
electrocardiogram (ECG) signals between life-threatening ventricular
tachycardia (VT) and supraventricular tachycardia with aberrancy (SVT-A).
Misdiagnosis carries fatal risks. We propose a computationally efficient deep
learning solution to improve diagnostic accuracy and provide model
interpretability for clinical deployment.
  Methods: A novel lightweight parallel deep architecture is introduced. Each
pipeline processes individual ECG leads using two 1D-CNN blocks to extract
local features. Feature maps are concatenated across leads, followed by LSTM
layers to capture temporal dependencies. Final classification employs fully
connected layers. Explainability is achieved via Shapley Additive Explanations
(SHAP) for local/global interpretation. The model was evaluated on a 35-subject
ECG database using standard performance metrics.
  Results: The model achieved $95.63\%$ accuracy ($95\%$ CI: $93.07-98.19\%$),
with sensitivity=$95.10\%$, specificity=$96.06\%$, and F1-score=$95.12\%$. It
outperformed state-of-the-art methods in both accuracy and computational
efficiency, requiring minimal CNN blocks per pipeline. SHAP analysis
demonstrated clinically interpretable feature contributions.
  Conclusions: Our end-to-end framework delivers high-precision WCT
classification with minimal computational overhead. The integration of SHAP
enhances clinical trust by elucidating decision logic, supporting rapid,
informed diagnosis. This approach shows significant promise for real-world ECG
analysis tools.

</details>


### [27] [Toward intelligent wireless networks in computer chassis](https://arxiv.org/abs/2507.14208)
*Mohammadreza F. Imani,Alexander L. Colson,Leslie K. Miller,Jorge A. Valdez,Jose C. Sanchez,Richard F. Rader*

Main category: eess.SP

TL;DR: 提出利用可重构智能表面（RIS）优化计算机机箱内的短距离无线通信（SRWC），以解决多径散射导致的信道脉冲响应（CIR）延长和符号间干扰（ISI）问题。


<details>
  <summary>Details</summary>
Motivation: 随着计算需求增长，传统有线互连在高速数据传输中效率降低，SRWC虽灵活但受多径散射限制，需解决ISI问题。

Method: 通过RIS调整反射波相位，使多径分量在接收端形成脉冲状CIR，实验验证了其在计算机机箱内的有效性。

Result: 实验证明RIS-enabled SRWC能有效优化无线链路，提升数据传输速率。

Conclusion: RIS技术为当前和未来数据处理单元的无线通信提供了可行解决方案。

Abstract: Processing the exponentially growing amount of data produced daily requires
efficient communication between different processing units in a computer.
Traditionally, wired interconnects have been used to maintain these data links
due to their energy efficiency and ability to support high data rates. However,
as computing demands continue to increase in size and speed, these wired
interconnects can become longer and less effective. One possible solution is to
enhance the wired interconnects with short-range wireless communication (SRWC),
which offers flexible resource allocation and the ability to broadcast data.
However, implementing SRWC inside a computer chassis presents challenges due to
multiple scattering. This scattering stretches the channel impulse response
(CIR), leading to inter-symbol interference (ISI) and limiting data rates. To
address this issue, we propose transforming the computer chassis into a smart
radio environment by utilizing a reconfigurable intelligent surface (RIS). The
RIS elements adjust the phase of reflected waves so that the multipath
components combine at the receiver in a way that creates a pulse-like CIR. This
approach has been experimentally validated within a typical computer chassis.
The results of this study pave the way for integrating RIS-enabled SRWC to
enhance wireless links in both current and future data processing units.

</details>


### [28] [System Design and Performance Analysis for RIS-assisted Terahertz Self-Alignment Beamforming](https://arxiv.org/abs/2507.14210)
*Jiayuan Wei,Qingwei Jiang,Wen Fang,Mingqing Liu,Qingwen Liu,Wen Chen,Qingqing Wu*

Main category: eess.SP

TL;DR: 论文提出了一种基于可重构智能表面（RIS）的太赫兹频段无线信息与能量同传（SWIPT）系统，通过自对准波束成形和主动放大技术，显著提升了传输效率和能量传输能力。


<details>
  <summary>Details</summary>
Motivation: 物联网（IoT）设备的广泛部署需要可持续的无线解决方案，能够同时传输信息和能量。太赫兹频段虽然提供高数据速率和带宽，但其窄波束特性导致严重的路径损耗和波束失准。

Method: 提出了一种RIS辅助的发射器架构，利用相位共轭电路实现自对准波束成形，并通过RIS阵列中的主动放大补偿级联信道衰减。

Result: 理论模型和仿真表明，系统显著减少了旁瓣干扰，在2米距离内实现了73.26%的传输效率。

Conclusion: RIS辅助的THz-SWIPT系统通过自对准和主动放大技术，有效解决了太赫兹频段的传输问题，为物联网设备提供了高效的无线信息与能量同传方案。

Abstract: The widespread deployment of Internet of Things(IoT) devices underscores the
need for sustainable wireless solutions capable of simultaneously transferring
both energy and information. Terahertz (THz) band-enabled simultaneous wireless
information and power transfer (SWIPT) systems offer ultra-high data rates and
expansive bandwidth. However, THz waves are inherently susceptible to severe
path loss and beam misalignment due to their narrow-beam characteristics. In
this context, this paper proposes a reconfigurable intelligent
surface(RIS)-assisted transmitter architecture for the THz-SWIPT system, which
enables end-to-end self-alignment for steady-state transmission. The proposed
system incorporates phase conjugate circuits to achieve self-aligned
beamforming, facilitating the dynamic tracking of mobile IoT devices without
the need for beam training. Additionally, active amplification within the RIS
arrays compensates for cascaded channel attenuation through an iterative power
cycle, thereby enhancing the energy transmission efficiency. Theoretical models
and simulations indicate that the proposed system significantly mitigates
sidelobe interference, achieving a transmission efficiency of up to 73.26% over
a 2 meter distance with self-alignment.

</details>


### [29] [Distributed Machine Learning Approach for Low-Latency Localization in Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2507.14216)
*Manish Kumar,Tzu-Hsuan Chou,Byunghyun Lee,Nicolò Michelusi,David J. Love,Yaguang Zhang,James V. Krogmeier*

Main category: eess.SP

TL;DR: 提出了一种分布式机器学习框架，用于6G网络中的低延迟定位，通过独立训练高斯过程回归模型并融合估计结果，减少延迟和计算负担。


<details>
  <summary>Details</summary>
Motivation: 支持实时应用的低延迟定位需求，适应6G网络架构。

Method: 每个接入点独立训练高斯过程回归模型，用户设备融合估计结果。

Result: 分布式框架在定位精度上与集中式方法相当，同时减少延迟和计算负担。

Conclusion: 分布式机器学习框架在6G网络中具有低延迟、高精度定位的潜力。

Abstract: Low-latency localization is critical in cellular networks to support
real-time applications requiring precise positioning. In this paper, we propose
a distributed machine learning (ML) framework for fingerprint-based
localization tailored to cell-free massive multiple-input multiple-output
(MIMO) systems, an emerging architecture for 6G networks. The proposed
framework enables each access point (AP) to independently train a Gaussian
process regression model using local angle-of-arrival and received signal
strength fingerprints. These models provide probabilistic position estimates
for the user equipment (UE), which are then fused by the UE with minimal
computational overhead to derive a final location estimate. This decentralized
approach eliminates the need for fronthaul communication between the APs and
the central processing unit (CPU), thereby reducing latency. Additionally,
distributing computational tasks across the APs alleviates the processing
burden on the CPU compared to traditional centralized localization schemes.
Simulation results demonstrate that the proposed distributed framework achieves
localization accuracy comparable to centralized methods, despite lacking the
benefits of centralized data aggregation. Moreover, it effectively reduces
uncertainty of the location estimates, as evidenced by the 95\% covariance
ellipse. The results highlight the potential of distributed ML for enabling
low-latency, high-accuracy localization in future 6G networks.

</details>


### [30] [Advanced Space Mapping Technique Integrating a Shared Coarse Model for Multistate Tuning-Driven Multiphysics Optimization of Tunable Filters](https://arxiv.org/abs/2507.14220)
*Haitian Hu,Wei Zhang,Feng Feng,Zhiguo Zhang,Qi-Jun Zhang*

Main category: eess.SP

TL;DR: 提出了一种基于共享电磁粗模型的空间映射技术，用于多状态调谐驱动的多物理场滤波器优化，结合计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 解决多物理场优化中计算成本高和精度不足的问题。

Method: 使用共享电磁粗模型和映射神经网络构建多子代理模型，同时优化多状态调谐。

Result: 相比现有方法，以更少的训练样本和计算成本实现了更高的多物理场建模精度。

Conclusion: 该方法在多物理场优化中具有显著优势，适用于多状态调谐需求。

Abstract: This article introduces an advanced space mapping (SM) technique that applies
a shared electromagnetic (EM)-based coarse model for multistate tuning-driven
multiphysics optimization of tunable filters. The SM method combines the
computational efficiency of EM single-physics simulations with the precision of
multiphysics simulations. The shared coarse model is based on EM single-physics
responses corresponding to various nontunable design parameters values.
Conversely, the fine model is implemented to delineate the behavior of
multiphysics responses concerning both nontunable and tunable design parameter
values. The proposed overall surrogate model comprises multiple subsurrogate
models, each consisting of one shared coarse model and two distinct mapping
neural networks. The responses from the shared coarse model in the EM
single-physics filed offer a suitable approximation for the fine responses in
the multiphysics filed, whereas the mapping neural networks facilitate
transition from the EM single-physics field to the multiphysics field. Each
subsurrogate model maintains consistent nontunable design parameter values but
possesses unique tunable design parameter values. By developing multiple
subsurrogate models, optimization can be simultaneously performed for each
tuning state. Nontunable design parameter values are constrained by all tuning
states, whereas tunable design parameter values are confined to their
respective tuning states. This optimization technique simultaneously accounts
for all the tuning states to fulfill the necessary multiple tuning state
requirements. Multiple EM and multiphysics training samples are generated
concurrently to develop the surrogate model. Compared with existing direct
multiphysics parameterized modeling techniques, our proposed method achieves
superior multiphysics modeling accuracy with fewer training samples and reduced
computational costs.

</details>


### [31] [Diffusion-based translation between unpaired spontaneous premature neonatal EEG and fetal MEG](https://arxiv.org/abs/2507.14224)
*Benoît Brebion,Alban Gallard,Katrin Sippel,Amer Zaylaa,Hubert Preissl,Sahar Moghimi,Fabrice Wallois,Yaël Frégier*

Main category: eess.SP

TL;DR: 该研究利用人工智能技术，将成熟的EEG知识迁移到fMEG中，以提升对胎儿大脑发育的理解，并开发了一种基于双扩散桥的非配对扩散翻译方法。


<details>
  <summary>Details</summary>
Motivation: 传统EEG研究早产儿脑活动已有显著进展，但胎儿阶段的脑发育仍是未知领域。fMEG是唯一能记录宫内神经活动的技术，但数据质量和稀缺性存在挑战。

Method: 开发了一种基于双扩散桥的非配对扩散翻译方法，改进了数值积分以降低计算成本，并利用30个高分辨率EEG和44个fMEG记录的数据集进行训练。

Result: 该方法在时间域均方误差上比GAN提升近5%，并完全解决了频域的模式崩溃问题，信号保真度接近完美。

Conclusion: 在EEG-fMEG非配对翻译问题上达到了新水平，为早期脑活动分析铺平了道路，且方法可推广至其他非配对信号翻译应用。

Abstract: Background and objective: Brain activity in premature newborns has
traditionally been studied using electroencephalography (EEG), leading to
substantial advances in our understanding of early neural development. However,
since brain development takes root at the fetal stage, a critical window of
this process remains largely unknown. The only technique capable of recording
neural activity in the intrauterine environment is fetal magnetoencephalography
(fMEG), but this approach presents challenges in terms of data quality and
scarcity. Using artificial intelligence, the present research aims to transfer
the well-established knowledge from EEG studies to fMEG to improve
understanding of prenatal brain development, laying the foundations for better
detection and treatment of potential pathologies. Methods: We developed an
unpaired diffusion translation method based on dual diffusion bridges, which
notably includes numerical integration improvements to obtain more qualitative
results at a lower computational cost. Models were trained on our unpaired
dataset of bursts of spontaneous activity from 30 high-resolution premature
newborns EEG recordings and 44 fMEG recordings. Results: We demonstrate that
our method achieves significant improvement upon previous results obtained with
Generative Adversarial Networks (GANs), by almost 5% on the mean squared error
in the time domain, and completely eliminating the mode collapse problem in the
frequency domain, thus achieving near-perfect signal fidelity. Conclusion: We
set a new state of the art in the EEG-fMEG unpaired translation problem, as our
developed tool completely paves the way for early brain activity analysis.
Overall, we also believe that our method could be reused for other unpaired
signal translation applications.

</details>


### [32] [Design of A New Multiple-Chirp-Rate Index Modulation for LoRa Networks](https://arxiv.org/abs/2507.14228)
*Xiaobin Zhu,Minling Zhang,Guofa Cai,Jiguang He,Georges Kaddoum*

Main category: eess.SP

TL;DR: 提出了一种基于Zadoff-Chu序列的多啁啾速率索引调制（MCR-IM）系统，解决了传统LoRa网络传输速率低和大规模接入的问题。该系统具有极低的跨相关性和更高的频谱效率，并通过PD-SIC算法支持更多用户。


<details>
  <summary>Details</summary>
Motivation: 解决传统LoRa网络传输速率低和大规模接入的问题。

Method: 基于Zadoff-Chu序列的多啁啾速率索引调制（MCR-IM）系统，结合PD-SIC算法。

Result: MCR-IM系统在Nakagami-m衰落信道下具有较低的误码率（BER），频谱效率提升16%至21%。

Conclusion: MCR-IM系统适用于大规模、高速率的LoRa网络应用。

Abstract: We propose a multiple chirp rate index modulation (MCR-IM) system based on
Zadoff-Chu (ZC) sequences that overcomes the problems of low transmission rate
and large-scale access in classical LoRa networks. We demonstrate the extremely
low cross-correlation of MCR-IM signals across different spread factors,
showing that the proposed MCR-IM system also inherits the characteristics of ZC
sequences modulation. Moreover, we derive an approximate closed-form expression
for the bit-error rate (BER) of the proposed MCR-IM system over Nakagami-m
fading channels. Simulation results confirm the accuracy of the derived
closed-form expression and demonstrate that the MCR-IM system achieves higher
levels of spectral efficiency (SE) compared to existing systems. In this
context, assigning multiple chirp rates to each user results in a reduction in
the number of parallel channels. To mitigate this issue, we propose a peak
detection based successive interference cancellation (PD-SIC) algorithm to
accommodate more users. Compared to orthogonal scatter chirp spreading spectrum
system that names OrthoRa, the MCR-IM system with PD-SIC algorithm achieves
lower BER levels. For a similar number of collision signals, the throughput of
the MCR-IM system is enhanced by 16% to 21%. Owing to these advantages, the
proposed MCR-IM is well suited for large-scale, high-rate LoRa network
applications.

</details>


### [33] [Age of Information Minimization in UAV-Enabled Integrated Sensing and Communication Systems](https://arxiv.org/abs/2507.14299)
*Yu Bai,Yifan Zhang,Boxuan Xie,Zheng Chang,Yanru Zhang,Riku Jantti,Zhu Han*

Main category: eess.SP

TL;DR: 提出了一种基于信息新鲜度（AoI）的无人机-ISAC系统，通过深度强化学习优化轨迹和波束成形，平衡感知与通信性能。


<details>
  <summary>Details</summary>
Motivation: 无人机在无线网络中具有灵活性，但资源受限下联合优化轨迹、通信和感知仍具挑战性。

Method: 采用深度强化学习（DRL）和卡尔曼滤波，优化无人机轨迹与波束成形，平衡感知精度与通信质量。

Result: 仿真表明，该方法在平均AoI上优于基线方法。

Conclusion: 提出的AoI中心框架有效解决了无人机-ISAC系统的联合优化问题。

Abstract: Unmanned aerial vehicles (UAVs) equipped with integrated sensing and
communication (ISAC) capabilities are envisioned to play a pivotal role in
future wireless networks due to their enhanced flexibility and efficiency.
However, jointly optimizing UAV trajectory planning, multi-user communication,
and target sensing under stringent resource constraints and time-critical
conditions remains a significant challenge. To address this, we propose an Age
of Information (AoI)-centric UAV-ISAC system that simultaneously performs
target sensing and serves multiple ground users, emphasizing information
freshness as the core performance metric. We formulate a long-term average AoI
minimization problem that jointly optimizes the UAV's flight trajectory and
beamforming. To tackle the high-dimensional, non-convexity of this problem, we
develop a deep reinforcement learning (DRL)-based algorithm capable of
providing real-time decisions on UAV movement and beamforming for both radar
sensing and multi-user communication. Specifically, a Kalman filter is employed
for accurate target state prediction, regularized zero-forcing is utilized to
mitigate inter-user interference, and the Soft Actor-Critic algorithm is
applied for training the DRL agent on continuous actions. The proposed
framework adaptively balances the trade-offs between sensing accuracy and
communication quality. Extensive simulation results demonstrate that our
proposed method consistently achieves lower average AoI compared to baseline
approaches.

</details>


### [34] [Fast and Robust Stationary Crowd Counting with Commodity WiFi](https://arxiv.org/abs/2507.14309)
*Mert Torun,Alireza Parsay,Yasamin Mostofi*

Main category: eess.SP

TL;DR: 提出一种利用WiFi信号通过人体自然晃动行为估计坐姿人群规模的新方法，通过信号带宽作为更细粒度的指标，结合异常检测模块提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法对人群规模的估计通常基于二元的晃动表示，缺乏细粒度和鲁棒性。本文旨在利用WiFi信号带宽作为更精确的指标，并通过异常检测模块解决无关运动的干扰。

Method: 提出数学模型，将信号带宽的概率密度函数（PDF）与人群规模关联，通过公开视频提取个体晃动PDF，并引入异常检测模块过滤无关运动。

Result: 在42次实验中，平均绝对误差为1.04，归一化均方误差为0.15，收敛时间平均51秒，优于现有技术。

Conclusion: 该方法能够快速、鲁棒且高精度地估计坐姿人群规模，并具备扩展到更大规模人群的潜力。

Abstract: This paper introduces a novel method for estimating the size of seated crowds
with commodity WiFi signals, by leveraging natural body fidgeting behaviors as
a passive sensing cue. Departing from prior binary fidget representations, our
approach leverages the bandwidth of the received signal as a finer-grained and
robust indicator of crowd counts. More specifically, we propose a mathematical
model that relates the probability density function (PDF) of the signal
bandwidth to the crowd size, using a principled derivation based on the PDF of
an individual's fidget-induced bandwidth. To characterize the individual
fidgeting PDF, we use publicly available online videos, each of a seated
individual, from which we extract body motion profiles using vision techniques,
followed by a speed-to-bandwidth conversion inspired by Carson's Rule from
analog FM radio design. Finally, to enhance robustness in real-world
deployments where unrelated motions may occur nearby, we further introduce an
anomaly detection module that filters out non-fidget movements. We validate our
system through 42 experiments across two indoor environments with crowd sizes
up to and including 13 people, achieving a mean absolute error of 1.04 and a
normalized mean square error of 0.15, with an average convergence time of 51
seconds, significantly reducing the convergence time as compared to the state
of the art. Additional simulation results demonstrate scalability to larger
crowd sizes. Overall, our results show that our pipeline enables fast, robust,
and highly accurate counting of seated crowds.

</details>


### [35] [Optimizing Network Performance and Resource Allocation in HAPS-UAV Integrated Sensing and Communication Systems for 6G](https://arxiv.org/abs/2507.14310)
*Parisa Kanani,Mohammad Javad Omidi,Mahmoud Modarres-Hashemi,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: 本文提出了一种利用无人机（UAV）作为基站（BS）和高空平台站（HAPS）作为中央处理单元（CPU）的6G网络集成感知与通信（ISAC）系统，优化位置和功率控制以提升性能和覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 探索6G网络中ISAC系统的挑战、应用和优势，特别是在偏远地区增强无线覆盖的需求。

Method: 通过HAPS和UAV的协同工作，UAV作为双功能接入点，分时传输通信和感知信号，并通过HAPS进行信号对齐和优化。

Result: 仿真结果表明该方法能有效提升网络性能、资源分配公平性和系统优化。

Conclusion: 利用HAPS作为CPU减轻UAV的计算负担，提高能效和网络性能。

Abstract: This paper proposes an innovative approach by leveraging uncrewed aerial
vehicles (UAVs) as base stations (BSs) and a high-altitude platform station
(HAPS) as the central processing unit (CPU) in an integrated sensing and
communication (ISAC) system for 6G networks. We explore the challenges,
applications, and advantages of ISAC systems in next-generation networks,
highlighting the significance of optimizing position and power control. Our
approach integrates HAPS and UAVs to enhance wireless coverage, particularly in
remote areas. UAVs function as dual-purpose access points (APs), using their
maneuverability and line-of-sight (LoS) aerial-to-ground (A2G) links to
transmit combined communication and sensing signals. The scheme operates in two
time slots: in the first slot, UAVs transmit dedicated signals to communication
users (CUs) and potential targets. UAVs detect targets in specific ground
locations and, after signal transmission, receive reflected signals from
targets. In the second slot, UAVs relay these signals to HAPS, which performs
beamforming to align signals for each CU from various UAVs. UAVs decode
information from HAPS and adjust transmissions to maximize the beam pattern
efficiency toward the desired targets. We formulate a multi-objective
optimization problem to maximize both the minimum
signal-to-interference-plus-noise ratio (SINR) for CUs and the echo signal
power from the targets. This is achieved by finding the optimal power
allocation for CUs in each UAV, subject to constraints on the maximum total
power in each UAV and the transmitted beam pattern gain. Simulation results
demonstrate the effectiveness of this approach in enhancing network
performance, resource allocation, fairness, and system optimization. Using HAPS
as the CPU, computational tasks are offloaded from UAVs, which conserves energy
and improves network performance.

</details>


### [36] [Spatially tailored spin wave excitation for spurious-free, low-loss magnetostatic wave filters with ultra-wide frequency tunability](https://arxiv.org/abs/2507.14469)
*Shuxian Wu,Shun Yao,Xingyu Du,Chin-Yu Chang,Roy H. Olsson III*

Main category: eess.SP

TL;DR: 论文提出了一种半锥形换能器，用于优化钇铁石榴石磁静波射频腔滤波器，显著抑制杂散模式，提升6G通信系统的性能。


<details>
  <summary>Details</summary>
Motivation: 钇铁石榴石磁静波射频腔滤波器在6G通信系统中具有广阔的应用前景，但其杂散模式严重影响了滤波器性能。

Method: 采用半锥形换能器，通过空间定制自旋波激发，选择性增强主腔模式并抑制杂散模式。

Result: 实验验证了单腔和双腔半锥形磁静波滤波器，分别实现了6.3-16.8 GHz和9.8-31.5 GHz的无杂散调谐范围，插入损耗低至2.4-3.8 dB。

Conclusion: 该技术显著提升了滤波器的性能，为6G网络提供了高度可重构和稳健的解决方案。

Abstract: Yttrium iron garnet magnetostatic wave (MSW) radio frequency (RF) cavity
filters are promising for sixth-generation (6G) communication systems due to
their wide frequency tunability. However, the presence of severe spurious modes
arising from the finite cavity dimensions severely degrades the filter
performance. We present a half-cone transducer that spatially tailors spin wave
excitation to selectively enhance the primary cavity modes comprising the MSW
filter passband, while strongly suppressing the undesired spurious modes.
Theoretical analysis, numerical simulations and experiments verify the
effectiveness of the spatially tailored technique. We utilize the half-cone
transducer to demonstrate a spurious-free, single-cavity half-cone MSW filter
(HC-MSWF) with an insertion loss (IL) of 2.4-3.2 dB over a frequency tuning
range of 6.3-16.8 GHz. Extending our study, we further demonstrate a
spurious-free, dual-cavity HC-MSWF with an unprecedented tuning range of 21.7
GHz (9.8-31.5 GHz) while maintaining a low IL of 2.9-3.8 dB. This significant
advance in performance will enable highly reconfigurable and robust 6G
networks.

</details>


### [37] [Propagation Channel Modeling for LEO Satellite Missions Using Ray-Tracing Simulations](https://arxiv.org/abs/2507.14622)
*Wahab Khawaja,Ismail Guvenc,Rune Hylsberg Jacobsen*

Main category: eess.SP

TL;DR: 本文提出了一种基于射线追踪的高分辨率LEO卫星到地面链路的X波段信道模型，结合了大尺度和小尺度衰落效应，并首次整合了射线追踪环境动态、仰角相关衰落和相控阵波束失准效应。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了在郊区环境中为LEO卫星到地面链路开发一个全面的X波段信道模型，以解决大尺度和小尺度衰落效应，并量化地面站天线失准对链路性能的影响。

Method: 方法包括使用Wireless InSite进行射线追踪模拟，开发参数化信道模型，比较3GPP NTN信道模型，并拟合阴影和非阴影Rician分布来建模小尺度衰落。

Result: 结果表明，模型成功表征了不同卫星仰角下的衰落效应，并量化了天线失准导致的链路退化。

Conclusion: 结论是本文首次提出了一个全面的仰角感知信道模型，为X波段卫星到地面传播提供了新的建模方法。

Abstract: This work presents a high-resolution, ray-tracing-based channel modeling for
Low Earth Orbit (LEO) satellite-to-ground links in a suburban environment at
X-band. Using simulations conducted in Wireless InSite, we develop a parametric
channel model that characterizes both large- and small-scale fading effects
across different satellite elevation angles. Large-scale fading incorporates
attenuation due to terrain-induced shadowing and dynamic environmental factors
such as weather conditions, and is compared with 3GPP NTN channel model.
Additionally, we quantify link degradation resulting from ground station (GS)
antenna misalignment, considering both fixed single-element and electronically
steerable phased-array antennas. Small-scale fading is modeled by fitting a
shadowed and non-shadowed Rician distribution to the fading statistics at
various satellite elevations. To the best of our knowledge, this is the first
study to propose a comprehensive elevation-aware channel model for
satellite-to-ground propagation at X-band, integrating ray-traced environmental
dynamics, elevation-dependent fading, and phased-array beam misalignment
effects.

</details>


### [38] [Movable-Element STARS-Aided Secure Communications](https://arxiv.org/abs/2507.14804)
*Jingjing Zhao,Qian Xu,Kaiquan Cai,Yanbo Zhu,Xidong Mu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 论文提出了一种基于可移动元件（ME）的STARS系统（ME-STARS），用于增强物理层安全性，通过优化被动波束成形、ME位置和主动波束成形来最大化保密速率。


<details>
  <summary>Details</summary>
Motivation: 针对全空间窃听问题，通过部署可移动元件（ME）以利用更高的空间自由度来增强通信系统的物理层安全性。

Method: 采用交替优化算法，将非凸优化问题分解为三个子问题：ME位置优化（使用梯度上升算法）、被动波束成形和主动波束成形（使用逐次凸近似）。

Result: 数值结果表明，ME-STARS相比固定元件STARS显著提升了保密性能，但保密速率在有限移动区域内趋于饱和。

Conclusion: ME-STARS通过优化可移动元件位置和波束成形，有效提升了通信系统的安全性，但需注意移动区域的限制对性能的影响。

Abstract: A novel movable-element (ME) enabled simultaneously transmitting and
reflecting surface (ME-STARS)-aided secure communication system is
investigated. Against the full-space eavesdropping, MEs are deployed at the
STARS for enhancing the physical layer security by exploiting higher spatial
degrees of freedom. Specifically, a sum secrecy rate maximization problem is
formulated, which jointly optimizes the passive beamforming and the MEs
positions at the ME-STARS, as well as the active beamforming at the base
station. To solve the resultant non-convex optimization problem involving
highly-coupled variables, an alternating optimization-based iterative algorithm
is developed, decomposing the original problem into three subproblems. In
particular, for the MEs position optimization subproblem, a gradient ascent
algorithm is employed to iteratively refine the MEs' locations within the
confined region. Moreover, the the active and passive beamforming subproblems
are solved by employing successive convex approximation. Numerical results
unveil that: 1) ME-STARS significantly improves the secrecy performance
compared to the conventional STARS with fixed-position elements; and 2) The
secrecy rate achieved by the ME-STARS gets saturated within limited movable
region size.

</details>


### [39] [Pinching-Antenna-based Communications: Spectral Efficiency Analysis and Deployment Strategies](https://arxiv.org/abs/2507.14831)
*Mengyu Qian,Xidong Mu,Li You,Michail Matthaiou*

Main category: eess.SP

TL;DR: 研究了基于多波导夹持天线（PA）的多用户通信系统，比较了集中式和分布式部署策略的频谱效率（SE）。集中式部署通过波束成形增益服务用户，而分布式部署利用复用增益同时服务多用户。分析表明，高信噪比（SNR）下分布式更优，低SNR下集中式更佳。


<details>
  <summary>Details</summary>
Motivation: 探索多波导夹持天线系统在不同部署策略下的性能差异，以优化多用户通信的频谱效率。

Method: 比较集中式和分布式PA部署策略，分析其频谱效率，并利用最大比传输（MRT）和稳态相位方法进行近似分析。

Result: 高SNR下分布式部署更优，低SNR下集中式部署更佳。MRT在波导间距较大时可忽略用户间干扰。

Conclusion: 根据SNR选择部署策略可最大化系统频谱效率，理论分析通过仿真验证。

Abstract: A multiple-waveguide pinching-antenna (PA)-based multi-user communication
system is investigated. With a given number of PAs, two deployment strategies
are considered, namely the centralized PA deployment, where all PAs are
switched between waveguides to serve users in a time-division manner to avail
of beamforming gain, and the distributed PA deployment, where a single PA is
deployed on each waveguide to simultaneously serve multiple users by leveraging
the multiplexing gain. The spectral efficiency (SE) achieved by each deployment
strategy is analyzed: i) For the centralized deployment, the positioning
strategy of PAs on each waveguide is determined first with the aim of
maximizing the channel gain of the corresponding nearest served user. Based on
this, the corresponding system SE is derived. ii) For the distributed
deployment, the system SE under the maximum ratio transmission (MRT) is first
obtained. To obtain an analytically tractable form, the stationary phase method
is utilized to approximate the system SE. The approximation result reveals that
the average inter-user interference can be negligible with a large waveguide
spacing and thus the simple MRT is appealing for PA-based multi-user
communications. Furthermore, the system SEs achieved by the two strategies are
compared in both the high and low signal-to-noise ratio (SNR) regimes. Our
analysis suggests that at high SNRs, the distributed deployment is superior to
achieve the maximal system SE, while the centralized deployment is more
suitable for the low-SNR regime. Finally, the theoretical analysis is verified
through simulations.

</details>


### [40] [Integrated Radio Sensing Capabilities for 6G Networks: AI/ML Perspective](https://arxiv.org/abs/2507.14856)
*Victor Shatov,Steffen Schieler,Charlotte Muth,José Miguel Mateos-Ramos,Ivo Bizon,Florian Euchner,Sebastian Semper,Stephan ten Brink,Gerhard Fettweis,Christian Häger,Henk Wymeersch,Laurent Schmalen,Reiner Thomä,Norman Franchi*

Main category: eess.SP

TL;DR: 本文是关于6G无线通信中AI和ML如何增强感知能力的教程式综述，涵盖雷达、频谱感知等多种应用。


<details>
  <summary>Details</summary>
Motivation: 6G通信被称为“连接智能”，结合AI和ML的无线感知技术有望提升环境感知能力，推动下一代无线网络的发展。

Method: 在ISAC框架下，扩展“感知”概念，涵盖雷达、频谱感知等应用，并详细介绍了AI技术解决无线感知问题的前沿方法。

Result: 讨论了AI驱动的多模态多任务网络中集成感知能力的优势、推动因素和挑战。

Conclusion: 本文为初学者提供了研究起点，也为无线通信背景的读者提供了全面的研究概述。

Abstract: The sixth-generation wireless communications (6G) is often labeled as
"connected intelligence". Radio sensing, aligned with machine learning (ML) and
artificial intelligence (AI), promises, among other benefits, breakthroughs in
the system's ability to perceive the environment and effectively utilize this
awareness. This article offers a tutorial-style survey of AI and ML approaches
to enhance the sensing capabilities of next-generation wireless networks. To
this end, while staying in the framework of integrated sensing and
communication (ISAC), we expand the term "sensing" from radar, via spectrum
sensing, to miscellaneous applications of radio sensing like non-cooperative
transmitter localization. We formulate the problems, explain the
state-of-the-art approaches, and detail AI-based techniques to tackle various
objectives in the context of wireless sensing. We discuss the advantages,
enablers, and challenges of integrating various sensing capabilities into an
envisioned AI-powered multimodal multi-task network. In addition to the
tutorial-style core of this work based on direct authors' involvement in 6G
research problems, we review the related literature, and provide both a good
start for those entering this field of research, and a topical overview for a
general reader with a background in wireless communications

</details>


### [41] [Stabilization of the bias point in MZM modulators](https://arxiv.org/abs/2507.14888)
*Zhuo Wang*

Main category: eess.SP

TL;DR: 本文介绍了MZM在通信系统中的作用、材料（如铌酸锂）、工作原理及其受环境因素影响的原因，提出了一种通过算法控制电压来稳定MZM的方法，并通过实验验证了该算法。最后总结了MZM的未来发展前景。


<details>
  <summary>Details</summary>
Motivation: 研究MZM在通信系统中的实际应用及其稳定性问题，提出一种算法控制方法以解决环境因素对MZM性能的影响。

Method: 通过算法控制电压来稳定MZM的工作点，并通过实验验证该方法的有效性。

Result: 实验验证了算法控制电压的方法可以有效稳定MZM的性能。

Conclusion: MZM在通信系统中具有重要作用，未来可通过进一步优化算法提升其稳定性和应用范围。

Abstract: This article mainly introduces the role of MZM in practical communication
systems, the materials used to make MZM modulators such as lithium niobate, and
its working principle. It also explains why it changes due to environmental
factors. This leads to the introduction of a method that controls the stable
points of MZM by algorithmically controlling the voltage, and the algorithm is
verified through experiments. Finally, a summary and outlook on the future
development of MZM are provided.

</details>


### [42] [Phase-optimised linearly-constrained minimum-variance beamformers](https://arxiv.org/abs/2507.14937)
*Hugh L Kennedy*

Main category: eess.SP

TL;DR: 提出了一种确定线性约束最小方差（LCMV）波束形成器最优群延迟的新方法，包括最小化噪声功率和处理延迟两种选择。


<details>
  <summary>Details</summary>
Motivation: 探索LCMV波束形成器中未充分利用的设计自由度，以优化性能。

Method: 通过模拟VHF通信和UHF双基地雷达应用，评估两种最优延迟选择方法。

Result: 验证了两种方法在噪声功率和处理延迟优化中的有效性。

Conclusion: 该方法为LCMV波束形成器设计提供了新的优化维度。

Abstract: A novel procedure for the determination of the optimal group-delay for a
Linearly-Constrained Minimum-Variance (LCMV) beamformer is proposed. Two ways
of selecting the optimal delay are recommended: the first is the solution that
minimizes the noise power; the second is the solution that minimizes the
processing delay. The potential of this hitherto unexplored degree of design
freedom is explored using simulated Very-High-Frequency (VHF) communication,
and Ultra-High-Frequency (UHF) bistatic radar, applications.

</details>


### [43] [Jamming-Resistant AAV Communications: A Multichannel-Aided Approach](https://arxiv.org/abs/2507.14945)
*Bin Wang,Jun Fang,Jieru Du,Shihai Shao*

Main category: eess.SP

TL;DR: 提出一种多通道辅助的干扰消除方法，用于无人自主车辆通信，无需信道状态信息，仅利用合法发送方的导频序列即可实现同步和干扰消除。


<details>
  <summary>Details</summary>
Motivation: 在恶意干扰存在的情况下，确保无人自主车辆通信的可靠性至关重要。

Method: 开发了一种多通道辅助的干扰消除方法，无需信号或干扰的信道状态信息，仅利用合法发送方的导频序列。

Result: 实验结果表明，该方法在干扰信号比通信信号强40dB时仍能成功解码目标信号。

Conclusion: 该方法为无人自主车辆通信提供了一种实用的干扰消除解决方案。

Abstract: Jamming cancellation is essential to reliable unmanned autonomous vehicle
(AAV) communications in the presence of malicious jammers. In this paper, we
develop a practical multichannel-aided jamming cancellation method to realize
secure AAV communications. The proposed method is capable of simultaneously
achieving timing/frequency synchronization as well as jamming cancellation.
More importantly, our method does not need the signal's/jammer's channel state
information. It only utilizes the knowledge of the legitimate sender's preamble
sequence that is available in existing communication protocols. We also analyze
the length of the preamble sequence required for successful synchronization and
signal recovery. Experimental results on the built hardware platform show that,
with a two-antenna receiver, the proposed method can successfully decode the
signal of interest even when the jamming signal is $40$dB stronger than the
communication signal.

</details>


### [44] [Latent-attention Based Transformer for Near ML Polar Decoding in Short-code Regime](https://arxiv.org/abs/2507.14951)
*Hongzhi Zhu,Wei Xu,Xiaohu You*

Main category: eess.SP

TL;DR: 提出了一种基于潜在注意力的Transformer解码器（LAT），通过改进注意力机制、训练框架和掩码方案，显著提升了短码极化码的解码性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的解码器在短码极化码解码中性能不足，无法与传统代数解码器竞争，因此需要改进。

Method: 1. 提出潜在注意力机制替代自注意力；2. 设计包含熵感知重要性采样、经验回流和动态标签平滑的训练框架；3. 引入代码感知掩码方案。

Result: LAT解码器在短码极化码中接近最大似然性能（BER和BLER），且在不同码率和码长下表现鲁棒。

Conclusion: LAT解码器通过创新设计显著提升了Transformer在极化码解码中的性能和泛化能力。

Abstract: Transformer architectures have emerged as promising deep learning (DL) tools
for modeling complex sequence-to-sequence interactions in channel decoding.
However, current transformer-based decoders for error correction codes (ECCs)
demonstrate inferior performance and generalization capabilities compared to
conventional algebraic decoders, especially in short-code regimes. In this
work, we propose a novel latent-attention based transformer (LAT) decoder for
polar codes that addresses the limitations on performance and generalization
through three pivotal innovations. First, we develop a latent-attention
mechanism that supersedes the conventional self-attention mechanism. This
architectural modification enables independent learning of the Query and Key
matrices for code-aware attention computation, decoupling them from the Value
matrix to emphasize position-wise decoding interactions while reducing context
correlation interference. Second, we devise an advanced training framework
incorporating three synergistic components: entropy-aware importance sampling
that emphasizes low-probability regions in the signal constellation space,
experience reflow that introduces empirical labels to improve characterization
of decoding boundaries, and dynamic label smoothing for likelihood-based
regularization. Third, we propose a code-aware mask scheme which allows dynamic
adaptation for varying code configurations. Numerical evaluations demonstrate
that the proposed LAT decoder achieves near maximum-likelihood (ML) performance
in terms of both bit error rate (BER) and block error rate (BLER) for
short-length polar codes. Furthermore, the architecture exhibits robust
generalization capabilities across diverse code rates and code lengths.

</details>


### [45] [How Many Simultaneous Beamformers are Needed for Integrated Sensing and Communications?](https://arxiv.org/abs/2507.14982)
*Kareem M. Attiah,Wei Yu*

Main category: eess.SP

TL;DR: 论文研究了ISAC系统中所需的最小波束形成器数量，分析了感知和通信性能的权衡，并给出了不同干扰条件下的理论界限。


<details>
  <summary>Details</summary>
Motivation: 探索在ISAC系统中如何通过优化波束形成器数量，同时实现高效的通信和感知性能。

Method: 通过理论分析，建立基于Cramér-Rao界和信号干扰噪声比的性能度量，推导波束形成器数量的下限。

Result: 在干扰可消除时，波束形成器数量上限为K + √(L(L+1)/2)；不可消除时为√(K² + L(L+1)/2)。

Conclusion: ISAC系统的波束形成器数量受干扰条件影响，优化设计可显著提升性能。

Abstract: Consider a downlink integrated sensing and communications (ISAC) system in
which a base station employs linear beamforming to communicate to $K$ users,
while simultaneously uses sensing beams to perform a sensing task of estimating
$L$ real parameters. How many beamformers are needed to achieve the best
performance for both sensing and communications? This paper establishes bounds
on the minimum number of downlink beamformers, in which sensing performance is
measured in terms of the Cram\'{e}r-Rao bound for parameter estimation and
communications performance is measured in terms of the
signal-to-interference-and-noise ratios. We show that an ISAC system requires
at most $K + \sqrt{\frac{L(L+1)}{2}}$ beamformers if the remote users have the
ability to cancel the interference caused by the sensing beams. If cancelling
interference due to the sensing beams is not possible, the bound becomes
$\sqrt{K^2 + \frac{L(L+1)}{2}}$. Interestingly, in the latter case, the bound
on the number of beamformers is less than the sum of the bounds for each task
individually. These results can be extended to sensing tasks for which the
performance is measured as a function of $d$ quadratic terms in the
beamformers. In this case, the bound becomes $K + \sqrt{d}$ and $\sqrt{K^2 +
d}$, respectively. Specifically, for estimating complex path losses and
angles-of-arrival of $N_\text{tr}$ targets while communicating to $K$ users,
the bound on the minimum number of beamformers scales linearly in $K$ and in
$N_\text{tr}$, assuming interference from sensing can be cancelled. When
interference cancellation is not possible, the following exact characterization
for the case of $N_\text{tr} = 1$ can be obtained: when $K=0$ or $1$, two
beamformers should be used; when $K \ge 2$, exactly $K$ beamformers should be
used, i.e., communication beamformers alone are already sufficient.

</details>


### [46] [PAPR Analysis for MIMO FTN Signaling with Gaussian Symbols](https://arxiv.org/abs/2507.15116)
*Zichao Zhang,Melda Yuksel,Gokhan M. Guvensen,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: 研究了MIMO FTN系统中高斯符号在两种功率约束下的PAPR行为，发现PAPR主要由加速因子和功率约束决定，功率分配优化对高斯信号的PAPR行为无影响。


<details>
  <summary>Details</summary>
Motivation: FTN信号作为提高频谱效率的解决方案，但其快速加速特性导致脉冲高度重叠，恶化了PAPR性能。

Method: 分析MIMO FTN系统中高斯符号在固定发射功率和固定接收SNR两种功率约束下的PAPR行为。

Result: PAPR主要由加速因子和功率约束决定，功率分配优化对高斯信号的PAPR行为无显著影响。

Conclusion: 在MIMO FTN系统中，PAPR行为受加速因子和功率约束主导，功率分配优化对高斯信号无效。

Abstract: Faster-than-Nyquist signaling serves as a promising solution for improving
spectral efficiency in future generations of communications. However, its
nature of fast acceleration brings highly overlapped pulses that lead to worse
peak-to-average power ratio (PAPR) performance. In this paper, we investigate
the PAPR behavior of MIMO FTN using Gaussian symbols under optimal power
allocation for two power constraints: fixed transmit power and fixed received
signal-to-noise-ratio (SNR). Our findings reveal that PAPR is mainly determined
by the acceleration factor and the power constraint, but power allocation
optimization does not change the PAPR behavior for Gaussian signaling.

</details>


### [47] [Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings](https://arxiv.org/abs/2507.15118)
*Szymon Mazurek,Stephen Moore,Alessandro Crimi*

Main category: eess.SP

TL;DR: 提出了一种基于图深度学习的框架，用于从低成本脑电图（EEG）硬件中检测癫痫，并在尼日利亚和几内亚比绍的测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 癫痫在低收入国家诊断不足，原因是神经科医生稀缺和诊断工具昂贵。目标是开发公平、可访问且可解释的自动评估方法。

Method: 将EEG信号建模为时空图，使用图注意力网络（GAT）进行分类，并识别通道间关系和时态动态。针对低质量记录设计了信号预处理和轻量级GAT架构。

Result: 分类性能优于随机森林和图卷积网络，突出了额颞区的特定连接。

Conclusion: GATs为欠发达地区提供了有潜力且可扩展的癫痫诊断支持，推动了经济实惠的神经诊断工具的发展。

Abstract: Goal: Epilepsy remains under-diagnosed in low-income countries due to scarce
neurologists and costly diagnostic tools. We propose a graph-based deep
learning framework to detect epilepsy from low-cost Electroencephalography
(EEG) hardware, tested on recordings from Nigeria and Guinea-Bissau. Our focus
is on fair, accessible automatic assessment and explainability to shed light on
epilepsy biomarkers. Methods: We model EEG signals as spatio-temporal graphs,
classify them, and identify interchannel relationships and temporal dynamics
using graph attention networks (GAT). To emphasize connectivity biomarkers, we
adapt the inherently node-focused GAT to analyze edges. We also designed signal
preprocessing for low-fidelity recordings and a lightweight GAT architecture
trained on Google Colab and deployed on RaspberryPi devices. Results: The
approach achieves promising classification performance, outperforming a
standard classifier based on random forest and graph convolutional networks in
terms of accuracy and robustness over multiple sessions, but also highlighting
specific connections in the fronto-temporal region. Conclusions: The results
highlight the potential of GATs to provide insightful and scalable diagnostic
support for epilepsy in underserved regions, paving the way for affordable and
accessible neurodiagnostic tools.

</details>


### [48] [MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images, Features and Interpretations](https://arxiv.org/abs/2507.15255)
*Deyun Zhang,Xiang Lan,Shijia Geng,Qinghao Zhao,Sumei Fan,Mengling Feng,Shenda Hong*

Main category: eess.SP

TL;DR: MEETI是一个多模态ECG数据集，整合了原始波形、图像、文本和特征参数，为心血管AI研究提供了全面基准。


<details>
  <summary>Details</summary>
Motivation: 现有ECG数据集多为单模态或双模态，限制了多模态AI系统的开发。MEETI填补了这一空白。

Method: MEETI同步了原始波形、高分辨率图像、详细文本解释和特征参数，通过统一标识符对齐。

Result: MEETI支持基于Transformer的多模态学习，为可解释的心血管AI提供了基础。

Conclusion: MEETI为下一代可解释、多模态心血管AI建立了坚实基础，是研究和评估ECG AI系统的综合基准。

Abstract: Electrocardiogram (ECG) plays a foundational role in modern cardiovascular
care, enabling non-invasive diagnosis of arrhythmias, myocardial ischemia, and
conduction disorders. While machine learning has achieved expert-level
performance in ECG interpretation, the development of clinically deployable
multimodal AI systems remains constrained, primarily due to the lack of
publicly available datasets that simultaneously incorporate raw signals,
diagnostic images, and interpretation text. Most existing ECG datasets provide
only single-modality data or, at most, dual modalities, making it difficult to
build models that can understand and integrate diverse ECG information in
real-world settings. To address this gap, we introduce MEETI (MIMIC-IV-Ext
ECG-Text-Image), the first large-scale ECG dataset that synchronizes raw
waveform data, high-resolution plotted images, and detailed textual
interpretations generated by large language models. In addition, MEETI includes
beat-level quantitative ECG parameters extracted from each lead, offering
structured parameters that support fine-grained analysis and model
interpretability. Each MEETI record is aligned across four components: (1) the
raw ECG waveform, (2) the corresponding plotted image, (3) extracted feature
parameters, and (4) detailed interpretation text. This alignment is achieved
using consistent, unique identifiers. This unified structure supports
transformer-based multimodal learning and supports fine-grained, interpretable
reasoning about cardiac health. By bridging the gap between traditional signal
analysis, image-based interpretation, and language-driven understanding, MEETI
established a robust foundation for the next generation of explainable,
multimodal cardiovascular AI. It offers the research community a comprehensive
benchmark for developing and evaluating ECG-based AI systems.

</details>


### [49] [Optimal Transceiver Design in Over-the-Air Federated Distillation](https://arxiv.org/abs/2507.15256)
*Zihao Hu,Jia Yan,Ying-Jun Angela Zhang,Jun Zhang,Khaled B. Letaief*

Main category: eess.SP

TL;DR: 提出了一种新颖的空中联合蒸馏（FD）框架，结合联合学习（FL）和知识蒸馏，减少通信开销。通过共享模型输出而非参数，利用多址信道的叠加特性进行聚合。优化收发器设计以最大化学习收敛速度，同时满足功率约束。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型的出现使现有FL方法因通信开销大而效率低下，需提出更高效的解决方案。

Method: 提出空中FD框架，共享模型输出（知识）而非参数；推导收敛速率表达式，优化收发器功率和接收波束成形向量。

Result: 显著减少通信开销，测试精度仅略低于传统FL基准。

Conclusion: 空中FD框架在减少通信开销的同时保持较高学习性能，适用于大规模AI模型训练。

Abstract: The rapid proliferation and growth of artificial intelligence (AI) has led to
the development of federated learning (FL). FL allows wireless devices (WDs) to
cooperatively learn by sharing only local model parameters, without needing to
share the entire dataset. However, the emergence of large AI models has made
existing FL approaches inefficient, due to the significant communication
overhead required. In this paper, we propose a novel over-the-air federated
distillation (FD) framework by synergizing the strength of FL and knowledge
distillation to avoid the heavy local model transmission. Instead of sharing
the model parameters, only the WDs' model outputs, referred to as knowledge,
are shared and aggregated over-the-air by exploiting the superposition property
of the multiple-access channel. We shall study the transceiver design in
over-the-air FD, aiming to maximize the learning convergence rate while meeting
the power constraints of the transceivers. The main challenge lies in the
intractability of the learning performance analysis, as well as the non-convex
nature and the optimization spanning the whole FD training period. To tackle
this problem, we first derive an analytical expression of the convergence rate
in over-the-air FD. Then, the closed-form optimal solutions of the WDs'
transmit power and the estimator for over-the-air aggregation are obtained
given the receiver combining strategy. Accordingly, we put forth an efficient
approach to find the optimal receiver beamforming vector via semidefinite
relaxation. We further prove that there is no optimality gap between the
original and relaxed problem for the receiver beamforming design. Numerical
results will show that the proposed over-the-air FD approach achieves a
significant reduction in communication overhead, with only a minor compromise
in testing accuracy compared to conventional FL benchmarks.

</details>


### [50] [A Novel Domain-Aware CNN Architecture for Faster-than-Nyquist Signaling Detection](https://arxiv.org/abs/2507.15291)
*Osman Tokluoglu,Enver Cavus,Ebrahim Bedeer,Halim Yanikomeroglu*

Main category: eess.SP

TL;DR: 提出一种基于固定核卷积神经网络（CNN）的FTN信号检测器，通过分层滤波分配和固定核设计提高检测精度并减少冗余计算。


<details>
  <summary>Details</summary>
Motivation: 传统滑动核CNN难以有效捕捉FTN信号中的符号间干扰（ISI），需设计更高效的检测方法。

Method: 采用固定位置核直接捕捉不同距离的ISI效应，并分层分配滤波器以优化计算资源。

Result: 在τ≥0.7时接近最优BER性能，计算效率比M-BCJR提升46%（BPSK）和84%（QPSK）。

Conclusion: 固定核CNN架构首次应用于FTN检测，显著提升了效率和性能。

Abstract: This paper proposes a convolutional neural network (CNN)-based detector for
faster-than-Nyquist (FTN) signaling that employs structured fixed kernel layers
with domain-informed masking to mitigate intersymbol interference (ISI). Unlike
standard CNNs with sliding kernels, the proposed method utilizes fixed-position
kernels to directly capture ISI effects at varying distances from the central
symbol. A hierarchical filter allocation strategy is also introduced, assigning
more filters to earlier layers for strong ISI patterns and fewer to later
layers for weaker ones. This design improves detection accuracy while reducing
redundant operations. Simulation results show that the detector achieves
near-optimal bit error rate (BER) performance for $\tau \geq 0.7$, closely
matching the BCJR algorithm, and offers computational gains of up to $46\%$ and
$84\%$ over M-BCJR for BPSK and QPSK, respectively. Comparative analysis with
other methods further highlights the efficiency and effectiveness of the
proposed approach. To the best of our knowledge, this is the first application
of a fixed-kernel CNN architecture tailored for FTN detection in the
literature.

</details>


### [51] [BEAM-Net: A Deep Learning Framework with Bone Enhancement Attention Mechanism for High Resolution High Frame Rate Ultrasound Beamforming](https://arxiv.org/abs/2507.15306)
*Midhila Madhusoodanan,Mahesh Raveendranatha Panicker,Pisharody Harikrishnan Gopalakrishnan,Abhilash Rakkunedeth Hareendranathan*

Main category: eess.SP

TL;DR: 提出BEAM-Net，一种端到端深度神经网络，用于超声波束成形并增强骨骼结构，显著提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统超声波束成形方法（DASB）在骨骼结构成像中的不足，如图像噪声、低分辨率和对比度差。

Method: 使用单平面波射频数据作为输入，结合骨骼概率图（BPM）作为注意力机制，直接嵌入骨骼增强功能。

Result: BEAM-Net在活体和合成数据集上表现优异，对比度比（CR）和信噪比（SNR）显著提升。

Conclusion: BEAM-Net首次将骨骼增强直接融入超声波束成形，为骨骼超声成像提供了新解决方案。

Abstract: Pocket-sized, low-cost point-of-care ultrasound (POCUS) devices are
increasingly used in musculoskeletal (MSK) applications for structural
examination of bone tissue. However, the image quality in MSK ultrasound is
often limited by speckle noise, low resolution, poor contrast, and anisotropic
reflections, making bone images difficult to interpret without additional
post-processing. Typically, medical ultrasound systems use delay and sum
beamforming (DASB) for image reconstruction, which is not specifically
optimized for bone structures. To address these limitations, we propose
BEAM-Net, a novel end-to-end deep neural network (DNN) that performs
high-frame-rate ultrasound beamforming with integrated bone enhancement, using
single-plane-wave (SPW) radio frequency (RF) data as input. Our approach embeds
a Bone Probability Map (BPM), which acts as an attention mechanism to enforce
higher structural similarity around bony regions in the image. The proposed
approach is the first of its kind to incorporate bone enhancement directly into
ultrasound beamforming using deep learning. BEAM-Net was trained and evaluated
on in-vivo MSK and synthetic RF ultrasound datasets. This paper introduces the
Edge Preservation Index (EPI) as a new region-focused metric for evaluating
structural fidelity in bone-enhanced ultrasound images. The performance of
BEAM-Net was compared with conventional DASB and existing deep learning
architectures using the EPI, Contrast Ratio (CR), Signal-to-Noise ratio (SNR),
Speckle Similarity Index (SSI), and Structural Similarity Index (SSIM).
BEAM-Net showed substantial gains over SPW-DASB, achieving 51.4-51% higher CR
and 94.2-73.3% higher SNR on in-vivo MSK and synthetic RF datasets. It
outperformed multiple steered plane wave DASB (MPW-DASB), with 19.8-24.0%
improvements in CR and SNR on in-vivo MSK and 2.5-12.8% improvements on
synthetic data.

</details>


### [52] [EEG-based Epileptic Prediction via a Two-stage Channel-aware Set Transformer Network](https://arxiv.org/abs/2507.15364)
*Ruifeng Zheng,Cong Chen,Shuang Wang,Yiming Liu,Lin You,Jindong Lu,Ruizhe Zhu,Guodao Zhang,Kejie Huang*

Main category: eess.SP

TL;DR: 提出了一种新型的两阶段通道感知Set Transformer网络，用于减少EEG通道传感器的数量，同时提高癫痫预测的敏感性和准确性。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作对患者生活质量有重大影响，但现有可穿戴预测设备因EEG设备体积庞大而受限。

Method: 采用两阶段通道感知Set Transformer网络和癫痫独立分割方法，以减少EEG通道数量并避免训练与测试数据的相邻性。

Result: 在CHB-MIT数据集上，通道选择后平均通道数从18降至2.8，敏感性从76.4%提升至80.1%，FPR略有增加。

Conclusion: 研究支持在EEG记录丰富的患者中使用更严格的癫痫独立分割方法，并验证了通道选择的有效性。

Abstract: Epilepsy is a chronic, noncommunicable brain disorder, and sudden seizure
onsets can significantly impact patients' quality of life and health. However,
wearable seizure-predicting devices are still limited, partly due to the bulky
size of EEG-collecting devices. To relieve the problem, we proposed a novel
two-stage channel-aware Set Transformer Network that could perform seizure
prediction with fewer EEG channel sensors. We also tested a seizure-independent
division method which could prevent the adjacency of training and test data.
Experiments were performed on the CHB-MIT dataset which includes 22 patients
with 88 merged seizures. The mean sensitivity before channel selection was
76.4% with a false predicting rate (FPR) of 0.09/hour. After channel selection,
dominant channels emerged in 20 out of 22 patients; the average number of
channels was reduced to 2.8 from 18; and the mean sensitivity rose to 80.1%
with an FPR of 0.11/hour. Furthermore, experimental results on the
seizure-independent division supported our assertion that a more rigorous
seizure-independent division should be used for patients with abundant EEG
recordings.

</details>


### [53] [Robust ISAC Transceiver Beamforming Design under Low-Resolution AD/DA Converters](https://arxiv.org/abs/2507.15373)
*Tiantian Xu,Zhenyao He,Jindan Xu,Wei Xu,Jianfeng Wang,Derrick Wing Kwan Ng*

Main category: eess.SP

TL;DR: 研究了低分辨率DAC和ADC的ISAC系统鲁棒波束成形设计，通过优化雷达SQNR并保证通信用户SQINR，提出了一种基于MM的低复杂度算法。


<details>
  <summary>Details</summary>
Motivation: 解决低分辨率量化噪声下ISAC系统的波束成形设计问题，兼顾雷达和通信性能。

Method: 针对点目标和均匀分辨率DAC场景，使用SDR技术；针对混合DAC和扩展目标场景，提出基于MM的迭代算法。

Result: 仿真表明，算法在低分辨率量化场景下具有鲁棒性和高效性，优于非鲁棒算法。

Conclusion: 提出的算法在低分辨率量化下显著提升了检测性能，适用于实际ISAC系统。

Abstract: In this letter, we investigate the robust beamforming design for an
integrated sensing and communication (ISAC) system featuring low-resolution
digital-to-analog converters (DACs) and analog-to-digital converters (ADCs).
Taking into account quantization noise, we aim at maximizing the radar
signal-to-quantization-plus-noise ratio (SQNR) while guaranteeing the minimum
required signal-to-quantization-plus-interference-plus-noise ratio (SQINR) for
communication users. To address this nonconvex design problem, we first examine
a scenario involving a point target and uniform-resolution DACs, where the
globally optimal solution is obtained by applying the semidefinite relaxation
(SDR) technique. For more general scenarios, including those with mixed-DACs
and/or an extended target, we develop a low-complexity
majorization-minimization (MM)-based algorithm to tackle the problem
iteratively. Compared to the non-robust algorithm, the proposed algorithm
demonstrates improved detection performance under practical quantization.
Simulation results confirm the robustness and efficacy of our proposed
algorithm in low-resolution quantization scenarios.

</details>


### [54] [On the Distribution of a Two-Dimensional Random Walk with Restricted Angles](https://arxiv.org/abs/2507.15475)
*Karl-Ludwig Besser*

Main category: eess.SP

TL;DR: 本文研究了二维（复数）随机游走的分布，其中每一步的角度被限制在圆的一个子集内，推导了精确的联合和边缘分布，并提供了数值解和近似解。


<details>
  <summary>Details</summary>
Motivation: 研究此类随机游走在信号处理等领域（如空中计算）中的应用需求。

Method: 推导了两步的精确分布，提供了多步的数值解和大步数的近似解，并精确描述了支撑集。

Result: 得到了两步的精确分布、多步的数值解和大步数的近似解，以及支撑集的精确描述。

Conclusion: 结果为未来涉及此类问题的研究提供了参考。

Abstract: In this paper, we derive the distribution of a two-dimensional (complex)
random walk in which the angle of each step is restricted to a subset of the
circle. This setting appears in various domains, such as in over-the-air
computation in signal processing. In particular, we derive the exact joint and
marginal distributions for two steps, numerical solutions for a general number
of steps, and approximations for a large number of steps. Furthermore, we
provide an exact characterization of the support for an arbitrary number of
steps. The results in this work provide a reference for future work involving
such problems.

</details>


### [55] [Movable-Antenna Empowered AAV-Enabled Data Collection over Low-Altitude Wireless Networks](https://arxiv.org/abs/2507.15515)
*Xuhui Zhang,Wenchao Liu,Jinke Ren,Chunjie Wang,Huijun Xing,Yanyan Shen,Shuguang Cui*

Main category: eess.SP

TL;DR: 论文研究了在低空无线网络（LAWNs）中利用可移动天线（MAs）的自主飞行器（AAV）系统，通过联合优化AAV轨迹、接收波束成形和MA位置，最大化总可达速率。


<details>
  <summary>Details</summary>
Motivation: 可移动天线（MAs）为下一代无线系统提供了灵活的波束成形能力，本文旨在探索其在低空无线网络中的应用潜力，以提升上行数据收集的性能。

Method: 提出了一种高效的交替优化（AO）算法，结合了逐次凸近似、加权最小均方误差和粒子群优化技术。

Result: 仿真结果表明，所提方案在总可达速率和服务可靠性方面优于多个基准方案，展现了自适应波束-用户对齐和空间干扰管理的优势。

Conclusion: 研究证明了MA赋能的LAWNs在提升频谱效率和收集可靠性方面的独特优势，具有实际应用潜力。

Abstract: Movable-antennas (MAs) are revolutionizing spatial signal processing by
providing flexible beamforming in next-generation wireless systems. This paper
investigates an MA-empowered autonomous aerial vehicle (AAV) system in
low-altitude wireless networks (LAWNs) for uplink data collection from ground
users. We aim to maximize the sum achievable rate by jointly optimizing the AAV
trajectory, receive beamforming, and MA positions. An efficient alternating
optimization (AO) algorithm that incorporates successive convex approximation,
weighted minimum mean square error, and particle swarm optimization is
developed. The analysis of the computational complexity and convergence
features is provided. Extensive simulations demonstrate superior performance in
terms of the sum achievable rate and the service reliability comparing to
several benchmark schemes. These results demonstrate the distinctive advantages
of the proposed scheme: enhanced spectral efficiency via adaptive beam-user
alignment and improved collection reliability through spatial interference
management, highlighting the implementation potential of the MA-empowered
LAWNs.

</details>


### [56] [Sum-Rate Maximization for Movable-Antenna Array Enhanced Downlink NOMA Systems](https://arxiv.org/abs/2507.15555)
*Nianzu Li,Peiran Wu,Lipeng Zhu,Weidong Mei,Boyu Ning,Derrick Wing Kwan Ng*

Main category: eess.SP

TL;DR: 论文研究了可移动天线（MA）系统在非正交多址（NOMA）下行链路中的资源分配问题，通过联合优化波束成形、MA位置、SIC解码顺序和用户解码指示矩阵，最大化用户总速率。


<details>
  <summary>Details</summary>
Motivation: 可移动天线系统因其灵活调整无线信道的能力受到关注，本文旨在探索其在NOMA系统中的性能优势。

Method: 提出两阶段优化算法：第一阶段通过信道增益最大化确定SIC解码顺序；第二阶段利用交替优化、SCA和GA迭代优化波束成形、MA位置和解码指示矩阵。

Result: 仿真结果显示，MA-NOMA系统的总速率显著优于固定天线系统，且天线位置优化进一步增强了NOMA相对于SDMA的优势。

Conclusion: MA系统在NOMA中具有显著性能提升潜力，提出的算法为实际应用提供了有效解决方案。

Abstract: Movable antenna (MA) systems have recently attracted significant attention in
the field of wireless communications owing to their exceptional capability to
proactively reconfigure wireless channels via flexible antenna movements. In
this paper, we investigate the resource allocation design for an MA
array-enhanced downlink non-orthogonal multiple access (NOMA) system, where a
base station deploys multiple MAs to serve multiple single-antenna users. Our
goal is to maximize the sum rate of all users by jointly optimizing the
transmit beamforming, positions of MAs, successive interference cancellation
(SIC) decoding order, and users' corresponding decoding indicator matrix, while
adhering to constraints on the maximum transmit power and finite MA moving
region. The formulated problem is inherently highly non-convex, rendering it
challenging to acquire a globally optimal solution. As a compromise, we propose
a low-complexity two-stage optimization algorithm to obtain an effective
suboptimal solution. Specifically, in stage one, the SIC decoding order is
first determined by solving a channel gain maximization problem. Then, in stage
two, with the given SIC decoding order, the beamforming vectors, MA positions,
and users' decoding indicator matrix are iteratively optimized by capitalizing
on alternating optimization, successive convex approximation (SCA), and genetic
algorithm (GA). Simulation results unveil that the sum-rate performance of the
proposed MA-enabled downlink NOMA system significantly outperforms that of
conventional fixed-position antenna (FPA) systems. Moreover, the results also
show that the antenna position optimization in the proposed algorithm can
further enhance the advantages of NOMA over space division multiple access
(SDMA).

</details>


### [57] [Zak-OTFS based Multiuser Uplink in Doubly-Spread Channels](https://arxiv.org/abs/2507.15621)
*Imran Ali Khan,Saif Khan Mohammed,Ronny Hadani,Ananthanarayanan Chockalingam,Robert Calderbank*

Main category: eess.SP

TL;DR: 论文提出了一种基于Zak-OTFS的新型调制方案，解决了OFDM中子载波间距不灵活的问题，实现了多用户上行链路中灵活的资源分配。


<details>
  <summary>Details</summary>
Motivation: OFDM在无线网络中虽然易于分配资源，但其子载波间距固定，导致单个用户的干扰会影响所有用户。Zak-OTFS作为一种替代方案，具有可预测的输入输出关系，有望解决这一问题。

Method: 设计了一种新的延迟-多普勒（DD）域方法，通过成形Zak-OTFS脉冲实现灵活的非重叠时频资源分配。基站通过匹配滤波器分离用户数据。

Result: 理论分析和数值模拟表明，Zak-OTFS在多用户上行链路中无需保护频带即可实现单用户性能。

Conclusion: Zak-OTFS的可预测波形优势可在上行链路架构中实现，为下一代通信网络提供了灵活的资源分配方案。

Abstract: Wireless users with different characteristics will be expected to share
spectrum in next generation communication networks. One of the great strengths
of wireless networks based on Orthogonal Frequency Division Multiplexing (OFDM)
is the ease with which different non-overlapping time-frequency (TF) resources
can be allocated to different users by simply shifting each user's signal in
time and frequency. However, a significant weaknesses of OFDM is the
inflexibility of sub-carrier spacing. Since OFDM does not allow users to have
different sub-carrier spacing, a single user subject to inter-carrier
interference causes carrier spacing to increase for all users. Zak-OTFS is an
alternative delay-Doppler (DD) domain modulation scheme, where, in contrast to
OFDM, the Input-Output (I/O) relation is predictable. We match the strength of
OFDM by designing a novel DD domain method of shaping the transmitted Zak-OTFS
pulse on the uplink that enables flexible non-overlapping TF resource
allocation. The base station (BS) receives a superposition of uplink signals
and applies individual matched filters to obtain the data specific to
individual users. We develop theoretical measures of interference between
users, and present numerical simulations for a vehicular channel model
representative of next generation propagation environments. We demonstrate
single-user performance in a multiuser Zak-OTFS uplink system without needing
to provision guard bands between TF resources allocated to different users.
These performance results demonstrate that the benefits of a predictable
Zak-OTFS waveform can be realized within an architecture for uplink
communication.

</details>


### [58] [Fluid Antenna-enabled Near-Field Integrated Sensing, Computing and Semantic Communication for Emerging Applications](https://arxiv.org/abs/2507.15800)
*Yinchao Yang,Jingxuan Zhou,Zhaohui Yang,Mohammad Shikh-Bahaei*

Main category: eess.SP

TL;DR: 提出了一种结合近场感知、计算和语义通信的新框架（NF-ISCSC），利用流体天线（FAs）动态适应信道变化，通过联合优化提升数据速率和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 高频段和大规模天线阵列扩展了瑞利距离，需要近场模型，但近场ISAC带来高数据量和隐私风险，需解决这些问题。

Method: 提出NF-ISCSC框架，结合语义通信减少数据量，引入FAs动态适应信道，通过交替优化方法解决联合优化问题。

Result: 仿真结果显示，该框架实现了更高的数据速率和更好的隐私保护。

Conclusion: NF-ISCSC框架有效解决了近场ISAC的挑战，提升了性能和隐私保护。

Abstract: The integration of sensing and communication (ISAC) is a key enabler for
next-generation technologies. With high-frequency bands and large-scale antenna
arrays, the Rayleigh distance extends, necessitating near-field (NF) models
where waves are spherical. Although NF-ISAC improves both sensing and
communication, it also poses challenges such as high data volume and potential
privacy risks. To address these, we propose a novel framework: near-field
integrated sensing, computing, and semantic communication (NF-ISCSC), which
leverages semantic communication to transmit contextual information only,
thereby reducing data overhead and improving efficiency. However, semantic
communication is sensitive to channel variations, requiring adaptive
mechanisms. To this end, fluid antennas (FAs) are introduced to support the
NF-ISCSC system, enabling dynamic adaptability to changing channels. The
proposed FA-enabled NF-ISCSC framework considers multiple communication users
and extended targets comprising several scatterers. A joint optimization
problem is formulated to maximize data rate while accounting for sensing
quality, computational load, and power budget. Using an alternating
optimization (AO) approach, the original problem is divided into three
sub-problems: ISAC beamforming, FA positioning, and semantic extraction ratio.
Beamforming is optimized using the successive convex approximation method. FA
positioning is solved via a projected Broyden-Fletcher-Goldfarb-Shanno (BFGS)
algorithm, and the semantic extraction ratio is optimized using bisection
search. Simulation results demonstrate that the proposed framework achieves
higher data rates and better privacy preservation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [59] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种新的结构化剪枝方法Catalyst正则化，通过引入辅助变量确保剪枝公平性和鲁棒性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法（如L1或Group Lasso）存在幅度偏差和决策边界不稳定问题，影响剪枝效果。

Method: 基于代数条件设计Catalyst正则化，通过辅助变量实现公平剪枝和宽边界决策。

Result: 在多种数据集和模型上表现优于现有方法，验证了其鲁棒性和公平性。

Conclusion: Catalyst正则化提供了一种理论支持的高效剪枝方法，具有实际应用潜力。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [60] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种基于投影空间的新型剪枝策略IPPRO，通过PROscore衡量滤波器剪枝可能性，挑战了传统基于幅度的剪枝方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于幅度的剪枝方法限制了剪枝决策的能力，即使冗余滤波器也可能因幅度较大而未被剪枝。

Method: 将滤波器置于投影空间，观察梯度下降运动方向（是否趋近原点）来衡量剪枝可能性，构建PROscore。

Result: 实验表明，该方法实现了接近无损的剪枝，减少了性能下降，并在微调后表现优异。

Conclusion: 打破了剪枝中‘大小决定一切’的迷思，从理论和实践上拓展了基于重要性的剪枝方法。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [61] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR是一种通过将语言模型集成到自我改进的进化循环中来学习程序合成的方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在单次尝试中难以解决复杂的程序合成任务，而基于搜索的进化方法受限于生成模型的固定能力。

Method: SOAR交替进行进化搜索和事后学习，利用LLM采样和优化候选解决方案，并通过微调提升模型能力。

Result: 在ARC-AGI基准测试中，SOAR实现了显著的性能提升，解决了52%的公共测试集问题。

Conclusion: SOAR通过结合进化搜索和事后学习，有效提升了程序合成的能力，并展示了迁移学习的潜力。

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [62] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: 论文研究了潜在空间融合方法在预测抑郁症状中的表现，优于传统融合方法。


<details>
  <summary>Details</summary>
Motivation: 改善精神疾病的早期检测和个性化干预方法，传统模型未能充分利用多模态数据。

Method: 使用BRIGHTEN临床试验数据，比较潜在空间融合（CM）与传统随机森林（RF）和线性回归（LR）的性能。

Result: CM在所有设置中表现最佳，MSE更低（0.4985 vs. 0.5305），R2更高（0.4695 vs. 0.4356），且泛化能力更强。

Conclusion: 潜在空间融合是多模态心理健康数据预测的可靠方法，未来需关注模型可解释性和个体化预测。

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [63] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: 论文提出Predictive Representativity（PR）框架，用于公平性审计，强调从数据集组成转向结果层面的公平性。通过皮肤病学案例研究，发现AI皮肤癌分类器在深色皮肤人群中表现较差，并提出外部可迁移性标准。


<details>
  <summary>Details</summary>
Motivation: AI在医疗决策中的应用日益增多，但算法偏见和不公平结果问题突出，尤其是对历史上边缘化群体。研究旨在解决这一问题。

Method: 使用HAM10000数据集和哥伦比亚的BOSQUE Test set，评估AI皮肤癌分类器的性能，并提出PR框架和外部可迁移性标准。

Result: 研究发现分类器在深色皮肤人群中表现较差，尽管源数据中采样比例均衡。

Conclusion: 研究强调事后公平性审计、数据集透明度和包容性模型验证的必要性，为AI系统中的结构性不平等提供了诊断工具。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [64] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 本文研究了两层神经网络的训练解，揭示了其隐藏层平滑激活函数下的工作原理，包括泰勒展开、严格节点偏序、平滑样条实现和连续性限制。


<details>
  <summary>Details</summary>
Motivation: 理解反向传播算法在平滑激活函数下的解空间，揭示神经网络“黑箱”的机制。

Method: 通过泰勒级数展开、严格节点偏序、平滑样条实现和连续性限制四种机制分析。

Result: 证明了任意输入维度的通用逼近能力，并通过实验验证。

Conclusion: 揭示了神经网络解空间的“黑箱”机制，丰富了逼近理论。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [65] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出了一种名为Feature Bank Enhancement (FBE)的方法，用于改进基于距离的OOD检测方法，通过约束极端特征提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于距离的OOD检测方法因数据特征分布偏差和极端特征导致性能受限，需要改进。

Method: 使用数据集的统计特征识别并约束极端特征，扩大分布内外样本的距离。

Result: 在ImageNet-1k和CIFAR-10上实现了最先进的性能。

Conclusion: FBE方法简单有效，显著提升了OOD检测能力。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [66] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 论文提出了一种基于聚类的激活模式压缩框架，通过将相似的激活模式分组为少量代表性聚类，高效预测和利用LLMs中的激活稀疏性，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的激活稀疏性为降低计算成本提供了机会，但直接预测神经元级别的激活模式计算成本过高。

Method: 提出聚类方法，将相似激活模式分组为少量代表性聚类，预测聚类分配而非单个神经元状态。

Result: 聚类精度达79.34%，困惑度（PPL）最低12.49，在保持模型质量的同时显著降低计算开销。

Conclusion: 该方法为激活模式预测提供了高效基础，有望提升大规模语言模型的推理效率。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [67] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: 论文提出了一种基于深度学习的毫米波MIMO系统波束对齐引擎（BAE），通过数字孪生和迁移学习减少数据收集开销，并利用SHAP和DkNN提高透明度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在6G愿景下，毫米波系统的可解释性和鲁棒性对建立信任和确保可靠性能至关重要，但现有深度学习方法面临数据收集开销大、硬件限制和对抗攻击等问题。

Method: 使用数字孪生生成合成数据，通过迁移学习微调模型；结合SHAP和DkNN实现特征重要性排序和异常检测。

Result: 实验表明，该方法减少70%真实数据需求、62%波束训练开销，异常检测鲁棒性提升8.5倍，接近最优频谱效率。

Conclusion: 该框架为毫米波系统提供了一种高效、透明且鲁棒的波束对齐解决方案。

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [68] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 论文提出了一种基于AI的框架，用于计算绿色氢产量和选址适宜性指数，结合了多变量聚类、机器学习分类器和SHAP算法，为数据稀缺地区提供了可复制的决策工具。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏直接的氢产量数据，传统方法依赖主观专家权重，本研究旨在提供一种客观、可重复的替代方案，以优化绿色氢生产选址。

Method: 采用多阶段AI框架，包括无监督多变量聚类、监督机器学习分类器和SHAP算法，整合气象、地形和时间数据进行分析。

Result: 模型预测准确率达98%，揭示了水接近度、海拔和季节变化是阿曼绿色氢选址的最关键因素（SHAP值分别为2.470891、2.376296和1.273216）。

Conclusion: 该研究为行业和政策制定者提供了一种可扩展的工具，适用于数据稀缺地区的绿色氢基础设施规划，避免了主观假设的局限性。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [69] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 提出了一种半监督联邦学习框架SSFL-DCSL，结合双对比损失和软标签，解决分布式客户端数据与标签稀缺问题，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习方法需要大量标注数据且成本高，而客户端数据分布差异可能影响模型性能。

Method: 设计基于拉普拉斯分布的样本加权函数、引入双对比损失（局部与全局对比损失）、通过加权平均和动量更新聚合本地原型。

Result: 在仅10%数据标注的最具挑战性任务中，SSFL-DCSL比现有方法准确率提升1.15%至7.85%。

Conclusion: SSFL-DCSL有效解决了数据与标签稀缺问题，提升了模型性能并保护了隐私。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [70] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为FedClusAvg的隐私保护联邦学习框架，用于在非独立同分布（Non-IID）和资源受限的环境中提高虚假数据注入攻击（FDIA）的检测能力。


<details>
  <summary>Details</summary>
Motivation: 虚假数据注入攻击（FDIAs）对智能电网构成严重安全威胁，而传统集中式训练方法存在隐私风险、数据共享限制和高传输成本问题。

Method: FedClusAvg采用基于聚类的分层抽样和分层通信（客户端-子服务器-服务器）机制，支持本地化训练和加权参数聚合，避免集中敏感数据。

Result: 实验结果表明，FedClusAvg在异构数据分布下提高了检测精度，同时显著减少了通信轮次和带宽消耗。

Conclusion: 该研究为大规模分布式电力系统中安全高效的FDIA检测提供了有效解决方案。

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [71] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 论文提出B4模型，通过联合嵌入价格序列和外部信号，捕捉牛市和熊市动态，提升市场趋势预测。


<details>
  <summary>Details</summary>
Motivation: 金融市场行为复杂且受多种因素影响，现有模型难以捕捉投资者偏见和行为动态。

Method: 提出B4模型，结合价格序列和外部信号，通过惯性配对和双竞争机制建模市场动态。

Result: 实验显示B4在预测市场趋势上表现优越，并提供可解释的偏见与行为关系。

Conclusion: B4模型有效捕捉市场动态，为投资者行为和市场趋势提供新视角。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [72] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: 提出了一种基于C$^2$感知的低延迟联邦学习框架，通过优化批量大小控制来平衡梯度估计精度与通信轮次，适应设备异构性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在6G网络中部署时面临的高维模型更新和通信计算异构性带来的延迟问题。

Method: 设计了一种C$^2$感知框架，通过优化批量大小控制来最小化端到端学习延迟，并确保收敛。

Result: 实验表明，提出的策略优于不考虑C$^2$权衡或设备异构性的传统批量大小调整方案。

Conclusion: 该框架有效解决了联邦学习中的延迟问题，适用于多种物联网应用场景。

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [73] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache是一种无需训练的KV缓存优化方法，通过梯形状KV缓存模式和迭代压缩机制，提升LLMs的长距离建模能力和持续生成效率。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度增加，LLMs中的KV对数量激增，导致效率瓶颈，需要一种方法同时解决长距离建模和内存不足问题。

Method: LaCache采用梯形状KV缓存模式存储KV对，并结合迭代压缩机制动态压缩旧缓存，以在固定缓存预算下提升效率和准确性。

Result: 实验验证LaCache能有效增强LLMs的长距离能力，并在多种任务和模型上表现一致。

Conclusion: LaCache为LLMs的长距离建模和持续生成提供了一种高效且无需训练的解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [74] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: 开发了一种基于深度学习的无障碍设备，用于聋人或听力受损者，实时定位和识别声源。


<details>
  <summary>Details</summary>
Motivation: 填补当前研究的空白，利用机器学习技术服务于弱势群体。

Method: 系统包括三个组件：JerryNet（CNN架构确定声源方向）、音频分类（基于CLAP模型）、多模态集成模型（结合音频、视觉和文本数据）。

Result: JerryNet方向精度91.1%，CLAP模型在自定义和AudioSet数据集上分别达到98.5%和95%准确率，多模态模型的cIoU为0.892。

Conclusion: 研究为新一代无障碍设备奠定了基础，具有广阔的未来潜力。

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [75] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: 论文提出了一种新的不可靠行为源——glitches，即输入空间中模型输出突然振荡的小邻域，并证明了其在广泛存在的模型中的影响。


<details>
  <summary>Details</summary>
Motivation: 确保机器学习模型的决策可信且可靠，输出在相似输入下一致。

Method: 形式化定义glitches，提出基于MILP编码的算法检测梯度提升决策树（GBDT）模型中的glitches。

Result: 证明glitches在广泛使用的模型中普遍存在，且检测问题对深度4的树集成是NP完全的。

Conclusion: glitches通常指示模型不一致性，提出的算法有效且计算可行。

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [76] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 提出了一种交互式学习框架，结合非线性效用聚合和几何感知查询选择，解决模式挖掘中的模式爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 解决模式挖掘中的模式爆炸问题，通过建模用户偏好和优化查询选择。

Method: 使用Choquet积分建模用户偏好，结合几何感知查询选择和分支定界策略。

Result: 在UCI数据集上表现优于现有方法（如ChoquetRank），以更少用户交互实现更高排名准确性。

Conclusion: 提出的方法有效解决了模式爆炸问题，提升了用户交互效率和排名准确性。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [77] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: 论文提出了一种名为ASTRA的算法，通过结合EKFAC预条件和Neumann级数迭代，高效且准确地近似逆Hessian-向量积（iHVP），从而提升训练数据归因（TDA）的性能。


<details>
  <summary>Details</summary>
Motivation: 训练数据归因（TDA）方法（如影响函数和展开微分）在计算逆Hessian-向量积（iHVP）时效率低下且难以近似，这限制了TDA的实用性和准确性。

Method: 提出ASTRA算法，利用EKFAC预条件器和Neumann级数迭代，高效且准确地近似iHVP。

Result: ASTRA在iHVP近似上比传统方法更准确，且需要更少的迭代次数，显著提升了TDA的性能。

Conclusion: 通过ASTRA算法，可以高效且准确地近似iHVP，从而显著改善训练数据归因的效果。

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [78] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: 论文提出了两种高斯过程（GP）的后验采样方法，用于支持全局敏感性分析和优化任务。


<details>
  <summary>Details</summary>
Motivation: 高保真仿真和物理实验成本高，限制了其在全局敏感性分析和优化中的应用，因此需要高效的代理模型（如高斯过程）来提供不确定性感知预测。

Method: 介绍了两种采样方法——随机傅里叶特征和路径条件采样，并详细描述了其实现过程。

Result: 通过数值实验验证了这些采样方法在全局敏感性分析、单目标和多目标优化中的成功应用。

Conclusion: 高斯过程采样方法为工程优化提供了高效的工具，支持不确定性下的决策。

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [79] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种新的POGM方法，通过梯度轨迹优化解决领域泛化问题，避免了梯度波动和高计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有方法在梯度匹配时存在梯度波动和高计算开销问题，影响学习效果。

Method: 利用梯度轨迹作为数据，在元学习器中独立训练，最大化梯度内积同时限制偏离经验风险最小化梯度轨迹。

Result: 在DomainBed数据集上表现优于基线方法，同时计算高效。

Conclusion: POGM方法有效解决了梯度波动和计算开销问题，提升了领域泛化性能。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [80] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: 论文研究了自监督特征学习在强化学习中的可识别性，通过对比后继特征（CSF）方法证明了其能恢复环境的真实特征。


<details>
  <summary>Details</summary>
Motivation: 探索互信息技能学习（MISL）中表示和互信息参数化的理论作用，填补现有研究的空白。

Method: 采用对比后继特征（CSF）方法，分析其特征内积参数化和技能多样性对特征可识别性的影响。

Result: 理论证明CSF能恢复环境的真实特征（线性变换内），并通过实验在MuJoCo和DeepMind Control中验证。

Conclusion: CSF为强化学习中的表示学习提供了首个可识别性保证，并揭示了互信息目标和熵正则化的潜在问题。

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [81] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: NanoPro-3M是最大的纳米材料-蛋白质相互作用数据集，结合NanoProFormer模型，通过多模态学习显著提升了预测性能，并减少实验依赖。


<details>
  <summary>Details</summary>
Motivation: 纳米材料在医学和环境科学中的应用潜力受限于其与蛋白质相互作用的复杂性，现有模型因数据集小和泛化能力差而进展缓慢。

Method: 提出NanoPro-3M数据集（320万样本，3.7万独特蛋白质），并开发NanoProFormer模型，通过多模态表示学习预测亲和力。

Result: 多模态模型显著优于单模态方法，能处理缺失特征和未知纳米材料/蛋白质，并识别冠形成的关键因素。

Conclusion: 该研究为高性能、泛化的纳米材料-蛋白质相互作用预测奠定了基础，加速体外应用。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [82] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 论文提出了一种分布式的机器学习遗忘框架，通过移除特定分布的数据点，减少对模型性能的影响，同时满足隐私或法律要求。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘工具主要针对单个样本，难以满足删除整个主题领域的需求（如GDPR或版权要求），且残留信号仍可能被下游学习器恢复。

Method: 提出分布遗忘框架，通过Kullback-Leibler散度量化移除和保留的分布差异，推导高斯情况下的帕累托边界，并提出基于距离的选择规则以减少删除量。

Result: 实验表明，相比随机删除，该方法在合成高斯、Jigsaw Toxic Comments等数据集上减少15-72%的删除量，且对保留性能影响可忽略。

Conclusion: 分布遗忘是一种高效、模型无关的解决方案，适用于大规模数据删除需求。

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [83] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LDM是一种新的线性降维方法，通过扩散映射核的线性近似构建，结合了几何直观性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 结合非线性扩散方法的几何直观性与线性嵌入（如PCA）的计算效率和可解释性。

Method: 通过线性近似扩散映射核构建LDM，并在合成和真实数据集上进行实验。

Result: LDM在具有明确流形结构的数据集上优于PCA，而PCA在噪声或方差主导的场景中更优。

Conclusion: LDM是一种有潜力的线性降维技术，适用于理论和实际扩展。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [84] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: 论文提出了在更自然的观测模型下学习Ising模型结构和参数的算法，仅需观察配置变化，无需完全观测所有更新尝试。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设观测所有站点更新尝试（包括未改变配置的尝试），这在现实场景中不切实际。论文旨在解决仅观测配置变化时的学习问题。

Method: 提出了一种高效算法，能在最大度为d的Ising模型中恢复依赖图，并在额外时间内恢复参数，适用于可逆单站点马尔可夫链。

Result: 算法在更弱的观测模型下，性能与i.i.d.设置下的最新成果相当，且适用于更广泛的马尔可夫链类。

Conclusion: 论文首次在仅观测配置变化的自然模型下解决了Ising模型学习问题，扩展了算法的适用性。

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [85] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: 论文提出了一种多轮强化学习方法（UFO），通过单轮反馈提升大型推理模型的多轮问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在单轮训练中表现良好，但在多轮问题解决中模型容易失去修正能力并重复回答。

Method: 引入单轮反馈作为观察（UFO），结合多轮强化学习训练模型，设计奖励机制以鼓励多样化的推理。

Result: 实验表明，UFO方法在保持单轮性能的同时，多轮推理准确率提升高达14%。

Conclusion: UFO方法有效提升了模型在多轮问题解决中的反馈反应能力，同时减少了所需轮次。

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [86] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: 论文提出了一种精确约束重构方法，用于解决不平衡分类中的直接度量优化问题，包括固定精度优化召回率、固定召回率优化精度以及优化Fβ分数。实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在不平衡分类中，现有方法通常优化平衡准确率，但无法处理类别重要性不同或特定指标需达到预设水平的情况。因此，研究直接优化精度和召回率等关键指标具有重要意义。

Method: 提出了精确约束重构方法（ERO框架），用于解决直接度量优化问题，并通过精确惩罚方法有效求解。

Result: 在多个基准数据集上的实验表明，该方法在三种直接度量优化问题上优于现有方法。

Conclusion: 论文的ERO框架不仅适用于不平衡分类，还可推广到更广泛的直接度量优化问题。

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [87] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: FedStrategist是一个基于元学习的框架，通过动态选择聚合规则来应对联邦学习中的模型中毒攻击，优于静态防御方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使其易受模型中毒攻击，现有静态防御方法在适应性攻击或异构数据环境下效果有限。

Method: 设计了一个轻量级上下文多臂老虎机代理，根据实时诊断指标动态选择最优聚合规则。

Result: 实验表明，FedStrategist在多样化场景中学习到更优策略，并能通过单一参数控制性能与安全的权衡。

Conclusion: FedStrategist提供了一种实用且可分析的方法，用于构建弹性和智能的去中心化AI系统。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [88] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: 论文提出了一种简单算法，在log-concave先验假设下，证明其收敛到近端算子，为实践中常用的启发式方法提供了理论支持。


<details>
  <summary>Details</summary>
Motivation: 现有去噪模型在逆问题中被广泛使用，但缺乏对用预训练去噪器替代近端算子的理论证明。

Method: 提出一种与实践中常用方法相关的简单算法，并证明其在log-concave先验假设下收敛到近端算子。

Result: 算法可解释为对平滑近端目标的梯度下降，为经验成功的启发式方法提供了理论基础。

Conclusion: 研究为缺乏理论支持的实践方法提供了严格的理论依据。

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [89] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: 论文提出了一种提升深度伪造检测中个体公平性的通用框架，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的滥用对特定群体存在偏见，而现有研究多关注群体公平性，个体公平性未被充分探索。

Method: 提出首个可集成到现有检测器中的通用框架，以增强个体公平性和泛化能力。

Result: 在多个主流数据集上的实验表明，该方法显著提升个体公平性，同时保持检测性能。

Conclusion: 该框架填补了深度伪造检测中个体公平性的研究空白，具有实际应用价值。

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [90] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: GeoHNN是一种神经网络框架，通过显式编码物理定律的几何先验来学习动力学，显著提升了长期稳定性、准确性和能量守恒。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习方法常忽略物理系统的几何本质，导致预测不稳定，尤其是在高维和混沌系统中。

Method: GeoHNN通过参数化惯性矩阵为对称正定矩阵，并使用约束自编码器保持相空间体积，强制实施黎曼几何和辛几何结构。

Result: 实验表明，GeoHNN在耦合振子和高维可变形物体等系统中表现优于现有模型。

Conclusion: 嵌入物理几何不仅是理论上的吸引力，更是构建稳健、可推广物理模型的实践需求。

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [91] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: 该研究开发并验证了四种机器学习模型，用于预测环形几何结构中的临界热通量（CHF），显著优于传统经验方法。


<details>
  <summary>Details</summary>
Motivation: 临界热通量（CHF）的准确预测对核反应堆安全分析至关重要，传统经验方法存在误差大、缺乏可解释性等问题。

Method: 研究采用混合机器学习方法，结合确定性基础模型（Biasi、Bowring、Katto）和ML预测残差，训练并测试了四种ML模型。

Result: ML模型的平均相对误差低于3.5%，显著优于经验模型的26%以上误差。

Conclusion: 混合ML模型在环形几何结构中的CHF预测中表现出色，为核反应堆安全分析提供了更可靠的工具。

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [92] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: 该论文提出了一种基于共轭梯度近似影响函数的方法，用于过滤噪声训练数据，实验显示过滤后模型性能提升1.5%。


<details>
  <summary>Details</summary>
Motivation: 语言模型在强化学习微调中常受噪声数据影响，特别是人类偏好数据集。小规模后训练数据集和参数高效微调方法使得影响函数近似成为可能。

Method: 使用共轭梯度近似影响函数来检测和删除对验证集性能有害的训练样本，并在TL;DR数据集上进行实验。

Result: 过滤10%的训练样本后，模型性能提升1.5%。梯度相似性在检测有益样本上优于影响函数。

Conclusion: 局部曲率对检测有害样本重要，但对识别有益样本影响较小。

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [93] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Solo Connection是一种新颖的PEFT方法，通过调整解码器块级别的表示而非单个权重矩阵，显著减少可训练参数，并在自然语言生成任务中优于LoRA。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于对现有PEFT方法（如LoRA）的局限性，以及大型语言模型（如GPT2）中解码器块数量增加的需求，需要更高效的适应方法。

Method: Solo Connection引入可训练的线性变换，逐步在零向量和任务特定表示之间插值，实现平滑稳定的适应，并采用长跳跃连接增强任务适应能力。

Result: Solo Connection在E2E自然语言生成任务中优于LoRA，可训练参数减少59%（相比LoRA）和99%以上（相比全微调）。

Conclusion: Solo Connection为大型语言模型的高效微调提供了新思路，尤其适用于解码器块数量增加的模型架构。

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [94] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: INCADET是一种用于实时网络攻击检测的增量因果图学习框架，通过动态更新因果图来适应系统行为变化，显著提高了检测准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 网络攻击对关键基础设施的威胁日益严重，传统方法因高数据方差和类别不平衡导致误报率高，且静态因果图方法无法适应实时动态变化。

Method: INCADET包含三个模块：早期症状检测、增量因果图学习和因果图分类，利用流式时间窗口动态更新因果图，并结合GCN分类系统状态。

Result: 在真实关键基础设施数据集上的实验表明，INCADET在准确性和适应性上优于静态因果和深度时序基线方法。

Conclusion: INCADET通过增量学习动态捕捉系统行为变化，为实时网络攻击检测提供了更优的解决方案。

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [95] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 本文分析了简单测试时缩放方法，发现其主要通过限制最大长度实现缩放，而通过追加“Wait”的方式会导致不一致性。与o1类模型相比，简单测试时缩放无法实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 研究简单测试时缩放方法的实际效果，并与o1类模型的自然缩放行为进行对比。

Method: 分析简单测试时缩放（包括限制最大长度和追加“Wait”）的效果，并与o1类模型的自然缩放行为进行比较。

Result: 限制最大长度是简单测试时缩放的主要方式，而追加“Wait”会导致模型振荡。o1类模型通过强化学习自然缩放，性能超越峰值。

Conclusion: 简单测试时缩放无法实现性能提升，真正的目标是解锁更高性能，而非仅模仿缩放行为。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [96] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: 提出一种结合强化学习和深度学习的方法，通过干预模型高效解决大规模随机优化问题，并在供应链库存管理中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模随机优化问题中探索解空间的效率问题，特别是在复杂供应链场景中。

Method: 利用预训练的深度学习模型模拟和组合随机过程，结合强化学习优化，并引入约束协调机制。

Result: 在真实供应链数据集上表现优异，模块化设计提升了可扩展性和性能。

Conclusion: 该方法为大规模随机优化问题提供了高效解决方案，未来可进一步研究其普适性。

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [97] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: ReDiSC是一种基于重参数化掩码扩散模型的图神经网络方法，用于结构化节点分类，通过变分EM框架优化，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法假设节点标签条件独立，忽略了图结构中标签的相关性，ReDiSC旨在解决这一问题。

Method: 使用重参数化掩码扩散模型估计节点标签的联合分布，通过变分EM框架学习。

Result: 在多种图上性能优于现有GNN、标签传播和扩散模型，尤其在大规模数据集上表现突出。

Conclusion: ReDiSC在结构化节点分类任务中具有显著优势，解决了计算效率问题。

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [98] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: 论文提出了一种联邦强化学习框架FRL-EH，解决局部环境统计异质性问题，通过聚合经验学习全局策略并保护隐私。提出新目标函数优化全局策略，确保鲁棒性。提出FedRQ算法并证明其收敛性，扩展至连续状态空间。实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究联邦强化学习中局部环境统计异质性对全局策略学习的影响，提出更贴近现实的鲁棒框架。

Method: 提出FRL-EH框架及新目标函数，设计FedRQ算法并证明其收敛性，扩展至连续状态空间。

Result: FedRQ算法在异构环境中表现优异，实验验证其有效性和鲁棒性。

Conclusion: FRL-EH框架及FedRQ算法在联邦强化学习中具有显著优势，适用于复杂异构环境。

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [99] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: 论文提出了一种基于条件生成问题的知识蒸馏框架GenDD，通过Split Tokenization和Distribution Contraction技术解决了高维优化和标签语义监督不足的问题，实验表明其在无监督和有监督设置下均表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在高维优化和缺乏标签语义监督方面存在挑战，需要一种更高效的框架来解决这些问题。

Method: 提出GenDD框架，结合Split Tokenization实现稳定无监督知识蒸馏，并通过Distribution Contraction技术整合标签监督。

Result: 在无监督设置下，GenDD显著优于KL基线（16.29%提升）；有监督设置下，ResNet-50在ImageNet上达到82.28% top-1准确率。

Conclusion: GenDD框架在知识蒸馏任务中表现出色，尤其在无监督和有监督场景下均取得显著成果。

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [100] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: 提出了一种结构感知的度量函数SDSC，用于时间序列自监督表示学习，解决了传统距离目标（如MSE）的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统SSL方法（如MSE）对振幅敏感、对波形极性不变且尺度无界，阻碍了语义对齐和可解释性。

Method: SDSC基于Dice相似系数，量化时间信号的结构一致性，并可作为损失函数使用。还提出了一种混合损失，结合SDSC与MSE。

Result: 实验表明，SDSC在预测和分类任务中表现优于或与MSE相当，尤其在领域内和低资源场景中。

Conclusion: 结构感知度量（如SDSC）能提升信号表示的语义质量，可作为传统距离方法的替代方案。

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [101] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: 论文提出使用正未标记（PU）学习框架，从无标记数据中高置信度识别控制单元，以解决观察性研究中缺乏明确控制组的问题。


<details>
  <summary>Details</summary>
Motivation: 在观察性研究中，缺乏明确标记的控制单元是估计平均处理效应（ATE）的主要挑战。

Method: 采用PU学习方法，仅基于处理单元从无标记数据中识别控制单元，并通过模拟和真实数据评估其可靠性。

Result: PU学习能成功识别控制单元，并估计出接近真实值的ATE。

Conclusion: 该方法为观察性因果推断提供了新工具，尤其适用于难以进行随机实验的领域。

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [102] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于最大因果熵逆强化学习的方法，用于无限时域平稳平均场博弈，利用再生核希尔伯特空间建模未知奖励函数，并通过梯度上升算法求解。


<details>
  <summary>Details</summary>
Motivation: 现有平均场博弈的逆强化学习方法通常限制奖励函数为固定基函数的线性组合，且多基于有限时域。本文旨在解决这些限制，直接推断非线性奖励结构。

Method: 引入拉格朗日松弛将问题转化为无约束对数似然最大化，并通过梯度上升算法求解。证明了软贝尔曼算子在再生核希尔伯特空间参数下的Fréchet可微性。

Result: 在平均场交通路由博弈中，方法能准确恢复专家行为。

Conclusion: 该方法在无限时域平均场博弈中有效推断非线性奖励函数，优于现有线性方法。

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [103] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: 论文将自注意力机制置于更广泛的基于亲和力的计算范式中，追溯其概念起源，并强调无限特征选择（Inf-FS）作为其基础方法。


<details>
  <summary>Details</summary>
Motivation: 揭示自注意力机制与更广泛的亲和矩阵计算之间的联系，统一不同领域的机器学习研究。

Method: 通过比较自注意力机制与Inf-FS，分析亲和矩阵的定义和应用方式。

Result: 自注意力是Inf-FS的特例，两者共享基于成对关系的推理结构。

Conclusion: 论文通过亲和矩阵的统一视角，为多样化的模型和任务提供了共同的数学基础。

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [104] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: LPS-GNN是一种高效、低成本的GNN框架，能在单GPU上处理1000亿规模的图数据，并在用户获取场景中提升13.8%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法在效率和准确性之间难以平衡，尤其是大规模图数据中的计算和内存问题。

Method: 提出LPS-GNN框架，结合LPMetis图分区算法和子图增强策略，兼容多种GNN算法。

Result: 在Tencent平台上测试，性能提升8.24%至13.89%，优于现有SOTA模型。

Conclusion: LPS-GNN为大规模图数据提供了一种高效、灵活的解决方案。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [105] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: 论文提出了一种结合Transformer-GAN和MILET的新框架，用于无人机飞行状态分类，显著提升了准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 无人机飞行状态的准确分类对安全和高效操作至关重要，但现有方法在动态环境和计算成本方面存在不足。

Method: 集成Transformer-GAN生成合成数据，并结合MILET聚焦关键输入段，减少噪声和计算开销。

Result: 在DroneDetect和DroneRF数据集上分别达到96.5%和98.6%的准确率，优于现有方法。

Conclusion: 该框架在计算效率和泛化能力上表现优异，适合资源受限环境中的实时部署。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [106] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: 论文提出了一种多项式时间确定性算法，用于近似计算k-子空间中位数，其运行时间和近似因子均不随k指数增长。


<details>
  <summary>Details</summary>
Motivation: k-子空间中位数比均值更具鲁棒性和稀疏性，但由于其非凸性，难以高效近似。

Method: 提出了一种新的确定性算法，其近似因子为√d，运行时间为多项式时间。

Result: 算法在真实数据集上表现良好，代码已开源。

Conclusion: 该技术有望应用于其他相关问题，如处理ℓ2,z范数或异常值/稀疏性。

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [107] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 本文提出Rec-AD框架，结合Tensor Train分解与深度学习推荐模型，提升智能电网中虚假数据注入攻击检测的计算效率。


<details>
  <summary>Details</summary>
Motivation: 智能电网中虚假数据注入攻击检测的计算和内存负担随系统规模和数据维度增加而显著上升，限制了检测效率。

Method: Rec-AD通过嵌入压缩、索引重排序优化数据访问及流水线训练机制，减少内存通信开销，并与PyTorch完全兼容。

Result: 实验表明，Rec-AD显著提高计算吞吐量和实时检测性能，缩小攻击窗口并增加攻击成本。

Conclusion: Rec-AD增强了边缘计算能力和可扩展性，为智能电网安全提供了强有力的技术支持。

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [108] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为AD-GCL的新型图对比学习框架，专注于解决现有方法在结构不平衡（如幂律分布网络）下对尾部异常检测的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法在异常检测中过于关注整体性能，而忽视了对结构不平衡的鲁棒性，尤其是对低度节点的尾部异常检测不足。

Method: AD-GCL通过邻居修剪策略过滤噪声边，并通过异常引导的邻居补全扩大尾部节点的感知范围，同时引入视图一致性损失增强表示。

Result: 在多个数据集上的实验表明，AD-GCL在整体、头部和尾部节点的异常检测中均表现出优越性。

Conclusion: AD-GCL显著提升了图异常检测在结构不平衡网络中的鲁棒性和适用性。

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [109] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: 提出了一种名为GCC-Spam的新型垃圾文本检测框架，通过字符相似性网络、对比学习和GAN生成伪样本，解决了对抗策略和标注数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 互联网上垃圾文本的快速增长需要有效的检测机制，以应对信息泄露和社会不稳定等风险。

Method: 结合字符相似性网络捕捉拼写和语音特征，使用对比学习优化潜在空间距离，并通过GAN生成伪样本缓解数据稀缺。

Result: 在真实数据集上的实验表明，该模型优于基线方法，用更少的标注样本实现了更高的检测率。

Conclusion: GCC-Spam框架在垃圾文本检测中表现出色，尤其在对抗策略和数据稀缺场景下具有优势。

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [110] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: 提出了一种基于空间-时间变换器和课程学习的框架SST-CL，用于解决EEG情感识别中的非平稳空间-时间模式整合和动态情感强度适应问题。


<details>
  <summary>Details</summary>
Motivation: EEG情感识别在脑机通信系统中至关重要，但面临非平稳空间-时间模式整合和动态情感强度变化的挑战。

Method: 结合空间编码器（建模通道间关系）和时间编码器（通过窗口注意力机制捕获多尺度依赖），并采用基于强度感知的课程学习策略。

Result: 在三个基准数据集上实现了最先进的性能，消融实验验证了框架各部分的必要性。

Conclusion: SST-CL框架有效整合了空间-时间信息并适应动态情感强度，为EEG情感识别提供了新思路。

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [111] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: 提出了一种名为CPAC的新型分类器，用于解决信用卡欺诈检测中的类别不平衡问题，通过原型注意力机制和VAE-GAN结合，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 信用卡欺诈检测面临类别不平衡和欺诈模式难以捕捉的挑战，现有生成模型（如GANs、VAEs）在少数类样本生成上效果有限，导致分类器性能不佳。

Method: 提出CPAC架构，结合原型注意力机制和VAE-GAN，优化潜在空间结构，提升类别聚类效果。与传统过采样方法（如SMOTE）和生成模型对比。

Result: CPAC在F1-score（93.14%）和召回率（90.18%）上表现优异，潜在聚类分离效果更好。

Conclusion: CPAC通过分类器驱动的潜在空间优化，显著提升了欺诈检测性能，为相关研究提供了新思路。

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [112] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: 论文研究了实时生成AI（RTGen）工作负载在异构SoC上的调度策略，发现调度决策显著影响性能，强调了动态异构调度的重要性。


<details>
  <summary>Details</summary>
Motivation: 实时生成AI（RTGen）工作负载在异构SoC上的调度复杂性和性能影响尚未充分研究，需要探索以支持高性能应用。

Method: 在AMD的Ryzen AI异构SoC上构建多模型场景，分析五种调度策略对实时指标和LLM性能的影响。

Result: 调度决策显著影响性能（如平均41.7%的截止时间违规率差异），需动态异构调度策略。

Conclusion: 动态异构调度对高性能实时生成AI应用至关重要。

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [113] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: 论文介绍了LeanTree，一种白盒工具，用于分解复杂证明状态为独立分支，提升自动定理证明的效率。


<details>
  <summary>Details</summary>
Motivation: 自动定理证明（ATP）因状态和动作空间庞大而具挑战性，白盒方法在LLM时代发展滞后，需填补这一空白。

Method: 提出LeanTree，包含基于Lean 4的工具和分解后的中间状态数据集，支持并行搜索和状态复用。

Result: 初步结果显示白盒方法在某些场景下优于黑盒方法。

Conclusion: 白盒工具LeanTree为ATP提供了更高效的解决方案，具有多项优势。

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [114] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: GRID是一个统一的框架，解决了基于提示的持续学习中的潜在遗忘和提示内存爆炸问题，通过任务感知解码和梯度提示选择策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在任务无关推理下存在潜在遗忘和提示内存爆炸问题，限制了可扩展性。

Method: GRID结合任务感知解码机制（利用代表性输入、自动任务识别和约束解码）和梯度提示选择策略（压缩非信息提示）。

Result: 实验表明，GRID显著提升后向转移，减少遗忘任务达80%，在T5和Flan-T5上优于现有方法。

Conclusion: GRID通过高效解码和提示压缩，实现了可扩展的持续学习，性能优越。

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [115] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: 论文研究了可训练有理激活函数在强化学习和持续学习中的表现，发现其灵活性可能导致训练不稳定，并提出了一种约束变体以平衡表达性和可塑性。


<details>
  <summary>Details</summary>
Motivation: 探索可训练有理激活函数在动态环境中的表现及其对训练稳定性的影响。

Method: 提出一种约束变体，限制输出缩放以提升稳定性，并在MetaWorld和DMC环境中进行实验。

Result: 约束变体在强化学习和持续学习中均提升了稳定性和性能。

Conclusion: 研究揭示了表达性与可塑性之间的权衡，并提出了设计原则以优化可训练激活函数在动态环境中的表现。

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [116] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: 论文提出了一种新框架，通过整合Rashomon集合中多个近优模型的PDP，生成解释不确定性感知的特征效应视图，提升了模型解释的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有自动化机器学习系统通常只关注单一最优模型，忽视了解释不确定性，这在以人为中心的解释性AI中至关重要。

Method: 提出了一种新框架，通过聚合Rashomon集合中多个近优模型的PDP，生成Rashomon PDP，捕捉解释的变异性。

Result: 实验表明，Rashomon PDP在大多数情况下仅覆盖最佳模型PDP的不到70%，揭示了单一模型解释的局限性。

Conclusion: Rashomon PDP通过补充被忽视的信息，提升了模型解释的可靠性和可信度，尤其适用于高风险领域。

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [117] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: 论文探讨了方向性作为人工神经网络的归纳偏置的作用，通过修剪技术诱导方向性，而非硬编码。


<details>
  <summary>Details</summary>
Motivation: 受生物大脑中循环电路的启发，研究方向性是否对人工神经网络有帮助。

Method: 提出一种全连接的感知层（数学上等价于权重绑定的循环神经网络），并通过修剪技术诱导方向性。

Result: 修剪方案成功诱导神经元间信息流的拓扑排序，且不影响性能。

Conclusion: 方向性并非学习的必要条件，但可能是梯度下降和稀疏化可发现的有利归纳偏置。

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [118] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT是一种新型多模态框架，结合稀疏时间点的胸片和临床数据，预测危重患者胸片结果的动态变化，提前12小时发现异常。


<details>
  <summary>Details</summary>
Motivation: ICU患者胸片获取不规律，现有工具无法捕捉时间动态，需要一种能整合多源数据并预测胸片结果的方法。

Method: 通过视觉编码器生成潜在嵌入，与高频临床数据时间对齐，用Transformer模型预测未来胸片结果。

Result: 在2万ICU患者中，CXR-TFT能提前12小时高精度预测胸片异常。

Conclusion: CXR-TFT为时间敏感疾病提供早期干预潜力，改善临床结果。

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [119] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: 本文研究了语言模型中的记忆化问题，探讨了记忆化是否可以通过最优学习避免，以及记忆化对隐私的威胁是否被夸大。通过重新审视现有记忆化测量方法并引入新的上下文记忆化概念，研究发现不同记忆化测量方法结果不一致，最优学习无法完全避免记忆化，且改进学习会减少上下文和反事实记忆化但增加基于回忆的记忆化。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决语言模型中的记忆化问题，尤其是其对隐私的潜在威胁，以及记忆化是否可以通过最优学习避免。

Method: 方法包括重新审视基于回忆和反事实的记忆化测量，并引入新的上下文记忆化概念，通过实验验证不同记忆化测量方法的效果。

Result: 实验结果表明：(a) 不同记忆化测量方法对字符串的记忆化顺序不一致；(b) 最优学习无法完全避免训练字符串的部分记忆化；(c) 改进学习减少上下文和反事实记忆化但增加基于回忆的记忆化；(d) 现有基于回忆的记忆化报告可能并不构成隐私威胁。

Conclusion: 结论指出记忆化是语言模型学习中的固有现象，但不同测量方法对其评估不一致，改进学习可以减少部分记忆化，但无法完全避免。

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [120] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: Omni-Think是一个统一的强化学习框架，通过结合规则奖励和生成偏好信号提升LLM性能，课程学习显著优于联合训练和模型合并。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法在泛化性上表现不佳，倾向于记忆而非迁移学习，需要一种更有效的后训练方法。

Method: 提出Omni-Think框架，结合规则奖励和LLM-as-a-Judge生成偏好信号，采用课程学习策略从结构化到开放任务逐步训练。

Result: 实验显示课程学习性能提升5.2%（联合训练）和9.1%（模型合并），验证了任务感知采样和混合监督的有效性。

Conclusion: Omni-Think框架为通用LLM的后训练提供了可扩展的强化学习方法，任务感知策略是关键。

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [121] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: 论文探讨了利用大型语言模型（LLM）对金融知识图谱中的局部子图进行推理，以检测洗钱行为。


<details>
  <summary>Details</summary>
Motivation: 洗钱行为涉及的实体复杂且高度互联，需要基于图结构数据的推理方法。

Method: 提出了一种轻量级流程：提取感兴趣实体的k跳邻域，将其序列化为结构化文本，并通过少量示例提示LLM评估可疑性并生成解释。

Result: 在合成反洗钱（AML）场景中，LLM能够模拟分析师逻辑，识别风险信号并提供合理解释。

Conclusion: 研究表明LLM在图推理中具有潜力，为可解释的语言驱动金融犯罪分析奠定了基础。

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [122] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: 论文提出了一种将等变性网络理论扩展到时间序列模型的方法，以处理连续对称性（如视觉运动），并展示了其在训练速度和泛化能力上的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的等变性网络仅适用于静态变换和前馈网络，无法处理时间参数化的序列变换，如RNN。本文旨在填补这一空白。

Method: 扩展等变性网络理论到时间参数化的序列变换（“流”），提出一种使RNN具备流等变性的方法。

Result: 流等变性模型在训练速度、长度泛化和速度泛化上显著优于非等变性模型。

Conclusion: 本文为构建尊重时间参数化对称性的序列模型迈出了第一步。

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [123] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: 研究发现语言模型通过语义无关数据传递行为特征，称为“潜意识学习”，即使过滤相关数据仍能传递，可能对AI开发带来潜在风险。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型是否能够通过看似无关的数据传递行为特征，揭示AI开发中的潜在问题。

Method: 通过“教师”模型生成仅含数字序列的数据集，训练“学生”模型，观察其是否学习到教师的行为特征，并在不同场景下验证。

Result: 学生模型确实学习到教师的行为特征，即使数据中未直接提及该特征；不同基础模型的师生组合无法实现此效果。

Conclusion: 潜意识学习是普遍现象，可能通过蒸馏传播未预期的行为特征，需引起AI开发者的警惕。

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [124] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: 本文提出了一个综合基准，评估了基础模型在电子健康记录（EHRs）处理中的性能、公平性和可解释性，并展示了多模态数据对预测性能的提升。


<details>
  <summary>Details</summary>
Motivation: 研究旨在支持开发高效且可信赖的多模态人工智能系统，以应对临床应用中处理多样化医疗数据的挑战。

Method: 使用MIMIC-IV数据库，开发标准化数据处理流程，系统比较了八种基础模型（包括单模态和多模态模型）。

Result: 多模态数据的结合显著提升了预测性能，且未引入额外偏差。

Conclusion: 该基准为开发适用于真实临床场景的多模态AI系统提供了支持，代码已开源。

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [125] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: 论文提出了一种名为CSG的新框架，首次将信道估计与网格化统一起来，仅使用RSRP数据实现高效网格化，并通过CSG-AE模型和PIDA训练方案显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有网格化方法（如GSG或BSG）依赖不可用的位置数据或错误的信号强度假设，限制了大规模网络优化的效率。

Method: 提出CSG框架，通过联合优化问题利用RSRP估计CAPS并分区；开发CSG-AE模型，包含RSRP-to-CAPS编码器、稀疏码书量化器和物理信息解码器；提出PIDA训练方案解决训练稳定性问题。

Result: 在合成数据上，CSG-AE在CAPS估计和聚类质量上表现优异；在真实数据上，RSRP预测误差显著降低（Active MAE降低30%，Overall MAE降低65%），同时改善了信道一致性和集群平衡。

Conclusion: CSG框架和CSG-AE模型显著提升了网格化性能，为大规模网络优化提供了更高效的工具。

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [126] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: 研究探讨了在对比学习框架中引入自适应边距（eMargin）对时间序列表示学习的影响，发现其在无监督聚类指标上表现优异，但在下游分类任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索自适应边距是否能通过调整相似性阈值，改善时间序列中相邻但不相似时间步的分离效果，从而提升下游任务性能。

Method: 在对比损失函数中引入自适应边距（eMargin），并在三个基准数据集上评估其对聚类和分类性能的影响。

Result: eMargin在无监督聚类指标上优于基线方法，但在下游分类任务中表现不佳。

Conclusion: 高无监督聚类分数并不一定意味着嵌入在下游任务中有效，需进一步优化自适应边距的设计。

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [127] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: 提出了一种名为CSE-FSL的新型联邦分裂学习方法，通过辅助网络减少通信和存储开销，同时保证收敛性。


<details>
  <summary>Details</summary>
Motivation: 联邦分裂学习（FSL）虽然减轻了边缘设备的计算负担，但仍存在高通信开销和服务器存储需求大的问题。

Method: 利用辅助网络在客户端本地更新权重，服务器仅维护单一模型，并选择性地传输数据以减少通信量。

Result: 理论分析和实验结果表明，CSE-FSL显著降低了通信开销，并在真实任务中表现优异。

Conclusion: CSE-FSL是一种高效且可扩展的联邦分裂学习方法，解决了现有方法的通信和存储问题。

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [128] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: RLVR（带可验证奖励的强化学习）虽能提升AI在复杂逻辑任务中的表现，但其是否能真正扩展模型的推理边界尚不明确。研究发现RLVR受限于基础模型的支持范围，可能抑制原创解决方案的发现，并存在熵-奖励权衡。实验表明RLVR虽提高pass@1，但支持范围缩小，且可能忽略正确但少见的解。


<details>
  <summary>Details</summary>
Motivation: 探讨RLVR是否真正扩展模型的推理能力，还是仅放大已知高奖励输出以提高精度。

Method: 结合理论和实证研究，分析RLVR的局限性，包括其对基础模型支持的依赖和熵-奖励权衡。

Result: RLVR虽提高精度，但支持范围缩小，且可能忽略正确解；生成路径的熵增加，但答案多样性减少。

Conclusion: RLVR在扩展推理能力方面存在潜在限制，未来需探索显式探索机制或混合策略以突破限制。

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [129] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: TALE-EHR提出了一种基于Transformer的框架，通过时间感知注意力机制和预训练语言模型，解决了EHR数据异质性和复杂时间模式的问题，显著提升了疾病进展预测的性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）包含丰富的临床信息，但数据异质性和复杂时间模式使其建模困难，传统方法难以处理不规则时间间隔。

Method: TALE-EHR采用Transformer框架，结合时间感知注意力机制和预训练语言模型生成的嵌入，以捕捉精细时间动态和语义信息。

Result: 在MIMIC-IV和PIC数据集上的实验表明，TALE-EHR在疾病进展预测等任务上优于现有基线方法。

Conclusion: TALE-EHR展示了结合显式时间建模和强语义表示在EHR分析中的优势，为未来研究提供了有力工具。

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [130] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: 提出了一种基于控制屏障函数（CBFs）的分层多智能体强化学习（HMARL）方法，用于安全关键多智能体系统中的安全策略学习。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体安全关键系统中，如何在满足安全要求的同时实现任务协作的问题。

Method: 采用分层方法，高层学习联合协作行为，低层基于高层策略学习安全个体行为，结合HMARL-CBF算法。

Result: 在复杂环境中显著提升安全性（接近完美成功率），同时提高性能。

Conclusion: HMARL-CBF方法在多智能体安全导航任务中表现出色，优于现有方法。

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [131] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: Graph Tsetlin Machine (GraphTM) 是一种用于从图结构输入中学习可解释深度子句的方法，通过消息传递构建嵌套子句，提高可解释性和数据利用率，并在多个领域表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统 Tsetlin Machine (TM) 在处理图结构输入时存在局限性，GraphTM 的引入旨在扩展其能力，支持序列、网格、关系和多模态输入，同时保持可解释性和高效性。

Method: GraphTM 通过消息传递构建嵌套深度子句，识别子图模式，减少子句数量，提高数据利用率。

Result: GraphTM 在图像分类、动作共指跟踪、推荐系统和病毒基因组序列分析中表现优异，准确率显著高于对比方法，且训练速度更快。

Conclusion: GraphTM 展示了图表示学习和深度子句为 TM 学习带来的新可能性，适用于多种领域。

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [132] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: 提出了一种改进的结构化剪枝重要性度量框架，旨在减少模型大小的同时保持应用特定性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的高复杂性和计算需求限制了其广泛应用，而传统剪枝方法在保持性能方面存在不足。

Method: 采用多策略确定每组剪枝的最优幅度，平衡压缩与任务性能。

Result: 在MNIST图像重建任务中，该方法有效保留了任务相关性能，即使大幅剪枝后仍满足应用需求。

Conclusion: 所提方法在模型压缩中成功平衡了剪枝与性能，具有实际应用价值。

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [133] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: 论文探讨了量子机器学习中的模型透明度问题，提出将经典不确定性量化方法映射到量子领域。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习和量子机器学习中模型透明度不足导致过拟合和预测过度自信，缺乏对量子机器学习黑盒性质的研究。

Method: 基于经典不确定性量化和量子贝叶斯建模的理论发展，实证评估将经典方法映射到量子机器学习的技术。

Result: 研究发现需利用经典不确定性量化见解，为量子机器学习模型设计引入不确定性意识。

Conclusion: 强调在量子机器学习模型设计中融入经典不确定性量化方法的必要性。

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [134] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWCM方法通过动态调整动量解决联邦学习中非独立同分布数据的收敛问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布数据（尤其是长尾分布）中面临收敛困难和模型偏差问题。

Method: 提出FedWCM方法，动态调整动量以纠正长尾分布引入的方向偏差。

Result: 实验表明，FedWCM解决了不收敛问题，优于现有方法。

Conclusion: FedWCM有效提升了联邦学习在数据不平衡和客户端异构场景下的效率和效果。

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [135] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: 论文提出了一种新任务Time-RA，将时间序列异常检测从判别式任务转变为生成式任务，并引入多模态数据集RATs40K，用于异常推理。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法仅关注二元分类，缺乏详细分类和解释性推理。

Method: 提出Time-RA任务，利用LLMs实现生成式推理；构建RATs40K数据集，包含多模态数据和精细标注。

Result: 通过LLMs和多模态LLMs的基准测试，展示了模型的潜力与局限性，强调监督微调的重要性。

Conclusion: 该任务和数据集为可解释的时间序列异常检测和推理开辟了新方向。

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [136] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种名为ROBAD的新型Transformer模型，用于检测互联网平台上的不良行为者，并通过局部和全局信息捕获以及对抗性训练增强模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在检测不良行为者时对输入序列的微小变化敏感，缺乏鲁棒性。为了解决这一问题，论文旨在提升模型的理解能力和知识，以识别潜在的输入修改。

Method: ROBAD结合Transformer编码器和解码器块，分别捕获帖子级别的局部信息和序列级别的全局信息，并通过对比学习增强的分类层利用模拟攻击者的修改序列进行训练。

Result: 在Yelp和Wikipedia数据集上的实验表明，ROBAD能够有效抵御最先进的对抗攻击，准确检测不良行为者。

Conclusion: ROBAD通过局部和全局信息捕获以及对抗性训练，显著提升了模型在对抗环境下的鲁棒性和检测能力。

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [137] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 论文提出通过强化学习训练流匹配策略，超越原始演示策略性能，并介绍两种方法：RWFM和GRPO，实验显示GRPO显著优于模仿学习。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过强化学习提升流匹配策略的性能，超越由次优策略（如人类操作员）生成的演示数据。

Method: 提出两种方法：Reward-Weighted Flow Matching (RWFM) 和 Group Relative Policy Optimization (GRPO)，结合可变时间范围的流匹配规划。

Result: 在模拟任务中，GRPO方法比模仿学习方法减少50%至85%的成本。

Conclusion: 强化学习可以显著提升流匹配策略的性能，GRPO方法尤其有效。

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [138] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: 提出了一种名为iQRA的新方法，通过集成点预测生成概率预测，改进不确定性估计，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 电力市场等波动领域需要准确预测和不确定性评估，以降低决策风险。现有机器学习模型缺乏不确定性估计。

Method: 基于QRA框架，引入随机顺序约束，提出iQRA方法，改进预测准确性、可靠性和计算成本。

Result: 在德国日前电力市场的预测研究中，iQRA在可靠性和锐度上优于现有后处理方法，提供校准良好的预测区间。

Conclusion: iQRA通过等渗正则化简化分位数回归问题，提供无超参数变量选择方法，显著提升预测性能。

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [139] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: 论文提出了一种新的鲁棒控制理论扩展，明确处理价值函数梯度的不确定性，适用于强化学习等领域。通过零和动态博弈和新的非线性偏微分方程（GU-HJBI）分析问题，并提出了新的算法（GURAC）验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在强化学习等应用中，价值函数梯度的不确定性普遍存在，但传统方法未明确处理这一问题。本文旨在填补这一空白。

Method: 通过零和动态博弈建模，引入GU-HJBI方程，分析其解的性质，并在LQ情况下验证理论。最终提出GURAC算法。

Result: 证明了经典二次价值函数假设在梯度不确定性下失效，并提出了非线性修正。GURAC算法在实验中表现出稳定训练的效果。

Conclusion: 本文为鲁棒控制提供了新方向，对强化学习和计算金融等领域有重要意义。

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [140] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: 论文提出AnalogFed，一种支持去中心化协作的生成式AI框架，用于模拟电路拓扑发现，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计数据具有专有性和保密性，限制了生成式AI的研究进展，需要一种不共享原始数据的协作方法。

Method: 提出AnalogFed框架，结合联邦学习和隐私保护技术，处理数据异构性并开发生成模型。

Result: 实验表明，AnalogFed在性能上与集中式基线相当，同时确保数据隐私，生成模型在拓扑设计上表现出高效和可扩展性。

Conclusion: AnalogFed为模拟电路设计的协作创新提供了可行方案，解决了数据隐私和分散性问题。

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [141] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: 论文提出U-Cast模型和Time-HD基准，解决高维时间序列预测（HDTSF）中的复杂通道相关性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测（TSF）模型在高维场景下难以处理复杂的通道相关性，亟需新方法。

Method: 提出U-Cast，基于查询注意力学习潜在层次通道结构，并引入全秩正则化解耦高相关通道表示。

Result: 理论证明跨通道信息降低预测风险，实验显示U-Cast在Time-HD基准上优于基线模型。

Conclusion: U-Cast和Time-HD为HDTSF研究奠定基础。

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [142] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 提出一种遗传算法，通过优化分类和回归任务的复杂性度量，生成具有不同难度级别的合成数据集。


<details>
  <summary>Details</summary>
Motivation: 研究社区需要更先进的合成数据生成器来评估机器学习方法的优缺点，本研究旨在通过生成多样化的数据集来满足这一需求。

Method: 使用遗传算法优化分类和回归任务的复杂性度量，通过线性特征投影调整合成数据集的复杂性。

Result: 实验表明，算法能生成具有目标复杂性的数据集，且数据复杂性与识别质量相关。

Conclusion: 该遗传算法能有效生成多样化的合成数据集，为机器学习方法评估提供可靠工具。

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [143] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: 研究多标签分类问题，利用逻辑约束和序列模型建模标签相关性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模标签分类中标签间的逻辑约束问题。

Method: 使用个体标签分类器输入到序列模型中，生成联合分布。

Result: 实验证明该架构能有效利用约束进行训练和推理。

Conclusion: 该架构能建模标签相关性并利用约束提升分类效果。

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [144] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: 提出了一种基于共振隧穿二极管（RTD）的神经形态计算架构，用于物理储层计算（RC），并在图像识别任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能向实时、边缘和资源受限环境扩展，需要硬件高效的计算模型。

Method: 理论构建并数值实现基于RTD的RC系统，应用于手写数字分类和Fruit~360数据集的对象识别。

Result: 该架构在性能表现良好的同时，遵循新一代RC原则，用确定性非线性变换替代随机连接。

Conclusion: RTD-based RC架构在资源受限环境中具有潜力。

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [145] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: 论文研究了反事实解释（CFEs）在机器学习决策模型中的应用，发现现有评估指标与用户偏好不一致，提出了一种用户中心的两阶段模型AWP，显著提高了预测用户偏好CFEs的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CFE评估指标（如接近性）可能忽略用户实际偏好和约束，导致解释与实际需求脱节。

Method: 通过两个用户研究（小规模试点和详细的两天研究）验证现有指标的局限性，并提出AWP模型。

Result: 试点研究显示用户偏好与现有指标仅63.81%一致；AWP模型预测用户偏好CFEs的准确率达84.37%。

Conclusion: 研究强调了用户中心评估指标的重要性，AWP模型为个性化CFE生成提供了有效支持。

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [146] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: JL-GAT是一种将GAT应用于多智能体强化学习（MARL）的交通信号控制方法，通过结合邻近智能体信息，平衡可扩展性与增强的接地能力，有效缩小模拟与现实的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现实交通网络中多交叉口的交互特性更适合MARL框架，但现有GAT方法仅适用于单智能体RL，无法直接应用于MARL。因此，需要一种既能扩展又保留关键交互的方法。

Method: 提出JL-GAT，采用分散式GAT方法，结合邻近智能体信息，增强MARL在交通信号控制中的接地能力。

Result: 在模拟恶劣天气条件下的多种道路网络中，JL-GAT表现出色，并通过消融实验验证其有效性。

Conclusion: JL-GAT成功将GAT扩展到MARL框架，为现实交通网络中的信号控制提供了可扩展且高效的解决方案。

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [147] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 论文提出利用平均可控性和新型排名编码方法提升GNN在社交网络分类任务中的性能，解决了节点特征缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 社交网络中节点特征常因隐私或固有属性缺失而不可用，影响GNN性能。

Method: 提出两种策略：1）使用平均可控性等中心性指标（NCT-EFA）作为节点特征；2）开发排名编码方法将图论指标转化为固定维度特征。

Result: 实验表明，平均可控性显著提升GNN性能，排名编码方法优于传统独热编码（ROC AUC从68.7%提升至73.9%）。

Conclusion: 平均可控性和排名编码方法有效解决了节点特征表达不足的问题，提升了GNN在社交网络中的表现。

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [148] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的多模态方法LSDGNN，通过长距离和短距离图神经网络分别获取远距离和近距离话语的多模态特征，并引入差分正则器和双仿射模块优化特征交互。此外，提出改进的课程学习（ICL）解决数据不平衡问题，实验表明模型在IEMOCAP和MELD数据集上表现优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 情感识别在对话中（ERC）是一个实际且具有挑战性的任务，需要有效捕捉远距离和近距离话语的特征。

Method: 基于有向无环图（DAG）构建长距离和短距离图神经网络，使用差分正则器和双仿射模块优化特征交互，并提出改进的课程学习（ICL）解决数据不平衡问题。

Result: 在IEMOCAP和MELD数据集上的实验结果表明，模型性能优于现有基准。

Conclusion: LSDGNN通过多模态特征提取和优化学习策略，显著提升了情感识别在对话中的表现。

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [149] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: 提出了一种基于注意力机制的图神经网络框架，用于捕捉食品配送中的时空依赖性，提升订单量预测准确性。


<details>
  <summary>Details</summary>
Motivation: 食品配送平台的效率和响应能力依赖于准确的需求预测，而订单量的时空异质性直接影响运营决策。

Method: 将配送环境建模为图，节点代表配送区域，边反映空间邻近性和历史订单流模式；通过注意力机制动态加权邻近区域的影响。

Result: 在真实数据集上验证了模型的高预测准确性，支持车队定位、资源分配和调度优化。

Conclusion: 该框架为城市食品配送提供了一种可扩展且自适应的解决方案。

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [150] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: CHORDS提出了一种基于多核并行的训练免费、模型无关的扩散采样加速框架，显著提升采样速度且不降低质量。


<details>
  <summary>Details</summary>
Motivation: 扩散生成模型的高保真生成能力受限于计算昂贵的推理过程，现有加速方法需重新训练或牺牲质量。

Method: 将多核扩散采样视为ODE求解器管道，通过理论支持的核间通信机制，慢而准的求解器逐步修正快而粗糙的求解器。

Result: CHORDS在多样大规模图像和视频扩散模型中实现显著加速，四核提速2.1倍，八核提速2.9倍，无质量损失。

Conclusion: CHORDS为实时高保真扩散生成奠定基础，提供高效且通用的加速方案。

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [151] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: 论文提出了一种基于时间基函数模型（TBFMs）的闭环神经刺激方法，用于个性化治疗神经系统疾病，如帕金森病，并展示了其在单次试验中的高效预测能力。


<details>
  <summary>Details</summary>
Motivation: 解决闭环神经刺激在个体化治疗中的样本效率、训练时间和延迟问题，以推动AI技术在临床中的应用。

Method: 采用时间基函数模型（TBFMs）进行单次试验的时空前向预测，模拟闭环刺激对神经活动的调控。

Result: TBF模型在40次实验中表现出高效性（训练时间2-4分钟，延迟0.2ms），预测精度与基线非线性模型相当，优于线性模型。

Conclusion: TBF模型为AI驱动的闭环神经刺激提供了高效、低延迟的解决方案，有望推动临床应用的进展。

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [152] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: 论文提出了一种流式遗忘学习范式，解决了现有方法在处理流式数据遗忘请求时的效率和性能问题。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法通常批量处理遗忘数据，而实际场景中数据遗忘请求往往是流式的，导致效率和效果下降。

Method: 将遗忘问题形式化为分布偏移问题，提出一种无需原始训练数据的流式遗忘算法。

Result: 理论分析证明了算法的误差界限，实验验证了其在多种模型和数据集上的性能。

Conclusion: 提出的流式遗忘方法在理论和实验上均表现出色，解决了流式遗忘的挑战。

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [153] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出了一种利用不完整专家演示的强化学习框架，通过映射函数将状态与专家数据的相似性转化为内在奖励，支持灵活探索。


<details>
  <summary>Details</summary>
Motivation: 现实环境中，显式奖励稀缺且专家演示常不完整，需开发能有效利用这些信号的通用智能体。

Method: 使用映射函数将状态与专家数据的相似性转化为内在奖励，并采用混合自编码专家模型处理不完整演示。

Result: 实验表明，该方法在稀疏和密集奖励环境中均表现优异，即使演示数据不完整。

Conclusion: 为现实场景中缺乏最优数据和精确奖励控制的强化学习提供了实用框架。

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [154] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 该论文扩展了PSID方法，引入滤波和平滑技术，优化多变量时间序列分析。


<details>
  <summary>Details</summary>
Motivation: 现有PSID方法仅利用过去数据预测，而滤波和平滑技术可提升离线应用的估计精度。

Method: 提出PSID滤波扩展和新型前向-后向PSID平滑算法，通过降秩回归学习最优增益。

Result: 在模拟数据中验证了方法能恢复真实模型参数，并实现最优滤波和平滑解码性能。

Conclusion: 为多变量时间序列分析提供了线性滤波和平滑的理论框架，扩展了动态交互分析工具。

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [155] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: FG-TS通过乐观奖励提升探索能力，在精确后验下表现优异，但在近似后验下表现未充分验证。实验表明FG-TS在线性和逻辑问题中优于传统TS，但在神经网络问题中较弱。


<details>
  <summary>Details</summary>
Motivation: 解决Thompson Sampling在高维问题中探索不足的问题，并验证FG-TS在近似后验下的性能。

Method: 使用FG-TS及其平滑变体SFG-TS，在11个真实和合成基准上测试，比较精确和近似后验下的表现。

Result: FG-TS在线性和逻辑问题中优于传统TS，但在神经网络问题中表现较差。实验揭示了奖励规模和采样噪声的权衡。

Conclusion: FG-TS及其变体易于使用且性能有竞争力，建议作为现代上下文赌博机基准的基线。实验代码已开源。

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [156] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: MGT是一种多视图图变换器框架，通过融合SE3不变和SO3等变图表示，显著提升了晶体属性预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉晶体结构的复杂几何和拓扑特征，限制了机器学习在大规模晶体材料模拟中的应用。

Method: 提出MGT框架，结合SE3不变和SO3等变图表示，并采用轻量级专家混合路由器动态调整权重。

Result: MGT在晶体属性预测任务中平均绝对误差降低21%，在迁移学习场景中性能提升高达58%。

Conclusion: MGT是一个有效的晶体材料属性预测模型，具有跨领域扩展性，为新材料发现提供了有力工具。

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [157] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: M-DESIGN提出了一种动态模型知识库（MKB）管道，通过自适应地整合模型架构修改的先验知识，填补了数据库研究中模型优化的空白。


<details>
  <summary>Details</summary>
Motivation: 传统静态模型选择方法忽视了任务查询与模型架构之间的动态依赖关系，导致匹配不优且无法有效优化模型。

Method: M-DESIGN通过知识编织引擎将模型优化重构为任务元数据的自适应查询问题，利用图关系知识模式支持细粒度分析，并驱动预测性查询规划器。

Result: 在33个数据-任务对中，M-DESIGN在有限预算内为26对提供了最优模型。

Conclusion: M-DESIGN通过动态知识库和自适应查询优化模型选择，显著提升了模型匹配和优化效果。

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [158] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock是一个去中心化框架，用于安全高效的LLM微调，通过区块链信任层和经济激励替代中心化聚合器，首次实现70B参数模型在异构、无信任环境中的微调。


<details>
  <summary>Details</summary>
Motivation: 解决中心化控制的不足和去中心化方案中的计算与通信开销问题，同时避免联邦学习中的单点攻击和数据中毒风险。

Method: 集成区块链信任层与经济激励，设计安全、可审计的合作协议，替代中心化聚合器。

Result: 实验显示FLock能抵御后门中毒攻击，降低对抗攻击成功率68%以上，并实现跨领域知识协同。

Conclusion: FLock在安全、多领域、去中心化环境中成功微调70B LLM，表现出优越的跨领域泛化能力。

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [159] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM是一个统一的数学模型，用于分析主动学习（AL）的动态过程，通过四个关键参数预测性能并比较不同策略。


<details>
  <summary>Details</summary>
Motivation: 传统AL评估方法仅关注最终准确性，无法全面反映学习过程动态。PALM填补了这一空白。

Method: 提出PALM模型，通过四个参数（可达到的准确性、覆盖效率、早期性能和可扩展性）描述AL轨迹。

Result: 在多个数据集和策略上验证PALM，证明其能有效预测学习曲线并揭示AL方法的效率、覆盖范围和可扩展性。

Conclusion: PALM为AL的系统性评估提供了基础，支持在有限预算下选择高效策略，促进研究和实际应用中的数据效率。

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [160] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 论文通过拉格朗日优化在令牌空间上为Transformer提供了理论数学背景，将其视为高维单位球上的切纤维流图，并推导了其变分问题的欧拉-拉格朗日方程。


<details>
  <summary>Details</summary>
Motivation: 为Transformer建立数学框架，填补其在流形变分计算领域的空白。

Method: 通过变分法开发功能框架，证明Transformer是变分问题的自然求解器，并推导其欧拉-拉格朗日方程。

Result: 提出了新的适用场景，证明了Transformer作为变分问题求解器的有效性，并提供了相关数学结果。

Conclusion: 论文为Transformer在变分计算领域奠定了基础，展示了其在神经网络近似下的变分分析潜力。

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [161] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: 提出一种基于自适应随机傅里叶特征（ARFF）的训练算法，用于从快照数据中学习随机微分方程的漂移和扩散分量，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决从快照数据中学习随机微分方程漂移和扩散分量的挑战，提供更高效和准确的建模方法。

Method: 采用自适应随机傅里叶特征（ARFF）结合Metropolis采样和重采样，基于Euler-Maruyama积分的似然损失函数。

Result: 在多项基准测试中，ARFF方法在损失最小化和收敛速度上均优于传统Adam优化方法。

Conclusion: ARFF方法为数据驱动的随机动力学建模提供了一种高效且性能优越的替代方案。

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [162] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: FedMultiEmo是一个隐私保护的多模态情感识别框架，结合视觉和生理数据，通过联邦学习实现高精度且保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决车载情感识别中的模态脆弱性、生理变异性和隐私风险问题。

Method: 使用CNN处理面部图像，随机森林分类生理数据，通过多数投票融合多模态数据，并采用联邦学习和个性化联邦平均方案。

Result: 联邦CNN准确率77%，随机森林74%，融合后87%，与集中式基线相当，同时保护数据隐私。

Conclusion: FedMultiEmo为车载环境提供了一种实时、隐私保护的情感识别实用方案。

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [163] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 论文研究了RLHF中的过优化问题，提出了一种无需新标签的离线策略校正奖励建模方法（OCRM），实验证明其优于标准RLHF方法。


<details>
  <summary>Details</summary>
Motivation: RLHF训练中，奖励模型（RM）因分布偏移导致过优化，无法准确反映人类偏好。

Method: 提出OCRM方法，通过重要性加权迭代校正RM，无需新数据。

Result: 在摘要和聊天机器人数据集上，OCRM显著优于标准RLHF方法。

Conclusion: OCRM有效解决了RLHF中的过优化问题，提升了最终策略的准确性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [164] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: 研究通过测试时适应（TTA）技术解决音频分类中的域偏移问题，比较了TTT、TENT和CoNMix方法，改进的CoNMix表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习中的域偏移问题，特别是在音频分类中由背景噪声引起的性能下降。

Method: 采用TTT、TENT和CoNMix三种TTA方法，在AudioMNIST和SpeechCommands V1数据集上测试不同背景噪声和噪声强度下的性能。

Result: 改进的CoNMix在域偏移下表现最优（AM数据集在10 dB运动自行车噪声下错误率为5.31%，3 dB流水噪声下为12.75%）。

Conclusion: 这是首次利用TTA技术解决音频分类中的域偏移问题，改进的CoNMix方法显著提升了分类准确性。

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [165] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: 论文提出了一种名为“数据感知可微分神经架构搜索”的新方法，通过同时优化模型架构和输入数据特性，简化TinyML系统设计。


<details>
  <summary>Details</summary>
Motivation: 机器学习的高资源消耗限制了其广泛应用，尤其是TinyML系统设计的复杂性阻碍了其普及。

Method: 扩展了可微分神经架构搜索的搜索空间，包括数据配置参数，实现模型架构与输入数据的协同优化。

Result: 在关键词识别任务中，该方法成功生成了轻量且高精度的系统。

Conclusion: 该方法为TinyML系统设计提供了一种高效且简化的解决方案。

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [166] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: 本研究评估了传统放射组学（CR）和深度学习（DL）MRI放射组学在胶质母细胞瘤预后中的附加价值，发现其相对于临床和分子预测因子的优势有限。


<details>
  <summary>Details</summary>
Motivation: 验证放射组学在胶质母细胞瘤预后中的实际价值，尤其是在多中心数据集上的表现。

Method: 使用1152例胶质母细胞瘤患者的多中心数据，开发并评估CR和DL模型，比较不同特征集（影像、临床/分子、组合）和患者子集的表现。

Result: 组合特征的CR模型在外部验证中AUC为0.75，略优于临床（0.74）和影像（0.68）模型，但优势有限。DL模型趋势类似但无统计学意义。

Conclusion: 标准CR和DL放射组学方法在胶质母细胞瘤预后中的附加价值有限，临床预测因子（如年龄和性别）仍为主要影响因素。

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [167] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: PhysGym是一个用于评估基于大型语言模型的科学推理能力的基准测试套件和模拟平台，通过控制先验知识水平和问题复杂性来区分模型表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏评估大型语言模型在科学发现中应对环境复杂性和利用先验能力的专用基准，PhysGym填补了这一空白。

Method: PhysGym提供交互式物理环境模拟，要求模型主动探索、收集数据并形成假设，同时标准化评估协议和指标。

Result: 基准测试展示了PhysGym能有效区分不同先验知识和任务复杂性下的模型能力。

Conclusion: PhysGym为评估科学推理能力提供了有效的工具，未来可扩展至更广泛的研究领域。

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [168] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: 本文探讨了住院时间（LOS）预测准确性与床位调度灵活性之间的关系，研究了在预测误差下最有效的患者重新调度策略，以优化资源利用并防止床位溢出。


<details>
  <summary>Details</summary>
Motivation: 下游资源（如床位）的可用性对计划选择性手术患者的入院至关重要。由于实际LOS可能与预测值差异较大，导致调度不可行，因此需要研究如何通过灵活的调度策略应对这种不确定性。

Method: 基于模拟机器学习方法，评估不同纠正策略下LOS预测准确性与调度灵活性的关系，分析最有效的重新调度策略。

Result: 研究发现，更准确的LOS预测可以减少重新调度的影响，但训练高精度模型成本较高。通过灵活的调度策略（如调整入院日期或转移患者），可以有效防止床位溢出。

Conclusion: 在LOS预测误差下，灵活的调度策略是优化资源利用和防止床位溢出的关键，而预测精度的提升虽有益但需权衡成本。

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [169] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: 论文探讨了AI在优化卫星巨型星座运营中的作用，通过强化学习（RL）改进数据路由和资源分配，展示了其优于传统方法的灵活性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 近地轨道卫星星座的快速扩张对卫星网络管理提出了高效、可扩展和弹性运营的需求，需要创新方法解决。

Method: 研究团队开发了AI驱动的算法，利用强化学习（RL）优化数据路由（降低端到端延迟）和资源分配（高效利用电池和内存等有限资源）。

Result: RL在多种卫星星座配置和操作场景中表现优于传统最短路径算法，提供了更高的灵活性、可扩展性和通用性。

Conclusion: AI（尤其是RL）能够为卫星星座管理提供更自适应、稳健且经济高效的解决方案，改变其管理格局。

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [170] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: 论文指出当前异常检测算法的评估方式存在问题，导致进展停滞，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前异常检测算法的评估方式未能充分反映实际应用的多样性，限制了算法的进步。

Method: 提出重新思考异常检测的基准测试方法，建议基于共同分类法定义场景，分析端到端流程，并根据场景目标评估算法。

Result: 识别了三个关键改进领域：场景分类、端到端分析和目标导向评估。

Conclusion: 需要通过改进评估方法来推动异常检测领域的进步。

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [171] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: 提出了一种红队多智能体强化学习框架，通过干扰和探索生成安全关键场景中的极端案例。


<details>
  <summary>Details</summary>
Motivation: 现有方法在安全关键场景中难以捕捉极端案例，需要更高效的生成方法。

Method: 使用红队多智能体强化学习框架，结合约束图表示马尔可夫决策过程，确保安全规则下干扰自动驾驶车辆。

Result: 实验表明，该框架显著影响自动驾驶车辆决策安全，并生成多种极端案例。

Conclusion: 该方法为安全关键场景研究提供了新方向。

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [172] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的替代模型，用于加速HEC-RAS洪水模拟，结合GRU和Geo-FNO，显著提升计算速度且保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统物理求解器（如HEC-RAS）计算成本高，难以实时决策，需在不牺牲精度的情况下加速模拟。

Method: 采用混合自回归架构，结合GRU捕捉短期时间动态和Geo-FNO建模长程空间依赖，通过八通道特征向量从HEC-RAS文件中学习物理规律。

Result: 在密西西比河流域67个河段上验证，模型预测精度高（中位绝对水位误差0.31英尺），计算速度提升3.5倍（从139分钟降至40分钟）。

Conclusion: 数据驱动方法通过特征工程可替代传统水力模型，提升大规模洪水预报的计算可行性。

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [173] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的端到端框架Data Mixing Agent，用于自动调整源领域和目标领域数据的权重，以避免大语言模型在持续预训练中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在任务特定数据上持续预训练时可能出现的灾难性遗忘问题，传统方法依赖人工启发式调整数据权重，缺乏通用性。

Method: 提出Data Mixing Agent，通过强化学习从大量数据混合轨迹中学习通用启发式，自动调整源和目标领域的数据权重。

Result: 实验表明，该方法在数学推理任务中优于基线，且能泛化到未见过的源领域、目标模型和领域空间。在代码生成任务中也表现良好。

Conclusion: Data Mixing Agent能够高效学习与人类直觉一致的启发式，减少源领域数据需求，提升模型性能。

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [174] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 论文提出了一种可解释的异常检测框架，用于共享单车系统，结合多源数据和Isolation Forest算法，分析外部因素对异常的影响。


<details>
  <summary>Details</summary>
Motivation: 共享出行系统（如共享单车）对城市交通至关重要，识别异常有助于优化运营、提高服务可靠性和用户体验。

Method: 采用Isolation Forest算法进行无监督异常检测，结合DIFFI算法提供可解释性，整合共享单车行程记录、天气条件和公共交通可用性等多源数据。

Result: 站点级分析能有效识别异常，外部因素（如恶劣天气和公共交通不足）对异常有显著影响。

Conclusion: 研究成果有助于提升共享出行系统的决策能力。

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [175] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 本文研究电动汽车充电站异常检测，结合可解释AI技术提升异常原因分析。


<details>
  <summary>Details</summary>
Motivation: 支持可再生能源交通转型需确保充电站可靠性，需有效检测异常并分析原因。

Method: 采用无监督异常检测技术（Isolation Forest）和DIFFI方法识别异常特征。

Result: 在真实工业案例中验证了方法的有效性。

Conclusion: 结合可解释AI技术能有效检测异常并揭示其根本原因。

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [176] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: 论文提出了一种多代理滑雪租赁问题，扩展了经典滑雪租赁困境，研究了代理在个体和共享成本下的决策策略，并分析了三种竞争比率下的最优策略。


<details>
  <summary>Details</summary>
Motivation: 研究多代理环境下的滑雪租赁问题，探索代理在动态状态和不同成本选项下的最优决策策略。

Method: 定义了三种竞争比率（整体、状态依赖和个体理性），设计了确定性和随机化策略，并分析了对称与非对称策略的性能。

Result: 对称策略优于非对称策略，提供了竞争比率的上限和下限，并将经典滑雪租赁问题的结论扩展到多代理场景。

Conclusion: 研究为不确定性下的群体决策提供了理论和实践启示，扩展了经典滑雪租赁问题的应用范围。

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [177] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: 论文提出了一种基于多模态感知的毫米波信号遮挡预测框架，结合摄像头、GPS、LiDAR和雷达数据，通过深度学习模型和软加权融合策略，实现了高精度（97.2% F1）和低延迟（95.7ms）的预测。


<details>
  <summary>Details</summary>
Motivation: 毫米波频段的车载通信系统易受动态障碍物（如车辆、行人）的信号遮挡，需解决这一问题以提高通信可靠性。

Method: 采用多模态感知（摄像头、GPS、LiDAR、雷达），通过独立处理各传感器数据并基于验证性能的软加权融合策略进行预测。

Result: 摄像头单独模型表现最佳（F1 97.1%，延迟89.8ms），摄像头+雷达组合进一步提升至F1 97.2%，延迟95.7ms。

Conclusion: 多模态感知在毫米波遮挡预测中高效且有效，为动态环境中的主动无线通信提供了可行方案。

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [178] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: DIVA是一种基于变分自编码器的自动化工作流，用于通过拉曼光谱检测植物应激，无需手动预处理。


<details>
  <summary>Details</summary>
Motivation: 植物应激检测对农业至关重要，传统拉曼分析方法存在偏见和不一致问题。

Method: DIVA利用变分自编码器处理原始拉曼光谱（包括荧光背景），自动识别和量化光谱特征。

Result: DIVA成功检测了多种植物应激（如光照、温度变化和细菌感染）。

Conclusion: DIVA结合深度学习与振动光谱，为AI驱动的植物健康评估提供了新途径。

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [179] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: 论文提出了一种名为PRO-DYN的框架，用于分析时间序列预测模型中动态学习的重要性，并验证了动态学习块的位置对模型性能的关键影响。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在时间序列预测任务中表现不佳，作者假设这是因为模型未能充分学习数据的底层动态特性。

Method: 通过系统和实证研究，提出PRO-DYN框架分析现有模型的动态学习能力，并设计实验验证动态学习块的位置对性能的影响。

Result: 实验表明，性能较差的模型仅部分学习动态特性，且动态学习块作为最终预测器时效果最佳。

Conclusion: 研究证实了动态学习块的重要性，并建议将其作为模型的最终预测模块。

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [180] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wasserstein-Rubinstein距离的专家融合模型（WR-EFM），用于解决图节点分类中类别间性能差异问题，显著提升了类别2的准确率。


<details>
  <summary>Details</summary>
Motivation: 在PubMed引文网络数据集中，传统GCN模型在类别2上的分类准确率仅为74.4%，比类别1低7.5%，亟需解决类别间性能不平衡问题。

Method: WR-EFM结合了针对类别0/1的GNN模型（带层归一化和残差连接）和针对类别2的多跳图注意力网络（GAT），并通过WR距离优化模型间表示相似性。自适应融合策略动态加权模型。

Result: WR-EFM在类别0、1、2上的准确率分别为77.8%、78.0%和79.9%，优于单一模型和标准融合方法，类别2准确率提升5.5%。

Conclusion: WR-EFM为处理类别不平衡的图分类任务提供了新范式，并通过WR距离实现了更稳定的性能表现。

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [181] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: 研究探讨小规模LLMs是否能通过RLVR获得通用的ToM能力，发现其仅能提升训练数据的表现，但无法泛化到新任务。


<details>
  <summary>Details</summary>
Motivation: 探索RL方法是否能为LLMs赋予更细腻的社会智能（如ToM）。

Method: 使用RLVR训练小规模LLMs，并在多个ToM数据集上评估其泛化能力。

Result: 小规模LLMs无法获得通用ToM能力，RL训练导致过拟合训练数据。

Conclusion: RLVR未能实现抽象的ToM能力，仅表现为狭窄的过拟合行为。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [182] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: 提出了一种结合CNN-LSTM-注意力-Adaboost的混合神经网络模型，通过改进的蛇群优化算法优化超参数，显著提升了4D轨迹预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决中长期4D轨迹预测模型的局限性，提高对大规模高维轨迹数据的处理能力。

Method: 采用Adaboost算法划分多个弱学习器，每个子模型结合CNN、LSTM和注意力机制提取特征，并通过改进的蛇群优化算法优化超参数。

Result: 实验表明，该模型在处理大规模高维数据时优于传统优化器，预测精度提升了39.89%。

Conclusion: 提出的混合模型和优化算法显著提升了4D轨迹预测的性能，具有实际应用潜力。

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [183] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 本文提出了一种优化差分隐私学习算法隐私参数审计的方法，通过优化审计者的“金丝雀”数据集，显著提高了隐私参数的下界估计效果。


<details>
  <summary>Details</summary>
Motivation: 差分隐私学习算法的隐私参数审计通常依赖于固定的“金丝雀”数据集，但其效果有限。本文旨在通过优化这些数据集，提升审计的准确性和效率。

Method: 利用元梯度优化技术优化“金丝雀”数据集，使其在审计差分隐私学习算法时更有效。实验基于DP-SGD算法进行验证。

Result: 实验表明，优化后的“金丝雀”数据集可将隐私参数的下界估计提升超过2倍，且该方法具有可迁移性和高效性。

Conclusion: 通过优化“金丝雀”数据集，本文方法显著提升了差分隐私学习算法的隐私审计效果，为实际应用提供了更可靠的隐私保障。

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


### [184] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: 提出了一种利用LLMs快速生成高质量表格数据的方法，通过编码字段分布为可重用脚本，显著降低时间和成本。


<details>
  <summary>Details</summary>
Motivation: 解决直接使用LLMs生成大量数据时的时间和成本问题。

Method: 利用LLMs推断字段分布并编码为采样脚本，自动分类字段类型以高效生成数据。

Result: 方法在多样性和数据真实性上优于传统方法，大幅减少生成负担。

Conclusion: 该方法可加速生产流程测试，缩短开发周期，为合成数据生成提供高效解决方案。

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>


### [185] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 论文提出GUI-G²奖励框架，通过高斯分布建模GUI元素的空间交互，显著提升GUI定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法使用二元奖励，忽略了空间交互的连续性，而人类点击行为自然形成高斯分布。

Method: 引入GUI-G²框架，结合高斯点奖励和覆盖奖励，并开发自适应方差机制处理不同元素尺度。

Result: 在多个基准测试中，GUI-G²显著优于现有方法，最高提升24.7%。

Conclusion: 连续建模提供了更强的鲁棒性和泛化能力，为GUI交互任务建立了新范式。

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [186] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: 扩散模型在数据受限情况下优于自回归模型，尤其在计算资源充足时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在数据稀缺环境下相对于自回归模型的优势。

Method: 系统研究掩码扩散模型在数据受限设置中的表现，并与自回归模型对比。

Result: 扩散模型在数据稀缺时表现更优，验证损失更低，下游任务性能更好。

Conclusion: 当数据成为瓶颈时，扩散模型是自回归模型的有力替代方案。

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [187] [Statistical and Algorithmic Foundations of Reinforcement Learning](https://arxiv.org/abs/2507.14444)
*Yuejie Chi,Yuxin Chen,Yuting Wei*

Main category: stat.ML

TL;DR: 该教程介绍了强化学习（RL）在样本稀缺情况下的挑战，并探讨了提高样本和计算效率的方法，涵盖了多种RL场景和主流方法。


<details>
  <summary>Details</summary>
Motivation: 在数据收集昂贵或高风险的应用中，如何提高RL的样本和计算效率是一个重要问题。

Method: 使用马尔可夫决策过程（MDP）作为核心模型，涵盖多种RL场景（如模拟器RL、在线RL、离线RL等）和主流方法（如基于模型、基于价值和策略优化）。

Result: 讨论了样本复杂度、计算效率以及非渐近视角下的算法依赖和信息论下界。

Conclusion: 该教程为理解和提升RL算法的效率提供了重要的理论和算法发展视角。

Abstract: As a paradigm for sequential decision making in unknown environments,
reinforcement learning (RL) has received a flurry of attention in recent years.
However, the explosion of model complexity in emerging applications and the
presence of nonconvexity exacerbate the challenge of achieving efficient RL in
sample-starved situations, where data collection is expensive, time-consuming,
or even high-stakes (e.g., in clinical trials, autonomous systems, and online
advertising). How to understand and enhance the sample and computational
efficacies of RL algorithms is thus of great interest. In this tutorial, we aim
to introduce several important algorithmic and theoretical developments in RL,
highlighting the connections between new ideas and classical topics. Employing
Markov Decision Processes as the central mathematical model, we cover several
distinctive RL scenarios (i.e., RL with a simulator, online RL, offline RL,
robust RL, and RL with human feedback), and present several mainstream RL
approaches (i.e., model-based approach, value-based approach, and policy
optimization). Our discussions gravitate around the issues of sample
complexity, computational efficiency, as well as algorithm-dependent and
information-theoretic lower bounds from a non-asymptotic viewpoint.

</details>


### [188] [Diffusion Models for Time Series Forecasting: A Survey](https://arxiv.org/abs/2507.14507)
*Chen Su,Zhengzhou Cai,Yuanhe Tian,Zihong Zheng,Yan Song*

Main category: stat.ML

TL;DR: 本文综述了扩散模型在时间序列预测（TSF）中的应用，包括模型变体、条件信息整合机制、现有方法的分类总结，以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成中表现出色，但其在时间序列预测中的应用尚未系统总结，本文旨在填补这一空白。

Method: 介绍标准扩散模型及其变体，分析其在TSF中的适应方法，并系统分类现有方法。

Result: 综述了扩散模型在TSF中的进展，包括数据集、评估指标及模型表现。

Conclusion: 扩散模型在TSF中具有潜力，但仍存在局限性，未来研究需进一步探索。

Abstract: Diffusion models, initially developed for image synthesis, demonstrate
remarkable generative capabilities. Recently, their application has expanded to
time series forecasting (TSF), yielding promising results. In this survey, we
firstly introduce the standard diffusion models and their prevalent variants,
explaining their adaptation to TSF tasks. We then provide a comprehensive
review of diffusion models for TSF, paying special attention to the sources of
conditional information and the mechanisms for integrating this conditioning
within the models. In analyzing existing approaches using diffusion models for
TSF, we provide a systematic categorization and a comprehensive summary of them
in this survey. Furthermore, we examine several foundational diffusion models
applied to TSF, alongside commonly used datasets and evaluation metrics.
Finally, we discuss current limitations in these approaches and potential
future research directions. Overall, this survey details recent progress and
future prospects for diffusion models in TSF, serving as a reference for
researchers in the field.

</details>


### [189] [Deep Learning-Based Survival Analysis with Copula-Based Activation Functions for Multivariate Response Prediction](https://arxiv.org/abs/2507.14641)
*Jong-Min Kim,Il Do Ha,Sangjin Kim*

Main category: stat.ML

TL;DR: 该研究结合深度学习、Copula函数和生存分析，处理高度相关且右删失的多变量生存数据，提出基于Copula的激活函数，提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决高度相关且右删失的多变量生存数据的建模问题，捕捉非线性依赖关系。

Method: 提出CNN-LSTM模型，结合Clayton、Gumbel等Copula激活函数，通过仿真和乳腺癌数据分析验证。

Result: 模型显著提升预测准确性，通过Shewhart控制图和平均运行长度（ARL）评估性能。

Conclusion: 基于Copula的激活函数能有效处理复杂生存数据，为多类型生存响应提供新方法。

Abstract: This research integrates deep learning, copula functions, and survival
analysis to effectively handle highly correlated and right-censored
multivariate survival data. It introduces copula-based activation functions
(Clayton, Gumbel, and their combinations) to model the nonlinear dependencies
inherent in such data. Through simulation studies and analysis of real breast
cancer data, our proposed CNN-LSTM with copula-based activation functions for
multivariate multi-types of survival responses enhances prediction accuracy by
explicitly addressing right-censored data and capturing complex patterns. The
model's performance is evaluated using Shewhart control charts, focusing on the
average run length (ARL).

</details>


### [190] [Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators](https://arxiv.org/abs/2507.14652)
*Ponkrshnan Thiagarajan,Tamer A. Zaki,Michael D. Shields*

Main category: stat.ML

TL;DR: 提出了一种结合变分推理（VI）和哈密顿蒙特卡洛（HMC）的混合方法，用于高效准确地量化神经网络的不确定性。


<details>
  <summary>Details</summary>
Motivation: HMC在贝叶斯神经网络中计算成本高，而现有近似方法（如VI）会引入不准确性。

Method: 先使用VI训练网络，识别对预测不确定性影响大的参数子集，再对这部分参数应用HMC。

Result: 该方法显著降低了参数空间维度，适用于大规模网络，并在复杂物理系统中验证了有效性。

Conclusion: 混合方法在保持准确性的同时提高了计算效率，适用于高维参数空间。

Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample
from the posterior distribution in Bayesian inference. However, HMC techniques
are computationally demanding for Bayesian neural networks due to the high
dimensionality of the network's parameter space and the non-convexity of their
posterior distributions. Therefore, various approximation techniques, such as
variational inference (VI) or stochastic gradient MCMC, are often employed to
infer the posterior distribution of the network parameters. Such approximations
introduce inaccuracies in the inferred distributions, resulting in unreliable
uncertainty estimates. In this work, we propose a hybrid approach that combines
inexpensive VI and accurate HMC methods to efficiently and accurately quantify
uncertainties in neural networks and neural operators. The proposed approach
leverages an initial VI training on the full network. We examine the influence
of individual parameters on the prediction uncertainty, which shows that a
large proportion of the parameters do not contribute substantially to
uncertainty in the network predictions. This information is then used to
significantly reduce the dimension of the parameter space, and HMC is performed
only for the subset of network parameters that strongly influence prediction
uncertainties. This yields a framework for accelerating the full batch HMC for
posterior inference in neural networks. We demonstrate the efficiency and
accuracy of the proposed framework on deep neural networks and operator
networks, showing that inference can be performed for large networks with tens
to hundreds of thousands of parameters. We show that this method can
effectively learn surrogates for complex physical systems by modeling the
operator that maps from upstream conditions to wall-pressure data on a cone in
hypersonic flow.

</details>


### [191] [When few labeled target data suffice: a theory of semi-supervised domain adaptation via fine-tuning from multiple adaptive starts](https://arxiv.org/abs/2507.14661)
*Wooseok Ha,Yuansi Chen*

Main category: stat.ML

TL;DR: 论文提出了一种基于结构因果模型（SCM）的理论框架，用于分析半监督域适应（SSDA）方法的性能，并提出了三种针对不同源-目标关系假设的SSDA方法。其中，MASFT算法通过多起点微调和模型选择，显著减少对目标域标记数据的需求。


<details>
  <summary>Details</summary>
Motivation: 半监督域适应（SSDA）在实际应用中具有重要意义，但其理论有效性尚未充分探索，尤其是在涉及多种源-目标分布偏移的情况下。

Method: 基于结构因果模型（SCM）构建理论框架，提出三种SSDA方法，并设计MASFT算法，通过多起点微调和模型选择优化性能。

Result: 在有限目标标记数据下，所提方法能实现最小最大最优目标性能，并通过实验验证了其有效性。

Conclusion: 论文为SSDA提供了理论支持，提出的方法在多种分布偏移下表现优异，显著减少了对目标域标记数据的依赖。

Abstract: Semi-supervised domain adaptation (SSDA) aims to achieve high predictive
performance in the target domain with limited labeled target data by exploiting
abundant source and unlabeled target data. Despite its significance in numerous
applications, theory on the effectiveness of SSDA remains largely unexplored,
particularly in scenarios involving various types of source-target
distributional shifts. In this work, we develop a theoretical framework based
on structural causal models (SCMs) which allows us to analyze and quantify the
performance of SSDA methods when labeled target data is limited. Within this
framework, we introduce three SSDA methods, each having a fine-tuning strategy
tailored to a distinct assumption about the source and target relationship.
Under each assumption, we demonstrate how extending an unsupervised domain
adaptation (UDA) method to SSDA can achieve minimax-optimal target performance
with limited target labels. When the relationship between source and target
data is only vaguely known -- a common practical concern -- we propose the
Multi Adaptive-Start Fine-Tuning (MASFT) algorithm, which fine-tunes UDA models
from multiple starting points and selects the best-performing one based on a
small hold-out target validation dataset. Combined with model selection
guarantees, MASFT achieves near-optimal target predictive performance across a
broad range of types of distributional shifts while significantly reducing the
need for labeled target data. We empirically validate the effectiveness of our
proposed methods through simulations.

</details>


### [192] [Uncertainty Quantification for Machine Learning-Based Prediction: A Polynomial Chaos Expansion Approach for Joint Model and Input Uncertainty Propagation](https://arxiv.org/abs/2507.14782)
*Xiaoping Du*

Main category: stat.ML

TL;DR: 该论文提出了一种基于多项式混沌展开（PCE）的框架，用于处理机器学习和输入不确定性传播，特别关注高斯过程回归模型，以实现高效、可靠的工程预测。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理模型在工程分析中广泛应用，但其预测存在误差，且与输入变量的不确定性耦合。准确量化这些不确定性对生成可靠的工程预测至关重要。

Method: 通过将随机输入转换为统一标准空间，构建PCE代理模型，高效计算输出的均值和标准差，并提供全局敏感性分析机制。

Result: 该方法实现了对输入变量和模型不确定性对输出变异性贡献的准确量化，为下游工程应用提供了可信赖的预测框架。

Conclusion: 提出的PCE框架为综合不确定性量化提供了计算高效且可解释的方法，支持工程应用中的可靠机器学习预测。

Abstract: Machine learning (ML) surrogate models are increasingly used in engineering
analysis and design to replace computationally expensive simulation models,
significantly reducing computational cost and accelerating decision-making
processes. However, ML predictions contain inherent errors, often estimated as
model uncertainty, which is coupled with variability in model inputs.
Accurately quantifying and propagating these combined uncertainties is
essential for generating reliable engineering predictions. This paper presents
a robust framework based on Polynomial Chaos Expansion (PCE) to handle joint
input and model uncertainty propagation. While the approach applies broadly to
general ML surrogates, we focus on Gaussian Process regression models, which
provide explicit predictive distributions for model uncertainty. By
transforming all random inputs into a unified standard space, a PCE surrogate
model is constructed, allowing efficient and accurate calculation of the mean
and standard deviation of the output. The proposed methodology also offers a
mechanism for global sensitivity analysis, enabling the accurate quantification
of the individual contributions of input variables and ML model uncertainty to
the overall output variability. This approach provides a computationally
efficient and interpretable framework for comprehensive uncertainty
quantification, supporting trustworthy ML predictions in downstream engineering
applications.

</details>


### [193] [Learning Nonlinear Causal Reductions to Explain Reinforcement Learning Policies](https://arxiv.org/abs/2507.14901)
*Armin Kekić,Jan Schneider,Dieter Büchler,Bernhard Schölkopf,Michel Besserve*

Main category: stat.ML

TL;DR: 论文提出了一种从因果视角解释RL策略行为的方法，通过扰动动作学习高层因果模型，确保干预一致性。


<details>
  <summary>Details</summary>
Motivation: 解释RL策略成功或失败的原因，因其复杂高维特性而具有挑战性。

Method: 引入动作随机扰动，观察其对累积奖励的影响，学习高层因果模型，并提出非线性因果模型简化框架。

Result: 实验证明方法能揭示RL策略的行为模式、偏见和失败原因。

Conclusion: 通过因果模型简化框架，能够有效解释RL策略的行为，确保干预一致性。

Abstract: Why do reinforcement learning (RL) policies fail or succeed? This is a
challenging question due to the complex, high-dimensional nature of
agent-environment interactions. In this work, we take a causal perspective on
explaining the behavior of RL policies by viewing the states, actions, and
rewards as variables in a low-level causal model. We introduce random
perturbations to policy actions during execution and observe their effects on
the cumulative reward, learning a simplified high-level causal model that
explains these relationships. To this end, we develop a nonlinear Causal Model
Reduction framework that ensures approximate interventional consistency,
meaning the simplified high-level model responds to interventions in a similar
way as the original complex system. We prove that for a class of nonlinear
causal models, there exists a unique solution that achieves exact
interventional consistency, ensuring learned explanations reflect meaningful
causal patterns. Experiments on both synthetic causal models and practical RL
tasks-including pendulum control and robot table tennis-demonstrate that our
approach can uncover important behavioral patterns, biases, and failure modes
in trained RL policies.

</details>


### [194] [Learning under Latent Group Sparsity via Diffusion on Networks](https://arxiv.org/abs/2507.15097)
*Subhroshekhar Ghosh,Soumendu Sundar Mukherjee*

Main category: stat.ML

TL;DR: 提出一种无需先验分组信息的稀疏学习方法，通过基于热流的网络动态惩罚，自动在lasso和group lasso之间插值。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中变量分组结构的普遍问题，无需依赖先验分组信息。

Method: 利用网络拉普拉斯几何和热流动态构建惩罚项，自动适应分组强度。

Result: 理论保证性能，样本复杂度低，扩散时间仅需对数级。

Conclusion: 方法结合几何、动态和随机结构，可推广至其他学习任务。

Abstract: Group or cluster structure on explanatory variables in machine learning
problems is a very general phenomenon, which has attracted broad interest from
practitioners and theoreticians alike. In this work we contribute an approach
to sparse learning under such group structure, that does not require prior
information on the group identities. Our paradigm is motivated by the Laplacian
geometry of an underlying network with a related community structure, and
proceeds by directly incorporating this into a penalty that is effectively
computed via a heat-flow-based local network dynamics. The proposed penalty
interpolates between the lasso and the group lasso penalties, the runtime of
the heat-flow dynamics being the interpolating parameter. As such it can
automatically default to lasso when the group structure reflected in the
Laplacian is weak. In fact, we demonstrate a data-driven procedure to construct
such a network based on the available data. Notably, we dispense with
computationally intensive pre-processing involving clustering of variables,
spectral or otherwise. Our technique is underpinned by rigorous theorems that
guarantee its effective performance and provide bounds on its sample
complexity. In particular, in a wide range of settings, it provably suffices to
run the diffusion for time that is only logarithmic in the problem dimensions.
We explore in detail the interfaces of our approach with key statistical
physics models in network science, such as the Gaussian Free Field and the
Stochastic Block Model. Our work raises the possibility of applying similar
diffusion-based techniques to classical learning tasks, exploiting the
interplay between geometric, dynamical and stochastic structures underlying the
data.

</details>


### [195] [Accelerated Bayesian Optimal Experimental Design via Conditional Density Estimation and Informative Data](https://arxiv.org/abs/2507.15235)
*Miao Huang,Hongqiao Wang,Kunyu Wu*

Main category: stat.ML

TL;DR: 该研究提出了一种基于贝叶斯框架的实验设计优化方法，通过改进效用期望的计算形式和使用条件密度估计，显著提高了数值效率。


<details>
  <summary>Details</summary>
Motivation: 在实验效率低和数据获取成本高的场景下，传统实验设计方法面临挑战，需要更高效和可靠的方法。

Method: 利用贝叶斯定理重构效用期望的计算形式，采用条件密度估计近似高斯随机场比率，并以协方差作为选择标准。

Result: 理论分析和实际应用验证了该方法在提高实验效率和不确定性决策中的有效性。

Conclusion: 该贝叶斯实验设计框架为实验优化和决策提供了高效且可靠的工具。

Abstract: The Design of Experiments (DOEs) is a fundamental scientific methodology that
provides researchers with systematic principles and techniques to enhance the
validity, reliability, and efficiency of experimental outcomes. In this study,
we explore optimal experimental design within a Bayesian framework, utilizing
Bayes' theorem to reformulate the utility expectation--originally expressed as
a nested double integral--into an independent double integral form,
significantly improving numerical efficiency. To further accelerate the
computation of the proposed utility expectation, conditional density estimation
is employed to approximate the ratio of two Gaussian random fields, while
covariance serves as a selection criterion to identify informative datasets
during model fitting and integral evaluation. In scenarios characterized by low
simulation efficiency and high costs of raw data acquisition, key challenges
such as surrogate modeling, failure probability estimation, and parameter
inference are systematically restructured within the Bayesian experimental
design framework. The effectiveness of the proposed methodology is validated
through both theoretical analysis and practical applications, demonstrating its
potential for enhancing experimental efficiency and decision-making under
uncertainty.

</details>


### [196] [Missing value imputation with adversarial random forests -- MissARF](https://arxiv.org/abs/2507.15681)
*Pegah Golchian,Jan Kapar,David S. Watson,Marvin N. Wright*

Main category: stat.ML

TL;DR: 提出了一种基于生成式机器学习的快速、易用的缺失值填补方法MissARF，结合对抗随机森林（ARF）进行密度估计和数据合成，实验表明其性能与现有最优方法相当。


<details>
  <summary>Details</summary>
Motivation: 生物统计分析中缺失值处理是常见挑战，传统填补方法存在不足，需提出更高效、灵活的解决方案。

Method: 使用对抗随机森林（ARF）进行密度估计和数据合成，基于非缺失值条件采样填补缺失值。

Result: MissARF在填补质量和运行速度上与现有最优方法相当，且多填补无额外成本。

Conclusion: MissARF是一种高效、灵活的缺失值填补方法，适用于单填补和多填补场景。

Abstract: Handling missing values is a common challenge in biostatistical analyses,
typically addressed by imputation methods. We propose a novel, fast, and
easy-to-use imputation method called missing value imputation with adversarial
random forests (MissARF), based on generative machine learning, that provides
both single and multiple imputation. MissARF employs adversarial random forest
(ARF) for density estimation and data synthesis. To impute a missing value of
an observation, we condition on the non-missing values and sample from the
estimated conditional distribution generated by ARF. Our experiments
demonstrate that MissARF performs comparably to state-of-the-art single and
multiple imputation methods in terms of imputation quality and fast runtime
with no additional costs for multiple imputation.

</details>


### [197] [Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces](https://arxiv.org/abs/2507.15741)
*Gábor Lugosi,Marcos Matabuena*

Main category: stat.ML

TL;DR: 提出了一种在度量空间中回归模型的不确定性量化框架，包括同方差和异方差情况下的方法，具有实用性和理论保证。


<details>
  <summary>Details</summary>
Motivation: 解决度量空间中回归模型的不确定性量化问题，为个性化医疗等应用提供理论支持。

Method: 提出同方差情况下的保形预测算法和异方差情况下的局部k近邻方法，无需保形校准。

Result: 证明了估计器的一致性，并在个性化医疗应用中验证了实用性。

Conclusion: 框架灵活、可扩展，适用于多种回归算法和领域知识。

Abstract: This paper introduces a framework for uncertainty quantification in
regression models defined in metric spaces. Leveraging a newly defined notion
of homoscedasticity, we develop a conformal prediction algorithm that offers
finite-sample coverage guarantees and fast convergence rates of the oracle
estimator. In heteroscedastic settings, we forgo these non-asymptotic
guarantees to gain statistical efficiency, proposing a local
$k$--nearest--neighbor method without conformal calibration that is adaptive to
the geometry of each particular nonlinear space. Both procedures work with any
regression algorithm and are scalable to large data sets, allowing
practitioners to plug in their preferred models and incorporate domain
expertise. We prove consistency for the proposed estimators under minimal
conditions. Finally, we demonstrate the practical utility of our approach in
personalized--medicine applications involving random response objects such as
probability distributions and graph Laplacians.

</details>


### [198] [Hypergraphs on high dimensional time series sets using signature transform](https://arxiv.org/abs/2507.15802)
*Rémi Vaucher,Paul Minchella*

Main category: stat.ML

TL;DR: 本文提出了一种从多元时间序列集合构建超图的方法，通过签名变换引入随机性，提高了构建过程的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决从多元时间序列集合构建超图的挑战，扩展了单时间序列的现有框架。

Method: 利用签名变换的特性引入可控随机性，扩展了Chretien等人的方法。

Result: 在合成数据集上验证了方法的有效性，展示了有前景的结果。

Conclusion: 该方法为多元时间序列集合的超图构建提供了鲁棒且通用的解决方案。

Abstract: In recent decades, hypergraphs and their analysis through Topological Data
Analysis (TDA) have emerged as powerful tools for understanding complex data
structures. Various methods have been developed to construct hypergraphs --
referred to as simplicial complexes in the TDA framework -- over datasets,
enabling the formation of edges between more than two vertices. This paper
addresses the challenge of constructing hypergraphs from collections of
multivariate time series. While prior work has focused on the case of a single
multivariate time series, we extend this framework to handle collections of
such time series. Our approach generalizes the method proposed in Chretien and
al. by leveraging the properties of signature transforms to introduce
controlled randomness, thereby enhancing the robustness of the construction
process. We validate our method on synthetic datasets and present promising
results.

</details>
