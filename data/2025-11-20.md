<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 25]
- [cs.LG](#cs.LG) [Total: 56]
- [stat.ML](#stat.ML) [Total: 13]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Ein Fenster zur gleichzeitigen Messung der Uebertragungsfunktion eines realen Systems und des Leistungsdichtespektrums des ueberlagerten Rauschens am Systemausgang (Teil 1)](https://arxiv.org/abs/2511.14890)
*Helmut Repp*

Main category: eess.SP

TL;DR: 提出了一种改进的测量方法，通过使用合适的窗函数显著提高噪声功率谱密度测量的频率选择性，同时保持计算复杂度基本不变。


<details>
  <summary>Details</summary>
Motivation: 现有的方法可以同时测量系统的传递函数和叠加噪声的功率谱密度，但频率选择性有待提高。

Method: 使用合适的窗函数来改进功率谱密度测量的频率选择性，证明该方法具有无偏性和一致性，并推导窗函数应满足的要求及其高精度数值计算方法。

Result: 窗函数的使用显著提高了噪声功率谱密度测量的频率选择性，同时计算复杂度没有显著增加。

Conclusion: 该方法为测量非线性时不变系统的传递函数和噪声功率谱密度提供了一种高效且精确的解决方案，特别适用于实值和复值信号的处理。

Abstract: There is already a method known from the literature with which it is possible to measure both the transfer function and the noise power spectral density of the superimposed noise at the output of a disturbed, time-invariant, real system with nonlinearities in one measurement. By using a suitable window, the frequency selectivity of the measurement of the power spectral density of the superimposed noise can be noticeably improved without significantly increasing the computational complexity of the method. The unbiasedness and consistency of the measurement method with a suitable window are proven. The measurement of the power spectral density of a zero-mean, stationary process without measuring the transfer function is investigated as a special case for both real-valued and complex-valued signals. It is derived which requirements a suitable window should meet and how it can be calculated with high numerical accuracy.
  --
  Es gibt bereits eine aus der Literatur bekannte Methode mit der es moeglich ist in einer Messung sowohl die Uebertragungsfunktion als auch das Rauschleistungsdichtespektrum des ueberlagerten Rauschens am Ausgang eines gestoerten, zeitinvarianten, realen Systems mit Nichtlinearitaeten zu messen. Durch den Einsatz eines geeigneten Fensters kann die Frequenzselektivitaet der Messung des Leistungsdichtespektrums der Stoerung deutlich verbessert werden, ohne den Aufwand der Berechnungen des Verfahrens nennenswert zu erhoehen. Die Erwartungstreue und die Konsistenz des Messverfahren mit Fensterung wird gezeigt. Die Messung des Leistungsdichtespektrums eines mittelwertfreien, stationaeren Prozesses ohne Messung der Uebertragungsfunktion wird als Sonderfall sowohl fuer reellwertige, als auch fuer komplexwertige Signale untersucht. Es wird hergeleitet, welche Forderungen ein geeignetes Fenster erfuellen sollte und wie es sich numerisch hochgenau berechnen laesst.

</details>


### [2] [Ein Fenster zur gleichzeitigen Messung der Uebertragungsfunktion eines realen Systems und des Leistungsdichtespektrums des ueberlagerten Rauschens am Systemausgang (Teil 2)](https://arxiv.org/abs/2511.14957)
*Helmut Repp*

Main category: eess.SP

TL;DR: 扩展了第一部分中基于窗函数的频率选择性测量方法，使其适用于周期性时变系统和循环平稳噪声过程，能够测量输入输出信号之间的所有相关性。


<details>
  <summary>Details</summary>
Motivation: 原有的测量方法仅限于时不变系统和平稳零均值过程，无法处理周期性时变系统和循环平稳噪声过程，需要扩展方法来测量这些更复杂系统中的所有输入输出相关性。

Method: 提出了窗口构造算法的扩展版本，利用之前未使用的自由度来根据应用需求适当影响窗口序列的特性，从而实现对周期性时变系统和循环平稳噪声过程的测量。

Result: 开发了一种扩展的窗口构造算法，能够处理周期性时变系统在循环平稳噪声干扰下的测量问题，可以测量输入输出信号之间的所有相关性。

Conclusion: 成功扩展了原有的频率选择性测量方法，使其适用于更广泛的系统类型，包括周期性时变系统和循环平稳噪声过程，为复杂系统的测量提供了有效工具。

Abstract: The method described in the first part for frequency-selectively measuring the transfer function and the noise power spectral density of the superimposed noise at the output of a disturbed, real system with nonlinearities using windowing was limited to time-invariant systems with stationary and zero-mean processes. Here, we investigate how this measurement method can be extended so that all correlations existing between the input and output signals can also be measured using windowing for a periodically time-varying system disturbed by a cyclostationary noise process. An extended version of the window construction algorithm presented in the first part is introduced, in which some degrees of freedom not used there can be used to appropriately influence the properties of the window sequence depending on the application.
  --
  Das im ersten Teil beschriebene Verfahren die Uebertragungsfunktion und das Rauschleistungsdichtespektrum des ueberlagerten Rauschens am Ausgang eines gestoerten, realen Systems mit Nichtlinearitaeten mit Hilfe der Fensterung frequenzselektiv zu vermessen beschraenkte sich auf zeitinvariante Systeme mit stationaeren und mittelwertfreien Prozessen. Hier wird untersucht, wie dieses Messverfahren zu erweitern ist, so dass man damit auch alle Korrelationen, die zwischen Ein- und Ausgangssignal bestehen, bei einem periodisch zeitvarianten System, das von einem zyklostationaeren Rauschprozess gestoert wird, mit einer Fensterung messen kann. Es wird eine erweiterte Variante des im ersten Teil vorgestellten Algorithmus zur Konstruktion des Fensters angegeben, bei der einige dort nicht genutzte Freiheitsgrade dazu verwendet werden koennen, die Eigenschaften der Fensterfolge je nach Applikation geeignet zu beeinflussen.

</details>


### [3] [Lightweight Foundation Model for Wireless Time Series Downstream Tasks on Edge Devices](https://arxiv.org/abs/2511.14895)
*Mohammad Cheraghinia,Eli De Poorter,Jaron Fontaine,Kwang Soon Kim,Merouane Debbah,Adnan Shahid*

Main category: eess.SP

TL;DR: 提出基于MLP的轻量级基础模型，替代计算密集的Transformer，支持多种无线通信任务，仅需21K参数，在边缘设备上实现0.33ms推理时间。


<details>
  <summary>Details</summary>
Motivation: 传统方法为每个通信和定位任务单独训练模型成本高昂，现有基础模型基于Transformer计算密集，不适合边缘设备部署。

Method: 使用简单的多层感知机编码器独立处理输入补丁，支持4种下游任务（长/短距离技术识别、调制识别、视距检测），处理多种输入类型和采样率。

Result: 模型保持稳健泛化性能，对未见数据类实现超过97%的准确率，Transformer在添加下游任务时可能出现性能下降。

Conclusion: MLP基础模型在保持高性能的同时大幅减少计算需求，适合资源受限的实时部署场景。

Abstract: While machine learning is widely used to optimize wireless networks, training a separate model for each task in communication and localization is becoming increasingly unsustainable due to the significant costs associated with training and deployment. Foundation models offer a more scalable alternative by enabling a single model to be adapted across multiple tasks through fine-tuning with limited samples. However, current foundation models mostly rely on large-scale Transformer architectures, resulting in computationally intensive models unsuitable for deployment on typical edge devices. This paper presents a lightweight foundation model based on simple Multi-Layer-Perceptron (MLP) encoders that independently process input patches. Our model supports 4 types of downstream tasks (long-range technology recognition, short-range technology recognition, modulation recognition and line-of-sight-detection) from multiple input types (IQ and CIR) and different sampling rates. We show that, unlike Transformers, which can exhibit performance drops as downstream tasks are added, our MLP model maintains robust generalization performance, achieving over 97% accurate fine-tuning results for previously unseen data classes. These results are achieved despite having only 21K trainable parameters, allowing an inference time of 0.33 ms on common edge devices, making the model suitable for constrained real-time deployments.

</details>


### [4] [Spatially Consistent Air-to-Ground Channel Modeling and Simulation via 3D Shadow Projections](https://arxiv.org/abs/2511.15412)
*Evgenii Vinogradov,Aymen Fakhreddine,Abdul Saboor,Sergi Abadal,Sofie Pollin*

Main category: eess.SP

TL;DR: 提出了一种空间一致的半确定性空地信道建模方法，通过3D建筑阴影投影确定视距区域，结合确定性路径损耗和随机阴影衰落，生成适用于6G非地面网络的无线电地图。


<details>
  <summary>Details</summary>
Motivation: 为无人机辅助网络提供高效的空地信道建模方法，替代计算复杂的射线追踪或完全随机模型，特别适用于用户移动性、链路规划和无线电地图生成。

Method: 使用高效的3D建筑阴影投影确定视距区域，快速生成视距地图，将视距感知的确定性路径损耗与随机阴影衰落相结合。

Result: 在符合ITU标准的曼哈顿网格环境中，模型能够反映关键的城市场播特性，如视距阻塞模式和中断行为。

Conclusion: 该方法为6G非地面网络提供了一种高效的替代方案，特别适用于用户移动性、链路规划和无线电地图生成。

Abstract: We present an approach for spatially-consistent semi-deterministic Air-to-Ground (A2G) channel modeling in Unmanned Aerial Vehicle-assisted networks. We use efficient 3D building shadow projections to determine Line-of-Sight (LOS) regions, enabling fast generation of LOS maps. By integrating LOS-aware deterministic path loss with stochastic shadow fading, the approach produces spatially consistent A2G radio maps suitable for environment- and mobility-aware channel evaluation and performance prediction. Simulation results in ITU-compliant Manhattan grid environments demonstrate the model's ability to reflect key urban propagation characteristics, such as LOS blockage patterns and outage behavior. The proposed approach provides an efficient alternative to ray tracing or fully stochastic models, with particular relevance for user mobility, link planning, and radio map generation in 6G non-terrestrial networks.

</details>


### [5] [WiCo-MG: Wireless Channel Foundation Model for Multipath Generation via Synesthesia of Machines](https://arxiv.org/abs/2511.15026)
*Zengrui Han,Lu Bai,Xuesong Cai,Xiang Cheng*

Main category: eess.SP

TL;DR: 提出WiCo-MG无线信道基础模型，通过机器联觉实现多径生成，为6G AI原生通信系统提供大规模高质量信道数据。


<details>
  <summary>Details</summary>
Motivation: 6G AI原生通信系统需要大量高质量多径信道数据来支持智能模型训练和性能优化，但现有方法难以满足需求。

Method: 采用两阶段训练框架：第一阶段将感知图像和多径图嵌入到离散-连续联觉特征空间进行表示对齐；第二阶段使用频率感知专家路由的S-R MoE Transformer学习从感知到信道联觉特征空间的映射。

Result: WiCo-MG在分布内生成性能上达到最先进水平，分布外泛化性能优越，NMSE比基线降低超过2.59 dB，在模型和数据集扩展方面表现出强可扩展性。

Conclusion: WiCo-MG具有更高精度、更强泛化能力和更好可扩展性，有望为6G AI原生通信系统的发展提供大规模高保真信道数据生成能力。

Abstract: Precise modeling of channel multipath is essential for understanding wireless propagation environments and optimizing communication systems. In particular, sixth-generation (6G) artificial intelligence (AI)-native communication systems demand massive and high-quality multipath channel data to enable intelligent model training and performance optimization. In this paper, we propose a wireless channel foundation model (WiCo) for multipath generation (WiCo-MG) via Synesthesia of Machines (SoM). To provide a solid training foundation for WiCo-MG, a new synthetic intelligent sensing-communication dataset for uncrewed aerial vehicle (UAV)-to-ground (U2G) communications is constructed. To overcome the challenges of cross-modal alignment and mapping, a two-stage training framework is proposed. In the first stage, sensing images are embedded into discrete-continuous SoM feature spaces, and multipath maps are embedded into a sensing-initialized discrete SoM space to align the representations. In the second stage, a mixture of shared and routed experts (S-R MoE) Transformer with frequency-aware expert routing learns the mapping from sensing to channel SoM feature spaces, enabling decoupled and adaptive multipath generation. Experimental results demonstrate that WiCo-MG achieves state-of-the-art in-distribution generation performance and superior out-of-distribution generalization, reducing NMSE by more than 2.59 dB over baselines, while exhibiting strong scalability in model and dataset growth and extensibility to new multipath parameters and tasks. Owing to higher accuracy, stronger generalization, and better scalability, WiCo-MG is expected to enable massive and high-fidelity channel data generation for the development of 6G AI-native communication systems.

</details>


### [6] [WiCo-PG: Wireless Channel Foundation Model for Pathloss Map Generation via Synesthesia of Machines](https://arxiv.org/abs/2511.15030)
*Mingran Sun,Lu Bai,Ziwei Huang,Xuesong Cai,Xiang Cheng,Jianjun Wu*

Main category: eess.SP

TL;DR: 提出了首个基于机器联觉的无线信道基础模型WiCo-PG，用于路径损耗地图生成，在6G无人机对地通信场景中通过多模态数据预训练实现跨模态路径损耗地图生成。


<details>
  <summary>Details</summary>
Motivation: 解决6G无人机对地通信场景中路径损耗地图生成的挑战，利用多模态感知通信数据构建基础模型，实现跨模态的路径损耗预测。

Method: 基于双向量量化生成对抗网络和Transformer的新网络架构，设计了频率引导的共享路由混合专家架构，利用RGB图像和不同场景、飞行高度数据进行跨模态路径损耗地图生成。

Result: WiCo-PG实现了归一化均方误差0.012的路径损耗地图生成精度，比基于大语言模型的LLM4PG方案和传统深度学习方法提升超过6.98dB，在少样本泛化中仅用2.7%样本即可比LLM4PG提升至少1.37dB。

Conclusion: WiCo-PG作为首个无线信道基础模型，在路径损耗地图生成方面表现出优越的性能和泛化能力，为6G无人机通信提供了有效的解决方案。

Abstract: A wireless channel foundation model for pathloss map generation (WiCo-PG) via Synesthesia of Machines (SoM) is developed for the first time. Considering sixth-generation (6G) uncrewed aerial vehicle (UAV)-to-ground (U2G) scenarios, a new multi-modal sensing-communication dataset is constructed for WiCo-PG pre-training, including multiple U2G scenarios, diverse flight altitudes, and diverse frequency bands. Based on the constructed dataset, the proposed WiCo-PG enables cross-modal pathloss map generation by leveraging RGB images from different scenarios and flight altitudes. In WiCo-PG, a novel network architecture designed for cross-modal pathloss map generation based on dual vector quantized generative adversarial networks (VQGANs) and Transformer is proposed. Furthermore, a novel frequency-guided shared-routed mixture of experts (S-R MoE) architecture is designed for cross-modal pathloss map generation. Simulation results demonstrate that the proposed WiCo-PG achieves improved pathloss map generation accuracy through pre-training with a normalized mean squared error (NMSE) of 0.012, outperforming the large language model (LLM)-based scheme, i.e., LLM4PG, and the conventional deep learning-based scheme by more than 6.98 dB. The enhanced generality of the proposed WiCo-PG can further outperform the LLM4PG by at least 1.37 dB using 2.7% samples in few-shot generalization.

</details>


### [7] [The Triple-C Paradigm: Cooperative, Complementary, and Competitive Modes for TBS-HAPS-LEO Integration](https://arxiv.org/abs/2511.15064)
*Eros Kuikel,Sidrah Javed,Baha Eddine Youcef Belmekki,Yunfei Chen,Ning Wang,Mohamed-Slim Alouini*

Main category: eess.SP

TL;DR: 提出了一个名为"三重C框架"的新颖统一框架，用于协调地面基站(TBS)、高空平台站(HAPS)和低地球轨道(LEO)卫星之间的互动，以提供无缝、弹性和可扩展的连接。


<details>
  <summary>Details</summary>
Motivation: 现有网络难以满足无处不在和弹性全球覆盖的需求，需要整合TBS、HAPS和LEO卫星来提供解决方案，但这些异构平台之间的协调仍是一个开放挑战。

Method: 提出了三重C框架：合作、互补和竞争，详细阐述了每种交互模式的架构方法、先决条件和可衡量的交付成果，并确定了在物理层、逻辑层和认知层实现该框架的使能技术。

Result: 通过丰富的用例和应用组合展示了该技术飞跃的可行性和影响力，全面的性能分析和仿真结果量化了此类集成网络的权衡。

Conclusion: 分析了经济、环境、安全、隐私、标准化和监管影响，提供了差距分析，概述了关键技术/非技术挑战，并规划了未来研究方向，以充分释放TBS-HAPS-LEO集成网络中合作、互补和竞争运营的潜力。

Abstract: The growing demands of ubiquitous and resilient global coverage have pushed existing networks to their operational limits, making it increasingly difficult to meet all requirements on their own. Integrating \emph{Terrestrial Base Stations (TBS), High Altitude Platform Stations (HAPS)} and \emph{Low-Earth-Orbit (LEO)} satellites is envisioned as a promising solution, yet the coordination across these heterogeneous platforms remains an open challenge. This paper proposes a novel unifying \emph{Triple-C framework: Cooperation, Complementarity, and Competition}, that systematically defines the TBS-HAPS-LEO interaction to deliver seamless resilient and scalable connectivity. For each C, we detail the architectural methodology, required pre-requisites, and measurable deliverables that govern when and how the three layers should collaborate, complement each other, or contend. We further identify the enabling technologies across physical, logical, and cognitive layers to operationalize the proposed 3C paradigm. A rich portfolio of use cases and targeted applications demonstrates how this technological leap will make such integration both feasible and impactful. Comprehensive performance analysis and emulation results quantify the trade-offs of such integrated networks. In addition, we examine the economical, environmental, safety, privacy, standardization, and regulatory implications that shape the real-world implementation of the proposed framework. eventually, we provide the gap analysis, outline key technical/non-technical challenges, and a road-map of future research directions needed to unlock the full potential of Cooperation, Complementarity, and Competition operations in TBS-HAPS-LEO integrated networks.

</details>


### [8] [Enhancing Physical Layer Security in MIMO Systems Assisted by Beyond-Diagonal Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2511.15093)
*Weijie Xiong,Jingran Lin,Cunhua Pan,Yilong Zeng,Qiang Li*

Main category: eess.SP

TL;DR: 本文研究了基于超对角可重构智能表面(BD-RIS)的MIMO系统物理层安全性能，提出了一种低复杂度的惩罚乘积黎曼共轭梯度下降算法来优化发射波束成形和BD-RIS反射单元，显著提升了保密率。


<details>
  <summary>Details</summary>
Motivation: 传统RIS使用对角散射矩阵，仅能实现各反射单元的独立反射，限制了信道操控的灵活性。而BD-RIS通过共享阻抗网络实现单元间可调连接，采用非对角散射矩阵，显著增强了信道整形能力，为物理层安全技术创造了新机会。

Method: 将保密率最大化问题建模为非凸优化问题，联合优化发射波束成形和BD-RIS反射单元。引入辅助变量解耦BD-RIS约束，提出惩罚乘积黎曼共轭梯度下降(P-PRCGD)方法，结合增广拉格朗日方法和乘积流形梯度下降法来获得KKT解。

Result: 仿真结果表明，BD-RIS辅助系统在物理层安全性能上显著优于传统RIS辅助系统。

Conclusion: BD-RIS通过非对角散射矩阵架构显著提升了MIMO系统的物理层安全性能，提出的P-PRCGD算法有效解决了相应的非凸优化问题。

Abstract: Reconfigurable intelligent surfaces (RISs) hold significant promise for enhancing physical layer security (PLS). However, conventional RISs are typically modeled using diagonal scattering matrices, capturing only independent reflections from each reflecting element, which limits their flexibility in channel manipulation. In contrast, beyond-diagonal RISs (BD-RISs) employ non-diagonal scattering matrices enabled by active and tunable inter-element connections through a shared impedance network. This architecture significantly enhances channel shaping capabilities, creating new opportunities for advanced PLS techniques. This paper investigates PLS in a multiple-input multiple-output (MIMO) system assisted by BD-RISs, where a multi-antenna transmitter sends confidential information to a multi-antenna legitimate user while a multi-antenna eavesdropper attempts interception. To maximize the secrecy rate (SR), we formulate it as a non-convex optimization problem by jointly optimizing the transmit beamforming and BD-RIS REs under power and structural constraints. To solve this problem, we first introduce an auxiliary variable to decouple BD-RIS constraints. We then propose a low-complexity penalty product Riemannian conjugate gradient descent (P-PRCGD) method, which combines the augmented Lagrangian (AL) approach with the product manifold gradient descent (PMGD) method to obtain a Karush-Kuhn-Tucker (KKT) solution. Simulation results confirm that BD-RIS-assisted systems significantly outperform conventional RIS-assisted systems in PLS performance.

</details>


### [9] [Constant-Modulus Secure Analog Beamforming for an IRS-Assisted Communication System with Large-Scale Antenna Array](https://arxiv.org/abs/2511.15095)
*Weijie Xiong,Jingran Lin,Zhiling Xiao,Qiang Li*

Main category: eess.SP

TL;DR: 本文研究了IRS辅助通信系统中的物理层安全，提出了两种算法来联合设计发射端的恒定模数模拟波束成形和IRS的相位偏移，以最大化保密率。


<details>
  <summary>Details</summary>
Motivation: 大规模天线阵列和智能反射面虽然能增强物理层安全，但硬件成本和功耗较高。恒定模数模拟波束成形作为一种成本效益高的解决方案受到关注。

Method: 提出了两种算法：1）Dinkelbach-BSUM算法，将分数问题转化为一系列二次规划问题；2）PMCGD算法，将问题转化为黎曼乘积流形上的无约束优化问题。

Result: 仿真结果验证了所提算法的有效性，并突出了它们各自的优势。

Conclusion: 所提出的算法能有效解决IRS辅助通信系统中的保密率最大化问题，Dinkelbach-BSUM算法时间效率高，PMCGD算法能提供更好的解但计算时间稍长。

Abstract: Physical layer security (PLS) is an important technology in wireless communication systems to safeguard communication privacy and security between transmitters and legitimate users. The integration of large-scale antenna arrays (LSAA) and intelligent reflecting surfaces (IRS) has emerged as a promising approach to enhance PLS. However, LSAA requires a dedicated radio frequency (RF) chain for each antenna element, and IRS comprises hundreds of reflecting micro-antennas, leading to increased hardware costs and power consumption. To address this, cost-effective solutions like constant modulus analog beamforming (CMAB) have gained attention. This paper investigates PLS in IRS-assisted communication systems with a focus on jointly designing the CMAB at the transmitter and phase shifts at the IRS to maximize the secrecy rate. The resulting secrecy rate maximization (SRM) problem is non-convex. To solve the problem efficiently, we propose two algorithms: (1) the time-efficient Dinkelbach-BSUM algorithm, which reformulates the fractional problem into a series of quadratic programs using the Dinkelbach method and solves them via block successive upper-bound minimization (BSUM), and (2) the product manifold conjugate gradient descent (PMCGD) algorithm, which provides a better solution at the cost of slightly higher computational time by transforming the problem into an unconstrained optimization on a Riemannian product manifold and solving it using the conjugate gradient descent (CGD) algorithm. Simulation results validate the effectiveness of the proposed algorithms and highlight their distinct advantages.

</details>


### [10] [Joint Analog Beamforming and Antenna Position Design for Secure Communication systems With Movable Antennas](https://arxiv.org/abs/2511.15096)
*Weijie Xiong,Kai Zhong,Zhiling Xiao,Jingran Lin,Qiang Li*

Main category: eess.SP

TL;DR: 本文研究使用可移动天线提升模拟波束成形系统的物理层安全性能，通过联合优化发射波束成形和天线位置来最大化保密速率，提出了基于惩罚乘积流形的高效求解方法。


<details>
  <summary>Details</summary>
Motivation: 可移动天线技术能够灵活调整天线位置，为提升无线通信系统性能提供了新途径。本文旨在利用可移动天线增强模拟波束成形系统的物理层安全性。

Method: 提出惩罚乘积流形方法，将天线位置不等式约束转化为惩罚函数，在乘积流形空间上重构为无约束优化问题，并设计并行共轭梯度下降算法进行求解。

Result: 仿真结果表明，可移动天线系统相比固定位置天线系统能够获得更高的保密速率。

Conclusion: 可移动天线技术能有效提升物理层安全性能，所提出的PPM方法能够高效求解非凸优化问题并保证收敛到KKT点。

Abstract: Movable antennas (MA) are a novel technology that allows for the flexible adjustment of antenna positions within a specified region, thereby enhancing the performance of wireless communication systems. In this paper, we explore the use of MA to improve physical layer security in an analog beamforming (AB) communication system. Our goal is to maximize the secrecy rate by jointly optimizing the transmit AB and MA position, subject to constant modulus (CM) constraints on the AB and position constraints for the MA. The resulting problem is non-convex, and we propose a penalty product manifold (PPM) method to solve it efficiently. Specifically, we convert the inequality constraints related to MA position into a penalty function using smoothing techniques, thereby reformulating the problem as an unconstrained optimization on the product manifold space (PMS). We then derive a parallel conjugate gradient descent (PCGD) algorithm to update both the AB and MA position on the PMS. This method is efficient, providing an analytical solution at each step and ensuring convergence to a KKT point. Simulation results show that the MA system achieves a higher secrecy rate than systems with fixed-position antennas.

</details>


### [11] [Position Optimization for Two-layer Movable Antenna Systems](https://arxiv.org/abs/2511.15108)
*Liujia Yao,Changsheng You,Chao Zhou,Beixiong Zheng,Weidong Mei*

Main category: eess.SP

TL;DR: 提出了一种双层可移动天线阵列(TL-MA)，通过子阵列的大规模移动和每个子阵列天线的小规模微调来联合优化天线位置，在保持相似速率性能的同时显著降低了电机总位移距离。


<details>
  <summary>Details</summary>
Motivation: 现有单层可移动天线(SL-MA)阵列中所有天线都在给定区域内移动，导致控制复杂度和硬件成本较高，需要一种更实用的解决方案。

Method: 提出TL-MA结构，联合优化子阵列位置、每个子阵列的相对天线位置和接收波束成形；开发了基于交替优化(AO)的粒子群优化(PSO)算法来求解这个非凸问题。

Result: 数值结果表明，TL-MA在保持与SL-MA相当的速率性能的同时，显著降低了天线电机的总位移距离。

Conclusion: TL-MA是解决可移动天线系统控制复杂度和硬件成本问题的有效方法，在性能与成本之间取得了良好平衡。

Abstract: Movable antenna (MA) is a promising technology for improving the performance of wireless communication systems by providing new degrees-of-freedom (DoFs) in antenna position optimization. However, existing works on MA systems have mostly considered element-wise single-layer MA (SL-MA) arrays, where all the MAs move within the given movable region, hence inevitably incurring high control complexity and hardware cost in practice. To address this issue, we propose in this letter a new two-layer MA array (TL-MA), where the positions of MAs are jointly determined by the large-scale movement of multiple subarrays and the small-scale fine-tuning of per-subarray MAs. In particular, an optimization problem is formulated to maximize the sum-rate of the TL-MA-aided communication system by jointly optimizing the subarray-positions, per-subarray (relative) MA positions, and receive beamforming. To solve this non-convex problem, we propose an alternating optimization (AO)-based particle swarm optimization (PSO) algorithm, which alternately optimizes the positions of subarrays and per-subarray MAs, given the optimal receive beamforming. Numerical results verify that the proposed TL-MA significantly reduces the sum-displacement of MA motors (i.e., the total moving distances of all motors) of element-wise SL-MA, while achieving comparable rate performance.

</details>


### [12] [Multimodal Wireless Foundation Models](https://arxiv.org/abs/2511.15162)
*Ahmed Aboulfotouh,Hatem Abou-Zeid*

Main category: eess.SP

TL;DR: 提出了首个多模态无线基础模型，能够同时处理原始IQ流和图像类无线模态，并在两种模态上执行多个任务。


<details>
  <summary>Details</summary>
Motivation: 当前无线基础模型只能处理单一模态，但不同任务和操作条件下最有信息量的模态会变化，没有单一模态适用于所有任务，因此需要设计能够接受多模态的模型。

Method: 引入了多模态设置下的掩码无线建模，这是一种自监督目标和预训练方法，从IQ流和图像类无线模态中学习联合表示。

Result: 在五个任务上评估模型，涵盖图像类（人体活动感知、RF信号分类、5G NR定位）和IQ类（RF设备指纹识别、干扰检测/分类）。多模态WFM与单模态WFM具有竞争力，并在多个情况下超越其性能。

Conclusion: 结果证明了开发支持不同模态多样化无线任务的多模态WFM的强大潜力，为实现AI原生6G和联合感知、通信与定位愿景提供了具体步骤。

Abstract: Wireless foundation models (WFMs) have recently demonstrated promising capabilities, jointly performing multiple wireless functions and adapting effectively to new environments. However, while current WFMs process only one modality, depending on the task and operating conditions, the most informative modality changes and no single modality is best for all tasks. WFMs should therefore be designed to accept multiple modalities to enable a broader and more diverse range of tasks and scenarios. In this work, we propose and build the first multimodal wireless foundation model capable of processing both raw IQ streams and image-like wireless modalities (e.g., spectrograms and CSI) and performing multiple tasks across both. We introduce masked wireless modeling for the multimodal setting, a self-supervised objective and pretraining recipe that learns a joint representation from IQ streams and image-like wireless modalities. We evaluate the model on five tasks across both modality families: image-based (human activity sensing, RF signal classification, 5G NR positioning) and IQ-based (RF device fingerprinting, interference detection/classification). The multimodal WFM is competitive with single-modality WFMs, and in several cases surpasses their performance. Our results demonstrates the strong potential of developing multimodal WFMs that support diverse wireless tasks across different modalities. We believe this provides a concrete step toward both AI-native 6G and the vision of joint sensing, communication, and localization.

</details>


### [13] [Spectrum and Orthogonality of Orthogonal Delay-Doppler Division Multiplexing Modulation Waveforms](https://arxiv.org/abs/2511.15184)
*Akram Shafie,Jun Tong,Jinhong Yuan,Taka Sakurai,Paul Fitzpatrick,Yuting Fang,Yixuan Xie*

Main category: eess.SP

TL;DR: 本文分析了正交时延多普勒(ODDM)调制在模拟和近似数字实现中的频谱特性和正交性特征，证明了近似数字ODDM波形无需额外时域资源即可满足正交性要求。


<details>
  <summary>Details</summary>
Motivation: ODDM调制在双选择性信道中具有可靠通信潜力，但需要深入理解其模拟和数字实现的频谱特性与正交性差异，以指导实际系统设计。

Method: 推导了模拟和近似数字ODDM系统波形基函数的时域和频域表示，计算了功率谱密度，分析了正交性特征，并与其它DD调制变体进行了比较。

Result: 模拟ODDM波形的频谱在过渡区域呈现阶梯状行为，而近似数字ODDM波形的频谱受限于子脉冲频谱；近似数字ODDM波形无需额外时域资源即可满足正交性。

Conclusion: 近似数字ODDM实现相比模拟实现具有更好的频谱约束和正交性特性，为实际系统设计提供了重要指导。

Abstract: Orthogonal delay-Doppler (DD) division multiplexing (ODDM) modulation has recently emerged as a promising paradigm for ensuring reliable communications in doubly-selective channels. This work investigates the spectra and orthogonality characteristics of analog (direct) and approximate digital implementations of ODDM systems. We first determine the time and frequency domain representations of the basis functions for waveform in analog and approximate digital ODDM systems. Thereafter, we derive their power spectral densities and show that while the spectrum of analog ODDM waveforms exhibits a step-wise behavior in its transition regions, the spectrum of approximate digital ODDM waveforms is confined to that of the ODDM sub-pulse. Next, we prove the orthogonality characteristics of approximate digital ODDM waveforms and show that, unlike analog ODDM waveforms, the approximate digital ODDM waveforms satisfy orthogonality without the need of additional time domain resources. Additionally, we examine the similarities and differences that implementations of approximate digital ODDM share with the other variants of DD modulations, focusing on the domain changes the symbols undergo, the type of pulse shaping and windowing used, and the domains and the sequence in which they are performed. Finally, we present numerical results to validate our findings and draw further insights.

</details>


### [14] [Theoretical Bounds on Parallel Imaging Implicit Data Crimes in an MRI Reproducing Kernel Hilbert Space](https://arxiv.org/abs/2511.15187)
*Evan Frenklak,Yamin Arefeen,Jonathan I Tamir*

Main category: eess.SP

TL;DR: 提出一个数学框架来重新定义MRI中隐式数据犯罪问题，将其视为评估坐标集间插值误差减少问题，并建立了重建误差上界的通用矩阵定义。


<details>
  <summary>Details</summary>
Motivation: MRI扫描时间长导致成本高和可及性受限，AI方法虽然能减少扫描时间，但由于隐式数据犯罪（由MRI数据集不完全建模采集物理特性引入的隐藏偏差）导致临床转化失败。

Method: 将问题重新定义为评估坐标集间插值误差减少问题，建立基于矩阵的重建误差上界通用定义，并在相关采样模式结构上进行实验验证。

Result: 实验证明了该框架的相关性，并为数据犯罪分析提供了未来研究方向。

Conclusion: 提出的数学框架为理解和分析MRI中隐式数据犯罪提供了新的理论工具，有助于改进AI方法在MRI加速中的临床转化。

Abstract: Magnetic Resonance Imaging (MRI) diagnoses and manages a wide range of diseases, yet long scan times drive high costs and limit accessibility. AI methods have demonstrated substantial potential for reducing scan times, but despite rapid progress, clinical translation of AI often fails. One particular class of failure modes, referred to as implicit data crimes, are a result of hidden biases introduced when MRI datasets incompletely model the MRI physics of the acquisition. Previous work identified data crimes resulting from algorithmic completion of k-space with parallel imaging and drew on simulation to demonstrate the resulting downstream biases. This work proposes a mathematical framework to re-characterize the problem as one of error reduction during interpolation between sets of evaluation coordinates. We establish a generalized matrix-based definition of the reconstruction error upper bound as a function of the input sampling pattern. Experiments on relevant sampling pattern structures demonstrate the relevance of the framework and suggest future directions for analysis of data crimes.

</details>


### [15] [Space-Time-Frequency Synthetic Integrated Sensing and Communication Networks](https://arxiv.org/abs/2511.15198)
*Henglin Pu,Xuefeng Wang,Lu Su,Husheng Li*

Main category: eess.SP

TL;DR: 提出了一种时空频合成ISAC架构，通过融合分布式收发器的观测数据来提升感知性能，并比较了集中式MLE和两阶段融合方法的性能差异。


<details>
  <summary>Details</summary>
Motivation: 商用蜂窝网络由于孔径、带宽和相干观测时间有限，难以提供精细的角度、距离和多普勒分辨率，需要新的ISAC架构来克服这些限制。

Method: 开发了统一的多基地和单基地配置信号模型，推导了位置和速度估计的CRLB，提出了集中式MLE和两阶段信息融合(TSIF)两种估计方法。

Result: 数值结果显示MLE在高SNR时接近CRLB，两阶段方法在中高SNR时具有竞争力但在低SNR时性能下降，完全合成的网络处理比各基站单独估计后融合更优。

Conclusion: 该框架为将现有通信基础设施升级为密集感知网络提供了实用指导，强调了完全合成网络处理的重要性。

Abstract: Integrated sensing and communication (ISAC) promises high spectral and power efficiencies by sharing waveforms, spectrum, and hardware across sensing and data links. Yet commercial cellular networks struggle to deliver fine angular, range, and Doppler resolution due to limited aperture, bandwidth, and coherent observation time. In this paper, we propose a space-time-frequency synthetic ISAC architecture that fuses observations from distributed transmitters and receivers across time intervals and frequency bands. We develop a unified signal model for multistatic and monostatic configurations, derive Cramer-Rao lower bounds (CRLBs) for the estimations of position and velocity. The analysis shows how spatial diversity, multiband operation, and observation scheduling impact the Fisher information. We also compare the estimation performance between a concentrated maximum likelihood estimator (MLE) and a two stage information fusion (TSIF) method that first estimates per-path delay and radial speed and then fuses them by solving a weighted nonlinear least-squares problem via the Gauss-Newton algorithm. Numerical results show that MLE approaches the CRLB in the high signal-to-noise ratio (SNR) regime, while the two stage method remains competitive at moderate to high SNR but degrades at low SNR. A central finding is that fully synthesized network processing is essential, as estimations by individual base stations (BSs) followed by fusion are consistently inferior and unstable at low SNR. This framework offers a practical guidance for upgrading existing communication infrastructure into dense sensing networks.

</details>


### [16] [Theoretical and Empirical Study of Spatial Power Focusing Effect for Sparse Arrays at Terahertz Band](https://arxiv.org/abs/2511.15221)
*Yongchao He,Taihao Zhang,Cunhua Pan,Hong Ren,Xianzhe Chen,Tian Qiu,Bingchang Hua,Jiangzhou Wang*

Main category: eess.SP

TL;DR: 本文研究了太赫兹频段大规模稀疏阵列的空间功率聚焦效应，结合理论分析和实验验证，推导了闭式表达式并构建了300 GHz测量平台进行验证。


<details>
  <summary>Details</summary>
Motivation: 研究太赫兹频段大规模稀疏阵列的空间功率聚焦效应，通过理论分析和实验验证来理解和表征这一现象。

Method: 基于格林函数信道模型分析功率分布，推导闭式表达式，理论分析相位噪声和位置偏差等影响因素，构建300 GHz矢量网络分析仪测量平台进行实验验证。

Result: 测量结果与理论仿真结果高度一致，证实了稀疏阵列的空间功率聚焦效应。

Conclusion: 成功验证了太赫兹频段稀疏阵列的空间功率聚焦效应，理论与实验结果吻合良好。

Abstract: This work investigates the spatial power focusing effect for large-scale sparse arrays at terahertz (THz) band, combining theoretical analysis with experimental validation. Specifically, based on a Green's function channel model, we analyze the power distribution along the $z$-axis, deriving a closed-form expression to characterize the focusing effect. Furthermore, the factors influencing the focusing effect, including phase noise and positional deviations, are theoretically analyzed and numerically simulated. Finally, a 300 GHz measurement platform based on a vector network analyzer (VNA) is constructed for experimental validation. The measurement results demonstrate close consistence with theoretical simulation results, confirming the spatial power focusing effect for sparse arrays.

</details>


### [17] [Enabling NLOS Imaging Capabilities at the Initial Access of 6G Base Stations](https://arxiv.org/abs/2511.15416)
*Davide Tornielli Bellini,Dario Tagliaferri,Pietro Grassi,Davide Scazzoli,Stefano Tebaldini,Umberto Spagnolini*

Main category: eess.SP

TL;DR: 该论文提出了一种在基站初始接入过程中集成非视距成像功能的方法，通过非可重构模块化反射器和增强的波束码本设计，实现NLOS区域的高分辨率成像。


<details>
  <summary>Details</summary>
Motivation: 解决集成感知与通信系统中非视距感知的挑战，现有方法要么依赖环境先验知识，要么需要部署异常反射器。本文旨在将单站NLOS成像功能集成到下一代基站的初始接入过程中。

Method: 通过增强标准初始接入的波束码本，添加成像专用条目，并与模块化反射器的角度配置联合设计，使基站能够相干处理所有回波，实现NLOS区域的高分辨率成像。

Result: 推导了近场空间分辨率和有效孔径的闭式表达式，提出了移动目标速度的最大似然估计方法，并通过数值模拟和实验验证了所提方法的有效性。

Conclusion: 提出的基于模块化反射器的成像方法能够有效实现NLOS成像，同时揭示了通信与成像性能之间的权衡关系，为系统设计提供了理论指导。

Abstract: Sensing in non-line-of-sight (NLOS) is one of the major challenges for integrated sensing and communication systems. Existing countermeasures for NLOS either use prior knowledge on the environment to characterize all the multiple bounces or deploy anomalous reflectors in the environment to enable communication infrastructure to ''\textit{see behind the corner}''. This work addresses the integration of monostatic NLOS imaging functionalities into the initial access (IA) procedure of a next generation base station (BS), by means of a non-reconfigurable modular reflector. During standard-compliant IA, the BS sweeps a narrow beam using a pre-defined dedicated codebook to achieve the beam alignment with users. We introduce the imaging functionality by enhancing such codebook with imaging-specific entries that are jointly designed with the angular configuration of the modular reflector to enable high-resolution imaging of a region in NLOS by \textit{coherently} processing all the echoes at the BS. We derive closed-form expressions for the near-field (NF) spatial resolution, as well as for the \textit{effective aperture} (i.e., the portion of the reflector that actively contributes to improve image resolution). The problem of imaging of moving targets in NLOS is also addressed, and we propose a maximum-likelihood estimation for target's velocity in NF and related theoretical bound. Further, we discuss and quantify the inherent communication-imaging performance trade-offs and related system design challenges through numerical simulations. Finally, the proposed imaging method employing modular reflectors is validated both numerically and experimentally, showing the effectiveness of our concept.

</details>


### [18] [Pinching-Antenna System-Assisted Localization: A Stochastic Geometry Perspective](https://arxiv.org/abs/2511.15444)
*Jiajun He,Xidong Mu,Hien Quoc Ngo,Michail Matthaiou*

Main category: eess.SP

TL;DR: 提出了一种基于夹持天线系统的定位框架，使用从夹持天线发射的下行信号的接收信号强度进行目标定位，通过随机几何建模分析网络级性能，推导了目标可定位性和CRLB分布的闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 传统固定天线系统在定位性能上存在局限，需要开发更有效的定位框架来提升定位精度和网络级性能分析能力。

Method: 使用随机几何建模夹持天线的空间分布，通过接收信号强度测量进行目标定位，推导闭式表达式分析目标可定位性和CRLB分布。

Result: 获得了目标可定位性和CRLB分布的闭式表达式，数值结果表明PA辅助方法在CRLB方面优于传统固定天线系统。

Conclusion: 提出的PA辅助定位框架提供了网络级性能分析的理论基础，并为选择最优波导数量以最大化定位性能提供了实用指导。

Abstract: This paper proposes a novel localization framework underpinned by a pinching-antenna (PA) system, in which the target location is estimated using received signal strength (RSS) measurements obtained from downlink signals transmitted by the PAs. To develop a comprehensive analytical framework, we employ stochastic geometry to model the spatial distribution of the PAs, enabling tractable and insightful network-level performance analysis. Closed-form expressions for target localizability and the Cramer-Rao lower bound (CRLB) distribution are analytically derived, enabling the evaluation of the fundamental limits of PA-assisted localization systems without extensive simulations. Furthermore, the proposed framework provides practical guidance for selecting the optimal waveguide number to maximize localization performance. Numerical results also highlight the superiority of the PA-assisted approach over conventional fixed-antenna systems in terms of the CRLB.

</details>


### [19] [Division-based Receiver-agnostic RFF Identification in WiFi Systems](https://arxiv.org/abs/2511.15458)
*Xuan Yang,Dongming Li,Dong Wei,Meng Zhang*

Main category: eess.SP

TL;DR: 提出一种基于频域划分的接收器无关RFF提取方法，通过在频域划分不同前导码来消除接收器差异，仅需单个接收器进行训练，在平坦衰落和频率选择性衰落信道下都能有效提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 物理层安全方案中，WiFi设备的射频指纹识别容易受到接收器差异的影响，当模型在一个接收器上训练但在另一个接收器上测试时，分类性能会显著下降。

Method: 提出基于频域划分的接收器无关RFF提取方法：在平坦衰落信道下，将未知设备的L-STF和L-LTF与参考设备在频域相除；在频率选择性衰落信道下，将HT-LTF与L-LTF在频域相除。仅需单个接收器训练，无需额外校准或堆叠过程。

Result: 仿真和实验结果表明，该方法有效缓解了信道变化和接收器差异的影响。在单个接收器训练、不同接收器测试的情况下，相比最先进方法，在平坦衰落和频率选择性衰落信道下的分类准确率分别提升了15.5%和28.45%。

Conclusion: 该方法成功实现了接收器无关的射频指纹提取，仅需单个接收器训练就能在不同接收器上保持良好性能，为物理层安全提供了有效的解决方案。

Abstract: In physical-layer security schemes, radio frequency fingerprint (RFF) identification of WiFi devices is susceptible to receiver differences, which can significantly degrade classification performance when a model is trained on one receiver but tested on another. In this paper, we propose a division-based receiver-agnostic RFF extraction method for WiFi systems, which removes the receivers' effects by dividing different preambles in the frequency domain. The proposed method requires only a single receiver for training and does not rely on additional calibration or stacking processes. First, for flat fading channel scenarios, the legacy short training field (L-STF) and legacy long training field (L-LTF) of the unknown device are divided by those of the reference device in the frequency domain. The receiver-dependent effects can be eliminated with the requirement of only a single receiver for training, and the higher-dimensional RFF features can be extracted. Second, for frequency-selective fading channel scenarios, the high-throughput long training field (HT-LTF) is divided by the L-LTF in the frequency domain. Only a single receiver is required for training and the higher-dimensional RFF features that are both channel-invariant and receiver-agnostic are extracted. Finally, simulation and experimental results demonstrate that the proposed method effectively mitigate the impacts of channel variations and receiver differences. The classification results show that, even when training on a single receiver and testing on a different one, the proposed method achieves classification accuracy improvements of 15.5% and 28.45% over the state-of-the-art approach in flat fading and frequency-selective fading channel scenarios, respectively.

</details>


### [20] [Collision Resolution in RFID Systems Using Antenna Arrays and Mix Source Separation](https://arxiv.org/abs/2511.15490)
*Mohamed Siala,Noura Sellami*

Main category: eess.SP

TL;DR: 提出了一种用于RFID系统中碰撞解决的高效混合源分离算法，利用零常数模准则和梯度下降方法分离碰撞标签，无需导频符号。


<details>
  <summary>Details</summary>
Motivation: 解决RFID系统中多个标签同时传输导致的碰撞问题，提高标签识别效率。

Method: 首先使用零常数模准则通过梯度下降分离碰撞标签，然后提出混合目标函数，结合新的模糊消除准则来解决ZCM单独使用时的模糊性问题。

Result: 算法能够有效分离碰撞标签，解决了单独使用ZCM准则时的模糊性问题。

Conclusion: 提出的混合目标函数方法比单独使用ZCM准则更有效，能够更好地解决RFID系统中的标签碰撞问题。

Abstract: In this letter, we propose an efficient mix source separation algorithm for collision resolution in radio frequency identification (RFID) systems equipped with an antenna array at the reader. We first introduce an approach that exploits the zero constant modulus (ZCM) criterion to separate colliding tags through gradient descent, without using pilot symbols. We show that the ZCM characteristic, considered alone, in the design of the objective function can lead to significant ambiguities in the determination of the beamformers used in the recovery of tag messages. To address this limitation, we propose a more sophisticated approach, relying on a hybrid objective function, incorporating a new ambiguity-raising criterion in addition to the ZCM criterion.

</details>


### [21] [A Review of Machine Learning for Cavitation Intensity Recognition in Complex Industrial Systems](https://arxiv.org/abs/2511.15497)
*Yu Sha,Ningtao Liu,Haofeng Liu,Junqi Tao,Zhenxing Niu,Guojun Huang,Yao Yao,Jiaqi Liang,Moxian Qian,Horst Stoecker,Domagoj Vnucec,Andreas Widl,Kai Zhou*

Main category: eess.SP

TL;DR: 本文对2002-2025年间液压机械空化强度识别技术的演进进行了系统综述，分析了从传统机器学习到深度学习再到物理知识融合诊断模型的发展轨迹，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 填补空化强度识别领域缺乏系统性发展轨迹回顾和明确未来研究指导的空白，为复杂工业系统中的智能空化诊断提供重要参考。

Method: 对数百篇关于智能空化强度识别的文献进行全面回顾和分析，总结技术演进路径，涵盖传统机器学习、深度学习和物理知识融合模型。

Result: 系统梳理了空化强度识别技术从依赖专家知识的特征工程到端到端深度学习模型，再到物理知识增强模型的发展历程，识别了性能提升和鲁棒性改善的关键节点。

Conclusion: 空化强度识别技术正朝着深度学习与物理知识融合的方向发展，未来需要关注迁移学习、多模态融合、轻量网络架构和工业代理部署等方向，以解决多源数据获取、标准化评估和工业实施等挑战。

Abstract: Cavitation intensity recognition (CIR) is a critical technology for detecting and evaluating cavitation phenomena in hydraulic machinery, with significant implications for operational safety, performance optimization, and maintenance cost reduction in complex industrial systems. Despite substantial research progress, a comprehensive review that systematically traces the development trajectory and provides explicit guidance for future research is still lacking. To bridge this gap, this paper presents a thorough review and analysis of hundreds of publications on intelligent CIR across various types of mechanical equipment from 2002 to 2025, summarizing its technological evolution and offering insights for future development. The early stages are dominated by traditional machine learning approaches that relied on manually engineered features under the guidance of domain expert knowledge. The advent of deep learning has driven the development of end-to-end models capable of automatically extracting features from multi-source signals, thereby significantly improving recognition performance and robustness. Recently, physical informed diagnostic models have been proposed to embed domain knowledge into deep learning models, which can enhance interpretability and cross-condition generalization. In the future, transfer learning, multi-modal fusion, lightweight network architectures, and the deployment of industrial agents are expected to propel CIR technology into a new stage, addressing challenges in multi-source data acquisition, standardized evaluation, and industrial implementation. The paper aims to systematically outline the evolution of CIR technology and highlight the emerging trend of integrating deep learning with physical knowledge. This provides a significant reference for researchers and practitioners in the field of intelligent cavitation diagnosis in complex industrial systems.

</details>


### [22] [Randomized Power Transmission with Optimized Level Selection Probabilities in Uncoordinated Uplink NOMA](https://arxiv.org/abs/2511.15523)
*Noura Sellami,Mohamed Siala*

Main category: eess.SP

TL;DR: 优化非正交多址接入系统中功率等级选择概率，以最小化块错误概率或比特错误概率


<details>
  <summary>Details</summary>
Motivation: 在无协调随机上行链路NOMA系统中，通过优化功率等级选择概率来提升系统性能

Method: 将优化问题建模为二次规划问题（最多两个用户冲突时），对于更多用户冲突情况采用迭代求解方法

Result: 提出了一种适用于任何多用户检测算法和功率等级集的通用解决方案

Conclusion: 该方法具有原创性，能够有效降低NOMA系统的错误概率

Abstract: We consider uncoordinated random uplink non-orthogonal multiple access (NOMA) systems using a set of predetermined power levels. We propose to optimize the probabilities of selection of power levels in order to minimize performance metrics as block error probability (BLEP) or bit error probability (BEP). When the multiuser detection algorithm at the BS treats at most two colliding users' packets, our optimization problem is a quadratic programming problem. For more colliding users' packets, we solve the problem iteratively. Our solution is original because it applies to any multiuser detection algorithm and any set of power levels.

</details>


### [23] [Uncoordinated Cooperative OFDM Multi-Hop UAV Relay Networks Using Virtual Channels Based on All-Pass Filters](https://arxiv.org/abs/2511.15545)
*Noura Sellami,Mohamed Siala*

Main category: eess.SP

TL;DR: 提出了一种基于OFDM的多跳无人机中继网络高效传输方案，通过虚拟传输通道和截断全通滤波器解决协作无人机间的破坏性干扰问题。


<details>
  <summary>Details</summary>
Motivation: 自主协作OFDM多跳无人机中继网络中，由于协作无人机对共同数据包的非协调传输，在目的节点处会产生破坏性干扰，影响系统性能。

Method: 在每个无人机引入虚拟传输通道概念，使用截断全通滤波器实现，确保所有子载波获得可比较的发射功率；结合分布式随机空时分组编码方案增强传输可靠性；提出复合信道估计算法。

Result: 仿真结果表明，所提方案在各种场景下显著优于经典的相位抖动方案。

Conclusion: 该方案通过虚拟传输通道和复合信道估计，有效解决了多跳无人机中继网络中的干扰问题，提高了系统性能。

Abstract: In this paper, we propose an efficient transmission scheme for autonomous cooperative Orthogonal Frequency Division Multiplexing (OFDM) based multi-hop Unmanned Aerial Vehicle (UAV) relay networks. These systems often suffer from destructive interference at the destination node due to uncoordinated transmissions of common packets by cooperating UAVs. To address this issue, we introduce the concept of virtual transmit channels at each UAV, implemented using truncated all-pass filters (APFs). This approach ensures that all subcarriers benefit from comparable transmit powers, guaranteeing excellent performance when a single UAV is transmitting. In scenarios where multiple UAVs cooperate without coordination, the inherent randomness of the generated virtual channels facilitates cooperative diversity, effectively mitigating destructive interference. We further integrate this method with the distributed randomized space-time block coding (STBC) scheme to enhance transmission reliability. Additionally, we propose efficient algorithms for estimating the composite channels that combine both the true propagation channels and the virtual channels. Simulation results demonstrate that our proposed scheme significantly outperforms the classical phase dithering scheme across various scenarios.

</details>


### [24] [CODE-II: A large-scale dataset for artificial intelligence in ECG analysis](https://arxiv.org/abs/2511.15632)
*Petrus E. O. G. B. Abreu,Gabriela M. M. Paixão,Jiawei Li,Paulo R. Gomes,Peter W. Macfarlane,Ana C. S. Oliveira,Vinicius T. Carvalho,Thomas B. Schön,Antonio Luiz P. Ribeiro,Antônio H. Ribeiro*

Main category: eess.SP

TL;DR: CODE-II是一个大规模真实世界心电图数据集，包含273万份12导联心电图，来自209万成年患者，具有66个临床诊断类别，并提供了公开子集用于评估。


<details>
  <summary>Details</summary>
Motivation: 当前AI心电图分析面临注释质量、数据规模和范围等限制，需要大规模高质量数据集来推动进展。

Method: 收集巴西米纳斯吉拉斯州远程医疗网络的273万份心电图，采用标准化诊断标准并由心脏病专家审核，开发了66个临床诊断类别。

Result: 在CODE-II上预训练的神经网络在外部基准测试（PTB-XL和CPSC 2018）中表现出优越的迁移性能，且优于在更大数据集上训练的替代模型。

Conclusion: CODE-II数据集为心电图AI分析提供了高质量的大规模资源，能够显著提升模型在外部数据集上的表现。

Abstract: Data-driven methods for electrocardiogram (ECG) interpretation are rapidly progressing. Large datasets have enabled advances in artificial intelligence (AI) based ECG analysis, yet limitations in annotation quality, size, and scope remain major challenges. Here we present CODE-II, a large-scale real-world dataset of 2,735,269 12-lead ECGs from 2,093,807 adult patients collected by the Telehealth Network of Minas Gerais (TNMG), Brazil. Each exam was annotated using standardized diagnostic criteria and reviewed by cardiologists. A defining feature of CODE-II is a set of 66 clinically meaningful diagnostic classes, developed with cardiologist input and routinely used in telehealth practice. We additionally provide an open available subset: CODE-II-open, a public subset of 15,000 patients, and the CODE-II-test, a non-overlapping set of 8,475 exams reviewed by multiple cardiologists for blinded evaluation. A neural network pre-trained on CODE-II achieved superior transfer performance on external benchmarks (PTB-XL and CPSC 2018) and outperformed alternatives trained on larger datasets.

</details>


### [25] [Joint Semantic-Channel Coding and Modulation for Token Communications](https://arxiv.org/abs/2511.15699)
*Jingkai Ying,Zhijin Qin,Yulong Feng,Liejun Wang,Xiaoming Tao*

Main category: eess.SP

TL;DR: 提出了一种用于点云数据传输的联合语义-信道调制方案，通过将点云令牌映射到数字星座点来实现高效可靠的通信，相比传统方法在重建质量和压缩比方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在各种任务中表现出色，但点云作为复杂的三维数据格式，其令牌通信问题尚未得到充分研究。需要开发能够同时考虑语义信息和信道条件的高效传输方法。

Method: 使用集合抽象方法获取点令牌，提出JSCCM方案包含两个并行Point Transformer编码器和一个结合Gumbel-softmax与软量化的差分调制器，并开发了速率分配器和信道适配器。

Result: 在重建质量上获得超过1dB的增益，在调制符号压缩比方面达到6倍以上，优于传统的分离编码和联合语义-信道编码方法。

Conclusion: 所提出的JSCCM方案能够有效处理点云令牌通信问题，在保持语义信息的同时实现高效的压缩和可靠的传输。

Abstract: In recent years, the Transformer architecture has achieved outstanding performance across a wide range of tasks and modalities. Token is the unified input and output representation in Transformer-based models, which has become a fundamental information unit. In this work, we consider the problem of token communication, studying how to transmit tokens efficiently and reliably. Point cloud, a prevailing three-dimensional format which exhibits a more complex spatial structure compared to image or video, is chosen to be the information source. We utilize the set abstraction method to obtain point tokens. Subsequently, to get a more informative and transmission-friendly representation based on tokens, we propose a joint semantic-channel and modulation (JSCCM) scheme for the token encoder, mapping point tokens to standard digital constellation points (modulated tokens). Specifically, the JSCCM consists of two parallel Point Transformer-based encoders and a differential modulator which combines the Gumel-softmax and soft quantization methods. Besides, the rate allocator and channel adapter are developed, facilitating adaptive generation of high-quality modulated tokens conditioned on both semantic information and channel conditions. Extensive simulations demonstrate that the proposed method outperforms both joint semantic-channel coding and traditional separate coding, achieving over 1dB gain in reconstruction and more than 6x compression ratio in modulated symbols.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 该论文研究了仅解码器Transformer中从离散提示到最后一个token隐藏状态的映射的注入性。理论分析表明，在温和条件下，这种映射在参数空间中通常是处处可逆的，且这种性质在训练过程中持续存在。实证研究通过几何诊断指标验证了预训练模型在完整精度下的注入性，并量化了4位量化对可逆性的影响。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer表示的可逆性对于解释模型行为、分析表示空间结构以及评估模型鲁棒性具有重要意义。现有工作表明离散提示到隐藏状态的映射通常是单射的，但缺乏对参数空间中这一性质的系统性分析以及量化评估方法。

Method: 理论部分：定义碰撞判别式和单射层，证明参数空间中的二分类性质，分析训练轨迹上的泛化单射性。实证部分：定义分离边界和共Lipschitz常数，通过最近邻统计在大规模提示集上估计这些几何诊断指标，应用于LLaMA-3、Qwen和GPT-2模型。

Result: 理论证明：在非奇异优化器和绝对连续初始化条件下，泛化单射性在平滑训练轨迹上持续存在。实证发现：完整精度和8位量化下未观察到碰撞，4位量化导致少量碰撞并显著降低共Lipschitz估计；小规模GPT-2训练中标准化指标保持稳定。

Conclusion: Transformer表示在连续参数理想化下通常是泛化且持续单射的，其实际可逆性可通过简单几何诊断进行探测。量化特别是4位量化会损害表示的可逆性，但模型在训练过程中保持稳定的几何特性。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [27] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: 本文提出了推导关系(DR)和推导能力(DC)的概念，用于评估LLMs基于输入变化进行相应输出修改的推理能力，并开发了DEVAL评估框架和推导提示(DP)方法。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在数据上的推理能力是一个重要研究问题，人类能够根据输入变化进行相应输出修改，但这种基于抽象规则的推理模式在LLMs中尚未得到全面描述和评估。

Method: 提出了DEVAL评估框架来系统评估推导能力，并在七个主流任务中测试了五个流行LLMs和一个大型推理模型。同时提出了推导提示(DP)方法。

Result: 主流LLMs如GPT-4o和Claude3.5表现出中等的DR识别能力，但在问题解决场景中应用DR时出现显著下降。推导提示方法使所有测试LLMs的DC平均提高了15.2%。

Conclusion: LLMs在推导能力方面存在不足，但通过推导提示方法可以显著改善其性能，这为提升LLMs的推理能力提供了有效途径。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [28] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 提出动态嵌套层次结构，通过自主调整优化层级、嵌套结构和更新频率来解决机器学习模型在非平稳环境中的适应性问题，实现真正的终身学习。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型（包括大语言模型）在静态任务中表现出色，但在非平稳环境中由于刚性架构而表现不佳，无法实现持续适应和终身学习。

Method: 基于嵌套学习范式，提出动态嵌套层次结构，允许模型在训练或推理过程中自主调整优化层级数量、嵌套结构和更新频率，无需预定义约束。

Result: 通过严格的数学公式、收敛性理论证明、表达能力边界分析，以及在语言建模、持续学习和长上下文推理中的实证演示，证明了该方法的优越性能。

Conclusion: 动态嵌套层次结构为实现自适应、通用智能奠定了基础性进展，解决了现有模型的前向遗忘问题，促进了真正的终身学习。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [29] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: 提出了GTPO算法，通过回合级奖励分配、基于回报的优势估计和自监督奖励塑造来解决多轮工具集成推理中传统RL方法奖励信号不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多轮工具集成推理任务中面临奖励信号粗粒度、学习信号不足的问题，导致训练停滞。

Method: GTPO算法包含三个关键创新：回合级奖励分配、基于回报的优势估计、自监督奖励塑造。

Result: 在多样化推理基准测试中，GTPO平均比GRPO性能提升3.0%。

Conclusion: GTPO算法能有效推进复杂数学推理在现实世界中的应用。

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [30] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: FinTRec是一个基于Transformer的金融推荐框架，解决了金融服务中长序列交互、多产品协调等挑战，相比传统树模型在性能和效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 金融服务中的推荐系统面临独特挑战：长范围用户交互（数字和物理渠道）、多产品协调需求、业务目标平衡，而传统树模型虽然可解释但性能有限。

Method: 提出FinTRec框架，基于Transformer架构，支持产品适配微调，实现跨产品信号共享，通过历史模拟和A/B测试验证。

Result: FinTRec在历史模拟和线上A/B测试中持续优于生产级树模型基线，统一架构减少了训练成本和技术债务，同时提升了所有产品的离线性能。

Conclusion: FinTRec证明了Transformer架构在金融服务推荐中的可行性和有效性，是首个全面解决技术和业务考量的统一序列推荐模型研究。

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [31] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: 提出基于Transformer的深度强化学习方法，用于优化eVTOL无人机起飞轨迹以最小化能耗，相比传统DRL训练效率提升75%，能耗优化精度达97.2%。


<details>
  <summary>Details</summary>
Motivation: eVTOL飞机有望缓解城市交通拥堵，但需要开发最优起飞轨迹以最小化能耗。传统最优控制方法受限于问题维度和复杂性，而深度强化学习虽然能处理复杂非线性系统，但训练难度大。

Method: 提出transformer-guided DRL方法，使用transformer在每个时间步探索真实状态空间，应用于eVTOL无人机起飞轨迹优化，通过调整功率和机翼角度来控制能耗。

Result: transformer-guided DRL仅需4.57×10^6时间步完成训练，是传统DRL所需时间步的25%；在能耗优化方面达到97.2%的精度，优于传统DRL的96.3%。

Conclusion: transformer-guided DRL在训练效率和最优设计验证方面均优于传统DRL，为eVTOL飞机能耗优化提供了有效解决方案。

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [32] [Bringing Federated Learning to Space](https://arxiv.org/abs/2511.14889)
*Grace Kim,Filip Svoboda,Nicholas Lane*

Main category: cs.LG

TL;DR: 本文首次系统分析了将联邦学习(FL)算法应用于卫星星座的可行性，提出了"空间化"框架来适应轨道约束，并在768种星座配置下验证了空间化FL算法可扩展到100颗卫星，性能接近集中式理想情况，训练周期从数月缩短到数天。


<details>
  <summary>Details</summary>
Motivation: 随着低地球轨道卫星星座扩展到数百数千颗卫星，下行带宽限制使得分布式星上机器学习变得至关重要。联邦学习为卫星网络协同模型训练提供了有前景的框架，但需要解决空间特有的约束条件。

Method: 引入全面的"空间化"框架，将地面算法(FedAvg、FedProx、FedBuff)适应轨道约束，创建轨道就绪的FL算法套件。通过768种星座配置的广泛参数扫描进行评估，包括集群大小、每集群卫星数和地面站网络的变化。

Result: 空间适应的FL算法可有效扩展到100颗卫星星座，性能接近集中式理想情况。通过轨道调度和卫星集群内本地协调，多个月训练周期可缩短到数天，实现9倍加速。

Conclusion: 研究结果为未来任务设计者提供了可行的见解，使分布式星上学习能够实现更自主、弹性和数据驱动的卫星操作。

Abstract: As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive "space-ification" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.

</details>


### [33] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: 提出了LIT框架，通过强制LLM使用外部可靠工具来解决复杂问题，提高解决方案的可信度和可调试性


<details>
  <summary>Details</summary>
Motivation: LLM的推理过程不透明，在高风险领域难以信任，且可能选择不可靠的解决方案，即使有更好的选择可用

Method: 基于现有LLM的工具调用能力构建框架，让LLM选择最可靠且易于调试的解决方案路径，可能涉及多个顺序工具调用

Result: 创建了包含1,300个问题的新基准数据集和可定制的可靠性成本函数，LLM在使用该框架时能实现更可靠和明智的问题解决，同时保持任务性能

Conclusion: LIT框架使LLM能够实现更可靠和可调试的问题解决，在高风险应用中具有重要价值

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [34] [Structured Contrastive Learning for Interpretable Latent Representations](https://arxiv.org/abs/2511.14920)
*Zhengyang Shen,Hua Tu,Mayue Shi*

Main category: cs.LG

TL;DR: 提出结构化对比学习(SCL)框架，通过将潜在空间划分为不变特征、变异特征和自由特征三组，解决神经网络对语义无关变换的脆弱性问题，实现同时具备鲁棒性和可解释性的表示学习。


<details>
  <summary>Details</summary>
Motivation: 神经网络对语义无关变换（如ECG相位偏移、IMU传感器旋转）表现出严重脆弱性，导致性能急剧下降。根本原因在于"放任式"表示学习，只要任务性能满足，潜在空间就无约束演化。

Method: 提出结构化对比学习(SCL)，将潜在空间表示划分为三个语义组：不变特征（在给定变换下保持一致）、变异特征（通过新颖变异机制主动区分变换）、自由特征（保持任务灵活性）。这种方法创建了可控的推拉动态，不同潜在维度服务于不同可解释目的。

Result: 在ECG相位不变性和IMU旋转鲁棒性实验中表现优异：ECG相似度从0.25提升到0.91，WISDM活动识别达到86.65%准确率和95.38%旋转一致性，始终优于传统数据增强方法。

Conclusion: 这项工作代表了从反应式数据增强到主动结构化学习的范式转变，使神经网络中的潜在表示具有可解释性，为构建更鲁棒和可解释的AI系统提供了新途径。

Abstract: Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as "laissez-faire" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.

</details>


### [35] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: 提出Causal-GCN框架，通过do-calculus后门调整识别对阿尔茨海默病进展具有稳定因果影响的大脑区域，解决传统图神经网络中人口统计学和遗传因素与疾病特征混淆的问题。


<details>
  <summary>Details</summary>
Motivation: 传统深度图学习方法在阿尔茨海默病MRI分类中存在混淆问题，将人口统计学和遗传因素与疾病特异性特征混为一谈，需要因果推理方法来识别真正具有因果影响的大脑区域。

Method: 构建结构连接组表示MRI，节点为皮层和皮层下区域，边编码解剖连接性。集成do-calculus后门调整，将年龄、性别和APOE4基因型等混杂因素通过主成分分析纳入因果调整集，通过切断传入边和改变节点特征来模拟对单个区域的干预。

Result: 在ADNI队列的484名受试者中，Causal-GCN性能与基线GNN相当，同时提供可解释的因果效应排名，突出显示后部、扣带回和岛叶枢纽，与已建立的AD神经病理学一致。

Conclusion: Causal-GCN框架成功识别了对阿尔茨海默病进展具有因果影响的大脑区域，为理解疾病机制提供了可解释的因果见解，同时保持了与传统方法的可比性能。

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [36] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文研究了按需采样中样本复杂度与轮数复杂度之间的权衡关系，在可实现的MDL中证明了r轮算法的最优样本复杂度约为dk^{Θ(1/r)}/ε，在不可知情况下提出了在O(√k)轮内达到接近最优样本复杂度O((d+k)/ε²)的算法，并引入了OODS框架来抽象样本自适应权衡。


<details>
  <summary>Details</summary>
Motivation: 研究多分布学习中样本复杂度与轮数复杂度之间的基本权衡关系，特别是在按需采样设置下，算法需要自适应地从k个分布中采样，但轮数有限。

Method: 提出了Optimization via On-Demand Sampling (OODS)框架来抽象样本自适应权衡，在可实现的MDL中分析了r轮算法的样本复杂度，在不可知情况下设计了在O(√k)轮内达到接近最优样本复杂度的算法。

Result: 在可实现的MDL中，r轮算法的最优样本复杂度为dk^{Θ(1/r)}/ε；在不可知情况下，提出了在O(√k)轮内达到样本复杂度O((d+k)/ε²)的算法；建立了OODS设置中轮数复杂度的紧界。

Conclusion: 实现了样本复杂度与轮数复杂度之间的近乎最优权衡，OODS框架捕获了大多数现有MDL算法，下界表明要获得亚多项式轮数复杂度需要绕过OODS固有硬度的新技术。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>


### [37] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: 本文首次系统比较了四种用于医院出院总结自动诊断编码的训练流程，发现在中等隐私预算下，从DP训练教师模型的知识蒸馏方法优于直接DP-SGD和DP合成数据训练，能恢复高达63%的非私有性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床文本训练中可能暴露敏感患者信息，但差分隐私方法通常会严重降低诊断准确性，不清楚哪种隐私保护策略在临床语言任务中效果最佳。

Method: 使用相同的1B参数模型和匹配的隐私预算，比较四种训练流程：直接DP-SGD、DP合成数据训练、知识蒸馏等，用于预测ICD-9诊断编码。

Result: 在中等隐私预算下，知识蒸馏方法表现最佳，能恢复63%的非私有性能，同时保持强大的经验隐私保护（成员推断AUC≈0.5）。

Conclusion: 知识蒸馏是实现隐私保护临床NLP的最实用途径，不同架构在隐私-效用权衡方面存在显著差异。

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [38] [CODE: A global approach to ODE dynamics learning](https://arxiv.org/abs/2511.15619)
*Nils Wildt,Daniel M. Tartakovsky,Sergey Oladyshkin,Wolfgang Nowak*

Main category: cs.LG

TL;DR: 提出了ChaosODE（CODE）方法，使用任意多项式混沌展开来表示ODE的右侧，在稀疏采样数据下学习动力学系统，相比神经网络和核方法具有更好的外推能力。


<details>
  <summary>Details</summary>
Motivation: 传统ODE建模需要密集测量数据，但实际中通常只能获得稀疏采样数据。现有数据驱动方法（如NeuralODE、KernelODE）在稀疏数据和噪声条件下外推能力不足。

Method: 使用任意多项式混沌展开（aPCE）来表示ODE的右侧，构建全局正交多项式表示动力学系统。

Result: 在Lotka-Volterra系统上的实验表明，CODE在不同噪声水平、初始条件和长期预测中表现出色，即使对未见过的初始条件也具有良好外推能力，优于NeuralODE和KernelODE。

Conclusion: CODE方法在稀疏数据和噪声条件下具有更强的鲁棒性和外推能力，为动力学学习问题提供了实用的优化指南。

Abstract: Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.

</details>


### [39] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: Graph Memory (GM) 是一个结构化非参数框架，通过区域级原型的关系记忆增强基于嵌入的推理，统一了实例检索、原型推理和图标签传播。


<details>
  <summary>Details</summary>
Motivation: 传统方法孤立处理训练实例，GM旨在通过总结嵌入空间为带可靠性指标的原型节点，编码几何和上下文关系，连接局部证据与全局一致性。

Method: GM将嵌入空间总结为带可靠性指标的原型节点，通过边编码几何和上下文关系，支持实例检索、原型推理和图标签传播的归纳模型。

Result: 在合成和真实数据集（包括乳腺组织病理学IDC）上，GM达到与kNN和Label Spreading相当的准确率，但校准更好、决策边界更平滑，且样本数量少一个数量级。

Conclusion: GM通过显式建模可靠性和关系结构，为非参数学习中局部证据与全局一致性提供了原则性桥梁。

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [40] [IonCast: A Deep Learning Framework for Forecasting Ionospheric Dynamics](https://arxiv.org/abs/2511.15004)
*Halil S. Kelebek,Linnea M. Wolniewicz,Michael D. Vergalla,Simone Mestici,Giacomo Acciarini,Bala Poduval,Olga Verkhoglyadova,Madhulika Guhathakurta,Thomas E. Berger,Frank Soboczenski,Atılım Güneş Baydin*

Main category: cs.LG

TL;DR: IonCast是一个基于深度学习的电离层预报模型套件，通过图神经网络和时空学习来预测全球总电子含量(TEC)，在风暴期和宁静期都表现出比持续性预报更好的性能。


<details>
  <summary>Details</summary>
Motivation: 电离层对GNSS精度、高频通信和航空运营至关重要，但准确预报和建模电离层变异性仍存在挑战，需要开发更有效的预测方法。

Method: 采用受GraphCast启发的图神经网络模型，结合时空学习技术，整合多种物理驱动因素和观测数据集来预测全球TEC。

Result: 在风暴期和宁静期的验证测试中，IonCast相比持续性预报显示出更好的预报技能。

Conclusion: 通过将异构数据与可扩展的基于图的时空学习相结合，IonCast展示了机器学习如何增强对电离层变异性的物理理解，并推进空间天气业务化韧性。

Abstract: The ionosphere is a critical component of near-Earth space, shaping GNSS accuracy, high-frequency communications, and aviation operations. For these reasons, accurate forecasting and modeling of ionospheric variability has become increasingly relevant. To address this gap, we present IonCast, a suite of deep learning models that include a GraphCast-inspired model tailored for ionospheric dynamics. IonCast leverages spatiotemporal learning to forecast global Total Electron Content (TEC), integrating diverse physical drivers and observational datasets. Validating on held-out storm-time and quiet conditions highlights improved skill compared to persistence. By unifying heterogeneous data with scalable graph-based spatiotemporal learning, IonCast demonstrates how machine learning can augment physical understanding of ionospheric variability and advance operational space weather resilience.

</details>


### [41] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 开发了一个模拟教室环境的动态时间序列系统，结合强化学习智能辅导系统，通过探测性干预来平衡学生状态估计与教学干扰，比较了RL算法与启发式方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统需要处理每个学生的独特性，而学习过程是部分可观测的，因此需要开发能够结合个体状态学习和群体信息的系统。

Method: 创建动态时间序列模拟环境，设计不同级别的探测干预措施，开发结合学生个体状态学习和群体信息的强化学习ITS，并与基于规则的启发式方法进行比较。

Result: RL算法和启发式方法提供不同解决方案但效果相似；探测干预能显著提升隐藏信息情况下的表现；两种方法对变化的学生群体分布都具灵活性，但RL在困难班级中表现较差；在测验和期中考试结构中比期末考试结构更能提升表现。

Conclusion: 探测性干预能有效平衡信息获取与教学干扰，强化学习和启发式方法在智能辅导系统中各有优势，课程结构设计对系统效果有重要影响。

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [42] [Oversampling techniques for predicting COVID-19 patient length of stay](https://arxiv.org/abs/2511.15048)
*Zachariah Farahany,Jiawei Wu,K M Sajjadul Islam,Praveen Madiraju*

Main category: cs.LG

TL;DR: 使用电子健康记录预测COVID-19患者住院时间长短，通过过采样处理数据不平衡问题，采用人工神经网络和贝叶斯优化进行模型训练。


<details>
  <summary>Details</summary>
Motivation: COVID-19症状严重程度差异大，部分高危患者住院时间长甚至死亡，需要预测患者病情严重程度以优化医疗资源分配。

Method: 使用电子健康记录数据，以住院时间为严重程度指标；通过过采样处理数据不平衡问题；采用人工神经网络模型，使用贝叶斯优化调整超参数。

Result: 选择F1分数最高的模型进行评估和讨论。

Conclusion: 该方法能够有效预测COVID-19患者的病情严重程度，为医疗决策提供支持。

Abstract: COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.

</details>


### [43] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 提出了一种结合局部和全局信息提取与注意力融合的框架，用于心律失常检测和分类，在MITDB和AFDB数据库上取得了优于基准模型的性能。


<details>
  <summary>Details</summary>
Motivation: 心律失常分类任务面临挑战，因为心律失常的持续时间各不相同，且发作时间变化，现有方法未充分考虑这些条件。

Method: 框架包含(i)局部和全局特征提取，(ii)基于注意力的局部-全局信息融合，能够在受限输入长度内进行心律失常检测和分类。

Result: 在MITDB和AFDB数据库上，持续时间、事件和Dice分数的F1得分分别为96.45%/82.05%/96.31%和97.57%/98.31%/97.45%，性能优于基准模型。

Conclusion: 该方法能有效捕获局部和全局信息及动态变化，无显著信息损失，可更准确地检测心律失常并精确定位发作时间，有助于制定更精确的治疗方案。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [44] [Deep Pathomic Learning Defines Prognostic Subtypes and Molecular Drivers in Colorectal Cancer](https://arxiv.org/abs/2511.15067)
*Zisong Wang,Xuanyu Wang,Hang Chen,Haizhou Wang,Yuxin Chen,Yihang Xu,Yunhe Yuan,Lihuan Luo,Xitong Ling,Xiaoping Liu*

Main category: cs.LG

TL;DR: 开发了基于病理全切片图像的TDAM-CRC多实例学习模型，用于结直肠癌预后预测，通过多组学分析揭示了代谢重编程和免疫抑制微环境的分子机制，并识别了MRPL37作为关键预后生物标志物。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌具有高度异质性，传统的TNM分期系统无法满足个性化医疗需求，需要开发更准确的预后预测工具。

Method: 使用TCGA队列（n=581）训练TDAM-CRC模型，在独立外部队列（n=1031）验证，整合多组学数据提高模型可解释性并识别预后生物标志物。

Result: TDAM-CRC在两个队列中均实现稳健的风险分层，预测性能显著优于传统临床分期系统和现有先进模型，高风险亚型与代谢重编程和免疫抑制微环境相关，MRPL37被识别为关键预后基因。

Conclusion: TDAM-CRC为结直肠癌提供了改进的风险分层工具，揭示了新的分子靶点，并促进了个性化临床决策。

Abstract: Precise prognostic stratification of colorectal cancer (CRC) remains a major clinical challenge due to its high heterogeneity. The conventional TNM staging system is inadequate for personalized medicine. We aimed to develop and validate a novel multiple instance learning model TDAM-CRC using histopathological whole-slide images for accurate prognostic prediction and to uncover its underlying molecular mechanisms. We trained the model on the TCGA discovery cohort (n=581), validated it in an independent external cohort (n=1031), and further we integrated multi-omics data to improve model interpretability and identify novel prognostic biomarkers. The results demonstrated that the TDAM-CRC achieved robust risk stratification in both cohorts. Its predictive performance significantly outperformed the conventional clinical staging system and multiple state-of-the-art models. The TDAM-CRC risk score was confirmed as an independent prognostic factor in multivariable analysis. Multi-omics analysis revealed that the high-risk subtype is closely associated with metabolic reprogramming and an immunosuppressive tumor microenvironment. Through interaction network analysis, we identified and validated Mitochondrial Ribosomal Protein L37 (MRPL37) as a key hub gene linking deep pathomic features to clinical prognosis. We found that high expression of MRPL37, driven by promoter hypomethylation, serves as an independent biomarker of favorable prognosis. Finally, we constructed a nomogram incorporating the TDAM-CRC risk score and clinical factors to provide a precise and interpretable clinical decision-making tool for CRC patients. Our AI-driven pathological model TDAM-CRC provides a robust tool for improved CRC risk stratification, reveals new molecular targets, and facilitates personalized clinical decision-making.

</details>


### [45] [Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection](https://arxiv.org/abs/2511.15083)
*Xiancheng Wang,Lin Wang,Rui Wang,Zhibo Zhang,Minghang Zhao*

Main category: cs.LG

TL;DR: 提出Fourier-KAN-Mamba混合架构，结合傅里叶层、KAN网络和Mamba状态空间模型，用于时间序列异常检测，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: Mamba模型在长序列建模中表现出色，但直接应用于异常检测任务时难以捕捉复杂的时间模式和非线性动态。

Method: 集成傅里叶层提取多尺度频率特征，KAN增强非线性表示能力，并引入时间门控控制机制来区分正常和异常模式。

Result: 在MSL、SMAP和SWaT数据集上的广泛实验表明，该方法显著优于现有的最先进方法。

Conclusion: Fourier-KAN-Mamba混合架构通过结合频率分析、非线性建模和选择性状态空间模型，有效提升了时间序列异常检测的性能。

Abstract: Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches.
  Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network

</details>


### [46] [Semiconductor Industry Trend Prediction with Event Intervention Based on LSTM Model in Sentiment-Enhanced Time Series Data](https://arxiv.org/abs/2511.15112)
*Wei-hsiang Yen,Lyn Chao-ling Chen*

Main category: cs.LG

TL;DR: 该研究将深度学习方法与情感分析整合到传统商业模式分析中，以台积电为研究对象预测台湾半导体行业趋势。结合文本情感分析和时间序列数据，使用LSTM模型进行行业趋势预测。


<details>
  <summary>Details</summary>
Motivation: 半导体行业市场变化快速，传统数据分析方法在处理高变化性和时间序列数据时表现不佳。需要结合内部公司事件和外部全球事件的情感分析来提升预测准确性。

Method: 收集台积电季度报告的文本数据和时间序列数据，通过情感分析考虑内部和外部事件干预，使用情感增强的时间序列数据训练LSTM模型进行行业趋势预测。

Result: 预测结果揭示了台积电晶圆技术的显著发展和全球市场的潜在威胁，与台积电产品发布新闻和国际新闻相符。

Conclusion: 该工作通过考虑内外事件干预，在半导体行业趋势预测方面表现准确，为研究和商业领域提供了有价值的半导体行业信息。

Abstract: The innovation of the study is that the deep learning method and sentiment analysis are integrated in traditional business model analysis and forecasting, and the research subject is TSMC for industry trend prediction of semiconductor industry in Taiwan. For the rapid market changes and development of wafer technologies of semiconductor industry, traditional data analysis methods not perform well in the high variety and time series data. Textual data and time series data were collected from seasonal reports of TSMC including financial information. Textual data through sentiment analysis by considering the event intervention both from internal events of the company and the external global events. Using the sentiment-enhanced time series data, the LSTM model was adopted for predicting industry trend of TSMC. The prediction results reveal significant development of wafer technology of TSMC and the potential threatens in the global market, and matches the product released news of TSMC and the international news. The contribution of the work performed accurately in industry trend prediction of the semiconductor industry by considering both the internal and external event intervention, and the prediction results provide valuable information of semiconductor industry both in research and business aspects.

</details>


### [47] [Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling](https://arxiv.org/abs/2511.15125)
*Huifan Zhang,Pingqiang Zhou*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的贝叶斯在线学习框架，用于高效建模RF无源组件，相比传统基于机器学习的方法仅需2.86%的电磁仿真时间，实现了35倍加速。


<details>
  <summary>Details</summary>
Motivation: 传统基于机器学习的RF无源组件建模需要大量电磁仿真来覆盖几何和频率设计空间，造成计算瓶颈。

Method: 1) 具有可重构头部的贝叶斯神经网络，用于联合几何-频率域建模并量化不确定性；2) 自适应采样策略，利用不确定性指导同时优化几何参数和频率域的训练数据采样。

Result: 在三个RF无源组件上验证，该框架实现了精确建模，同时仅使用传统基于机器学习流程2.86%的电磁仿真时间。

Conclusion: 该不确定性感知贝叶斯在线学习框架显著提高了RF无源组件参数化建模的效率，实现了35倍的加速效果。

Abstract: Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.

</details>


### [48] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 提出了一种新的稀疏矩阵乘法算法，能够将自组织映射应用于整个Medline数据集，解决了现有算法内存和处理需求指数增长的限制。


<details>
  <summary>Details</summary>
Motivation: 过去由于现有算法的内存和处理需求呈指数增长，Medline数据库的映射工作仅限于可用数据的小子集，无法充分利用完整数据集。

Method: 设计了一种新颖的稀疏矩阵乘法算法，使自组织映射能够应用于整个Medline数据集。

Result: 成功创建了更完整的医学知识地图，并提高了随时间更新自组织映射的可行性。

Conclusion: 该算法突破了现有技术的限制，为全面映射医学知识提供了可行方案，并支持随时间变化的动态更新。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [49] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: 提出GRPO-Verif算法，通过统一损失函数联合优化解答生成和自我验证能力，可调节验证信号权重，在保持推理性能的同时增强自我验证能力。


<details>
  <summary>Details</summary>
Motivation: 尽管通过强化学习显著提升了大型语言模型的推理能力，但它们仍难以一致地验证自己的推理过程，因此需要研究如何增强LLMs的自我验证能力以及这种能力是否能进一步改善推理性能。

Method: 提出GRPO-Verif算法，在统一损失函数中联合优化解答生成和自我验证，使用可调节超参数控制验证信号的权重。

Result: 实验结果表明，该方法在保持推理性能的同时增强了自我验证能力。

Conclusion: GRPO-Verif算法能够有效提升大型语言模型的自我验证能力，同时维持推理性能，为解决LLMs自我验证不一致的问题提供了可行方案。

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [50] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 提出了一种不确定性感知的主动学习框架，通过联合利用模型不确定性和跨模态一致性来增强对标签噪声的鲁棒性，用于EEG情感识别。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪影和个体差异影响，情感标签通常来自主观且不一致的报告，使得稳健的情感解码特别困难。需要一种对噪声标签具有鲁棒性的方法。

Method: 使用不确定性感知主动学习框架，通过表示对齐模块将EEG和面部特征嵌入共享潜在空间，评估跨模态对齐来确定不确定性来源，并选择性查询oracle反馈。

Result: 在ASCERTAIN数据集上的实验证明了该方法的效率和鲁棒性，显示出作为脑机接口系统中数据高效且噪声容忍的EEG情感解码方法的潜力。

Conclusion: 该方法通过联合利用模型不确定性和跨模态一致性，提供了一种数据高效且对噪声标签具有鲁棒性的EEG情感解码解决方案。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [51] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 本文提出了一种针对复数变分自编码器的Kähler几何结构方法，通过Fisher信息度量和Kähler势函数来正则化潜在空间，实现了更平滑的表示和更少的语义异常值。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现欧几里得变分自编码器具有黎曼几何结构，但复数VAEs的几何结构研究较少。本文旨在探索复数VAEs中的Kähler几何结构，特别是解码器几何。

Method: 在复数高斯正则化下推导Fisher信息度量，利用KL散度的Hessian与Fisher信息的等价关系，提出复数高斯混合的Kähler势函数导数，通过PSH函数高效计算度量。

Result: 提出的方法能够通过解码器几何正则化潜在空间，按照加权的复数体积元素进行采样，在牺牲样本变异性的情况下获得更平滑的表示和更少的语义异常值。

Conclusion: 复数VAEs确实展现了一定程度的Kähler几何结构，通过Kähler势函数可以高效计算几何度量，实现潜在空间的正则化和改进的表示学习。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [52] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 提出基于扩散模型的少样本故障时间序列生成框架，通过正负差异适配器和多样性损失解决故障数据稀缺问题，在真实性和多样性方面显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 工业设备监控中故障数据稀缺，现有时间序列生成模型难以在少样本场景下准确捕捉故障分布，生成的样本缺乏真实性和多样性。

Method: 使用扩散模型框架，引入正负差异适配器利用预训练的正常数据分布建模正常与故障域之间的差异，并采用多样性损失防止模式崩溃。

Result: 实验结果表明该模型在真实性和多样性方面显著优于传统方法，在关键基准测试中达到最先进性能。

Conclusion: 提出的少样本故障时间序列生成框架有效解决了故障数据稀缺问题，能够生成真实且多样的故障样本，为工业设备故障诊断提供了有力支持。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [53] [Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning](https://arxiv.org/abs/2511.15175)
*Le Tung Giang,Vu Hoang Viet,Nguyen Xuan Tung,Trinh Van Chien,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出了一种量子图注意力网络(Q-GAT)，在深度强化学习框架中用参数化量子电路替代传统MLP，用于解决车辆路径问题，减少了50%以上可训练参数，并实现了更快的收敛和约5%的路由成本降低。


<details>
  <summary>Details</summary>
Motivation: 传统基于图神经网络的深度强化学习方法依赖参数繁多的多层感知机，存在内存限制问题，需要开发更紧凑有效的模型来解决车辆路径问题。

Method: 在DRL框架中构建量子图注意力网络，用参数化量子电路替代传统MLP在关键读出阶段，结合近端策略优化和贪心/随机解码策略。

Result: 在VRP基准测试中，Q-GAT相比经典GAT基线减少了50%以上可训练参数，收敛速度更快，路由成本降低约5%。

Conclusion: 参数化量子电路增强的图神经网络可作为紧凑有效的求解器，适用于大规模路由和物流优化问题。

Abstract: The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.

</details>


### [54] [KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials](https://arxiv.org/abs/2511.15327)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出了KrawtchoukNet，一种基于离散Krawtchouk多项式的GNN滤波器，解决了谱图神经网络在异配图和过度平滑问题上的性能崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于多项式滤波器的谱图神经网络存在两个关键限制：1)在异配图上性能崩溃；2)在高多项式度数时出现过度平滑。这些问题源于标准滤波器的静态低通特性。

Method: 使用离散Krawtchouk多项式构建GNN滤波器，通过将多项式域N固定为小常数实现固有有界递归系数，并通过可学习的形状参数p使滤波器能够自适应地调整其频谱响应。

Result: KrawtchoukNet在过度平滑问题上表现出色（在K=10时达到SOTA结果），在异配图基准测试（Texas、Cornell）上显著优于标准GNN如GAT和APPNP。

Conclusion: KrawtchoukNet通过有界递归系数和自适应频谱响应，为谱图神经网络的异配图和过度平滑问题提供了统一解决方案。

Abstract: Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on "heterophilic" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.

</details>


### [55] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVAL是一个基于蒸馏的框架，将掩码自回归扩散模型的扩散链压缩为单个AR生成步骤，实现30倍以上的推理加速，同时保持样本质量，并支持强化学习后训练。


<details>
  <summary>Details</summary>
Motivation: 解决传统掩码自回归扩散模型因分层推理机制导致的推理速度慢问题，使其能够实际应用于强化学习后训练。

Method: 提出基于分数的变分目标进行蒸馏，将扩散链压缩为单步生成；开发MARVAL-RL框架实现掩码自回归模型的高效强化学习。

Result: 在ImageNet 256*256上，MARVAL-Huge达到FID 2.00，相比MAR-diffusion加速30倍以上；MARVAL-RL在CLIP和图像奖励分数上持续改进。

Conclusion: MARVAL展示了掩码自回归扩散模型蒸馏和强化学习的首个实用路径，实现快速采样和更好的偏好对齐。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [56] [LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials](https://arxiv.org/abs/2511.15328)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出了LaguerreNet，一种基于连续拉盖尔多项式的新型GNN滤波器，通过可训练的alpha参数学习滤波器谱形状，解决了异质图性能差和多项式度高时过平滑的问题。


<details>
  <summary>Details</summary>
Motivation: 传统谱图神经网络在异质图上表现差，且在高多项式度时会出现过平滑问题。现有自适应多项式滤波器在连续域扩展和系数无界稳定性方面存在问题。

Method: 使用连续拉盖尔多项式构建GNN滤波器，将核心alpha参数设为可训练以学习谱形状，并通过LayerNorm稳定化技术解决无界多项式的数值不稳定性。

Result: LaguerreNet在异质图基准测试中达到最先进结果，对过平滑具有极强鲁棒性，性能在K=10时达到峰值，比ChebyNet崩溃点高一个数量级。

Conclusion: LaguerreNet通过连续拉盖尔多项式和可训练参数成功解决了谱GNN的两个关键问题，在异质图性能和过平滑鲁棒性方面表现优异。

Abstract: Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on "heterophilic" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.

</details>


### [57] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: 提出了ATPO方法，通过动态识别和重分配梯度更新到轨迹中的关键步骤（"困惑区"），显著提升扩散大语言模型的推理准确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的强化学习方法均匀分配策略梯度到所有去噪步骤，隐含假设所有步骤同等重要。作者挑战这一假设，发现轨迹中存在结构化的"困惑区"——不确定性和不稳定的短暂峰值，这些区域强烈预测最终成功或失败。

Method: 提出了自适应轨迹策略优化（ATPO），使用混合RoEC+CM规则动态识别高杠杆步骤，在不改变RL目标、奖励或计算预算的情况下，将梯度更新重新分配到这些关键步骤。

Result: ATPO在多个基准测试中显著提升了推理准确性和训练稳定性，证明了利用轨迹动态是推进dLLM RL的关键。

Conclusion: 通过分析轨迹动态并针对性优化关键步骤，可以显著提升扩散大语言模型的强化学习性能，这种方法比均匀分配梯度更有效。

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [58] [D2D Power Allocation via Quantum Graph Neural Network](https://arxiv.org/abs/2511.15246)
*Tung Giang Le,Xuan Tung Nguyen,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 提出一种完全量子图神经网络(QGNN)，使用参数化量子电路实现消息传递，用于无线网络资源管理，在保持性能的同时减少参数数量并实现并行处理。


<details>
  <summary>Details</summary>
Motivation: 无线网络复杂性日益增加需要可扩展的资源管理方法，经典图神经网络在图学习方面表现出色但在大规模场景下计算成本高昂。

Method: 通过量子图卷积层(QGCLs)将特征编码为量子态，使用NISQ兼容的酉变换处理图结构，通过测量获取嵌入表示。

Result: 在D2D功率控制用于SINR最大化任务中，QGNN与经典方法性能相当，但参数更少且具有固有并行性。

Conclusion: 这种基于PQC的端到端GNN标志着向量子加速无线优化迈出了一步。

Abstract: Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.

</details>


### [59] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 提出EntroPIC方法，通过比例积分控制动态调整正负样本的损失系数，稳定大语言模型强化学习训练中的熵值，确保有效探索和稳定进展。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以在训练过程中维持适当的熵水平，因为正负样本在不同步骤中对熵的影响方式不同，导致模型可能陷入次优行为。

Method: EntroPIC方法使用比例积分控制机制，自适应地调整正负样本的损失系数，从而稳定训练过程中的熵值。

Result: 实验结果显示该方法能成功维持期望的熵水平，实现大语言模型稳定且最优的强化学习训练。

Conclusion: EntroPIC方法通过动态调整损失系数有效控制熵值，为大语言模型的长期稳定训练提供了有效解决方案。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [60] [Optimized scheduling of electricity-heat cooperative system considering wind energy consumption and peak shaving and valley filling](https://arxiv.org/abs/2511.15250)
*Jin Ye,Lingmei Wang,Shujian Zhang,Haihang WU*

Main category: cs.LG

TL;DR: 提出基于改进PVTD3算法的电热联合系统智能调度方法，在可再生能源接入和多重不确定性下优化系统运行，显著降低综合成本和电网购电波动。


<details>
  <summary>Details</summary>
Motivation: 随着全球能源转型和可再生能源快速发展，电热联合系统在新能源接入和多重不确定性下的调度优化挑战日益突出。

Method: 采用改进的双延迟深度确定性策略梯度（PVTD3）算法，通过引入电网购电变化惩罚项实现系统优化。

Result: 在10%、20%、30%可再生能源渗透率三种典型场景下，PVTD3算法相比传统TD3算法分别降低系统综合成本6.93%、12.68%、13.59%，平均降低电网购电波动幅度12.8%。在储能管理方面，低温储热罐终态值降低7.67-17.67单位，高温储热罐维持在3.59-4.25安全运行范围。

Conclusion: 多场景对比验证表明，所提算法在经济性、电网稳定性和储能设备管理的可持续调度能力方面均表现出优越性能。

Abstract: With the global energy transition and rapid development of renewable energy, the scheduling optimization challenge for combined power-heat systems under new energy integration and multiple uncertainties has become increasingly prominent. Addressing this challenge, this study proposes an intelligent scheduling method based on the improved Dual-Delay Deep Deterministic Policy Gradient (PVTD3) algorithm. System optimization is achieved by introducing a penalty term for grid power purchase variations. Simulation results demonstrate that under three typical scenarios (10%, 20%, and 30% renewable penetration), the PVTD3 algorithm reduces the system's comprehensive cost by 6.93%, 12.68%, and 13.59% respectively compared to the traditional TD3 algorithm. Concurrently, it reduces the average fluctuation amplitude of grid power purchases by 12.8%. Regarding energy storage management, the PVTD3 algorithm reduces the end-time state values of low-temperature thermal storage tanks by 7.67-17.67 units while maintaining high-temperature tanks within the 3.59-4.25 safety operating range. Multi-scenario comparative validation demonstrates that the proposed algorithm not only excels in economic efficiency and grid stability but also exhibits superior sustainable scheduling capabilities in energy storage device management.

</details>


### [61] [PLATONT: Learning a Platonic Representation for Unified Network Tomography](https://arxiv.org/abs/2511.15251)
*Chengze Du,Heng Xu,Zhiwei Yu,Bo Liu,Jialong Li*

Main category: cs.LG

TL;DR: PLATONT是一个统一的网络层析成像框架，通过将不同网络指标建模为共享潜在网络状态的投影，实现多任务学习，提高跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有网络层析方法通常单独解决不同问题，依赖有限的任务特定信号，这限制了方法的泛化性和可解释性。

Method: 基于柏拉图表示假说，通过多模态对齐和对比学习学习共享潜在网络状态，在共享潜在空间中训练多个层析任务。

Result: 在合成和真实数据集上的实验表明，PLATONT在链路估计、拓扑推断和流量预测方面始终优于现有方法，在不同网络条件下具有更高的准确性和更强的鲁棒性。

Conclusion: PLATONT通过构建紧凑且结构化的表示，实现了网络层析任务的统一建模，显著提升了跨任务泛化性能。

Abstract: Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.

</details>


### [62] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 提出了GRPO-RM方法，将GRPO从语言模型扩展到表示学习模型，通过预定义输出集和专门设计的奖励函数来优化表示模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索GRPO强化学习方法能否从语言模型推广到表示学习模型，解决表示模型的后训练优化问题。

Method: 建立预定义输出集替代语言模型中的token序列采样，生成输出组用于GRPO的概率驱动优化，并设计专门的奖励函数适应表示模型特性。

Result: 在多个真实世界数据集上进行广泛实验，验证了所提方法的有效性。

Conclusion: GRPO-RM成功将GRPO方法扩展到表示学习领域，为表示模型的后训练优化提供了有效解决方案。

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [63] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: SNAP是一个稀疏测试时适应框架，通过减少适应频率和数据使用量，在边缘设备上实现高效测试时适应，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法需要频繁适应和高计算成本，不适合资源受限的边缘环境。

Method: 提出两个关键组件：CnDRM（类和域代表性记忆）存储代表性样本支持有限数据下的高效适应；IoBMN（仅推理批量感知记忆归一化）利用这些样本动态调整归一化统计量。

Result: 与五种最先进TTA算法集成，SNAP将延迟降低高达93.12%，精度下降保持在3.3%以下，即使适应率仅为1%-50%。

Conclusion: SNAP在边缘设备上具有强大的实际应用潜力，特别适合延迟敏感的应用场景。

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [64] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: Quant-Trim是一种训练阶段方法，通过渐进式伪量化和反向剪枝技术，生成硬件无关的检查点，解决边缘加速器因不同厂商编译器量化实现差异导致的精度不一致问题。


<details>
  <summary>Details</summary>
Motivation: 边缘加速器依赖低比特量化，但不同厂商编译器在缩放、裁剪和内核支持方面存在差异，导致相同浮点检查点在不同后端产生不一致的精度，迫使开发者调整参数或重构模型以适应特定厂商。

Method: 结合渐进式伪量化（使训练与部署的整数网格对齐）和反向剪枝（控制异常值驱动的尺度膨胀同时保持可学习性），生成硬件无关的检查点。

Result: Quant-Trim缩小了浮点与低比特之间的精度差距，减少了对编译器启发式/校准的依赖，避免了针对每个后端的重新训练，并在各种量化方案下保持良好性能。

Conclusion: 该方法提供了一种硬件中立的解决方案，能够应对不同后端和精度选择，提高了边缘部署的鲁棒性和效率。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [65] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型的概念可解释性，发现早期层主要编码局部时间域模式，深层编码离散度和变化时间信号，但概念组合时存在干扰问题。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在经验上取得成功，但其内部如何表示基本时间序列概念的机制仍不清楚，需要系统研究其概念可解释性。

Method: 使用分层分析、线性可恢复性测试和表示相似性度量等方法，系统探究概念在模型中的编码位置、线性可恢复性、表示演化以及概念组合处理。

Result: 早期层主要捕获局部时间域模式（如AR(1)、水平偏移、趋势），深层编码离散度和变化时间信号，谱和扭曲因子最难线性恢复。概念组合时探针性能下降，显示概念间存在干扰。

Conclusion: 虽然原子概念能可靠定位，但概念组合仍是当前TSFMs的关键限制，表明模型在表示交互时间现象方面存在不足。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [66] [STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection](https://arxiv.org/abs/2511.15339)
*Kadir-Kaan Özer,René Ebeling,Markus Enzweiler*

Main category: cs.LG

TL;DR: STREAM-VAE是一个用于汽车遥测时间序列异常检测的变分自编码器，通过双路径编码器分离慢漂移和快尖峰信号动态，提高异常检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 汽车遥测数据同时存在慢漂移和快尖峰，标准重建方法使用单一潜在过程会混合不同时间尺度，导致尖峰被平滑或方差膨胀，削弱异常分离能力。

Method: 使用双路径编码器分离慢漂移和快尖峰信号动态，解码器将瞬态偏差与正常操作模式分开表示，专为部署设计，能在不同操作模式下产生稳定的异常分数。

Result: 在汽车遥测数据集和公开SMD基准测试上的实验表明，显式分离漂移和尖峰动态相比强基线方法（预测、注意力、图和VAE）提高了鲁棒性。

Conclusion: STREAM-VAE通过分离不同时间尺度的信号动态，有效提升了汽车遥测数据异常检测的性能和部署稳定性。

Abstract: Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.
  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.
  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.

</details>


### [67] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 本文系统评估了33种时间序列预测集成模型，发现堆叠方法能持续提升精度，并提出多层堆叠框架以在不同任务中获得最优性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测中集成方法未被充分利用，简单线性组合仍被视为最优方法，需要系统探索更有效的集成策略。

Method: 评估33种集成模型（包括现有和新提出的），在50个真实数据集上进行测试，并提出多层堆叠框架来结合不同堆叠模型的优势。

Result: 堆叠方法能持续提高预测精度，但没有单一堆叠器在所有任务中表现最佳，多层堆叠框架在不同预测场景中都能提供优越的准确性。

Conclusion: 基于堆叠的方法有潜力改进时间序列预测的AutoML系统，多层堆叠框架是有效的解决方案。

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [68] [Cost-Aware Prediction (CAP): An LLM-Enhanced Machine Learning Pipeline and Decision Support System for Heart Failure Mortality Prediction](https://arxiv.org/abs/2511.15357)
*Yinan Yu,Falk Dippel,Christina E. Lundberg,Martin Lindgren,Annika Rosengren,Martin Adiels,Helen Sjöland*

Main category: cs.LG

TL;DR: 该论文提出了成本感知预测(CAP)框架，结合成本效益分析和LLM代理，为心力衰竭患者的家庭护理资格预测提供更透明的决策支持。


<details>
  <summary>Details</summary>
Motivation: 机器学习预测模型通常未考虑下游价值权衡和临床可解释性，需要开发能沟通预测应用权衡的框架。

Method: 开发预测心力衰竭患者1年死亡率的ML模型，引入临床影响投影(CIP)曲线可视化成本维度，使用四个LLM代理生成患者特定描述，并由临床医生评估系统价值。

Result: XGBoost模型表现最佳(AUROC=0.804)，CIP曲线提供群体级成本组成概览，LLM生成个体级成本效益分析，系统获得临床医生良好评价但需加强技术准确性。

Conclusion: CAP利用LLM代理整合ML分类器结果和成本效益分析，为临床决策提供更透明和可解释的支持。

Abstract: Objective: Machine learning (ML) predictive models are often developed without considering downstream value trade-offs and clinical interpretability. This paper introduces a cost-aware prediction (CAP) framework that combines cost-benefit analysis assisted by large language model (LLM) agents to communicate the trade-offs involved in applying ML predictions. Materials and Methods: We developed an ML model predicting 1-year mortality in patients with heart failure (N = 30,021, 22% mortality) to identify those eligible for home care. We then introduced clinical impact projection (CIP) curves to visualize important cost dimensions - quality of life and healthcare provider expenses, further divided into treatment and error costs, to assess the clinical consequences of predictions. Finally, we used four LLM agents to generate patient-specific descriptions. The system was evaluated by clinicians for its decision support value. Results: The eXtreme gradient boosting (XGB) model achieved the best performance, with an area under the receiver operating characteristic curve (AUROC) of 0.804 (95% confidence interval (CI) 0.792-0.816), area under the precision-recall curve (AUPRC) of 0.529 (95% CI 0.502-0.558) and a Brier score of 0.135 (95% CI 0.130-0.140). Discussion: The CIP cost curves provided a population-level overview of cost composition across decision thresholds, whereas LLM-generated cost-benefit analysis at individual patient-levels. The system was well received according to the evaluation by clinicians. However, feedback emphasizes the need to strengthen the technical accuracy for speculative tasks. Conclusion: CAP utilizes LLM agents to integrate ML classifier outcomes and cost-benefit analysis for more transparent and interpretable decision support.

</details>


### [69] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 提出了一种新的后处理局部特征重要性方法CID，通过生成正负反事实样本、使用核密度估计建模分布，并基于分布差异度量对特征进行排序，该方法在忠实性指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 机器学习中评估单个特征重要性对于理解模型决策过程至关重要，但由于缺乏明确的真实基准，需要开发替代的、有理论基础的重要性度量方法。

Method: CID方法生成正负反事实样本集，使用核密度估计建模其分布，并基于分布差异度量对特征进行排序，该度量具有严格的数学基础并满足有效度量的关键属性。

Result: 与现有局部特征重要性解释方法相比，CID方法不仅提供了互补视角，还在忠实性指标（全面性和充分性）上表现更好，能提供更忠实的系统解释。

Conclusion: CID方法作为一种有价值的模型分析工具具有显著潜力，能够提供更准确的特征重要性解释。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [70] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: PIECE是一种基于参数重要性估计的持续增强方法，通过选择性更新仅0.1%的核心参数，在保持通用能力的同时高效学习领域知识，无需访问历史数据或增加模型参数。


<details>
  <summary>Details</summary>
Motivation: 领域特定后训练通常会导致灾难性遗忘，使基础模型失去通用推理能力，限制其在动态现实环境中的适应性。在获取下游领域知识的同时保持通用能力是大型语言和多模态模型的核心挑战。

Method: PIECE方法使用两个重要性估计器：基于Fisher信息的PIECE-F和基于二阶归一化的PIECE-S（结合梯度和曲率信息），选择性更新仅0.1%与任务最相关的核心参数。

Result: 在三个语言模型和两个多模态模型上的实验表明，PIECE保持了通用能力，并在各种下游任务中实现了最先进的持续学习性能。

Conclusion: PIECE为构建可扩展、领域自适应且无灾难性遗忘的基础模型提供了一条实用路径。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [71] [EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG](https://arxiv.org/abs/2511.15393)
*Kunyu Zhang,Mingxuan Wang,Xiangjie Shi,Haoxing Xu,Chao Zhang*

Main category: cs.LG

TL;DR: EVA-Net是一个可解释的脑年龄异常检测框架，使用稀疏注意力Transformer处理长EEG序列，通过变分信息瓶颈学习鲁棒表示，并与连续原型网络对齐来学习健康衰老流形。


<details>
  <summary>Details</summary>
Motivation: 现有脑年龄模型在处理不完美医疗数据（如仅从健康队列学习"正常"基线）方面存在困难，且标准模型通常是缺乏可解释结构的黑箱。

Method: 使用稀疏注意力Transformer建模长EEG序列，采用变分信息瓶颈处理噪声和变异性，通过连续原型网络学习健康衰老流形以实现可解释性。

Result: 在1297名健康受试者上训练，达到最先进精度；在27名MCI和AD患者队列验证中，病理组显示显著更高的脑年龄差距和原型对齐误差。

Conclusion: EVA-Net为使用不完美医疗数据的医疗智能提供了一个可解释框架。

Abstract: The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.

</details>


### [72] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 提出一类用于非线性非高斯状态空间模型状态估计的算法，基于变分拉格朗日框架，将贝叶斯推断转化为带动态约束的熵信任区域更新序列。


<details>
  <summary>Details</summary>
Motivation: 解决非线性非高斯状态空间模型中的状态估计问题，传统方法在处理此类复杂模型时存在局限性。

Method: 采用变分拉格朗日公式，将贝叶斯推断转化为序列化的熵信任区域更新，结合动态约束。针对高斯-马尔可夫近似推导递归方案，对一般非线性非高斯模型使用广义统计线性回归和傅里叶-埃尔米特矩匹配来闭合递归。

Result: 开发出具有良好计算复杂度的前向-后向算法家族，结构由变分后验的因子分解决定。

Conclusion: 该框架为非线性非高斯状态估计提供了一类有效的算法，能够处理复杂的状态空间模型。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [73] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 分析表格ICL模型中各层的作用，发现存在结构冗余，为模型压缩和可解释性提供机会


<details>
  <summary>Details</summary>
Motivation: 尽管表格ICL模型与大型语言模型架构相似，但对其各层在表格预测中的贡献了解甚少

Method: 通过"层作为画家"的视角分析TabPFN和TabICL，研究各层潜在空间的演化

Result: 只有部分层共享共同表示语言，表明存在结构冗余

Conclusion: 表格ICL模型存在层间冗余，这为模型压缩和提升可解释性提供了可能性

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [74] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 提出了一种基于时间序列基础模型（TSFM）的上下文学习分类方法，无需微调模型即可对训练数据之外的数据进行分类，应用于轴承健康状态评估。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要微调模型的问题，利用预训练模型的可扩展性，从定制化窄AI解决方案向更广泛的AI驱动维护系统发展。

Method: 将频域参考信号转换为伪时间序列模式，生成对齐的协变量和目标信号，通过TSFM预测分类数据与预定义标签的对应概率。

Result: 该方法在不同操作条件下都表现出良好效果，展示了预训练模型的可扩展性优势。

Conclusion: 该方法标志着从定制化窄AI解决方案向更广泛AI驱动维护系统的重大进展，无需微调即可实现有效分类。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [75] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: FairEnergy是一个联邦学习框架，通过联合优化设备选择、带宽分配和压缩级别，在保证模型精度的同时显著降低能耗并提高公平性。


<details>
  <summary>Details</summary>
Motivation: 在无线边缘系统中，联邦学习面临异构资源、不公平客户端贡献和有限通信容量的挑战，需要在保持数据隐私的同时平衡能源效率和公平参与。

Method: 提出FairEnergy框架，将包含更新幅度和压缩率的贡献分数整合到联合优化中，通过松弛二进制选择变量和应用拉格朗日分解来处理全局带宽耦合，然后进行每设备子问题优化。

Result: 在非独立同分布数据上的实验表明，FairEnergy相比基线策略实现了更高的准确率，同时能耗降低了高达79%。

Conclusion: FairEnergy框架有效解决了联邦学习中的能源效率和公平性平衡问题，为无线边缘系统的实际部署提供了可行方案。

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [76] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: 提出NTK引导的隐式神经教学(NINT)方法，通过动态选择最大化全局功能更新的坐标来加速隐式神经表示的训练，减少近一半训练时间同时保持或提高表示质量。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示(INRs)在拟合高分辨率信号时需要优化数百万个坐标，产生过高的计算成本，需要有效的加速方法。

Method: 利用神经正切核(NTK)对示例进行评分，通过NTK增强的损失梯度范数来选择坐标，同时考虑拟合误差和异构杠杆效应(自影响和跨坐标耦合)。

Result: NINT显著减少近一半训练时间，同时保持或提高表示质量，在基于采样的加速策略中达到最先进水平。

Conclusion: NINT通过NTK引导的坐标选择有效加速隐式神经表示训练，为高分辨率信号建模提供了实用的解决方案。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


### [77] [PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles](https://arxiv.org/abs/2511.15522)
*Yinan Yu,Samuel Scheidegger*

Main category: cs.LG

TL;DR: 提出PCARNN-DCBF方法，将物理编码的控制仿射残差神经网络与基于预览的离散控制屏障函数结合，用于地面车辆的实时地理围栏控制。


<details>
  <summary>Details</summary>
Motivation: 现有解决方案难以在高保真学习与可验证控制的结构要求之间取得平衡，特别是在确保车辆运行设计域(ODD)的地理围栏执行方面。

Method: 使用PCARNN-DCBF管道，PCARNN显式保留车辆动力学的控制仿射结构，确保优化的线性要求；DCBF通过实时二次规划(QP)强制执行多边形保持约束，处理高相对度并缓解执行器饱和。

Result: 在CARLA中对电动和燃烧平台进行的实验表明，这种结构保持方法显著优于分析性和非结构化神经基线。

Conclusion: PCARNN-DCBF通过结构保持的方法成功解决了高保真学习与可验证控制之间的平衡问题，为地面车辆的实时地理围栏提供了有效解决方案。

Abstract: Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.

</details>


### [78] [Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges](https://arxiv.org/abs/2511.15652)
*Kim N. Nolle,Ivana Dusparic,Rhodri Cusack,Vinny Cahill*

Main category: cs.LG

TL;DR: 本文通过自动驾驶环境中的实验，揭示了持续强化学习面临的挑战，包括环境抽象、超参数敏感性、灾难性遗忘和神经网络容量利用问题，并提出了相关研究方向和跨学科合作的必要性。


<details>
  <summary>Details</summary>
Motivation: 持续学习在非平稳环境（如自动驾驶）中具有重要应用价值，但将持续学习成功应用于强化学习仍是一个开放性问题，需要探索其中的挑战和解决方案。

Method: 在自动驾驶环境中，使用PPO算法让智能体依次学习四个不同角度的停车场景，构建持续学习环境，通过实验识别CRL面临的关键挑战。

Result: 实验揭示了CRL的四个主要挑战：寻找合适的环境抽象、对超参数的过度敏感性、灾难性遗忘问题以及神经网络容量的有效利用。

Conclusion: 需要解决CRL中的基础挑战，质疑神经网络在持续学习中的适用性，并强调计算机科学与神经科学跨学科研究的重要性。

Abstract: Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.
  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.
  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.

</details>


### [79] [DeepThinkVLA: Enhancing Reasoning Capability of Vision-Language-Action Models](https://arxiv.org/abs/2511.15669)
*Cheng Yin,Yankai Lin,Wang Xu,Sikyuen Tam,Xiangrui Zeng,Zhiyuan Liu,Zhouping Yin*

Main category: cs.LG

TL;DR: DeepThinkVLA通过混合注意力解码器解决视觉-语言-动作模型中思维与动作的冲突，采用因果注意力生成思维链，双向注意力并行解码动作，结合监督微调和强化学习训练，在LIBERO基准上达到97.0%成功率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型使用单一自回归解码器同时处理顺序思维链推理和高维并行机器人动作，导致运动控制性能下降和思维与动作间因果联系薄弱。

Method: 提出混合注意力解码器架构：用因果注意力生成顺序思维链，切换到双向注意力并行解码动作向量；采用两阶段训练：监督微调教授基础推理，强化学习用任务成功奖励对齐推理-动作序列与期望结果。

Result: 在LIBERO基准上达到97.0%成功率，混合架构比标准解码器提升15.5%，强化学习阶段额外提供2%性能提升。

Conclusion: DeepThinkVLA通过紧密集成的架构和训练策略有效解决了思维与动作间的冲突，实现了最先进的性能表现。

Abstract: Enabling Vision-Language-Action (VLA) models to "think before acting" via Chain-of-Thought (CoT) is a promising path to overcoming the data-hungry nature of end-to-end robot policies. However, progress is stalled by a fundamental conflict: existing models use a single autoregressive decoder for both sequential CoT reasoning and high-dimensional, parallelizable robot actions. This architectural mismatch degrades motor control and fails to forge a strong causal link between thought and action. We introduce DeepThinkVLA, which resolves this conflict through a tightly integrated architecture and training strategy. Architecturally, our hybrid-attention decoder generates sequential CoT with causal attention and then switches to bidirectional attention for fast, parallel decoding of action vectors. This design is complemented by a two-stage training pipeline: we first use Supervised Fine-Tuning (SFT) to teach the model foundational reasoning, then apply Reinforcement Learning (RL) with task-success rewards to causally align the full reasoning-action sequence with desired outcomes. This synergy leads to state-of-the-art performance, achieving a 97.0% success rate on the LIBERO benchmark. Our ablations confirm the design's effectiveness: the hybrid architecture alone outperforms standard decoders by 15.5%, and the final RL stage provides a crucial 2% boost to secure top performance.

</details>


### [80] [Walrus: A Cross-Domain Foundation Model for Continuum Dynamics](https://arxiv.org/abs/2511.15684)
*Michael McCabe,Payel Mukhopadhyay,Tanya Marwah,Bruno Regaldo-Saint Blancard,Francois Rozet,Cristiana Diaconu,Lucas Meyer,Kaze W. K. Wong,Hadi Sotoudeh,Alberto Bietti,Irina Espejo,Rio Fear,Siavash Golkar,Tom Hehir,Keiya Hirashima,Geraud Krawezik,Francois Lanusse,Rudy Morel,Ruben Ohana,Liam Parker,Mariel Pettee,Jeff Shen,Kyunghyun Cho,Miles Cranmer,Shirley Ho*

Main category: cs.LG

TL;DR: Walrus是一个基于Transformer的基础模型，主要用于流体类连续介质动力学仿真，通过谐波分析稳定化、负载均衡分布式训练和计算自适应标记化等技术，在19个不同场景上预训练，在短期和长期预测方面优于现有基础模型。


<details>
  <summary>Details</summary>
Motivation: 基础模型在语言和视觉领域取得了成功，但在物理仿真中面临数据异构性、不稳定长期动态、不同分辨率和维度等挑战，需要开发专门的方法来实现类似的影响力。

Method: 采用谐波分析稳定化方法、负载均衡的2D和3D分布式训练策略、计算自适应标记化技术，构建基于Transformer的Walrus模型，在19个多样化场景上进行预训练。

Result: Walrus在短期和长期预测任务上均优于现有基础模型，消融研究证实了所提方法在预测稳定性、训练吞吐量和迁移性能方面的价值。

Conclusion: Walrus展示了在物理仿真领域构建有效基础模型的可行性，通过专门的技术解决了数据异构性和训练效率等关键挑战，代码和权重已开源供社区使用。

Abstract: Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.

</details>


### [81] [The Impact of Quantization on Large Reasoning Model Reinforcement Learning](https://arxiv.org/abs/2511.15694)
*Medha Kumar,Zifei Xu,Xin Wang,Tristan Webb*

Main category: cs.LG

TL;DR: 量化感知的强化学习训练对数学推理模型的性能有负面影响，而训练后量化(PTQ)和QLoRA方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究量化对大型推理模型(RL训练)的影响，填补量化在强化学习场景中研究的空白。

Method: 通过系统实验比较训练后量化(PTQ)、量化感知训练(QAT)和量化感知强化学习训练的效果。

Result: 发现量化感知的RL训练会负面影响学习过程，而PTQ和QLoRA方法在数学基准测试中表现更优。

Conclusion: 对于强化学习训练的大型推理模型，训练后量化比量化感知的RL训练更有效。

Abstract: Strong reasoning capabilities can now be achieved by large-scale reinforcement learning (RL) without any supervised fine-tuning. Although post-training quantization (PTQ) and quantization-aware training (QAT) are well studied in the context of fine-tuning, how quantization impacts RL in large reasoning models (LRMs) remains an open question. To answer this question, we conducted systematic experiments and discovered a significant gap in reasoning performance on mathematical benchmarks between post-RL quantized models and their quantization-aware RL optimized counterparts. Our findings suggest that quantization-aware RL training negatively impacted the learning process, whereas PTQ and QLoRA led to greater performance.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [82] [Convex Clustering Redefined: Robust Learning with the Median of Means Estimator](https://arxiv.org/abs/2511.14784)
*Sourav De,Koustav Chowdhury,Bibhabasu Mandal,Sagar Ghosh,Swagatam Das,Debolina Paul,Saptarshi Chakraborty*

Main category: stat.ML

TL;DR: 提出了一种结合凸聚类和中位数均值估计器的鲁棒聚类方法，无需预先指定聚类数量，对噪声和异常值具有强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法需要预先指定聚类数量且对初始化敏感，凸聚类虽然稳定但处理高维数据和噪声时效果不佳，强融合正则化也会阻碍有效的聚类形成。

Method: 将凸聚类与中位数均值估计器结合，利用MoM的鲁棒性和凸聚类的稳定性，构建对异常值具有抵抗力的高效聚类框架。

Result: 理论分析显示在特定条件下具有弱一致性，在合成和真实数据集上的实验验证了该方法相比现有方法的优越性能。

Conclusion: 该方法提供了一种无需先验知识、对异常值鲁棒且高效的聚类解决方案，特别适用于大规模数据集。

Abstract: Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.

</details>


### [83] [Implicit Bias of the JKO Scheme](https://arxiv.org/abs/2511.14827)
*Peter Halmos,Boris Hanin*

Main category: stat.ML

TL;DR: 本文分析了JKO方案在二阶时间步长下的隐式偏差，发现该方案实际上是在最小化一个修正的能量函数J^η，该函数在原能量J基础上减去与度量曲率相关的项，这导致在曲率变化快的地方产生减速效应。


<details>
  <summary>Details</summary>
Motivation: 理解JKO方案为何具有其他一阶积分器不具备的优异性质（如保持能量耗散、无条件稳定性等），通过分析其二阶隐式偏差来揭示其内在机制。

Method: 通过数学分析证明JKO方案在二阶时间步长下近似于在修正能量J^η上的Wasserstein梯度流，其中J^η = J - (η/4)∫∥∇(δJ/δρ)∥²ρ(dx)。

Result: 发现JKO方案在二阶精度下相当于最小化修正能量J^η，这解释了其独特的稳定性性质。对于常见泛函，这种隐式偏差对应特定的信息量度：熵对应Fisher信息，KL散度对应Fisher-Hyvärinen散度，黎曼梯度下降对应动能。

Conclusion: JKO方案的优异性能源于其在二阶精度下对原始能量函数的隐式修正，这种修正通过减去度量曲率项来实现，在曲率变化剧烈的方向产生减速效应，从而增强稳定性。

Abstract: Wasserstein gradient flow provides a general framework for minimizing an energy functional $J$ over the space of probability measures on a Riemannian manifold $(M,g)$. Its canonical time-discretization, the Jordan-Kinderlehrer-Otto (JKO) scheme, produces for any step size $η>0$ a sequence of probability distributions $ρ_k^η$ that approximate to first order in $η$ Wasserstein gradient flow on $J$. But the JKO scheme also has many other remarkable properties not shared by other first order integrators, e.g. it preserves energy dissipation and exhibits unconditional stability for $λ$-geodesically convex functionals $J$. To better understand the JKO scheme we characterize its implicit bias at second order in $η$. We show that $ρ_k^η$ are approximated to order $η^2$ by Wasserstein gradient flow on a \emph{modified} energy \[ J^η(ρ) = J(ρ) - \fracη{4}\int_M \Big\lVert \nabla_g \frac{δJ}{δρ} (ρ) \Big\rVert_{2}^{2} \,ρ(dx), \] obtained by subtracting from $J$ the squared metric curvature of $J$ times $η/4$. The JKO scheme therefore adds at second order in $η$ a \textit{deceleration} in directions where the metric curvature of $J$ is rapidly changing. This corresponds to canonical implicit biases for common functionals: for entropy the implicit bias is the Fisher information, for KL-divergence it is the Fisher-Hyv{ä}rinen divergence, and for Riemannian gradient descent it is the kinetic energy in the metric $g$. To understand the differences between minimizing $J$ and $J^η$ we study \emph{JKO-Flow}, Wasserstein gradient flow on $J^η$, in several simple numerical examples. These include exactly solvable Langevin dynamics on the Bures-Wasserstein space and Langevin sampling from a quartic potential in 1D.

</details>


### [84] [Latent space analysis and generalization to out-of-distribution data](https://arxiv.org/abs/2511.15010)
*Katie Rainey,Erin Hausmann,Donald Waagen,David Gray,Donald Hulsey*

Main category: stat.ML

TL;DR: 论文研究发现，在深度学习的潜在空间中，基于分布外（OOD）检测的方法不能作为模型性能的代理指标。


<details>
  <summary>Details</summary>
Motivation: 理解深度学习系统在潜在决策空间中数据点之间的关系对于评估和解释系统在真实世界数据上的性能至关重要。检测分布外数据是深度学习领域的一个活跃研究方向。

Method: 使用开源模拟和实测合成孔径雷达（SAR）数据集，实证研究潜在空间OOD检测与模型分类准确性之间的联系。

Result: 实证研究表明，OOD检测不能作为模型性能的代理衡量标准。

Conclusion: 希望激发对潜在空间几何特性的进一步研究，这可能为深度学习鲁棒性和泛化性提供未来洞见。

Abstract: Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.

</details>


### [85] [Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit](https://arxiv.org/abs/2511.15120)
*Bohan Zhang,Zihao Wang,Hengyu Fu,Jason D. Lee*

Main category: stat.ML

TL;DR: 该论文证明了两层神经网络通过分层梯度下降可以最优地学习高斯多索引模型，样本和时间复杂度都达到信息论极限。研究发现第一层需要训练超过常数步数才能获得最优结果。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络如何高效学习高维特征是深度学习的核心问题。本文旨在探索神经网络在表示学习中的能力，特别是研究标准两层神经网络能否最优地学习高斯多索引模型。

Method: 使用分层梯度下降训练标准两层神经网络来学习高斯多索引模型f(x)=g(Ux)。证明过程中显示内层权重可以执行幂迭代过程，隐式模拟隐藏子空间的谱初始化。

Result: 在通用非退化假设下，神经网络能以o_d(1)测试误差不可知地学习目标函数，使用Õ(d)样本和Õ(d²)时间。样本和时间复杂度都达到信息论极限，因此是最优的。

Conclusion: 这项工作展示了神经网络在样本和时间效率方面有效学习层次函数的能力，表明第一层需要训练超过常数步数才能获得最优结果。

Abstract: In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\boldsymbol{x})=g(\boldsymbol{U}\boldsymbol{x})$ with hidden subspace $\boldsymbol{U}\in \mathbb{R}^{r\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\widetilde{\mathcal{O}}(d)$ samples and $\widetilde{\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.

</details>


### [86] [Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings](https://arxiv.org/abs/2511.15146)
*Eugene Ndiaye*

Main category: stat.ML

TL;DR: 本文提出了基于最优传输的向量值共形预测方法，解决了多维度预测集构造问题，并首次构建了具有有限样本校准的多变量共形预测分布。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测方法仅限于标量值，难以处理向量值分数。最优传输方法虽然能定义向量排序，但只有渐近覆盖保证。需要恢复有限样本、分布无关的覆盖保证。

Method: 通过共形化向量值OT分位数区域，将候选输出的秩定义为校准分数与该候选分数增强后的传输映射。证明最优分配在分数空间的固定多面体划分上是分段常数。

Result: 构建了第一个具有有限样本校准的多变量共形预测分布，任何导出的不确定区域自动具有保证覆盖。提出了保守和精确随机化版本。

Conclusion: 该方法将共形预测扩展到多维度，解决了预测集只指示哪些结果可能但无法提供相对似然性的深层限制，实现了多变量预测分布的有效构造。

Abstract: Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.

</details>


### [87] [Particle Monte Carlo methods for Lattice Field Theory](https://arxiv.org/abs/2511.15196)
*David Yallup*

Main category: stat.ML

TL;DR: GPU加速的粒子方法（SMC和嵌套采样）在格点场论的高维多模态采样问题中，提供了比最先进神经采样器更优的经典基准，在样本质量和计算时间上表现更好，同时还能估计配分函数。


<details>
  <summary>Details</summary>
Motivation: 解决格点场论中的高维多模态采样问题，为机器学习辅助采样方法建立更强的经典基准，评估学习型采样器是否值得其训练成本。

Method: 使用GPU加速的粒子方法，包括顺序蒙特卡洛（SMC）和嵌套采样，仅使用单一数据驱动的协方差进行调优，无需问题特定结构。

Result: 这些方法在标准标量场论基准测试中，在样本质量和壁钟时间方面匹配或优于最先进的神经采样器，同时还能估计配分函数。

Conclusion: GPU加速的粒子方法为高维多模态采样问题提供了强大的经典基准，提高了学习型采样器需要证明其训练成本合理性的门槛。

Abstract: High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.

</details>


### [88] [Robust Bayesian Optimisation with Unbounded Corruptions](https://arxiv.org/abs/2511.15315)
*Abdelhamid Ezzerg,Ilija Bogunovic,Jeremias Knoblauch*

Main category: stat.ML

TL;DR: 提出了RCGP-UCB算法，通过结合鲁棒共轭高斯过程和UCB方法，解决了贝叶斯优化对极端异常值的脆弱性问题，能够在异常值数量达到O(T^{1/2})和O(T^{1/3})时仍保持次线性遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒方法通常假设有界累积腐败预算，这使得它们即使面对单个足够大的腐败也无法防御。需要解决贝叶斯优化对极端异常值的严重脆弱性问题。

Method: 引入频率有界但幅度无界的对手模型，开发RCGP-UCB算法，将上置信界方法与鲁棒共轭高斯过程相结合，提出了稳定和自适应两个版本。

Result: RCGP-UCB算法在异常值数量达到O(T^{1/2})和O(T^{1/3})时仍能实现次线性遗憾，且在没有异常值时其遗憾界与标准GP-UCB算法相匹配。

Conclusion: RCGP-UCB算法以近乎零的代价实现了对极端异常值的鲁棒性，在保持标准算法性能的同时显著提升了鲁棒性。

Abstract: Bayesian Optimization is critically vulnerable to extreme outliers. Existing provably robust methods typically assume a bounded cumulative corruption budget, which makes them defenseless against even a single corruption of sufficient magnitude. To address this, we introduce a new adversary whose budget is only bounded in the frequency of corruptions, not in their magnitude. We then derive RCGP-UCB, an algorithm coupling the famous upper confidence bound (UCB) approach with a Robust Conjugate Gaussian Process (RCGP). We present stable and adaptive versions of RCGP-UCB, and prove that they achieve sublinear regret in the presence of up to $O(T^{1/2})$ and $O(T^{1/3})$ corruptions with possibly infinite magnitude. This robustness comes at near zero cost: without outliers, RCGP-UCB's regret bounds match those of the standard GP-UCB algorithm.

</details>


### [89] [Exponential Lasso: robust sparse penalization under heavy-tailed noise and outliers with exponential-type loss](https://arxiv.org/abs/2511.15332)
*The Tien Mai*

Main category: stat.ML

TL;DR: 提出了Exponential Lasso方法，通过指数型损失函数在Lasso框架中实现鲁棒性，在保持高斯噪声下统计效率的同时，有效抵抗异常值和重尾噪声的影响。


<details>
  <summary>Details</summary>
Motivation: 传统Lasso方法依赖平方损失函数，对异常值和重尾噪声高度敏感，导致模型选择不可靠和估计偏差。需要一种既能保持统计效率又具有鲁棒性的方法。

Method: 在Lasso框架中集成指数型损失函数，通过Majorization-Minimization算法迭代求解加权Lasso子问题进行优化。

Result: 理论保证显示Exponential Lasso在理想条件下达到经典Lasso的收敛速率，在污染数据下保持鲁棒性。数值实验表明该方法在污染设置下优于经典Lasso，在Gaussian噪声下仍保持强性能。

Conclusion: Exponential Lasso是一种有效的鲁棒变量选择方法，成功平衡了统计效率和鲁棒性，在多种噪声条件下都表现出色。

Abstract: In high-dimensional statistics, the Lasso is a cornerstone method for simultaneous variable selection and parameter estimation. However, its reliance on the squared loss function renders it highly sensitive to outliers and heavy-tailed noise, potentially leading to unreliable model selection and biased estimates. To address this limitation, we introduce the Exponential Lasso, a novel robust method that integrates an exponential-type loss function within the Lasso framework. This loss function is designed to achieve a smooth trade-off between statistical efficiency under Gaussian noise and robustness against data contamination. Unlike other methods that cap the influence of large residuals, the exponential loss smoothly redescends, effectively downweighting the impact of extreme outliers while preserving near-quadratic behavior for small errors. We establish theoretical guarantees showing that the Exponential Lasso achieves strong statistical convergence rates, matching the classical Lasso under ideal conditions while maintaining its robustness in the presence of heavy-tailed contamination. Computationally, the estimator is optimized efficiently via a Majorization-Minimization (MM) algorithm that iteratively solves a series of weighted Lasso subproblems. Numerical experiments demonstrate that the proposed method is highly competitive, outperforming the classical Lasso in contaminated settings and maintaining strong performance even under Gaussian noise.
  Our method is implemented in the \texttt{R} package \texttt{heavylasso} available on Github: https://github.com/tienmt/heavylasso

</details>


### [90] [Gini Score under Ties and Case Weights](https://arxiv.org/abs/2511.15446)
*Alexej Brauer,Mario V. Wüthrich*

Main category: stat.ML

TL;DR: 本文讨论了基尼分数在统计建模和机器学习中的应用，特别是在存在并列排名和案例权重的情况下如何扩展基尼分数的使用。


<details>
  <summary>Details</summary>
Motivation: 基尼分数主要用于二元响应情况，但在精算实践中经常遇到并列排名和案例权重的情况，需要扩展其应用范围。

Method: 通过洛伦兹曲线和集中度曲线将基尼分数扩展到实值随机变量，并讨论如何处理并列排名和案例权重的情况。

Result: 提出了在存在并列排名和案例权重情况下使用基尼分数的方法，扩展了其在精算实践中的应用。

Conclusion: 基尼分数可以成功扩展到处理并列排名和案例权重的情况，为精算建模提供了更灵活的风险排名评估工具。

Abstract: The Gini score is a popular tool in statistical modeling and machine learning for model validation and model selection. It is a purely rank based score that allows one to assess risk rankings. The Gini score for statistical modeling has mainly been used in a binary context, in which it has many equivalent reformulations such as the receiver operating characteristic (ROC) or the area under the curve (AUC). In the actuarial literature, this rank based score for binary responses has been extended to general real-valued random variables using Lorenz curves and concentration curves. While these initial concepts assume that the risk ranking is generated by a continuous distribution function, we discuss in this paper how the Gini score can be used in the case of ties in the risk ranking. Moreover, we adapt the Gini score to the common actuarial situation of having case weights.

</details>


### [91] [A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation](https://arxiv.org/abs/2511.15543)
*Georgios Venianakis,Constantinos Theodoropoulos,Michail Kavousanakis*

Main category: stat.ML

TL;DR: 提出了一个基于PINNs的综合框架，同时解决最优传感器布局和参数估计问题，通过将参数作为额外输入训练PINN模型，利用自动微分计算灵敏度函数，并使用D最优性准则确定最优传感器位置。


<details>
  <summary>Details</summary>
Motivation: 在分布式参数系统中，传感器布局对参数估计精度至关重要，但现有PINNs方法对此关注不足。数据采集成本高、数据有限或存在噪声不确定性，需要找到能提供最多参数信息的传感器配置。

Method: 训练包含待估参数作为额外输入的PINN模型，通过自动微分计算灵敏度函数，应用D最优性准则确定最优传感器位置，在反应-扩散-平流问题上验证。

Result: 在复杂程度递增的两个分布式参数问题上验证，相比直觉或随机选择的传感器位置，该方法始终获得更高的参数估计精度。

Conclusion: PINNs-based方法能有效同时优化传感器布局和参数估计，在分布式参数系统中显著提高参数估计精度。

Abstract: Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.

</details>


### [92] [Near-optimal delta-convex estimation of Lipschitz functions](https://arxiv.org/abs/2511.15615)
*Gábor Balázs*

Main category: stat.ML

TL;DR: 提出了一种可处理的算法，用于从噪声观测中估计未知Lipschitz函数，并建立了其收敛速率的上界。该方法将max-affine方法从凸形状限制回归扩展到更一般的Lipschitz设置。


<details>
  <summary>Details</summary>
Motivation: 将凸形状限制回归中的max-affine方法扩展到更一般的Lipschitz函数估计问题，开发一个既能保持理论保证又具有实际计算可行性的估计器。

Method: 使用非线性特征扩展将max-affine函数映射到delta-凸函数子类，作为Lipschitz函数的通用逼近器。算法整合了自适应分区、基于惩罚的正则化机制，以及结合凸初始化和局部精化的两阶段优化过程。

Result: 该估计器在平方损失和次高斯分布的随机设计设置下，相对于数据的内在维度达到了极小极大收敛速率（最多相差对数因子）。实验表明相对于其他理论证明的方法（包括最近邻和基于核的回归器）具有竞争力。

Conclusion: 该框架成功地将max-affine方法扩展到Lipschitz函数估计，实现了理论最优的收敛速率，同时保持了计算可行性，并且可以轻松适应凸形状限制回归问题。

Abstract: This paper presents a tractable algorithm for estimating an unknown Lipschitz function from noisy observations and establishes an upper bound on its convergence rate. The approach extends max-affine methods from convex shape-restricted regression to the more general Lipschitz setting. A key component is a nonlinear feature expansion that maps max-affine functions into a subclass of delta-convex functions, which act as universal approximators of Lipschitz functions while preserving their Lipschitz constants. Leveraging this property, the estimator attains the minimax convergence rate (up to logarithmic factors) with respect to the intrinsic dimension of the data under squared loss and subgaussian distributions in the random design setting. The algorithm integrates adaptive partitioning to capture intrinsic dimension, a penalty-based regularization mechanism that removes the need to know the true Lipschitz constant, and a two-stage optimization procedure combining a convex initialization with local refinement. The framework is also straightforward to adapt to convex shape-restricted regression. Experiments demonstrate competitive performance relative to other theoretically justified methods, including nearest-neighbor and kernel-based regressors.

</details>


### [93] [Rényi Differential Privacy for Heavy-Tailed SDEs via Fractional Poincaré Inequalities](https://arxiv.org/abs/2511.15634)
*Benjamin Dupuis,Mert Gürbüzbalaban,Umut Şimşekli,Jian Wang,Sinan Yildirim,Lingjiong Zhu*

Main category: stat.ML

TL;DR: 本文提出了首个针对重尾随机微分方程及其离散化版本的Rényi差分隐私保证，通过新的Rényi流计算和分数Poincaré不等式，显著降低了维度依赖。


<details>
  <summary>Details</summary>
Motivation: 现有DP保证主要针对轻尾噪声，而重尾SGD的DP研究仍有限，特别是缺乏RDP保证且维度依赖性强。

Method: 基于新的Rényi流计算和分数Poincaré不等式，推导重尾SDEs及其离散化版本的RDP保证。

Result: 在满足分数Poincaré不等式的条件下，获得了比现有方法维度依赖更弱的DP保证。

Conclusion: 该框架为重尾算法的差分隐私分析提供了新的理论基础，显著改善了维度依赖问题。

Abstract: Characterizing the differential privacy (DP) of learning algorithms has become a major challenge in recent years. In parallel, many studies suggested investigating the behavior of stochastic gradient descent (SGD) with heavy-tailed noise, both as a model for modern deep learning models and to improve their performance. However, most DP bounds focus on light-tailed noise, where satisfactory guarantees have been obtained but the proposed techniques do not directly extend to the heavy-tailed setting. Recently, the first DP guarantees for heavy-tailed SGD were obtained. These results provide $(0,δ)$-DP guarantees without requiring gradient clipping. Despite casting new light on the link between DP and heavy-tailed algorithms, these results have a strong dependence on the number of parameters and cannot be extended to other DP notions like the well-established Rényi differential privacy (RDP). In this work, we propose to address these limitations by deriving the first RDP guarantees for heavy-tailed SDEs, as well as their discretized counterparts. Our framework is based on new Rényi flow computations and the use of well-established fractional Poincaré inequalities. Under the assumption that such inequalities are satisfied, we obtain DP guarantees that have a much weaker dependence on the dimension compared to prior art.

</details>


### [94] [Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion](https://arxiv.org/abs/2511.15679)
*Jianqiao Mao,Max A. Little*

Main category: stat.ML

TL;DR: 本文提出了前门可约性(FDR)概念，将经典前门准则扩展到更复杂的因果图中，通过将变量聚合为超节点来简化干预分布的估计。


<details>
  <summary>Details</summary>
Motivation: 经典前门准则适用范围有限，而ID算法虽然通用但得到的表达式往往复杂难用。作者认为前门准则的适用性比看起来更广泛，许多复杂因果图可以简化为前门设置。

Method: 引入前门可约性(FDR)作为有向无环混合图上的图形条件，通过将变量聚合为超节点(FDR三元组)来简化因果图。提出了FDR-TID算法来检测可容许的FDR三元组。

Result: 证明了FDR准则满足性与FDR调整适用性之间的图级等价性。FDR-TID算法被证明具有正确性、完备性和有限终止性。

Conclusion: FDR通过优先考虑可解释性和计算简单性来补充现有识别方法，而不牺牲混合图的通用性，许多教科书前门设置之外的图都是FDR可约的。

Abstract: Front-door adjustment provides a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow and strict. Although ID algorithm is very useful and is proved effective for causal relation identification in general causal graphs (if it is identifiable), performing ID algorithm does not guarantee to obtain a practical, easy-to-estimate interventional distribution expression. We argue that the applicability of the front-door criterion is not as limited as it seems: many more complicated causal graphs can be reduced to the front-door criterion. In this paper, We introduce front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs (ADMGs) that extends the applicability of the classic front-door criterion to reduce a large family of complicated causal graphs to a front-door setting by aggregating variables into super-nodes (FDR triple) $\left(\boldsymbol{X}^{*},\boldsymbol{Y}^{*},\boldsymbol{M}^{*}\right)$. After characterizing FDR criterion, we prove a graph-level equivalence between the satisfication of FDR criterion and the applicability of FDR adjustment. Meanwhile, we then present FDR-TID, an exact algorithm that detects an admissible FDR triple, together with established the algorithm's correctness, completeness, and finite termination. Empirically-motivated examples illustrate that many graphs outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR thus complements existing identification method by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs.

</details>
