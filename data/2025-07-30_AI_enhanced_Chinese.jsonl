{"id": "2507.21347", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21347", "abs": "https://arxiv.org/abs/2507.21347", "authors": ["Haonan Si", "Zhaolin Wang", "Xiansheng Guo", "Jin Zhang", "Yuanwei Liu"], "title": "DOA Estimation via Continuous Aperture Arrays: MUSIC and CRLB", "comment": "Submit to possible IEEE journal", "summary": "Direction-of-arrival (DOA) estimation using continuous aperture array (CAPA)\nis studied. Compared to the conventional spatially discrete array (SPDA), CAPA\nsignificantly enhances the spatial degrees-of-freedoms (DoFs) for DOA\nestimation, but its infinite-dimensional continuous signals render the\nconventional estimation algorithm non-applicable. To address this challenge, a\nnew multiple signal classification (MUSIC) algorithm is proposed for CAPAs. In\nparticular, an equivalent continuous-discrete transformation is proposed to\nfacilitate the eigendecomposition of continuous operators. Subsequently, the\nMUSIC spectrum is accurately approximated using the Gauss-Legendre quadrature,\neffectively reducing the computational complexity. Furthermore, the\nCram\\'er-Rao lower bounds (CRLBs) for DOA estimation using CAPAs are analyzed\nfor both cases with and without priori knowledge of snapshot signals. It is\ntheoretically proved that CAPAs significantly improve the DOA estimation\naccuracy compared to traditional SPDAs. Numerical results further validate this\ninsight and demonstrate the effectiveness of the proposed MUSIC algorithm for\nCAPA. The proposed method achieves near-optimal estimation performance while\nmaintaining a low computational complexity.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u8fde\u7eed\u5b54\u5f84\u9635\u5217\uff08CAPA\uff09\u8fdb\u884c\u65b9\u5411\u5230\u8fbe\uff08DOA\uff09\u4f30\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MUSIC\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7b97\u6cd5\u4e0d\u9002\u7528\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4f20\u7edf\u79bb\u6563\u9635\u5217\uff08SPDA\uff09\u5728DOA\u4f30\u8ba1\u4e2d\u7684\u81ea\u7531\u5ea6\u6709\u9650\uff0c\u800c\u8fde\u7eed\u5b54\u5f84\u9635\u5217\uff08CAPA\uff09\u867d\u7136\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u81ea\u7531\u5ea6\uff0c\u4f46\u5176\u65e0\u9650\u7ef4\u8fde\u7eed\u4fe1\u53f7\u4f7f\u5f97\u4f20\u7edf\u4f30\u8ba1\u7b97\u6cd5\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MUSIC\u7b97\u6cd5\uff0c\u901a\u8fc7\u7b49\u6548\u8fde\u7eed-\u79bb\u6563\u53d8\u6362\u5b9e\u73b0\u8fde\u7eed\u7b97\u5b50\u7684\u7279\u5f81\u5206\u89e3\uff0c\u5e76\u4f7f\u7528\u9ad8\u65af-\u52d2\u8ba9\u5fb7\u79ef\u5206\u8fd1\u4f3cMUSIC\u8c31\u4ee5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0cCAPA\u663e\u8457\u63d0\u9ad8\u4e86DOA\u4f30\u8ba1\u7684\u7cbe\u5ea6\uff0c\u4e14\u63d0\u51fa\u7684MUSIC\u7b97\u6cd5\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "CAPA\u5728DOA\u4f30\u8ba1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u63d0\u51fa\u7684MUSIC\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21454", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21454", "abs": "https://arxiv.org/abs/2507.21454", "authors": ["Zhuoran Xiao", "Chenhui Ye", "Yijia Feng", "Yunbo Hu", "Tianyu Jiao", "Liyu Cai", "Guangyi Liu"], "title": "Transmission With Machine Language Tokens: A Paradigm for Task-Oriented Agent Communication", "comment": "Accepted by IEEE Globecom 2025", "summary": "The rapid advancement in large foundation models is propelling the paradigm\nshifts across various industries. One significant change is that agents,\ninstead of traditional machines or humans, will be the primary participants in\nthe future production process, which consequently requires a novel AI-native\ncommunication system tailored for agent communications. Integrating the ability\nof large language models (LLMs) with task-oriented semantic communication is a\npotential approach. However, the output of existing LLM is human language,\nwhich is highly constrained and sub-optimal for agent-type communication. In\nthis paper, we innovatively propose a task-oriented agent communication system.\nSpecifically, we leverage the original LLM to learn a specialized machine\nlanguage represented by token embeddings. Simultaneously, a multi-modal LLM is\ntrained to comprehend the application task and to extract essential implicit\ninformation from multi-modal inputs, subsequently expressing it using machine\nlanguage tokens. This representation is significantly more efficient for\ntransmission over the air interface. Furthermore, to reduce transmission\noverhead, we introduce a joint token and channel coding (JTCC) scheme that\ncompresses the token sequence by exploiting its sparsity while enhancing\nrobustness against channel noise. Extensive experiments demonstrate that our\napproach reduces transmission overhead for downstream tasks while enhancing\naccuracy relative to the SOTA methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u4efb\u52a1\u7684\u667a\u80fd\u4f53\u901a\u4fe1\u7cfb\u7edf\uff0c\u5229\u7528LLM\u5b66\u4e60\u4e13\u7528\u673a\u5668\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u591a\u6a21\u6001LLM\u63d0\u53d6\u4efb\u52a1\u4fe1\u606f\uff0c\u7ed3\u5408\u8054\u5408\u4ee4\u724c\u548c\u4fe1\u9053\u7f16\u7801\uff08JTCC\uff09\u964d\u4f4e\u4f20\u8f93\u5f00\u9500\u3002", "motivation": "\u968f\u7740\u5927\u578b\u57fa\u7840\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u672a\u6765\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u667a\u80fd\u4f53\u5c06\u6210\u4e3a\u4e3b\u8981\u53c2\u4e0e\u8005\uff0c\u9700\u8981\u4e00\u79cd\u4e13\u4e3a\u667a\u80fd\u4f53\u901a\u4fe1\u8bbe\u8ba1\u7684AI\u539f\u751f\u901a\u4fe1\u7cfb\u7edf\u3002", "method": "\u5229\u7528\u539f\u59cbLLM\u5b66\u4e60\u4e13\u7528\u673a\u5668\u8bed\u8a00\uff0c\u8bad\u7ec3\u591a\u6a21\u6001LLM\u7406\u89e3\u4efb\u52a1\u5e76\u63d0\u53d6\u4fe1\u606f\uff0c\u91c7\u7528JTCC\u65b9\u6848\u538b\u7f29\u4ee4\u724c\u5e8f\u5217\u5e76\u589e\u5f3a\u6297\u566a\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u964d\u4f4e\u4f20\u8f93\u5f00\u9500\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u4e0b\u6e38\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u901a\u4fe1\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.21511", "categories": ["eess.SP", "26A33, 42A38, 94A08, 94A12", "I.4.3; I.6.3; I.5.2; G.1.2"], "pdf": "https://arxiv.org/pdf/2507.21511", "abs": "https://arxiv.org/abs/2507.21511", "authors": ["Daxiang Li", "Zhichao Zhang", "Wei Yao"], "title": "Two-Dimensional Nonseparable Fractional Fourier Transform: Theory and Application", "comment": "26 pages, 11 figures", "summary": "The one-dimensional (1D) fractional Fourier transform (FRFT) generalizes the\n1D Fourier transform, offering significant advantages in time-frequency\nanalysis of non-stationary signals. To extend the benefits of the 1D FRFT to\nhigher-dimensional signals, 2D FRFTs, such as the 2D separable FRFT (SFRFT),\ngyrator transform (GT), and coupled FRFT (CFRFT), have been developed. However,\nexisting 2D FRFTs suffer from several limitations: (1) a lack of theoretical\nuniformity and general applicability, (2) an inability to handle 2D\nnon-stationary signals with nonseparable terms, and (3) failure to maintain a\nconsistent 4D rotational relationship with the 2D Wigner distribution (WD),\nwhich is essential for ensuring geometric consistency and symmetry in\ntime-frequency analysis. These limitations restrict the methods' performance in\npractical applications, such as radar, communication, sonar, and optical\nimaging, in which nonseparable terms frequently arise. To address these\nchallenges, we introduce a more general definition of the 2D FRFT, termed the\n2D nonseparable FRFT (NSFRFT). The 2D NSFRFT has four degrees of freedom,\nincludes the 2D SFRFT, GT, and CFRFT as special cases, and maintains a more\ngeneral 4D rotational relationship with the 2D WD. We derive its properties and\npresent three discrete algorithms, two of which are fast algorithms with\ncomputational complexity $O(N^2 \\log N)$ comparable to that of the 2D SFRFT.\nNumerical simulations and experiments demonstrate the superior performance of\nthe 2D NSFRFT in applications such as image encryption, decryption, filtering,\nand denoising.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76842D\u975e\u53ef\u5206\u5206\u6570\u5085\u91cc\u53f6\u53d8\u6362\uff08NSFRFT\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u67092D FRFT\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u67092D FRFT\u5728\u7406\u8bba\u7edf\u4e00\u6027\u3001\u5904\u7406\u975e\u53ef\u5206\u4fe1\u53f7\u548c\u4fdd\u6301\u4e0e2D Wigner\u5206\u5e03\u7684\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u63d0\u51fa2D NSFRFT\uff0c\u5177\u6709\u56db\u4e2a\u81ea\u7531\u5ea6\uff0c\u5305\u542b\u73b0\u67092D FRFT\u4e3a\u7279\u4f8b\uff0c\u5e76\u4fdd\u6301\u66f4\u4e00\u822c\u76844D\u65cb\u8f6c\u5173\u7cfb\u3002\u63a8\u5bfc\u5176\u6027\u8d28\u5e76\u5f00\u53d1\u4e09\u79cd\u79bb\u6563\u7b97\u6cd5\uff0c\u5176\u4e2d\u4e24\u79cd\u4e3a\u5feb\u901f\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u6a21\u62df\u548c\u5b9e\u9a8c\u8868\u660e\uff0c2D NSFRFT\u5728\u56fe\u50cf\u52a0\u5bc6\u3001\u89e3\u5bc6\u3001\u6ee4\u6ce2\u548c\u53bb\u566a\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "2D NSFRFT\u662f\u4e00\u79cd\u66f4\u901a\u7528\u76842D FRFT\u5b9a\u4e49\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.21527", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21527", "abs": "https://arxiv.org/abs/2507.21527", "authors": ["Ziqi Yan", "Zhichao Zhang"], "title": "Trainable Joint Time-Vertex Fractional Fourier Transform", "comment": "35 pages,5 figures", "summary": "To address limitations of the graph fractional Fourier transform (GFRFT)\nWiener filtering and the traditional joint time-vertex fractional Fourier\ntransform (JFRFT) Wiener filtering, this study proposes a filtering method\nbased on the hyper-differential form of the JFRFT. The gradient backpropagation\nmechanism is employed to enable the adaptive selection of transform order pair\nand filter coefficients. First, leveraging the hyper-differential form of the\nGFRFT and the fractional Fourier transform, the hyper-differential form of the\nJFRFT is constructed and its properties are analyzed. Second, time-varying\ngraph signals are divided into dynamic graph sequences of equal span along the\ntemporal dimension. A spatiotemporal joint representation is then established\nthrough vectorized reorganization, followed by the joint time-vertex Wiener\nfiltering. Furthermore, by rigorously proving the differentiability of the\ntransform orders, both the transform orders and filter coefficients are\nembedded as learnable parameters within a neural network architecture. Through\ngradient backpropagation, their synchronized iterative optimization is\nachieved, constructing a parameters-adaptive learning filtering framework. This\nmethod leverages a model-driven approach to learn the optimal transform order\npair and filter coefficients. Experimental results indicate that the proposed\nframework improves the time-varying graph signals denoising performance, while\nreducing the computational burden of the traditional grid search strategy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u5fae\u5206\u5f62\u5f0f\u7684JFRFT\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u81ea\u9002\u5e94\u9009\u62e9\u53d8\u6362\u9636\u6570\u548c\u6ee4\u6ce2\u5668\u7cfb\u6570\uff0c\u63d0\u5347\u4e86\u65f6\u53d8\u56fe\u4fe1\u53f7\u7684\u53bb\u566a\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u89e3\u51b3GFRFT\u548c\u4f20\u7edfJFRFT\u7ef4\u7eb3\u6ee4\u6ce2\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u6ee4\u6ce2\u65b9\u6cd5\u3002", "method": "\u6784\u5efaJFRFT\u7684\u8d85\u5fae\u5206\u5f62\u5f0f\u5e76\u5206\u6790\u5176\u6027\u8d28\uff0c\u5c06\u65f6\u53d8\u56fe\u4fe1\u53f7\u5206\u5272\u4e3a\u52a8\u6001\u56fe\u5e8f\u5217\uff0c\u5efa\u7acb\u65f6\u7a7a\u8054\u5408\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u53d8\u6362\u9636\u6570\u548c\u6ee4\u6ce2\u5668\u7cfb\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u53bb\u566a\u6027\u80fd\u5e76\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d1f\u62c5\u3002", "conclusion": "\u63d0\u51fa\u7684\u53c2\u6570\u81ea\u9002\u5e94\u5b66\u4e60\u6ee4\u6ce2\u6846\u67b6\u5728\u65f6\u53d8\u56fe\u4fe1\u53f7\u5904\u7406\u4e2d\u5177\u6709\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.21334", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21334", "abs": "https://arxiv.org/abs/2507.21334", "authors": ["Zhanhong Cheng", "Lingqian Hu", "Yuheng Bu", "Yuqi Zhou", "Shenhao Wang"], "title": "Graph neural networks for residential location choice: connection to classical logit models", "comment": null, "summary": "Researchers have adopted deep learning for classical discrete choice analysis\nas it can capture complex feature relationships and achieve higher predictive\nperformance. However, the existing deep learning approaches cannot explicitly\ncapture the relationship among choice alternatives, which has been a\nlong-lasting focus in classical discrete choice models. To address the gap,\nthis paper introduces Graph Neural Network (GNN) as a novel framework to\nanalyze residential location choice. The GNN-based discrete choice models\n(GNN-DCMs) offer a structured approach for neural networks to capture\ndependence among spatial alternatives, while maintaining clear connections to\nclassical random utility theory. Theoretically, we demonstrate that the\nGNN-DCMs incorporate the nested logit (NL) model and the spatially correlated\nlogit (SCL) model as two specific cases, yielding novel algorithmic\ninterpretation through message passing among alternatives' utilities.\nEmpirically, the GNN-DCMs outperform benchmark MNL, SCL, and feedforward neural\nnetworks in predicting residential location choices among Chicago's 77\ncommunity areas. Regarding model interpretation, the GNN-DCMs can capture\nindividual heterogeneity and exhibit spatially-aware substitution patterns.\nOverall, these results highlight the potential of GNN-DCMs as a unified and\nexpressive framework for synergizing discrete choice modeling and deep learning\nin the complex spatial choice contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u79bb\u6563\u9009\u62e9\u6a21\u578b\uff08GNN-DCMs\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u663e\u5f0f\u6355\u6349\u9009\u62e9\u66ff\u4ee3\u9879\u4e4b\u95f4\u5173\u7cfb\u7684\u95ee\u9898\u3002\u8be5\u6a21\u578b\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u6a21\u578b\u89e3\u91ca\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u79bb\u6563\u9009\u62e9\u5206\u6790\u4e2d\u65e0\u6cd5\u663e\u5f0f\u6355\u6349\u66ff\u4ee3\u9879\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u800c\u8fd9\u662f\u7ecf\u5178\u79bb\u6563\u9009\u62e9\u6a21\u578b\u7684\u6838\u5fc3\u5173\u6ce8\u70b9\u3002", "method": "\u5f15\u5165\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u4f5c\u4e3a\u65b0\u6846\u67b6\uff0c\u6784\u5efaGNN-DCMs\uff0c\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u6355\u6349\u7a7a\u95f4\u66ff\u4ee3\u9879\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "GNN-DCMs\u5728\u829d\u52a0\u54e577\u4e2a\u793e\u533a\u533a\u57df\u7684\u4f4f\u5b85\u9009\u5740\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u4e8eMNL\u3001SCL\u548c\u524d\u9988\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u80fd\u6355\u6349\u4e2a\u4f53\u5f02\u8d28\u6027\u548c\u7a7a\u95f4\u611f\u77e5\u7684\u66ff\u4ee3\u6a21\u5f0f\u3002", "conclusion": "GNN-DCMs\u4e3a\u590d\u6742\u7a7a\u95f4\u9009\u62e9\u80cc\u666f\u4e0b\u79bb\u6563\u9009\u62e9\u5efa\u6a21\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u6846\u67b6\u3002"}}
{"id": "2507.21153", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.21153", "abs": "https://arxiv.org/abs/2507.21153", "authors": ["Abderaouf Bahi", "Amel Ourici"], "title": "Deep Reinforcement Learning for Real-Time Green Energy Integration in Data Centers", "comment": null, "summary": "This paper explores the implementation of a Deep Reinforcement Learning\n(DRL)-optimized energy management system for e-commerce data centers, aimed at\nenhancing energy efficiency, cost-effectiveness, and environmental\nsustainability. The proposed system leverages DRL algorithms to dynamically\nmanage the integration of renewable energy sources, energy storage, and grid\npower, adapting to fluctuating energy availability in real time. The study\ndemonstrates that the DRL-optimized system achieves a 38\\% reduction in energy\ncosts, significantly outperforming traditional Reinforcement Learning (RL)\nmethods (28\\%) and heuristic approaches (22\\%). Additionally, it maintains a\nlow SLA violation rate of 1.5\\%, compared to 3.0\\% for RL and 4.8\\% for\nheuristic methods. The DRL-optimized approach also results in an 82\\%\nimprovement in energy efficiency, surpassing other methods, and a 45\\%\nreduction in carbon emissions, making it the most environmentally friendly\nsolution. The system's cumulative reward of 950 reflects its superior\nperformance in balancing multiple objectives. Through rigorous testing and\nablation studies, the paper validates the effectiveness of the DRL model's\narchitecture and parameters, offering a robust solution for energy management\nin data centers. The findings highlight the potential of DRL in advancing\nenergy optimization strategies and addressing sustainability challenges.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u7535\u5b50\u5546\u52a1\u6570\u636e\u4e2d\u5fc3\u80fd\u6e90\u7ba1\u7406\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6e90\u6548\u7387\u3001\u6210\u672c\u6548\u76ca\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u6570\u636e\u4e2d\u5fc3\u80fd\u6e90\u7ba1\u7406\u4e2d\u7684\u52a8\u6001\u4f18\u5316\u95ee\u9898\uff0c\u6574\u5408\u53ef\u518d\u751f\u80fd\u6e90\u3001\u50a8\u80fd\u548c\u7535\u7f51\u7535\u529b\uff0c\u4ee5\u5e94\u5bf9\u5b9e\u65f6\u6ce2\u52a8\u7684\u80fd\u6e90\u4f9b\u5e94\u3002", "method": "\u91c7\u7528DRL\u7b97\u6cd5\u52a8\u6001\u7ba1\u7406\u80fd\u6e90\u8d44\u6e90\uff0c\u5b9e\u65f6\u9002\u5e94\u80fd\u6e90\u53ef\u7528\u6027\u53d8\u5316\u3002", "result": "DRL\u7cfb\u7edf\u5b9e\u73b0\u4e8638%\u7684\u80fd\u6e90\u6210\u672c\u964d\u4f4e\u300182%\u7684\u80fd\u6e90\u6548\u7387\u63d0\u5347\u548c45%\u7684\u78b3\u51cf\u6392\uff0c\u4f18\u4e8e\u4f20\u7edfRL\u548c\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "DRL\u4e3a\u6570\u636e\u4e2d\u5fc3\u80fd\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u73af\u4fdd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5176\u5728\u80fd\u6e90\u4f18\u5316\u548c\u53ef\u6301\u7eed\u53d1\u5c55\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.21109", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21109", "abs": "https://arxiv.org/abs/2507.21109", "authors": ["Prital Bamnodkar"], "title": "Task-Focused Consolidation with Spaced Recall: Making Neural Networks learn like college students", "comment": null, "summary": "Deep Neural Networks often suffer from a critical limitation known as\nCatastrophic Forgetting, where performance on past tasks degrades after\nlearning new ones. This paper introduces a novel continual learning approach\ninspired by human learning strategies like Active Recall, Deliberate Practice\nand Spaced Repetition, named Task Focused Consolidation with Spaced Recall\n(TFC-SR). TFC-SR enhances the standard experience replay with a mechanism we\ntermed the Active Recall Probe. It is a periodic, task-aware evaluation of the\nmodel's memory that stabilizes the representations of past knowledge. We test\nTFC-SR on the Split MNIST and Split CIFAR-100 benchmarks against leading\nregularization-based and replay-based baselines. Our results show that TFC-SR\nperforms significantly better than these methods. For instance, on the Split\nCIFAR-100, it achieves a final accuracy of 13.17% compared to standard replay's\n7.40%. We demonstrate that this advantage comes from the stabilizing effect of\nthe probe itself, and not from the difference in replay volume. Additionally,\nwe analyze the trade-off between memory size and performance and show that\nwhile TFC-SR performs better in memory-constrained environments, higher replay\nvolume is still more effective when available memory is abundant. We conclude\nthat TFC-SR is a robust and efficient approach, highlighting the importance of\nintegrating active memory retrieval mechanisms into continual learning systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTFC-SR\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4e3b\u52a8\u56de\u5fc6\u673a\u5236\uff08Active Recall Probe\uff09\u6539\u8fdb\u6807\u51c6\u7ecf\u9a8c\u56de\u653e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728Split MNIST\u548cSplit CIFAR-100\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9762\u4e34\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u501f\u9274\u4eba\u7c7b\u5b66\u4e60\u7b56\u7565\uff08\u5982\u4e3b\u52a8\u56de\u5fc6\u3001\u523b\u610f\u7ec3\u4e60\u548c\u95f4\u9694\u91cd\u590d\uff09\u6765\u7a33\u5b9a\u8fc7\u53bb\u4efb\u52a1\u7684\u77e5\u8bc6\u8868\u793a\u3002", "method": "\u63d0\u51faTask Focused Consolidation with Spaced Recall (TFC-SR)\uff0c\u5f15\u5165Active Recall Probe\u673a\u5236\uff0c\u5b9a\u671f\u8bc4\u4f30\u6a21\u578b\u8bb0\u5fc6\u4ee5\u7a33\u5b9a\u77e5\u8bc6\u8868\u793a\u3002", "result": "\u5728Split CIFAR-100\u4e0a\uff0cTFC-SR\u7684\u6700\u7ec8\u51c6\u786e\u7387\u4e3a13.17%\uff0c\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u56de\u653e\u65b9\u6cd5\u76847.40%\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u4f18\u52bf\u6e90\u4e8e\u63a2\u9488\u7684\u7a33\u5b9a\u4f5c\u7528\u800c\u975e\u56de\u653e\u91cf\u3002", "conclusion": "TFC-SR\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u5728\u7cfb\u7edf\u4e2d\u96c6\u6210\u4e3b\u52a8\u8bb0\u5fc6\u68c0\u7d22\u673a\u5236\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.21570", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21570", "abs": "https://arxiv.org/abs/2507.21570", "authors": ["Joni Shaska", "Urbashi Mitra"], "title": "Causal Link Discovery with Unequal Edge Error Tolerance", "comment": "14 pages, 6 figures, portions presented at International Symposium on\n  Information Theory (ISIT) 2024 and Asilomar 2024", "summary": "This paper proposes a novel framework for causal discovery with asymmetric\nerror control, called Neyman-Pearson causal discovery. Despite the importance\nof applications where different types of edge errors may have different\nimportance, current state-of-the-art causal discovery algorithms do not\ndifferentiate between the types of edge errors, nor provide any finite-sample\nguarantees on the edge errors. Hence, this framework seeks to minimize one type\nof error while keeping the other below a user-specified tolerance level. Using\ntechniques from information theory, fundamental performance limits are found,\ncharacterized by the R\\'enyi divergence, for Neyman-Pearson causal discovery.\nFurthermore, a causal discovery algorithm is introduced for the case of linear\nadditive Gaussian noise models, called epsilon-CUT, that provides finite-sample\nguarantees on the false positive rate, while staying competitive with\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNeyman-Pearson\u56e0\u679c\u53d1\u73b0\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u63a7\u5236\u4e0d\u5bf9\u79f0\u8bef\u5dee\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u6280\u672f\u627e\u5230\u6027\u80fd\u6781\u9650\uff0c\u5e76\u9488\u5bf9\u7ebf\u6027\u9ad8\u65af\u566a\u58f0\u6a21\u578b\u63d0\u51fa\u4e86\u4e00\u79cd\u7b97\u6cd5epsilon-CUT\u3002", "motivation": "\u73b0\u6709\u56e0\u679c\u53d1\u73b0\u7b97\u6cd5\u672a\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u8fb9\u9519\u8bef\uff0c\u4e5f\u672a\u63d0\u4f9b\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u540c\u7c7b\u578b\u7684\u9519\u8bef\u91cd\u8981\u6027\u4e0d\u540c\u3002", "method": "\u63d0\u51faNeyman-Pearson\u56e0\u679c\u53d1\u73b0\u6846\u67b6\uff0c\u6700\u5c0f\u5316\u4e00\u7c7b\u9519\u8bef\u7684\u540c\u65f6\u63a7\u5236\u53e6\u4e00\u7c7b\u9519\u8bef\u5728\u7528\u6237\u6307\u5b9a\u6c34\u5e73\uff0c\u5e76\u57fa\u4e8eR\u00e9nyi\u6563\u5ea6\u5206\u6790\u6027\u80fd\u6781\u9650\u3002\u9488\u5bf9\u7ebf\u6027\u9ad8\u65af\u566a\u58f0\u6a21\u578b\uff0c\u63d0\u51faepsilon-CUT\u7b97\u6cd5\u3002", "result": "epsilon-CUT\u7b97\u6cd5\u5728\u6709\u9650\u6837\u672c\u4e0b\u4fdd\u8bc1\u5047\u9633\u6027\u7387\uff0c\u540c\u65f6\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ade\u4e89\u3002", "conclusion": "\u8be5\u6846\u67b6\u586b\u8865\u4e86\u56e0\u679c\u53d1\u73b0\u4e2d\u4e0d\u5bf9\u79f0\u8bef\u5dee\u63a7\u5236\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7epsilon-CUT\u7b97\u6cd5\u5b9e\u73b0\u4e86\u7406\u8bba\u4fdd\u8bc1\u4e0e\u5b9e\u7528\u6027\u7ed3\u5408\u3002"}}
{"id": "2507.21429", "categories": ["stat.ML", "cs.LG", "68T07, 90C26, 65K10"], "pdf": "https://arxiv.org/pdf/2507.21429", "abs": "https://arxiv.org/abs/2507.21429", "authors": ["Agnideep Aich", "Ashit Baran Aich", "Bruce Wade"], "title": "From Sublinear to Linear: Fast Convergence in Deep Networks via Locally Polyak-Lojasiewicz Regions", "comment": null, "summary": "The convergence of gradient descent (GD) on the non-convex loss landscapes of\ndeep neural networks (DNNs) presents a fundamental theoretical challenge. While\nrecent work has established that GD converges to a stationary point at a\nsublinear rate within locally quasi-convex regions (LQCRs), this fails to\nexplain the exponential convergence rates consistently observed in practice. In\nthis paper, we resolve this discrepancy by proving that under a mild assumption\non Neural Tangent Kernel (NTK) stability, these same regions satisfy a local\nPolyak-Lojasiewicz (PL) condition. We introduce the concept of a Locally\nPolyak-Lojasiewicz Region (LPLR), where the squared gradient norm lower-bounds\nthe suboptimality gap, prove that properly initialized finite-width networks\nadmit such regions around initialization, and establish that GD achieves linear\nconvergence within an LPLR, providing the first finite-width guarantee that\nmatches empirically observed rates. We validate our theory across diverse\nsettings, from controlled experiments on fully-connected networks to modern\nResNet architectures trained with stochastic methods, demonstrating that LPLR\nstructure emerges robustly in practical deep learning scenarios. By rigorously\nconnecting local landscape geometry to fast optimization through the NTK\nframework, our work provides a definitive theoretical explanation for the\nremarkable efficiency of gradient-based optimization in deep learning.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u8bc1\u660e\u5728NTK\u7a33\u5b9a\u6027\u5047\u8bbe\u4e0b\uff0c\u975e\u51f8\u635f\u5931\u51fd\u6570\u533a\u57df\u6ee1\u8db3\u5c40\u90e8PL\u6761\u4ef6\uff0c\u89e3\u91ca\u4e86\u68af\u5ea6\u4e0b\u964d\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u89c2\u5bdf\u5230\u7684\u6307\u6570\u6536\u655b\u901f\u7387\u3002", "motivation": "\u89e3\u51b3\u68af\u5ea6\u4e0b\u964d\u5728\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u975e\u51f8\u635f\u5931\u51fd\u6570\u4e2d\u6536\u655b\u901f\u7387\u7406\u8bba\u4e0e\u5b9e\u9645\u89c2\u5bdf\u4e0d\u7b26\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5c40\u90e8PL\u533a\u57df\uff08LPLR\uff09\u6982\u5ff5\uff0c\u8bc1\u660e\u6709\u9650\u5bbd\u5ea6\u7f51\u7edc\u5728\u521d\u59cb\u5316\u9644\u8fd1\u5b58\u5728LPLR\uff0c\u5e76\u5206\u6790\u68af\u5ea6\u4e0b\u964d\u5728LPLR\u5185\u7684\u7ebf\u6027\u6536\u655b\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86LPLR\u7ed3\u6784\u5728\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u7406\u8bba\u9996\u6b21\u5339\u914d\u4e86\u5b9e\u9645\u89c2\u5bdf\u5230\u7684\u6536\u655b\u901f\u7387\u3002", "conclusion": "\u901a\u8fc7NTK\u6846\u67b6\u5c06\u5c40\u90e8\u51e0\u4f55\u4e0e\u5feb\u901f\u4f18\u5316\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u68af\u5ea6\u4e0b\u964d\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u9ad8\u6548\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002"}}
{"id": "2507.21394", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.21394", "abs": "https://arxiv.org/abs/2507.21394", "authors": ["Shiva Raja", "Cansu Demirkiran", "Aakash Sarkar", "Milos Popovic", "Ajay Joshi"], "title": "Systolic Array-based Accelerator for State-Space Models", "comment": null, "summary": "Sequence modeling is crucial for AI to understand temporal data and detect\ncomplex time-dependent patterns. While recurrent neural networks (RNNs),\nconvolutional neural networks (CNNs), and Transformers have advanced in\ncapturing long-range dependencies, they struggle with achieving high accuracy\nwith very long sequences due to limited memory retention (fixed context\nwindow). State-Space Models (SSMs) leverage exponentially decaying memory\nenabling lengthy context window and so they process very long data sequences\nmore efficiently than recurrent and Transformer-based models. Unlike\ntraditional neural models like CNNs and RNNs, SSM-based models require solving\ndifferential equations through continuous integration, making training and\ninference both compute- and memory-intensive on conventional CPUs and GPUs. In\nthis paper we introduce a specialized hardware accelerator, EpochCore, for\naccelerating SSMs. EpochCore is based on systolic arrays (SAs) and is designed\nto enhance the energy efficiency and throughput of inference of SSM-based\nmodels for long-range sequence tasks. Within the SA, we propose a versatile\nprocessing element (PE) called LIMA-PE to perform traditional and specialized\nMAC operations to support traditional DNNs and SSMs. To complement the\nEpochCore microarchitecture, we propose a novel dataflow, ProDF, which enables\nhighly efficient execution of SSM-based models. By leveraging the LIMA-PE\nmicroarchitecture and ProDF, EpochCore achieves on average 250x gains in\nperformance and 45x improvement in energy efficiency, at the expense of 2x\nincrease in area cost over traditional SA-based accelerators, and around\n~2,000x improvement in latency/inference on LRA datasets compared to GPU kernel\noperations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEpochCore\u7684\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u7528\u4e8e\u9ad8\u6548\u5904\u7406\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u7684\u957f\u5e8f\u5217\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\uff08\u5982RNN\u3001CNN\u548cTransformer\uff09\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u56e0\u5185\u5b58\u9650\u5236\u800c\u8868\u73b0\u4e0d\u4f73\uff0cSSMs\u867d\u80fd\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u8f83\u9ad8\uff0c\u9700\u8981\u4e13\u7528\u786c\u4ef6\u52a0\u901f\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u8109\u52a8\u9635\u5217\uff08SAs\uff09\u7684EpochCore\u52a0\u901f\u5668\uff0c\u5f15\u5165\u591a\u529f\u80fd\u5904\u7406\u5355\u5143LIMA-PE\u548c\u9ad8\u6548\u6570\u636e\u6d41ProDF\uff0c\u4ee5\u652f\u6301SSMs\u7684\u9ad8\u6548\u6267\u884c\u3002", "result": "EpochCore\u5728\u6027\u80fd\u548c\u80fd\u6548\u4e0a\u5206\u522b\u5b9e\u73b0\u4e86250\u500d\u548c45\u500d\u7684\u63d0\u5347\uff0c\u5c3d\u7ba1\u9762\u79ef\u6210\u672c\u589e\u52a0\u4e862\u500d\uff0c\u4e14\u5728LRA\u6570\u636e\u96c6\u4e0a\u7684\u63a8\u7406\u5ef6\u8fdf\u6bd4GPU\u63d0\u9ad8\u4e86\u7ea62000\u500d\u3002", "conclusion": "EpochCore\u4e3aSSM\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u786c\u4ef6\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u5e8f\u5217\u4efb\u52a1\u7684\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2507.21119", "categories": ["cs.LG", "eess.SP", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.21119", "abs": "https://arxiv.org/abs/2507.21119", "authors": ["Yousuf Moiz Ali", "Jaroslaw E. Prilepsky", "Nicola Sambo", "Jo\u00e3o Pedro", "Mohammad M. Hosseini", "Antonio Napoli", "Sergei K. Turitsyn", "Pedro Freire"], "title": "Pre-, In-, and Post-Processing Class Imbalance Mitigation Techniques for Failure Detection in Optical Networks", "comment": "3 pages + 1 page for acknowledgement and references", "summary": "We compare pre-, in-, and post-processing techniques for class imbalance\nmitigation in optical network failure detection. Threshold Adjustment achieves\nthe highest F1 gain (15.3%), while Random Under-sampling (RUS) offers the\nfastest inference, highlighting a key performance-complexity trade-off.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u5149\u7f51\u7edc\u6545\u969c\u68c0\u6d4b\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u7f13\u89e3\u7684\u9884\u5904\u7406\u3001\u5904\u7406\u4e2d\u548c\u540e\u5904\u7406\u6280\u672f\uff0c\u9608\u503c\u8c03\u6574\u65b9\u6cd5F1\u589e\u76ca\u6700\u9ad8\uff0815.3%\uff09\uff0c\u800c\u968f\u673a\u6b20\u91c7\u6837\uff08RUS\uff09\u63a8\u7406\u901f\u5ea6\u6700\u5feb\u3002", "motivation": "\u89e3\u51b3\u5149\u7f51\u7edc\u6545\u969c\u68c0\u6d4b\u4e2d\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u7684\u6027\u80fd\u4e0e\u6548\u7387\u3002", "method": "\u91c7\u7528\u9884\u5904\u7406\uff08\u5982RUS\uff09\u3001\u5904\u7406\u4e2d\u548c\u540e\u5904\u7406\uff08\u5982\u9608\u503c\u8c03\u6574\uff09\u6280\u672f\u8fdb\u884c\u5b9e\u9a8c\u6bd4\u8f83\u3002", "result": "\u9608\u503c\u8c03\u6574\u65b9\u6cd5F1\u589e\u76ca\u6700\u9ad8\uff0815.3%\uff09\uff0cRUS\u63a8\u7406\u901f\u5ea6\u6700\u5feb\u3002", "conclusion": "\u4e0d\u540c\u65b9\u6cd5\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e0a\u5b58\u5728\u6743\u8861\uff0c\u9608\u503c\u8c03\u6574\u6548\u679c\u6700\u4f73\uff0cRUS\u901f\u5ea6\u6700\u5feb\u3002"}}
{"id": "2507.21593", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21593", "abs": "https://arxiv.org/abs/2507.21593", "authors": ["Erdeng Zhang", "Shuntian Zheng", "Sheng Wu", "Haoge Jia", "Zhe Ji", "Ailing Xiao"], "title": "Affine Invariant Semi-Blind Receiver: Joint Channel Estimation and High-Order Signal Detection for Multiuser Massive MIMO-OFDM Systems", "comment": null, "summary": "Massive multiple input and multiple output (MIMO) systems with orthogonal\nfrequency division multiplexing (OFDM) are foundational for downlink multi-user\n(MU) communication in future wireless networks, for their ability to enhance\nspectral efficiency and support a large number of users simultaneously.\nHowever, high user density intensifies severe inter-user interference (IUI) and\npilot overhead. Consequently, existing blind and semi-blind channel estimation\n(CE) and signal detection (SD) algorithms suffer performance degradation and\nincreased complexity, especially when further challenged by frequency-selective\nchannels and high-order modulation demands. To this end, this paper proposes a\nnovel semi-blind joint channel estimation and signal detection (JCESD) method.\nSpecifically, the proposed approach employs a hybrid precoding architecture to\nsuppress IUI. Furthermore we formulate JCESD as a non-convex constellation\nfitting optimization exploiting constellation affine invariance. Few pilots are\nused to achieve coarse estimation for initialization and ambiguity resolution.\nFor high-order modulations, a data augmentation mechanism utilizes the symmetry\nof quadrature amplitude modulation (QAM) constellations to increase the\neffective number of samples. To address frequency-selective channels, CE\naccuracy is then enhanced via an iterative refinement strategy that leverages\nimproved SD results. Simulation results demonstrate an average throughput gain\nof 11\\% over widely used pilot-based methods in MU scenarios, highlighting the\nproposed method's potential to improve spectral efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u534a\u76f2\u8054\u5408\u4fe1\u9053\u4f30\u8ba1\u4e0e\u4fe1\u53f7\u68c0\u6d4b\uff08JCESD\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21MIMO-OFDM\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u9884\u7f16\u7801\u6291\u5236\u7528\u6237\u95f4\u5e72\u6270\uff0c\u5229\u7528\u661f\u5ea7\u62df\u5408\u4f18\u5316\u548c\u8fed\u4ee3\u7ec6\u5316\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u9ad8\u7528\u6237\u5bc6\u5ea6\u5bfc\u81f4\u4e25\u91cd\u7684\u7528\u6237\u95f4\u5e72\u6270\u548c\u5bfc\u9891\u5f00\u9500\uff0c\u73b0\u6709\u76f2\u548c\u534a\u76f2\u65b9\u6cd5\u5728\u9891\u7387\u9009\u62e9\u6027\u4fe1\u9053\u548c\u9ad8\u9636\u8c03\u5236\u4e0b\u6027\u80fd\u4e0b\u964d\u4e14\u590d\u6742\u5ea6\u9ad8\u3002", "method": "\u91c7\u7528\u6df7\u5408\u9884\u7f16\u7801\u67b6\u6784\u6291\u5236\u5e72\u6270\uff0c\u5c06JCESD\u5efa\u6a21\u4e3a\u975e\u51f8\u661f\u5ea7\u62df\u5408\u4f18\u5316\uff0c\u5229\u7528\u5c11\u91cf\u5bfc\u9891\u521d\u59cb\u5316\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u8fed\u4ee3\u7ec6\u5316\u63d0\u5347\u6027\u80fd\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u5728\u591a\u7528\u6237\u573a\u666f\u4e2d\u5e73\u5747\u541e\u5410\u91cf\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u4e8e\u5bfc\u9891\u7684\u65b9\u6cd5\u63d0\u9ad811%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9891\u8c31\u6548\u7387\uff0c\u9002\u7528\u4e8e\u672a\u6765\u65e0\u7ebf\u7f51\u7edc\u7684\u9ad8\u5bc6\u5ea6\u7528\u6237\u573a\u666f\u3002"}}
{"id": "2507.21434", "categories": ["stat.ML", "cs.LG", "62H05, 62H12, 65C05, 68T05"], "pdf": "https://arxiv.org/pdf/2507.21434", "abs": "https://arxiv.org/abs/2507.21434", "authors": ["Agnideep Aich", "Ashit Baran Aich", "Bruce Wade"], "title": "Measuring Sample Quality with Copula Discrepancies", "comment": null, "summary": "The scalable Markov chain Monte Carlo (MCMC) algorithms that underpin modern\nBayesian machine learning, such as Stochastic Gradient Langevin Dynamics\n(SGLD), sacrifice asymptotic exactness for computational speed, creating a\ncritical diagnostic gap: traditional sample quality measures fail\ncatastrophically when applied to biased samplers. While powerful Stein-based\ndiagnostics can detect distributional mismatches, they provide no direct\nassessment of dependence structure, often the primary inferential target in\nmultivariate problems. We introduce the Copula Discrepancy (CD), a principled\nand computationally efficient diagnostic that leverages Sklar's theorem to\nisolate and quantify the fidelity of a sample's dependence structure\nindependent of its marginals. Our theoretical framework provides the first\nstructure-aware diagnostic specifically designed for the era of approximate\ninference. Empirically, we demonstrate that a moment-based CD dramatically\noutperforms standard diagnostics like effective sample size for hyperparameter\nselection in biased MCMC, correctly identifying optimal configurations where\ntraditional methods fail. Furthermore, our robust MLE-based variant can detect\nsubtle but critical mismatches in tail dependence that remain invisible to rank\ncorrelation-based approaches, distinguishing between samples with identical\nKendall's tau but fundamentally different extreme-event behavior. With\ncomputational overhead orders of magnitude lower than existing Stein\ndiscrepancies, the CD provides both immediate practical value for MCMC\npractitioners and a theoretical foundation for the next generation of\nstructure-aware sample quality assessment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCopula Discrepancy (CD)\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u8fd1\u4f3cMCMC\u91c7\u6837\u5668\u7684\u4f9d\u8d56\u7ed3\u6784\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u504f\u7f6e\u91c7\u6837\u5668\u4e2d\u7684\u5931\u6548\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u8d1d\u53f6\u65af\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u53ef\u6269\u5c55MCMC\u7b97\u6cd5\uff08\u5982SGLD\uff09\u727a\u7272\u4e86\u6e10\u8fd1\u7cbe\u786e\u6027\u4ee5\u6362\u53d6\u8ba1\u7b97\u901f\u5ea6\uff0c\u5bfc\u81f4\u4f20\u7edf\u6837\u672c\u8d28\u91cf\u8bca\u65ad\u65b9\u6cd5\u5728\u504f\u7f6e\u91c7\u6837\u5668\u4e2d\u5931\u6548\u3002\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bca\u65ad\u5de5\u5177\u6765\u8bc4\u4f30\u4f9d\u8d56\u7ed3\u6784\u7684\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8eSklar\u5b9a\u7406\uff0c\u63d0\u51faCopula Discrepancy (CD)\uff0c\u901a\u8fc7\u5206\u79bb\u548c\u91cf\u5316\u6837\u672c\u7684\u4f9d\u8d56\u7ed3\u6784\u6765\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u57fa\u4e8e\u77e9\u7684CD\u548c\u57fa\u4e8eMLE\u7684\u53d8\u4f53\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCD\u5728\u8d85\u53c2\u6570\u9009\u62e9\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u6709\u6548\u6837\u672c\u91cf\uff09\uff0c\u5e76\u80fd\u68c0\u6d4b\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u53d1\u73b0\u7684\u5c3e\u90e8\u4f9d\u8d56\u5dee\u5f02\u3002", "conclusion": "CD\u4e0d\u4ec5\u4e3aMCMC\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u8fd8\u4e3a\u4e0b\u4e00\u4ee3\u7ed3\u6784\u611f\u77e5\u6837\u672c\u8d28\u91cf\u8bc4\u4f30\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.21479", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.SY", "eess.SY", "math.IT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21479", "abs": "https://arxiv.org/abs/2507.21479", "authors": ["Zheng Wen", "Doina Precup", "Benjamin Van Roy", "Satinder Singh"], "title": "Capacity-Constrained Continual Learning", "comment": null, "summary": "Any agents we can possibly build are subject to capacity constraints, as\nmemory and compute resources are inherently finite. However, comparatively\nlittle attention has been dedicated to understanding how agents with limited\ncapacity should allocate their resources for optimal performance. The goal of\nthis paper is to shed some light on this question by studying a simple yet\nrelevant continual learning problem: the capacity-constrained\nlinear-quadratic-Gaussian (LQG) sequential prediction problem. We derive a\nsolution to this problem under appropriate technical conditions. Moreover, for\nproblems that can be decomposed into a set of sub-problems, we also demonstrate\nhow to optimally allocate capacity across these sub-problems in the steady\nstate. We view the results of this paper as a first step in the systematic\ntheoretical study of learning under capacity constraints.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6709\u9650\u5bb9\u91cf\u4ee3\u7406\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5bb9\u91cf\u53d7\u9650\u7684\u7ebf\u6027-\u4e8c\u6b21-\u9ad8\u65af\uff08LQG\uff09\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u6700\u4f18\u5206\u914d\u5b50\u95ee\u9898\u7684\u5bb9\u91cf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u6709\u9650\u5bb9\u91cf\u4ee3\u7406\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\u5173\u6ce8\u8f83\u5c11\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u5bb9\u91cf\u53d7\u9650\u7684LQG\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u63a8\u5bfc\u51fa\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63a2\u8ba8\u5b50\u95ee\u9898\u7684\u7a33\u6001\u5bb9\u91cf\u5206\u914d\u3002", "result": "\u5728\u9002\u5f53\u6280\u672f\u6761\u4ef6\u4e0b\uff0c\u5f97\u51fa\u4e86\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5c55\u793a\u4e86\u5b50\u95ee\u9898\u7684\u6700\u4f18\u5bb9\u91cf\u5206\u914d\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u4e3a\u5bb9\u91cf\u7ea6\u675f\u4e0b\u5b66\u4e60\u7684\u7cfb\u7edf\u6027\u7406\u8bba\u7814\u7a76\u63d0\u4f9b\u4e86\u521d\u6b65\u6210\u679c\u3002"}}
{"id": "2507.21135", "categories": ["cs.LG", "quant-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21135", "abs": "https://arxiv.org/abs/2507.21135", "authors": ["Alexander G. Abanov", "Luca Candelori", "Harold C. Steinacker", "Martin T. Wells", "Jerome R. Busemeyer", "Cameron J. Hogan", "Vahagn Kirakosyan", "Nicola Marzari", "Sunil Pinnamaneni", "Dario Villani", "Mengjia Xu", "Kharen Musaelian"], "title": "Quantum Geometry of Data", "comment": "27 pages, 14 figures, 1 table", "summary": "We demonstrate how Quantum Cognition Machine Learning (QCML) encodes data as\nquantum geometry. In QCML, features of the data are represented by learned\nHermitian matrices, and data points are mapped to states in Hilbert space. The\nquantum geometry description endows the dataset with rich geometric and\ntopological structure - including intrinsic dimension, quantum metric, and\nBerry curvature - derived directly from the data. QCML captures global\nproperties of data, while avoiding the curse of dimensionality inherent in\nlocal methods. We illustrate this on a number of synthetic and real-world\nexamples. Quantum geometric representation of QCML could advance our\nunderstanding of cognitive phenomena within the framework of quantum cognition.", "AI": {"tldr": "QCML\u901a\u8fc7\u91cf\u5b50\u51e0\u4f55\u7f16\u7801\u6570\u636e\uff0c\u5229\u7528Hermitian\u77e9\u9635\u8868\u793a\u7279\u5f81\uff0c\u6570\u636e\u70b9\u6620\u5c04\u5230Hilbert\u7a7a\u95f4\uff0c\u8d4b\u4e88\u6570\u636e\u96c6\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u62d3\u6251\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u8ba4\u77e5\u6846\u67b6\u4e0b\u6570\u636e\u8868\u793a\u7684\u5168\u5c40\u6027\u8d28\uff0c\u907f\u514d\u5c40\u90e8\u65b9\u6cd5\u7684\u7ef4\u5ea6\u8bc5\u5492\u3002", "method": "\u4f7f\u7528Hermitian\u77e9\u9635\u8868\u793a\u6570\u636e\u7279\u5f81\uff0c\u5c06\u6570\u636e\u70b9\u6620\u5c04\u5230Hilbert\u7a7a\u95f4\uff0c\u63d0\u53d6\u91cf\u5b50\u51e0\u4f55\u7ed3\u6784\uff08\u5982\u7ef4\u5ea6\u3001\u91cf\u5b50\u5ea6\u91cf\u548cBerry\u66f2\u7387\uff09\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u5c55\u793a\u4e86QCML\u7684\u5168\u5c40\u6027\u8d28\u6355\u6349\u80fd\u529b\u3002", "conclusion": "QCML\u7684\u91cf\u5b50\u51e0\u4f55\u8868\u793a\u6709\u52a9\u4e8e\u5728\u91cf\u5b50\u8ba4\u77e5\u6846\u67b6\u4e0b\u7406\u89e3\u8ba4\u77e5\u73b0\u8c61\u3002"}}
{"id": "2507.21626", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.21626", "abs": "https://arxiv.org/abs/2507.21626", "authors": ["\u00d6zlem Tu\u011ffe Demir", "Muhammed Selman Somuncu", "Ahmet M. Elbir", "Emil Bj\u00f6rnson"], "title": "Comprehensive Analysis of Behavioral Hardware Impairments in Cell-Free Massive MIMO-OFDM Uplink: Centralized Operation", "comment": "4 pages, 2 figures, presented at IEEE Signal Processing and\n  Communications Applications Conference (SIU), 2025", "summary": "Cell-free massive MIMO is a key 6G technology, offering superior spectral and\nenergy efficiency. However, its dense deployment of low-cost access points\n(APs) makes hardware impairments unavoidable. While narrowband impairments are\nwell-studied, their impact in wideband systems remains unexplored. This paper\nprovides the first comprehensive analysis of hardware impairments, such as\nnonlinear distortion in low-noise amplifiers, phase noise, in-phase-quadrature\nimbalance, and low-resolution analog-to-digital converters, on uplink spectral\nefficiency in cell-free massive MIMO. Using an OFDM waveform and centralized\nprocessing, APs share channel state information for joint uplink combining.\nLeveraging Bussgang decomposition, we derive a distortion-aware combining\nvector that optimizes spectral efficiency by modeling distortion as independent\ncolored noise.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5168\u9762\u5206\u6790\u4e86\u786c\u4ef6\u635f\u4f24\u5bf9\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u4e0a\u884c\u94fe\u8def\u9891\u8c31\u6548\u7387\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eBussgang\u5206\u89e3\u7684\u5931\u771f\u611f\u77e5\u5408\u5e76\u5411\u91cf\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u662f6G\u5173\u952e\u6280\u672f\uff0c\u4f46\u5176\u5bc6\u96c6\u90e8\u7f72\u7684\u4f4e\u6210\u672c\u63a5\u5165\u70b9\uff08APs\uff09\u4e0d\u53ef\u907f\u514d\u5b58\u5728\u786c\u4ef6\u635f\u4f24\uff0c\u5c24\u5176\u662f\u5bbd\u5e26\u7cfb\u7edf\u4e2d\u7684\u5f71\u54cd\u5c1a\u672a\u7814\u7a76\u3002", "method": "\u91c7\u7528OFDM\u6ce2\u5f62\u548c\u96c6\u4e2d\u5f0f\u5904\u7406\uff0cAPs\u5171\u4eab\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u8fdb\u884c\u8054\u5408\u4e0a\u884c\u94fe\u8def\u5408\u5e76\uff0c\u5229\u7528Bussgang\u5206\u89e3\u5efa\u6a21\u5931\u771f\u4e3a\u72ec\u7acb\u6709\u8272\u566a\u58f0\uff0c\u4f18\u5316\u9891\u8c31\u6548\u7387\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5931\u771f\u611f\u77e5\u5408\u5e76\u5411\u91cf\uff0c\u80fd\u591f\u6709\u6548\u4f18\u5316\u9891\u8c31\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u5bbd\u5e26\u7cfb\u7edf\u4e2d\u786c\u4ef6\u635f\u4f24\u5bf9\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u5f71\u54cd\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.21449", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21449", "abs": "https://arxiv.org/abs/2507.21449", "authors": ["Rohan Hitchcock", "Jesse Hoogland"], "title": "From Global to Local: A Scalable Benchmark for Local Posterior Sampling", "comment": "25 pages", "summary": "Degeneracy is an inherent feature of the loss landscape of neural networks,\nbut it is not well understood how stochastic gradient MCMC (SGMCMC) algorithms\ninteract with this degeneracy. In particular, current global convergence\nguarantees for common SGMCMC algorithms rely on assumptions which are likely\nincompatible with degenerate loss landscapes. In this paper, we argue that this\ngap requires a shift in focus from global to local posterior sampling, and, as\na first step, we introduce a novel scalable benchmark for evaluating the local\nsampling performance of SGMCMC algorithms. We evaluate a number of common\nalgorithms, and find that RMSProp-preconditioned SGLD is most effective at\nfaithfully representing the local geometry of the posterior distribution.\nAlthough we lack theoretical guarantees about global sampler convergence, our\nempirical results show that we are able to extract non-trivial local\ninformation in models with up to O(100M) parameters.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u968f\u673a\u68af\u5ea6MCMC\uff08SGMCMC\uff09\u7b97\u6cd5\u5728\u795e\u7ecf\u7f51\u7edc\u9000\u5316\u635f\u5931\u666f\u89c2\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u4ece\u5168\u5c40\u6536\u655b\u8f6c\u5411\u5c40\u90e8\u540e\u9a8c\u91c7\u6837\uff0c\u5e76\u5f15\u5165\u65b0\u57fa\u51c6\u8bc4\u4f30\u5c40\u90e8\u91c7\u6837\u6027\u80fd\u3002", "motivation": "\u7406\u89e3SGMCMC\u7b97\u6cd5\u5982\u4f55\u4e0e\u9000\u5316\u635f\u5931\u666f\u89c2\u4ea4\u4e92\uff0c\u586b\u8865\u5f53\u524d\u5168\u5c40\u6536\u655b\u5047\u8bbe\u4e0e\u9000\u5316\u666f\u89c2\u4e0d\u517c\u5bb9\u7684\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u53ef\u6269\u5c55\u7684\u5c40\u90e8\u91c7\u6837\u6027\u80fd\u8bc4\u4f30\u57fa\u51c6\uff0c\u6d4b\u8bd5\u591a\u79cd\u5e38\u89c1SGMCMC\u7b97\u6cd5\uff0c\u91cd\u70b9\u5206\u6790RMSProp\u9884\u5904\u7406\u7684SGLD\u3002", "result": "RMSProp\u9884\u5904\u7406\u7684SGLD\u6700\u80fd\u51c6\u786e\u53cd\u6620\u540e\u9a8c\u5206\u5e03\u7684\u5c40\u90e8\u51e0\u4f55\u7279\u6027\uff0c\u4e14\u5728O(100M)\u53c2\u6570\u6a21\u578b\u4e2d\u4ecd\u80fd\u63d0\u53d6\u6709\u6548\u5c40\u90e8\u4fe1\u606f\u3002", "conclusion": "\u5c3d\u7ba1\u7f3a\u4e4f\u5168\u5c40\u6536\u655b\u7406\u8bba\u4fdd\u8bc1\uff0c\u5c40\u90e8\u91c7\u6837\u65b9\u6cd5\u5728\u9000\u5316\u666f\u89c2\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.21136", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21136", "abs": "https://arxiv.org/abs/2507.21136", "authors": ["Mojtaba Moattari"], "title": "A Study on Variants of Conventional, Fuzzy, and Nullspace-Based Independence Criteria for Improving Supervised and Unsupervised Learning", "comment": null, "summary": "Unsupervised and supervised learning methods conventionally use kernels to\ncapture nonlinearities inherent in data structure. However experts have to\nensure their proposed nonlinearity maximizes variability and capture inherent\ndiversity of data. We reviewed all independence criteria to design unsupervised\nlearners. Then we proposed 3 independence criteria and used them to design\nunsupervised and supervised dimensionality reduction methods. We evaluated\ncontrast, accuracy and interpretability of these methods in both linear and\nneural nonlinear settings. The results show that the methods have outperformed\nthe baseline (tSNE, PCA, regularized LDA, VAE with (un)supervised learner and\nlayer sharing) and opened a new line of interpretable machine learning (ML) for\nthe researchers.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u72ec\u7acb\u6027\u51c6\u5219\uff0c\u7528\u4e8e\u8bbe\u8ba1\u65e0\u76d1\u7763\u548c\u76d1\u7763\u964d\u7ef4\u65b9\u6cd5\uff0c\u5e76\u5728\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u8bbe\u7f6e\u4e2d\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u7ed3\u679c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6838\u51fd\u6570\u6355\u6349\u6570\u636e\u975e\u7ebf\u6027\uff0c\u4f46\u9700\u4e13\u5bb6\u786e\u4fdd\u975e\u7ebf\u6027\u6700\u5927\u5316\u6570\u636e\u591a\u6837\u6027\u548c\u53d8\u5f02\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u72ec\u7acb\u6027\u51c6\u5219\uff0c\u8bbe\u8ba1\u65e0\u76d1\u7763\u548c\u76d1\u7763\u964d\u7ef4\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u5bf9\u6bd4\u5ea6\u3001\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982tSNE\u3001PCA\u7b49\uff09\uff0c\u4e3a\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002", "conclusion": "\u7814\u7a76\u4e3a\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002"}}
{"id": "2507.21635", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21635", "abs": "https://arxiv.org/abs/2507.21635", "authors": ["\u00d6zlem Tu\u011ffe Demir", "Emil Bj\u00f6rnson"], "title": "Impact of Phase Noise and Power Amplifier Non-Linearities on Downlink Cell-Free Massive MIMO-OFDM Systems", "comment": "6 pages, 3 figures, presented at IEEE SmartNets 2025", "summary": "Cell-free massive MIMO (multiple-input multiple-output) is a key enabler for\nthe sixth generation (6G) of mobile networks, offering significant spectral and\nenergy efficiency gains through user-centric operation of distributed access\npoints (APs). However, its reliance on low-cost APs introduces inevitable\nhardware impairments, whose combined impact on wideband downlink systems\nremains unexplored when analyzed using behavioral models. This paper presents a\ncomprehensive analysis of the downlink spectral efficiency (SE) in cell-free\nmassive MIMO-OFDM systems under practical hardware impairments, including phase\nnoise and third-order power amplifier nonlinearities. Both centralized and\ndistributed precoding strategies are examined. By leveraging the Bussgang\ndecomposition, we derive an SE expression and quantify the relative impact of\nimpairments through simulations. Our results reveal that phase noise causes\nmore severe degradation than power amplifier distortions, especially in\ndistributed operation, highlighting the need for future distortion-aware\nprecoding designs.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728\u786c\u4ef6\u635f\u4f24\uff08\u5982\u76f8\u4f4d\u566a\u58f0\u548c\u529f\u7387\u653e\u5927\u5668\u975e\u7ebf\u6027\uff09\u4e0b\uff0c\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO-OFDM\u7cfb\u7edf\u7684\u4e0b\u884c\u94fe\u8def\u9891\u8c31\u6548\u7387\uff08SE\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u9884\u7f16\u7801\u7b56\u7565\u7684\u5f71\u54cd\u3002", "motivation": "\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u662f6G\u79fb\u52a8\u7f51\u7edc\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u5176\u4f9d\u8d56\u4f4e\u6210\u672c\u63a5\u5165\u70b9\uff08APs\uff09\u5f15\u5165\u4e86\u786c\u4ef6\u635f\u4f24\uff0c\u8fd9\u4e9b\u635f\u4f24\u5bf9\u5bbd\u5e26\u4e0b\u884c\u94fe\u8def\u7cfb\u7edf\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u5229\u7528Bussgang\u5206\u89e3\u63a8\u5bfcSE\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u91cf\u5316\u76f8\u4f4d\u566a\u58f0\u548c\u529f\u7387\u653e\u5927\u5668\u975e\u7ebf\u6027\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u76f8\u5bf9\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u4f4d\u566a\u58f0\u6bd4\u529f\u7387\u653e\u5927\u5668\u975e\u7ebf\u6027\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u635f\u5bb3\u66f4\u4e25\u91cd\uff0c\u5c24\u5176\u662f\u5728\u5206\u5e03\u5f0f\u64cd\u4f5c\u4e2d\u3002", "conclusion": "\u672a\u6765\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u8003\u8651\u786c\u4ef6\u635f\u4f24\u7684\u9884\u7f16\u7801\u65b9\u6848\u3002"}}
{"id": "2507.21486", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21486", "abs": "https://arxiv.org/abs/2507.21486", "authors": ["Satoshi Kumabe", "Tianyu Song", "Ton Viet Ta"], "title": "Stochastic forest transition model dynamics and parameter estimation via deep learning", "comment": null, "summary": "Forest transitions, characterized by dynamic shifts between forest,\nagricultural, and abandoned lands, are complex phenomena. This study developed\na stochastic differential equation model to capture the intricate dynamics of\nthese transitions. We established the existence of global positive solutions\nfor the model and conducted numerical analyses to assess the impact of model\nparameters on deforestation incentives. To address the challenge of parameter\nestimation, we proposed a novel deep learning approach that estimates all model\nparameters from a single sample containing time-series observations of forest\nand agricultural land proportions. This innovative approach enables us to\nunderstand forest transition dynamics and deforestation trends at any future\ntime.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6a21\u578b\u6765\u7814\u7a76\u68ee\u6797\u8f6c\u578b\u7684\u590d\u6742\u52a8\u6001\uff0c\u63d0\u51fa\u4e86\u53c2\u6570\u4f30\u8ba1\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u6a21\u578b\u53c2\u6570\u5bf9\u68ee\u6797\u780d\u4f10\u6fc0\u52b1\u7684\u5f71\u54cd\u3002", "motivation": "\u68ee\u6797\u8f6c\u578b\u6d89\u53ca\u68ee\u6797\u3001\u519c\u4e1a\u548c\u5e9f\u5f03\u571f\u5730\u4e4b\u95f4\u7684\u52a8\u6001\u53d8\u5316\uff0c\u7814\u7a76\u5176\u590d\u6742\u52a8\u6001\u6709\u52a9\u4e8e\u7406\u89e3\u68ee\u6797\u780d\u4f10\u8d8b\u52bf\u3002", "method": "\u4f7f\u7528\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u4f30\u8ba1\u6240\u6709\u53c2\u6570\u3002", "result": "\u5efa\u7acb\u4e86\u6a21\u578b\u7684\u5168\u5c40\u6b63\u89e3\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u5206\u6790\u8bc4\u4f30\u4e86\u53c2\u6570\u5bf9\u68ee\u6797\u780d\u4f10\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u9884\u6d4b\u672a\u6765\u68ee\u6797\u8f6c\u578b\u52a8\u6001\u548c\u780d\u4f10\u8d8b\u52bf\u3002"}}
{"id": "2507.21147", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21147", "abs": "https://arxiv.org/abs/2507.21147", "authors": ["Fabrizio Lo Scudo", "Alessio De Rango", "Luca Furnari", "Alfonso Senatore", "Donato D'Ambrosio", "Giuseppe Mendicino", "Gianluigi Greco"], "title": "Advancing Wildfire Risk Prediction via Morphology-Aware Curriculum Contrastive Learning", "comment": "To appear in the Proceedings of ECAI 2025", "summary": "Wildfires significantly impact natural ecosystems and human health, leading\nto biodiversity loss, increased hydrogeological risks, and elevated emissions\nof toxic substances. Climate change exacerbates these effects, particularly in\nregions with rising temperatures and prolonged dry periods, such as the\nMediterranean. This requires the development of advanced risk management\nstrategies that utilize state-of-the-art technologies. However, in this\ncontext, the data show a bias toward an imbalanced setting, where the incidence\nof wildfire events is significantly lower than typical situations. This\nimbalance, coupled with the inherent complexity of high-dimensional\nspatio-temporal data, poses significant challenges for training deep learning\narchitectures. Moreover, since precise wildfire predictions depend mainly on\nweather data, finding a way to reduce computational costs to enable more\nfrequent updates using the latest weather forecasts would be beneficial. This\npaper investigates how adopting a contrastive framework can address these\nchallenges through enhanced latent representations for the patch's dynamic\nfeatures. We thus introduce a new morphology-based curriculum contrastive\nlearning that mitigates issues associated with diverse regional characteristics\nand enables the use of smaller patch sizes without compromising performance. An\nexperimental analysis is performed to validate the effectiveness of the\nproposed modeling strategies.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u91ce\u706b\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u548c\u9ad8\u7ef4\u65f6\u7a7a\u6570\u636e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f62\u6001\u5b66\u7684\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u91ce\u706b\u5bf9\u751f\u6001\u7cfb\u7edf\u548c\u4eba\u7c7b\u5065\u5eb7\u9020\u6210\u4e25\u91cd\u5f71\u54cd\uff0c\u6c14\u5019\u53d8\u5316\u52a0\u5267\u4e86\u8fd9\u4e00\u95ee\u9898\u3002\u73b0\u6709\u6570\u636e\u5b58\u5728\u4e0d\u5e73\u8861\u548c\u9ad8\u7ef4\u590d\u6742\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u98ce\u9669\u7ba1\u7406\u548c\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f62\u6001\u5b66\u7684\u8bfe\u7a0b\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u589e\u5f3a\u6f5c\u5728\u8868\u793a\u6765\u5904\u7406\u52a8\u6001\u7279\u5f81\uff0c\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u5e76\u652f\u6301\u66f4\u9891\u7e41\u7684\u5929\u6c14\u6570\u636e\u66f4\u65b0\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u4f7f\u7528\u66f4\u5c0f\u7684\u8865\u4e01\u5c3a\u5bf8\uff0c\u5e76\u9002\u5e94\u4e0d\u540c\u533a\u57df\u7279\u5f81\u3002", "conclusion": "\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4e3a\u89e3\u51b3\u91ce\u706b\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u548c\u9ad8\u7ef4\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u4e3a\u672a\u6765\u7684\u98ce\u9669\u7ba1\u7406\u548c\u9884\u6d4b\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.21644", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.21644", "abs": "https://arxiv.org/abs/2507.21644", "authors": ["Derya Nurcan-Atceken", "\u00d6zlem Tu\u011ffe Demir", "Aysegul Altin-Kayhan", "Emil Bj\u00f6rnson", "Cicek Cavdar", "Bulent Tavli"], "title": "Energy-Aware Resource Allocation for Multi-Operator Cell-Free Massive MIMO in V-CRAN Architectures", "comment": "6 pages, 2 figures, to be presented at 2025 International Conference\n  on Future Communications and Networks (FCN)", "summary": "Cell-free massive multiple-input multiple-output (MIMO) implemented in\nvirtualized cloud radio access networks (V-CRAN) has emerged as a promising\narchitecture to enhance spectral efficiency (SE), network flexibility, and\nenergy efficiency (EE) in next-generation wireless systems. In this work, we\ndevelop a holistic optimization framework for the efficient deployment of\ncell-free massive MIMO in V-CRAN with multiple mobile network operators (MNOs).\nSpecifically, we formulate a set of mixed-integer programming (MIP) models to\njointly optimize access point (AP) selection, user equipment (UE) association,\ncloud resource allocation, and MNO assignment while minimizing the maximum\ntotal power consumption (TPC) across MNOs. We consider two different scenarios\nbased on whether UEs can be assigned to arbitrary MNOs or not. The numerical\nresults demonstrate the impact of different deployment assumptions on power\nconsumption, highlighting that flexible UE-MNO assignment significantly reduces\nTPC. The findings provide key insights into optimizing resource management in\ncell-free massive MIMO V-CRAN, paving the way for energy-efficient wireless\nnetwork implementations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u865a\u62df\u5316\u4e91\u65e0\u7ebf\u63a5\u5165\u7f51\u7edc\uff08V-CRAN\uff09\u4e2d\u90e8\u7f72\u65e0\u5c0f\u533a\u5927\u89c4\u6a21MIMO\u7684\u4f18\u5316\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u8054\u5408\u4f18\u5316AP\u9009\u62e9\u3001UE\u5173\u8054\u3001\u4e91\u8d44\u6e90\u5206\u914d\u548cMNO\u5206\u914d\uff0c\u6700\u5c0f\u5316\u591a\u4e2aMNO\u7684\u603b\u529f\u8017\u3002", "motivation": "\u63d0\u5347\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u7cfb\u7edf\u7684\u9891\u8c31\u6548\u7387\u3001\u7f51\u7edc\u7075\u6d3b\u6027\u548c\u80fd\u6e90\u6548\u7387\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u6a21\u578b\uff0c\u8054\u5408\u4f18\u5316AP\u9009\u62e9\u3001UE\u5173\u8054\u3001\u4e91\u8d44\u6e90\u5206\u914d\u548cMNO\u5206\u914d\uff0c\u5e76\u8003\u8651\u4e24\u79cdUE-MNO\u5206\u914d\u573a\u666f\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u7075\u6d3b\u7684UE-MNO\u5206\u914d\u663e\u8457\u964d\u4f4e\u4e86\u603b\u529f\u8017\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65e0\u5c0f\u533a\u5927\u89c4\u6a21MIMO V-CRAN\u7684\u8d44\u6e90\u7ba1\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u80fd\u6e90\u9ad8\u6548\u7684\u65e0\u7ebf\u7f51\u7edc\u90e8\u7f72\u3002"}}
{"id": "2507.21712", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.21712", "abs": "https://arxiv.org/abs/2507.21712", "authors": ["Urban Eriksson"], "title": "An Equal-Probability Partition of the Sample Space: A Non-parametric Inference from Finite Samples", "comment": null, "summary": "This paper investigates what can be inferred about an arbitrary continuous\nprobability distribution from a finite sample of $N$ observations drawn from\nit. The central finding is that the $N$ sorted sample points partition the real\nline into $N+1$ segments, each carrying an expected probability mass of exactly\n$1/(N+1)$. This non-parametric result, which follows from fundamental\nproperties of order statistics, holds regardless of the underlying\ndistribution's shape. This equal-probability partition yields a discrete\nentropy of $\\log_2(N+1)$ bits, which quantifies the information gained from the\nsample and contrasts with Shannon's results for continuous variables. I compare\nthis partition-based framework to the conventional ECDF and discuss its\nimplications for robust non-parametric inference, particularly in density and\ntail estimation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4ece\u6709\u9650\u6837\u672c\u4e2d\u63a8\u65ad\u4efb\u610f\u8fde\u7eed\u6982\u7387\u5206\u5e03\u7684\u6027\u8d28\uff0c\u53d1\u73b0N\u4e2a\u6392\u5e8f\u6837\u672c\u70b9\u5c06\u5b9e\u7ebf\u5212\u5206\u4e3aN+1\u6bb5\uff0c\u6bcf\u6bb5\u671f\u671b\u6982\u7387\u8d28\u91cf\u5747\u4e3a1/(N+1)\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u4ece\u6709\u9650\u6837\u672c\u4e2d\u63a8\u65ad\u8fde\u7eed\u6982\u7387\u5206\u5e03\u7684\u7279\u6027\uff0c\u7279\u522b\u662f\u57fa\u4e8e\u6392\u5e8f\u6837\u672c\u70b9\u7684\u975e\u53c2\u6570\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u987a\u5e8f\u7edf\u8ba1\u91cf\u7684\u57fa\u672c\u6027\u8d28\uff0c\u5206\u6790N\u4e2a\u6392\u5e8f\u6837\u672c\u70b9\u5bf9\u5b9e\u7ebf\u7684\u5212\u5206\u53ca\u5176\u6982\u7387\u8d28\u91cf\u5206\u5e03\u3002", "result": "\u53d1\u73b0\u6bcf\u6bb5\u5212\u5206\u7684\u671f\u671b\u6982\u7387\u8d28\u91cf\u4e3a1/(N+1)\uff0c\u79bb\u6563\u71b5\u4e3alog2(N+1)\u6bd4\u7279\uff0c\u4e0eShannon\u7684\u8fde\u7eed\u53d8\u91cf\u7ed3\u679c\u5f62\u6210\u5bf9\u6bd4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u975e\u53c2\u6570\u63a8\u65ad\uff08\u5982\u5bc6\u5ea6\u548c\u5c3e\u90e8\u4f30\u8ba1\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5e76\u4e0e\u4f20\u7edfECDF\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002"}}
{"id": "2507.21152", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21152", "abs": "https://arxiv.org/abs/2507.21152", "authors": ["Hangli Ge", "Noboru Koshizuka"], "title": "Deep Unfolding for MIMO Signal Detection", "comment": null, "summary": "In this paper, we propose a deep unfolding neural network-based MIMO detector\nthat incorporates complex-valued computations using Wirtinger calculus. The\nmethod, referred as Dynamic Partially Shrinkage Thresholding (DPST), enables\nefficient, interpretable, and low-complexity MIMO signal detection. Unlike\nprior approaches that rely on real-valued approximations, our method operates\nnatively in the complex domain, aligning with the fundamental nature of signal\nprocessing tasks. The proposed algorithm requires only a small number of\ntrainable parameters, allowing for simplified training. Numerical results\ndemonstrate that the proposed method achieves superior detection performance\nwith fewer iterations and lower computational complexity, making it a practical\nsolution for next-generation massive MIMO systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5c55\u5f00\u795e\u7ecf\u7f51\u7edc\u7684MIMO\u68c0\u6d4b\u5668\uff0c\u91c7\u7528\u590d\u6570\u57df\u8ba1\u7b97\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u590d\u6742\u5ea6\u4f4e\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u4f9d\u8d56\u5b9e\u6570\u57df\u8fd1\u4f3c\uff0c\u800c\u4fe1\u53f7\u5904\u7406\u672c\u8d28\u4e0a\u662f\u590d\u6570\u57df\u95ee\u9898\uff0c\u9700\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u52a8\u6001\u90e8\u5206\u6536\u7f29\u9608\u503c\uff08DPST\uff09\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u590d\u6570\u57df\u64cd\u4f5c\uff0c\u53c2\u6570\u5c11\u4e14\u8bad\u7ec3\u7b80\u5355\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u68c0\u6d4b\u6027\u80fd\u4f18\u8d8a\uff0c\u8fed\u4ee3\u6b21\u6570\u5c11\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4e0b\u4e00\u4ee3\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.21696", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21696", "abs": "https://arxiv.org/abs/2507.21696", "authors": ["Abdelaziz Salama", "Zeinab Nezami", "Mohammed M. H. Qazzaz", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "title": "Edge Agentic AI Framework for Autonomous Network Optimisation in O-RAN", "comment": null, "summary": "The deployment of AI agents within legacy Radio Access Network (RAN)\ninfrastructure poses significant safety and reliability challenges for future\n6G networks. This paper presents a novel Edge AI framework for autonomous\nnetwork optimisation in Open RAN environments, addressing these challenges\nthrough three core innovations: (1) a persona-based multi-tools architecture\nenabling distributed, context-aware decision-making; (2) proactive anomaly\ndetection agent powered by traffic predictive tool; and (3) a safety, aligned\nreward mechanism that balances performance with operational stability.\nIntegrated into the RAN Intelligent Controller (RIC), our framework leverages\nmultimodal data fusion, including network KPIs, a traffic prediction model, and\nexternal information sources, to anticipate and respond to dynamic network\nconditions. Extensive evaluation using realistic 5G scenarios demonstrates that\nthe edge framework achieves zero network outages under high-stress conditions,\ncompared to 8.4% for traditional fixed-power networks and 3.3% for large\nlanguage model (LLM) agent-based approaches, while maintaining near real-time\nresponsiveness and consistent QoS. These results establish that, when equipped\nwith the right tools and contextual awareness, AI agents can be safely and\neffectively deployed in critical network infrastructure, laying the framework\nfor intelligent and autonomous 5G and beyond network operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fb9\u7f18AI\u6846\u67b6\uff0c\u7528\u4e8e\u5728Open RAN\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u7f51\u7edc\u4f18\u5316\uff0c\u901a\u8fc7\u4e09\u9879\u521b\u65b0\u89e3\u51b3\u4e866G\u7f51\u7edc\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "AI\u4ee3\u7406\u5728\u4f20\u7edfRAN\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u90e8\u7f72\u5bf96G\u7f51\u7edc\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u89d2\u8272\u7684\u591a\u5de5\u5177\u67b6\u6784\u3001\u4e3b\u52a8\u5f02\u5e38\u68c0\u6d4b\u4ee3\u7406\u548c\u5b89\u5168\u5bf9\u9f50\u5956\u52b1\u673a\u5236\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\uff0c\u5b9e\u73b0\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\u7684\u9884\u6d4b\u548c\u54cd\u5e94\u3002", "result": "\u5728\u771f\u5b9e5G\u573a\u666f\u4e2d\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u96f6\u7f51\u7edc\u4e2d\u65ad\uff0c\u4f18\u4e8e\u4f20\u7edf\u56fa\u5b9a\u529f\u7387\u7f51\u7edc\uff088.4%\uff09\u548c\u57fa\u4e8eLLM\u4ee3\u7406\u7684\u65b9\u6cd5\uff083.3%\uff09\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u914d\u5907\u9002\u5f53\u5de5\u5177\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u7684AI\u4ee3\u7406\u53ef\u4ee5\u5b89\u5168\u6709\u6548\u5730\u90e8\u7f72\u5728\u5173\u952e\u7f51\u7edc\u57fa\u7840\u8bbe\u65bd\u4e2d\uff0c\u4e3a\u667a\u80fd\u81ea\u4e3b\u76845G\u53ca\u672a\u6765\u7f51\u7edc\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.21807", "categories": ["stat.ML", "cs.LG", "62J07, 62H12, 62F07"], "pdf": "https://arxiv.org/pdf/2507.21807", "abs": "https://arxiv.org/abs/2507.21807", "authors": ["Robert Kuchen"], "title": "MIBoost: A Gradient Boosting Algorithm for Variable Selection After Multiple Imputation", "comment": "21 pages, 2 algorithms, includes a simulation study", "summary": "Statistical learning methods for automated variable selection, such as LASSO,\nelastic nets, or gradient boosting, have become increasingly popular tools for\nbuilding powerful prediction models. Yet, in practice, analyses are often\ncomplicated by missing data. The most widely used approach to address\nmissingness is multiple imputation, which creates several completed datasets.\nHowever, there is an ongoing debate on how to perform model selection in the\npresence of multiple imputed datasets. Simple strategies, such as pooling\nmodels across datasets, have been shown to have suboptimal properties. Although\nmore sophisticated methods exist, they are often difficult to implement and\ntherefore not widely applied. In contrast, two recent approaches modify the\nregularization methods LASSO and elastic nets by defining a single loss\nfunction, resulting in a unified set of coefficients across imputations. Our\nkey contribution is to extend this principle to the framework of component-wise\ngradient boosting by proposing MIBoost, a novel algorithm that employs a\nuniform variable-selection mechanism across imputed datasets. Simulation\nstudies suggest that our approach yields prediction performance comparable to\nthat of these recently proposed methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMIBoost\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u7f3a\u5931\u6570\u636e\u60c5\u51b5\u4e0b\u7edf\u4e00\u53d8\u91cf\u9009\u62e9\u673a\u5236\uff0c\u6269\u5c55\u4e86\u68af\u5ea6\u63d0\u5347\u6846\u67b6\u3002", "motivation": "\u7f3a\u5931\u6570\u636e\u662f\u7edf\u8ba1\u5206\u6790\u4e2d\u7684\u5e38\u89c1\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u591a\u91cd\u63d2\u8865\u5728\u6a21\u578b\u9009\u62e9\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u68af\u5ea6\u63d0\u5347\u6846\u67b6\uff0c\u63d0\u51faMIBoost\u7b97\u6cd5\uff0c\u5728\u591a\u91cd\u63d2\u8865\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u7edf\u4e00\u7684\u53d8\u91cf\u9009\u62e9\u673a\u5236\u3002", "result": "\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0cMIBoost\u7684\u9884\u6d4b\u6027\u80fd\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "MIBoost\u4e3a\u7f3a\u5931\u6570\u636e\u4e0b\u7684\u53d8\u91cf\u9009\u62e9\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u6613\u4e8e\u5b9e\u73b0\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.21698", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21698", "abs": "https://arxiv.org/abs/2507.21698", "authors": ["Abdelaziz Salama", "Mohammed M. H. Qazzaz", "Syed Danial Ali Shah", "Maryam Hafeez", "Syed Ali Zaidi", "Hamed Ahmadi"], "title": "EcoFL: Resource Allocation for Energy-Efficient Federated Learning in Multi-RAT ORAN Networks", "comment": null, "summary": "Federated Learning (FL) enables distributed model training on edge devices\nwhile preserving data privacy. However, FL deployments in wireless networks\nface significant challenges, including communication overhead, unreliable\nconnectivity, and high energy consumption, particularly in dynamic\nenvironments. This paper proposes EcoFL, an integrated FL framework that\nleverages the Open Radio Access Network (ORAN) architecture with multiple Radio\nAccess Technologies (RATs) to enhance communication efficiency and ensure\nrobust FL operations. EcoFL implements a two-stage optimisation approach: an\nRL-based rApp for dynamic RAT selection that balances energy efficiency with\nnetwork performance, and a CNN-based xApp for near real-time resource\nallocation with adaptive policies. This coordinated approach significantly\nenhances communication resilience under fluctuating network conditions.\nExperimental results demonstrate competitive FL model performance with 19\\%\nlower power consumption compared to baseline approaches, highlighting\nsubstantial potential for scalable, energy-efficient collaborative learning\napplications.", "AI": {"tldr": "EcoFL\u662f\u4e00\u4e2a\u96c6\u6210\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528ORAN\u67b6\u6784\u548c\u591aRAT\u6280\u672f\u4f18\u5316\u901a\u4fe1\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\uff08RL\u548cCNN\uff09\u964d\u4f4e\u80fd\u801719%\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ebf\u7f51\u7edc\u4e2d\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u5f00\u9500\u3001\u4e0d\u53ef\u9760\u8fde\u63a5\u548c\u9ad8\u80fd\u8017\u95ee\u9898\u3002", "method": "\u91c7\u7528ORAN\u67b6\u6784\u548c\u591aRAT\u6280\u672f\uff0c\u7ed3\u5408RL\u548cCNN\u8fdb\u884c\u52a8\u6001RAT\u9009\u62e9\u548c\u5b9e\u65f6\u8d44\u6e90\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u80fd\u8017\u964d\u4f4e19%\uff0c\u6a21\u578b\u6027\u80fd\u4e0e\u57fa\u7ebf\u76f8\u5f53\u3002", "conclusion": "EcoFL\u5728\u52a8\u6001\u7f51\u7edc\u4e2d\u5177\u6709\u9ad8\u6548\u3001\u8282\u80fd\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14144", "categories": ["eess.SP", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.14144", "abs": "https://arxiv.org/abs/2507.14144", "authors": ["Cyril Falcon", "Hassan Mortada", "Math\u00e9o Clavaud", "Jean-Philippe Michel"], "title": "Recursive KalmanNet: Analyse des capacit\u00e9s de g\u00e9n\u00e9ralisation d'un r\u00e9seau de neurones r\u00e9current guid\u00e9 par un filtre de Kalman", "comment": "4 pages, in French language. 4 figures. Accepted for publication in\n  GRETSI 2025 proceedings", "summary": "The Recursive KalmanNet, recently introduced by the authors, is a recurrent\nneural network guided by a Kalman filter, capable of estimating the state\nvariables and error covariance of stochastic dynamic systems from noisy\nmeasurements, without prior knowledge of the noise characteristics. This paper\nexplores its generalization capabilities in out-of-distribution scenarios,\nwhere the temporal dynamics of the test measurements differ from those\nencountered during training.\n  Le Recursive KalmanNet, r\\'ecemment introduit par les auteurs, est un\nr\\'eseau de neurones r\\'ecurrent guid\\'e par un filtre de Kalman, capable\nd'estimer les variables d'\\'etat et la covariance des erreurs des syst\\`emes\ndynamiques stochastiques \\`a partir de mesures bruit\\'ees, sans connaissance\npr\\'ealable des caract\\'eristiques des bruits. Cet article explore ses\ncapacit\\'es de g\\'en\\'eralisation dans des sc\\'enarios hors distribution, o\\`u\nles dynamiques temporelles des mesures de test diff\\`erent de celles\nrencontr\\'ees \\`a l'entra\\^inement.", "AI": {"tldr": "Recursive KalmanNet\u662f\u4e00\u79cd\u7531\u5361\u5c14\u66fc\u6ee4\u6ce2\u5f15\u5bfc\u7684\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u591f\u5728\u672a\u77e5\u566a\u58f0\u7279\u6027\u7684\u60c5\u51b5\u4e0b\u4f30\u8ba1\u52a8\u6001\u7cfb\u7edf\u7684\u72b6\u6001\u53d8\u91cf\u548c\u8bef\u5dee\u534f\u65b9\u5dee\u3002\u672c\u6587\u63a2\u8ba8\u4e86\u5176\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76Recursive KalmanNet\u5728\u6d4b\u8bd5\u6570\u636e\u4e0e\u8bad\u7ec3\u6570\u636e\u52a8\u6001\u7279\u6027\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u7ed3\u5408\u5361\u5c14\u66fc\u6ee4\u6ce2\uff0c\u65e0\u9700\u566a\u58f0\u5148\u9a8c\u77e5\u8bc6\u3002", "result": "\u5c55\u793a\u4e86Recursive KalmanNet\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "Recursive KalmanNet\u5728\u52a8\u6001\u7279\u6027\u4e0d\u540c\u7684\u6d4b\u8bd5\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.21155", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21155", "abs": "https://arxiv.org/abs/2507.21155", "authors": ["Malcolm Wolff", "Matthew Li", "Ravi Kiran Selvam", "Hanjing Zhu", "Kin G. Olivares", "Ruijun Ma", "Abhinav Katoch", "Shankar Ramasubramanian", "Mengfei Cao", "Roberto Bandarra", "Rahul Gopalsamy", "Stefania La Vattiata", "Sitan Yang", "Michael M. Mahoney"], "title": "SPADE-S: A Sparsity-Robust Foundational Forecaster", "comment": null, "summary": "Despite significant advancements in time series forecasting, accurate\nmodeling of time series with strong heterogeneity in magnitude and/or sparsity\npatterns remains challenging for state-of-the-art deep learning architectures.\nWe identify several factors that lead existing models to systematically\nunderperform on low-magnitude and sparse time series, including loss functions\nwith implicit biases toward high-magnitude series, training-time sampling\nmethods, and limitations of time series encoding methods.\n  SPADE-S is a robust forecasting architecture that significantly reduces\nmagnitude- and sparsity-based systematic biases and improves overall prediction\naccuracy. Empirical results demonstrate that SPADE-S outperforms existing\nstate-of-the-art approaches across a diverse set of use cases in demand\nforecasting. In particular, we show that, depending on the quantile forecast\nand magnitude of the series, SPADE-S can improve forecast accuracy by up to\n15%. This results in P90 overall forecast accuracy gains of 2.21%, 6.58%, and\n4.28%, and P50 forecast accuracy gains of 0.92%, 0.77%, and 1.95%,\nrespectively, for each of three distinct datasets, ranging from 3 million to\n700 million series, from a large online retailer.", "AI": {"tldr": "SPADE-S\u662f\u4e00\u79cd\u9488\u5bf9\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u9c81\u68d2\u67b6\u6784\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u57fa\u4e8e\u5e45\u5ea6\u548c\u7a00\u758f\u6027\u7684\u7cfb\u7edf\u504f\u5dee\uff0c\u5e76\u5728\u9700\u6c42\u9884\u6d4b\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u5728\u5e45\u5ea6\u548c\u7a00\u758f\u6027\u5dee\u5f02\u8f83\u5927\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "method": "SPADE-S\u901a\u8fc7\u6539\u8fdb\u635f\u5931\u51fd\u6570\u3001\u8bad\u7ec3\u91c7\u6837\u65b9\u6cd5\u548c\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u65b9\u6cd5\uff0c\u51cf\u5c11\u5bf9\u9ad8\u5e45\u5ea6\u5e8f\u5217\u7684\u504f\u5411\u3002", "result": "SPADE-S\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u6700\u9ad8\u53ef\u8fbe15%\uff0c\u5177\u4f53\u8868\u73b0\u4e3aP90\u548cP50\u51c6\u786e\u7387\u7684\u63d0\u5347\u3002", "conclusion": "SPADE-S\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u5f02\u8d28\u6027\u548c\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.21704", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21704", "abs": "https://arxiv.org/abs/2507.21704", "authors": ["Hyeon Seok Rou", "Kuranage Roche Rayan Ranasinghe", "Vincent Savaux", "Giuseppe Thadeu Freitas de Abreu", "David Gonz\u00e1lez G.", "Christos Masouros"], "title": "Affine Frequency Division Multiplexing (AFDM) for 6G: Properties, Features, and Challenges", "comment": null, "summary": "Affine frequency division multiplexing (AFDM) is an emerging waveform\ncandidate for future sixth generation (6G) systems offering a range of\npromising features, such as enhanced robustness in heterogeneous and\nhigh-mobility environments, as well as inherent suitability for integrated\nsensing and communications (ISAC) applications. In addition, unlike other\ncandidates such as orthogonal time-frequency space (OTFS) modulation, AFDM\nprovides several unique advantages that strengthen its relevance to practical\ndeployment and standardization in 6G. Notably, as a natural generalization of\northogonal frequency division multiplexing (OFDM), strong backward\ncompatibility with existing conventional systems is guaranteed, while also\noffering novel possibilities in waveform design, for example to enable\nphysical-layer security through its inherent chirp parametrization. In all,\nthis article provides an overview of AFDM, emphasizing its suitability as a\ncandidate waveform for 6G standardization. First, we provide a concise\nintroduction to the fundamental properties and unique characteristics of AFDM,\nfollowed by highlights of its advantageous features, and finally a discussion\nof its potential and challenges in 6G standardization efforts and\nrepresentative requirements.", "AI": {"tldr": "AFDM\u662f\u4e00\u79cd\u65b0\u5174\u76846G\u6ce2\u5f62\u5019\u9009\u6280\u672f\uff0c\u5177\u6709\u5728\u9ad8\u79fb\u52a8\u6027\u548c\u5f02\u6784\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u9002\u7528\u4e8eISAC\u5e94\u7528\u7684\u6f5c\u529b\u3002\u5b83\u4f5c\u4e3aOFDM\u7684\u81ea\u7136\u6269\u5c55\uff0c\u4fdd\u8bc1\u4e86\u4e0e\u73b0\u6709\u7cfb\u7edf\u7684\u517c\u5bb9\u6027\uff0c\u5e76\u901a\u8fc7\u5176\u56fa\u6709\u7684\u7ebf\u6027\u8c03\u9891\u53c2\u6570\u5316\u63d0\u4f9b\u7269\u7406\u5c42\u5b89\u5168\u6027\u3002", "motivation": "\u7814\u7a76AFDM\u4f5c\u4e3a6G\u6807\u51c6\u5316\u5019\u9009\u6ce2\u5f62\u7684\u6f5c\u529b\uff0c\u5f3a\u8c03\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u517c\u5bb9\u6027\u4f18\u52bf\u3002", "method": "\u6982\u8ff0AFDM\u7684\u57fa\u672c\u7279\u6027\u548c\u72ec\u7279\u7279\u5f81\uff0c\u5206\u6790\u5176\u4f18\u52bf\uff0c\u5e76\u8ba8\u8bba\u5176\u57286G\u6807\u51c6\u5316\u4e2d\u7684\u6f5c\u529b\u548c\u6311\u6218\u3002", "result": "AFDM\u5728\u9c81\u68d2\u6027\u3001\u517c\u5bb9\u6027\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6709\u671b\u6210\u4e3a6G\u6807\u51c6\u5316\u7684\u6709\u529b\u5019\u9009\u3002", "conclusion": "AFDM\u56e0\u5176\u72ec\u7279\u4f18\u52bf\u548c\u517c\u5bb9\u6027\uff0c\u5177\u5907\u6210\u4e3a6G\u6807\u51c6\u5316\u6ce2\u5f62\u7684\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u76f8\u5173\u6311\u6218\u3002"}}
{"id": "2507.21160", "categories": ["cs.LG", "cs.AI", "68T07 (Primary), 68T45, 68T10 (Secondary)", "I.5.1"], "pdf": "https://arxiv.org/pdf/2507.21160", "abs": "https://arxiv.org/abs/2507.21160", "authors": ["Lakpa Tamang", "Mohamed Reda Bouadjenek", "Richard Dazeley", "Sunil Aryal"], "title": "Handling Out-of-Distribution Data: A Survey", "comment": "20 pages, 6 figures, 6 tables. Accepted at IEEE Transactions on\n  Knowledge and Data Engineering", "summary": "In the field of Machine Learning (ML) and data-driven applications, one of\nthe significant challenge is the change in data distribution between the\ntraining and deployment stages, commonly known as distribution shift. This\npaper outlines different mechanisms for handling two main types of distribution\nshifts: (i) Covariate shift: where the value of features or covariates change\nbetween train and test data, and (ii) Concept/Semantic-shift: where model\nexperiences shift in the concept learned during training due to emergence of\nnovel classes in the test phase. We sum up our contributions in three folds.\nFirst, we formalize distribution shifts, recite on how the conventional method\nfails to handle them adequately and urge for a model that can simultaneously\nperform better in all types of distribution shifts. Second, we discuss why\nhandling distribution shifts is important and provide an extensive review of\nthe methods and techniques that have been developed to detect, measure, and\nmitigate the effects of these shifts. Third, we discuss the current state of\ndistribution shift handling mechanisms and propose future research directions\nin this area. Overall, we provide a retrospective synopsis of the literature in\nthe distribution shift, focusing on OOD data that had been overlooked in the\nexisting surveys.", "AI": {"tldr": "\u672c\u6587\u603b\u7ed3\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u6570\u636e\u5206\u5e03\u53d8\u5316\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u5904\u7406\u534f\u53d8\u91cf\u504f\u79fb\u548c\u6982\u5ff5\u504f\u79fb\u7684\u65b9\u6cd5\uff0c\u5e76\u56de\u987e\u4e86\u76f8\u5173\u6280\u672f\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u8bad\u7ec3\u4e0e\u90e8\u7f72\u9636\u6bb5\u6570\u636e\u5206\u5e03\u53d8\u5316\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6027\u80fd\u3002", "method": "\u5f62\u5f0f\u5316\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u56de\u987e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "\u603b\u7ed3\u4e86\u5206\u5e03\u504f\u79fb\u7684\u68c0\u6d4b\u3001\u6d4b\u91cf\u548c\u7f13\u89e3\u6280\u672f\uff0c\u5f3a\u8c03\u4e86OOD\u6570\u636e\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u5206\u5e03\u504f\u79fb\u662f\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u95ee\u9898\uff0c\u672a\u6765\u9700\u5f00\u53d1\u66f4\u5168\u9762\u7684\u5904\u7406\u65b9\u6cd5\u3002"}}
{"id": "2507.21879", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.21879", "abs": "https://arxiv.org/abs/2507.21879", "authors": ["Xianxin Song", "Xianghao Yu", "Jie Xu", "Derrick Wing Kwan Ng"], "title": "CRB-Rate Tradeoff for Bistatic ISAC with Gaussian Information and Deterministic Sensing Signals", "comment": "13 pages,6 figures", "summary": "In this paper, we investigate a bistatic integrated sensing and\ncommunications (ISAC) system, consisting of a multi-antenna base station (BS),\na multi-antenna sensing receiver, a single-antenna communication user (CU), and\na point target to be sensed. Specifically, the BS transmits a superposition of\nGaussian information and deterministic sensing signals. The BS aims to deliver\ninformation symbols to the CU, while the sensing receiver aims to estimate the\ntarget's direction-of-arrival (DoA) with respect to the sensing receiver by\nprocessing the echo signals. For the sensing receiver, we assume that only the\nsequences of the deterministic sensing signals and the covariance matrix of the\ninformation signals are perfectly known, whereas the specific realizations of\nthe information signals remain unavailable. Under this setup, we first derive\nthe corresponding Cram\\'er-Rao bounds (CRBs) for DoA estimation and propose\npractical estimators to accurately estimate the target's DoA. Subsequently, we\nformulate the transmit beamforming design as an optimization problem aiming to\nminimize the CRB, subject to a minimum signal-to-interference-plus-noise ratio\n(SINR) requirement at the CU and a maximum transmit power constraint at the BS.\nWhen the BS employs only Gaussian information signals, the resulting\nbeamforming optimization problem is convex, enabling the derivation of an\noptimal solution. In contrast, when both Gaussian information and deterministic\nsensing signals are transmitted, the resulting problem is non-convex and a\nlocally optimal solution is acquired by exploiting successive convex\napproximation (SCA). Finally, numerical results demonstrate that employing\nGaussian information signals leads to a notable performance degradation for\ntarget sensing and the proposed transmit beamforming design achieves a superior\nISAC performance boundary compared with various benchmark schemes.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e00\u79cd\u53cc\u57fa\u5730\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u901a\u4fe1\u4e0e\u611f\u77e5\u6027\u80fd\u7684\u5e73\u8861\u3002", "motivation": "\u63a2\u7d22\u5728\u53cc\u57fa\u5730ISAC\u7cfb\u7edf\u4e2d\uff0c\u5982\u4f55\u901a\u8fc7\u4f18\u5316\u53d1\u5c04\u4fe1\u53f7\u8bbe\u8ba1\uff0c\u540c\u65f6\u6ee1\u8db3\u901a\u4fe1\u7528\u6237\u7684\u4fe1\u606f\u4f20\u8f93\u9700\u6c42\u548c\u611f\u77e5\u63a5\u6536\u5668\u7684\u76ee\u6807\u65b9\u5411\u4f30\u8ba1\u9700\u6c42\u3002", "method": "\u63a8\u5bfc\u4e86\u65b9\u5411\u4f30\u8ba1\u7684Cram\u00e9r-Rao\u754c\uff08CRB\uff09\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8eCRB\u7684\u6ce2\u675f\u6210\u5f62\u4f18\u5316\u95ee\u9898\uff0c\u5206\u522b\u9488\u5bf9\u4ec5\u9ad8\u65af\u4fe1\u606f\u4fe1\u53f7\u548c\u6df7\u5408\u4fe1\u53f7\u8bbe\u8ba1\u4e86\u51f8\u4f18\u5316\u548cSCA\u65b9\u6cd5\u6c42\u89e3\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u4ec5\u4f7f\u7528\u9ad8\u65af\u4fe1\u606f\u4fe1\u53f7\u4f1a\u663e\u8457\u964d\u4f4e\u611f\u77e5\u6027\u80fd\uff0c\u800c\u63d0\u51fa\u7684\u6ce2\u675f\u6210\u5f62\u8bbe\u8ba1\u5728ISAC\u6027\u80fd\u8fb9\u754c\u4e0a\u4f18\u4e8e\u57fa\u51c6\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u53d1\u5c04\u4fe1\u53f7\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347ISAC\u7cfb\u7edf\u7684\u7efc\u5408\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6df7\u5408\u4fe1\u53f7\u4f20\u8f93\u573a\u666f\u4e0b\u3002"}}
{"id": "2507.21164", "categories": ["cs.LG", "cs.AI", "eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21164", "abs": "https://arxiv.org/abs/2507.21164", "authors": ["Nicolas Pinon", "Carole Lartizien"], "title": "OCSVM-Guided Representation Learning for Unsupervised Anomaly Detection", "comment": null, "summary": "Unsupervised anomaly detection (UAD) aims to detect anomalies without labeled\ndata, a necessity in many machine learning applications where anomalous samples\nare rare or not available. Most state-of-the-art methods fall into two\ncategories: reconstruction-based approaches, which often reconstruct anomalies\ntoo well, and decoupled representation learning with density estimators, which\ncan suffer from suboptimal feature spaces. While some recent methods attempt to\ncouple feature learning and anomaly detection, they often rely on surrogate\nobjectives, restrict kernel choices, or introduce approximations that limit\ntheir expressiveness and robustness. To address this challenge, we propose a\nnovel method that tightly couples representation learning with an analytically\nsolvable one-class SVM (OCSVM), through a custom loss formulation that directly\naligns latent features with the OCSVM decision boundary. The model is evaluated\non two tasks: a new benchmark based on MNIST-C, and a challenging brain MRI\nsubtle lesion detection task. Unlike most methods that focus on large,\nhyperintense lesions at the image level, our approach succeeds to target small,\nnon-hyperintense lesions, while we evaluate voxel-wise metrics, addressing a\nmore clinically relevant scenario. Both experiments evaluate a form of\nrobustness to domain shifts, including corruption types in MNIST-C and\nscanner/age variations in MRI. Results demonstrate performance and robustness\nof our proposed mode,highlighting its potential for general UAD and real-world\nmedical imaging applications. The source code is available at\nhttps://github.com/Nicolas-Pinon/uad_ocsvm_guided_repr_learning", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u8868\u793a\u5b66\u4e60\u548c\u4e00\u7c7bSVM\uff0c\u76f4\u63a5\u4f18\u5316\u6f5c\u5728\u7279\u5f81\u4e0e\u51b3\u7b56\u8fb9\u754c\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u7279\u5f81\u7a7a\u95f4\u6216\u91cd\u5efa\u6548\u679c\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u8868\u8fbe\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u5c06\u8868\u793a\u5b66\u4e60\u4e0e\u89e3\u6790\u53ef\u89e3\u7684\u4e00\u7c7bSVM\u8026\u5408\u3002", "result": "\u5728MNIST-C\u548c\u8111MRI\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u68c0\u6d4b\u5c0f\u3001\u975e\u9ad8\u4fe1\u53f7\u75c5\u53d8\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u548c\u533b\u5b66\u5f71\u50cf\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.21956", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.21956", "abs": "https://arxiv.org/abs/2507.21956", "authors": ["Atiquzzaman Mondal", "Amira Bendaimi", "Huseyin Arslan"], "title": "A Novel Framework for Near-Field Covert Communications with RIS and RSMA", "comment": "12 pages, 7 figures, IEEE Transactions on Communications", "summary": "This paper explores the near field (NF) covert communication with the aid of\nrate-splitting multiple access (RSMA) and reconfigurable intelligent surfaces\n(RIS). In particular, the RIS operates in the NF of both the legitimate user\nand the passive adversary, enhancing the legitimate users received signal while\nsuppressing the adversarys detection capability. Whereas, the base station (BS)\napplies RSMA to increase the covert communication rate composed of a private\nand a shared rate component. To characterize system covertness, we derive\nclosed form expressions for the detection error probability (DEP), outage\nprobability (OP), and optimal detection threshold for the adversary. We\nformulate a non-convex joint beamforming optimization problem at the BS and RIS\nunder unit-modulus constraints to maximize the covert rate. To tackle this, we\npropose an alternating optimization (AO) algorithm, where the BS beamformer is\ndesigned using a two-stage iterative method based on successive convex\napproximation (SCA). Additionally, two low-complexity techniques are introduced\nto further reduce the adversarys received power. Simulation results demonstrate\nthat the proposed algorithm effectively improves the covert communication rate,\nhighlighting the potential of near field RSMA-RIS integration in covert\ncommunication.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u8fd1\u573a\uff08NF\uff09\u9690\u853d\u901a\u4fe1\uff0c\u7ed3\u5408\u901f\u7387\u5206\u5272\u591a\u5740\u63a5\u5165\uff08RSMA\uff09\u548c\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u6ce2\u675f\u6210\u5f62\u63d0\u5347\u9690\u853d\u901a\u4fe1\u901f\u7387\u3002", "motivation": "\u63a2\u7d22\u8fd1\u573a\u73af\u5883\u4e0bRIS\u548cRSMA\u7684\u7ed3\u5408\uff0c\u4ee5\u589e\u5f3a\u5408\u6cd5\u7528\u6237\u7684\u4fe1\u53f7\u5e76\u6291\u5236\u88ab\u52a8\u5bf9\u624b\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4ea4\u66ff\u4f18\u5316\uff08AO\uff09\u7b97\u6cd5\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u8fed\u4ee3\u65b9\u6cd5\u548c\u4f4e\u590d\u6742\u5ea6\u6280\u672f\uff0c\u4f18\u5316BS\u548cRIS\u7684\u6ce2\u675f\u6210\u5f62\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u9690\u853d\u901a\u4fe1\u901f\u7387\uff0c\u9a8c\u8bc1\u4e86\u8fd1\u573aRSMA-RIS\u96c6\u6210\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd1\u573aRSMA-RIS\u96c6\u6210\u5728\u9690\u853d\u901a\u4fe1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.21166", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21166", "abs": "https://arxiv.org/abs/2507.21166", "authors": ["Ren Zhuang", "Ben Wang", "Shuifa Sun"], "title": "AGORA: Incentivizing Group Emergence Capability in LLMs via Group Distillation", "comment": null, "summary": "Progress in complex reasoning is constrained by the static nature of the\ncurrent training datasets. We propose structured interaction as a new scaling\naxis, moving beyond the prevailing paradigm of increasing model parameters. Our\nself-evolving framework, AGORA, enables a collaborative ensemble to achieve\nreasoning performance exceeding state-of-the-art monolithic systems by up to\n4.45 percentage points on challenging mathematical benchmarks. This gain stems\nfrom group emergent ability-the synthesis of collective capabilities\nunattainable by isolated models, validating interaction as a scalable driver of\nintelligence. Our results position the engineering of collaborative ecosystems\nas a vital frontier for capability emergence.", "AI": {"tldr": "AGORA\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u4ea4\u4e92\u63d0\u5347\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u4f20\u7edf\u6a21\u578b\u53c2\u6570\u589e\u52a0\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u53474.45%\u3002", "motivation": "\u5f53\u524d\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u9759\u6001\u7279\u6027\u9650\u5236\u4e86\u590d\u6742\u63a8\u7406\u7684\u8fdb\u5c55\uff0c\u9700\u8981\u65b0\u7684\u6269\u5c55\u65b9\u5411\u3002", "method": "\u63d0\u51faAGORA\u81ea\u6f14\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u534f\u4f5c\u96c6\u6210\u5b9e\u73b0\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002", "result": "\u5728\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u8d85\u8fc7\u73b0\u6709\u6700\u4f73\u7cfb\u7edf4.45\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u534f\u4f5c\u751f\u6001\u7cfb\u7edf\u5de5\u7a0b\u662f\u5b9e\u73b0\u80fd\u529b\u6d8c\u73b0\u7684\u91cd\u8981\u524d\u6cbf\u3002"}}
{"id": "2507.22013", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22013", "abs": "https://arxiv.org/abs/2507.22013", "authors": ["Alessandro Mirri", "Venkatesh Khammammetti", "Beyza Dabak", "Enrico Paolini", "Krishna Narayanan", "Robert Calderbank"], "title": "Zak-OTFS Based Coded Random Access for Uplink mMTC", "comment": null, "summary": "This paper proposes a grant-free coded random access (CRA) scheme for uplink\nmassive machine-type communications (mMTC), based on Zak-orthogonal time\nfrequency space (Zak-OTFS) modulation in the delay-Doppler domain. The scheme\nis tailored for doubly selective wireless channels, where conventional\northogonal frequency-division multiplexing (OFDM)-based CRA suffers from\nunreliable inter-slot channel prediction due to time-frequency variability. By\nexploiting the predictable nature of Zak-OTFS, the proposed approach enables\naccurate channel estimation across slots, facilitating reliable successive\ninterference cancellation across user packet replicas. A fair comparison with\nan OFDM-based CRA baseline shows that the proposed scheme achieves\nsignificantly lower packet loss rates under high mobility and user density.\nExtensive simulations over the standardized Veh-A channel confirm the\nrobustness and scalability of Zak-OTFS-based CRA, supporting its applicability\nto future mMTC deployments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eZak-OTFS\u8c03\u5ea6\u7684\u514d\u6388\u6743\u7f16\u7801\u968f\u673a\u63a5\u5165\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9ad8\u79fb\u52a8\u6027\u548c\u7528\u6237\u5bc6\u5ea6\u7684mMTC\u573a\u666f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6570\u636e\u5305\u4e22\u5931\u7387\u3002", "motivation": "\u4f20\u7edfOFDM-based CRA\u5728\u53cc\u9009\u62e9\u6027\u65e0\u7ebf\u4fe1\u9053\u4e2d\u7531\u4e8e\u65f6\u9891\u53d8\u5316\u5bfc\u81f4\u4fe1\u9053\u9884\u6d4b\u4e0d\u53ef\u9760\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5065\u7684\u65b9\u6848\u3002", "method": "\u5229\u7528Zak-OTFS\u7684\u53ef\u9884\u6d4b\u6027\uff0c\u5b9e\u73b0\u8de8\u65f6\u9699\u7684\u51c6\u786e\u4fe1\u9053\u4f30\u8ba1\uff0c\u652f\u6301\u7528\u6237\u6570\u636e\u5305\u590d\u672c\u7684\u53ef\u9760\u5e72\u6270\u6d88\u9664\u3002", "result": "\u4e0eOFDM-based CRA\u76f8\u6bd4\uff0c\u5728\u9ad8\u79fb\u52a8\u6027\u548c\u7528\u6237\u5bc6\u5ea6\u4e0b\u663e\u8457\u964d\u4f4e\u6570\u636e\u5305\u4e22\u5931\u7387\u3002", "conclusion": "Zak-OTFS-based CRA\u5728\u6807\u51c6\u5316Veh-A\u4fe1\u9053\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u672a\u6765mMTC\u90e8\u7f72\u3002"}}
{"id": "2507.21179", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21179", "abs": "https://arxiv.org/abs/2507.21179", "authors": ["Yuqi Jin", "Zihan Hu", "Weiteng Zhang", "Weihao Xie", "Jianwei Shuai", "Xian Shen", "Zhen Feng"], "title": "LLM-Adapted Interpretation Framework for Machine Learning Models", "comment": "11 pages, 8 figures, 2 tables", "summary": "Background & Aims: High-performance machine learning models like XGBoost are\noften \"black boxes,\" limiting their clinical adoption due to a lack of\ninterpretability. This study aims to bridge the gap between predictive accuracy\nand narrative transparency for sarcopenia risk assessment. Methods: We propose\nthe LLM-Adapted Interpretation Framework (LAI-ML), a novel knowledge\ndistillation architecture. LAI-ML transforms feature attributions from a\ntrained XGBoost model into a probabilistic format using specialized techniques\n(HAGA and CACS). A Large Language Model (LLM), guided by a reinforcement\nlearning loop and case-based retrieval, then generates data-faithful diagnostic\nnarratives. Results: The LAI-ML framework achieved 83% prediction accuracy,\nsignificantly outperforming the baseline XGBoost model, 13% higher. Notably,\nthe LLM not only replicated the teacher model's logic but also corrected its\npredictions in 21.7% of discordant cases, demonstrating enhanced reasoning.\nConclusion: LAI-ML effectively translates opaque model predictions into\ntrustworthy and interpretable clinical insights, offering a deployable solution\nto the \"black-box\" problem in medical AI.", "AI": {"tldr": "LAI-ML\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548cLLM\u751f\u6210\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u53d9\u8ff0\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u89e3\u51b3\u9ad8\u6027\u80fd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982XGBoost\uff09\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u56e0\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLAI-ML\u6846\u67b6\uff0c\u7ed3\u5408HAGA\u548cCACS\u6280\u672f\u5c06XGBoost\u7279\u5f81\u5f52\u56e0\u8f6c\u5316\u4e3a\u6982\u7387\u683c\u5f0f\uff0c\u5e76\u5229\u7528LLM\u751f\u6210\u8bca\u65ad\u53d9\u8ff0\u3002", "result": "LAI-ML\u9884\u6d4b\u51c6\u786e\u7387\u8fbe83%\uff0c\u6bd4\u57fa\u7ebfXGBoost\u9ad813%\uff0c\u4e14LLM\u572821.7%\u4e0d\u4e00\u81f4\u6848\u4f8b\u4e2d\u4fee\u6b63\u9884\u6d4b\u3002", "conclusion": "LAI-ML\u6210\u529f\u5c06\u9ed1\u76d2\u9884\u6d4b\u8f6c\u5316\u4e3a\u53ef\u4fe1\u4e14\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u89c1\u89e3\uff0c\u4e3a\u533b\u7597AI\u63d0\u4f9b\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.22027", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.22027", "abs": "https://arxiv.org/abs/2507.22027", "authors": ["Mingjun Ying", "Dipankar Shakya", "Peijie Ma", "Guanyue Qian", "Theodore S. Rappaport"], "title": "Site-Specific Location Calibration and Validation of Ray-Tracing Simulator NYURay at Upper Mid-Band Frequencies", "comment": "16 pages, 7 figures", "summary": "Ray-tracing (RT) simulators are essential for wireless digital twins,\nenabling accurate site-specific radio channel prediction for next-generation\nwireless systems. Yet, RT simulation accuracy is often limited by insufficient\nmeasurement data and a lack of systematic validation. This paper presents\nsite-specific location calibration and validation of NYURay, NYU's in-house ray\ntracer, at upper mid-band frequencies (6.75 GHz and 16.95 GHz). We propose a\nlocation calibration algorithm that corrects GPS-induced position errors by\noptimizing transmitter-receiver (TX-RX) locations to align simulated and\nmeasured power delay profiles, improving TX-RX location accuracy by 42.3% for\nline-of-sight (LOS) and 13.5% for non-line-of-sight (NLOS) scenarios.\nValidation across 18 TX-RX locations shows excellent RT accuracy in path loss\nprediction, with path loss exponent (PLE) deviations under 0.14. While RT\nunderestimates delay spread and angular spreads, their cumulative distributions\nremain statistically similar. The validated NYURay advances RT validation and\nprovides reliable channel statistics for 6G deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4d\u7f6e\u6821\u51c6\u7b97\u6cd5\uff0c\u4f18\u5316\u4e86NYURay\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u5668\u7684\u7cbe\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LOS\u548cNLOS\u573a\u666f\u4e0b\u7684\u4f4d\u7f6e\u51c6\u786e\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u8def\u5f84\u635f\u8017\u9884\u6d4b\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u5668\u5728\u65e0\u7ebf\u6570\u5b57\u5b6a\u751f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7cbe\u5ea6\u5e38\u53d7\u9650\u4e8e\u6d4b\u91cf\u6570\u636e\u4e0d\u8db3\u548c\u7f3a\u4e4f\u7cfb\u7edf\u9a8c\u8bc1\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u4f4d\u7f6e\u6821\u51c6\u548c\u9a8c\u8bc1\u63d0\u5347NYURay\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4d\u7f6e\u6821\u51c6\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316TX-RX\u4f4d\u7f6e\u4ee5\u5bf9\u9f50\u6a21\u62df\u548c\u5b9e\u6d4b\u529f\u7387\u5ef6\u8fdf\u5256\u9762\uff0c\u5e76\u57286.75 GHz\u548c16.95 GHz\u9891\u6bb5\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u6821\u51c6\u540e\uff0cTX-RX\u4f4d\u7f6e\u7cbe\u5ea6\u5728LOS\u548cNLOS\u573a\u666f\u4e0b\u5206\u522b\u63d0\u9ad8\u4e8642.3%\u548c13.5%\uff0c\u8def\u5f84\u635f\u8017\u9884\u6d4b\u504f\u5dee\u5c0f\u4e8e0.14\u3002", "conclusion": "\u9a8c\u8bc1\u540e\u7684NYURay\u63d0\u5347\u4e86\u5c04\u7ebf\u8ffd\u8e2a\u9a8c\u8bc1\u7684\u53ef\u9760\u6027\uff0c\u4e3a6G\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u4fe1\u7684\u4fe1\u9053\u7edf\u8ba1\u6570\u636e\u3002"}}
{"id": "2507.21197", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.21197", "abs": "https://arxiv.org/abs/2507.21197", "authors": ["Ling Liao", "Eva Aagaard"], "title": "AdaptHetero: Machine Learning Interpretation-Driven Subgroup Adaptation for EHR-Based Clinical Prediction", "comment": "11 pages, 3 figures", "summary": "Machine learning interpretation has primarily been leveraged to build\nclinician trust and uncover actionable insights in EHRs. However, the intrinsic\ncomplexity and heterogeneity of EHR data limit its effectiveness in guiding\nsubgroup-specific modeling. We propose AdaptHetero, a novel MLI-driven\nframework that transforms interpretability insights into actionable guidance\nfor tailoring model training and evaluation across subpopulations within\nindividual hospital systems. Evaluated on three large-scale EHR datasets -\nGOSSIS-1-eICU, WiDS, and MIMIC-IV - AdaptHetero consistently identifies\nheterogeneous model behaviors in predicting ICU mortality, in-hospital death,\nand hidden hypoxemia. By integrating SHAP-based interpretation and unsupervised\nclustering, the framework enhances the identification of clinically meaningful\nsubgroup-specific characteristics, leading to improved predictive performance.", "AI": {"tldr": "AdaptHetero\u662f\u4e00\u4e2a\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7SHAP\u89e3\u91ca\u548c\u65e0\u76d1\u7763\u805a\u7c7b\uff0c\u4f18\u5316EHR\u6570\u636e\u4e2d\u5b50\u7fa4\u4f53\u7684\u5efa\u6a21\u548c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "EHR\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u5f02\u8d28\u6027\u9650\u5236\u4e86\u673a\u5668\u5b66\u4e60\u89e3\u91ca\u5728\u5b50\u7fa4\u4f53\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\uff0cAdaptHetero\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408SHAP\u89e3\u91ca\u548c\u65e0\u76d1\u7763\u805a\u7c7b\uff0c\u5c06\u89e3\u91ca\u6027\u6d1e\u5bdf\u8f6c\u5316\u4e3a\u5b50\u7fa4\u4f53\u5efa\u6a21\u7684\u5177\u4f53\u6307\u5bfc\u3002", "result": "\u5728\u4e09\u4e2a\u5927\u89c4\u6a21EHR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cAdaptHetero\u80fd\u8bc6\u522b\u5f02\u8d28\u6027\u6a21\u578b\u884c\u4e3a\u5e76\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "AdaptHetero\u4e3aEHR\u6570\u636e\u4e2d\u7684\u5b50\u7fa4\u4f53\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u610f\u4e49\u3002"}}
{"id": "2507.21183", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.21183", "abs": "https://arxiv.org/abs/2507.21183", "authors": ["Guangchen Lan", "Sipeng Zhang", "Tianle Wang", "Yuwei Zhang", "Daoan Zhang", "Xinpeng Wei", "Xiaoman Pan", "Hongming Zhang", "Dong-Jun Han", "Christopher G. Brinton"], "title": "MaPPO: Maximum a Posteriori Preference Optimization with Prior Knowledge", "comment": null, "summary": "As the era of large language models (LLMs) on behalf of users unfolds,\nPreference Optimization (PO) methods have become a central approach to aligning\nLLMs with human preferences and improving performance. We propose Maximum a\nPosteriori Preference Optimization (MaPPO), a framework for learning from\npreferences that explicitly incorporates prior reward knowledge into the\noptimization objective. While existing methods such as Direct Preference\nOptimization (DPO) and its variants treat preference learning as a Maximum\nLikelihood Estimation (MLE) problem, MaPPO extends this paradigm by integrating\nprior reward estimates into a principled Maximum a Posteriori (MaP) objective.\nThis not only generalizes DPO and its variants, but also enhances alignment by\nmitigating the oversimplified binary classification of responses. More\nimportantly, MaPPO introduces no additional hyperparameter, and supports\npreference optimization in both offline and online settings. In addition, MaPPO\ncan be used as a plugin with consistent improvement on DPO variants, including\nwidely used SimPO, IPO, and CPO. Extensive empirical evaluations of different\nmodel sizes and model series on three standard benchmarks, including MT-Bench,\nAlpacaEval 2.0, and Arena-Hard, demonstrate consistent improvements in\nalignment performance without sacrificing computational efficiency.", "AI": {"tldr": "MaPPO\u662f\u4e00\u79cd\u65b0\u7684\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u5148\u9a8c\u5956\u52b1\u77e5\u8bc6\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\uff08\u5982DPO\uff09\uff0c\u65e0\u9700\u989d\u5916\u8d85\u53c2\u6570\uff0c\u9002\u7528\u4e8e\u79bb\u7ebf\u548c\u5728\u7ebf\u8bbe\u7f6e\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff08\u5982DPO\uff09\u5c06\u504f\u597d\u5b66\u4e60\u89c6\u4e3a\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u95ee\u9898\uff0c\u5ffd\u7565\u4e86\u5148\u9a8c\u5956\u52b1\u77e5\u8bc6\uff0c\u53ef\u80fd\u5bfc\u81f4\u5bf9\u54cd\u5e94\u7684\u4e8c\u5143\u5206\u7c7b\u8fc7\u4e8e\u7b80\u5316\u3002", "method": "MaPPO\u5c06\u5148\u9a8c\u5956\u52b1\u77e5\u8bc6\u6574\u5408\u5230\u6700\u5927\u540e\u9a8c\u4f18\u5316\u76ee\u6807\u4e2d\uff0c\u6269\u5c55\u4e86DPO\u53ca\u5176\u53d8\u4f53\uff0c\u652f\u6301\u79bb\u7ebf\u548c\u5728\u7ebf\u8bbe\u7f6e\uff0c\u5e76\u53ef\u4f5c\u4e3a\u63d2\u4ef6\u4f7f\u7528\u3002", "result": "\u5728MT-Bench\u3001AlpacaEval 2.0\u548cArena-Hard\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMaPPO\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u9f50\u6027\u80fd\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u672a\u53d7\u5f71\u54cd\u3002", "conclusion": "MaPPO\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u573a\u666f\u3002"}}
{"id": "2507.21184", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.21184", "abs": "https://arxiv.org/abs/2507.21184", "authors": ["Haowei Lin", "Xiangyu Wang", "Jianzhu Ma", "Yitao Liang"], "title": "EvoSLD: Automated Neural Scaling Law Discovery With Large Language Models", "comment": null, "summary": "Scaling laws are fundamental mathematical relationships that predict how\nneural network performance evolves with changes in variables such as model\nsize, dataset size, and computational resources. Traditionally, discovering\nthese laws requires extensive human expertise and manual experimentation. We\nintroduce EvoSLD, an automated framework for Scaling Law Discovery (SLD) that\nleverages evolutionary algorithms guided by Large Language Models (LLMs) to\nco-evolve symbolic expressions and their optimization routines. Formulated to\nhandle scaling variables, control variables, and response metrics across\ndiverse experimental settings, EvoSLD searches for parsimonious, universal\nfunctional forms that minimize fitting errors on grouped data subsets.\nEvaluated on five real-world scenarios from recent literature, EvoSLD\nrediscovers exact human-derived laws in two cases and surpasses them in others,\nachieving up to orders-of-magnitude reductions in normalized mean squared error\non held-out test sets. Compared to baselines like symbolic regression and\nablated variants, EvoSLD demonstrates superior accuracy, interpretability, and\nefficiency, highlighting its potential to accelerate AI research. Code is\navailable at https://github.com/linhaowei1/SLD.", "AI": {"tldr": "EvoSLD\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5229\u7528\u8fdb\u5316\u7b97\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5171\u540c\u6f14\u5316\u7b26\u53f7\u8868\u8fbe\u5f0f\u53ca\u5176\u4f18\u5316\u7a0b\u5e8f\uff0c\u7528\u4e8e\u53d1\u73b0\u795e\u7ecf\u7f51\u7edc\u7684\u7f29\u653e\u89c4\u5f8b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u7ecf\u9a8c\u548c\u624b\u52a8\u5b9e\u9a8c\u6765\u53d1\u73b0\u7f29\u653e\u89c4\u5f8b\uff0cEvoSLD\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "EvoSLD\u7ed3\u5408\u8fdb\u5316\u7b97\u6cd5\u548cLLMs\uff0c\u5904\u7406\u7f29\u653e\u53d8\u91cf\u3001\u63a7\u5236\u53d8\u91cf\u548c\u54cd\u5e94\u6307\u6807\uff0c\u5bfb\u627e\u7b80\u6d01\u4e14\u901a\u7528\u7684\u51fd\u6570\u5f62\u5f0f\u3002", "result": "\u5728\u4e94\u4e2a\u5b9e\u9645\u573a\u666f\u4e2d\uff0cEvoSLD\u91cd\u65b0\u53d1\u73b0\u4e86\u4e24\u4e2a\u5df2\u77e5\u89c4\u5f8b\uff0c\u5e76\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u8d85\u8d8a\u4eba\u7c7b\u7ed3\u679c\uff0c\u6d4b\u8bd5\u96c6\u8bef\u5dee\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "EvoSLD\u5728\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u671b\u52a0\u901fAI\u7814\u7a76\u3002"}}
{"id": "2507.21188", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21188", "abs": "https://arxiv.org/abs/2507.21188", "authors": ["Raj Krishnan Vijayaraj"], "title": "Embeddings to Diagnosis: Latent Fragility under Agentic Perturbations in Clinical LLMs", "comment": null, "summary": "LLMs for clinical decision support often fail under small but clinically\nmeaningful input shifts such as masking a symptom or negating a finding,\ndespite high performance on static benchmarks. These reasoning failures\nfrequently go undetected by standard NLP metrics, which are insensitive to\nlatent representation shifts that drive diagnosis instability. We propose a\ngeometry-aware evaluation framework, LAPD (Latent Agentic Perturbation\nDiagnostics), which systematically probes the latent robustness of clinical\nLLMs under structured adversarial edits. Within this framework, we introduce\nLatent Diagnosis Flip Rate (LDFR), a model-agnostic diagnostic signal that\ncaptures representational instability when embeddings cross decision boundaries\nin PCA-reduced latent space. Clinical notes are generated using a structured\nprompting pipeline grounded in diagnostic reasoning, then perturbed along four\naxes: masking, negation, synonym replacement, and numeric variation to simulate\ncommon ambiguities and omissions. We compute LDFR across both foundation and\nclinical LLMs, finding that latent fragility emerges even under minimal\nsurface-level changes. Finally, we validate our findings on 90 real clinical\nnotes from the DiReCT benchmark (MIMIC-IV), confirming the generalizability of\nLDFR beyond synthetic settings. Our results reveal a persistent gap between\nsurface robustness and semantic stability, underscoring the importance of\ngeometry-aware auditing in safety-critical clinical AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u8bc4\u4f30\u6846\u67b6LAPD\uff0c\u7528\u4e8e\u68c0\u6d4b\u4e34\u5e8aLLMs\u5728\u5bf9\u6297\u6027\u8f93\u5165\u53d8\u5316\u4e0b\u7684\u6f5c\u5728\u9c81\u68d2\u6027\uff0c\u5e76\u5f15\u5165LDFR\u6307\u6807\u91cf\u5316\u8bca\u65ad\u4e0d\u7a33\u5b9a\u6027\u3002", "motivation": "\u4e34\u5e8aLLMs\u5728\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8f93\u5165\u5fae\u5c0f\u53d8\u5316\uff08\u5982\u75c7\u72b6\u63a9\u76d6\u6216\u5426\u5b9a\uff09\u65f6\u5bb9\u6613\u5931\u8d25\uff0c\u800c\u6807\u51c6NLP\u6307\u6807\u65e0\u6cd5\u68c0\u6d4b\u8fd9\u79cd\u6f5c\u5728\u8868\u793a\u53d8\u5316\u3002", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u6297\u7f16\u8f91\uff08\u5982\u63a9\u76d6\u3001\u5426\u5b9a\u3001\u540c\u4e49\u8bcd\u66ff\u6362\u548c\u6570\u503c\u53d8\u5316\uff09\u751f\u6210\u4e34\u5e8a\u7b14\u8bb0\uff0c\u5229\u7528PCA\u964d\u7ef4\u7684\u6f5c\u5728\u7a7a\u95f4\u8ba1\u7b97LDFR\uff0c\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u5373\u4f7f\u8868\u9762\u53d8\u5316\u6781\u5c0f\uff0c\u4e34\u5e8aLLMs\u4ecd\u8868\u73b0\u51fa\u6f5c\u5728\u8106\u5f31\u6027\uff0c\u4e14\u5728\u771f\u5b9e\u4e34\u5e8a\u6570\u636e\uff08DiReCT\u57fa\u51c6\uff09\u4e2d\u9a8c\u8bc1\u4e86LDFR\u7684\u666e\u9002\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u8868\u9762\u9c81\u68d2\u6027\u4e0e\u8bed\u4e49\u7a33\u5b9a\u6027\u5b58\u5728\u5dee\u8ddd\uff0c\u5f3a\u8c03\u5728\u5b89\u5168\u5173\u952e\u4e34\u5e8aAI\u4e2d\u51e0\u4f55\u611f\u77e5\u5ba1\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.21189", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21189", "abs": "https://arxiv.org/abs/2507.21189", "authors": ["Andrew Kiruluta", "Andreas Lemos", "Priscilla Burity"], "title": "Operator-Based Machine Intelligence: A Hilbert Space Framework for Spectral Learning and Symbolic Reasoning", "comment": null, "summary": "Traditional machine learning models, particularly neural networks, are rooted\nin finite-dimensional parameter spaces and nonlinear function approximations.\nThis report explores an alternative formulation where learning tasks are\nexpressed as sampling and computation in infinite dimensional Hilbert spaces,\nleveraging tools from functional analysis, signal processing, and spectral\ntheory. We review foundational concepts such as Reproducing Kernel Hilbert\nSpaces (RKHS), spectral operator learning, and wavelet-domain representations.\nWe present a rigorous mathematical formulation of learning in Hilbert spaces,\nhighlight recent models based on scattering transforms and Koopman operators,\nand discuss advantages and limitations relative to conventional neural\narchitectures. The report concludes by outlining directions for scalable and\ninterpretable machine learning grounded in Hilbertian signal processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u8868\u8fbe\u5b66\u4e60\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6cdb\u51fd\u5206\u6790\u3001\u4fe1\u53f7\u5904\u7406\u548c\u8c31\u7406\u8bba\u5de5\u5177\uff0c\u5bf9\u6bd4\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u57fa\u4e8e\u6709\u9650\u7ef4\u53c2\u6570\u7a7a\u95f4\u548c\u975e\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u65e0\u9650\u7ef4\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u5b66\u4e60\u4efb\u52a1\u8868\u8fbe\uff0c\u4ee5\u63d0\u4f9b\u66f4\u7075\u6d3b\u548c\u7406\u8bba\u57fa\u7840\u66f4\u5f3a\u7684\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u56de\u987e\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\uff08RKHS\uff09\u3001\u8c31\u7b97\u5b50\u5b66\u4e60\u548c\u5c0f\u6ce2\u57df\u8868\u793a\u7b49\u57fa\u7840\u6982\u5ff5\uff0c\u63d0\u51fa\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u5b66\u4e60\u6570\u5b66\u6846\u67b6\uff0c\u5e76\u4ecb\u7ecd\u57fa\u4e8e\u6563\u5c04\u53d8\u6362\u548cKoopman\u7b97\u5b50\u7684\u65b0\u6a21\u578b\u3002", "result": "\u8bba\u6587\u5c55\u793a\u4e86\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5b66\u4e60\u6846\u67b6\u7684\u6f5c\u529b\uff0c\u5e76\u8ba8\u8bba\u4e86\u5176\u76f8\u5bf9\u4e8e\u4f20\u7edf\u795e\u7ecf\u67b6\u6784\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u57fa\u4e8e\u5e0c\u5c14\u4f2f\u7279\u4fe1\u53f7\u5904\u7406\u7684\u53ef\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u3002"}}
{"id": "2507.21190", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21190", "abs": "https://arxiv.org/abs/2507.21190", "authors": ["Andrew Kiruluta", "Andreas Lemos", "Priscilla Burity"], "title": "Beyond Neural Networks: Symbolic Reasoning over Wavelet Logic Graph Signals", "comment": null, "summary": "We present a fully non neural learning framework based on Graph Laplacian\nWavelet Transforms (GLWT). Unlike traditional architectures that rely on\nconvolutional, recurrent, or attention based neural networks, our model\noperates purely in the graph spectral domain using structured multiscale\nfiltering, nonlinear shrinkage, and symbolic logic over wavelet coefficients.\nSignals defined on graph nodes are decomposed via GLWT, modulated with\ninterpretable nonlinearities, and recombined for downstream tasks such as\ndenoising and token classification. The system supports compositional reasoning\nthrough a symbolic domain-specific language (DSL) over graph wavelet\nactivations. Experiments on synthetic graph denoising and linguistic token\ngraphs demonstrate competitive performance against lightweight GNNs with far\ngreater transparency and efficiency. This work proposes a principled,\ninterpretable, and resource-efficient alternative to deep neural architectures\nfor learning on graphs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u62c9\u666e\u62c9\u65af\u5c0f\u6ce2\u53d8\u6362\uff08GLWT\uff09\u7684\u975e\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u591a\u5c3a\u5ea6\u6ee4\u6ce2\u3001\u975e\u7ebf\u6027\u6536\u7f29\u548c\u5c0f\u6ce2\u7cfb\u6570\u7684\u7b26\u53f7\u903b\u8f91\uff0c\u5728\u56fe\u5f62\u8c31\u57df\u4e2d\u64cd\u4f5c\uff0c\u9002\u7528\u4e8e\u53bb\u566a\u548c\u6807\u8bb0\u5206\u7c7b\u7b49\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4f9d\u8d56\u5377\u79ef\u3001\u5faa\u73af\u6216\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u6548\u7387\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u53ef\u89e3\u91ca\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u56fe\u5f62\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528GLWT\u5206\u89e3\u56fe\u5f62\u8282\u70b9\u4fe1\u53f7\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u8c03\u5236\u548c\u7b26\u53f7\u903b\u8f91\u91cd\u7ec4\u4fe1\u53f7\uff0c\u652f\u6301\u57fa\u4e8e\u56fe\u5c0f\u6ce2\u6fc0\u6d3b\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff08DSL\uff09\u8fdb\u884c\u7ec4\u5408\u63a8\u7406\u3002", "result": "\u5728\u5408\u6210\u56fe\u5f62\u53bb\u566a\u548c\u8bed\u8a00\u6807\u8bb0\u56fe\u5f62\u4efb\u52a1\u4e2d\uff0c\u6027\u80fd\u4e0e\u8f7b\u91cf\u7ea7GNN\u76f8\u5f53\uff0c\u4f46\u900f\u660e\u6027\u548c\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u56fe\u5f62\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u3001\u53ef\u89e3\u91ca\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002"}}
{"id": "2507.21191", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21191", "abs": "https://arxiv.org/abs/2507.21191", "authors": ["Garv Kaushik"], "title": "Exploring Adaptive Structure Learning for Heterophilic Graphs", "comment": "Initially submitted this draft at Tiny ICLR 2025", "summary": "Graph Convolutional Networks (GCNs) gained traction for graph representation\nlearning, with recent attention on improving performance on heterophilic graphs\nfor various real-world applications. The localized feature aggregation in a\ntypical message-passing paradigm hinders the capturing of long-range\ndependencies between non-local nodes of the same class. The inherent\nconnectivity structure in heterophilic graphs often conflicts with information\nsharing between distant nodes of same class. We propose structure learning to\nrewire edges in shallow GCNs itself to avoid performance degradation in\ndownstream discriminative tasks due to oversmoothing. Parameterizing the\nadjacency matrix to learn connections between non-local nodes and extend the\nhop span of shallow GCNs facilitates the capturing of long-range dependencies.\nHowever, our method is not generalizable across heterophilic graphs and\nperforms inconsistently on node classification task contingent to the graph\nstructure.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u7ed3\u6784\u5b66\u4e60\u91cd\u8fde\u6d45\u5c42GCN\u7684\u8fb9\uff0c\u4ee5\u89e3\u51b3\u5f02\u8d28\u56fe\u4e2d\u957f\u8ddd\u79bb\u4f9d\u8d56\u95ee\u9898\uff0c\u4f46\u65b9\u6cd5\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\u3002", "motivation": "\u89e3\u51b3\u5f02\u8d28\u56fe\u4e2d\u56e0\u5c40\u90e8\u7279\u5f81\u805a\u5408\u548c\u6d88\u606f\u4f20\u9012\u8303\u5f0f\u5bfc\u81f4\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u6355\u83b7\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u53c2\u6570\u5316\u90bb\u63a5\u77e9\u9635\u5b66\u4e60\u975e\u5c40\u90e8\u8282\u70b9\u95f4\u7684\u8fde\u63a5\uff0c\u6269\u5c55\u6d45\u5c42GCN\u7684\u8df3\u6570\u8303\u56f4\u3002", "result": "\u65b9\u6cd5\u5728\u90e8\u5206\u5f02\u8d28\u56fe\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u56e0\u56fe\u7ed3\u6784\u4e0d\u540c\u800c\u8868\u73b0\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u7ed3\u6784\u5b66\u4e60\u80fd\u6539\u5584\u957f\u8ddd\u79bb\u4f9d\u8d56\u95ee\u9898\uff0c\u4f46\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u6709\u9650\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2507.21196", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21196", "abs": "https://arxiv.org/abs/2507.21196", "authors": ["Abir Ray"], "title": "EdgeAgentX-DT: Integrating Digital Twins and Generative AI for Resilient Edge Intelligence in Tactical Networks", "comment": "13 pages, 6 figures", "summary": "We introduce EdgeAgentX-DT, an advanced extension of the EdgeAgentX framework\nthat integrates digital twin simulations and generative AI-driven scenario\ntraining to significantly enhance edge intelligence in military networks.\nEdgeAgentX-DT utilizes network digital twins, virtual replicas synchronized\nwith real-world edge devices, to provide a secure, realistic environment for\ntraining and validation. Leveraging generative AI methods, such as diffusion\nmodels and transformers, the system creates diverse and adversarial scenarios\nfor robust simulation-based agent training. Our multi-layer architecture\nincludes: (1) on-device edge intelligence; (2) digital twin synchronization;\nand (3) generative scenario training. Experimental simulations demonstrate\nnotable improvements over EdgeAgentX, including faster learning convergence,\nhigher network throughput, reduced latency, and improved resilience against\njamming and node failures. A case study involving a complex tactical scenario\nwith simultaneous jamming attacks, agent failures, and increased network loads\nillustrates how EdgeAgentX-DT sustains operational performance, whereas\nbaseline methods fail. These results highlight the potential of\ndigital-twin-enabled generative training to strengthen edge AI deployments in\ncontested environments.", "AI": {"tldr": "EdgeAgentX-DT\u901a\u8fc7\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u548c\u751f\u6210\u5f0fAI\uff0c\u63d0\u5347\u519b\u4e8b\u7f51\u7edc\u8fb9\u7f18\u667a\u80fd\uff0c\u663e\u8457\u4f18\u5316\u8bad\u7ec3\u548c\u6027\u80fd\u3002", "motivation": "\u589e\u5f3a\u519b\u4e8b\u7f51\u7edc\u4e2d\u7684\u8fb9\u7f18\u667a\u80fd\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u548c\u5bf9\u6297\u6027\u73af\u5883\u3002", "method": "\u5229\u7528\u6570\u5b57\u5b6a\u751f\u540c\u6b65\u771f\u5b9e\u8bbe\u5907\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548cTransformer\u751f\u6210\u591a\u6837\u5316\u5bf9\u6297\u573a\u666f\uff0c\u91c7\u7528\u591a\u5c42\u67b6\u6784\uff08\u8fb9\u7f18\u667a\u80fd\u3001\u6570\u5b57\u5b6a\u751f\u540c\u6b65\u3001\u751f\u6210\u5f0f\u8bad\u7ec3\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u66f4\u5feb\u7684\u5b66\u4e60\u6536\u655b\u3001\u66f4\u9ad8\u541e\u5410\u91cf\u3001\u66f4\u4f4e\u5ef6\u8fdf\u53ca\u66f4\u5f3a\u7684\u6297\u5e72\u6270\u80fd\u529b\u3002", "conclusion": "\u6570\u5b57\u5b6a\u751f\u548c\u751f\u6210\u5f0f\u8bad\u7ec3\u80fd\u6709\u6548\u63d0\u5347\u8fb9\u7f18AI\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u80fd\u529b\u3002"}}
{"id": "2507.21198", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21198", "abs": "https://arxiv.org/abs/2507.21198", "authors": ["Xinguo Feng", "Zhongkui Ma", "Zihan Wang", "Eu Joe Chegne", "Mengyao Ma", "Alsharif Abuadbba", "Guangdong Bai"], "title": "Uncovering Gradient Inversion Risks in Practical Language Model Training", "comment": "15 Pages, 5 figures, 10 tables. Accepted by ACM CCS 2024", "summary": "The gradient inversion attack has been demonstrated as a significant privacy\nthreat to federated learning (FL), particularly in continuous domains such as\nvision models. In contrast, it is often considered less effective or highly\ndependent on impractical training settings when applied to language models, due\nto the challenges posed by the discrete nature of tokens in text data. As a\nresult, its potential privacy threats remain largely underestimated, despite FL\nbeing an emerging training method for language models. In this work, we propose\na domain-specific gradient inversion attack named Grab (gradient inversion with\nhybrid optimization). Grab features two alternating optimization processes to\naddress the challenges caused by practical training settings, including a\nsimultaneous optimization on dropout masks between layers for improved token\nrecovery and a discrete optimization for effective token sequencing. Grab can\nrecover a significant portion (up to 92.9% recovery rate) of the private\ntraining data, outperforming the attack strategy of utilizing discrete\noptimization with an auxiliary model by notable improvements of up to 28.9%\nrecovery rate in benchmark settings and 48.5% recovery rate in practical\nsettings. Grab provides a valuable step forward in understanding this privacy\nthreat in the emerging FL training mode of language models.", "AI": {"tldr": "Grab\u662f\u4e00\u79cd\u9488\u5bf9\u8bed\u8a00\u6a21\u578b\u7684\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u4f18\u5316\u663e\u8457\u63d0\u9ad8\u4e86\u9690\u79c1\u6570\u636e\u6062\u590d\u7387\u3002", "motivation": "\u63ed\u793a\u8054\u90a6\u5b66\u4e60\u4e2d\u8bed\u8a00\u6a21\u578b\u7684\u9690\u79c1\u5a01\u80c1\uff0c\u586b\u8865\u68af\u5ea6\u53cd\u8f6c\u653b\u51fb\u5728\u79bb\u6563\u6570\u636e\u9886\u57df\u7684\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u4ea4\u66ff\u4f18\u5316\u8fc7\u7a0b\uff0c\u5305\u62ec\u5c42\u95f4\u4e22\u5f03\u63a9\u7801\u7684\u540c\u6b65\u4f18\u5316\u548c\u79bb\u6563\u4f18\u5316\u7684\u4ee4\u724c\u5e8f\u5217\u6062\u590d\u3002", "result": "Grab\u5728\u57fa\u51c6\u548c\u5b9e\u9645\u8bbe\u7f6e\u4e2d\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe92.9%\u548c48.5%\u7684\u6062\u590d\u7387\u63d0\u5347\u3002", "conclusion": "Grab\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u8054\u90a6\u5b66\u4e60\u7684\u9690\u79c1\u5a01\u80c1\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.21199", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.21199", "abs": "https://arxiv.org/abs/2507.21199", "authors": ["Xinye Cao", "Hongcan Guo", "Guoshun Nan", "Jiaoyang Cui", "Haoting Qian", "Yihan Lin", "Yilin Peng", "Diyang Zhang", "Yanzhao Hou", "Huici Wu", "Xiaofeng Tao", "Tony Q. S. Quek"], "title": "Advancing Compositional LLM Reasoning with Structured Task Relations in Interactive Multimodal Communications", "comment": "Accepted by IEEE JSAC. This work has been submitted to the IEEE for\n  possible publication", "summary": "Interactive multimodal applications (IMAs), such as route planning in the\nInternet of Vehicles, enrich users' personalized experiences by integrating\nvarious forms of data over wireless networks. Recent advances in large language\nmodels (LLMs) utilize mixture-of-experts (MoE) mechanisms to empower multiple\nIMAs, with each LLM trained individually for a specific task that presents\ndifferent business workflows. In contrast to existing approaches that rely on\nmultiple LLMs for IMAs, this paper presents a novel paradigm that accomplishes\nvarious IMAs using a single compositional LLM over wireless networks. The two\nprimary challenges include 1) guiding a single LLM to adapt to diverse IMA\nobjectives and 2) ensuring the flexibility and efficiency of the LLM in\nresource-constrained mobile environments. To tackle the first challenge, we\npropose ContextLoRA, a novel method that guides an LLM to learn the rich\nstructured context among IMAs by constructing a task dependency graph. We\npartition the learnable parameter matrix of neural layers for each IMA to\nfacilitate LLM composition. Then, we develop a step-by-step fine-tuning\nprocedure guided by task relations, including training, freezing, and masking\nphases. This allows the LLM to learn to reason among tasks for better\nadaptation, capturing the latent dependencies between tasks. For the second\nchallenge, we introduce ContextGear, a scheduling strategy to optimize the\ntraining procedure of ContextLoRA, aiming to minimize computational and\ncommunication costs through a strategic grouping mechanism. Experiments on\nthree benchmarks show the superiority of the proposed ContextLoRA and\nContextGear. Furthermore, we prototype our proposed paradigm on a real-world\nwireless testbed, demonstrating its practical applicability for various IMAs.\nWe will release our code to the community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5355\u4e00\u7ec4\u5408\u5f0fLLM\u7684\u4ea4\u4e92\u5f0f\u591a\u6a21\u6001\u5e94\u7528\uff08IMAs\uff09\u65b0\u8303\u5f0f\uff0c\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u9002\u5e94\u6027\u548c\u8d44\u6e90\u6548\u7387\u95ee\u9898\u3002\u901a\u8fc7ContextLoRA\u548cContextGear\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u5b66\u4e60\u548c\u8bad\u7ec3\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u591a\u4e2aLLM\u5904\u7406\u4e0d\u540cIMAs\u4efb\u52a1\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u6548\u7387\u4f4e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5355\u4e00LLM\u5b9e\u73b0\u591a\u6837\u5316IMAs\u4efb\u52a1\uff0c\u63d0\u5347\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faContextLoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u4f9d\u8d56\u56fe\u5b66\u4e60\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u5e76\u5206\u533a\u53c2\u6570\u77e9\u9635\uff1b\u5f00\u53d1\u5206\u6b65\u5fae\u8c03\u6d41\u7a0b\u3002ContextGear\u7b56\u7565\u4f18\u5316\u8bad\u7ec3\uff0c\u964d\u4f4e\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u5e76\u5728\u771f\u5b9e\u65e0\u7ebf\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "\u5355\u4e00\u7ec4\u5408\u5f0fLLM\u8303\u5f0f\u5728IMAs\u4e2d\u5177\u6709\u9ad8\u6548\u6027\u548c\u5b9e\u7528\u6027\uff0cContextLoRA\u548cContextGear\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u9002\u5e94\u6027\u548c\u8d44\u6e90\u6548\u7387\u3002"}}
{"id": "2507.21205", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.21205", "abs": "https://arxiv.org/abs/2507.21205", "authors": ["Harsh Rangwani"], "title": "Learning from Limited and Imperfect Data", "comment": "PhD Thesis", "summary": "The distribution of data in the world (eg, internet, etc.) significantly\ndiffers from the well-curated datasets and is often over-populated with samples\nfrom common categories. The algorithms designed for well-curated datasets\nperform suboptimally when used for learning from imperfect datasets with\nlong-tailed imbalances and distribution shifts. To expand the use of deep\nmodels, it is essential to overcome the labor-intensive curation process by\ndeveloping robust algorithms that can learn from diverse, real-world data\ndistributions. Toward this goal, we develop practical algorithms for Deep\nNeural Networks which can learn from limited and imperfect data present in the\nreal world. This thesis is divided into four segments, each covering a scenario\nof learning from limited or imperfect data. The first part of the thesis\nfocuses on Learning Generative Models from Long-Tail Data, where we mitigate\nthe mode-collapse and enable diverse aesthetic image generations for tail\n(minority) classes. In the second part, we enable effective generalization on\ntail classes through Inductive Regularization schemes, which allow tail classes\nto generalize as effectively as the head classes without requiring explicit\ngeneration of images. In the third part, we develop algorithms for Optimizing\nRelevant Metrics for learning from long-tailed data with limited annotation\n(semi-supervised), followed by the fourth part, which focuses on the Efficient\nDomain Adaptation of the model to various domains with very few to zero labeled\nsamples.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9488\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u6570\u636e\u5206\u5e03\u4e0d\u5747\u8861\u548c\u6807\u6ce8\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u56db\u79cd\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\uff0c\u5206\u522b\u89e3\u51b3\u751f\u6210\u6a21\u578b\u3001\u5f52\u7eb3\u6b63\u5219\u5316\u3001\u534a\u76d1\u7763\u5b66\u4e60\u548c\u9ad8\u6548\u57df\u9002\u5e94\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u5206\u5e03\u4e0d\u5747\u8861\u4e14\u6807\u6ce8\u4e0d\u8db3\uff0c\u73b0\u6709\u7b97\u6cd5\u5728\u975e\u7406\u60f3\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u7b97\u6cd5\u4ee5\u6269\u5c55\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u3002", "method": "\u5206\u56db\u90e8\u5206\uff1a1) \u957f\u5c3e\u6570\u636e\u751f\u6210\u6a21\u578b\uff1b2) \u5f52\u7eb3\u6b63\u5219\u5316\u63d0\u5347\u5c3e\u90e8\u7c7b\u6cdb\u5316\uff1b3) \u534a\u76d1\u7763\u5b66\u4e60\u4f18\u5316\u6307\u6807\uff1b4) \u9ad8\u6548\u57df\u9002\u5e94\u3002", "result": "\u5b9e\u73b0\u4e86\u5728\u957f\u5c3e\u6570\u636e\u4e0b\u751f\u6210\u591a\u6837\u56fe\u50cf\u3001\u63d0\u5347\u5c3e\u90e8\u7c7b\u6cdb\u5316\u80fd\u529b\u3001\u4f18\u5316\u534a\u76d1\u7763\u5b66\u4e60\u6307\u6807\uff0c\u4ee5\u53ca\u9ad8\u6548\u9002\u5e94\u65b0\u9886\u57df\u3002", "conclusion": "\u901a\u8fc7\u56db\u79cd\u7b97\u6cd5\u89e3\u51b3\u4e86\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u5b66\u4e60\u7684\u5173\u952e\u95ee\u9898\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u5728\u975e\u7406\u60f3\u6570\u636e\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2507.21244", "categories": ["cs.LG", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.21244", "abs": "https://arxiv.org/abs/2507.21244", "authors": ["Sheikh Md Shakeel Hassan", "Xianwei Zou", "Akash Dhruv", "Vishwanath Ganesan", "Aparna Chandramowlishwaran"], "title": "Bubbleformer: Forecasting Boiling with Transformers", "comment": "39 pages, 13 figures, Submitted to NeurIPS 2025", "summary": "Modeling boiling (an inherently chaotic, multiphase process central to energy\nand thermal systems) remains a significant challenge for neural PDE surrogates.\nExisting models require future input (e.g., bubble positions) during inference\nbecause they fail to learn nucleation from past states, limiting their ability\nto autonomously forecast boiling dynamics. They also fail to model flow boiling\nvelocity fields, where sharp interface-momentum coupling demands long-range and\ndirectional inductive biases. We introduce Bubbleformer, a transformer-based\nspatiotemporal model that forecasts stable and long-range boiling dynamics\nincluding nucleation, interface evolution, and heat transfer without dependence\non simulation data during inference. Bubbleformer integrates factorized axial\nattention, frequency-aware scaling, and conditions on thermophysical parameters\nto generalize across fluids, geometries, and operating conditions. To evaluate\nphysical fidelity in chaotic systems, we propose interpretable physics-based\nmetrics that evaluate heat-flux consistency, interface geometry, and mass\nconservation. We also release BubbleML 2.0, a high-fidelity dataset that spans\ndiverse working fluids (cryogens, refrigerants, dielectrics), boiling\nconfigurations (pool and flow boiling), flow regimes (bubbly, slug, annular),\nand boundary conditions. Bubbleformer sets new benchmark results in both\nprediction and forecasting of two-phase boiling flows.", "AI": {"tldr": "Bubbleformer\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u65f6\u7a7a\u6a21\u578b\uff0c\u80fd\u591f\u81ea\u4e3b\u9884\u6d4b\u6cb8\u817e\u52a8\u529b\u5b66\uff0c\u5305\u62ec\u6210\u6838\u3001\u754c\u9762\u6f14\u5316\u548c\u70ed\u4f20\u9012\uff0c\u65e0\u9700\u4f9d\u8d56\u6a21\u62df\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u63a8\u7406\u65f6\u9700\u8981\u672a\u6765\u8f93\u5165\uff0c\u65e0\u6cd5\u4ece\u8fc7\u53bb\u72b6\u6001\u5b66\u4e60\u6210\u6838\uff0c\u4e14\u65e0\u6cd5\u5efa\u6a21\u6d41\u52a8\u6cb8\u817e\u901f\u5ea6\u573a\uff0c\u9650\u5236\u4e86\u5176\u81ea\u4e3b\u9884\u6d4b\u80fd\u529b\u3002", "method": "Bubbleformer\u91c7\u7528\u56e0\u5b50\u5316\u8f74\u5411\u6ce8\u610f\u529b\u3001\u9891\u7387\u611f\u77e5\u7f29\u653e\u548c\u70ed\u7269\u7406\u53c2\u6570\u6761\u4ef6\u5316\uff0c\u4ee5\u6cdb\u5316\u4e0d\u540c\u6d41\u4f53\u3001\u51e0\u4f55\u548c\u64cd\u4f5c\u6761\u4ef6\u3002", "result": "Bubbleformer\u5728\u9884\u6d4b\u548c\u9884\u62a5\u4e24\u76f8\u6cb8\u817e\u6d41\u52a8\u65b9\u9762\u53d6\u5f97\u4e86\u65b0\u7684\u57fa\u51c6\u7ed3\u679c\u3002", "conclusion": "Bubbleformer\u901a\u8fc7\u7269\u7406\u6307\u6807\u8bc4\u4f30\u548c\u9ad8\u8d28\u91cf\u6570\u636e\u96c6BubbleML 2.0\u7684\u53d1\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cb8\u817e\u52a8\u529b\u5b66\u5efa\u6a21\u7684\u81ea\u4e3b\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.21260", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.21260", "abs": "https://arxiv.org/abs/2507.21260", "authors": ["Amartya Banerjee", "Xingyu Xu", "Caroline Moosm\u00fcller", "Harlin Lee"], "title": "Adaptive Multimodal Protein Plug-and-Play with Diffusion-Based Priors", "comment": "Code: https://github.com/amartya21/Adam-PnP", "summary": "In an inverse problem, the goal is to recover an unknown parameter (e.g., an\nimage) that has typically undergone some lossy or noisy transformation during\nmeasurement. Recently, deep generative models, particularly diffusion models,\nhave emerged as powerful priors for protein structure generation. However,\nintegrating noisy experimental data from multiple sources to guide these models\nremains a significant challenge. Existing methods often require precise\nknowledge of experimental noise levels and manually tuned weights for each data\nmodality. In this work, we introduce Adam-PnP, a Plug-and-Play framework that\nguides a pre-trained protein diffusion model using gradients from multiple,\nheterogeneous experimental sources. Our framework features an adaptive noise\nestimation scheme and a dynamic modality weighting mechanism integrated into\nthe diffusion process, which reduce the need for manual hyperparameter tuning.\nExperiments on complex reconstruction tasks demonstrate significantly improved\naccuracy using Adam-PnP.", "AI": {"tldr": "Adam-PnP\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684Plug-and-Play\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u591a\u6e90\u5b9e\u9a8c\u6570\u636e\u4e2d\u6062\u590d\u86cb\u767d\u8d28\u7ed3\u6784\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u566a\u58f0\u4f30\u8ba1\u548c\u52a8\u6001\u6a21\u6001\u52a0\u6743\u51cf\u5c11\u624b\u52a8\u8c03\u53c2\u9700\u6c42\u3002", "motivation": "\u89e3\u51b3\u591a\u6e90\u566a\u58f0\u5b9e\u9a8c\u6570\u636e\u4e0e\u6269\u6563\u6a21\u578b\u7ed3\u5408\u7684\u6311\u6218\uff0c\u51cf\u5c11\u5bf9\u566a\u58f0\u6c34\u5e73\u548c\u624b\u52a8\u8c03\u53c2\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51faAdam-PnP\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u566a\u58f0\u4f30\u8ba1\u548c\u52a8\u6001\u6a21\u6001\u52a0\u6743\u673a\u5236\uff0c\u6307\u5bfc\u9884\u8bad\u7ec3\u86cb\u767d\u8d28\u6269\u6563\u6a21\u578b\u3002", "result": "\u5728\u590d\u6742\u91cd\u5efa\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "Adam-PnP\u4e3a\u591a\u6e90\u6570\u636e\u9a71\u52a8\u7684\u86cb\u767d\u8d28\u7ed3\u6784\u6062\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21273", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21273", "abs": "https://arxiv.org/abs/2507.21273", "authors": ["Johannes Exenberger", "Sascha Ranftl", "Robert Peharz"], "title": "Deep Polynomial Chaos Expansion", "comment": "8th Workshop on Tractable Probabilistic Modeling, UAI 2025", "summary": "Polynomial chaos expansion (PCE) is a classical and widely used surrogate\nmodeling technique in physical simulation and uncertainty quantification. By\ntaking a linear combination of a set of basis polynomials - orthonormal with\nrespect to the distribution of uncertain input parameters - PCE enables\ntractable inference of key statistical quantities, such as (conditional) means,\nvariances, covariances, and Sobol sensitivity indices, which are essential for\nunderstanding the modeled system and identifying influential parameters and\ntheir interactions. As the number of basis functions grows exponentially with\nthe number of parameters, PCE does not scale well to high-dimensional problems.\nWe address this challenge by combining PCE with ideas from probabilistic\ncircuits, resulting in the deep polynomial chaos expansion (DeepPCE) - a deep\ngeneralization of PCE that scales effectively to high-dimensional input spaces.\nDeepPCE achieves predictive performance comparable to that of multi-layer\nperceptrons (MLPs), while retaining PCE's ability to compute exact statistical\ninferences via simple forward passes.", "AI": {"tldr": "DeepPCE\u7ed3\u5408\u591a\u9879\u5f0f\u6df7\u6c8c\u5c55\u5f00\u548c\u6982\u7387\u7535\u8def\u601d\u60f3\uff0c\u89e3\u51b3\u4e86\u9ad8\u7ef4\u95ee\u9898\u4e2dPCE\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u6027\u80fd\u63a5\u8fd1MLP\uff0c\u540c\u65f6\u4fdd\u7559\u4e86PCE\u7684\u7edf\u8ba1\u63a8\u65ad\u80fd\u529b\u3002", "motivation": "\u591a\u9879\u5f0f\u6df7\u6c8c\u5c55\u5f00\uff08PCE\uff09\u5728\u9ad8\u7ef4\u95ee\u9898\u4e2d\u56e0\u57fa\u51fd\u6570\u6570\u91cf\u6307\u6570\u589e\u957f\u800c\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301PCE\u7edf\u8ba1\u63a8\u65ad\u80fd\u529b\u53c8\u80fd\u9002\u5e94\u9ad8\u7ef4\u8f93\u5165\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDeepPCE\uff0c\u5c06PCE\u4e0e\u6982\u7387\u7535\u8def\u601d\u60f3\u7ed3\u5408\uff0c\u5f62\u6210\u4e00\u79cd\u6df1\u5ea6\u63a8\u5e7f\u7684PCE\u6a21\u578b\u3002", "result": "DeepPCE\u5728\u9ad8\u7ef4\u8f93\u5165\u7a7a\u95f4\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u9884\u6d4b\u6027\u80fd\u63a5\u8fd1\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\uff0c\u540c\u65f6\u80fd\u901a\u8fc7\u7b80\u5355\u524d\u5411\u4f20\u64ad\u8fdb\u884c\u7cbe\u786e\u7edf\u8ba1\u63a8\u65ad\u3002", "conclusion": "DeepPCE\u6210\u529f\u89e3\u51b3\u4e86PCE\u5728\u9ad8\u7ef4\u95ee\u9898\u4e2d\u7684\u6269\u5c55\u6027\u9650\u5236\uff0c\u517c\u5177\u9ad8\u6027\u80fd\u548c\u7edf\u8ba1\u63a8\u65ad\u80fd\u529b\u3002"}}
{"id": "2507.21274", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21274", "abs": "https://arxiv.org/abs/2507.21274", "authors": ["Jiin Woo", "Alireza Bagheri Garakani", "Tianchen Zhou", "Zhishen Huang", "Yan Gao"], "title": "Large Language Model-Enhanced Reinforcement Learning for Diverse and Novel Recommendations", "comment": null, "summary": "In recommendation systems, diversity and novelty are essential for capturing\nvaried user preferences and encouraging exploration, yet many systems\nprioritize click relevance. While reinforcement learning (RL) has been explored\nto improve diversity, it often depends on random exploration that may not align\nwith user interests. We propose LAAC (LLM-guided Adversarial Actor Critic), a\nnovel method that leverages large language models (LLMs) as reference policies\nto suggest novel items, while training a lightweight policy to refine these\nsuggestions using system-specific data. The method formulates training as a\nbilevel optimization between actor and critic networks, enabling the critic to\nselectively favor promising novel actions and the actor to improve its policy\nbeyond LLM recommendations. To mitigate overestimation of unreliable LLM\nsuggestions, we apply regularization that anchors critic values for unexplored\nitems close to well-estimated dataset actions. Experiments on real-world\ndatasets show that LAAC outperforms existing baselines in diversity, novelty,\nand accuracy, while remaining robust on imbalanced data, effectively\nintegrating LLM knowledge without expensive fine-tuning.", "AI": {"tldr": "LAAC\u5229\u7528LLM\u4f5c\u4e3a\u53c2\u8003\u7b56\u7565\u63d0\u51fa\u65b0\u9896\u63a8\u8350\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7b56\u7565\u4f18\u5316\uff0c\u63d0\u5347\u591a\u6837\u6027\u548c\u65b0\u9896\u6027\u3002", "motivation": "\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u6ce8\u91cd\u70b9\u51fb\u76f8\u5173\u6027\uff0c\u800c\u5ffd\u89c6\u591a\u6837\u6027\u548c\u65b0\u9896\u6027\uff0cLAAC\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408LLM\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u8bad\u7ec3actor-critic\u7f51\u7edc\uff0c\u6b63\u5219\u5316LLM\u5efa\u8bae\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cLAAC\u5728\u591a\u6837\u6027\u3001\u65b0\u9896\u6027\u548c\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "LAAC\u6709\u6548\u6574\u5408LLM\u77e5\u8bc6\uff0c\u65e0\u9700\u6602\u8d35\u5fae\u8c03\uff0c\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2507.21299", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21299", "abs": "https://arxiv.org/abs/2507.21299", "authors": ["Alex Guo", "Michael D. Graham"], "title": "Blending data and physics for reduced-order modeling of systems with spatiotemporal chaotic dynamics", "comment": null, "summary": "While data-driven techniques are powerful tools for reduced-order modeling of\nsystems with chaotic dynamics, great potential remains for leveraging known\nphysics (i.e. a full-order model (FOM)) to improve predictive capability. We\ndevelop a hybrid reduced order model (ROM), informed by both data and FOM, for\nevolving spatiotemporal chaotic dynamics on an invariant manifold whose\ncoordinates are found using an autoencoder. This approach projects the vector\nfield of the FOM onto the invariant manifold; then, this physics-derived vector\nfield is either corrected using dynamic data, or used as a Bayesian prior that\nis updated with data. In both cases, the neural ordinary differential equation\napproach is used. We consider simulated data from the Kuramoto-Sivashinsky and\ncomplex Ginzburg-Landau equations. Relative to the data-only approach, for\nscenarios of abundant data, scarce data, and even an incorrect FOM (i.e.\nerroneous parameter values), the hybrid approach yields substantially improved\ntime-series predictions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u636e\u548c\u5168\u9636\u6a21\u578b\uff08FOM\uff09\u7684\u6df7\u5408\u964d\u9636\u6a21\u578b\uff08ROM\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u65f6\u7a7a\u6df7\u6c8c\u52a8\u529b\u5b66\uff0c\u663e\u8457\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "motivation": "\u5229\u7528\u5df2\u77e5\u7269\u7406\u6a21\u578b\uff08FOM\uff09\u63d0\u5347\u6570\u636e\u9a71\u52a8\u6280\u672f\u5728\u6df7\u6c8c\u52a8\u529b\u5b66\u964d\u9636\u5efa\u6a21\u4e2d\u7684\u9884\u6d4b\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u81ea\u7f16\u7801\u5668\u627e\u5230\u4e0d\u53d8\u6d41\u5f62\u5750\u6807\uff0c\u5c06FOM\u7684\u5411\u91cf\u573a\u6295\u5f71\u5230\u6d41\u5f62\u4e0a\uff0c\u5e76\u7528\u52a8\u6001\u6570\u636e\u6821\u6b63\u6216\u4f5c\u4e3a\u8d1d\u53f6\u65af\u5148\u9a8c\u66f4\u65b0\u3002\u4f7f\u7528\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\u65b9\u6cd5\u3002", "result": "\u5728Kuramoto-Sivashinsky\u548c\u590d\u6742Ginzburg-Landau\u65b9\u7a0b\u6a21\u62df\u6570\u636e\u4e2d\uff0c\u6df7\u5408\u65b9\u6cd5\u5728\u6570\u636e\u4e30\u5bcc\u3001\u7a00\u7f3a\u751a\u81f3FOM\u53c2\u6570\u9519\u8bef\u60c5\u51b5\u4e0b\u5747\u663e\u8457\u63d0\u5347\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u7ed3\u5408\u7269\u7406\u6a21\u578b\u548c\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df7\u6c8c\u52a8\u529b\u5b66\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.21350", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21350", "abs": "https://arxiv.org/abs/2507.21350", "authors": ["Wenkai Tan", "Alvaro Velasquez", "Houbing Song"], "title": "DEM-NeRF: A Neuro-Symbolic Method for Scientific Discovery through Physics-Informed Simulation", "comment": null, "summary": "Neural networks have emerged as a powerful tool for modeling physical\nsystems, offering the ability to learn complex representations from limited\ndata while integrating foundational scientific knowledge. In particular,\nneuro-symbolic approaches that combine data-driven learning, the neuro, with\nsymbolic equations and rules, the symbolic, address the tension between methods\nthat are purely empirical, which risk straying from established physical\nprinciples, and traditional numerical solvers that demand complete geometric\nknowledge and can be prohibitively expensive for high-fidelity simulations. In\nthis work, we present a novel neuro-symbolic framework for reconstructing and\nsimulating elastic objects directly from sparse multi-view image sequences,\nwithout requiring explicit geometric information. Specifically, we integrate a\nneural radiance field (NeRF) for object reconstruction with physics-informed\nneural networks (PINN) that incorporate the governing partial differential\nequations of elasticity. In doing so, our method learns a spatiotemporal\nrepresentation of deforming objects that leverages both image supervision and\nsymbolic physical constraints. To handle complex boundary and initial\nconditions, which are traditionally confronted using finite element methods,\nboundary element methods, or sensor-based measurements, we employ an\nenergy-constrained Physics-Informed Neural Network architecture. This design\nenhances both simulation accuracy and the explainability of results.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u548c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u7a00\u758f\u591a\u89c6\u89d2\u56fe\u50cf\u5e8f\u5217\u91cd\u5efa\u548c\u6a21\u62df\u5f39\u6027\u7269\u4f53\uff0c\u65e0\u9700\u663e\u5f0f\u51e0\u4f55\u4fe1\u606f\u3002", "motivation": "\u89e3\u51b3\u7eaf\u7ecf\u9a8c\u65b9\u6cd5\u53ef\u80fd\u504f\u79bb\u7269\u7406\u539f\u7406\u4e0e\u4f20\u7edf\u6570\u503c\u6c42\u89e3\u5668\u9700\u8981\u5b8c\u6574\u51e0\u4f55\u4fe1\u606f\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u6574\u5408NeRF\u7528\u4e8e\u7269\u4f53\u91cd\u5efa\u548cPINN\u7528\u4e8e\u5f39\u6027\u529b\u5b66\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u7ed3\u5408\u56fe\u50cf\u76d1\u7763\u548c\u7269\u7406\u7ea6\u675f\u5b66\u4e60\u65f6\u7a7a\u8868\u793a\u3002", "result": "\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u62df\u7cbe\u5ea6\u548c\u7ed3\u679c\u53ef\u89e3\u91ca\u6027\uff0c\u80fd\u5904\u7406\u590d\u6742\u8fb9\u754c\u548c\u521d\u59cb\u6761\u4ef6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f39\u6027\u7269\u4f53\u91cd\u5efa\u548c\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7269\u7406\u4e00\u81f4\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.21357", "categories": ["cs.LG", "62M10", "I.5.1"], "pdf": "https://arxiv.org/pdf/2507.21357", "abs": "https://arxiv.org/abs/2507.21357", "authors": ["Yaoyu Zhang", "Chi-Guhn Lee"], "title": "A Contrastive Diffusion-based Network (CDNet) for Time Series Classification", "comment": "19 pages, conference", "summary": "Deep learning models are widely used for time series classification (TSC) due\nto their scalability and efficiency. However, their performance degrades under\nchallenging data conditions such as class similarity, multimodal distributions,\nand noise. To address these limitations, we propose CDNet, a Contrastive\nDiffusion-based Network that enhances existing classifiers by generating\ninformative positive and negative samples via a learned diffusion process.\nUnlike traditional diffusion models that denoise individual samples, CDNet\nlearns transitions between samples--both within and across classes--through\nconvolutional approximations of reverse diffusion steps. We introduce a\ntheoretically grounded CNN-based mechanism to enable both denoising and mode\ncoverage, and incorporate an uncertainty-weighted composite loss for robust\ntraining. Extensive experiments on the UCR Archive and simulated datasets\ndemonstrate that CDNet significantly improves state-of-the-art (SOTA) deep\nlearning classifiers, particularly under noisy, similar, and multimodal\nconditions.", "AI": {"tldr": "CDNet\u662f\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u6269\u6563\u7684\u7f51\u7edc\uff0c\u901a\u8fc7\u751f\u6210\u6b63\u8d1f\u6837\u672c\u589e\u5f3a\u73b0\u6709\u5206\u7c7b\u5668\uff0c\u663e\u8457\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u566a\u58f0\u3001\u76f8\u4f3c\u548c\u591a\u6a21\u6001\u6570\u636e\u6761\u4ef6\u4e0b\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4e2d\u56e0\u7c7b\u76f8\u4f3c\u6027\u3001\u591a\u6a21\u6001\u5206\u5e03\u548c\u566a\u58f0\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51faCDNet\uff0c\u5229\u7528\u5377\u79ef\u8fd1\u4f3c\u53cd\u5411\u6269\u6563\u6b65\u9aa4\u5b66\u4e60\u6837\u672c\u95f4\u8f6c\u6362\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u590d\u5408\u635f\u5931\u8fdb\u884c\u9c81\u68d2\u8bad\u7ec3\u3002", "result": "\u5728UCR Archive\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\uff0cCDNet\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u6027\u80fd\u3002", "conclusion": "CDNet\u5728\u6311\u6218\u6027\u6570\u636e\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21386", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21386", "abs": "https://arxiv.org/abs/2507.21386", "authors": ["Xuan Wu", "Di Wang", "Chunguo Wu", "Kaifang Qi", "Chunyan Miao", "Yubin Xiao", "Jian Zhang", "You Zhou"], "title": "Efficient Neural Combinatorial Optimization Solver for the Min-max Heterogeneous Capacitated Vehicle Routing Problem", "comment": null, "summary": "Numerous Neural Combinatorial Optimization (NCO) solvers have been proposed\nto address Vehicle Routing Problems (VRPs). However, most of these solvers\nfocus exclusively on single-vehicle VRP variants, overlooking the more\nrealistic min-max Heterogeneous Capacitated Vehicle Routing Problem (MMHCVRP),\nwhich involves multiple vehicles. Existing MMHCVRP solvers typically select a\nvehicle and its next node to visit at each decoding step, but often make myopic\ndecoding decisions and overlook key properties of MMHCVRP, including local\ntopological relationships, vehicle permutation invariance, and node symmetry,\nresulting in suboptimal performance. To better address these limitations, we\npropose ECHO, an efficient NCO solver. First, ECHO exploits the proposed\ndual-modality node encoder to capture local topological relationships among\nnodes. Subsequently, to mitigate myopic decisions, ECHO employs the proposed\nParameter-Free Cross-Attention mechanism to prioritize the vehicle selected in\nthe preceding decoding step. Finally, leveraging vehicle permutation invariance\nand node symmetry, we introduce a tailored data augment strategy for MMHCVRP to\nstabilize the Reinforcement Learning training process. To assess the\nperformance of ECHO, we conduct extensive experiments. The experimental results\ndemonstrate that ECHO outperforms state-of-the-art NCO solvers across varying\nnumbers of vehicles and nodes, and exhibits well-performing generalization\nacross both scales and distribution patterns. Finally, ablation studies\nvalidate the effectiveness of all proposed methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aECHO\u7684NCO\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u89e3\u51b3MMHCVRP\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u6a21\u6001\u8282\u70b9\u7f16\u7801\u5668\u548c\u65e0\u53c2\u6570\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709NCO\u6c42\u89e3\u5668\u591a\u5173\u6ce8\u5355\u8f66\u8f86VRP\u53d8\u4f53\uff0c\u5ffd\u89c6\u4e86\u66f4\u73b0\u5b9e\u7684MMHCVRP\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u77ed\u89c6\u89e3\u7801\u51b3\u7b56\u548c\u5ffd\u7565\u5173\u952e\u7279\u6027\u7684\u95ee\u9898\u3002", "method": "ECHO\u91c7\u7528\u53cc\u6a21\u6001\u8282\u70b9\u7f16\u7801\u5668\u6355\u6349\u5c40\u90e8\u62d3\u6251\u5173\u7cfb\uff0c\u5229\u7528\u65e0\u53c2\u6570\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u51cf\u5c11\u77ed\u89c6\u51b3\u7b56\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cECHO\u5728\u4e0d\u540c\u8f66\u8f86\u548c\u8282\u70b9\u6570\u91cf\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709NCO\u6c42\u89e3\u5668\uff0c\u5e76\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ECHO\u901a\u8fc7\u521b\u65b0\u7684\u7f16\u7801\u5668\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86MMHCVRP\u95ee\u9898\u7684\u6c42\u89e3\u6027\u80fd\u3002"}}
{"id": "2507.21397", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21397", "abs": "https://arxiv.org/abs/2507.21397", "authors": ["Fnu Hairi", "Jiao Yang", "Tianchen Zhou", "Haibo Yang", "Chaosheng Dong", "Fan Yang", "Michinari Momma", "Yan Gao", "Jia Liu"], "title": "Enabling Pareto-Stationarity Exploration in Multi-Objective Reinforcement Learning: A Multi-Objective Weighted-Chebyshev Actor-Critic Approach", "comment": null, "summary": "In many multi-objective reinforcement learning (MORL) applications, being\nable to systematically explore the Pareto-stationary solutions under multiple\nnon-convex reward objectives with theoretical finite-time sample complexity\nguarantee is an important and yet under-explored problem. This motivates us to\ntake the first step and fill the important gap in MORL. Specifically, in this\npaper, we propose a \\uline{M}ulti-\\uline{O}bjective weighted-\\uline{CH}ebyshev\n\\uline{A}ctor-critic (MOCHA) algorithm for MORL, which judiciously integrates\nthe weighted-Chebychev (WC) and actor-critic framework to enable\nPareto-stationarity exploration systematically with finite-time sample\ncomplexity guarantee. Sample complexity result of MOCHA algorithm reveals an\ninteresting dependency on $p_{\\min}$ in finding an $\\epsilon$-Pareto-stationary\nsolution, where $p_{\\min}$ denotes the minimum entry of a given weight vector\n$\\mathbf{p}$ in WC-scarlarization. By carefully choosing learning rates, the\nsample complexity for each exploration can be\n$\\tilde{\\mathcal{O}}(\\epsilon^{-2})$. Furthermore, simulation studies on a\nlarge KuaiRand offline dataset, show that the performance of MOCHA algorithm\nsignificantly outperforms other baseline MORL approaches.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.21404", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.21404", "abs": "https://arxiv.org/abs/2507.21404", "authors": ["Amber Huang", "Ian Scott Knight", "Slava Naprienko"], "title": "Data Leakage and Redundancy in the LIT-PCBA Benchmark", "comment": null, "summary": "LIT-PCBA is a widely used benchmark for virtual screening, but our audit\nreveals it is fundamentally compromised. The dataset suffers from egregious\ndata leakage, rampant duplication, and pervasive analog redundancy -- flaws\nthat invalidate its use for fair model evaluation. Notably, we identify 2,491\ninactives duplicated across training and validation sets, and thousands more\nrepeated within individual data splits (2,945 in training, 789 in validation).\nCritically, three ligands in the query set -- meant to represent unseen test\ncases -- are leaked: two appear in the training set, one in validation.\nStructural redundancy compounds these issues: for some targets, over 80% of\nquery ligands are near duplicates, with Tanimoto similarity >= 0.9. In ALDH1\nalone, we find 323 highly similar active pairs between training and validation\nsets, invalidating claims of chemical diversity. These and other flaws\ncollectively cause models trained on LIT-PCBA to memorize rather than\ngeneralize. To demonstrate the consequences of these data integrity failures,\nwe implement a trivial memorization-based baseline -- using no learning, no\nphysics, and no modeling -- that outperforms state-of-the-art models, including\ndeep neural networks like CHEESE, on LIT-PCBA simply by exploiting these\nartifacts. Our findings render the benchmark unfit for its intended purpose and\ncall into question previous results based on its use. We share this audit to\nraise awareness and provide tooling to help the community develop more rigorous\nand reliable datasets going forward. All scripts necessary to reproduce our\naudit and the baseline implementation are available at:\nhttps://github.com/sievestack/LIT-PCBA-audit", "AI": {"tldr": "LIT-PCBA\u57fa\u51c6\u6570\u636e\u96c6\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u6cc4\u6f0f\u3001\u91cd\u590d\u548c\u7ed3\u6784\u5197\u4f59\u95ee\u9898\uff0c\u5bfc\u81f4\u5176\u65e0\u6cd5\u516c\u5e73\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u63ed\u793aLIT-PCBA\u6570\u636e\u96c6\u5728\u865a\u62df\u7b5b\u9009\u57fa\u51c6\u4e2d\u7684\u7f3a\u9677\uff0c\u4ee5\u63d0\u9192\u793e\u533a\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u6570\u636e\u96c6\u3002", "method": "\u901a\u8fc7\u5ba1\u8ba1\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u6570\u636e\u6cc4\u6f0f\u3001\u91cd\u590d\u548c\u7ed3\u6784\u5197\u4f59\u95ee\u9898\uff0c\u5e76\u8bbe\u8ba1\u4e00\u4e2a\u57fa\u4e8e\u8bb0\u5fc6\u7684\u57fa\u7ebf\u65b9\u6cd5\u9a8c\u8bc1\u95ee\u9898\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5927\u91cf\u6570\u636e\u6cc4\u6f0f\u548c\u5197\u4f59\uff0c\u5bfc\u81f4\u6a21\u578b\u4ec5\u901a\u8fc7\u8bb0\u5fc6\u800c\u975e\u6cdb\u5316\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8d8a\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "LIT-PCBA\u4e0d\u9002\u5408\u4f5c\u4e3a\u516c\u5e73\u8bc4\u4f30\u7684\u57fa\u51c6\uff0c\u547c\u5401\u793e\u533a\u6539\u8fdb\u6570\u636e\u96c6\u8bbe\u8ba1\u3002"}}
{"id": "2507.21422", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21422", "abs": "https://arxiv.org/abs/2507.21422", "authors": ["Sujia Huang", "Lele Fu", "Zhen Cui", "Tong Zhang", "Na Song", "Bo Huang"], "title": "Torque-based Graph Surgery:Enhancing Graph Neural Networks with Hierarchical Rewiring", "comment": null, "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning from\ngraph-structured data, leveraging message passing to diffuse information and\nupdate node representations. However, most efforts have suggested that native\ninteractions encoded in the graph may not be friendly for this process,\nmotivating the development of graph rewiring methods. In this work, we propose\na torque-driven hierarchical rewiring strategy, inspired by the notion of\ntorque in classical mechanics, dynamically modulating message passing to\nimprove representation learning in heterophilous graphs and enhance robustness\nagainst noisy graphs. Specifically, we define an interference-aware torque\nmetric that integrates structural distance and energy scores to quantify the\nperturbation induced by edges, thereby encouraging each node to aggregate\ninformation from its nearest low-energy neighbors. We use the metric to\nhierarchically reconfigure the receptive field of each layer by judiciously\npruning high-torque edges and adding low-torque links, suppressing propagation\nnoise and boosting pertinent signals. Extensive evaluations on benchmark\ndatasets show that our approach surpasses state-of-the-art methods on both\nheterophilous and homophilous graphs, and maintains high accuracy on noisy\ngraph.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u626d\u77e9\u7684\u5206\u5c42\u56fe\u91cd\u8fde\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6d88\u606f\u4f20\u9012\u6765\u63d0\u5347\u5f02\u8d28\u56fe\u548c\u566a\u58f0\u56fe\u4e2d\u7684\u8868\u793a\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u539f\u751f\u56fe\u7ed3\u6784\u4e0a\u7684\u6d88\u606f\u4f20\u9012\u53ef\u80fd\u4e0d\u591f\u9ad8\u6548\uff0c\u5c24\u5176\u662f\u5728\u5f02\u8d28\u56fe\u548c\u566a\u58f0\u56fe\u4e2d\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u56fe\u91cd\u8fde\u65b9\u6cd5\u3002", "method": "\u5b9a\u4e49\u4e86\u4e00\u79cd\u5e72\u6270\u611f\u77e5\u7684\u626d\u77e9\u5ea6\u91cf\uff0c\u7ed3\u5408\u7ed3\u6784\u8ddd\u79bb\u548c\u80fd\u91cf\u5206\u6570\uff0c\u91cf\u5316\u8fb9\u7684\u6270\u52a8\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u4fee\u526a\u9ad8\u626d\u77e9\u8fb9\u548c\u6dfb\u52a0\u4f4e\u626d\u77e9\u8fb9\u6765\u91cd\u6784\u611f\u53d7\u91ce\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f02\u8d28\u56fe\u548c\u540c\u8d28\u56fe\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u566a\u58f0\u56fe\u4e2d\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "\u626d\u77e9\u9a71\u52a8\u7684\u5206\u5c42\u91cd\u8fde\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86GNNs\u5728\u590d\u6742\u56fe\u7ed3\u6784\u4e2d\u7684\u8868\u73b0\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.21433", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21433", "abs": "https://arxiv.org/abs/2507.21433", "authors": ["Kaiwen Chen", "Xin Tan", "Minchen Yu", "Hong Xu"], "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through KV Cache Reuse", "comment": "11 pages, 7 figures, submitted to AAAI 2026", "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.", "AI": {"tldr": "MemShare\u662f\u4e00\u79cd\u65b0\u578bKV\u7f13\u5b58\u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u548c\u91cd\u7528\u9ad8\u5ea6\u76f8\u4f3c\u7684\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684KV\u7f13\u5b58\u5757\uff0c\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5f00\u9500\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u751f\u6210\u7684\u5197\u957f\u601d\u7ef4\u94fe\u5bfc\u81f4\u5185\u5b58\u5f00\u9500\u5927\uff0c\u800c\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u4e3aKV\u7f13\u5b58\u7684\u91cd\u7528\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51faMemShare\uff0c\u91c7\u7528\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u8bc6\u522b\u53ef\u91cd\u7528\u7684KV\u7f13\u5b58\u5757\uff0c\u5e76\u5b9e\u73b0\u96f6\u62f7\u8d1d\u7f13\u5b58\u91cd\u7528\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMemShare\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe84.79%\u3002", "conclusion": "MemShare\u901a\u8fc7\u9ad8\u6548KV\u7f13\u5b58\u7ba1\u7406\uff0c\u663e\u8457\u4f18\u5316\u4e86LRMs\u7684\u5185\u5b58\u548c\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2507.21437", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21437", "abs": "https://arxiv.org/abs/2507.21437", "authors": ["Tiantian Sun", "Jian Zu"], "title": "PVD-ONet: A Multi-scale Neural Operator Method for Singularly Perturbed Boundary Layer Problems", "comment": "34pages,14figures", "summary": "Physics-informed neural networks and Physics-informed DeepONet excel in\nsolving partial differential equations; however, they often fail to converge\nfor singularly perturbed problems. To address this, we propose two novel\nframeworks, Prandtl-Van Dyke neural network (PVD-Net) and its operator learning\nextension Prandtl-Van Dyke Deep Operator Network (PVD-ONet), which rely solely\non governing equations without data. To address varying task-specific\nrequirements, both PVD-Net and PVD-ONet are developed in two distinct versions,\ntailored respectively for stability-focused and high-accuracy modeling. The\nleading-order PVD-Net adopts a two-network architecture combined with Prandtl's\nmatching condition, targeting stability-prioritized scenarios. The high-order\nPVD-Net employs a five-network design with Van Dyke's matching principle to\ncapture fine-scale boundary layer structures, making it ideal for high-accuracy\nscenarios. PVD-ONet generalizes PVD-Net to the operator learning setting by\nassembling multiple DeepONet modules, directly mapping initial conditions to\nsolution operators and enabling instant predictions for an entire family of\nboundary layer problems without retraining. Numerical experiments on various\nmodels show that our proposed methods consistently outperform existing\nbaselines under various error metrics, thereby offering a powerful new approach\nfor multi-scale problems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u6846\u67b6PVD-Net\u548cPVD-ONet\uff0c\u7528\u4e8e\u89e3\u51b3\u5947\u5f02\u6444\u52a8\u95ee\u9898\uff0c\u65e0\u9700\u6570\u636e\u4ec5\u4f9d\u8d56\u63a7\u5236\u65b9\u7a0b\uff0c\u5e76\u5728\u7a33\u5b9a\u6027\u548c\u9ad8\u7cbe\u5ea6\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u5728\u5947\u5f02\u6444\u52a8\u95ee\u9898\u4e0a\u6536\u655b\u56f0\u96be\uff0c\u9700\u8981\u65b0\u65b9\u6cd5\u89e3\u51b3\u3002", "method": "PVD-Net\u91c7\u7528\u4e24\u7f51\u7edc\u67b6\u6784\uff08\u7a33\u5b9a\u6027\u4f18\u5148\uff09\u548c\u4e94\u7f51\u7edc\u67b6\u6784\uff08\u9ad8\u7cbe\u5ea6\u4f18\u5148\uff09\uff0cPVD-ONet\u6269\u5c55\u4e3a\u7b97\u5b50\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u65b0\u65b9\u6cd5\u5728\u591a\u79cd\u8bef\u5dee\u6307\u6807\u4e0b\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "conclusion": "PVD-Net\u548cPVD-ONet\u4e3a\u591a\u5c3a\u5ea6\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.21452", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.21452", "abs": "https://arxiv.org/abs/2507.21452", "authors": ["Sodtavilan Odonchimed", "Tatsuya Matsushima", "Simon Holk", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Retrieve-Augmented Generation for Speeding up Diffusion Policy without Additional Training", "comment": null, "summary": "Diffusion Policies (DPs) have attracted attention for their ability to\nachieve significant accuracy improvements in various imitation learning tasks.\nHowever, DPs depend on Diffusion Models, which require multiple noise removal\nsteps to generate a single action, resulting in long generation times. To solve\nthis problem, knowledge distillation-based methods such as Consistency Policy\n(CP) have been proposed. However, these methods require a significant amount of\ntraining time, especially for difficult tasks. In this study, we propose RAGDP\n(Retrieve-Augmented Generation for Diffusion Policies) as a novel framework\nthat eliminates the need for additional training using a knowledge base to\nexpedite the inference of pre-trained DPs. In concrete, RAGDP encodes\nobservation-action pairs through the DP encoder to construct a vector database\nof expert demonstrations. During inference, the current observation is\nembedded, and the most similar expert action is extracted. This extracted\naction is combined with an intermediate noise removal step to reduce the number\nof steps required compared to the original diffusion step. We show that by\nusing RAGDP with the base model and existing acceleration methods, we improve\nthe accuracy and speed trade-off with no additional training. Even when\naccelerating the models 20 times, RAGDP maintains an advantage in accuracy,\nwith a 7% increase over distillation models such as CP.", "AI": {"tldr": "RAGDP\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u5e93\u52a0\u901f\u9884\u8bad\u7ec3\u6269\u6563\u7b56\u7565\u7684\u63a8\u7406\uff0c\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u901f\u5ea6\u3002", "motivation": "\u6269\u6563\u7b56\u7565\uff08DPs\uff09\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u751f\u6210\u52a8\u4f5c\u8017\u65f6\uff1b\u73b0\u6709\u65b9\u6cd5\u5982\u4e00\u81f4\u6027\u7b56\u7565\uff08CP\uff09\u8bad\u7ec3\u65f6\u95f4\u957f\u3002", "method": "RAGDP\u901a\u8fc7\u6784\u5efa\u4e13\u5bb6\u6f14\u793a\u7684\u5411\u91cf\u6570\u636e\u5e93\uff0c\u7ed3\u5408\u4e2d\u95f4\u566a\u58f0\u53bb\u9664\u6b65\u9aa4\uff0c\u51cf\u5c11\u63a8\u7406\u6b65\u9aa4\u3002", "result": "RAGDP\u572820\u500d\u52a0\u901f\u4e0b\u4ecd\u4fdd\u6301\u51c6\u786e\u6027\u4f18\u52bf\uff0c\u6bd4CP\u9ad87%\u3002", "conclusion": "RAGDP\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u7b56\u7565\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.21494", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21494", "abs": "https://arxiv.org/abs/2507.21494", "authors": ["Wenxuan Bao", "Ruxi Deng", "Ruizhong Qiu", "Tianxin Wei", "Hanghang Tong", "Jingrui He"], "title": "Latte: Collaborative Test-Time Adaptation of Vision-Language Models in Federated Learning", "comment": "Accepted by ICCV 2025", "summary": "Test-time adaptation with pre-trained vision-language models has gained\nincreasing attention for addressing distribution shifts during testing. Among\nthese approaches, memory-based algorithms stand out due to their training-free\nnature and ability to leverage historical test data. However, existing\ntest-time adaptation methods are typically designed for a single domain with\nabundant data. In decentralized settings such as federated learning, applying\nthese methods individually to each client suffers from limited test data, while\ndirectly sharing a single global memory via the server prevents proper\npersonalization to each client's unique distribution. To address this, we\npropose Latte, a novel framework where each client maintains a local memory to\nstore embeddings from its own historical test data and an external memory to\nstore class prototypes from other relevant clients. During communication, each\nclient retrieves prototypes from similar clients under the server's\ncoordination to expand its memory. For local adaptation, Latte utilizes both\nembedding similarity and uncertainty to enhance model performance. Our\ntheoretical analysis shows that Latte effectively leverages in-distribution\nclients while remaining robust to out-of-distribution clients. Extensive\nexperiments on domain adaptation and corruption benchmarks validate that Latte\nachieves superior performance in decentralized settings, while introducing only\nnegligible communication and computation costs. Our code is available at\nhttps://github.com/baowenxuan/Latte .", "AI": {"tldr": "Latte\u6846\u67b6\u901a\u8fc7\u672c\u5730\u548c\u5916\u90e8\u5185\u5b58\u5b58\u50a8\u5386\u53f2\u6d4b\u8bd5\u6570\u636e\uff0c\u5229\u7528\u5d4c\u5165\u76f8\u4f3c\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5206\u6563\u5f0f\u5b66\u4e60\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u5206\u6563\u5f0f\u5b66\u4e60\u4e2d\u6d4b\u8bd5\u6570\u636e\u6709\u9650\u548c\u4e2a\u6027\u5316\u5206\u5e03\u9002\u5e94\u7684\u95ee\u9898\u3002", "method": "\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7ef4\u62a4\u672c\u5730\u548c\u5916\u90e8\u5185\u5b58\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u534f\u8c03\u68c0\u7d22\u76f8\u4f3c\u5ba2\u6237\u7aef\u7684\u539f\u578b\uff0c\u7ed3\u5408\u5d4c\u5165\u76f8\u4f3c\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u672c\u5730\u9002\u5e94\u3002", "result": "\u5728\u9886\u57df\u9002\u5e94\u548c\u635f\u574f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u6781\u4f4e\u3002", "conclusion": "Latte\u5728\u5206\u6563\u5f0f\u73af\u5883\u4e2d\u9ad8\u6548\u4e14\u9c81\u68d2\uff0c\u9002\u7528\u4e8e\u591a\u5ba2\u6237\u7aef\u5206\u5e03\u9002\u5e94\u3002"}}
{"id": "2507.21504", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21504", "abs": "https://arxiv.org/abs/2507.21504", "authors": ["Mahmoud Mohammadi", "Yipeng Li", "Jane Lo", "Wendy Yip"], "title": "Evaluation and Benchmarking of LLM Agents: A Survey", "comment": null, "summary": "The rise of LLM-based agents has opened new frontiers in AI applications, yet\nevaluating these agents remains a complex and underdeveloped area. This survey\nprovides an in-depth overview of the emerging field of LLM agent evaluation,\nintroducing a two-dimensional taxonomy that organizes existing work along (1)\nevaluation objectives -- what to evaluate, such as agent behavior,\ncapabilities, reliability, and safety -- and (2) evaluation process -- how to\nevaluate, including interaction modes, datasets and benchmarks, metric\ncomputation methods, and tooling. In addition to taxonomy, we highlight\nenterprise-specific challenges, such as role-based access to data, the need for\nreliability guarantees, dynamic and long-horizon interactions, and compliance,\nwhich are often overlooked in current research. We also identify future\nresearch directions, including holistic, more realistic, and scalable\nevaluation. This work aims to bring clarity to the fragmented landscape of\nagent evaluation and provide a framework for systematic assessment, enabling\nresearchers and practitioners to evaluate LLM agents for real-world deployment.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86LLM\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u5206\u7c7b\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u6311\u6218\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u7684\u8bc4\u4f30\u9886\u57df\u590d\u6742\u4e14\u4e0d\u6210\u719f\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u6846\u67b6\u6765\u6307\u5bfc\u7814\u7a76\u548c\u5b9e\u8df5\u3002", "method": "\u63d0\u51fa\u4e8c\u7ef4\u5206\u7c7b\u6cd5\uff0c\u5305\u62ec\u8bc4\u4f30\u76ee\u6807\uff08\u5982\u884c\u4e3a\u3001\u80fd\u529b\u3001\u53ef\u9760\u6027\uff09\u548c\u8bc4\u4f30\u8fc7\u7a0b\uff08\u5982\u4ea4\u4e92\u6a21\u5f0f\u3001\u6570\u636e\u96c6\u3001\u5de5\u5177\uff09\u3002", "result": "\u603b\u7ed3\u4e86\u73b0\u6709\u5de5\u4f5c\uff0c\u7a81\u51fa\u4e86\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff08\u5982\u6570\u636e\u8bbf\u95ee\u3001\u53ef\u9760\u6027\uff09\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\uff08\u5982\u5168\u9762\u3001\u73b0\u5b9e\u7684\u8bc4\u4f30\uff09\u3002", "conclusion": "\u672c\u6587\u4e3aLLM\u667a\u80fd\u4f53\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u52a9\u529b\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u8bc4\u4f30\u5de5\u4f5c\u3002"}}
{"id": "2507.21531", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21531", "abs": "https://arxiv.org/abs/2507.21531", "authors": ["Pedram Rajaei", "Maryam Ostadsharif Memar", "Navid Ziaei", "Behzad Nazari", "Ali Yousefi"], "title": "Hierarchical Stochastic Differential Equation Models for Latent Manifold Learning in Neural Time Series", "comment": null, "summary": "The manifold hypothesis suggests that high-dimensional neural time series lie\non a low-dimensional manifold shaped by simpler underlying dynamics. To uncover\nthis structure, latent dynamical variable models such as state-space models,\nrecurrent neural networks, neural ordinary differential equations, and Gaussian\nProcess Latent Variable Models are widely used. We propose a novel hierarchical\nstochastic differential equation (SDE) model that balances computational\nefficiency and interpretability, addressing key limitations of existing\nmethods. Our model assumes the trajectory of a manifold can be reconstructed\nfrom a sparse set of samples from the manifold trajectory. The latent space is\nmodeled using Brownian bridge SDEs, with points - specified in both time and\nvalue - sampled from a multivariate marked point process. These Brownian\nbridges define the drift of a second set of SDEs, which are then mapped to the\nobserved data. This yields a continuous, differentiable latent process capable\nof modeling arbitrarily complex time series as the number of manifold points\nincreases. We derive training and inference procedures and show that the\ncomputational cost of inference scales linearly with the length of the\nobservation data. We then validate our model on both synthetic data and neural\nrecordings to demonstrate that it accurately recovers the underlying manifold\nstructure and scales effectively with data dimensionality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u5206\u5c42\u968f\u673a\u5fae\u5206\u65b9\u7a0b\uff08SDE\uff09\u6a21\u578b\uff0c\u7528\u4e8e\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u5730\u5efa\u6a21\u9ad8\u7ef4\u795e\u7ecf\u65f6\u95f4\u5e8f\u5217\u7684\u4f4e\u7ef4\u6d41\u5f62\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u7684\u6d41\u5f62\u7ed3\u6784\u3002", "method": "\u91c7\u7528\u5e03\u6717\u6865SDE\u5efa\u6a21\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u901a\u8fc7\u591a\u53d8\u91cf\u6807\u8bb0\u70b9\u8fc7\u7a0b\u91c7\u6837\uff0c\u6784\u5efa\u8fde\u7eed\u53ef\u5fae\u7684\u6f5c\u5728\u8fc7\u7a0b\u3002", "result": "\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u548c\u795e\u7ecf\u8bb0\u5f55\u4e0a\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6062\u590d\u6d41\u5f62\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u968f\u6570\u636e\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u3002"}}
{"id": "2507.21616", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21616", "abs": "https://arxiv.org/abs/2507.21616", "authors": ["Kevin Doran", "Tom Baden"], "title": "Categorical Distributions are Effective Neural Network Outputs for Event Prediction", "comment": "32 pages, 26 figures", "summary": "We demonstrate the effectiveness of using a simple neural network output, a\ncategorical probability distribution, for the task of next spike prediction.\nThis case study motivates an investigation into why this simple output\nstructure is not commonly used with neural temporal point process models. We\nfind evidence that many existing datasets for evaluating temporal point process\nmodels do not reveal much information about the underlying event generating\nprocesses, and many existing models perform well due to regularization effects\nof model size and constraints on output structure. We extend existing datasets\nand create new ones in order to explore outside of this information limited\nregime and find that outputting a simple categorical distribution is\ncompetitive across a wide range of datasets.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4f7f\u7528\u7b80\u5355\u7684\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\uff08\u5206\u7c7b\u6982\u7387\u5206\u5e03\uff09\u8fdb\u884c\u4e0b\u4e00\u4e8b\u4ef6\u9884\u6d4b\u7684\u6709\u6548\u6027\uff0c\u5e76\u5206\u6790\u4e86\u5176\u672a\u88ab\u5e7f\u6cdb\u91c7\u7528\u7684\u539f\u56e0\u3002", "motivation": "\u7814\u7a76\u4e3a\u4f55\u7b80\u5355\u7684\u8f93\u51fa\u7ed3\u6784\u5728\u795e\u7ecf\u65f6\u95f4\u70b9\u8fc7\u7a0b\u6a21\u578b\u4e2d\u4e0d\u5e38\u89c1\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u548c\u521b\u5efa\u65b0\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u7b80\u5355\u5206\u7c7b\u5206\u5e03\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u73b0\u6709\u6570\u636e\u96c6\u672a\u80fd\u5145\u5206\u63ed\u793a\u4e8b\u4ef6\u751f\u6210\u8fc7\u7a0b\uff0c\u800c\u7b80\u5355\u5206\u7c7b\u5206\u5e03\u5728\u5e7f\u6cdb\u6570\u636e\u96c6\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7b80\u5355\u5206\u7c7b\u5206\u5e03\u662f\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u8f93\u51fa\u7ed3\u6784\u3002"}}
{"id": "2507.21648", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21648", "abs": "https://arxiv.org/abs/2507.21648", "authors": ["Raiyan R. Khan", "Philippe Chlenski", "Itsik Pe'er"], "title": "Hyperbolic Genome Embeddings", "comment": "30 pages, 16 figures, 10 tables. Camera-ready version for ICLR 2025", "summary": "Current approaches to genomic sequence modeling often struggle to align the\ninductive biases of machine learning models with the evolutionarily-informed\nstructure of biological systems. To this end, we formulate a novel application\nof hyperbolic CNNs that exploits this structure, enabling more expressive DNA\nsequence representations. Our strategy circumvents the need for explicit\nphylogenetic mapping while discerning key properties of sequences pertaining to\ncore functional and regulatory behavior. Across 37 out of 42 genome\ninterpretation benchmark datasets, our hyperbolic models outperform their\nEuclidean equivalents. Notably, our approach even surpasses state-of-the-art\nperformance on seven GUE benchmark datasets, consistently outperforming many\nDNA language models while using orders of magnitude fewer parameters and\navoiding pretraining. Our results include a novel set of benchmark\ndatasets--the Transposable Elements Benchmark--which explores a major but\nunderstudied component of the genome with deep evolutionary significance. We\nfurther motivate our work by exploring how our hyperbolic models recognize\ngenomic signal under various data-generating conditions and by constructing an\nempirical method for interpreting the hyperbolicity of dataset embeddings.\nThroughout these assessments, we find persistent evidence highlighting the\npotential of our hyperbolic framework as a robust paradigm for genome\nrepresentation learning. Our code and benchmark datasets are available at\nhttps://github.com/rrkhan/HGE.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u66f2CNN\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u57fa\u56e0\u7ec4\u5e8f\u5217\u5efa\u6a21\uff0c\u901a\u8fc7\u5229\u7528\u751f\u7269\u7cfb\u7edf\u7684\u8fdb\u5316\u7ed3\u6784\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684DNA\u5e8f\u5217\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u6a21\u578b\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u4e00\u4e9b\u5148\u8fdb\u7684DNA\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u56e0\u7ec4\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u5c06\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u5f52\u7eb3\u504f\u7f6e\u4e0e\u751f\u7269\u7cfb\u7edf\u7684\u8fdb\u5316\u7ed3\u6784\u5bf9\u9f50\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cc\u66f2CNN\uff0c\u65e0\u9700\u663e\u5f0f\u7cfb\u7edf\u53d1\u80b2\u6620\u5c04\uff0c\u76f4\u63a5\u6355\u6349\u5e8f\u5217\u7684\u5173\u952e\u529f\u80fd\u4e0e\u8c03\u63a7\u7279\u6027\u3002", "result": "\u572842\u4e2a\u57fa\u56e0\u7ec4\u89e3\u91ca\u57fa\u51c6\u6570\u636e\u96c6\u4e2d\uff0c37\u4e2a\u4e0a\u53cc\u66f2\u6a21\u578b\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97\u6a21\u578b\uff0c\u5e76\u57287\u4e2aGUE\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u53cc\u66f2\u6846\u67b6\u4e3a\u57fa\u56e0\u7ec4\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u8303\u5f0f\uff0c\u5177\u6709\u663e\u8457\u6f5c\u529b\u3002"}}
{"id": "2507.21653", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21653", "abs": "https://arxiv.org/abs/2507.21653", "authors": ["Yuan Li", "Jun Hu", "Bryan Hooi", "Bingsheng He", "Cheng Chen"], "title": "DGP: A Dual-Granularity Prompting Framework for Fraud Detection with Graph-Enhanced LLMs", "comment": null, "summary": "Real-world fraud detection applications benefit from graph learning\ntechniques that jointly exploit node features, often rich in textual data, and\ngraph structural information. Recently, Graph-Enhanced LLMs emerge as a\npromising graph learning approach that converts graph information into prompts,\nexploiting LLMs' ability to reason over both textual and structural\ninformation. Among them, text-only prompting, which converts graph information\nto prompts consisting solely of text tokens, offers a solution that relies only\non LLM tuning without requiring additional graph-specific encoders. However,\ntext-only prompting struggles on heterogeneous fraud-detection graphs:\nmulti-hop relations expand exponentially with each additional hop, leading to\nrapidly growing neighborhoods associated with dense textual information. These\nneighborhoods may overwhelm the model with long, irrelevant content in the\nprompt and suppress key signals from the target node, thereby degrading\nperformance. To address this challenge, we propose Dual Granularity Prompting\n(DGP), which mitigates information overload by preserving fine-grained textual\ndetails for the target node while summarizing neighbor information into\ncoarse-grained text prompts. DGP introduces tailored summarization strategies\nfor different data modalities, bi-level semantic abstraction for textual fields\nand statistical aggregation for numerical features, enabling effective\ncompression of verbose neighbor content into concise, informative prompts.\nExperiments across public and industrial datasets demonstrate that DGP operates\nwithin a manageable token budget while improving fraud detection performance by\nup to 6.8% (AUPRC) over state-of-the-art methods, showing the potential of\nGraph-Enhanced LLMs for fraud detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDual Granularity Prompting (DGP)\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6587\u672c\u7ec6\u8282\u548c\u7c97\u7c92\u5ea6\u90bb\u5c45\u4fe1\u606f\u6458\u8981\uff0c\u89e3\u51b3\u5f02\u6784\u6b3a\u8bc8\u68c0\u6d4b\u56fe\u4e2d\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6587\u672c\u63d0\u793a\u5728\u5f02\u6784\u6b3a\u8bc8\u68c0\u6d4b\u56fe\u4e2d\u56e0\u591a\u8df3\u5173\u7cfb\u5bfc\u81f4\u7684\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u907f\u514d\u5173\u952e\u4fe1\u53f7\u88ab\u6df9\u6ca1\u3002", "method": "\u63d0\u51faDGP\u65b9\u6cd5\uff0c\u7ed3\u5408\u7ec6\u7c92\u5ea6\u76ee\u6807\u8282\u70b9\u6587\u672c\u548c\u7c97\u7c92\u5ea6\u90bb\u5c45\u4fe1\u606f\u6458\u8981\uff0c\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u6a21\u6001\u91c7\u7528\u5b9a\u5236\u5316\u6458\u8981\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDGP\u5728\u53ef\u63a7\u7684token\u9884\u7b97\u5185\uff0c\u5c06\u6b3a\u8bc8\u68c0\u6d4b\u6027\u80fd\u63d0\u53476.8%\uff08AUPRC\uff09\u3002", "conclusion": "DGP\u5c55\u793a\u4e86\u56fe\u589e\u5f3aLLMs\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u6709\u6548\u5e73\u8861\u4fe1\u606f\u538b\u7f29\u4e0e\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.21670", "categories": ["cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.21670", "abs": "https://arxiv.org/abs/2507.21670", "authors": ["Paul Patrone", "Anthony Kearsley"], "title": "Probabilistic Consistency in Machine Learning and Its Connection to Uncertainty Quantification", "comment": null, "summary": "Machine learning (ML) is often viewed as a powerful data analysis tool that\nis easy to learn because of its black-box nature. Yet this very nature also\nmakes it difficult to quantify confidence in predictions extracted from ML\nmodels, and more fundamentally, to understand how such models are mathematical\nabstractions of training data. The goal of this paper is to unravel these\nissues and their connections to uncertainty quantification (UQ) by pursuing a\nline of reasoning motivated by diagnostics. In such settings, prevalence - i.e.\nthe fraction of elements in class - is often of inherent interest. Here we\nanalyze the many interpretations of prevalence to derive a level-set theory of\nclassification, which shows that certain types of self-consistent ML models are\nequivalent to class-conditional probability distributions. We begin by studying\nthe properties of binary Bayes optimal classifiers, recognizing that their\nboundary sets can be reinterpreted as level-sets of pairwise density ratios. By\nparameterizing Bayes classifiers in terms of the prevalence, we then show that\nthey satisfy important monotonicity and class-switching properties that can be\nused to deduce the density ratios without direct access to the boundary sets.\nMoreover, this information is sufficient for tasks such as constructing the\nmulticlass Bayes-optimal classifier and estimating inherent uncertainty in the\nclass assignments. In the multiclass case, we use these results to deduce\nnormalization and self-consistency conditions, the latter being equivalent to\nthe law of total probability for classifiers. We also show that these are\nnecessary conditions for arbitrary ML models to have valid probabilistic\ninterpretations. Throughout we demonstrate how this analysis informs the\nbroader task of UQ for ML via an uncertainty propagation framework.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8bca\u65ad\u63a8\u7406\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6d41\u884c\u5ea6\u7684\u5206\u7c7b\u7406\u8bba\uff0c\u8bc1\u660e\u4e86\u81ea\u6d3d\u6a21\u578b\u4e0e\u7c7b\u6761\u4ef6\u6982\u7387\u5206\u5e03\u7684\u7b49\u4ef7\u6027\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7684\u9ed1\u7bb1\u7279\u6027\u4f7f\u5176\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u96be\u4ee5\u91cf\u5316\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u5e76\u63a2\u8ba8\u5176\u4e0e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u8054\u7cfb\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6d41\u884c\u5ea6\u7684\u591a\u79cd\u89e3\u91ca\uff0c\u63a8\u5bfc\u51fa\u5206\u7c7b\u7684\u5c42\u6b21\u96c6\u7406\u8bba\uff0c\u5e76\u7814\u7a76\u4e8c\u5143\u8d1d\u53f6\u65af\u6700\u4f18\u5206\u7c7b\u5668\u7684\u6027\u8d28\u53ca\u5176\u53c2\u6570\u5316\u3002", "result": "\u8bc1\u660e\u4e86\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\u6ee1\u8db3\u5355\u8c03\u6027\u548c\u7c7b\u5207\u6362\u6027\u8d28\uff0c\u53ef\u7528\u4e8e\u63a8\u65ad\u5bc6\u5ea6\u6bd4\uff0c\u5e76\u63a8\u5bfc\u51fa\u591a\u7c7b\u5206\u7c7b\u7684\u5f52\u4e00\u5316\u548c\u81ea\u6d3d\u6761\u4ef6\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6982\u7387\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u6846\u67b6\u4e3a\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.21710", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21710", "abs": "https://arxiv.org/abs/2507.21710", "authors": ["Hongwei Ma", "Junbin Gao", "Minh-Ngoc Tran"], "title": "PREIG: Physics-informed and Reinforcement-driven Interpretable GRU for Commodity Demand Forecasting", "comment": null, "summary": "Accurately forecasting commodity demand remains a critical challenge due to\nvolatile market dynamics, nonlinear dependencies, and the need for economically\nconsistent predictions. This paper introduces PREIG, a novel deep learning\nframework tailored for commodity demand forecasting. The model uniquely\nintegrates a Gated Recurrent Unit (GRU) architecture with physics-informed\nneural network (PINN) principles by embedding a domain-specific economic\nconstraint: the negative elasticity between price and demand. This constraint\nis enforced through a customized loss function that penalizes violations of the\nphysical rule, ensuring that model predictions remain interpretable and aligned\nwith economic theory. To further enhance predictive performance and stability,\nPREIG incorporates a hybrid optimization strategy that couples NAdam and L-BFGS\nwith Population-Based Training (POP). Experiments across multiple commodities\ndatasets demonstrate that PREIG significantly outperforms traditional\neconometric models (ARIMA,GARCH) and deep learning baselines (BPNN,RNN) in both\nRMSE and MAPE. When compared with GRU,PREIG maintains good explainability while\nstill performing well in prediction. By bridging domain knowledge, optimization\ntheory and deep learning, PREIG provides a robust, interpretable, and scalable\nsolution for high-dimensional nonlinear time series forecasting in economy.", "AI": {"tldr": "PREIG\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408GRU\u548cPINN\u539f\u7406\uff0c\u901a\u8fc7\u5d4c\u5165\u7ecf\u6d4e\u7ea6\u675f\u63d0\u5347\u5546\u54c1\u9700\u6c42\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5546\u54c1\u9700\u6c42\u9884\u6d4b\u9762\u4e34\u5e02\u573a\u52a8\u6001\u6ce2\u52a8\u548c\u975e\u7ebf\u6027\u4f9d\u8d56\u7b49\u6311\u6218\uff0c\u9700\u8981\u7ecf\u6d4e\u4e00\u81f4\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "PREIG\u6574\u5408GRU\u548cPINN\uff0c\u901a\u8fc7\u5b9a\u5236\u635f\u5931\u51fd\u6570\u5d4c\u5165\u4ef7\u683c\u4e0e\u9700\u6c42\u7684\u8d1f\u5f39\u6027\u7ea6\u675f\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u4f18\u5316\u7b56\u7565\uff08NAdam\u3001L-BFGS\u548cPOP\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPREIG\u5728RMSE\u548cMAPE\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8ba1\u91cf\u6a21\u578b\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u597d\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "PREIG\u901a\u8fc7\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u3001\u4f18\u5316\u7406\u8bba\u548c\u6df1\u5ea6\u5b66\u4e60\uff0c\u4e3a\u9ad8\u7ef4\u975e\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21720", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21720", "abs": "https://arxiv.org/abs/2507.21720", "authors": ["Gang Wang", "Peng Hu"], "title": "Data-Driven Extended Corresponding State Approach for Residual Property Prediction of Hydrofluoroolefins", "comment": null, "summary": "Hydrofluoroolefins are considered the most promising next-generation\nrefrigerants due to their extremely low global warming potential values, which\ncan effectively mitigate the global warming effect. However, the lack of\nreliable thermodynamic data hinders the discovery and application of newer and\nsuperior hydrofluoroolefin refrigerants. In this work, integrating the\nstrengths of theoretical method and data-driven method, we proposed a neural\nnetwork extended corresponding state model to predict the residual\nthermodynamic properties of hydrofluoroolefin refrigerants. The innovation is\nthat the fluids are characterized through their microscopic molecular\nstructures by the inclusion of graph neural network module and the specialized\ndesign of model architecture to enhance its generalization ability. The\nproposed model is trained using the highly accurate data of available known\nfluids, and evaluated via the leave-one-out cross-validation method. Compared\nto conventional extended corresponding state models or cubic equation of state,\nthe proposed model shows significantly improved accuracy for density and energy\nproperties in liquid and supercritical regions, with average absolute deviation\nof 1.49% (liquid) and 2.42% (supercritical) for density, 3.37% and 2.50% for\nresidual entropy, 1.85% and 1.34% for residual enthalpy. These results\ndemonstrate the effectiveness of embedding physics knowledge into the machine\nlearning model. The proposed neural network extended corresponding state model\nis expected to significantly accelerate the discovery of novel\nhydrofluoroolefin refrigerants.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u7269\u7406\u77e5\u8bc6\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u6c22\u6c1f\u70ef\u70c3\u5236\u51b7\u5242\u7684\u5269\u4f59\u70ed\u529b\u5b66\u6027\u8d28\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u6c22\u6c1f\u70ef\u70c3\u56e0\u5176\u6781\u4f4e\u7684\u5168\u7403\u53d8\u6696\u6f5c\u80fd\u503c\u6210\u4e3a\u4e0b\u4e00\u4ee3\u5236\u51b7\u5242\u7684\u6709\u529b\u5019\u9009\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u7684\u70ed\u529b\u5b66\u6570\u636e\u963b\u788d\u4e86\u5176\u53d1\u73b0\u548c\u5e94\u7528\u3002", "method": "\u7ed3\u5408\u7406\u8bba\u65b9\u6cd5\u548c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7f51\u7edc\u6269\u5c55\u5bf9\u5e94\u72b6\u6001\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u5757\u8868\u5f81\u5206\u5b50\u5fae\u89c2\u7ed3\u6784\u3002", "result": "\u6a21\u578b\u5728\u6db2\u4f53\u548c\u8d85\u4e34\u754c\u533a\u57df\u7684\u5bc6\u5ea6\u548c\u80fd\u91cf\u6027\u8d28\u9884\u6d4b\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e73\u5747\u7edd\u5bf9\u504f\u5dee\u5206\u522b\u4e3a1.49%\uff08\u6db2\u4f53\uff09\u548c2.42%\uff08\u8d85\u4e34\u754c\uff09\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u5d4c\u5165\u7269\u7406\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\uff0c\u6709\u671b\u52a0\u901f\u65b0\u578b\u6c22\u6c1f\u70ef\u70c3\u5236\u51b7\u5242\u7684\u53d1\u73b0\u3002"}}
{"id": "2507.21738", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21738", "abs": "https://arxiv.org/abs/2507.21738", "authors": ["Huiqiang Chen", "Tianqing Zhu", "Xin Yu", "Wanlei Zhou"], "title": "Zero-Shot Machine Unlearning with Proxy Adversarial Data Generation", "comment": "Accepted by IJCAI 2025", "summary": "Machine unlearning aims to remove the influence of specific samples from a\ntrained model. A key challenge in this process is over-unlearning, where the\nmodel's performance on the remaining data significantly drops due to the change\nin the model's parameters. Existing unlearning algorithms depend on the\nremaining data to prevent this issue. As such, these methods are inapplicable\nin a more practical scenario, where only the unlearning samples are available\n(i.e., zero-shot unlearning). This paper presents a novel framework, ZS-PAG, to\nfill this gap. Our approach offers three key innovations: (1) we approximate\nthe inaccessible remaining data by generating adversarial samples; (2)\nleveraging the generated samples, we pinpoint a specific subspace to perform\nthe unlearning process, therefore preventing over-unlearning in the challenging\nzero-shot scenario; and (3) we consider the influence of the unlearning process\non the remaining samples and design an influence-based pseudo-labeling\nstrategy. As a result, our method further improves the model's performance\nafter unlearning. The proposed method holds a theoretical guarantee, and\nexperiments on various benchmarks validate the effectiveness and superiority of\nour proposed method over several baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aZS-PAG\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u673a\u5668\u9057\u5fd8\u4e2d\u7684\u8fc7\u9057\u5fd8\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6837\u672c\u548c\u8bbe\u8ba1\u57fa\u4e8e\u5f71\u54cd\u7684\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u7b97\u6cd5\u4f9d\u8d56\u5269\u4f59\u6570\u636e\uff0c\u65e0\u6cd5\u9002\u7528\u4e8e\u4ec5\u63d0\u4f9b\u9057\u5fd8\u6837\u672c\u7684\u96f6\u6837\u672c\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86ZS-PAG\u6846\u67b6\uff0c\u5305\u62ec\u751f\u6210\u5bf9\u6297\u6837\u672c\u8fd1\u4f3c\u5269\u4f59\u6570\u636e\u3001\u5728\u7279\u5b9a\u5b50\u7a7a\u95f4\u8fdb\u884c\u9057\u5fd8\u4ee5\u9632\u6b62\u8fc7\u9057\u5fd8\uff0c\u4ee5\u53ca\u8bbe\u8ba1\u57fa\u4e8e\u5f71\u54cd\u7684\u4f2a\u6807\u7b7e\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u7406\u8bba\u5206\u6790\u4e5f\u652f\u6301\u5176\u53ef\u884c\u6027\u3002", "conclusion": "ZS-PAG\u5728\u96f6\u6837\u672c\u673a\u5668\u9057\u5fd8\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u89e3\u51b3\u4e86\u8fc7\u9057\u5fd8\u95ee\u9898\u5e76\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.21748", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.CE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.21748", "abs": "https://arxiv.org/abs/2507.21748", "authors": ["Simon Daubner", "Alexander E. Cohen", "Benjamin D\u00f6rich", "Samuel J. Cooper"], "title": "evoxels: A differentiable physics framework for voxel-based microstructure simulations", "comment": "9 pages, 3 figures, structure following JOSS style", "summary": "Materials science inherently spans disciplines: experimentalists use advanced\nmicroscopy to uncover micro- and nanoscale structure, while theorists and\ncomputational scientists develop models that link processing, structure, and\nproperties. Bridging these domains is essential for inverse material design\nwhere you start from desired performance and work backwards to optimal\nmicrostructures and manufacturing routes. Integrating high-resolution imaging\nwith predictive simulations and data-driven optimization accelerates discovery\nand deepens understanding of process-structure-property relationships. The\ndifferentiable physics framework evoxels is based on a fully Pythonic, unified\nvoxel-based approach that integrates segmented 3D microscopy data, physical\nsimulations, inverse modeling, and machine learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePython\u7684\u7edf\u4e00\u4f53\u7d20\u65b9\u6cd5evoxels\uff0c\u7ed3\u54083D\u663e\u5fae\u955c\u6570\u636e\u3001\u7269\u7406\u6a21\u62df\u3001\u9006\u5efa\u6a21\u548c\u673a\u5668\u5b66\u4e60\uff0c\u4ee5\u52a0\u901f\u6750\u6599\u79d1\u5b66\u4e2d\u7684\u9006\u5411\u8bbe\u8ba1\u3002", "motivation": "\u6750\u6599\u79d1\u5b66\u9700\u8981\u8de8\u5b66\u79d1\u6574\u5408\uff0c\u5c24\u5176\u662f\u5b9e\u9a8c\u4e0e\u7406\u8bba\u7684\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u4ece\u6027\u80fd\u9700\u6c42\u51fa\u53d1\u7684\u9006\u5411\u6750\u6599\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u53ef\u5fae\u5206\u7269\u7406\u6846\u67b6evoxels\uff0c\u7edf\u4e00\u5904\u74063D\u663e\u5fae\u955c\u6570\u636e\u3001\u7269\u7406\u6a21\u62df\u548c\u673a\u5668\u5b66\u4e60\u4f18\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u94fe\u63a5\u6750\u6599\u52a0\u5de5\u3001\u7ed3\u6784\u4e0e\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u52a0\u901f\u53d1\u73b0\u8fc7\u7a0b\u3002", "conclusion": "evoxels\u6846\u67b6\u4e3a\u6750\u6599\u79d1\u5b66\u7684\u9006\u5411\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u4fc3\u8fdb\u4e86\u8de8\u5b66\u79d1\u6574\u5408\u3002"}}
{"id": "2507.21762", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21762", "abs": "https://arxiv.org/abs/2507.21762", "authors": ["Nguyen Xuan-Vu", "Daniel Armstrong", "Zlatko Joncev", "Philippe Schwaller"], "title": "TempRe: Template generation for single and direct multi-step retrosynthesis", "comment": null, "summary": "Retrosynthesis planning remains a central challenge in molecular discovery\ndue to the vast and complex chemical reaction space. While traditional\ntemplate-based methods offer tractability, they suffer from poor scalability\nand limited generalization, and template-free generative approaches risk\ngenerating invalid reactions. In this work, we propose TempRe, a generative\nframework that reformulates template-based approaches as sequence generation,\nenabling scalable, flexible, and chemically plausible retrosynthesis. We\nevaluated TempRe across single-step and multi-step retrosynthesis tasks,\ndemonstrating its superiority over both template classification and\nSMILES-based generation methods. On the PaRoutes multi-step benchmark, TempRe\nachieves strong top-k route accuracy. Furthermore, we extend TempRe to direct\nmulti-step synthesis route generation, providing a lightweight and efficient\nalternative to conventional single-step and search-based approaches. These\nresults highlight the potential of template generative modeling as a powerful\nparadigm in computer-aided synthesis planning.", "AI": {"tldr": "TempRe\u662f\u4e00\u79cd\u751f\u6210\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u6a21\u677f\u7684\u65b9\u6cd5\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5e8f\u5217\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u7075\u6d3b\u4e14\u5316\u5b66\u5408\u7406\u7684\u9006\u5408\u6210\u5206\u6790\u3002", "motivation": "\u9006\u5408\u6210\u89c4\u5212\u5728\u5206\u5b50\u53d1\u73b0\u4e2d\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u800c\u6a21\u677f\u65e0\u5173\u7684\u751f\u6210\u65b9\u6cd5\u53ef\u80fd\u4ea7\u751f\u65e0\u6548\u53cd\u5e94\u3002", "method": "\u63d0\u51faTempRe\u6846\u67b6\uff0c\u5c06\u57fa\u4e8e\u6a21\u677f\u7684\u65b9\u6cd5\u8f6c\u5316\u4e3a\u5e8f\u5217\u751f\u6210\uff0c\u652f\u6301\u5355\u6b65\u548c\u591a\u6b65\u9006\u5408\u6210\u4efb\u52a1\u3002", "result": "\u5728PaRoutes\u591a\u6b65\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTempRe\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u6a21\u677f\u5206\u7c7b\u548cSMILES\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "TempRe\u5c55\u793a\u4e86\u6a21\u677f\u751f\u6210\u5efa\u6a21\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.21799", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21799", "abs": "https://arxiv.org/abs/2507.21799", "authors": ["Xie Zhang", "Yina Wang", "Chenshu Wu"], "title": "Unlocking Interpretability for RF Sensing: A Complex-Valued White-Box Transformer", "comment": null, "summary": "The empirical success of deep learning has spurred its application to the\nradio-frequency (RF) domain, leading to significant advances in Deep Wireless\nSensing (DWS). However, most existing DWS models function as black boxes with\nlimited interpretability, which hampers their generalizability and raises\nconcerns in security-sensitive physical applications. In this work, inspired by\nthe remarkable advances of white-box transformers, we present RF-CRATE, the\nfirst mathematically interpretable deep network architecture for RF sensing,\ngrounded in the principles of complex sparse rate reduction. To accommodate the\nunique RF signals, we conduct non-trivial theoretical derivations that extend\nthe original real-valued white-box transformer to the complex domain. By\nleveraging the CR-Calculus framework, we successfully construct a fully\ncomplex-valued white-box transformer with theoretically derived self-attention\nand residual multi-layer perceptron modules. Furthermore, to improve the\nmodel's ability to extract discriminative features from limited wireless data,\nwe introduce Subspace Regularization, a novel regularization strategy that\nenhances feature diversity, resulting in an average performance improvement of\n19.98% across multiple sensing tasks. We extensively evaluate RF-CRATE against\nseven baselines with multiple public and self-collected datasets involving\ndifferent RF signals. The results show that RF-CRATE achieves performance on\npar with thoroughly engineered black-box models, while offering full\nmathematical interpretability. More importantly, by extending CRATE to the\ncomplex domain, RF-CRATE yields substantial improvements, achieving an average\nclassification gain of 5.08% and reducing regression error by 10.34% across\ndiverse sensing tasks compared to CRATE. RF-CRATE is fully open-sourced at:\nhttps://github.com/rfcrate/RF_CRATE.", "AI": {"tldr": "RF-CRATE\u662f\u4e00\u79cd\u6570\u5b66\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8eRF\u4f20\u611f\uff0c\u57fa\u4e8e\u590d\u6742\u7a00\u758f\u7387\u964d\u4f4e\u539f\u7406\uff0c\u6027\u80fd\u4e0e\u9ed1\u76d2\u6a21\u578b\u76f8\u5f53\uff0c\u540c\u65f6\u63d0\u4f9b\u5b8c\u5168\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u65e0\u7ebf\u4f20\u611f\uff08DWS\uff09\u6a21\u578b\u591a\u4e3a\u9ed1\u76d2\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u5e76\u5f15\u53d1\u5b89\u5168\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u975e\u5e73\u51e1\u7684\u7406\u8bba\u63a8\u5bfc\uff0c\u5c06\u5b9e\u503c\u767d\u76d2\u53d8\u6362\u5668\u6269\u5c55\u5230\u590d\u6570\u57df\uff0c\u6784\u5efa\u5b8c\u5168\u590d\u6570\u57df\u7684\u767d\u76d2\u53d8\u6362\u5668\uff0c\u5e76\u5f15\u5165\u5b50\u7a7a\u95f4\u6b63\u5219\u5316\u589e\u5f3a\u7279\u5f81\u591a\u6837\u6027\u3002", "result": "RF-CRATE\u5728\u591a\u4e2a\u4f20\u611f\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u534719.98%\uff0c\u5206\u7c7b\u589e\u76ca5.08%\uff0c\u56de\u5f52\u8bef\u5dee\u964d\u4f4e10.34%\u3002", "conclusion": "RF-CRATE\u4e0d\u4ec5\u6027\u80fd\u4f18\u5f02\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6570\u5b66\u53ef\u89e3\u91ca\u6027\uff0c\u4e3aRF\u4f20\u611f\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21803", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21803", "abs": "https://arxiv.org/abs/2507.21803", "authors": ["Sofianos Panagiotis Fotias", "Vassilis Gaganis"], "title": "Bayesian Neural Network Surrogates for Bayesian Optimization of Carbon Capture and Storage Operations", "comment": null, "summary": "Carbon Capture and Storage (CCS) stands as a pivotal technology for fostering\na sustainable future. The process, which involves injecting supercritical\nCO$_2$ into underground formations, a method already widely used for Enhanced\nOil Recovery, serves a dual purpose: it not only curbs CO$_2$ emissions and\naddresses climate change but also extends the operational lifespan and\nsustainability of oil fields and platforms, easing the shift toward greener\npractices. This paper delivers a thorough comparative evaluation of strategies\nfor optimizing decision variables in CCS project development, employing a\nderivative-free technique known as Bayesian Optimization. In addition to\nGaussian Processes, which usually serve as the gold standard in BO, various\nnovel stochastic models were examined and compared within a BO framework. This\nresearch investigates the effectiveness of utilizing more exotic stochastic\nmodels than GPs for BO in environments where GPs have been shown to\nunderperform, such as in cases with a large number of decision variables or\nmultiple objective functions that are not similarly scaled. By incorporating\nNet Present Value (NPV) as a key objective function, the proposed framework\ndemonstrates its potential to improve economic viability while ensuring the\nsustainable deployment of CCS technologies. Ultimately, this study represents\nthe first application in the reservoir engineering industry of the growing body\nof BO research, specifically in the search for more appropriate stochastic\nmodels, highlighting its potential as a preferred method for enhancing\nsustainability in the energy sector.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u6bd4\u8f83\u4e86\u78b3\u6355\u83b7\u4e0e\u5c01\u5b58\uff08CCS\uff09\u9879\u76ee\u4e2d\u7684\u51b3\u7b56\u53d8\u91cf\u4f18\u5316\u7b56\u7565\uff0c\u63a2\u8ba8\u4e86\u9ad8\u65af\u8fc7\u7a0b\u4e4b\u5916\u7684\u968f\u673a\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u9ad8\u7ecf\u6d4e\u53ef\u884c\u6027\u548c\u53ef\u6301\u7eed\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u78b3\u6355\u83b7\u4e0e\u5c01\u5b58\uff08CCS\uff09\u662f\u5b9e\u73b0\u53ef\u6301\u7eed\u53d1\u5c55\u7684\u91cd\u8981\u6280\u672f\uff0c\u4f46\u5176\u4f18\u5316\u51b3\u7b56\u53d8\u91cf\u7684\u65b9\u6cd5\u9700\u8981\u6539\u8fdb\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u65af\u8fc7\u7a0b\u8868\u73b0\u4e0d\u4f73\u7684\u590d\u6742\u73af\u5883\u4e2d\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u9ad8\u65af\u8fc7\u7a0b\u4e0e\u5176\u4ed6\u65b0\u578b\u968f\u673a\u6a21\u578b\u5728CCS\u9879\u76ee\u4f18\u5316\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u5173\u6ce8\u51b3\u7b56\u53d8\u91cf\u591a\u6216\u76ee\u6807\u51fd\u6570\u5c3a\u5ea6\u4e0d\u540c\u7684\u60c5\u51b5\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u4f7f\u7528\u66f4\u9ad8\u7ea7\u7684\u968f\u673a\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u4f18\u5316\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u51c0\u73b0\u503c\uff08NPV\uff09\u8bc1\u660e\u4e86\u5176\u7ecf\u6d4e\u53ef\u884c\u6027\u3002", "conclusion": "\u8d1d\u53f6\u65af\u4f18\u5316\u53ca\u5176\u65b0\u578b\u968f\u673a\u6a21\u578b\u5728CCS\u9879\u76ee\u4e2d\u7684\u5e94\u7528\u5c55\u793a\u4e86\u5176\u5728\u80fd\u6e90\u884c\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.21833", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21833", "abs": "https://arxiv.org/abs/2507.21833", "authors": ["Taeyoung Kim"], "title": "Analysis of Fourier Neural Operators via Effective Field Theory", "comment": "37 pages, 10 figures", "summary": "Fourier Neural Operators (FNOs) have emerged as leading surrogates for\nhigh-dimensional partial-differential equations, yet their stability,\ngeneralization and frequency behavior lack a principled explanation. We present\nthe first systematic effective-field-theory analysis of FNOs in an\ninfinite-dimensional function space, deriving closed recursion relations for\nthe layer kernel and four-point vertex and then examining three practically\nimportant settings-analytic activations, scale-invariant cases and\narchitectures with residual connections. The theory shows that nonlinear\nactivations inevitably couple frequency inputs to high-frequency modes that are\notherwise discarded by spectral truncation, and experiments confirm this\nfrequency transfer. For wide networks we obtain explicit criticality conditions\non the weight-initialization ensemble that keep small input perturbations to\nhave uniform scale across depth, and empirical tests validate these\npredictions. Taken together, our results quantify how nonlinearity enables\nneural operators to capture non-trivial features, supply criteria for\nhyper-parameter selection via criticality analysis, and explain why\nscale-invariant activations and residual connections enhance feature learning\nin FNOs.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6709\u6548\u573a\u8bba\u5206\u6790Fourier\u795e\u7ecf\u7b97\u5b50\uff08FNOs\uff09\uff0c\u63ed\u793a\u4e86\u5176\u975e\u7ebf\u6027\u6fc0\u6d3b\u5982\u4f55\u8026\u5408\u9891\u7387\u8f93\u5165\u5230\u9ad8\u9891\u6a21\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u6743\u91cd\u521d\u59cb\u5316\u7684\u4e34\u754c\u6761\u4ef6\u3002", "motivation": "\u7814\u7a76FNOs\u7684\u7a33\u5b9a\u6027\u3001\u6cdb\u5316\u6027\u548c\u9891\u7387\u884c\u4e3a\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\uff0c\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u7684\u6709\u6548\u573a\u8bba\u5206\u6790\uff0c\u63a8\u5bfc\u5c42\u6838\u548c\u56db\u70b9\u9876\u70b9\u7684\u95ed\u5f0f\u9012\u63a8\u5173\u7cfb\uff0c\u5e76\u7814\u7a76\u4e09\u79cd\u5b9e\u9645\u573a\u666f\u3002", "result": "\u7406\u8bba\u8868\u660e\u975e\u7ebf\u6027\u6fc0\u6d3b\u4f1a\u8026\u5408\u9891\u7387\u8f93\u5165\u5230\u9ad8\u9891\u6a21\u5f0f\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u9891\u7387\u8f6c\u79fb\u73b0\u8c61\uff1b\u540c\u65f6\u63d0\u51fa\u4e86\u6743\u91cd\u521d\u59cb\u5316\u7684\u4e34\u754c\u6761\u4ef6\u3002", "conclusion": "\u975e\u7ebf\u6027\u6fc0\u6d3b\u4f7fFNOs\u80fd\u6355\u6349\u975e\u5e73\u51e1\u7279\u5f81\uff0c\u4e34\u754c\u5206\u6790\u4e3a\u8d85\u53c2\u6570\u9009\u62e9\u63d0\u4f9b\u4f9d\u636e\uff0c\u89e3\u91ca\u4e86\u5c3a\u5ea6\u4e0d\u53d8\u6fc0\u6d3b\u548c\u6b8b\u5dee\u8fde\u63a5\u5bf9\u7279\u5f81\u5b66\u4e60\u7684\u589e\u5f3a\u4f5c\u7528\u3002"}}
{"id": "2507.21841", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.21841", "abs": "https://arxiv.org/abs/2507.21841", "authors": ["Rahul Golder", "M. M. Faruque Hasan"], "title": "Discovering Interpretable Ordinary Differential Equations from Noisy Data", "comment": "20 pages, 11 figures, 7 tables", "summary": "The data-driven discovery of interpretable models approximating the\nunderlying dynamics of a physical system has gained attraction in the past\ndecade. Current approaches employ pre-specified functional forms or basis\nfunctions and often result in models that lack physical meaning and\ninterpretability, let alone represent the true physics of the system. We\npropose an unsupervised parameter estimation methodology that first finds an\napproximate general solution, followed by a spline transformation to linearly\nestimate the coefficients of the governing ordinary differential equation\n(ODE). The approximate general solution is postulated using the same functional\nform as the analytical solution of a general homogeneous, linear,\nconstant-coefficient ODE. An added advantage is its ability to produce a\nhigh-fidelity, smooth functional form even in the presence of noisy data. The\nspline approximation obtains gradient information from the functional form\nwhich are linearly independent and creates the basis of the gradient matrix.\nThis gradient matrix is used in a linear system to find the coefficients of the\nODEs. From the case studies, we observed that our modeling approach discovers\nODEs with high accuracy and also promotes sparsity in the solution without\nusing any regularization techniques. The methodology is also robust to noisy\ndata and thus allows the integration of data-driven techniques into real\nexperimental setting for data-driven learning of physical phenomena.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u76d1\u7763\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fd1\u4f3c\u89e3\u548c\u6837\u6761\u53d8\u6362\u7ebf\u6027\u4f30\u8ba1ODE\u7cfb\u6570\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u7a00\u758f\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7269\u7406\u610f\u4e49\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u7269\u7406\u7cfb\u7edf\u3002", "method": "\u5148\u627e\u5230\u8fd1\u4f3c\u901a\u89e3\uff0c\u518d\u901a\u8fc7\u6837\u6761\u53d8\u6362\u7ebf\u6027\u4f30\u8ba1ODE\u7cfb\u6570\u3002", "result": "\u65b9\u6cd5\u80fd\u9ad8\u7cbe\u5ea6\u53d1\u73b0ODE\uff0c\u4e14\u5bf9\u566a\u58f0\u6570\u636e\u9c81\u68d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u5408\u5c06\u6570\u636e\u9a71\u52a8\u6280\u672f\u5e94\u7528\u4e8e\u771f\u5b9e\u5b9e\u9a8c\u73af\u5883\u3002"}}
{"id": "2507.21898", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21898", "abs": "https://arxiv.org/abs/2507.21898", "authors": ["Risshab Srinivas Ramesh", "Roshani T S Udupa", "Monisha J", "Kushi K K S"], "title": "Cardiovascular Disease Prediction using Machine Learning: A Comparative Analysis", "comment": null, "summary": "Cardiovascular diseases (CVDs) are a main cause of mortality globally,\naccounting for 31% of all deaths. This study involves a cardiovascular disease\n(CVD) dataset comprising 68,119 records to explore the influence of numerical\n(age, height, weight, blood pressure, BMI) and categorical gender, cholesterol,\nglucose, smoking, alcohol, activity) factors on CVD occurrence. We have\nperformed statistical analyses, including t-tests, Chi-square tests, and ANOVA,\nto identify strong associations between CVD and elderly people, hypertension,\nhigher weight, and abnormal cholesterol levels, while physical activity (a\nprotective factor). A logistic regression model highlights age, blood pressure,\nand cholesterol as primary risk factors, with unexpected negative associations\nfor smoking and alcohol, suggesting potential data issues. Model performance\ncomparisons reveal CatBoost as the top performer with an accuracy of 0.734 and\nan ECE of 0.0064 and excels in probabilistic prediction (Brier score = 0.1824).\nData challenges, including outliers and skewed distributions, indicate a need\nfor improved preprocessing to enhance predictive reliability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u7edf\u8ba1\u5206\u679068,119\u6761\u5fc3\u8840\u7ba1\u75be\u75c5\uff08CVD\uff09\u6570\u636e\uff0c\u53d1\u73b0\u5e74\u9f84\u3001\u8840\u538b\u548c\u80c6\u56fa\u9187\u662f\u4e3b\u8981\u98ce\u9669\u56e0\u7d20\uff0c\u800c\u4f53\u80b2\u6d3b\u52a8\u662f\u4fdd\u62a4\u56e0\u7d20\u3002CatBoost\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6570\u636e\u9884\u5904\u7406\u9700\u6539\u8fdb\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6570\u503c\u548c\u5206\u7c7b\u56e0\u7d20\u5bf9CVD\u53d1\u751f\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528t\u68c0\u9a8c\u3001\u5361\u65b9\u68c0\u9a8c\u3001ANOVA\u7b49\u7edf\u8ba1\u65b9\u6cd5\u5206\u6790\u6570\u636e\uff0c\u5e76\u6784\u5efa\u903b\u8f91\u56de\u5f52\u6a21\u578b\u548cCatBoost\u6a21\u578b\u3002", "result": "\u5e74\u9f84\u3001\u9ad8\u8840\u538b\u3001\u9ad8\u4f53\u91cd\u548c\u5f02\u5e38\u80c6\u56fa\u9187\u4e0eCVD\u5f3a\u76f8\u5173\uff0cCatBoost\u6a21\u578b\u51c6\u786e\u7387\u4e3a0.734\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6570\u636e\u9884\u5904\u7406\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51faCatBoost\u5728\u9884\u6d4b\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.21938", "categories": ["cs.LG", "q-bio.BM", "I.2.6; J.3"], "pdf": "https://arxiv.org/pdf/2507.21938", "abs": "https://arxiv.org/abs/2507.21938", "authors": ["Alex Abrudan", "Sebastian Pujalte Ojeda", "Chaitanya K. Joshi", "Matthew Greenig", "Felipe Engelberger", "Alena Khmelinskaia", "Jens Meiler", "Michele Vendruscolo", "Tuomas P. J. Knowles"], "title": "Multi-state Protein Design with DynamicMPNN", "comment": "ICML 2025 GenBio Workshop", "summary": "Structural biology has long been dominated by the one sequence, one\nstructure, one function paradigm, yet many critical biological processes - from\nenzyme catalysis to membrane transport - depend on proteins that adopt multiple\nconformational states. Existing multi-state design approaches rely on post-hoc\naggregation of single-state predictions, achieving poor experimental success\nrates compared to single-state design. We introduce DynamicMPNN, an inverse\nfolding model explicitly trained to generate sequences compatible with multiple\nconformations through joint learning across conformational ensembles. Trained\non 46,033 conformational pairs covering 75% of CATH superfamilies and evaluated\nusing AlphaFold initial guess, DynamicMPNN outperforms ProteinMPNN by up to 13%\non structure-normalized RMSD across our challenging multi-state protein\nbenchmark.", "AI": {"tldr": "DynamicMPNN\u662f\u4e00\u79cd\u9006\u5411\u6298\u53e0\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u591a\u6784\u8c61\u96c6\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u72b6\u6001\u86cb\u767d\u8d28\u8bbe\u8ba1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5355\u72b6\u6001\u9884\u6d4b\u7684\u805a\u5408\uff0c\u5b9e\u9a8c\u6210\u529f\u7387\u4f4e\uff0c\u800c\u591a\u6784\u8c61\u86cb\u767d\u8d28\u5728\u751f\u7269\u8fc7\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "DynamicMPNN\u901a\u8fc7\u8054\u5408\u5b66\u4e6046,033\u4e2a\u6784\u8c61\u5bf9\uff0c\u8986\u76d675%\u7684CATH\u8d85\u5bb6\u65cf\uff0c\u5229\u7528AlphaFold\u521d\u59cb\u731c\u6d4b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "DynamicMPNN\u5728\u7ed3\u6784\u6807\u51c6\u5316RMSD\u4e0a\u6bd4ProteinMPNN\u63d0\u5347\u9ad8\u8fbe13%\u3002", "conclusion": "DynamicMPNN\u4e3a\u591a\u6784\u8c61\u86cb\u767d\u8d28\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.21963", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21963", "abs": "https://arxiv.org/abs/2507.21963", "authors": ["Siana Rizwan", "Tasnim Ahmed", "Salimur Choudhury"], "title": "SLA-Centric Automated Algorithm Selection Framework for Cloud Environments", "comment": null, "summary": "Cloud computing offers on-demand resource access, regulated by Service-Level\nAgreements (SLAs) between consumers and Cloud Service Providers (CSPs). SLA\nviolations can impact efficiency and CSP profitability. In this work, we\npropose an SLA-aware automated algorithm-selection framework for combinatorial\noptimization problems in resource-constrained cloud environments. The framework\nuses an ensemble of machine learning models to predict performance and rank\nalgorithm-hardware pairs based on SLA constraints. We also apply our framework\nto the 0-1 knapsack problem. We curate a dataset comprising instance specific\nfeatures along with memory usage, runtime, and optimality gap for 6 algorithms.\nAs an empirical benchmark, we evaluate the framework on both classification and\nregression tasks. Our ablation study explores the impact of hyperparameters,\nlearning approaches, and large language models effectiveness in regression, and\nSHAP-based interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSLA\u7684\u81ea\u52a8\u5316\u7b97\u6cd5\u9009\u62e9\u6846\u67b6\uff0c\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u4e91\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "SLA\u8fdd\u89c4\u4f1a\u5f71\u54cd\u4e91\u670d\u52a1\u6548\u7387\u4e0e\u63d0\u4f9b\u5546\u5229\u6da6\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u65b9\u6cd5\u6765\u4f18\u5316\u7b97\u6cd5\u9009\u62e9\u4ee5\u6ee1\u8db3SLA\u7ea6\u675f\u3002", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u96c6\u6210\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u57fa\u4e8eSLA\u7ea6\u675f\u5bf9\u7b97\u6cd5-\u786c\u4ef6\u5bf9\u8fdb\u884c\u6392\u5e8f\uff1b\u5e94\u7528\u4e8e0-1\u80cc\u5305\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u7814\u7a76\u4e86\u8d85\u53c2\u6570\u3001\u5b66\u4e60\u65b9\u6cd5\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u56de\u5f52\u4e2d\u7684\u6548\u679c\u3002", "conclusion": "\u6846\u67b6\u80fd\u6709\u6548\u4f18\u5316\u7b97\u6cd5\u9009\u62e9\uff0c\u6ee1\u8db3SLA\u7ea6\u675f\uff0c\u63d0\u5347\u4e91\u670d\u52a1\u6548\u7387\u3002"}}
{"id": "2507.21983", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.21983", "abs": "https://arxiv.org/abs/2507.21983", "authors": ["Daniel R. Jiang", "Alex Nikulkov", "Yu-Chia Chen", "Yang Bai", "Zheqing Zhu"], "title": "Improving Generative Ad Text on Facebook using Reinforcement Learning", "comment": "D.J. and A.N. contributed equally, 41 pages, 6 figures", "summary": "Generative artificial intelligence (AI), in particular large language models\n(LLMs), is poised to drive transformative economic change. LLMs are pre-trained\non vast text data to learn general language patterns, but a subsequent\npost-training phase is critical to align them for specific real-world tasks.\nReinforcement learning (RL) is the leading post-training technique, yet its\neconomic impact remains largely underexplored and unquantified. We examine this\nquestion through the lens of the first deployment of an RL-trained LLM for\ngenerative advertising on Facebook. Integrated into Meta's Text Generation\nfeature, our model, \"AdLlama,\" powers an AI tool that helps advertisers create\nnew variations of human-written ad text. To train this model, we introduce\nreinforcement learning with performance feedback (RLPF), a post-training method\nthat uses historical ad performance data as a reward signal. In a large-scale\n10-week A/B test on Facebook spanning nearly 35,000 advertisers and 640,000 ad\nvariations, we find that AdLlama improves click-through rates by 6.7%\n(p=0.0296) compared to a supervised imitation model trained on curated ads.\nThis represents a substantial improvement in advertiser return on investment on\nFacebook. We also find that advertisers who used AdLlama generated more ad\nvariations, indicating higher satisfaction with the model's outputs. To our\nknowledge, this is the largest study to date on the use of generative AI in an\necologically valid setting, offering an important data point quantifying the\ntangible impact of RL post-training. Furthermore, the results show that RLPF is\na promising and generalizable approach for metric-driven post-training that\nbridges the gap between highly capable language models and tangible outcomes.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u540e\u8bad\u7ec3\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7ecf\u6d4e\u5f71\u54cd\uff0c\u901a\u8fc7Meta\u7684\u5e7f\u544a\u751f\u6210\u5de5\u5177AdLlama\u5c55\u793a\u4e86RLPF\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e7f\u544a\u70b9\u51fb\u7387\u3002", "motivation": "\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u5bf9LLMs\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u7ecf\u6d4e\u5f71\u54cd\uff0c\u586b\u8865\u8fd9\u4e00\u9886\u57df\u7684\u91cf\u5316\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51faRLPF\uff08\u57fa\u4e8e\u6027\u80fd\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u5386\u53f2\u5e7f\u544a\u6570\u636e\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u8bad\u7ec3AdLlama\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u5927\u89c4\u6a21A/B\u6d4b\u8bd5\u3002", "result": "AdLlama\u5c06\u5e7f\u544a\u70b9\u51fb\u7387\u63d0\u53476.7%\uff0c\u5e7f\u544a\u4e3b\u6ee1\u610f\u5ea6\u63d0\u9ad8\uff0c\u751f\u6210\u66f4\u591a\u5e7f\u544a\u53d8\u4f53\u3002", "conclusion": "RLPF\u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u901a\u7528\u65b9\u6cd5\uff0c\u80fd\u591f\u5c06\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u8f6c\u5316\u4e3a\u5b9e\u9645\u6210\u679c\uff0c\u4e3a\u751f\u6210\u5f0fAI\u7684\u7ecf\u6d4e\u5f71\u54cd\u63d0\u4f9b\u4e86\u91cd\u8981\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2507.21992", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21992", "abs": "https://arxiv.org/abs/2507.21992", "authors": ["Siddhartha Pradhan", "Shikshya Shiwakoti", "Neha Bathuri"], "title": "Teach Me to Trick: Exploring Adversarial Transferability via Knowledge Distillation", "comment": "10 pages, 4 figures", "summary": "We investigate whether knowledge distillation (KD) from multiple\nheterogeneous teacher models can enhance the generation of transferable\nadversarial examples. A lightweight student model is trained using two KD\nstrategies: curriculum-based switching and joint optimization, with ResNet50\nand DenseNet-161 as teachers. The trained student is then used to generate\nadversarial examples using FG, FGS, and PGD attacks, which are evaluated\nagainst a black-box target model (GoogLeNet). Our results show that student\nmodels distilled from multiple teachers achieve attack success rates comparable\nto ensemble-based baselines, while reducing adversarial example generation time\nby up to a factor of six. An ablation study further reveals that lower\ntemperature settings and the inclusion of hard-label supervision significantly\nenhance transferability. These findings suggest that KD can serve not only as a\nmodel compression technique but also as a powerful tool for improving the\nefficiency and effectiveness of black-box adversarial attacks.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u63d0\u5347\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5b66\u751f\u6a21\u578b\u5728\u653b\u51fb\u6210\u529f\u7387\u548c\u6548\u7387\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a2\u7d22\u77e5\u8bc6\u84b8\u998f\u662f\u5426\u53ef\u4ee5\u4ece\u591a\u4e2a\u5f02\u6784\u6559\u5e08\u6a21\u578b\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff0c\u4ee5\u63d0\u5347\u5bf9\u6297\u6837\u672c\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "method": "\u4f7f\u7528ResNet50\u548cDenseNet-161\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5207\u6362\u548c\u8054\u5408\u4f18\u5316\u7b56\u7565\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u751f\u6210\u5bf9\u6297\u6837\u672c\u5e76\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u5b66\u751f\u6a21\u578b\u7684\u653b\u51fb\u6210\u529f\u7387\u4e0e\u96c6\u6210\u65b9\u6cd5\u76f8\u5f53\uff0c\u751f\u6210\u65f6\u95f4\u51cf\u5c11\u516d\u500d\uff0c\u4f4e\u6e29\u548c\u786c\u6807\u7b7e\u76d1\u7763\u663e\u8457\u63d0\u5347\u53ef\u8fc1\u79fb\u6027\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\u4e0d\u4ec5\u80fd\u538b\u7f29\u6a21\u578b\uff0c\u8fd8\u80fd\u9ad8\u6548\u63d0\u5347\u9ed1\u76d2\u5bf9\u6297\u653b\u51fb\u7684\u6548\u679c\u3002"}}
{"id": "2507.22032", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22032", "abs": "https://arxiv.org/abs/2507.22032", "authors": ["Mokhtar Al-Awadhi", "Ratnadeep Deshmukh"], "title": "Classification of Honey Botanical and Geographical Sources using Mineral Profiles and Machine Learning", "comment": "13 pages, 7 figures, conference paper", "summary": "This paper proposes a machine learning-based approach for identifying honey\nfloral and geographical sources using mineral element profiles. The proposed\nmethod comprises two steps: preprocessing and classification. The preprocessing\nphase involves missing-value treatment and data normalization. In the\nclassification phase, we employ various supervised classification models for\ndiscriminating between six botanical sources and 13 geographical origins of\nhoney. We test the classifiers' performance on a publicly available honey\nmineral element dataset. The dataset contains mineral element profiles of\nhoneys from various floral and geographical origins. Results show that mineral\nelement content in honey provides discriminative information useful for\nclassifying honey botanical and geographical sources. Results also show that\nthe Random Forests (RF) classifier obtains the best performance on this\ndataset, achieving a cross-validation accuracy of 99.30% for classifying honey\nbotanical origins and 98.01% for classifying honey geographical origins.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u8702\u871c\u82b1\u6e90\u548c\u5730\u7406\u6765\u6e90\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u77ff\u7269\u8d28\u5143\u7d20\u8c31\u5b9e\u73b0\u5206\u7c7b\u3002", "motivation": "\u5229\u7528\u8702\u871c\u4e2d\u7684\u77ff\u7269\u8d28\u5143\u7d20\u8c31\u4fe1\u606f\uff0c\u533a\u5206\u4e0d\u540c\u82b1\u6e90\u548c\u5730\u7406\u6765\u6e90\uff0c\u4e3a\u8702\u871c\u8d28\u91cf\u63a7\u5236\u548c\u6eaf\u6e90\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002", "method": "\u65b9\u6cd5\u5206\u4e3a\u9884\u5904\u7406\uff08\u7f3a\u5931\u503c\u5904\u7406\u548c\u6570\u636e\u6807\u51c6\u5316\uff09\u548c\u5206\u7c7b\uff08\u591a\u79cd\u76d1\u7763\u5206\u7c7b\u6a21\u578b\uff09\u4e24\u6b65\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u77ff\u7269\u8d28\u5143\u7d20\u8c31\u5bf9\u8702\u871c\u5206\u7c7b\u6709\u6548\uff0c\u968f\u673a\u68ee\u6797\uff08RF\uff09\u5206\u7c7b\u5668\u8868\u73b0\u6700\u4f73\uff0c\u82b1\u6e90\u548c\u5730\u7406\u6765\u6e90\u5206\u7c7b\u51c6\u786e\u7387\u5206\u522b\u4e3a99.30%\u548c98.01%\u3002", "conclusion": "\u77ff\u7269\u8d28\u5143\u7d20\u8c31\u53ef\u7528\u4e8e\u8702\u871c\u82b1\u6e90\u548c\u5730\u7406\u6765\u6e90\u7684\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0cRF\u5206\u7c7b\u5668\u5728\u6b64\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\u3002"}}
{"id": "2507.22040", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.22040", "abs": "https://arxiv.org/abs/2507.22040", "authors": ["Alvaro Maggiar", "Sohrab Andaz", "Akhil Bagaria", "Carson Eisenach", "Dean Foster", "Omer Gottesman", "Dominique Perrault-Joncas"], "title": "Structure-Informed Deep Reinforcement Learning for Inventory Management", "comment": null, "summary": "This paper investigates the application of Deep Reinforcement Learning (DRL)\nto classical inventory management problems, with a focus on practical\nimplementation considerations. We apply a DRL algorithm based on DirectBackprop\nto several fundamental inventory management scenarios including multi-period\nsystems with lost sales (with and without lead times), perishable inventory\nmanagement, dual sourcing, and joint inventory procurement and removal. The DRL\napproach learns policies across products using only historical information that\nwould be available in practice, avoiding unrealistic assumptions about demand\ndistributions or access to distribution parameters. We demonstrate that our\ngeneric DRL implementation performs competitively against or outperforms\nestablished benchmarks and heuristics across these diverse settings, while\nrequiring minimal parameter tuning. Through examination of the learned\npolicies, we show that the DRL approach naturally captures many known\nstructural properties of optimal policies derived from traditional operations\nresearch methods. To further improve policy performance and interpretability,\nwe propose a Structure-Informed Policy Network technique that explicitly\nincorporates analytically-derived characteristics of optimal policies into the\nlearning process. This approach can help interpretability and add robustness to\nthe policy in out-of-sample performance, as we demonstrate in an example with\nrealistic demand data. Finally, we provide an illustrative application of DRL\nin a non-stationary setting. Our work bridges the gap between data-driven\nlearning and analytical insights in inventory management while maintaining\npractical applicability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u7ecf\u5178\u5e93\u5b58\u7ba1\u7406\u95ee\u9898\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u5b9e\u9645\u5b9e\u65bd\u8003\u8651\u3002\u901a\u8fc7DirectBackprop\u7b97\u6cd5\uff0c\u8bba\u6587\u5728\u591a\u79cd\u5e93\u5b58\u7ba1\u7406\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86DRL\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e86Structure-Informed Policy Network\u6280\u672f\u4ee5\u63d0\u5347\u7b56\u7565\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u5e93\u5b58\u7ba1\u7406\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5bf9\u9700\u6c42\u5206\u5e03\u6216\u53c2\u6570\u7684\u5047\u8bbe\uff0c\u800cDRL\u4ec5\u9700\u5386\u53f2\u6570\u636e\u5373\u53ef\u5b66\u4e60\u7b56\u7565\uff0c\u907f\u514d\u4e86\u4e0d\u73b0\u5b9e\u7684\u5047\u8bbe\u3002", "method": "\u91c7\u7528\u57fa\u4e8eDirectBackprop\u7684DRL\u7b97\u6cd5\uff0c\u5e94\u7528\u4e8e\u591a\u5468\u671f\u7cfb\u7edf\u3001\u6613\u8150\u5e93\u5b58\u7ba1\u7406\u7b49\u573a\u666f\uff0c\u5e76\u63d0\u51faStructure-Informed Policy Network\u6280\u672f\u3002", "result": "DRL\u5728\u591a\u79cd\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\u76f8\u5f53\uff0c\u4e14\u65e0\u9700\u5927\u91cf\u53c2\u6570\u8c03\u6574\u3002\u5b66\u4e60\u5230\u7684\u7b56\u7565\u81ea\u7136\u6355\u6349\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u6700\u4f18\u7ed3\u6784\u7279\u6027\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u548c\u5206\u6790\u6d1e\u5bdf\uff0c\u586b\u8865\u4e86\u5e93\u5b58\u7ba1\u7406\u4e2d\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u9645\u9002\u7528\u6027\u3002"}}
{"id": "2507.22045", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.22045", "abs": "https://arxiv.org/abs/2507.22045", "authors": ["Haley Rosso", "Lars Ruthotto", "Khachik Sargsyan"], "title": "Weight-Parameterization in Continuous Time Deep Neural Networks for Surrogate Modeling", "comment": "34 pages, 6 figures, submitted to the MoRE24 special issue of\n  Computational Science and Engineering", "summary": "Continuous-time deep learning models, such as neural ordinary differential\nequations (ODEs), offer a promising framework for surrogate modeling of complex\nphysical systems. A central challenge in training these models lies in learning\nexpressive yet stable time-varying weights, particularly under computational\nconstraints. This work investigates weight parameterization strategies that\nconstrain the temporal evolution of weights to a low-dimensional subspace\nspanned by polynomial basis functions. We evaluate both monomial and Legendre\npolynomial bases within neural ODE and residual network (ResNet) architectures\nunder discretize-then-optimize and optimize-then-discretize training paradigms.\nExperimental results across three high-dimensional benchmark problems show that\nLegendre parameterizations yield more stable training dynamics, reduce\ncomputational cost, and achieve accuracy comparable to or better than both\nmonomial parameterizations and unconstrained weight models. These findings\nelucidate the role of basis choice in time-dependent weight parameterization\nand demonstrate that using orthogonal polynomial bases offers a favorable\ntradeoff between model expressivity and training efficiency.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u795e\u7ecfODE\u548cResNet\u67b6\u6784\u4e2d\uff0c\u4f7f\u7528\u591a\u9879\u5f0f\u57fa\u51fd\u6570\uff08\u5982Legendre\u591a\u9879\u5f0f\uff09\u53c2\u6570\u5316\u65f6\u95f4\u4f9d\u8d56\u6743\u91cd\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0Legendre\u57fa\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u8ba1\u7b97\u6210\u672c\u548c\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u8fde\u7eed\u65f6\u95f4\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982\u795e\u7ecfODE\uff09\u5728\u8bad\u7ec3\u4e2d\u5b66\u4e60\u8868\u8fbe\u529b\u5f3a\u4e14\u7a33\u5b9a\u7684\u65f6\u95f4\u4f9d\u8d56\u6743\u91cd\u7684\u6311\u6218\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u9879\u5f0f\u57fa\u51fd\u6570\uff08\u5305\u62ec\u5355\u9879\u5f0f\u548cLegendre\u591a\u9879\u5f0f\uff09\u5728\u795e\u7ecfODE\u548cResNet\u67b6\u6784\u4e2d\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u4e86\u79bb\u6563\u4f18\u5316\u548c\u4f18\u5316\u79bb\u6563\u4e24\u79cd\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLegendre\u57fa\u5728\u8bad\u7ec3\u7a33\u5b9a\u6027\u3001\u8ba1\u7b97\u6210\u672c\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5355\u9879\u5f0f\u57fa\u548c\u65e0\u7ea6\u675f\u6743\u91cd\u6a21\u578b\u3002", "conclusion": "\u9009\u62e9\u6b63\u4ea4\u591a\u9879\u5f0f\u57fa\uff08\u5982Legendre\uff09\u5728\u6a21\u578b\u8868\u8fbe\u529b\u548c\u8bad\u7ec3\u6548\u7387\u4e4b\u95f4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2507.22053", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.22053", "abs": "https://arxiv.org/abs/2507.22053", "authors": ["Wei Yang", "Defu Cao", "Yan Liu"], "title": "Foundation Models for Demand Forecasting via Dual-Strategy Ensembling", "comment": null, "summary": "Accurate demand forecasting is critical for supply chain optimization, yet\nremains difficult in practice due to hierarchical complexity, domain shifts,\nand evolving external factors. While recent foundation models offer strong\npotential for time series forecasting, they often suffer from architectural\nrigidity and limited robustness under distributional change. In this paper, we\npropose a unified ensemble framework that enhances the performance of\nfoundation models for sales forecasting in real-world supply chains. Our method\ncombines two complementary strategies: (1) Hierarchical Ensemble (HE), which\npartitions training and inference by semantic levels (e.g., store, category,\ndepartment) to capture localized patterns; and (2) Architectural Ensemble (AE),\nwhich integrates predictions from diverse model backbones to mitigate bias and\nimprove stability. We conduct extensive experiments on the M5 benchmark and\nthree external sales datasets, covering both in-domain and zero-shot\nforecasting. Results show that our approach consistently outperforms strong\nbaselines, improves accuracy across hierarchical levels, and provides a simple\nyet effective mechanism for boosting generalization in complex forecasting\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7edf\u4e00\u7684\u96c6\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u5c42\u6b21\u96c6\u6210\u548c\u67b6\u6784\u96c6\u6210\u7b56\u7565\uff0c\u63d0\u5347\u9500\u552e\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u9700\u6c42\u9884\u6d4b\u5bf9\u4f9b\u5e94\u94fe\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5c42\u6b21\u590d\u6742\u6027\u3001\u9886\u57df\u8f6c\u79fb\u548c\u5916\u90e8\u56e0\u7d20\u53d8\u5316\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u5177\u6311\u6218\u6027\u3002\u73b0\u6709\u57fa\u7840\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u8868\u73b0\u6709\u9650\u3002", "method": "\u91c7\u7528\u5c42\u6b21\u96c6\u6210\uff08HE\uff09\u548c\u67b6\u6784\u96c6\u6210\uff08AE\uff09\u7b56\u7565\uff0c\u5206\u522b\u6355\u6349\u5c40\u90e8\u6a21\u5f0f\u548c\u96c6\u6210\u591a\u6837\u5316\u6a21\u578b\u9884\u6d4b\u4ee5\u63d0\u9ad8\u7a33\u5b9a\u6027\u3002", "result": "\u5728M5\u57fa\u51c6\u548c\u4e09\u4e2a\u5916\u90e8\u9500\u552e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u63d0\u5347\u4e86\u8de8\u5c42\u6b21\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u9884\u6d4b\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
