<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 20]
- [cs.LG](#cs.LG) [Total: 54]
- [stat.ML](#stat.ML) [Total: 1]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Dual Actor DDPG for Airborne STAR-RIS Assisted Communications](https://arxiv.org/abs/2509.13328)
*Danish Rizvi,David Boyle*

Main category: eess.SP

TL;DR: 本研究提出了一种基于无人机搭载STAR-RIS的空中智能反射表面系统，采用耦合TRC相位偏移模型，通过DA-DDPG算法联合优化无人机轨迹、波束成形和RIS配置，显著提升了多用户通信效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有STAR-RIS研究通常假设传输和反射系数独立，但实际中存在耦合关系。本研究旨在探索更真实的耦合TRC模型，并解决无人机能量约束下的多用户通信优化问题。

Method: 提出DA-DDPG算法，使用双actor网络处理高维混合动作空间；设计谐波均值指数奖励函数保证用户公平性；联合优化无人机三维轨迹、基站波束成形和被动RIS TRC配置。

Result: DA-DDPG算法比传统DDPG和DQN分别提升24%和97%的累积奖励；三维轨迹优化比二维优化提升28%通信效率；HFI奖励函数降低41%QoS拒绝率；耦合相位STAR-RIS优于传统RIS配置。

Conclusion: Aerial-STAR系统展现出巨大潜力，提出的DA-DDPG方法能有效优化系统性能，耦合TRC模型和三维轨迹优化对提升通信效率和公平性具有重要价值。

Abstract: This study departs from the prevailing assumption of independent Transmission
and Reflection Coefficients (TRC) in Airborne Simultaneous Transmit and Reflect
Reconfigurable Intelligent Surface (STAR-RIS) research. Instead, we explore a
novel multi-user downlink communication system that leverages a UAV-mounted
STAR-RIS (Aerial-STAR) incorporating a coupled TRC phase shift model. Our key
contributions include the joint optimization of UAV trajectory, active
beamforming vectors at the base station, and passive RIS TRCs to enhance
communication efficiency, while considering UAV energy constraints. We design
the TRC as a combination of discrete and continuous actions, and propose a
novel Dual Actor Deep Deterministic Policy Gradient (DA-DDPG) algorithm. The
algorithm relies on two separate actor networks for high-dimensional hybrid
action space. We also propose a novel harmonic mean index (HFI)-based reward
function to ensure communication fairness amongst users. For comprehensive
analysis, we study the impact of RIS size on UAV aerodynamics showing that it
increases drag and energy demand. Simulation results demonstrate that the
proposed DA-DDPG algorithm outperforms conventional DDPG and DQN-based
solutions by 24% and 97%, respectively, in accumulated reward.
Three-dimensional UAV trajectory optimization achieves 28% higher communication
efficiency compared to two-dimensional and altitude optimization. The HFI based
reward function provides 41% lower QoS denial rates as compared to other
benchmarks. The mobile Aerial-STAR system shows superior performance over fixed
deployed counterparts, with the coupled phase STAR-RIS outperforming dual
Transmit/Reflect RIS and conventional RIS setups. These findings highlight the
potential of Aerial-STAR systems and the effectiveness of our proposed DA-DDPG
approach in optimizing their performance.

</details>


### [2] [Environment Reconstruction in Multi-Bounce Channels with Array Partial Blockage](https://arxiv.org/abs/2509.13559)
*Yuan Liu,Linlong Wu,Xuesong Cai,M. R. Bhavani Shankar*

Main category: eess.SP

TL;DR: 本文针对超大规模天线阵列(ELAA)中的空间非平稳(SNS)信道问题，提出了基于图模型的多跳空间交替广义期望最大化(GM-SAGE)算法，用于散射体定位和环境重建。


<details>
  <summary>Details</summary>
Motivation: 超大规模天线阵列在需要高角度分辨率的应用中很重要，但存在由于部分遮挡导致的空间非平稳信道问题，需要解决散射体定位和环境重建的挑战。

Method: 通过参数化建模空间变化的幅度稀疏性，应用图基字典辅助的多跳空间交替广义期望最大化(GM-SAGE)算法估计信道参数，并结合幅度估计经验检测信道稀疏性。使用射线追踪仿真生成多跳路径验证方法。

Result: 仿真结果表明，所提出的方法在处理空间非平稳信道方面表现出良好的鲁棒性。

Conclusion: 该方法能够有效处理部分遮挡引起的空间非平稳信道问题，为超大规模天线阵列的应用提供了可靠的散射体定位和环境重建解决方案。

Abstract: Extremely-large antenna arrays (ELAA) are important in applications requiring
high angular resolution. However, a prominent issue is the spatial
non-stationary (SNS) channels due to partial blockage to the ELAA. In this
paper, we address the scatterer localization and subsequent environment
reconstruction considering partially blocked SNS channels. Specifically, the
SNS effects are parametrically modeled through spatial-varying amplitudes with
sparsity. Based on the established signal model, the graph-based
dictionary-aided multi-bounce space-alternating generalized
expectation-maximization (GM-SAGE) algorithm is applied to estimate the channel
parameters and the channel sparsity is empirically detected along with
amplitude estimation. To validate the proposed approach, we generate
multi-bounce paths through ray tracing (RT) simulations, where the SNS channels
caused by partial blockage could be configured flexibly. The simulation results
demonstrate the robustness of the proposed approach in dealing with the SNS
channels.

</details>


### [3] [Quickest Change Detection with Cost-Constrained Experiment Design](https://arxiv.org/abs/2509.14186)
*Patrick Vincent N. Lubenia,Taposh Banerjee*

Main category: eess.SP

TL;DR: 本文研究了多实验选择的快速变化检测问题，开发了2E-CUSUM算法来最小化最坏情况平均检测延迟，同时满足误报和成本约束。


<details>
  <summary>Details</summary>
Motivation: 传统快速变化检测问题中观察者只能进行单一实验，而实际应用中决策者需要在每个观测时刻选择不同信息质量和成本的多重实验。

Method: 针对两实验情况开发了2E-CUSUM算法，并将其扩展到多实验设计，同时探索了数据效率（可选择不进行实验）的情况。

Result: 所提出的算法经过分析被证明是渐近最优的。

Conclusion: 该研究为多实验选择的快速变化检测问题提供了有效的算法解决方案，具有理论保证和实际应用价值。

Abstract: In the classical quickest change detection problem, an observer performs only
one experiment to monitor a stochastic process. This paper considers the case
where, at each observation time, the decision-maker needs to choose between
multiple experiments with different information qualities and costs. The goal
is to minimize the worst-case average detection delay subject to false alarm
and cost constraints. An algorithm called the 2E-CUSUM Algorithm has been
developed to achieve this goal for the two-experiment case. Extensions to
multiple-experiment designs are also studied, and 2E-CUSUM is extended
accordingly. Data efficiency, where the observer has the choice not to perform
an experiment, is explored as well. The proposed algorithms are analyzed and
shown to be asymptotically optimal.

</details>


### [4] [Fast Single-Snapshot Harmonic Recovery with 2D Sparse Arrays using BCCB Matrices](https://arxiv.org/abs/2509.13592)
*Youval Klioui*

Main category: eess.SP

TL;DR: 提出了一种高效的稀疏恢复方法实现，用于单快照2D稀疏阵列谐波估计问题，通过利用BCCB矩阵结构和2D FFT将计算复杂度从O((L1L2)^2)降低到O(L1L2 log(L1L2))


<details>
  <summary>Details</summary>
Motivation: 解决2D稀疏阵列谐波估计中Gram矩阵计算复杂度高的问题，传统方法需要O((L1L2)^2)次操作，计算效率低下

Method: 对稀疏恢复问题的子字典谐波网格施加均匀性约束，并限制阵列拓扑结构使阵元位于半波长单位的网格上，从而使Gram矩阵呈现BCCB结构，利用2D FFT加速矩阵-向量乘积计算

Result: 实验验证了在ISTA、FISTA和ADMM算法中的性能提升，计算复杂度显著降低

Conclusion: 该方法通过利用BCCB矩阵结构和FFT技术，有效降低了2D稀疏阵列谐波估计的计算复杂度，提高了算法效率

Abstract: We introduce an efficient implementation of sparse recovery methods for the
problem of harmonic estimation with 2D sparse arrays using a single snapshot.
By imposing a uniformity constraint on the harmonic grids of the
subdictionaries used in the sparse recovery problem, in addition to a mild
constraint on the array topology that consists in having the elements lie on a
grid specified in half-wavelength units, we show that the Gram matrices that
appear in these sparse recovery methods exhibit a block-circulant with
circulant blocks (BCCB) structure. The BCCB structure is then exploited to
reduce the computational complexity of the matrix-vector products that appear
in these methods through the use of 2D fast Fourier transforms (FFT) from
O((L1L2)^2) down to O(L1L2 log(L1L2)) operations per iterations, where L1, L2
are the lengths of the subdictionaries used for estimating the harmonics in the
first and second dimension, respectively. We experimentally verify the proposed
implementation using the iterative shrinkage thresholding algorithm (ISTA), the
fast iterative shrinkage-thresholding algorithm (FISTA), and the alternating
direction method of multipliers (ADMM) where we observe improvements

</details>


### [5] [Active Inference Framework for Closed-Loop Sensing, Communication, and Control in UAV Systems](https://arxiv.org/abs/2509.14201)
*Guangjin Pan,Liping Bai,Zhuojun Tian,Hui Chen,Mehdi Bennis,Henk Wymeersch*

Main category: eess.SP

TL;DR: 将主动推理框架(AIF)引入支持SCC的无人机系统，实现联合状态估计、控制和感知资源分配，相比基线方法降低了控制和感知成本


<details>
  <summary>Details</summary>
Motivation: 现有的SCC解决方案往往将感知和控制分开处理，导致性能次优和资源使用效率低下，需要统一的框架来优化集成感知通信控制

Method: 通过构建统一的生成模型，将问题转化为最小化变分自由能进行推理和最小化期望自由能进行动作规划

Result: 仿真结果显示，相对于基线方法，控制成本和感知成本都得到了降低

Conclusion: 主动推理框架为集成感知通信控制的无人机系统提供了有效的联合优化方法，能够同时改善状态估计、控制和资源分配性能

Abstract: Integrated sensing and communication (ISAC) is a core technology for 6G, and
its application to closed-loop sensing, communication, and control (SCC)
enables various services. Existing SCC solutions often treat sensing and
control separately, leading to suboptimal performance and resource usage. In
this work, we introduce the active inference framework (AIF) into SCC-enabled
unmanned aerial vehicle (UAV) systems for joint state estimation, control, and
sensing resource allocation. By formulating a unified generative model, the
problem reduces to minimizing variational free energy for inference and
expected free energy for action planning. Simulation results show that both
control cost and sensing cost are reduced relative to baselines.

</details>


### [6] [GNSS Jamming and Spoofing Monitoring Using Low-Cost COTS Receivers](https://arxiv.org/abs/2509.13600)
*Argyris Kriezis,Yu-Hsuan Chen,Dennis Akos,Sherman Lo,Todd Walter*

Main category: eess.SP

TL;DR: 提出了一种使用低成本商用GNSS接收机检测和分类射频干扰事件的方法，通过结合载噪比和校准接收功率构建二维检测空间，有效识别正常、干扰、欺骗和阻塞信号状态。


<details>
  <summary>Details</summary>
Motivation: 全球导航卫星系统(GNSS)日益容易受到射频干扰(RFI)，包括干扰和欺骗，这些威胁导航和授时服务的完整性，需要有效的监测方法。

Method: 结合载噪比(C/N0)测量和校准接收功率指标，构建二维检测空间来识别和区分正常、干扰、欺骗和阻塞信号条件。

Result: 通过在挪威的受控干扰测试以及在波兰和东南地中海的实际部署验证，结果表明经过适当校准的商用接收机检测方法对GNSS RFI监测是可行且有效的。

Conclusion: 基于商用接收机的检测方法在适当校准后，为GNSS射频干扰监测提供了一种可行且有效的解决方案。

Abstract: The Global Navigation Satellite System (GNSS) is increasingly vulnerable to
radio frequency interference (RFI), including jamming and spoofing, which
threaten the integrity of navigation and timing services. This paper presents a
methodology for detecting and classifying RFI events using low-cost commercial
off-the-shelf (COTS) GNSS receivers. By combining carrier-to-noise ratio (C/N0)
measurements with a calibrated received power metric, a two-dimensional
detection space is constructed to identify and distinguish nominal, jammed,
spoofed, and blocked signal conditions. The method is validated through both
controlled jamming tests in Norway and real-world deployments in Poland, and
the Southeast Mediterranean which have experienced such conditions. Results
demonstrate that COTS-based detection, when properly calibrated, offers a
viable and effective approach for GNSS RFI monitoring.

</details>


### [7] [Theoretical Validation of the Latent Optimally Partitioned-$\ell_2/\ell_1$ Penalty with Application to Angular Power Spectrum Estimation](https://arxiv.org/abs/2509.13745)
*Hiroki Kuroda,Renato Luis Garrido Cavalcante,Masahiro Yukawa*

Main category: eess.SP

TL;DR: LOP-ℓ₂/ℓ₁惩罚在理论和实践中都能有效利用块稀疏性，无需已知具体块结构。该惩罚的优化块分区满足块稀疏信号准确恢复的条件，应用于MIMO系统中未知块分区的角功率谱估计，显著提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 在MIMO通信系统中，角功率谱具有块稀疏特性但块分区未知，需要一种无需先验块结构知识的方法来有效利用这种块稀疏性。

Method: 提出使用LOP-ℓ₂/ℓ₁惩罚方法，该方法通过优化块分区来利用块稀疏性，无需已知具体的块结构信息。

Result: 数值模拟显示，使用LOP-ℓ₂/ℓ₁惩罚的块稀疏方法显著提高了角功率谱的估计精度。

Conclusion: LOP-ℓ₂/ℓ₁惩罚是一种有效的工具，能够在不知道具体块结构的情况下成功利用块稀疏性，在角功率谱估计等应用中表现出优越性能。

Abstract: This paper demonstrates that, in both theory and practice, the latent
optimally partitioned (LOP)-$\ell_2/\ell_1$ penalty is effective for exploiting
block-sparsity without the knowledge of the concrete block structure. More
precisely, we first present a novel theoretical result showing that the
optimized block partition in the LOP-$\ell_2/\ell_1$ penalty satisfy a
condition required for accurate recovery of block-sparse signals. Motivated by
this result, we present a new application of the LOP-$\ell_2/\ell_1$ penalty to
estimation of angular power spectrum, which is block-sparse with unknown block
partition, in MIMO communication systems. Numerical simulations show that the
proposed use of block-sparsity with the LOP-$\ell_2/\ell_1$ penalty
significantly improves the estimation accuracy of the angular power spectrum.

</details>


### [8] [Efficient Quantization-Aware Neural Receivers: Beyond Post-Training Quantization](https://arxiv.org/abs/2509.13786)
*SaiKrishna Saketh Yellapragada,Esa Ollila,Mario Costa*

Main category: eess.SP

TL;DR: 该论文研究了在6G无线通信系统中，通过量化感知训练(QAT)技术对基于深度学习的神经接收机进行低比特量化，在保持性能的同时实现8倍压缩和3dB的性能提升，为资源受限的边缘设备部署提供了有效解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着6G无线通信系统的发展，基于深度学习的神经接收机在物理层处理中展现出优越性能，但在资源受限的硬件上部署时需要高效的量化技术来降低延迟、能耗和内存占用，同时不牺牲可靠性。

Method: 扩展了后训练量化(PTQ)基线方法，采用量化感知训练(QAT)技术，在训练过程中融入低精度模拟，以提高在超低比特宽度下的鲁棒性。研究将QAT/PTQ应用于神经接收机架构，并在3GPP CDL-B/D信道、LoS/NLoS环境以及最高40m/s用户速度下进行评估。

Result: 结果显示，4位和8位QAT模型在10%目标BLER下实现了与FP32模型相似的性能。QAT模型比PTQ模型性能提升高达3dB，并实现了8倍压缩。

Conclusion: QAT是实现物理层低复杂度、低延迟推理的关键技术，能够促进6G边缘设备的实时处理能力，为神经接收机在资源受限环境中的实际部署提供了有效途径。

Abstract: As wireless communication systems advance toward Sixth Generation (6G) Radio
Access Networks (RAN), Deep Learning (DL)-based neural receivers are emerging
as transformative solutions for Physical Layer (PHY) processing, delivering
superior Block Error Rate (BLER) performance compared to traditional
model-based approaches. Practical deployment on resource-constrained hardware,
however, requires efficient quantization to reduce latency, energy, and memory
without sacrificing reliability. We extend Post-Training Quantization (PTQ)
baselines with Quantization-Aware Training (QAT), which incorporates
low-precision simulation during training for robustness at ultra-low bitwidths.
Our study applies QAT/PTQ to a neural receiver architecture and evaluates
across 3GPP Clustered Delay Line (CDL)-B/D channels in LoS and NLoS
environments at user velocities up to 40 m/s. Results show that 4-bit and 8-bit
QAT models achieve BLERs similar to that of FP32 models at 10% target BLER. QAT
models are also shown to outperform PTQ models by up to 3 dB, and yield 8x
compression. These results demonstrate that QAT is a key enabler of
low-complexity and latency-constrained inference at the PHY layer, facilitating
real-time processing in 6G edge devices

</details>


### [9] [Domino: Dominant Path-based Compensation for Hardware Impairments in Modern WiFi Sensing](https://arxiv.org/abs/2509.13807)
*Ruiqi Kong,He Chen*

Main category: eess.SP

TL;DR: Domino是一个新的WiFi感知框架，通过将信道状态信息转换为信道脉冲响应，利用延迟域处理来精确补偿硬件引起的射频失真，显著提高了呼吸监测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现代WiFi卡（支持802.11ac/ax协议）的自动增益控制和独立RF链会引入复杂的动态失真，导致现有补偿方法失效，WiFi感知面临可靠性挑战。

Method: 将信道状态信息（CSI）转换为信道脉冲响应（CIR），利用主导静态路径作为参考，通过延迟域处理进行有效的失真补偿。

Result: 在实际呼吸监测实验中，Domino相比现有方法实现了至少2倍的平均精度提升，中位误差低于0.24 bpm，在直视和非直视场景下均保持稳健性能。

Conclusion: Domino框架通过创新的延迟域处理方法，有效解决了现代WiFi硬件引起的射频失真问题，显著提升了WiFi感知的可靠性和准确性。

Abstract: WiFi sensing faces a critical reliability challenge due to hardware-induced
RF distortions, especially with modern, market-dominant WiFi cards supporting
802.11ac/ax protocols. These cards employ sensitive automatic gain control and
separate RF chains, introducing complex and dynamic distortions that render
existing compensation methods ineffective. In this paper, we introduce Domino,
a new framework that transforms channel state information (CSI) into channel
impulse response (CIR) and leverages it for precise distortion compensation.
Domino is built on the key insight that hardware-induced distortions impact all
signal paths uniformly, allowing the dominant static path to serve as a
reliable reference for effective compensation through delay-domain processing.
Real-world respiration monitoring experiments show that Domino achieves at
least 2x higher mean accuracy over existing methods, maintaining robust
performance with a median error below 0.24 bpm, even using a single antenna in
both direct line-of-sight and obstructed scenarios.

</details>


### [10] [Flow Matching-Based Active Learning for Radio Map Construction with Low-Altitude UAVs](https://arxiv.org/abs/2509.13822)
*Hao Sun,Shicong Liu,Xianghao Yu,Ying Sun*

Main category: eess.SP

TL;DR: 提出基于主动学习的无人机低空无线电地图构建框架，通过流匹配生成先验和不确定性量化，结合多目标候选选择和路径规划，显著降低重建误差。


<details>
  <summary>Details</summary>
Motivation: 无人机在低空经济中的应用需要精确实时的无线电地图，但受限于飞行续航能力无法进行详尽测量，因此需要基于有限测量的高效地图构建方法。

Method: 采用Plug-and-Play精炼的流匹配算法作为生成先验，利用流匹配生成多个地图来量化不确定性，通过多目标候选选择和效用感知路径搜索指导无人机采集最有价值的数据。

Result: 仿真结果显示该方法显著优于基线方法，归一化均方误差降低超过70%。

Conclusion: 所提出的主动学习框架能够有效利用有限测量数据构建高保真度的低空无线电地图，为无人机通信和导航提供可靠支持。

Abstract: The employment of unmanned aerial vehicles (UAVs) in the lowaltitude economy
necessitates precise and real-time radio maps for reliable communication and
safe navigation. However, constructing such maps is hindered by the
infeasibility of exhaustive measurements due to UAVs' limited flight endurance.
To address this, we propose a novel active learning framework for low-altitude
radio map construction based on limited measurements. First, a Plug-and-Play
(PnP)-refined flow matching algorithm is introduced, which leverages flow
matching as a powerful generative prior within a PnP scheme to reconstruct
high-fidelity radio maps. Second, the generative nature of flow matching is
exploited to quantify uncertainty by generating an ensemble of radio maps and
computing the location-wise variance. The resulting uncertainty map guides a
multi-objective candidate selection and then a trajectory is planned via
utility-aware path search (UAPS), directing the UAV to the most informative
locations while taking travel costs into account. Simulation results
demonstrate that our method significantly outperforms the baselines, achieving
more than a 70% reduction in normalized mean squared error (NMSE).

</details>


### [11] [FFT-Free PAPR Reduction Methods for OFDM Signals](https://arxiv.org/abs/2509.13851)
*Hao Su,Jiangtao Wang,Yongchao Wang*

Main category: eess.SP

TL;DR: 提出了两种低复杂度的OFDM信号峰均功率比(PAPR)降低算法：T-ADMM和TCU-ADMM，通过最小化信号失真功率建立非凸优化模型，采用定制化的ADMM算法求解，避免了传统方法中重复FFT/IFFT操作的高计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统OFDM信号PAPR降低算法需要重复进行FFT和IFFT操作，计算复杂度高，需要开发更高效的低复杂度算法来解决这一问题。

Method: 建立最小化信号失真功率的非凸优化模型，提出定制化的交替方向乘子法(T-ADMM)及其改进版本TCU-ADMM，所有子问题可解析求解，每次迭代具有线性计算复杂度。

Result: 算法避免了重复FFT/IFFT操作，T-ADMM在适当参数选择下具有理论收敛保证，仿真结果证明了所提方法的有效性。

Conclusion: 提出的T-ADMM和TCU-ADMM算法能够有效降低OFDM信号的PAPR，同时保持低计算复杂度，为实际应用提供了可行的解决方案。

Abstract: In this paper, we propose two low-complexity peak to average power
ratio(PAPR) reduction algorithms for orthogonal frequency division
multiplexing(OFDM) signals. The main content is as follows: First, a non-convex
optimization model is established by minimizing the signal distortion power.
Then, a customized alternating direction method of multipliers(ADMM) algorithm
is proposed to solve the problem, named time domain ADMM(T-ADMM) along with an
improved version called T-ADMM with constrain update(TCU-ADMM). In the
algorithms, all subproblems can be solved analytically, and each iteration has
linear computational complexity. These algorithms circumvents the challenges
posed by repeated fast Fourier transform(FFT) and inverse FFT(IFFT) operations
in traditional PAPR reduction algorithms. Additionally, we prove that the
T-ADMM algorithm is theoretically guaranteed convergent if proper parameter is
chosen. Finally, simulation results demonstrate the effectiveness of the
proposed methods.

</details>


### [12] [Reconfigurable Intelligent Surface-Assisted Multiuser Tracking and Signal Detection in ISAC](https://arxiv.org/abs/2509.13940)
*Weifeng Zhu,Junyuan Gao,Shuowen Zhang,Liang Liu*

Main category: eess.SP

TL;DR: 提出了一种基于变分消息传递的混合算法，用于RIS辅助ISAC系统中的多用户跟踪和信号检测，通过概率信号模型和贝叶斯方法有效提升跟踪和检测性能


<details>
  <summary>Details</summary>
Motivation: 解决集成传感与通信系统中由于用户高移动性导致的跟踪和信号检测性能下降问题，需要建立有效的状态更新机制

Method: 建立概率信号模型描述用户状态、发射信号和接收信号之间的依赖关系，提出混合变分消息传递算法进行在线用户状态估计

Result: 数值结果表明所提算法在跟踪和信号检测性能上显著优于代表性贝叶斯估计算法

Conclusion: 该混合变分消息传递算法能够以计算高效的方式迭代更新用户状态后验概率，有效提升RIS辅助ISAC系统的多用户跟踪和信号检测性能

Abstract: This paper investigates the multiuser tracking and signal detection problem
in integrated sensing and communication (ISAC) systems with the assistance of
reconfigurable intelligent surfaces (RISs). Due to the diverse and high user
mobility, the tracking and signal detection performance can be significantly
deteriorated without choreographed user state (position and velocity) updating
principle. To tackle this challenge, we manage to establish a comprehensive
probabilistic signal model to characterize the interdependencies among user
states, transmit signals, and received signals during the tracking procedure.
Based on the Bayesian problem formulation, we further propose a novel hybrid
variational message passing algorithm for the online estimation of user states,
which can iteratively update the posterior probabilities of user states during
each tracking frame with computational efficiency. Numerical results are
provided to demonstrate that the proposed algorithm can significantly improve
both of the tracking and signal detection performance over the representative
Bayesian estimation counterparts.

</details>


### [13] [Adaptive and robust smartphone-based step detection in multiple sclerosis](https://arxiv.org/abs/2509.13961)
*Lorenza Angelini,Dimitar Stanev,Marta Płonka,Rafał Klimas,Natan Napiórkowski,Gabriela González Chan,Lisa Bunn,Paul S Glazier,Richard Hosking,Jenny Freeman,Jeremy Hobart,Jonathan Marsden,Licinio Craveiro,Mike D Rinderknecht,Mattia Zanon*

Main category: eess.SP

TL;DR: 开发了一种步态检测算法，能够在不同测试设置、智能手机佩戴位置和步态障碍水平下准确检测初始/最终接触点，在实验室和真实世界环境中都表现出高精度。


<details>
  <summary>Details</summary>
Motivation: 现有的步态处理管道主要关注在监督设置下使用单个传感器检测初始接触点，需要验证算法在不同测试环境、多种手机佩戴位置和不同步态障碍程度下的性能。

Method: 通过GaitLab研究，让健康对照组和多发性硬化症患者在实验室进行监督式两分钟步行测试（使用6部手机），并在真实世界进行10-14天的无监督步行活动（使用单部手机），使用运动捕捉系统或Gait Up传感器作为参考数据。

Result: 研究35名健康人和93名多发性硬化症患者，在所有手机佩戴位置都能准确检测初始/最终接触点，F1分数在实验室测试中≥98.2%/96.5%（健康人）和≥98.5%/97.7%（患者），真实世界环境中F1分数保持在94.4%-100%，时间误差≤0.08秒。

Conclusion: 该步态管道能够在不同智能手机位置和环境中准确一致地检测初始和最终接触点，展示了其在真实世界步态评估中的潜力。

Abstract: Background: Many attempts to validate gait pipelines that process sensor data
to detect gait events have focused on the detection of initial contacts only in
supervised settings using a single sensor. Objective: To evaluate the
performance of a gait pipeline in detecting initial/final contacts using a step
detection algorithm adaptive to different test settings, smartphone wear
locations, and gait impairment levels. Methods: In GaitLab (ISRCTN15993728),
healthy controls (HC) and people with multiple sclerosis (PwMS; Expanded
Disability Status Scale 0.0-6.5) performed supervised Two-Minute Walk Test
[2MWT] (structured in-lab overground and treadmill 2MWT) during two on-site
visits carrying six smartphones and unsupervised walking activities (structured
and unstructured real-world walking) daily for 10-14 days using a single
smartphone. Reference gait data were collected with a motion capture system or
Gait Up sensors. The pipeline's performance in detecting initial/final contacts
was evaluated through F1 scores and absolute temporal error with respect to
reference measurement systems. Results: We studied 35 HC and 93 PwMS.
Initial/final contacts were accurately detected across all smartphone wear
locations. Median F1 scores for initial/final contacts on in-lab 2MWT were
>=98.2%/96.5% in HC and >=98.5%/97.7% in PwMS. F1 scores remained high on
structured (HC: 100% [0.3%]/100% [0.2%]; PwMS: 99.5% [1.9%]/99.4% [2.5%]) and
unstructured real-world walking (HC: 97.8% [2.6%]/97.8% [2.8%]; PwMS: 94.4%
[6.2%]/94.0% [6.5%]). Median temporal errors were <=0.08 s. Neither age, sex,
disease severity, walking aid use, nor setting (outdoor/indoor) impacted
pipeline performance (all p>0.05). Conclusion: This gait pipeline accurately
and consistently detects initial and final contacts in PwMS across different
smartphone locations and environments, highlighting its potential for
real-world gait assessment.

</details>


### [14] [Classification Filtering](https://arxiv.org/abs/2509.13975)
*Ilker Bayram*

Main category: eess.SP

TL;DR: 提出一种用于流式信号分类的状态空间模型和实时滤波器，通过融合多个分类器的输出并利用时间信息来提高分类准确率


<details>
  <summary>Details</summary>
Motivation: 在流式信号处理中，多个分类器以固定策略运行，但它们的准确率各不相同。需要一种方法来融合这些分类器的输出，同时利用时序信息来提升分类性能

Method: 设计状态空间模型并开发专门用于实时执行的滤波器，将多个分类器的概率输出进行时序融合

Result: 在基于可穿戴设备IMU数据的活动分类应用中验证了所提出滤波器的有效性

Conclusion: 提出的状态空间模型和实时滤波器能够有效融合多个分类器的输出，利用时序信息显著提升流式信号的分类准确率

Abstract: We consider a streaming signal in which each sample is linked to a latent
class. We assume that multiple classifiers are available, each providing class
probabilities with varying degrees of accuracy. These classifiers are employed
following a straightforward and fixed policy. In this setting, we consider the
problem of fusing the output of the classifiers while incorporating the
temporal aspect to improve classification accuracy. We propose a state-space
model and develop a filter tailored for realtime execution. We demonstrate the
effectiveness of the proposed filter in an activity classification application
based on inertial measurement unit (IMU) data from a wearable device.

</details>


### [15] [Distributed Coherent Beamforming at 60 GHz Enabled by Optically-Established Coherence](https://arxiv.org/abs/2509.13984)
*Drake Silbernagel,Yu Rong,Isabella Lenz,Prithvi Hemanth,Carl Morgenstern,Owen Ma,Nolan Matthews,Nader Zaki,Kyle W. Martin,John D. Elgin,Jacob Holtom,Daniel W. Bliss,Kimberly Frey*

Main category: eess.SP

TL;DR: 通过光学时间同步系统实现了60 GHz分布式网络的精确同步，在接收端实现放大和干扰拒止，在发射端实现信号空间控制，显著提升了信号质量和干扰拒止能力


<details>
  <summary>Details</summary>
Motivation: 使用分布式系统获得相位同步能力，在V波段实现高性能的收发空间处理技术，而无需GPS定时

Method: 采用光学时间同步系统，为分布式网络元素提供精确时间和频率对齐，并进行接收端政形处理和发射端空间控制

Result: 实验结果显示：接收端干扰情况下SINR提升14.3dB，对干扰形成13.5dB的空间拒止；发射端在目标收收机处SNR提升7.9dB，同时在其他收收机处SNR降低8.9dB

Conclusion: 这一方法成功在V波段实现了分布式同步技术，为高频分布式系统提供了高性能的空间处理能力

Abstract: We implement and experimentally demonstrate a 60 GHz distributed system
leveraging an optical time synchronization system that provides precise time
and frequency alignment between independent elements of the distributed mesh.
Utilizing such accurate coherence, we perform receive beamforming with
interference rejection and transmit nulling. In these configurations, the
system achieves a coherent gain over an incoherent network of N nodes,
significantly improving the relevant signal power ratios. Our system
demonstrates extended array phase coherence times, enabling advanced
techniques. Results from over-the-air experiments demonstrate a 14.3 dB
signal-to-interference-plus-noise improvement in interference-laden scenarios
with a contributing 13.5 dB null towards interference in receive beamforming.
In transmit nulling, a signal-to-noise ratio (SNR) gain of 7.9 dB is measured
towards an intended receiver while maintaining an SNR reduction of 8.9 dB at
another receiver. These findings represent the use of distributed coherence in
the V band without the use of GPS timing.

</details>


### [16] [Distributed Deep Learning with RIS Grouping for Accurate Cascaded Channel Estimation](https://arxiv.org/abs/2509.14062)
*Saifur Rahman,Syed Luqman Shah,Salman Khan,Jalal Khan,Muhammad Irfan,Maaz Shafi,Said Muhammad,Fazal Muhammad,Mohammad Shahed Akond*

Main category: eess.SP

TL;DR: 提出基于深度学习的RIS信道估计框架，通过RIS元素分组和分布式机器学习策略，显著降低导频开销并提升跨用户位置和传播场景的泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决RIS辅助6G系统中级联信道估计的高导频开销、计算复杂度和能耗问题，以及传统单用户DL模型在新场景下泛化能力差的问题

Method: 采用RIS元素分组减少导频观测，开发分布式机器学习策略让基站和用户协作训练共享神经网络，设计分层DML架构先分类传播条件再进行场景特定特征提取

Result: 仿真结果表明该框架大幅降低导频开销和复杂度，在信道估计精度上优于传统方法和单用户模型

Conclusion: 该方法为6G RIS辅助系统提供了实用有效的信道估计解决方案，展示了良好的实际应用价值

Abstract: Reconfigurable Intelligent Surface (RIS) panels are envisioned as a key
technology for sixth-generation (6G) wireless networks, providing a
cost-effective means to enhance coverage and spectral efficiency. A critical
challenge is the estimation of the cascaded base station (BS)-RIS-user channel,
since the passive nature of RIS elements prevents direct channel acquisition,
incurring prohibitive pilot overhead, computational complexity, and energy
consumption. To address this, we propose a deep learning (DL)-based channel
estimation framework that reduces pilot overhead by grouping RIS elements and
reconstructing the cascaded channel from partial pilot observations.
Furthermore, conventional DL models trained under single-user settings suffer
from poor generalization across new user locations and propagation scenarios.
We develop a distributed machine learning (DML) strategy in which the BS and
users collaboratively train a shared neural network using diverse channel
datasets collected across the network, thereby achieving robust generalization.
Building on this foundation, we design a hierarchical DML neural architecture
that first classifies propagation conditions and then employs scenario-specific
feature extraction to further improve estimation accuracy. Simulation results
confirm that the proposed framework substantially reduces pilot overhead and
complexity while outperforming conventional methods and single-user models in
channel estimation accuracy. These results demonstrate the practicality and
effectiveness of the proposed approach for 6G RIS-assisted systems.

</details>


### [17] [Novel Phase-Noise-Tolerant Variational-Autoencoder-Based Equalization Suitable for Space-Division-Multiplexed Transmission](https://arxiv.org/abs/2509.14072)
*Vincent Lauinger,Lennart Schmitz,Patrick Matalla,Andrej Rode,Sebastian Randel,Laurent Schmalen*

Main category: eess.SP

TL;DR: 提出一种基于变分自编码器的相位噪声容忍均衡方案，用于随机耦合多芯光纤的空间分复用传输实验


<details>
  <summary>Details</summary>
Motivation: 解决空间分复用传输中相位噪声对信号质量的影响问题，提高多芯光纤传输系统的性能

Method: 采用变分自编码器(VAE)架构的均衡方案，具有相位噪声容忍特性，在150km随机耦合多芯光纤上进行实验验证

Result: 实验证明了该方案的有效性，能够在长距离随机耦合多芯光纤传输中实现良好的均衡效果

Conclusion: 基于VAE的均衡方案是解决SDM传输中相位噪声问题的有效方法，为多芯光纤通信系统提供了新的技术途径

Abstract: We demonstrate the effectiveness of a novel phase-noise-tolerant,
variational-autoencoder-based equalization scheme for
space-division-multiplexed (SDM) transmission in an experiment over 150km of
randomly-coupled multi-core fibers.

</details>


### [18] [Hardware-Efficient Cognitive Radar: Multi-Target Detection with RL-Driven Transmissive RIS](https://arxiv.org/abs/2509.14160)
*Adam Umra,Aya Mostafa Ahmed,Stefan Roth,Aydin Sezgin*

Main category: eess.SP

TL;DR: 提出基于强化学习的透射式可重构智能表面雷达框架，在减少射频链数量的情况下实现多目标检测性能提升


<details>
  <summary>Details</summary>
Motivation: 传统认知MIMO雷达硬件复杂度和功耗过高，需要开发更高效的自适应波束成形方案

Method: 使用SARSA强化学习算法调整TRIS相位偏移，在低信噪比条件下优化多目标检测性能

Result: 仿真显示TRIS-RL雷达性能匹配甚至超越MIMO雷达，同时显著降低成本和能耗

Conclusion: TRIS-RL框架为下一代认知雷达提供了高效、低成本的解决方案，特别适合大规模阵列应用

Abstract: Cognitive radar has emerged as a key paradigm for next-generation sensing,
enabling adaptive, intelligent operation in dynamic and complex environments.
Yet, conventional cognitive multiple-input multiple-output (MIMO) radars offer
strong detection performance but suffer from high hardware complexity and power
demands. To overcome these limitations, we develop a reinforcement learning
(RL)-based framework that leverages a transmissive reconfigurable intelligent
surface (TRIS) for adaptive beamforming. A state-action-reward-state-action
(SARSA) agent tunes TRIS phase shifts to improve multi-target detection in low
signal-to-noise ratio (SNR) conditions while operating with far fewer radio
frequency (RF) chains. Simulations confirm that the proposed TRIS-RL radar
matches or, for large number of elements, even surpasses MIMO performance with
reduced cost and energy requirements.

</details>


### [19] [Goal-Oriented Joint Source-Channel Coding: Distortion-Classification-Power Trade-off](https://arxiv.org/abs/2509.14217)
*Andriy Enttsel,Weichen Wang,Mauro Mangia,Riccardo Rovatti,Deniz Gündüz*

Main category: eess.SP

TL;DR: 提出一个理论框架，将分类和异常检测集成到联合源信道编码中，在信号重建目标基础上增加分类功能


<details>
  <summary>Details</summary>
Motivation: 在低延迟、低复杂度通信需求下，需要将分类和异常检测功能整合到传统的联合源信道编码框架中

Method: 假设高斯标量源，约束编码器为分段线性映射，推导可处理的设计规则，明确表征失真、分类错误和传输功率之间的权衡关系

Result: 建立了理论框架，能够同时处理信号重建、分类和异常检测任务

Conclusion: 该工作为联合源信道编码提供了集成分类功能的理论基础，明确了不同性能指标之间的权衡关系

Abstract: Joint source-channel coding is a compelling paradigm when low-latency and
low-complexity communication is required. This work proposes a theoretical
framework that integrates classification and anomaly detection within the
conventional signal reconstruction objective. Assuming a Gaussian scalar source
and constraining the encoder to piecewise linear mappings, we derive tractable
design rules and explicitly characterize the trade-offs between distortion,
classification error, and transmission power.

</details>


### [20] [Self-Supervised and Topological Signal-Quality Assessment for Any PPG Device](https://arxiv.org/abs/2509.12510)
*Wei Shao,Ruoyu Zhang,Zequan Liang,Ehsan Kourkchi,Setareh Rafatirad,Houman Homayoun*

Main category: eess.SP

TL;DR: 提出首个完全无监督的腕部PPG信号质量评估管道，结合自监督学习和拓扑数据分析，实现跨设备的信号质量检测。


<details>
  <summary>Details</summary>
Motivation: 可穿戴光电容积脉搏波(PPG)信号易受运动、灌注损失和环境光干扰，现有信号质量评估方法要么依赖脆弱的启发式规则，要么需要大量标注数据。

Method: 两阶段方法：第一阶段使用对比学习训练1-D ResNet-18，获得光学发射器和运动不变的嵌入表示；第二阶段通过持久同构将嵌入转换为拓扑特征，使用HDBSCAN聚类生成二进制信号质量指数。

Result: 在10,000个窗口的分层样本上，获得Silhouette得分0.72、Davies-Bouldin得分0.34、Calinski-Harabasz得分6173，无需重新调参。

Conclusion: 提出的SSL-TDA框架为PPG信号提供了一个即插即用、可扩展、跨设备的质量门控解决方案。

Abstract: Wearable photoplethysmography (PPG) is embedded in billions of devices, yet
its optical waveform is easily corrupted by motion, perfusion loss, and ambient
light, jeopardizing downstream cardiometric analytics. Existing signal-quality
assessment (SQA) methods rely either on brittle heuristics or on data-hungry
supervised models. We introduce the first fully unsupervised SQA pipeline for
wrist PPG. Stage 1 trains a contrastive 1-D ResNet-18 on 276 h of raw,
unlabeled data from heterogeneous sources (varying in device and sampling
frequency), yielding optical-emitter- and motion-invariant embeddings (i.e.,
the learned representation is stable across differences in LED wavelength,
drive intensity, and device optics, as well as wrist motion). Stage 2 converts
each 512-D encoder embedding into a 4-D topological signature via persistent
homology (PH) and clusters these signatures with HDBSCAN. To produce a binary
signal-quality index (SQI), the acceptable PPG signals are represented by the
densest cluster while the remaining clusters are assumed to mainly contain
poor-quality PPG signals. Without re-tuning, the SQI attains Silhouette,
Davies-Bouldin, and Calinski-Harabasz scores of 0.72, 0.34, and 6173,
respectively, on a stratified sample of 10,000 windows. In this study, we
propose a hybrid self-supervised-learning--topological-data-analysis (SSL--TDA)
framework that offers a drop-in, scalable, cross-device quality gate for PPG
signals.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [21] [Towards a Physics Foundation Model](https://arxiv.org/abs/2509.13805)
*Florian Wiesner,Matthias Wessling,Stephen Baek*

Main category: cs.LG

TL;DR: 提出了通用物理变换器(GPhyT)，这是一个基于Transformer的物理基础模型，能够在多个物理领域实现零样本泛化，无需重新训练即可模拟各种物理系统。


<details>
  <summary>Details</summary>
Motivation: 当前基于物理的机器学习方法局限于单一狭窄领域，每次面对新系统都需要重新训练。物理基础模型(PFM)可以彻底改变这一现状，实现高保真仿真的民主化，加速科学发现，并消除对专业求解器开发的需求。

Method: 使用Transformer架构，在1.8TB的多样化仿真数据上进行训练。关键洞察是Transformer可以通过上下文推断控制动力学，使单个模型能够模拟流体-固体相互作用、冲击波、热对流和多相动力学，而无需被告知基础方程。

Result: GPhyT实现了三个关键突破：(1)在多个物理领域表现优异，比专业架构性能提升高达29倍；(2)通过上下文学习实现零样本泛化到完全未见过的物理系统；(3)通过50个时间步长的滚动预测实现稳定的长期预测。

Conclusion: 这项工作证明了单个模型可以仅从数据中学习可泛化的物理原理，为通向可能改变计算科学与工程的通用物理基础模型开辟了道路。

Abstract: Foundation models have revolutionized natural language processing through a
``train once, deploy anywhere'' paradigm, where a single pre-trained model
adapts to countless downstream tasks without retraining. Access to a Physics
Foundation Model (PFM) would be transformative -- democratizing access to
high-fidelity simulations, accelerating scientific discovery, and eliminating
the need for specialized solver development. Yet current physics-aware machine
learning approaches remain fundamentally limited to single, narrow domains and
require retraining for each new system. We present the General Physics
Transformer (GPhyT), trained on 1.8 TB of diverse simulation data, that
demonstrates foundation model capabilities are achievable for physics. Our key
insight is that transformers can learn to infer governing dynamics from
context, enabling a single model to simulate fluid-solid interactions, shock
waves, thermal convection, and multi-phase dynamics without being told the
underlying equations. GPhyT achieves three critical breakthroughs: (1) superior
performance across multiple physics domains, outperforming specialized
architectures by up to 29x, (2) zero-shot generalization to entirely unseen
physical systems through in-context learning, and (3) stable long-term
predictions through 50-timestep rollouts. By establishing that a single model
can learn generalizable physical principles from data alone, this work opens
the path toward a universal PFM that could transform computational science and
engineering.

</details>


### [22] [Defending Diffusion Models Against Membership Inference Attacks via Higher-Order Langevin Dynamics](https://arxiv.org/abs/2509.14225)
*Benjamin Sterling,Yousef El-Laham,Mónica F. Bugallo*

Main category: cs.LG

TL;DR: 提出使用临界阻尼高阶朗之万动力学防御扩散模型对抗成员推理攻击的方法，通过引入辅助变量和联合扩散过程来混合外部随机性，从而在扩散过程早期破坏敏感输入数据。


<details>
  <summary>Details</summary>
Motivation: 生成式AI应用的发展带来了新的数据安全问题，扩散模型虽然比其他生成模型对成员推理攻击更具抵抗力，但仍然存在被攻击的风险，需要有效的防御机制。

Method: 采用临界阻尼高阶朗之万动力学，引入多个辅助变量和联合扩散过程，通过辅助变量混合外部随机性来在扩散过程早期破坏敏感输入数据。

Result: 在玩具数据集和语音数据集上进行了理论分析和实验验证，使用AUROC曲线和FID指标评估防御效果。

Conclusion: 该方法为扩散模型提供了有效的成员推理攻击防御机制，通过理论分析和实验验证证明了其可行性。

Abstract: Recent advances in generative artificial intelligence applications have
raised new data security concerns. This paper focuses on defending diffusion
models against membership inference attacks. This type of attack occurs when
the attacker can determine if a certain data point was used to train the model.
Although diffusion models are intrinsically more resistant to membership
inference attacks than other generative models, they are still susceptible. The
defense proposed here utilizes critically-damped higher-order Langevin
dynamics, which introduces several auxiliary variables and a joint diffusion
process along these variables. The idea is that the presence of auxiliary
variables mixes external randomness that helps to corrupt sensitive input data
earlier on in the diffusion process. This concept is theoretically investigated
and validated on a toy dataset and a speech dataset using the Area Under the
Receiver Operating Characteristic (AUROC) curves and the FID metric.

</details>


### [23] [Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics](https://arxiv.org/abs/2509.13425)
*Julian Evan Chrisnanto,Yulison Herry Chrisnanto,Ferry Faizal*

Main category: cs.LG

TL;DR: USPIL框架通过物理信息神经网络和守恒定律的统一架构，成功建模了捕食者-猎物系统的多尺度时空动力学，在保持物理一致性的同时实现了高精度和计算效率。


<details>
  <summary>Details</summary>
Motivation: 生态系统的复杂多尺度动力学特性对传统建模方法构成挑战，需要能够同时捕捉时间振荡和涌现时空模式的新方法，同时必须遵守守恒定律。

Method: 提出统一时空物理信息学习(USPIL)框架，结合物理信息神经网络(PINNs)和守恒定律，使用自动微分实施物理约束和自适应损失加权来平衡数据保真度与物理一致性。

Result: 在Lotka-Volterra系统中，USPIL实现了1D时间动力学98.9%的相关性(损失:0.0219，MAE:0.0184)，在2D系统中捕捉到复杂螺旋波(损失:4.7656，模式相关性:0.94)，验证显示守恒定律遵守在0.5%以内，推理计算速度比数值求解器快10-50倍。

Conclusion: USPIL建立了物理信息深度学习作为强大且科学严谨的范式，为生态预测、保护规划和生态系统韧性理解提供了变革性工具，其在不同维度表述间转换的能力为多尺度生态建模开辟了新途径。

Abstract: Ecological systems exhibit complex multi-scale dynamics that challenge
traditional modeling. New methods must capture temporal oscillations and
emergent spatiotemporal patterns while adhering to conservation principles. We
present the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,
a deep learning architecture integrating physics-informed neural networks
(PINNs) and conservation laws to model predator-prey dynamics across
dimensional scales. The framework provides a unified solution for both ordinary
(ODE) and partial (PDE) differential equation systems, describing temporal
cycles and reaction-diffusion patterns within a single neural network
architecture. Our methodology uses automatic differentiation to enforce physics
constraints and adaptive loss weighting to balance data fidelity with physical
consistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%
correlation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures
complex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).
Validation confirms conservation law adherence within 0.5% and shows a 10-50x
computational speedup for inference compared to numerical solvers. USPIL also
enables mechanistic understanding through interpretable physics constraints,
facilitating parameter discovery and sensitivity analysis not possible with
purely data-driven methods. Its ability to transition between dimensional
formulations opens new avenues for multi-scale ecological modeling. These
capabilities make USPIL a transformative tool for ecological forecasting,
conservation planning, and understanding ecosystem resilience, establishing
physics-informed deep learning as a powerful and scientifically rigorous
paradigm.

</details>


### [24] [An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training](https://arxiv.org/abs/2509.13516)
*Tom Almog*

Main category: cs.LG

TL;DR: 本文通过360次实验对比8种优化器的能耗效率，发现AdamW和NAdam在多个数据集上表现均衡，SGD在复杂数据集上性能优异但碳排放较高，为平衡AI性能与可持续性提供实用指导。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型日益复杂和计算需求增长，理解训练决策对环境的影响对可持续AI发展至关重要。需要研究不同优化器选择与能源效率之间的关系。

Method: 在三个基准数据集(MNIST、CIFAR-10、CIFAR-100)上使用8种流行优化器进行360次受控实验，每个优化器使用15个随机种子，通过CodeCarbon在Apple M1 Pro硬件上精确追踪能耗指标。

Result: 发现训练速度、准确性和环境影响之间存在显著权衡，且随数据集和模型复杂度变化。AdamW和NAdam表现一致高效，SGD在复杂数据集上性能优越但排放更高。

Conclusion: 研究结果为从业者在机器学习工作流中平衡性能与可持续性提供了可操作的见解，强调了优化器选择对环境影响的的重要性。

Abstract: As machine learning models grow increasingly complex and computationally
demanding, understanding the environmental impact of training decisions becomes
critical for sustainable AI development. This paper presents a comprehensive
empirical study investigating the relationship between optimizer choice and
energy efficiency in neural network training. We conducted 360 controlled
experiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using
eight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,
NAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking
on Apple M1 Pro hardware, we measured training duration, peak memory usage,
carbon dioxide emissions, and final model performance. Our findings reveal
substantial trade-offs between training speed, accuracy, and environmental
impact that vary across datasets and model complexity. We identify AdamW and
NAdam as consistently efficient choices, while SGD demonstrates superior
performance on complex datasets despite higher emissions. These results provide
actionable insights for practitioners seeking to balance performance and
sustainability in machine learning workflows.

</details>


### [25] [Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework](https://arxiv.org/abs/2509.13520)
*Varun Kumar,Jing Bi,Cyril Ngo Ngoc,Victor Oancea,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出混合DeepONet-Transolver框架，用于解决PET瓶子屈曲分析问题，能够同时预测节点位移场和时间相关的反作用力，在几何参数化设计中表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络方法在处理非参数化几何域变化的PDE问题时泛化能力有限，而传统的有限元分析计算成本高昂，需要开发高效的计算替代模型。

Method: 采用混合DeepONet-Transolver框架，结合深度算子网络和变换求解器，对两个参数化瓶子几何家族（2参数和4参数）进行非线性有限元分析数据训练。

Result: 在4参数瓶子家族上，位移场的平均相对L2误差为2.5-13%，时间相关反作用力误差约2.4%，绝对位移误差在10^-4-10^-3量级，能够准确捕捉屈曲等关键物理现象。

Conclusion: 该框架作为可扩展的计算高效替代模型，特别适用于计算力学中的多任务预测和需要快速设计评估的应用场景。

Abstract: Neural surrogates and operator networks for solving partial differential
equation (PDE) problems have attracted significant research interest in recent
years. However, most existing approaches are limited in their ability to
generalize solutions across varying non-parametric geometric domains. In this
work, we address this challenge in the context of Polyethylene Terephthalate
(PET) bottle buckling analysis, a representative packaging design problem
conventionally solved using computationally expensive finite element analysis
(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously
predicts nodal displacement fields and the time evolution of reaction forces
during top load compression. Our methodology is evaluated on two families of
bottle geometries parameterized by two and four design variables. Training data
is generated using nonlinear FEA simulations in Abaqus for 254 unique designs
per family. The proposed framework achieves mean relative $L^{2}$ errors of
2.5-13% for displacement fields and approximately 2.4% for time-dependent
reaction forces for the four-parameter bottle family. Point-wise error analyses
further show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,
with the largest discrepancies confined to localized geometric regions.
Importantly, the model accurately captures key physical phenomena, such as
buckling behavior, across diverse bottle geometries. These results highlight
the potential of our framework as a scalable and computationally efficient
surrogate, particularly for multi-task predictions in computational mechanics
and applications requiring rapid design evaluation.

</details>


### [26] [AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions](https://arxiv.org/abs/2509.13523)
*Väinö Hatanpää,Eugene Ku,Jason Stock,Murali Emani,Sam Foreman,Chunyong Jung,Sandeep Madireddy,Tung Nguyen,Varuni Sastry,Ray A. O. Sinurat,Sam Wheeler,Huihuo Zheng,Troy Arcomano,Venkatram Vishwanath,Rao Kotamarthi*

Main category: cs.LG

TL;DR: AERIS是一个10-800亿参数的像素级Swin扩散变换器，通过SWiPe并行化技术实现高效扩展，在天气预测中超越IFS ENS系统并保持90天的季节尺度稳定性


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型在高分辨率天气预测中难以稳定扩展的问题，利用生成式机器学习更好地理解复杂地球系统动力学

Method: 采用像素级Swin扩散变换器架构，结合SWiPe并行化技术（窗口并行与序列/流水线并行组合），在Aurora超级计算机上实现高效计算

Result: 在0.25度ERA5数据集上实现10.21 ExaFLOPS持续性能和11.21 ExaFLOPS峰值性能，弱扩展效率95.5%，强扩展效率81.6%，性能超越IFS ENS系统

Conclusion: 十亿参数级别的扩散模型在天气和气候预测方面具有巨大潜力，能够提供比确定性方法更好的集合校准和光谱偏差处理

Abstract: Generative machine learning offers new opportunities to better understand
complex Earth system dynamics. Recent diffusion-based methods address spectral
biases and improve ensemble calibration in weather forecasting compared to
deterministic methods, yet have so far proven difficult to scale stably at high
resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin
diffusion transformer to address this gap, and SWiPe, a generalizable technique
that composes window parallelism with sequence and pipeline parallelism to
shard window-based transformers without added communication cost or increased
global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS
(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$
patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling
efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS
and remains stable on seasonal scales to 90 days, highlighting the potential of
billion-parameter diffusion models for weather and climate prediction.

</details>


### [27] [Meta-Learning Linear Models for Molecular Property Prediction](https://arxiv.org/abs/2509.13527)
*Yulia Pimonova,Michael G. Taylor,Alice Allen,Ping Yang,Nicholas Lubbers*

Main category: cs.LG

TL;DR: LAMeL是一种线性元学习算法，通过在相关任务间共享模型参数来提高化学性质预测的准确性和可解释性，性能比标准岭回归提升1.1-25倍。


<details>
  <summary>Details</summary>
Motivation: 化学研究中高质量数据集有限，机器学习方法对数据需求增加，需要平衡预测准确性和人类可理解性，因此开发既准确又可解释的AI方法。

Method: 采用元学习框架，识别相关任务间的共享模型参数，学习共同函数流形作为新任务的更优起点，即使任务间不共享数据。

Result: 性能比标准岭回归提升1.1-25倍，在不同任务上表现稳定，始终优于或匹配传统线性方法。

Conclusion: LAMeL是化学性质预测中可靠的工具，在准确性和可解释性都至关重要的场景中表现优异。

Abstract: Chemists in search of structure-property relationships face great challenges
due to limited high quality, concordant datasets. Machine learning (ML) has
significantly advanced predictive capabilities in chemical sciences, but these
modern data-driven approaches have increased the demand for data. In response
to the growing demand for explainable AI (XAI) and to bridge the gap between
predictive accuracy and human comprehensibility, we introduce LAMeL - a Linear
Algorithm for Meta-Learning that preserves interpretability while improving the
prediction accuracy across multiple properties. While most approaches treat
each chemical prediction task in isolation, LAMeL leverages a meta-learning
framework to identify shared model parameters across related tasks, even if
those tasks do not share data, allowing it to learn a common functional
manifold that serves as a more informed starting point for new unseen tasks.
Our method delivers performance improvements ranging from 1.1- to 25-fold over
standard ridge regression, depending on the domain of the dataset. While the
degree of performance enhancement varies across tasks, LAMeL consistently
outperforms or matches traditional linear methods, making it a reliable tool
for chemical property prediction where both accuracy and interpretability are
critical.

</details>


### [28] [Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection](https://arxiv.org/abs/2509.13608)
*Niruthiha Selvanayagam,Ted Kurti*

Main category: cs.LG

TL;DR: GPT-4o mini存在"单模态瓶颈"安全架构缺陷，其先进的多模态推理能力被上下文无关的安全过滤器系统性地阻断，导致在仇恨言论检测中出现可预测的误报问题


<details>
  <summary>Details</summary>
Motivation: 随着大型多模态模型(LMMs)成为日常生活的重要组成部分，理解其安全架构对于AI对齐至关重要，需要系统分析全球部署模型的安全性能

Method: 使用Hateful Memes Challenge数据集，对500个样本进行多阶段调查，分析模型推理和失败模式，并对144个内容策略拒绝进行定量验证

Result: 实验发现50%的拒绝由单模态视觉内容触发，50%由文本内容触发，安全系统脆弱，不仅阻止高风险图像，还阻止良性的常见meme格式

Conclusion: 这些发现揭示了最先进LMMs中能力与安全之间的根本张力，强调需要更集成、上下文感知的对齐策略，以确保AI系统能够安全有效地部署

Abstract: As Large Multimodal Models (LMMs) become integral to daily digital life,
understanding their safety architectures is a critical problem for AI
Alignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a
globally deployed model, on the difficult task of multimodal hate speech
detection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase
investigation on 500 samples to probe the model's reasoning and failure modes.
Our central finding is the experimental identification of a "Unimodal
Bottleneck," an architectural flaw where the model's advanced multimodal
reasoning is systematically preempted by context-blind safety filters. A
quantitative validation of 144 content policy refusals reveals that these
overrides are triggered in equal measure by unimodal visual 50% and textual 50%
content. We further demonstrate that this safety system is brittle, blocking
not only high-risk imagery but also benign, common meme formats, leading to
predictable false positives. These findings expose a fundamental tension
between capability and safety in state-of-the-art LMMs, highlighting the need
for more integrated, context-aware alignment strategies to ensure AI systems
can be deployed both safely and effectively.

</details>


### [29] [Unsupervised Anomaly Detection in ALS EPICS Event Logs](https://arxiv.org/abs/2509.13621)
*Antonin Sulc,Thorsten Hellert,Steven Hunt*

Main category: cs.LG

TL;DR: 基于语义嵌入和序列感知神经网络的自动化故障分析框架，用于处理ALS光源的实时事件日志并检测异常


<details>
  <summary>Details</summary>
Motivation: 为先进光源(ALS)控制系统开发自动化故障分析方法，帮助操作员快速识别导致复杂系统故障的关键事件序列

Method: 将EPICS控制系统的日志条目视为自然语言，使用语义嵌入技术转换为上下文向量表示，通过序列感知神经网络对正常操作数据进行训练，为每个事件分配实时异常分数

Result: 能够标记与基线行为的偏差，有效检测系统故障前的异常事件序列

Conclusion: 该方法实现了对复杂系统故障的自动化实时监测和预警，提高了故障诊断效率

Abstract: This paper introduces an automated fault analysis framework for the Advanced
Light Source (ALS) that processes real-time event logs from its EPICS control
system. By treating log entries as natural language, we transform them into
contextual vector representations using semantic embedding techniques. A
sequence-aware neural network, trained on normal operational data, assigns a
real-time anomaly score to each event. This method flags deviations from
baseline behavior, enabling operators to rapidly identify the critical event
sequences that precede complex system failures.

</details>


### [30] [Privacy-Aware In-Context Learning for Large Language Models](https://arxiv.org/abs/2509.13625)
*Bishnu Bhusal,Manoj Acharya,Ramneet Kaur,Colin Samplawski,Anirban Roy,Adam D. Cobb,Rohit Chadha,Susmit Jha*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私的文本生成框架，通过聚合每个token的输出分布来生成高质量合成文本，在保证隐私的同时保持高实用性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在隐私泄露风险，攻击者可能从提示中提取敏感信息，需要开发既能保护隐私又能保持文本质量的生成方法。

Method: 利用差分隐私框架，对私有记录进行推理并聚合每个token的输出分布，通过混合私有和公共推理的简单操作来提升实用性。

Result: 实证评估表明，该方法在上下文学习任务上优于现有最先进方法，能够生成长且连贯的合成文本。

Conclusion: 该方法为隐私保护文本生成提供了一个有前景的方向，在保持高实用性的同时提供强大的隐私保障。

Abstract: Large language models (LLMs) have significantly transformed natural language
understanding and generation, but they raise privacy concerns due to potential
exposure of sensitive information. Studies have highlighted the risk of
information leakage, where adversaries can extract sensitive information
embedded in the prompts. In this work, we introduce a novel private prediction
framework for generating high-quality synthetic text with strong privacy
guarantees. Our approach leverages the Differential Privacy (DP) framework to
ensure worst-case theoretical bounds on information leakage without requiring
any fine-tuning of the underlying models.The proposed method performs inference
on private records and aggregates the resulting per-token output distributions.
This enables the generation of longer and coherent synthetic text while
maintaining privacy guarantees. Additionally, we propose a simple blending
operation that combines private and public inference to further enhance
utility. Empirical evaluations demonstrate that our approach outperforms
previous state-of-the-art methods on in-context-learning (ICL) tasks, making it
a promising direction for privacy-preserving text generation while maintaining
high utility.

</details>


### [31] [Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection](https://arxiv.org/abs/2509.13974)
*Amirhossein Shahbazinia,Jonathan Dan,Jose A. Miranda,Giovanni Ansaloni,David Atienza*

Main category: cs.LG

TL;DR: EpiSMART是一个持续学习框架，用于癫痫发作检测，通过选择性保留高熵和预测为发作的样本，在有限内存和计算资源下实现个性化适应，在CHB-MIT数据集上F1分数提升21%。


<details>
  <summary>Details</summary>
Motivation: 当前癫痫诊断依赖专家分析EEG信号，过程耗时且需要专业知识。需要开发能够适应患者特异性EEG信号特征随时间变化的自动化癫痫发作检测方法。

Method: 提出EpiSMART持续学习框架，使用大小受限的重放缓冲区和信息样本选择策略，通过选择性保留高熵和预测为发作的样本，增量式适应患者特异性EEG信号。

Result: 在CHB-MIT数据集验证中，EpiSMART相比无更新的基线模型F1分数提升21%，平均每天仅需6.46分钟标记数据和6.28次更新，适合可穿戴系统实时部署。

Conclusion: EpiSMART能够在资源受限的现实条件下，有效整合新数据而不损害过去知识，实现鲁棒的个性化癫痫发作检测，推动可穿戴医疗系统的实际应用。

Abstract: Objective: Epilepsy, a prevalent neurological disease, demands careful
diagnosis and continuous care. Seizure detection remains challenging, as
current clinical practice relies on expert analysis of electroencephalography,
which is a time-consuming process and requires specialized knowledge.
Addressing this challenge, this paper explores automated epileptic seizure
detection using deep learning, focusing on personalized continual learning
models that adapt to each patient's unique electroencephalography signal
features, which evolve over time. Methods: In this context, our approach
addresses the challenge of integrating new data into existing models without
catastrophic forgetting, a common issue in static deep learning models. We
propose EpiSMART, a continual learning framework for seizure detection that
uses a size-constrained replay buffer and an informed sample selection strategy
to incrementally adapt to patient-specific electroencephalography signals. By
selectively retaining high-entropy and seizure-predicted samples, our method
preserves critical past information while maintaining high performance with
minimal memory and computational requirements. Results: Validation on the
CHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score
over a trained baseline without updates in all other patients. On average,
EpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,
making it suitable for real-time deployment in wearable systems.
Conclusion:EpiSMART enables robust and personalized seizure detection under
realistic and resource-constrained conditions by effectively integrating new
data into existing models without degrading past knowledge. Significance: This
framework advances automated seizure detection by providing a continual
learning approach that supports patient-specific adaptation and practical
deployment in wearable healthcare systems.

</details>


### [32] [DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis](https://arxiv.org/abs/2509.13633)
*Jeremy Oon,Rakhi Manohar Mepparambath,Ling Feng*

Main category: cs.LG

TL;DR: 提出DeepLogit模型，通过序列约束方法结合深度学习与离散选择模型，在保持参数可解释性的同时提升预测精度


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在规划和政策领域的应用受限于其黑盒特性，需要开发既能保持可解释性又能提高准确性的方法

Method: 采用两步法：先估计线性参数的CNN模型（等价于多项Logit模型），然后约束需要解释的参数值，引入高阶项或Transformer等先进架构

Result: 在真实世界新加坡公交刷卡数据上验证，方法在保持选定参数可解释性的同时，显著提高了模型准确性

Conclusion: 展示了理论驱动的离散选择模型与数据驱动的AI模型相结合的统一方法潜力，可在保持规划政策适用性的同时获得更准确模型

Abstract: Despite the significant progress of deep learning models in multitude of
applications, their adaption in planning and policy related areas remains
challenging due to the black-box nature of these models. In this work, we
develop a set of DeepLogit models that follow a novel sequentially constrained
approach in estimating deep learning models for transport policy analysis. In
the first step of the proposed approach, we estimate a convolutional neural
network (CNN) model with only linear terms, which is equivalent of a
linear-in-parameter multinomial logit model. We then estimate other deep
learning models by constraining the parameters that need interpretability at
the values obtained in the linear-in-parameter CNN model and including higher
order terms or by introducing advanced deep learning architectures like
Transformers. Our approach can retain the interpretability of the selected
parameters, yet provides significantly improved model accuracy than the
discrete choice model. We demonstrate our approach on a transit route choice
example using real-world transit smart card data from Singapore. This study
shows the potential for a unifying approach, where theory-based discrete choice
model (DCM) and data-driven AI models can leverage each other's strengths in
interpretability and predictive power. With the availability of larger datasets
and more complex constructions, such approach can lead to more accurate models
using discrete choice models while maintaining its applicability in planning
and policy-related areas. Our code is available on
https://github.com/jeremyoon/route-choice/ .

</details>


### [33] [Deep Learning-Driven Peptide Classification in Biological Nanopores](https://arxiv.org/abs/2509.14029)
*Samuel Tovey,Julian Hoßbach,Sandro Kuppel,Tobias Ensslen,Jan C. Behrends,Christian Holm*

Main category: cs.LG

TL;DR: 使用小波变换将纳米孔电流信号转换为尺度图图像，通过机器学习实现蛋白质实时分类，准确率达81%，创下新纪录


<details>
  <summary>Details</summary>
Motivation: 开发能够在临床环境中实时分类蛋白质的设备，实现廉价快速的疾病诊断。纳米孔技术虽然有望实现这一目标，但当前信号复杂性限制了分类准确性

Method: 将纳米孔电流信号通过小波变换转换为尺度图图像，捕捉振幅、频率和时间信息，使用机器学习算法进行分类

Result: 在42种肽测试中达到约81%的分类准确率，创下该领域新纪录，并展示了模型迁移技术

Conclusion: 该方法为实时疾病诊断开辟了新途径，向临床点护理蛋白质诊断迈出了重要一步

Abstract: A device capable of performing real time classification of proteins in a
clinical setting would allow for inexpensive and rapid disease diagnosis. One
such candidate for this technology are nanopore devices. These devices work by
measuring a current signal that arises when a protein or peptide enters a
nanometer-length-scale pore. Should this current be uniquely related to the
structure of the peptide and its interactions with the pore, the signals can be
used to perform identification. While such a method would allow for real time
identification of peptides and proteins in a clinical setting, to date, the
complexities of these signals limit their accuracy. In this work, we tackle the
issue of classification by converting the current signals into scaleogram
images via wavelet transforms, capturing amplitude, frequency, and time
information in a modality well-suited to machine learning algorithms. When
tested on 42 peptides, our method achieved a classification accuracy of
~$81\,\%$, setting a new state-of-the-art in the field and taking a step toward
practical peptide/protein diagnostics at the point of care. In addition, we
demonstrate model transfer techniques that will be critical when deploying
these models into real hardware, paving the way to a new method for real-time
disease diagnosis.

</details>


### [34] [Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs](https://arxiv.org/abs/2509.13634)
*Md Bokhtiar Al Zami,Md Raihan Uddin,Dinh C. Nguyen*

Main category: cs.LG

TL;DR: 本文提出了一种结合数字孪生(DT)和零知识联邦学习(zkFed)的创新框架，用于解决无人机辅助联邦学习系统中的能耗、通信效率和安全性问题。


<details>
  <summary>Details</summary>
Motivation: 无人机辅助联邦学习系统面临能耗过高、通信效率低下和安全漏洞等挑战，需要一种综合解决方案来确保系统可靠运行。

Method: 采用数字孪生技术实现实时系统监控和预测性维护，结合零知识证明技术增强安全性，并通过动态分配策略优化无人机飞行路径、传输功率和处理速率。使用块坐标下降和凸优化技术进行优化。

Result: 相比传统联邦学习方法，系统能耗降低高达29.6%，仿真结果显示学习性能、安全性和可扩展性均有显著提升。

Conclusion: 该框架为下一代无人机智能网络提供了一个有前景的解决方案，在能耗优化、安全增强和系统效率方面表现出色。

Abstract: Federated learning (FL) has gained popularity as a privacy-preserving method
of training machine learning models on decentralized networks. However to
ensure reliable operation of UAV-assisted FL systems, issues like as excessive
energy consumption, communication inefficiencies, and security vulnerabilities
must be solved. This paper proposes an innovative framework that integrates
Digital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to
tackle these challenges. UAVs act as mobile base stations, allowing scattered
devices to train FL models locally and upload model updates for aggregation. By
incorporating DT technology, our approach enables real-time system monitoring
and predictive maintenance, improving UAV network efficiency. Additionally,
Zero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification
without exposing sensitive data. To optimize energy efficiency and resource
management, we introduce a dynamic allocation strategy that adjusts UAV flight
paths, transmission power, and processing rates based on network conditions.
Using block coordinate descent and convex optimization techniques, our method
significantly reduces system energy consumption by up to 29.6% compared to
conventional FL approaches. Simulation results demonstrate improved learning
performance, security, and scalability, positioning this framework as a
promising solution for next-generation UAV-based intelligent networks.

</details>


### [35] [Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images](https://arxiv.org/abs/2509.13636)
*Yasin Hasanpoor,Bahram Tarvirdizadeh,Khalil Alipour,Mohammad Ghamari*

Main category: cs.LG

TL;DR: 将多模态生理信号（PPG、GSR、ACC）转换为2D图像矩阵，通过CNN提升压力检测性能的新方法


<details>
  <summary>Details</summary>
Motivation: 传统方法单独处理多模态信号或依赖固定编码，无法有效捕捉时序和跨信号依赖关系，需要更有效的融合表示方法

Method: 将生理信号融合为结构化图像表示，通过系统重组为多种格式，采用多阶段训练流程，利用CNN进行特征提取和分类

Result: 显著提升了分类性能，改善了模型泛化能力和鲁棒性，同时增强了结果的可解释性

Conclusion: 该方法不仅适用于压力检测，可广泛应用于任何涉及多模态生理信号的领域，为可穿戴技术实现更准确、个性化和实时的健康监测铺平道路

Abstract: This study introduces a novel method that transforms multimodal physiological
signalsphotoplethysmography (PPG), galvanic skin response (GSR), and
acceleration (ACC) into 2D image matrices to enhance stress detection using
convolutional neural networks (CNNs). Unlike traditional approaches that
process these signals separately or rely on fixed encodings, our technique
fuses them into structured image representations that enable CNNs to capture
temporal and cross signal dependencies more effectively. This image based
transformation not only improves interpretability but also serves as a robust
form of data augmentation. To further enhance generalization and model
robustness, we systematically reorganize the fused signals into multiple
formats, combining them in a multi stage training pipeline. This approach
significantly boosts classification performance. While demonstrated here in the
context of stress detection, the proposed method is broadly applicable to any
domain involving multimodal physiological signals, paving the way for more
accurate, personalized, and real time health monitoring through wearable
technologies.

</details>


### [36] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-Interleaved是一个将图像-文本交错生成重构为工具使用问题的动态框架，通过强化学习训练LLM智能调度多种视觉工具，在多个基准测试中大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前统一模型只能使用单一工具、局限于合成图像、难以处理需要事实基础或程序化精度的任务的瓶颈问题。

Method: 设计强化学习框架训练中央LLM/MLLM代理智能调度专业视觉工具（在线图像搜索、扩散生成、代码执行、图像编辑），采用结合规则逻辑和LLM/MLLM评估的混合奖励系统。

Result: 在四个基准测试中大幅超越现有方法，达到最先进性能，并引入新的测试时扩展策略进一步提升性能。

Conclusion: LLM-Interleaved框架通过工具使用范式有效解决了图像-文本交错生成的挑战，展示了在动态工具调度方面的强大能力。

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


### [37] [Sequential Data Augmentation for Generative Recommendation](https://arxiv.org/abs/2509.13648)
*Geon Lee,Bhuvesh Kumar,Clark Mingxuan Ju,Tong Zhao,Kijung Shin,Neil Shah,Liam Collins*

Main category: cs.LG

TL;DR: 本文提出了GenPAS框架，系统性地分析了生成式推荐中数据增强策略对模型性能的影响，并通过三个偏置控制步骤统一了现有方法，在多个数据集上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有生成式推荐模型中的数据增强策略往往被简化处理或应用不一致，缺乏系统性和原则性的理解，而不同的增强策略会导致显著的性能差异。

Method: 提出GenPAS框架，将数据增强建模为包含三个偏置控制步骤的随机采样过程：序列采样、目标采样和输入采样，统一了广泛使用的策略作为特例。

Result: 在基准和工业数据集上的广泛实验表明，GenPAS在准确性、数据效率和参数效率方面均优于现有策略。

Conclusion: GenPAS为生成式推荐中的训练数据构建提供了原则性指导，能够灵活控制训练分布，提升模型泛化能力。

Abstract: Generative recommendation plays a crucial role in personalized systems,
predicting users' future interactions from their historical behavior sequences.
A critical yet underexplored factor in training these models is data
augmentation, the process of constructing training data from user interaction
histories. By shaping the training distribution, data augmentation directly and
often substantially affects model generalization and performance. Nevertheless,
in much of the existing work, this process is simplified, applied
inconsistently, or treated as a minor design choice, without a systematic and
principled understanding of its effects.
  Motivated by our empirical finding that different augmentation strategies can
yield large performance disparities, we conduct an in-depth analysis of how
they reshape training distributions and influence alignment with future targets
and generalization to unseen inputs. To systematize this design space, we
propose GenPAS, a generalized and principled framework that models augmentation
as a stochastic sampling process over input-target pairs with three
bias-controlled steps: sequence sampling, target sampling, and input sampling.
This formulation unifies widely used strategies as special cases and enables
flexible control of the resulting training distribution. Our extensive
experiments on benchmark and industrial datasets demonstrate that GenPAS yields
superior accuracy, data efficiency, and parameter efficiency compared to
existing strategies, providing practical guidance for principled training data
construction in generative recommendation.

</details>


### [38] [Controllable Pareto Trade-off between Fairness and Accuracy](https://arxiv.org/abs/2509.13651)
*Yongkang Du,Jieyu Zhao,Yijun Yang,Tianyi Zhou*

Main category: cs.LG

TL;DR: 本文提出CPT方法，通过多目标优化实现公平性与准确性的可控权衡，使用移动平均梯度稳定性和关键参数梯度剪枝技术，在仇恨言论检测和职业分类任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前NLP任务中公平性与准确性的权衡研究通常寻找单一"最优"解决方案，但帕累托前沿存在多种可能解。本文旨在根据用户偏好提供可控的权衡方案。

Method: 提出Controllable Pareto Trade-off (CPT)方法：1) 使用随机梯度的移动平均来稳定公平性更新方向；2) 通过仅保留关键参数的梯度进行梯度剪枝。

Result: 在仇恨言论检测和职业分类任务上的实验表明，CPT能够在帕累托前沿获得比基线方法更高质量的解决方案集合，并展现出更好的可控性，能够精确遵循人工定义的参考向量。

Conclusion: CPT方法有效解决了公平性-准确性权衡的可控性问题，通过多目标优化技术实现了根据用户偏好精确调整模型性能的目标。

Abstract: The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work
focuses on finding a single "optimal" solution to balance the two objectives,
which is limited considering the diverse solutions on the Pareto front. This
work intends to provide controllable trade-offs according to the user's
preference of the two objectives, which is defined as a reference vector. To
achieve this goal, we apply multi-objective optimization (MOO), which can find
solutions from various regions of the Pareto front. However, it is challenging
to precisely control the trade-off due to the stochasticity of the training
process and the high dimentional gradient vectors. Thus, we propose
Controllable Pareto Trade-off (CPT) that can effectively train models to
perform different trade-offs according to users' preferences. CPT 1) stabilizes
the fairness update with a moving average of stochastic gradients to determine
the update direction, and 2) prunes the gradients by only keeping the gradients
of the critical parameters. We evaluate CPT on hate speech detection and
occupation classification tasks. Experiments show that CPT can achieve a
higher-quality set of solutions on the Pareto front than the baseline methods.
It also exhibits better controllability and can precisely follow the
human-defined reference vectors.

</details>


### [39] [RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization](https://arxiv.org/abs/2509.13686)
*Bingsheng Peng,Shutao Zhang,Xi Zheng,Ye Xue,Xinyu Qin,Tsung-Hui Chang*

Main category: cs.LG

TL;DR: RF-LSCM是一个基于辐射场的多域无线信道建模框架，通过物理感知的频率相关衰减模型和点云增强方法，解决了传统单细胞单频段信道建模的局限性，在计算效率和精度上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统局部统计信道建模方法局限于单细胞、单网格和单载波频率分析，无法捕捉复杂的跨域交互，需要新的框架来支持多细胞多频段的信道建模需求。

Method: 提出RF-LSCM框架，使用辐射场联合建模大尺度信号衰减和多径分量；引入物理感知的频率相关衰减模型实现跨频段泛化；采用点云增强方法支持多细胞多网格建模；利用低秩张量表示和分层张量角度建模算法提高计算效率。

Result: 在真实多细胞数据集上的实验表明，RF-LSCM显著优于现有方法：覆盖预测的MAE降低30%，通过多频数据融合实现22%的MAE改进，同时大幅减少GPU内存需求和训练时间。

Conclusion: RF-LSCM通过创新的辐射场建模和高效计算架构，成功解决了传统信道建模方法的局限性，为蜂窝网络优化提供了更准确和高效的多域信道建模解决方案。

Abstract: Accurate localized wireless channel modeling is a cornerstone of cellular
network optimization, enabling reliable prediction of network performance
during parameter tuning. Localized statistical channel modeling (LSCM) is the
state-of-the-art channel modeling framework tailored for cellular network
optimization. However, traditional LSCM methods, which infer the channel's
Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP)
measurements, suffer from critical limitations: they are typically confined to
single-cell, single-grid and single-carrier frequency analysis and fail to
capture complex cross-domain interactions. To overcome these challenges, we
propose RF-LSCM, a novel framework that models the channel APS by jointly
representing large-scale signal attenuation and multipath components within a
radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a
physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the
cross frequency generalization as well as a point-cloud-aided environment
enhanced method to enable multi-cell and multi-grid channel modeling.
Furthermore, to address the computational inefficiency of typical neural
radiance fields, RF-LSCM leverages a low-rank tensor representation,
complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.
This efficient design significantly reduces GPU memory requirements and
training time while preserving fine-grained accuracy. Extensive experiments on
real-world multi-cell datasets demonstrate that RF-LSCM significantly
outperforms state-of-the-art methods, achieving up to a 30% reduction in mean
absolute error (MAE) for coverage prediction and a 22% MAE improvement by
effectively fusing multi-frequency data.

</details>


### [40] [A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks](https://arxiv.org/abs/2509.13717)
*Yifan Yu,Cheuk Hin Ho,Yangshuai Wang*

Main category: cs.LG

TL;DR: 本文提出了一个基于无分布共形预测的PINNs不确定性量化框架，通过校准集构建非共形分数，为PINNs提供具有严格有限样本覆盖保证的分布无关不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有的PINNs不确定性量化方法缺乏严格的统计保证，需要一种能够提供可靠统计保证的UQ框架。

Method: 引入无分布共形预测框架，通过校准集构建非共形分数来校准预测区间；针对空间异方差性，提出局部共形分位数估计方法，实现空间自适应不确定性带。

Result: 在典型PDE系统（阻尼谐振子、泊松、Allen-Cahn和Helmholtz方程）上的系统评估表明，该框架实现了可靠的校准和局部自适应不确定性区间，一致优于启发式UQ方法。

Conclusion: 该工作通过将PINNs与无分布UQ相结合，不仅提高了校准性和可靠性，还为复杂PDE系统的不确定性感知建模开辟了新途径。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving PDEs, yet existing uncertainty quantification (UQ) approaches for
PINNs generally lack rigorous statistical guarantees. In this work, we bridge
this gap by introducing a distribution-free conformal prediction (CP) framework
for UQ in PINNs. This framework calibrates prediction intervals by constructing
nonconformity scores on a calibration set, thereby yielding distribution-free
uncertainty estimates with rigorous finite-sample coverage guarantees for
PINNs. To handle spatial heteroskedasticity, we further introduce local
conformal quantile estimation, enabling spatially adaptive uncertainty bands
while preserving theoretical guarantee. Through systematic evaluations on
typical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz
equations) and comprehensive testing across multiple uncertainty metrics, our
results demonstrate that the proposed framework achieves reliable calibration
and locally adaptive uncertainty intervals, consistently outperforming
heuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work
introduces a general framework that not only enhances calibration and
reliability, but also opens new avenues for uncertainty-aware modeling of
complex PDE systems.

</details>


### [41] [WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data](https://arxiv.org/abs/2509.13725)
*Md Sabbir Ahmed,Noah French,Mark Rucker,Zhiyuan Wang,Taylor Myers-Brower,Kaitlyn Petz,Mehdi Boukhechba,Bethany A. Teachman,Laura E. Barnes*

Main category: cs.LG

TL;DR: 本研究开发了一个基于智能手表的系统，通过心率数据和特质水平测量来预测社交焦虑患者的瞬时焦虑波动，在内部数据集上达到60.4%的平衡准确率，在外部数据集上达到59.1%的准确率，比现有方法提升至少7%。


<details>
  <summary>Details</summary>
Motivation: 社交焦虑是一种常见心理健康问题，但现有研究很少测量和预测日常生活中的瞬时焦虑波动。捕捉这些日内动态对于设计实时个性化干预措施（如JITAIs）至关重要。

Method: 使用定制智能手表系统收集91名社交焦虑大学生的数据，每天进行7次生态瞬时评估。基于10,000多天外部心率数据开发基础模型，迁移表示到研究数据集并进行微调，生成概率预测，再与特质水平测量结合使用元学习器。

Result: 在内部数据集上达到60.4%的平衡准确率；在TILES-18外部数据集上达到59.1%的平衡准确率，比先前工作至少提高7%。

Conclusion: 该方法能有效预测社交焦虑患者的瞬时焦虑波动，具有良好的泛化能力，为实时个性化干预提供了技术基础。

Abstract: Social anxiety is a common mental health condition linked to significant
challenges in academic, social, and occupational functioning. A core feature is
elevated momentary (state) anxiety in social situations, yet little prior work
has measured or predicted fluctuations in this anxiety throughout the day.
Capturing these intra-day dynamics is critical for designing real-time,
personalized interventions such as Just-In-Time Adaptive Interventions
(JITAIs). To address this gap, we conducted a study with socially anxious
college students (N=91; 72 after exclusions) using our custom smartwatch-based
system over an average of 9.03 days (SD = 2.95). Participants received seven
ecological momentary assessments (EMAs) per day to report state anxiety. We
developed a base model on over 10,000 days of external heart rate data,
transferred its representations to our dataset, and fine-tuned it to generate
probabilistic predictions. These were combined with trait-level measures in a
meta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety
detection in our dataset. To evaluate generalizability, we applied the training
approach to a separate hold-out set from the TILES-18 dataset-the same dataset
used for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%
balanced accuracy, outperforming prior work by at least 7%.

</details>


### [42] [State Space Models over Directed Graphs](https://arxiv.org/abs/2509.13735)
*Junzhi She,Xunkai Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: 本文提出了DirGraphSSM，首个将状态空间模型系统扩展到有向图学习的创新方法，通过k-hop ego图序列化有向图，在保持高效训练的同时实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有有向图神经网络面临两个主要挑战：有效捕捉长距离因果依赖关系，以及在处理大规模图数据时平衡准确性和训练效率。现有的图状态空间模型仅适用于无向图，限制了其性能。

Method: 提出DirEgo2Token方法通过k-hop ego图序列化有向图，并在此基础上开发DirGraphSSM架构，通过消息传递机制在有向图上实现状态空间模型。

Result: 在三个代表性有向图学习任务上达到最先进性能，在另外两个任务上获得竞争性性能，训练速度比现有最先进模型快1.5-2倍。

Conclusion: DirGraphSSM成功将有向图学习与状态空间模型相结合，为有向图学习提供了高效且准确的解决方案。

Abstract: Directed graphs are ubiquitous across numerous domains, where the
directionality of edges encodes critical causal dependencies. However, existing
GNNs and graph Transformers tailored for directed graphs face two major
challenges: (1) effectively capturing long-range causal dependencies derived
from directed edges; (2) balancing accuracy and training efficiency when
processing large-scale graph datasets. In recent years, state space models
(SSMs) have achieved substantial progress in causal sequence tasks, and their
variants designed for graphs have demonstrated state-of-the-art accuracy while
maintaining high efficiency across various graph learning benchmarks. However,
existing graph state space models are exclusively designed for undirected
graphs, which limits their performance in directed graph learning. To this end,
we propose an innovative approach DirEgo2Token which sequentializes directed
graphs via k-hop ego graphs. This marks the first systematic extension of state
space models to the field of directed graph learning. Building upon this, we
develop DirGraphSSM, a novel directed graph neural network architecture that
implements state space models on directed graphs via the message-passing
mechanism. Experimental results demonstrate that DirGraphSSM achieves
state-of-the-art performance on three representative directed graph learning
tasks while attaining competitive performance on two additional tasks with
1.5$\times $ to 2$\times $ training speed improvements compared to existing
state-of-the-art models.

</details>


### [43] [ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning](https://arxiv.org/abs/2509.13739)
*Zihou Wu,Yuecheng Li,Tianchi Liao,Jian Lou,Chuan Chen*

Main category: cs.LG

TL;DR: ParaAegis是一个并行保护框架，通过模型分割策略在联邦学习中实现隐私-效用-效率的灵活平衡，使用轻量级差分隐私保护低重要性部分，同态加密保护关键部分。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习保护机制（如差分隐私和同态加密）存在刚性权衡，必须在模型效用和计算效率之间做出选择，缺乏灵活性，阻碍了实际应用。

Method: 提出战略模型分割方案：对模型低范数部分应用轻量级差分隐私，其余部分使用同态加密保护，并通过分布式投票机制确保分割共识。

Result: 理论分析确认了在相同隐私保护下效率与效用之间的可调节性。实验结果表明通过调整超参数，可以灵活优先考虑模型精度或训练时间。

Conclusion: ParaAegis框架为实践者提供了对隐私-效用-效率平衡的灵活控制，解决了现有保护机制的刚性权衡问题。

Abstract: Federated learning (FL) faces a critical dilemma: existing protection
mechanisms like differential privacy (DP) and homomorphic encryption (HE)
enforce a rigid trade-off, forcing a choice between model utility and
computational efficiency. This lack of flexibility hinders the practical
implementation. To address this, we introduce ParaAegis, a parallel protection
framework designed to give practitioners flexible control over the
privacy-utility-efficiency balance. Our core innovation is a strategic model
partitioning scheme. By applying lightweight DP to the less critical, low norm
portion of the model while protecting the remainder with HE, we create a
tunable system. A distributed voting mechanism ensures consensus on this
partitioning. Theoretical analysis confirms the adjustments between efficiency
and utility with the same privacy. Crucially, the experimental results
demonstrate that by adjusting the hyperparameters, our method enables flexible
prioritization between model accuracy and training time.

</details>


### [44] [ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2509.13753)
*Hyotaek Jeon,Hyunwook Lee,Juwon Kim,Sungahn Ko*

Main category: cs.LG

TL;DR: ST-LINK是一个增强大语言模型时空依赖捕获能力的新框架，通过空间增强注意力和记忆检索前馈网络解决LLM在交通预测中的空间建模限制


<details>
  <summary>Details</summary>
Motivation: 大语言模型在交通预测中显示出潜力，但其主要为序列标记处理设计，难以有效捕获空间依赖关系，特别是在建模空间关系和图结构空间数据方面存在架构不兼容问题

Method: 提出ST-LINK框架，包含两个关键组件：空间增强注意力（SE-Attention）扩展旋转位置嵌入来整合空间相关性；记忆检索前馈网络（MRFFN）动态检索历史模式来捕获复杂时间依赖

Result: 在基准数据集上的综合实验表明，ST-LINK超越了传统的深度学习和LLM方法，能够有效捕获常规交通模式和突变变化

Conclusion: ST-LINK成功解决了LLM在交通预测中的空间建模挑战，通过创新的注意力机制和记忆检索网络显著提升了时空依赖的捕获能力

Abstract: Traffic forecasting represents a crucial problem within intelligent
transportation systems. In recent research, Large Language Models (LLMs) have
emerged as a promising method, but their intrinsic design, tailored primarily
for sequential token processing, introduces notable challenges in effectively
capturing spatial dependencies. Specifically, the inherent limitations of LLMs
in modeling spatial relationships and their architectural incompatibility with
graph-structured spatial data remain largely unaddressed. To overcome these
limitations, we introduce ST-LINK, a novel framework that enhances the
capability of Large Language Models to capture spatio-temporal dependencies.
Its key components are Spatially-Enhanced Attention (SE-Attention) and the
Memory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary
position embeddings to integrate spatial correlations as direct rotational
transformations within the attention mechanism. This approach maximizes spatial
learning while preserving the LLM's inherent sequential processing structure.
Meanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to
capture complex temporal dependencies and improve the stability of long-term
forecasting. Comprehensive experiments on benchmark datasets demonstrate that
ST-LINK surpasses conventional deep learning and LLM approaches, and
effectively captures both regular traffic patterns and abrupt changes.

</details>


### [45] [Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning](https://arxiv.org/abs/2509.13763)
*Zongxin Shen,Yanyong Huang,Bin Wang,Jinyuan Chang,Shiyu Liu,Tianrui Li*

Main category: cs.LG

TL;DR: 本文从因果视角分析多视图无监督特征选择，提出CAUSA方法，通过因果正则化模块消除混杂因素导致的伪相关，实现更可靠的特征选择。


<details>
  <summary>Details</summary>
Motivation: 现有多视图无监督特征选择方法依赖特征与聚类标签的相关性，但忽略了混杂因素导致的伪相关性，可能导致选择不相关特征。

Method: 提出CAUSA方法：1)使用广义无监督谱回归模型捕捉特征与共识聚类标签的依赖关系；2)引入因果正则化模块自适应分离混杂因素并学习视图共享样本权重来平衡混杂分布。

Result: 综合实验表明CAUSA优于多个最先进方法。

Conclusion: 这是首个在无监督设置下对因果多视图特征选择的深入研究，CAUSA能有效选择因果信息特征。

Abstract: Multi-view unsupervised feature selection (MUFS) has recently received
increasing attention for its promising ability in dimensionality reduction on
multi-view unlabeled data. Existing MUFS methods typically select
discriminative features by capturing correlations between features and
clustering labels. However, an important yet underexplored question remains:
\textit{Are such correlations sufficiently reliable to guide feature
selection?} In this paper, we analyze MUFS from a causal perspective by
introducing a novel structural causal model, which reveals that existing
methods may select irrelevant features because they overlook spurious
correlations caused by confounders. Building on this causal perspective, we
propose a novel MUFS method called CAusal multi-view Unsupervised feature
Selection leArning (CAUSA). Specifically, we first employ a generalized
unsupervised spectral regression model that identifies informative features by
capturing dependencies between features and consensus clustering labels. We
then introduce a causal regularization module that can adaptively separate
confounders from multi-view data and simultaneously learn view-shared sample
weights to balance confounder distributions, thereby mitigating spurious
correlations. Thereafter, integrating both into a unified learning framework
enables CAUSA to select causally informative features. Comprehensive
experiments demonstrate that CAUSA outperforms several state-of-the-art
methods. To our knowledge, this is the first in-depth study of causal
multi-view feature selection in the unsupervised setting.

</details>


### [46] [Floating-Body Hydrodynamic Neural Networks](https://arxiv.org/abs/2509.13783)
*Tianshuo Zhang,Wenzhe Zhai,Rui Yann,Jia Gao,He Cao,Xianglei Xing*

Main category: cs.LG

TL;DR: 提出了FHNN框架，通过物理结构化的神经网络预测可解释的水动力参数，结合解析运动方程，在涡流数据集上比Neural ODEs误差低一个数量级，能有效处理耗散动力学并保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒神经网络模型在流体-结构相互作用中回归状态导数时存在可解释性差和长期预测不稳定的问题，需要一种既能保持物理一致性又能处理耗散动力学的透明建模方法。

Method: 使用物理结构化的Floating-Body Hydrodynamic Neural Networks (FHNN)框架，预测可解释的水动力参数（如方向附加质量、阻力系数和基于流函数的流动），并将其与解析运动方程耦合。

Result: 在合成涡流数据集上，FHNN比Neural ODEs的误差低一个数量级，能够恢复物理一致的流场，相比哈密顿和拉格朗日神经网络更有效地处理耗散动力学。

Conclusion: FHNN填补了黑盒学习和透明系统识别之间的差距，通过约束假设空间、增强可解释性和稳定积分过程，为流体-结构相互作用提供了有效的物理结构化建模框架。

Abstract: Fluid-structure interaction is common in engineering and natural systems,
where floating-body motion is governed by added mass, drag, and background
flows. Modeling these dissipative dynamics is difficult: black-box neural
models regress state derivatives with limited interpretability and unstable
long-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks
(FHNN), a physics-structured framework that predicts interpretable hydrodynamic
parameters such as directional added masses, drag coefficients, and a
streamfunction-based flow, and couples them with analytic equations of motion.
This design constrains the hypothesis space, enhances interpretability, and
stabilizes integration. On synthetic vortex datasets, FHNN achieves up to an
order-of-magnitude lower error than Neural ODEs, recovers physically consistent
flow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN
more effectively handles dissipative dynamics while preserving
interpretability, which bridges the gap between black-box learning and
transparent system identification.

</details>


### [47] [Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment](https://arxiv.org/abs/2509.13818)
*Zheng-an Wang,Yanbo J. Wang,Jiachi Zhang,Qi Xu,Yilun Zhao,Jintao Li,Yipeng Zhang,Bo Yang,Xinkai Gao,Xiaofeng Cao,Kai Xu,Pengpeng Hao,Xuan Yang,Heng Fan*

Main category: cs.LG

TL;DR: 提出了一种混合量子-经典工作流，用于解决普惠金融中数据稀缺的信用风险评估问题，通过经典机器学习进行特征工程，量子神经网络作为分类器，在真实数据集上取得了优于经典方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决普惠金融中由于数据稀缺和类别不平衡导致的信用风险评估难题，传统方法在此类数据受限场景中效果有限，需要探索量子机器学习的新范式。

Method: 设计混合量子-经典工作流：首先使用经典机器学习模型（逻辑回归、随机森林、XGBoost）进行智能特征工程和降维，然后使用通过参数偏移规则训练的量子神经网络作为核心分类器。

Result: 在279个样本的真实信用数据集上，量子神经网络在模拟中达到0.852±0.027的平均AUC，在Quafu量子云平台的超导处理器上实验获得0.88的AUC，性能超越多个经典基准方法，特别是在召回率指标上表现突出。

Conclusion: 该研究为NISQ时代量子计算在数据受限金融场景中的应用提供了实用蓝图，并为量子机器学习在高风险应用（如普惠金融）中的潜力提供了有价值的实证证据。

Abstract: Quantum Machine Learning (QML) offers a new paradigm for addressing complex
financial problems intractable for classical methods. This work specifically
tackles the challenge of few-shot credit risk assessment, a critical issue in
inclusive finance where data scarcity and imbalance limit the effectiveness of
conventional models. To address this, we design and implement a novel hybrid
quantum-classical workflow. The methodology first employs an ensemble of
classical machine learning models (Logistic Regression, Random Forest, XGBoost)
for intelligent feature engineering and dimensionality reduction. Subsequently,
a Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as
the core classifier. This framework was evaluated through numerical simulations
and deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting
processor. On a real-world credit dataset of 279 samples, our QNN achieved a
robust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive
AUC of 0.88 in the hardware experiment. This performance surpasses a suite of
classical benchmarks, with a particularly strong result on the recall metric.
This study provides a pragmatic blueprint for applying quantum computing to
data-constrained financial scenarios in the NISQ era and offers valuable
empirical evidence supporting its potential in high-stakes applications like
inclusive finance.

</details>


### [48] [An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction](https://arxiv.org/abs/2509.13841)
*Qingqi Zhao,Heng Xiao*

Main category: cs.LG

TL;DR: 提出了一种端到端可微分混合框架，将图神经网络嵌入孔隙网络模型中，用于多孔介质渗透率预测，避免了传统方法的理想化几何假设，提高了准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动模型缺乏跨尺度泛化能力且不包含显式物理约束，而孔隙网络模型依赖理想化几何假设导致在复杂结构中精度有限，需要结合两者优势的新方法。

Method: 使用图神经网络替代孔隙网络模型中的解析公式进行水力传导度计算，通过端到端可微分训练，仅需渗透率标量作为训练目标，无需标记的传导度数据。

Result: 该模型实现了高精度和良好的跨尺度泛化能力，优于纯数据驱动和传统孔隙网络模型方法，梯度敏感性分析显示物理一致的特征影响。

Conclusion: 该方法为复杂多孔介质渗透率预测提供了可扩展且物理信息丰富的框架，减少了模型不确定性并提高了准确性。

Abstract: Accurate prediction of permeability in porous media is essential for modeling
subsurface flow. While pure data-driven models offer computational efficiency,
they often lack generalization across scales and do not incorporate explicit
physical constraints. Pore network models (PNMs), on the other hand, are
physics-based and efficient but rely on idealized geometric assumptions to
estimate pore-scale hydraulic conductance, limiting their accuracy in complex
structures. To overcome these limitations, we present an end-to-end
differentiable hybrid framework that embeds a graph neural network (GNN) into a
PNM. In this framework, the analytical formulas used for conductance
calculations are replaced by GNN-based predictions derived from pore and throat
features. The predicted conductances are then passed to the PNM solver for
permeability computation. In this way, the model avoids the idealized geometric
assumptions of PNM while preserving the physics-based flow calculations. The
GNN is trained without requiring labeled conductance data, which can number in
the thousands per pore network; instead, it learns conductance values by using
a single scalar permeability as the training target. This is made possible by
backpropagating gradients through both the GNN (via automatic differentiation)
and the PNM solver (via a discrete adjoint method), enabling fully coupled,
end-to-end training. The resulting model achieves high accuracy and generalizes
well across different scales, outperforming both pure data-driven and
traditional PNM approaches. Gradient-based sensitivity analysis further reveals
physically consistent feature influences, enhancing model interpretability.
This approach offers a scalable and physically informed framework for
permeability prediction in complex porous media, reducing model uncertainty and
improving accuracy.

</details>


### [49] [Graph-Regularized Learning of Gaussian Mixture Models](https://arxiv.org/abs/2509.13855)
*Shamsiiat Abdurakhmanova,Alex Jung*

Main category: cs.LG

TL;DR: 提出了一种在分布式环境下基于图正则化的高斯混合模型学习方法，利用相似性图指导节点间参数共享，避免原始数据传输，在异构小样本场景下优于集中式和本地训练方法


<details>
  <summary>Details</summary>
Motivation: 解决分布式环境中数据异构且样本有限的情况下，如何有效学习高斯混合模型而不需要传输原始数据的问题

Method: 使用图正则化方法，通过提供的相似性图来指导不同节点间的参数共享，实现灵活的邻居参数聚合

Result: 该方法在异构、小样本条件下表现优于集中式训练和本地单独训练的高斯混合模型

Conclusion: 图正则化的分布式高斯混合模型学习方法能够有效处理数据异构和样本限制问题，提供了一种隐私保护的分布式学习方案

Abstract: We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in
distributed settings with heterogeneous and limited local data. The method
exploits a provided similarity graph to guide parameter sharing among nodes,
avoiding the transfer of raw data. The resulting model allows for flexible
aggregation of neighbors' parameters and outperforms both centralized and
locally trained GMMs in heterogeneous, low-sample regimes.

</details>


### [50] [Masked Diffusion Models as Energy Minimization](https://arxiv.org/abs/2509.13866)
*Sitong Chen,Shen Nie,Jiacheng Sun,Zijin Feng,Zhenguo Li,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架，将掩码扩散模型解释为离散最优传输中的能量最小化问题，证明了三种能量公式的数学等价性，并通过Beta分布参数化插值调度，实现了高效的采样改进。


<details>
  <summary>Details</summary>
Motivation: 统一掩码扩散模型的理论基础，澄清其数学本质，并为实际采样改进提供理论指导。

Method: 通过数学证明三种能量公式（动能、条件动能和测地线能量）在MDMs结构下的等价性；使用Beta分布参数化插值调度，将调度设计空间简化为2D搜索。

Result: 实验证明，基于能量启发的调度在合成和真实基准测试中优于手工设计的基线方法，特别是在低步数采样设置中表现突出。

Conclusion: 该研究不仅为掩码扩散模型提供了统一的理论解释，还提出了实用的调度优化方法，显著提升了采样效率和质量。

Abstract: We present a systematic theoretical framework that interprets masked
diffusion models (MDMs) as solutions to energy minimization problems in
discrete optimal transport. Specifically, we prove that three distinct energy
formulations--kinetic, conditional kinetic, and geodesic energy--are
mathematically equivalent under the structure of MDMs, and that MDMs minimize
all three when the mask schedule satisfies a closed-form optimality condition.
This unification not only clarifies the theoretical foundations of MDMs, but
also motivates practical improvements in sampling. By parameterizing
interpolation schedules via Beta distributions, we reduce the schedule design
space to a tractable 2D search, enabling efficient post-training tuning without
model modification. Experiments on synthetic and real-world benchmarks
demonstrate that our energy-inspired schedules outperform hand-crafted
baselines, particularly in low-step sampling settings.

</details>


### [51] [FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning](https://arxiv.org/abs/2509.13895)
*Zhanting Zhou,Jinshan Lai,Fengchun Zhang,Zeqin Wu,Fengli Zhang*

Main category: cs.LG

TL;DR: FedSSG是一种联邦学习中的漂移对齐方法，通过随机采样引导和历史感知机制解决非IID数据和部分参与导致的客户端漂移问题，提高收敛稳定性和准确率


<details>
  <summary>Details</summary>
Motivation: 非IID数据和部分参与会导致联邦学习中的客户端漂移和局部最优不一致，造成收敛不稳定和准确率下降

Method: 维护每个客户端的漂移记忆，积累本地模型差异作为历史梯度的轻量级草图；通过基于参与比率的平滑门控函数控制记忆更新和本地对齐项

Result: 在CIFAR-10/100数据集上，相比基线方法测试准确率提升0.9-2.7个百分点，收敛速度提升约4.5倍

Conclusion: 采样统计可以转化为有原则的历史感知相位控制，稳定并加速联邦训练，方法仅需O(d)客户端内存和常数时间门控

Abstract: Non-IID data and partial participation induce client drift and inconsistent
local optima in federated learning, causing unstable convergence and accuracy
loss. We present FedSSG, a stochastic sampling-guided, history-aware drift
alignment method. FedSSG maintains a per-client drift memory that accumulates
local model differences as a lightweight sketch of historical gradients;
crucially, it gates both the memory update and the local alignment term by a
smooth function of the observed/expected participation ratio (a
phase-by-expectation signal derived from the server sampler). This
statistically grounded gate stays weak and smooth when sampling noise dominates
early, then strengthens once participation statistics stabilize, contracting
the local-global gap without extra communication. Across CIFAR-10/100 with
100/500 clients and 2-15 percent participation, FedSSG consistently outperforms
strong drift-aware baselines and accelerates convergence; on our benchmarks it
improves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and
about +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about
4.5x faster target-accuracy convergence on average. The method adds only O(d)
client memory and a constant-time gate, and degrades gracefully to a mild
regularizer under near-IID or uniform sampling. FedSSG shows that sampling
statistics can be turned into a principled, history-aware phase control to
stabilize and speed up federated training.

</details>


### [52] [TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates](https://arxiv.org/abs/2509.13906)
*Afrin Dange,Sunita Sarawagi*

Main category: cs.LG

TL;DR: TFMAdapter是一个轻量级适配器，无需微调即可为时间序列基础模型添加协变量信息，通过两阶段方法在有限计算开销下显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型无法有效利用领域特定的协变量信息，而这些外生变量在许多应用中对于准确预测至关重要，需要一种轻量级方法来增强基础模型的协变量处理能力。

Method: 提出TFMAdapter适配器，采用两阶段方法：1)使用简单回归模型生成伪预测；2)训练高斯过程回归器，结合伪预测、基础模型预测和协变量信息来优化预测结果。

Result: 在真实数据集上的广泛实验表明，TFMAdapter始终优于基础模型和监督基线，相比基础模型实现了24-27%的性能提升，且数据和计算开销最小。

Conclusion: 轻量级适配器有潜力弥合通用基础模型与领域特定预测需求之间的差距，为时间序列预测提供了高效且有效的协变量整合解决方案。

Abstract: Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art
performance in univariate forecasting on new time series simply by conditioned
on a brief history of past values. Their success demonstrates that large-scale
pretraining across diverse domains can acquire the inductive bias to generalize
from temporal patterns in a brief history. However, most TSFMs are unable to
leverage covariates -- future-available exogenous variables critical for
accurate forecasting in many applications -- due to their domain-specific
nature and the lack of associated inductive bias. We propose TFMAdapter, a
lightweight, instance-level adapter that augments TSFMs with covariate
information without fine-tuning. Instead of retraining, TFMAdapter operates on
the limited history provided during a single model call, learning a
non-parametric cascade that combines covariates with univariate TSFM forecasts.
However, such learning would require univariate forecasts at all steps in the
history, requiring too many calls to the TSFM. To enable training on the full
historical context while limiting TSFM invocations, TFMAdapter uses a two-stage
method: (1) generating pseudo-forecasts with a simple regression model, and (2)
training a Gaussian Process regressor to refine predictions using both pseudo-
and TSFM forecasts alongside covariates. Extensive experiments on real-world
datasets demonstrate that TFMAdapter consistently outperforms both foundation
models and supervised baselines, achieving a 24-27\% improvement over base
foundation models with minimal data and computational overhead. Our results
highlight the potential of lightweight adapters to bridge the gap between
generic foundation models and domain-specific forecasting needs.

</details>


### [53] [APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness](https://arxiv.org/abs/2509.13908)
*Priyobrata Mondal,Faizanuddin Ansari,Swagatam Das*

Main category: cs.LG

TL;DR: 提出了APFEx框架，首个显式建模交叉公平性的联合优化方法，通过自适应多目标优化器、可微交叉公平指标和理论收敛保证，在保持准确性的同时显著减少公平性违规


<details>
  <summary>Details</summary>
Motivation: 现有公平性方法只处理单一敏感属性，无法捕捉交叉子群体面临的复杂多重偏见，需要解决交叉公平性这一关键挑战

Method: APFEx框架包含三个创新：1）自适应多目标优化器动态切换帕累托锥投影、梯度加权和探索策略；2）可微交叉公平指标支持基于梯度的非平滑子群体差异优化；3）理论收敛保证

Result: 在四个真实数据集上的实验表明，APFEx在保持竞争力的准确性的同时，显著减少了公平性违规，优于现有方法

Conclusion: APFEx填补了公平机器学习的关键空白，为交叉公平性提供了可扩展、模型无关的解决方案

Abstract: Ensuring fairness in machine learning models is critical, especially when
biases compound across intersecting protected attributes like race, gender, and
age. While existing methods address fairness for single attributes, they fail
to capture the nuanced, multiplicative biases faced by intersectional
subgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first
framework to explicitly model intersectional fairness as a joint optimization
problem over the Cartesian product of sensitive attributes. APFEx combines
three key innovations- (1) an adaptive multi-objective optimizer that
dynamically switches between Pareto cone projection, gradient weighting, and
exploration strategies to navigate fairness-accuracy trade-offs, (2)
differentiable intersectional fairness metrics enabling gradient-based
optimization of non-smooth subgroup disparities, and (3) theoretical guarantees
of convergence to Pareto-optimal solutions. Experiments on four real-world
datasets demonstrate APFEx's superiority, reducing fairness violations while
maintaining competitive accuracy. Our work bridges a critical gap in fair ML,
providing a scalable, model-agnostic solution for intersectional fairness.

</details>


### [54] [Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction](https://arxiv.org/abs/2509.13914)
*Divya Thuremella,Yi Yang,Simon Wanna,Lars Kunze,Daniele De Martini*

Main category: cs.LG

TL;DR: 本文提出了一种简单的置信度加权平均集成方法，无需重新训练即可将多个先进的轨迹预测模型组合，在NuScenes和Argoverse数据集上实现了10%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶领域不断涌现更大更强的轨迹预测模型，如何在不进行昂贵重新训练的情况下整合这些大型模型的优势成为一个重要挑战。

Method: 使用置信度加权平均方法，直接组合多个最先进的深度学习轨迹预测模型，无需重新训练或微调。

Result: 该方法在NuScenes和Argoverse数据集上均实现了10%的性能提升，特别是在长尾指标上表现优异，且改进在整个数据分布范围内都有效。

Conclusion: 简单的置信度加权平均集成方法能够有效提升轨迹预测性能，证明了无需复杂重新训练即可整合多个先进模型的可行性。

Abstract: This work explores the application of ensemble modeling to the
multidimensional regression problem of trajectory prediction for vehicles in
urban environments. As newer and bigger state-of-the-art prediction models for
autonomous driving continue to emerge, an important open challenge is the
problem of how to combine the strengths of these big models without the need
for costly re-training. We show how, perhaps surprisingly, combining
state-of-the-art deep learning models out-of-the-box (without retraining or
fine-tuning) with a simple confidence-weighted average method can enhance the
overall prediction. Indeed, while combining trajectory prediction models is not
straightforward, this simple approach enhances performance by 10% over the best
prediction model, especially in the long-tailed metrics. We show that this
performance improvement holds on both the NuScenes and Argoverse datasets, and
that these improvements are made across the dataset distribution. The code for
our work is open source.

</details>


### [55] [Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning](https://arxiv.org/abs/2509.13933)
*Qiyue Li,Yingxin Liu,Hang Qi,Jieping Luo,Zhizhang Liu,Jingjin Wu*

Main category: cs.LG

TL;DR: 提出WILF-Q方法，使用Q-learning自适应学习客户端Whittle指数，用于无线联邦学习中的客户端选择，显著提升学习效率


<details>
  <summary>Details</summary>
Motivation: 解决无线联邦学习中客户端选择问题，目标是在达到特定学习精度时减少总时间消耗。由于服务器无法观测客户端的动态状态变化，需要设计无需显式状态转移知识的解决方案

Method: 将客户端选择建模为多臂老虎机问题，提出WILF-Q方法：使用Q-learning自适应学习和更新每个客户端的近似Whittle指数，然后选择指数最高的客户端

Result: 实验结果表明WILF-Q在无线联邦学习环境中显著优于现有基线策略，提供了鲁棒且高效的客户端选择方法

Conclusion: WILF-Q不需要客户端状态转移或数据分布的显式知识，适用于实际联邦学习部署，为无线联邦学习中的客户端选择问题提供了有效的解决方案

Abstract: We consider the client selection problem in wireless Federated Learning (FL),
with the objective of reducing the total required time to achieve a certain
level of learning accuracy. Since the server cannot observe the clients'
dynamic states that can change their computation and communication efficiency,
we formulate client selection as a restless multi-armed bandit problem. We
propose a scalable and efficient approach called the Whittle Index Learning in
Federated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and
update an approximated Whittle index associated with each client, and then
selects the clients with the highest indices. Compared to existing approaches,
WILF-Q does not require explicit knowledge of client state transitions or data
distributions, making it well-suited for deployment in practical FL settings.
Experiment results demonstrate that WILF-Q significantly outperforms existing
baseline policies in terms of learning efficiency, providing a robust and
efficient approach to client selection in wireless FL.

</details>


### [56] [eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems](https://arxiv.org/abs/2509.13952)
*Amin Lotfalian,Mohammad Reza Banan,Pooyan Broumand*

Main category: cs.LG

TL;DR: X-PINN是一种基于物理信息的神经网络扩展框架，用于解决含多裂纹的断裂力学问题，通过能量损失函数、定制积分方案和区域分解方法，结合XFEM思想在神经网络解空间中添加特殊函数来捕捉裂纹不连续性和奇异性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多裂纹断裂力学问题时面临挑战，需要开发一种能够有效捕捉裂纹不连续性和奇异性的新型计算框架。

Method: 提出扩展物理信息神经网络(X-PINN)，采用基于能量的损失函数、定制积分方案和区域分解，借鉴XFEM思想在神经网络解空间中添加特殊函数来显式处理裂纹不连续性和奇异性，使用不同的神经网络分别建模标准和增强解分量。

Result: 数值实验验证了该方法在1D和2D多裂纹问题中的有效性和鲁棒性，并具有良好的扩展到3D问题的能力。

Conclusion: X-PINN框架为复杂多裂纹断裂力学问题提供了一种灵活有效的解决方案，具有处理裂纹不连续性和奇异性的优势。

Abstract: This paper presents eXtended Physics-Informed Neural Network (X-PINN), a
novel and robust framework for addressing fracture mechanics problems involving
multiple cracks in fractured media. To address this, an energy-based loss
function, customized integration schemes, and domain decomposition procedures
are proposed. Inspired by the Extended Finite Element Method (XFEM), the neural
network solution space is enriched with specialized functions that allow crack
body discontinuities and singularities at crack tips to be explicitly captured.
Furthermore, a structured framework is introduced in which standard and
enriched solution components are modeled using distinct neural networks,
enabling flexible and effective simulations of complex multiple-crack problems
in 1D and 2D domains, with convenient extensibility to 3D problems. Numerical
experiments are conducted to validate the effectiveness and robustness of the
proposed method.

</details>


### [57] [Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations](https://arxiv.org/abs/2509.14000)
*Ivana Kesić,Aljaž Blatnik,Carolina Fortuna,Blaž Bertalanič*

Main category: cs.LG

TL;DR: 提出基于动态图回归的GNSS干扰抑制方法，使用异构图卷积LSTM网络实时预测并修正接收机水平偏差，在多种干扰场景下显著优于传统时间序列基线模型


<details>
  <summary>Details</summary>
Motivation: GNSS系统日益受到故意干扰的影响，在需要精确定位和授时的关键时刻降低系统可用性，需要开发有效的干扰抑制方法

Method: 将GNSS干扰抑制重构为动态图回归问题，构建接收机为中心的异构星型图表示卫星接收环境，使用单层异构图卷积LSTM（HeteroGCLSTM）聚合空间上下文和时间动态信息

Result: 在两种接收机和三种干扰模式下，模型在-45dBm干扰强度下达到3.64-7.74cm的MAE，在-60至-70dBm时提升至1.65-2.08cm，混合模式下MAE为3.78-4.25cm，数据效率显著优于基线模型

Conclusion: 该方法能有效实时抑制GNSS干扰，在不同干扰类型和功率水平下均表现优异，且具有出色的数据效率，仅需10%训练数据即可大幅超越基线模型性能

Abstract: Global Navigation Satellite Systems (GNSS) are increasingly disrupted by
intentional jamming, degrading availability precisely when positioning and
timing must remain operational. We address this by reframing jamming mitigation
as dynamic graph regression and introducing a receiver-centric deep temporal
graph network that predicts, and thus corrects, the receivers horizontal
deviation in real time. At each 1 Hz epoch, the satellite receiver environment
is represented as a heterogeneous star graph (receiver center, tracked
satellites as leaves) with time varying attributes (e.g., SNR, azimuth,
elevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM
(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a
short history to output the 2D deviation vector applied for on the fly
correction.
  We evaluate on datasets from two distinct receivers under three jammer
profiles, continuous wave (cw), triple tone (cw3), and wideband FM, each
exercised at six power levels between -45 and -70 dBm, with 50 repetitions per
scenario (prejam/jam/recovery). Against strong multivariate time series
baselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains
the lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm
(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and
4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode
datasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),
outperforming Seq2Point, MLP, and CNN. A split study shows superior data
efficiency: with only 10\% training data our approach remains well ahead of
baselines (20 cm vs. 36-42 cm).

</details>


### [58] [Differentially private federated learning for localized control of infectious disease dynamics](https://arxiv.org/abs/2509.14024)
*Raouf Kerkouche,Henrik Zunker,Mario Fritz,Martin J. Kühn*

Main category: cs.LG

TL;DR: 提出基于联邦学习和差分隐私的隐私保护流行病预测方法，在德国县级层面实现本地化预测，在保持数据隐私的同时达到接近非隐私保护模型的预测性能


<details>
  <summary>Details</summary>
Motivation: 在流行病爆发时需要快速反应，但本地化机器学习模型面临数据不足的问题，而集中数据又存在隐私敏感性挑战。需要找到既能保护隐私又能提供详细情境数据的预测方法

Method: 采用联邦学习框架，以县/社区作为客户端，使用多层感知机在滑动窗口上进行病例数预测。客户端只交换经过范数裁剪的更新，服务器使用差分隐私噪声聚合更新

Result: 在适度隐私保护水平下，DP模型接近非DP模型性能：2020年11月R²=0.94（vs 0.95），MAPE=26%；2022年3月R²=0.88（vs 0.93），MAPE=21%

Conclusion: 客户端级差分隐私联邦学习能够提供有用的县级预测，具有强大的隐私保证。可行的隐私预算取决于流行病阶段，允许卫生当局进行隐私合规的本地预测协作

Abstract: In times of epidemics, swift reaction is necessary to mitigate epidemic
spreading. For this reaction, localized approaches have several advantages,
limiting necessary resources and reducing the impact of interventions on a
larger scale. However, training a separate machine learning (ML) model on a
local scale is often not feasible due to limited available data. Centralizing
the data is also challenging because of its high sensitivity and privacy
constraints. In this study, we consider a localized strategy based on the
German counties and communities managed by the related local health authorities
(LHA). For the preservation of privacy to not oppose the availability of
detailed situational data, we propose a privacy-preserving forecasting method
that can assist public health experts and decision makers. ML methods with
federated learning (FL) train a shared model without centralizing raw data.
Considering the counties, communities or LHAs as clients and finding a balance
between utility and privacy, we study a FL framework with client-level
differential privacy (DP). We train a shared multilayer perceptron on sliding
windows of recent case counts to forecast the number of cases, while clients
exchange only norm-clipped updates and the server aggregated updates with DP
noise. We evaluate the approach on COVID-19 data on county-level during two
phases. As expected, very strict privacy yields unstable, unusable forecasts.
At a moderately strong level, the DP model closely approaches the non-DP model:
$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in
November 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,
client-level DP-FL can deliver useful county-level predictions with strong
privacy guarantees, and viable privacy budgets depend on epidemic phase,
allowing privacy-compliant collaboration among health authorities for local
forecasting.

</details>


### [59] [Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing](https://arxiv.org/abs/2509.14061)
*Chiara De Luca,Elisa Donati*

Main category: cs.LG

TL;DR: 提出基于温度、湿度和压力差的环境传感器融合轻量级系统，使用量化决策树在STM32微控制器上实现实时低功耗蜂王检测，准确率超99%


<details>
  <summary>Details</summary>
Motivation: 传统蜂王检测方法依赖人工检查，劳动强度大且干扰蜂群；现有音频方法功耗高、预处理复杂且易受环境噪声影响

Method: 采用环境传感器融合（内外蜂箱温湿度压力差），在商用STM32微控制器上部署量化决策树推理，实现边缘实时计算

Result: 仅使用环境输入即可达到99%以上的蜂王检测准确率，音频特征未带来显著性能提升

Conclusion: 提供了一种可扩展、可持续的非侵入式蜂箱监测解决方案，为使用现成节能硬件的自主精准养蜂铺平道路

Abstract: Queen bee presence is essential for the health and stability of honeybee
colonies, yet current monitoring methods rely on manual inspections that are
labor-intensive, disruptive, and impractical for large-scale beekeeping. While
recent audio-based approaches have shown promise, they often require high power
consumption, complex preprocessing, and are susceptible to ambient noise. To
overcome these limitations, we propose a lightweight, multimodal system for
queen detection based on environmental sensor fusion-specifically, temperature,
humidity, and pressure differentials between the inside and outside of the
hive. Our approach employs quantized decision tree inference on a commercial
STM32 microcontroller, enabling real-time, low-power edge computing without
compromising accuracy. We show that our system achieves over 99% queen
detection accuracy using only environmental inputs, with audio features
offering no significant performance gain. This work presents a scalable and
sustainable solution for non-invasive hive monitoring, paving the way for
autonomous, precision beekeeping using off-the-shelf, energy-efficient
hardware.

</details>


### [60] [Online Bayesian Risk-Averse Reinforcement Learning](https://arxiv.org/abs/2509.14077)
*Yuhao Wang,Enlu Zhou*

Main category: cs.LG

TL;DR: 本文研究强化学习中的贝叶斯风险规避方法，通过BRMDP处理模型参数不确定性，证明了贝叶斯风险值函数相对于真实值函数的渐近正态性，并在线性RL和CMAB中建立了次线性遗憾界。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中由于数据不足导致的认知不确定性问题，通过贝叶斯风险规避方法来处理未知基础模型的参数不确定性。

Method: 采用贝叶斯风险马尔可夫决策过程(BRMDP)，推导贝叶斯风险值函数与原始值函数差异的渐近正态性，并在在线RL和CMAB中使用后验采样方法。

Result: 贝叶斯风险规避方法会悲观地低估原始值函数，这种差异随风险厌恶程度增强而增大，随数据增加而减小。建立了次线性遗憾界，并通过数值实验验证了有效性。

Conclusion: 贝叶斯风险规避方法能有效处理认知不确定性，在在线RL和CMAB中表现出良好的理论性质和实际效果，风险厌恶程度和数据量是影响性能的关键因素。

Abstract: In this paper, we study the Bayesian risk-averse formulation in reinforcement
learning (RL). To address the epistemic uncertainty due to a lack of data, we
adopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the
parameter uncertainty of the unknown underlying model. We derive the asymptotic
normality that characterizes the difference between the Bayesian risk value
function and the original value function under the true unknown distribution.
The results indicate that the Bayesian risk-averse approach tends to
pessimistically underestimate the original value function. This discrepancy
increases with stronger risk aversion and decreases as more data become
available. We then utilize this adaptive property in the setting of online RL
as well as online contextual multi-arm bandits (CMAB), a special case of online
RL. We provide two procedures using posterior sampling for both the general RL
problem and the CMAB problem. We establish a sub-linear regret bound, with the
regret defined as the conventional regret for both the RL and CMAB settings.
Additionally, we establish a sub-linear regret bound for the CMAB setting with
the regret defined as the Bayesian risk regret. Finally, we conduct numerical
experiments to demonstrate the effectiveness of the proposed algorithm in
addressing epistemic uncertainty and verifying the theoretical properties.

</details>


### [61] [Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques](https://arxiv.org/abs/2509.14078)
*Robiul Islam,Dmitry I. Ignatov,Karl Kaberg,Roman Nabatchikov*

Main category: cs.LG

TL;DR: 该研究比较了不同优化器和神经网络架构在EEG频段分类中的性能，发现Adagrad和RMSprop优化器表现稳定，CNN在空间特征提取方面表现优异，SHAP分析揭示了EEG频段对分类准确性的贡献。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同优化器和神经网络架构在EEG频段分类任务中的性能差异，以及评估左右半球高效分类预测的方法，为神经影像分类任务提供优化策略和特征重要性分析。

Method: 使用TensorFlow和PyTorch框架实现三种神经网络架构（深度密集网络、浅层三层网络和CNN），比较多种优化器（Adagrad、RMSprop、Adadelta、SGD、FTRL）在不同EEG频段的性能，并采用SHAP进行特征重要性分析。

Result: Adagrad和RMSprop优化器在不同频段表现稳定，Adagrad在beta频段表现最佳，RMSprop在gamma频段表现最优。CNN获得第二高准确率，深度密集网络在学习复杂模式方面具有竞争力，浅层网络计算效率高但准确率较低。

Conclusion: 优化器选择、模型架构和EEG频段分析对分类器性能至关重要，研究为神经影像分类任务提供了重要的优化策略和特征贡献理解。SHAP分析有助于识别EEG频段对模型准确性的细微贡献。

Abstract: This study investigates classifier performance across EEG frequency bands
using various optimizers and evaluates efficient class prediction for the left
and right hemispheres. Three neural network architectures - a deep dense
network, a shallow three-layer network, and a convolutional neural network
(CNN) - are implemented and compared using the TensorFlow and PyTorch
frameworks. Results indicate that the Adagrad and RMSprop optimizers
consistently perform well across different frequency bands, with Adadelta
exhibiting robust performance in cross-model evaluations. Specifically, Adagrad
excels in the beta band, while RMSprop achieves superior performance in the
gamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among
the models, the CNN demonstrates the second highest accuracy, particularly in
capturing spatial features of EEG data. The deep dense network shows
competitive performance in learning complex patterns, whereas the shallow
three-layer network, sometimes being less accurate, provides computational
efficiency. SHAP (Shapley Additive Explanations) plots are employed to identify
efficient class prediction, revealing nuanced contributions of EEG frequency
bands to model accuracy. Overall, the study highlights the importance of
optimizer selection, model architecture, and EEG frequency band analysis in
enhancing classifier performance and understanding feature importance in
neuroimaging-based classification tasks.

</details>


### [62] [From Distributional to Quantile Neural Basis Models: the case of Electricity Price Forecasting](https://arxiv.org/abs/2509.14113)
*Alessandro Brusaferri,Danial Ramin,Andrea Ballarino*

Main category: cs.LG

TL;DR: 提出了Quantile Neural Basis Model，将分位数广义可加模型的解释性原理融入神经网络框架，在保持预测性能的同时提供模型行为的可解释性。


<details>
  <summary>Details</summary>
Motivation: 虽然神经网络在多水平概率预测中取得了高精度，但理解特征条件输出的底层机制仍然是一个重大挑战，需要提高模型的可解释性。

Method: 利用共享基分解和权重分解，将Quantile Generalized Additive Models的可解释性原则整合到端到端神经网络训练框架中，避免参数分布假设。

Result: 在日前电价预测任务上验证，预测性能与分布和分位数回归神经网络相当，同时通过学习到的从输入特征到输出预测的非线性映射提供有价值的模型行为洞察。

Conclusion: 该方法成功地将可解释性融入神经网络预测框架，在保持预测准确性的同时提供了对模型机制的深入理解，为概率预测领域提供了新的解释性解决方案。

Abstract: While neural networks are achieving high predictive accuracy in multi-horizon
probabilistic forecasting, understanding the underlying mechanisms that lead to
feature-conditioned outputs remains a significant challenge for forecasters. In
this work, we take a further step toward addressing this critical issue by
introducing the Quantile Neural Basis Model, which incorporates the
interpretability principles of Quantile Generalized Additive Models into an
end-to-end neural network training framework. To this end, we leverage shared
basis decomposition and weight factorization, complementing Neural Models for
Location, Scale, and Shape by avoiding any parametric distributional
assumptions. We validate our approach on day-ahead electricity price
forecasting, achieving predictive performance comparable to distributional and
quantile regression neural networks, while offering valuable insights into
model behavior through the learned nonlinear mappings from input features to
output predictions across the horizon.

</details>


### [63] [Breaking the Cycle of Incarceration With Targeted Mental Health Outreach: A Case Study in Machine Learning for Public Policy](https://arxiv.org/abs/2509.14129)
*Kit T. Rodolfa,Erika Salomon,Jin Yao,Steve Yoder,Robert Sullivan,Kevin McGuire,Allie Dickinson,Rob MacDougall,Brian Seidler,Christina Sung,Claire Herdeman,Rayid Ghani*

Main category: cs.LG

TL;DR: 该研究通过预测建模和实地试验，发现对高风险个体进行针对性心理健康干预能有效降低再监禁率，改善心理健康服务使用和减少司法系统介入。


<details>
  <summary>Details</summary>
Motivation: 监狱系统难以有效处理囚犯的心理健康、药物依赖和无家可归等复杂问题，导致再犯罪和监禁循环，特别是对有色人种社区造成严重种族差异。需要创新方法打破这一循环。

Method: 采用预测建模方法识别高风险个体，设计并实施实地试验进行针对性心理健康外展干预，评估模型预测能力和干预效果。

Result: 模型高度预测新监狱收容情况，最高风险组中超过一半人在一年内重返监狱。对最高风险个体的干预在心理健康服务使用、急救调度和司法介入方面效果显著。

Conclusion: 针对性心理健康外展干预对高风险个体最为有效，能够打破监禁循环，改善个体结局和公共安全，特别是对受种族差异影响最严重的群体。

Abstract: Many incarcerated individuals face significant and complex challenges,
including mental illness, substance dependence, and homelessness, yet jails and
prisons are often poorly equipped to address these needs. With little support
from the existing criminal justice system, these needs can remain untreated and
worsen, often leading to further offenses and a cycle of incarceration with
adverse outcomes both for the individual and for public safety, with
particularly large impacts on communities of color that continue to widen the
already extensive racial disparities in criminal justice outcomes. Responding
to these failures, a growing number of criminal justice stakeholders are
seeking to break this cycle through innovative approaches such as
community-driven and alternative approaches to policing, mentoring, community
building, restorative justice, pretrial diversion, holistic defense, and social
service connections. Here we report on a collaboration between Johnson County,
Kansas, and Carnegie Mellon University to perform targeted, proactive mental
health outreach in an effort to reduce reincarceration rates.
  This paper describes the data used, our predictive modeling approach and
results, as well as the design and analysis of a field trial conducted to
confirm our model's predictive power, evaluate the impact of this targeted
outreach, and understand at what level of reincarceration risk outreach might
be most effective. Through this trial, we find that our model is highly
predictive of new jail bookings, with more than half of individuals in the
trial's highest-risk group returning to jail in the following year. Outreach
was most effective among these highest-risk individuals, with impacts on mental
health utilization, EMS dispatches, and criminal justice involvement.

</details>


### [64] [A Compositional Kernel Model for Feature Learning](https://arxiv.org/abs/2509.14158)
*Feng Ruan,Keli Liu,Michael Jordan*

Main category: cs.LG

TL;DR: 该论文研究了一种组合式核岭回归变体，通过坐标重加权进行特征学习，证明了在噪声变量为高斯分布时，全局最小值和驻点都能有效消除噪声坐标，发现拉普拉斯核等ℓ1型核能恢复非线性特征，而高斯核只能恢复线性特征。


<details>
  <summary>Details</summary>
Motivation: 研究组合式核岭回归模型，为组合架构中的特征学习提供简单测试平台，探索变量选择中相关变量的恢复和噪声变量的消除机制。

Method: 采用变分问题形式的组合式核岭回归模型，对输入进行坐标重加权，分析全局最小值和驻点的特征选择性能，比较ℓ1型核（如拉普拉斯核）和高斯核的特征恢复能力。

Result: 证明了当噪声变量为高斯分布时，全局最小值和驻点都能成功消除噪声坐标；发现ℓ1型核能够在驻点恢复非线性效应的特征，而高斯核只能恢复线性特征。

Conclusion: 组合式核岭回归为特征学习提供了有效的测试框架，ℓ1型核在非线性特征恢复方面优于高斯核，这为组合架构中的特征选择提供了重要理论保证和实践指导。

Abstract: We study a compositional variant of kernel ridge regression in which the
predictor is applied to a coordinate-wise reweighting of the inputs. Formulated
as a variational problem, this model provides a simple testbed for feature
learning in compositional architectures. From the perspective of variable
selection, we show how relevant variables are recovered while noise variables
are eliminated. We establish guarantees showing that both global minimizers and
stationary points discard noise coordinates when the noise variables are
Gaussian distributed. A central finding is that $\ell_1$-type kernels, such as
the Laplace kernel, succeed in recovering features contributing to nonlinear
effects at stationary points, whereas Gaussian kernels recover only linear
ones.

</details>


### [65] [Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework](https://arxiv.org/abs/2509.14167)
*Md Rezwan Jaher,Abul Mukid Mohammad Mukaddes,A. B. M. Abdul Malek*

Main category: cs.LG

TL;DR: 提出了一种端到端框架，通过人工智能架构和非侵入性方法从稀疏常规数据中估计青光眼治疗中无法测量的关键参数（如小梁网渗透性），解决了医疗决策中缺乏地面真实数据和计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 青光眼治疗中关键参数（如眼内压的决定因素小梁网渗透性）无法在体内测量，临床医生只能依赖间接替代指标，同时缺乏真实数据和计算成本高的模拟阻碍了预测模型的发展。

Method: 采用多阶段人工智能架构功能分离问题，提出新颖的PCDS数据生成策略避免大量昂贵模拟，将有效计算时间从数年缩短到数小时，并使用贝叶斯引擎量化预测不确定性。

Result: 非侵入性估计的流出设施与最先进的眼压测量技术达成优秀一致，精度堪比直接物理仪器；新推导的渗透性生物标志物在按疾病风险分层临床队列方面表现出高准确性。

Conclusion: 该框架为其他数据稀缺、计算密集型领域的类似逆问题解决提供了可推广的蓝图，展示了在医疗诊断中的潜在应用价值。

Abstract: Many critical healthcare decisions are challenged by the inability to measure
key underlying parameters. Glaucoma, a leading cause of irreversible blindness
driven by elevated intraocular pressure (IOP), provides a stark example. The
primary determinant of IOP, a tissue property called trabecular meshwork
permeability, cannot be measured in vivo, forcing clinicians to depend on
indirect surrogates. This clinical challenge is compounded by a broader
computational one: developing predictive models for such ill-posed inverse
problems is hindered by a lack of ground-truth data and prohibitive cost of
large-scale, high-fidelity simulations. We address both challenges with an
end-to-end framework to noninvasively estimate unmeasurable variables from
sparse, routine data. Our approach combines a multi-stage artificial
intelligence architecture to functionally separate the problem; a novel data
generation strategy we term PCDS that obviates the need for hundreds of
thousands of costly simulations, reducing the effective computational time from
years to hours; and a Bayesian engine to quantify predictive uncertainty. Our
framework deconstructs a single IOP measurement into its fundamental components
from routine inputs only, yielding estimates for the unmeasurable tissue
permeability and a patient's outflow facility. Our noninvasively estimated
outflow facility achieved excellent agreement with state-of-the-art tonography
with precision comparable to direct physical instruments. Furthermore, the
newly derived permeability biomarker demonstrates high accuracy in stratifying
clinical cohorts by disease risk, highlighting its diagnostic potential. More
broadly, our framework establishes a generalizable blueprint for solving
similar inverse problems in other data-scarce, computationally-intensive
domains.

</details>


### [66] [TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits](https://arxiv.org/abs/2509.14169)
*Ziming Wei,Zichen Kong,Yuan Wang,David Z. Pan,Xiyuan Tang*

Main category: cs.LG

TL;DR: TopoSizing是一个端到端框架，通过图算法和LLM代理实现电路层次化理解，并将领域知识整合到贝叶斯优化中，提高模拟电路设计的采样效率和优化效果。


<details>
  <summary>Details</summary>
Motivation: 模拟和混合信号电路设计面临高质量数据短缺和领域知识难以嵌入自动化流程的挑战，传统黑盒优化缺乏电路理解，学习型方法成本高且缺乏通用性。

Method: 首先使用图算法将电路组织为层次化表示，然后LLM代理执行假设-验证-精炼循环进行显式标注，最后将验证后的洞察整合到贝叶斯优化中，通过LLM引导的初始采样和停滞触发信任区域更新。

Result: 该方法能够直接从原始网表实现稳健的电路理解，并将知识转化为优化增益，提高效率同时保持可行性。

Conclusion: TopoSizing框架有效解决了模拟电路设计中的知识嵌入和优化效率问题，为自动化电路设计提供了新的解决方案。

Abstract: Analog and mixed-signal circuit design remains challenging due to the
shortage of high-quality data and the difficulty of embedding domain knowledge
into automated flows. Traditional black-box optimization achieves sampling
efficiency but lacks circuit understanding, which often causes evaluations to
be wasted in low-value regions of the design space. In contrast, learning-based
methods embed structural knowledge but are case-specific and costly to retrain.
Recent attempts with large language models show potential, yet they often rely
on manual intervention, limiting generality and transparency. We propose
TopoSizing, an end-to-end framework that performs robust circuit understanding
directly from raw netlists and translates this knowledge into optimization
gains. Our approach first applies graph algorithms to organize circuits into a
hierarchical device-module-stage representation. LLM agents then execute an
iterative hypothesis-verification-refinement loop with built-in consistency
checks, producing explicit annotations. Verified insights are integrated into
Bayesian optimization through LLM-guided initial sampling and
stagnation-triggered trust-region updates, improving efficiency while
preserving feasibility.

</details>


### [67] [TGPO: Tree-Guided Preference Optimization for Robust Web Agent Reinforcement Learning](https://arxiv.org/abs/2509.14172)
*Ziyuan Chen,Zhenghui Zhao,Zhangye Han,Miancan Liu,Xianhang Ye,Yiqing Li,Hongbo Min,Jinkui Ren,Xiantao Zhang,Guitao Cao*

Main category: cs.LG

TL;DR: TGPO是一个离线强化学习框架，通过树形轨迹表示和过程奖励模型解决Web智能体训练中的信用分配、标注成本和奖励稀疏性问题，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型和视觉语言模型的发展，使用大模型作为Web智能体变得重要，但强化学习训练面临信用分配不当、标注成本过高和奖励稀疏等关键挑战。

Method: 提出Tree-Guided Preference Optimization (TGPO)框架，使用树形轨迹表示合并语义相同的状态消除标签冲突，包含过程奖励模型自动生成细粒度奖励，以及动态权重机制优先处理高影响力决策点。

Result: 在Online-Mind2Web和自建C-WebShop数据集上的实验表明，TGPO显著优于现有方法，以更少的冗余步骤实现更高的成功率。

Conclusion: TGPO框架有效解决了Web智能体训练中的关键问题，为自动化Web交互提供了更高效的解决方案。

Abstract: With the rapid advancement of large language models and vision-language
models, employing large models as Web Agents has become essential for automated
web interaction. However, training Web Agents with reinforcement learning faces
critical challenges including credit assignment misallocation, prohibitively
high annotation costs, and reward sparsity. To address these issues, we propose
Tree-Guided Preference Optimization (TGPO), an offline reinforcement learning
framework that proposes a tree-structured trajectory representation merging
semantically identical states across trajectories to eliminate label conflicts.
Our framework incorporates a Process Reward Model that automatically generates
fine-grained rewards through subgoal progress, redundancy detection, and action
verification. Additionally, a dynamic weighting mechanism prioritizes
high-impact decision points during training. Experiments on Online-Mind2Web and
our self-constructed C-WebShop datasets demonstrate that TGPO significantly
outperforms existing methods, achieving higher success rates with fewer
redundant steps.

</details>


### [68] [Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting](https://arxiv.org/abs/2509.14181)
*Yifan Hu,Jie Yang,Tian Zhou,Peiyuan Liu,Yujin Tang,Rong Jin,Liang Sun*

Main category: cs.LG

TL;DR: TimeAlign是一个轻量级的即插即用框架，通过简单的重构任务学习辅助特征，纠正历史输入与未来输出之间的频率不匹配问题，显著提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习等表示学习方法在时间序列预测中表现不佳，作者认为显式的表示对齐可以弥补输入历史与未来目标之间的分布差距。

Method: 提出TimeAlign框架，通过重构任务学习辅助特征并反馈给任何基础预测器，架构无关且开销极小。

Result: 在8个基准测试中验证了其优越性能，增益主要来自纠正频率不匹配，理论证明增加了学习表示与预测目标之间的互信息。

Conclusion: TimeAlign可作为现代深度学习时间序列预测系统的通用对齐模块，具有架构无关性和可忽略的开销优势。

Abstract: Representation learning techniques like contrastive learning have long been
explored in time series forecasting, mirroring their success in computer vision
and natural language processing. Yet recent state-of-the-art (SOTA) forecasters
seldom adopt these representation approaches because they have shown little
performance advantage. We challenge this view and demonstrate that explicit
representation alignment can supply critical information that bridges the
distributional gap between input histories and future targets. To this end, we
introduce TimeAlign, a lightweight, plug-and-play framework that learns
auxiliary features via a simple reconstruction task and feeds them back to any
base forecaster. Extensive experiments across eight benchmarks verify its
superior performance. Further studies indicate that the gains arises primarily
from correcting frequency mismatches between historical inputs and future
outputs. We also provide a theoretical justification for the effectiveness of
TimeAlign in increasing the mutual information between learned representations
and predicted targets. As it is architecture-agnostic and incurs negligible
overhead, TimeAlign can serve as a general alignment module for modern deep
learning time-series forecasting systems. The code is available at
https://github.com/TROUBADOUR000/TimeAlign.

</details>


### [69] [A Variational Framework for Residual-Based Adaptivity in Neural PDE Solvers and Operator Learning](https://arxiv.org/abs/2509.14198)
*Juan Diego Toscano,Daniel T. Chen,Vivek Oommen,George Em Karniadakis*

Main category: cs.LG

TL;DR: 提出了一个统一的变分框架来形式化基于残差的自适应策略，通过凸变换将离散化选择与误差度量直接关联，系统化设计自适应方案并提升性能


<details>
  <summary>Details</summary>
Motivation: 基于残差的自适应策略在科学机器学习中广泛使用但缺乏理论依据，需要建立统一的理论框架来形式化这些启发式方法

Method: 引入变分框架，通过凸变换整合残差，不同变换对应不同目标函数（指数权重对应均匀误差最小化，线性权重对应二次误差最小化），将自适应加权等价于选择优化原始目标的采样分布

Result: 框架带来三个好处：系统化设计跨范数的自适应方案、通过损失估计器方差减少降低离散化误差、通过改善梯度信噪比增强学习动态，在算子学习中展示显著性能提升

Conclusion: 为基于残差的自适应性提供了理论依据，建立了原则性离散化和训练策略的基础

Abstract: Residual-based adaptive strategies are widely used in scientific machine
learning but remain largely heuristic. We introduce a unifying variational
framework that formalizes these methods by integrating convex transformations
of the residual. Different transformations correspond to distinct objective
functionals: exponential weights target the minimization of uniform error,
while linear weights recover the minimization of quadratic error. Within this
perspective, adaptive weighting is equivalent to selecting sampling
distributions that optimize the primal objective, thereby linking
discretization choices directly to error metrics. This principled approach
yields three benefits: (1) it enables systematic design of adaptive schemes
across norms, (2) reduces discretization error through variance reduction of
the loss estimator, and (3) enhances learning dynamics by improving the
gradient signal-to-noise ratio. Extending the framework to operator learning,
we demonstrate substantial performance gains across optimizers and
architectures. Our results provide a theoretical justification of
residual-based adaptivity and establish a foundation for principled
discretization and training strategies.

</details>


### [70] [A Universal Banach--Bregman Framework for Stochastic Iterations: Unifying Stochastic Mirror Descent, Learning and LLM Training](https://arxiv.org/abs/2509.14216)
*Johnny R. Zhang,Xiaomei Mi,Gaoyuan Du,Qianyi Sun,Shiqi Wang,Jiaxuan Li,Wenhua Zhou*

Main category: cs.LG

TL;DR: 提出了一个开创性的Banach-Bregman框架，将随机优化从Hilbert空间扩展到更一般的Banach空间，统一了多种优化方法并实现了更快的收敛速度


<details>
  <summary>Details</summary>
Motivation: 现有优化理论主要局限于Hilbert空间，无法处理非欧几里得设置，如单纯形上的镜像下降、稀疏学习的Bregman近端方法、信息几何中的自然梯度下降等

Method: 引入Banach-Bregman框架，通过Bregman投影和Bregman-Fejer单调性提供统一模板，建立超松弛方法（λ>2），并在非Hilbert设置中实现灵活的几何结构

Result: 在机器学习（UCI基准）、深度学习（Transformer训练）、强化学习（actor-critic）和大语言模型（WikiText-2与distilGPT-2）上的实验显示，相比经典基线方法收敛速度提升高达20%，方差减小，准确性提高

Conclusion: Banach-Bregman几何成为统一优化理论和AI核心范式实践的基石，为下一代优化奠定了基础

Abstract: Stochastic optimization powers the scalability of modern artificial
intelligence, spanning machine learning, deep learning, reinforcement learning,
and large language model training. Yet, existing theory remains largely
confined to Hilbert spaces, relying on inner-product frameworks and
orthogonality. This paradigm fails to capture non-Euclidean settings, such as
mirror descent on simplices, Bregman proximal methods for sparse learning,
natural gradient descent in information geometry, or
Kullback--Leibler-regularized language model training. Unlike Euclidean-based
Hilbert-space methods, this approach embraces general Banach spaces. This work
introduces a pioneering Banach--Bregman framework for stochastic iterations,
establishing Bregman geometry as a foundation for next-generation optimization.
It (i) provides a unified template via Bregman projections and Bregman--Fejer
monotonicity, encompassing stochastic approximation, mirror descent, natural
gradient, adaptive methods, and mirror-prox; (ii) establishes super-relaxations
($\lambda > 2$) in non-Hilbert settings, enabling flexible geometries and
elucidating their acceleration effect; and (iii) delivers convergence theorems
spanning almost-sure boundedness to geometric rates, validated on synthetic and
real-world tasks. Empirical studies across machine learning (UCI benchmarks),
deep learning (e.g., Transformer training), reinforcement learning
(actor--critic), and large language models (WikiText-2 with distilGPT-2) show
up to 20% faster convergence, reduced variance, and enhanced accuracy over
classical baselines. These results position Banach--Bregman geometry as a
cornerstone unifying optimization theory and practice across core AI paradigms.

</details>


### [71] [Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems](https://arxiv.org/abs/2509.14219)
*Jiaqi Yao,Lewis Mitchell,John Maclean,Hemanth Saratchandran*

Main category: cs.LG

TL;DR: 提出了RKTV-INR去噪框架，结合隐式神经表示、龙格-库塔积分和全变分约束，从噪声观测中重建动力学系统轨迹，为SINDy提供干净的轨迹和导数以识别控制方程。


<details>
  <summary>Details</summary>
Motivation: 非线性动力系统的数据驱动建模常受测量噪声影响，需要有效去噪方法来获得准确的状态轨迹和导数，以便可靠地识别系统控制方程。

Method: 使用隐式神经表示(INR)直接拟合噪声观测，通过龙格-库塔积分和全变分约束确保重建状态符合动力学系统轨迹，利用自动微分获得精确导数，最后用SINDy识别控制方程。

Result: 实验表明该方法能有效抑制噪声，提供精确的导数估计，并实现可靠的系统识别。

Conclusion: RKTV-INR框架成功解决了噪声环境下的动力学系统建模问题，为数据驱动的系统识别提供了有效的去噪和导数估计解决方案。

Abstract: Data-driven modeling of nonlinear dynamical systems is often hampered by
measurement noise. We propose a denoising framework, called Runge-Kutta and
Total Variation Based Implicit Neural Representation (RKTV-INR), that
represents the state trajectory with an implicit neural representation (INR)
fitted directly to noisy observations. Runge-Kutta integration and total
variation are imposed as constraints to ensure that the reconstructed state is
a trajectory of a dynamical system that remains close to the original data. The
trained INR yields a clean, continuous trajectory and provides accurate
first-order derivatives via automatic differentiation. These denoised states
and derivatives are then supplied to Sparse Identification of Nonlinear
Dynamics (SINDy) to recover the governing equations. Experiments demonstrate
effective noise suppression, precise derivative estimation, and reliable system
identification.

</details>


### [72] [Language models' activations linearly encode training-order recency](https://arxiv.org/abs/2509.14223)
*Dmitrii Krasheninnikov,Richard E. Turner,David Krueger*

Main category: cs.LG

TL;DR: 研究发现语言模型的激活值线性编码了信息在训练过程中被学习的时间顺序，通过顺序微调实验证明模型能够区分不同时间学习的信息


<details>
  <summary>Details</summary>
Motivation: 探索语言模型是否以及如何编码信息被学习的时间顺序，这对于理解模型如何处理冲突数据和知识修改具有重要意义

Method: 通过顺序微调Llama-3.2-1B模型在六个不相交但相似的命名实体数据集上，分析激活值的线性编码特性，并使用线性探针和微调方法验证时间信号的检测能力

Result: 激活值中心点在2D子空间中按训练顺序直线排列；线性探针能准确区分早期和晚期实体（约90%准确率）；模型可微调以报告未见实体的训练阶段（约80%准确率）；时间信号与激活幅度、损失或模型置信度等简单差异无关

Conclusion: 语言模型能够通过激活值线性编码来区分信息的学习时间，这一发现对模型处理冲突数据和知识修改的方式具有重要启示意义

Abstract: We show that language models' activations linearly encode when information
was learned during training. Our setup involves creating a model with a known
training order by sequentially fine-tuning Llama-3.2-1B on six disjoint but
otherwise similar datasets about named entities. We find that the average
activations of test samples for the six training datasets encode the training
order: when projected into a 2D subspace, these centroids are arranged exactly
in the order of training and lie on a straight line. Further, we show that
linear probes can accurately (~90%) distinguish "early" vs. "late" entities,
generalizing to entities unseen during the probes' own training. The model can
also be fine-tuned to explicitly report an unseen entity's training stage (~80%
accuracy). Interestingly, this temporal signal does not seem attributable to
simple differences in activation magnitudes, losses, or model confidence. Our
paper demonstrates that models are capable of differentiating information by
its acquisition time, and carries significant implications for how they might
manage conflicting data and respond to knowledge modifications.

</details>


### [73] [NIRVANA: Structured pruning reimagined for large language models compression](https://arxiv.org/abs/2509.14230)
*Mengting Ai,Tianxin Wei,Sirui Chen,Jingrui He*

Main category: cs.LG

TL;DR: NIRVANA是一种新颖的结构化剪枝方法，通过神经正切核理论指导的显著性准则和自适应稀疏分配机制，在保持零样本准确性的同时提升LLM效率，无需昂贵的恢复技术。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM结构化剪枝方法在零样本设置下性能显著下降、需要昂贵恢复技术（如监督微调或适配器插入）的问题。

Method: 使用基于Adam优化动态下神经正切核的一阶显著性准则，结合跨层和模块的自适应稀疏分配机制，以及基于KL散度的校准数据选择策略。

Result: 在Llama3、Qwen和T5模型上的实验表明，NIRVANA在同等稀疏度约束下优于现有结构化剪枝方法。

Conclusion: NIRVANA提供了一个理论上有依据且实用的LLM压缩方法，平衡了零样本准确性保持和微调能力。

Abstract: Structured pruning of large language models (LLMs) offers substantial
efficiency improvements by removing entire hidden units, yet current approaches
often suffer from significant performance degradation, particularly in
zero-shot settings, and necessitate costly recovery techniques such as
supervised fine-tuning (SFT) or adapter insertion. To address these critical
shortcomings, we introduce NIRVANA, a novel pruning method explicitly designed
to balance immediate zero-shot accuracy preservation with robust fine-tuning
capability. Leveraging a first-order saliency criterion derived from the Neural
Tangent Kernel under Adam optimization dynamics, NIRVANA provides a
theoretically grounded pruning strategy that respects essential model training
behaviors. To further address the unique challenges posed by structured
pruning, NIRVANA incorporates an adaptive sparsity allocation mechanism across
layers and modules (attention vs. MLP), which adjusts pruning intensity between
modules in a globally balanced manner. Additionally, to mitigate the high
sensitivity of pruning decisions to calibration data quality, we propose a
simple yet effective KL divergence-based calibration data selection strategy,
ensuring more reliable and task-agnostic pruning outcomes. Comprehensive
experiments conducted on Llama3, Qwen, and T5 models demonstrate that NIRVANA
outperforms existing structured pruning methods under equivalent sparsity
constraints, providing a theoretically sound and practical approach to LLM
compression. The code is available at
https://github.com/iDEA-iSAIL-Lab-UIUC/NIRVANA.

</details>


### [74] [Compute as Teacher: Turning Inference Compute Into Reference-Free Supervision](https://arxiv.org/abs/2509.14234)
*Dulhan Jayalath,Shashwat Goel,Thomas Foster,Parag Jain,Suchin Gururangan,Cheng Zhang,Anirudh Goyal,Alan Schelten*

Main category: cs.LG

TL;DR: Compute as Teacher (CaT) 通过将模型在推理时的探索转化为无参考监督，利用并行rollouts合成参考信号，在可验证和不可验证任务中实现性能提升


<details>
  <summary>Details</summary>
Motivation: 解决后训练中缺乏真实监督信号的问题，探索如何将推理时的计算资源转化为有效的学习信号

Method: 使用当前策略生成一组并行rollouts，通过冻结的初始策略（anchor）协调冲突和遗漏来估计参考信号，在可验证任务中使用程序等价性，在不可验证任务中使用自提议的评分标准和独立LLM法官

Result: 在Gemma 3 4B、Qwen 3 4B和Llama 3.1 8B上显著提升性能（MATH-500上最高+27%，HealthBench上+12%），结合强化学习（CaT-RL）后获得进一步增益（最高+33%和+30%）

Conclusion: CaT能够有效将推理时计算转化为监督信号，合成方法优于选择方法，性能随rollouts数量扩展，训练后的策略可以超越初始教师信号

Abstract: Where do learning signals come from when there is no ground truth in
post-training? We propose turning exploration into supervision through Compute
as Teacher (CaT), which converts the model's own exploration at inference-time
into reference-free supervision by synthesizing a single reference from a group
of parallel rollouts and then optimizing toward it. Concretely, the current
policy produces a group of rollouts; a frozen anchor (the initial policy)
reconciles omissions and contradictions to estimate a reference, turning extra
inference-time compute into a teacher signal. We turn this into rewards in two
regimes: (i) verifiable tasks use programmatic equivalence on final answers;
(ii) non-verifiable tasks use self-proposed rubrics-binary, auditable criteria
scored by an independent LLM judge, with reward given by the fraction
satisfied. Unlike selection methods (best-of-N, majority, perplexity, or judge
scores), synthesis may disagree with the majority and be correct even when all
rollouts are wrong; performance scales with the number of rollouts. As a
test-time procedure, CaT improves Gemma 3 4B, Qwen 3 4B, and Llama 3.1 8B (up
to +27% on MATH-500; +12% on HealthBench). With reinforcement learning
(CaT-RL), we obtain further gains (up to +33% and +30%), with the trained
policy surpassing the initial teacher signal.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [75] [On the Rate of Gaussian Approximation for Linear Regression Problems](https://arxiv.org/abs/2509.14039)
*Marat Khusainov,Marina Sheshukova,Alain Durmus,Sergey Samsonov*

Main category: stat.ML

TL;DR: 该论文研究了在线线性回归任务的高斯近似问题，分析了恒定学习率设置下的收敛速率及其与问题维度d和设计矩阵相关量的显式依赖关系


<details>
  <summary>Details</summary>
Motivation: 研究在线线性回归中高斯近似的收敛性质，特别是在恒定学习率设置下，探索收敛速率与问题维度、设计矩阵特性之间的数学关系

Method: 推导恒定学习率设置下的高斯近似理论，分析收敛速率对维度d和设计矩阵相关量的显式依赖关系，建立数学理论框架

Result: 当迭代次数n已知时，在足够大的样本量n下，获得了阶数为√(log n/n)的正态近似速率

Conclusion: 论文为在线线性回归的高斯近似提供了理论保证，证明了在适当条件下可以获得快速的正态收敛速率，对高维在线学习理论有重要贡献

Abstract: In this paper, we consider the problem of Gaussian approximation for the
online linear regression task. We derive the corresponding rates for the
setting of a constant learning rate and study the explicit dependence of the
convergence rate upon the problem dimension $d$ and quantities related to the
design matrix. When the number of iterations $n$ is known in advance, our
results yield the rate of normal approximation of order $\sqrt{\log{n}/n}$,
provided that the sample size $n$ is large enough.

</details>
