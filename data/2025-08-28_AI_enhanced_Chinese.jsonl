{"id": "2508.19249", "categories": ["cs.LG", "math.DS", "stat.ME", "stat.ML", "37M99"], "pdf": "https://arxiv.org/pdf/2508.19249", "abs": "https://arxiv.org/abs/2508.19249", "authors": ["Jonas S\u00f8eborg Nielsen", "Marcus Galea Jacobsen", "Albert Brincker Olson", "Mads Peter S\u00f8rensen", "Allan Peter Engsig-Karup"], "title": "Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models", "comment": "For public PIR Julia package, see\n  https://github.com/MarcusGalea/PhysicsInformedRegression.jl", "summary": "We present a new efficient hybrid parameter estimation method based on the\nidea, that if nonlinear dynamic models are stated in terms of a system of\nequations that is linear in terms of the parameters, then regularized ordinary\nleast squares can be used to estimate these parameters from time series data.\nWe introduce the term \"Physics-Informed Regression\" (PIR) to describe the\nproposed data-driven hybrid technique as a way to bridge theory and data by use\nof ordinary least squares to efficiently perform parameter estimation of the\nmodel coefficients of different parameter-linear models; providing examples of\nmodels based on nonlinear ordinary equations (ODE) and partial differential\nequations (PDE). The focus is on parameter estimation on a selection of ODE and\nPDE models, each illustrating performance in different model characteristics.\nFor two relevant epidemic models of different complexity and number of\nparameters, PIR is tested and compared against the related technique,\nphysics-informed neural networks (PINN), both on synthetic data generated from\nknown target parameters and on real public Danish time series data collected\nduring the COVID-19 pandemic in Denmark. Both methods were able to estimate the\ntarget parameters, while PIR showed to perform noticeably better, especially on\na compartment model with higher complexity. Given the difference in\ncomputational speed, it is concluded that the PIR method is superior to PINN\nfor the models considered. It is also demonstrated how PIR can be applied to\nestimate the time-varying parameters of a compartment model that is fitted\nusing real Danish data from the COVID-19 pandemic obtained during a period from\n2020 to 2021. The study shows how data-driven and physics-informed techniques\nmay support reliable and fast -- possibly real-time -- parameter estimation in\nparameter-linear nonlinear dynamic models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6b63\u5219\u5316\u6700\u5c0f\u4e8c\u4e58\u7684\u7269\u7406\u4fe1\u606f\u56de\u5f52(PIR)\u65b9\u6cd5\uff0c\u7528\u4e8e\u53c2\u6570\u7ebf\u6027\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u7684\u53c2\u6570\u4f30\u8ba1\uff0c\u5728\u8ba1\u7b97\u901f\u5ea6\u548c\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINN)\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u53c2\u6570\u4f30\u8ba1\u7684\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u53c2\u6570\u7ebf\u6027\u6a21\u578b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5feb\u901f\u51c6\u786e\u4f30\u8ba1\u53c2\u6570\u7684\u65b9\u6cd5\u6765\u8fde\u63a5\u7406\u8bba\u548c\u6570\u636e\u3002", "method": "\u5229\u7528\u53c2\u6570\u7ebf\u6027\u6a21\u578b\u7684\u7279\u70b9\uff0c\u91c7\u7528\u6b63\u5219\u5316\u666e\u901a\u6700\u5c0f\u4e8c\u4e58\u6cd5\u4ece\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u4f30\u8ba1\u53c2\u6570\uff0c\u79f0\u4e3a\u7269\u7406\u4fe1\u606f\u56de\u5f52(PIR)\u3002", "result": "PIR\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9eCOVID-19\u6570\u636e\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u9694\u5ba4\u6a21\u578b\u4e2d\u660e\u663e\u4f18\u4e8ePINN\u65b9\u6cd5\uff0c\u8ba1\u7b97\u901f\u5ea6\u66f4\u5feb\u3002", "conclusion": "PIR\u65b9\u6cd5\u4e3a\u53c2\u6570\u7ebf\u6027\u975e\u7ebf\u6027\u52a8\u6001\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u5feb\u901f\u7684\u53c2\u6570\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5b9e\u65f6\u5e94\u7528\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8ePINN\u3002"}}
{"id": "2508.19263", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19263", "abs": "https://arxiv.org/abs/2508.19263", "authors": ["Anat Heilper", "Doron Singer"], "title": "Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats", "comment": "16 pages 9 images", "summary": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment.", "AI": {"tldr": "\u6269\u5c55ZipNN\u65e0\u635f\u538b\u7f29\u65b9\u6cd5\u81f3\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u683c\u5f0f(FP8/FP4)\uff0c\u901a\u8fc7\u5206\u79bb\u538b\u7f29\u6307\u6570\u548c\u5c3e\u6570\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u9ad8\u8fbe83%\u7684\u538b\u7f29\u6bd4\u3002\u540c\u65f6\u53d1\u73b0LLM\u4e2d\u7684K/V\u7f13\u5b58\u5f20\u91cf\u4e5f\u5177\u6709\u53ef\u538b\u7f29\u6027\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f53\u79ef\u589e\u957f\u548c\u90e8\u7f72\u666e\u53ca\uff0c\u51cf\u5c11\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u7684\u5b58\u50a8\u548c\u4f20\u8f93\u6210\u672c\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u867d\u7136ZipNN\u7b49\u65e0\u635f\u538b\u7f29\u65b9\u6cd5\u5728FP32/BF16\u9ad8\u7cbe\u5ea6\u683c\u5f0f\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9700\u8981\u6269\u5c55\u5230\u6b63\u5728\u4f18\u5316\u63a8\u7406\u6548\u7387\u7684\u4f4e\u7cbe\u5ea6\u683c\u5f0f(FP8/FP4)\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u538b\u7f29\u65b9\u6cd5\uff0c\u5c06\u6307\u6570\u548c\u5c3e\u6570\u7ec4\u4ef6\u5206\u79bb\u5e76\u72ec\u7acb\u5730\u4f7f\u7528\u4f86\u81ea\u4f59\u7f16\u7801\u7684\u65b9\u6cd5\u8fdb\u884c\u538b\u7f29\u3002\u5bf9\u4e8e\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u683c\u5f0f(FP8\u548cFP4)\u8fdb\u884c\u4e86\u6269\u5c55\u5904\u7406\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u538b\u7f29\u6bd4\u6700\u9ad8\u8fbe\u5230BF16\u768462%\u548cFP8\u768483%\u3002\u540c\u65f6\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u4e2d\u4f7f\u7528\u7684\u5173\u952e\u503c(K/V)\u7f13\u5b58\u5f20\u91cf\u4e5f\u5448\u73b0\u51fa\u53ef\u538b\u7f29\u7684\u6a21\u5f0f\uff0c\u80fd\u591f\u5728\u90e8\u7f72\u8fc7\u7a0b\u4e2d\u8282\u7701\u5185\u5b58\u3002", "conclusion": "\u6210\u529f\u5c06ZipNN\u65e0\u635f\u538b\u7f29\u6280\u672f\u6269\u5c55\u5230\u4f4e\u7cbe\u5ea6\u6d6e\u70b9\u683c\u5f0f\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u538b\u7f29\u6548\u679c\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u6a21\u578b\u6743\u91cd\uff0c\u8fd8\u53ef\u4ee5\u5e94\u7528\u4e8eLLM\u4e2d\u7684K/V\u7f13\u5b58\u538b\u7f29\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u5b58\u50a8\u548c\u5185\u5b58\u4f18\u5316\u624b\u6bb5\u3002"}}
{"id": "2508.19277", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19277", "abs": "https://arxiv.org/abs/2508.19277", "authors": ["Xinyu Li", "Tianjin Huang", "Ronghui Mu", "Xiaowei Huang", "Gaojie Jin"], "title": "POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization", "comment": null, "summary": "Recent advances in Chain-of-Thought (CoT) prompting have substantially\nenhanced the reasoning capabilities of large language models (LLMs), enabling\nsophisticated problem-solving through explicit multi-step reasoning traces.\nHowever, these enhanced reasoning processes introduce novel attack surfaces,\nparticularly vulnerabilities to computational inefficiency through\nunnecessarily verbose reasoning chains that consume excessive resources without\ncorresponding performance gains. Prior overthinking attacks typically require\nrestrictive conditions including access to external knowledge sources for data\npoisoning, reliance on retrievable poisoned content, and structurally obvious\ntemplates that limit practical applicability in real-world scenarios. To\naddress these limitations, we propose POT (Prompt-Only OverThinking), a novel\nblack-box attack framework that employs LLM-based iterative optimization to\ngenerate covert and semantically natural adversarial prompts, eliminating\ndependence on external data access and model retrieval. Extensive experiments\nacross diverse model architectures and datasets demonstrate that POT achieves\nsuperior performance compared to other methods.", "AI": {"tldr": "POT\u662f\u4e00\u79cd\u4ec5\u9700\u63d0\u793a\u7684\u9ed1\u76d2\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7LLM\u8fed\u4ee3\u4f18\u5316\u751f\u6210\u9690\u853d\u4e14\u8bed\u4e49\u81ea\u7136\u7684\u5bf9\u6297\u63d0\u793a\uff0c\u4f7f\u6a21\u578b\u4ea7\u751f\u8fc7\u5ea6\u5197\u957f\u7684\u63a8\u7406\u94fe\uff0c\u6d88\u8017\u8ba1\u7b97\u8d44\u6e90\u800c\u4e0d\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8fc7\u5ea6\u601d\u8003\u653b\u51fb\u9700\u8981\u5916\u90e8\u77e5\u8bc6\u6e90\u8fdb\u884c\u6570\u636e\u6295\u6bd2\u3001\u4f9d\u8d56\u53ef\u68c0\u7d22\u7684\u4e2d\u6bd2\u5185\u5bb9\u548c\u4f7f\u7528\u660e\u663e\u6a21\u677f\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u4e0d\u4f9d\u8d56\u5916\u90e8\u6570\u636e\u7684\u66f4\u5b9e\u7528\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528LLM\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u751f\u6210\u8bed\u4e49\u81ea\u7136\u7684\u5bf9\u6297\u63d0\u793a\uff0c\u8bf1\u5bfc\u6a21\u578b\u4ea7\u751f\u4e0d\u5fc5\u8981\u7684\u5197\u957f\u63a8\u7406\u8fc7\u7a0b\uff0c\u65e0\u9700\u5916\u90e8\u6570\u636e\u8bbf\u95ee\u548c\u6a21\u578b\u68c0\u7d22\u3002", "result": "\u5728\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cPOT\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u8bf1\u5bfc\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "POT\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u8fc7\u5ea6\u601d\u8003\u653b\u51fb\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5b9e\u7528\u548c\u6709\u6548\u7684\u9ed1\u76d2\u653b\u51fb\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86CoT\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u65b0\u653b\u51fb\u9762\u3002"}}
{"id": "2508.19750", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19750", "abs": "https://arxiv.org/abs/2508.19750", "authors": ["Binhui Zhang", "Jianwei Ma"], "title": "Fractal Flow: Hierarchical and Interpretable Normalizing Flow via Topic Modeling and Recursive Strategy", "comment": null, "summary": "Normalizing Flows provide a principled framework for high-dimensional density\nestimation and generative modeling by constructing invertible transformations\nwith tractable Jacobian determinants. We propose Fractal Flow, a novel\nnormalizing flow architecture that enhances both expressiveness and\ninterpretability through two key innovations. First, we integrate\nKolmogorov-Arnold Networks and incorporate Latent Dirichlet Allocation into\nnormalizing flows to construct a structured, interpretable latent space and\nmodel hierarchical semantic clusters. Second, inspired by Fractal Generative\nModels, we introduce a recursive modular design into normalizing flows to\nimprove transformation interpretability and estimation accuracy. Experiments on\nMNIST, FashionMNIST, CIFAR-10, and geophysical data demonstrate that the\nFractal Flow achieves latent clustering, controllable generation, and superior\nestimation accuracy.", "AI": {"tldr": "Fractal Flow\u662f\u4e00\u79cd\u65b0\u578b\u6807\u51c6\u5316\u6d41\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408Kolmogorov-Arnold\u7f51\u7edc\u548c\u6f5c\u5728\u72c4\u5229\u514b\u96f7\u5206\u914d\u6765\u6784\u5efa\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u91c7\u7528\u9012\u5f52\u6a21\u5757\u5316\u8bbe\u8ba1\u63d0\u9ad8\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63d0\u5347\u6807\u51c6\u5316\u6d41\u5728\u9ad8\u7ef4\u5bc6\u5ea6\u4f30\u8ba1\u548c\u751f\u6210\u5efa\u6a21\u4e2d\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u540c\u65f6\u6784\u5efa\u7ed3\u6784\u5316\u7684\u53ef\u89e3\u91ca\u6f5c\u5728\u7a7a\u95f4\u548c\u5c42\u6b21\u5316\u8bed\u4e49\u805a\u7c7b\u3002", "method": "1. \u6574\u5408Kolmogorov-Arnold\u7f51\u7edc\u548c\u6f5c\u5728\u72c4\u5229\u514b\u96f7\u5206\u914d(LDA)\u6765\u6784\u5efa\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\n2. \u5f15\u5165\u5206\u5f62\u751f\u6210\u6a21\u578b\u7684\u9012\u5f52\u6a21\u5757\u5316\u8bbe\u8ba1\n3. \u5728MNIST\u3001FashionMNIST\u3001CIFAR-10\u548c\u5730\u7403\u7269\u7406\u6570\u636e\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "Fractal Flow\u5b9e\u73b0\u4e86\u6f5c\u5728\u805a\u7c7b\u3001\u53ef\u63a7\u751f\u6210\u548c\u4f18\u8d8a\u7684\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u67b6\u6784\u6210\u529f\u63d0\u5347\u4e86\u6807\u51c6\u5316\u6d41\u7684\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u9ad8\u7ef4\u5bc6\u5ea6\u4f30\u8ba1\u548c\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19318", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19318", "abs": "https://arxiv.org/abs/2508.19318", "authors": ["Aohan Li", "Miyu Tsuzuki"], "title": "(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems", "comment": null, "summary": "Deep Reinforcement Learning (DRL) has emerged as an efficient approach to\nresource allocation due to its strong capability in handling complex\ndecision-making tasks. However, only limited research has explored the training\nof DRL models with real-world data in practical, distributed Internet of Things\n(IoT) systems. To bridge this gap, this paper proposes a novel framework for\ntraining DRL models in real-world distributed IoT environments. In the proposed\nframework, IoT devices select communication channels using a DRL-based method,\nwhile the DRL model is trained with feedback information. Specifically,\nAcknowledgment (ACK) information is obtained from actual data transmissions\nover the selected channels. Implementation and performance evaluation, in terms\nof Frame Success Rate (FSR), are carried out, demonstrating both the\nfeasibility and the effectiveness of the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u771f\u5b9e\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u73af\u5883\u4e2d\u8bad\u7ec3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7ACK\u53cd\u9988\u4fe1\u606f\u8fdb\u884c\u8bad\u7ec3\uff0c\u63d0\u9ad8\u4e86\u4fe1\u9053\u9009\u62e9\u7684\u5e27\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u63a2\u7d22\u5728\u771f\u5b9e\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u8bad\u7ec3DRL\u6a21\u578b\uff0c\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eDRL\u7684\u65b9\u6cd5\u8ba9\u7269\u8054\u7f51\u8bbe\u5907\u9009\u62e9\u901a\u4fe1\u4fe1\u9053\uff0c\u5e76\u901a\u8fc7\u5b9e\u9645\u6570\u636e\u4f20\u8f93\u7684ACK\u53cd\u9988\u4fe1\u606f\u6765\u8bad\u7ec3DRL\u6a21\u578b\u3002", "result": "\u5b9e\u73b0\u548c\u6027\u80fd\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u6846\u67b6\u5728\u5e27\u6210\u529f\u7387\u65b9\u9762\u65e2\u53ef\u884c\u53c8\u6709\u6548\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5728\u771f\u5b9e\u5206\u5e03\u5f0f\u7269\u8054\u7f51\u73af\u5883\u4e2d\u8bad\u7ec3DRL\u6a21\u578b\u7684\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.19841", "categories": ["stat.ML", "cs.LG", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.19841", "abs": "https://arxiv.org/abs/2508.19841", "authors": ["Fahime Seyedheydari", "Kevin Conley", "Simo S\u00e4rkk\u00e4"], "title": "Conditional Normalizing Flow Surrogate for Monte Carlo Prediction of Radiative Properties in Nanoparticle-Embedded Layers", "comment": "Version of record (publishers PDF) from META 2025 (CC BY). Please\n  cite the proceedings", "summary": "We present a probabilistic, data-driven surrogate model for predicting the\nradiative properties of nanoparticle embedded scattering media. The model uses\nconditional normalizing flows, which learn the conditional distribution of\noptical outputs, including reflectance, absorbance, and transmittance, given\ninput parameters such as the absorption coefficient, scattering coefficient,\nanisotropy factor, and particle size distribution. We generate training data\nusing Monte Carlo radiative transfer simulations, with optical properties\nderived from Mie theory. Unlike conventional neural networks, the conditional\nnormalizing flow model yields full posterior predictive distributions, enabling\nboth accurate forecasts and principled uncertainty quantification. Our results\ndemonstrate that this model achieves high predictive accuracy and reliable\nuncertainty estimates, establishing it as a powerful and efficient surrogate\nfor radiative transfer simulations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\u7684\u6982\u7387\u6570\u636e\u9a71\u52a8\u4ee3\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7eb3\u7c73\u7c92\u5b50\u5d4c\u5165\u6563\u5c04\u4ecb\u8d28\u7684\u8f90\u5c04\u7279\u6027\uff0c\u80fd\u591f\u63d0\u4f9b\u5b8c\u6574\u7684\u540e\u9a8c\u9884\u6d4b\u5206\u5e03\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u65e0\u6cd5\u63d0\u4f9b\u5b8c\u6574\u7684\u540e\u9a8c\u9884\u6d4b\u5206\u5e03\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u63d0\u4f9b\u51c6\u786e\u9884\u6d4b\u548c\u53ef\u9760\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u4ee3\u7406\u6a21\u578b", "method": "\u4f7f\u7528\u6761\u4ef6\u5f52\u4e00\u5316\u6d41\u5b66\u4e60\u5149\u5b66\u8f93\u51fa\uff08\u53cd\u5c04\u7387\u3001\u5438\u6536\u7387\u3001\u900f\u5c04\u7387\uff09\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u8f93\u5165\u53c2\u6570\u5305\u62ec\u5438\u6536\u7cfb\u6570\u3001\u6563\u5c04\u7cfb\u6570\u3001\u5404\u5411\u5f02\u6027\u56e0\u5b50\u548c\u7c92\u5f84\u5206\u5e03\uff0c\u8bad\u7ec3\u6570\u636e\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u8f90\u5c04\u4f20\u8f93\u6a21\u62df\u548cMie\u7406\u8bba\u751f\u6210", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1", "conclusion": "\u8be5\u6a21\u578b\u6210\u4e3a\u8f90\u5c04\u4f20\u8f93\u6a21\u62df\u7684\u5f3a\u5927\u9ad8\u6548\u4ee3\u7406\u5de5\u5177"}}
{"id": "2508.19945", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.19945", "abs": "https://arxiv.org/abs/2508.19945", "authors": ["Zhouyu Zhang", "Chih-Yuan Chiu", "Glen Chou"], "title": "Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions", "comment": null, "summary": "We present an inverse dynamic game-based algorithm to learn parametric\nconstraints from a given dataset of local generalized Nash equilibrium\ninteractions between multiple agents. Specifically, we introduce mixed-integer\nlinear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the\ninteracting agents, which recover constraints consistent with the Nash\nstationarity of the interaction demonstrations. We establish theoretical\nguarantees that our method learns inner approximations of the true safe and\nunsafe sets, as well as limitations of constraint learnability from\ndemonstrations of Nash equilibrium interactions. We also use the interaction\nconstraints recovered by our method to design motion plans that robustly\nsatisfy the underlying constraints. Across simulations and hardware\nexperiments, our methods proved capable of inferring constraints and designing\ninteractive motion plans for various classes of constraints, both convex and\nnon-convex, from interaction demonstrations of agents with nonlinear dynamics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9006\u52a8\u6001\u535a\u5f08\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5c40\u90e8\u5e7f\u4e49\u7eb3\u4ec0\u5747\u8861\u4ea4\u4e92\u6570\u636e\u5b66\u4e60\u53c2\u6570\u5316\u7ea6\u675f\uff0c\u4f7f\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7f16\u7801KKT\u6761\u4ef6\u6765\u6062\u590d\u4e0e\u7eb3\u4ec0\u5e73\u7a33\u6027\u4e00\u81f4\u7684\u7ea6\u675f\u3002", "motivation": "\u4ece\u667a\u80fd\u4f53\u4ea4\u4e92\u6f14\u793a\u4e2d\u5b66\u4e60\u7ea6\u675f\u6761\u4ef6\u5bf9\u4e8e\u7406\u89e3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u884c\u4e3a\u89c4\u5219\u548c\u5b89\u5168\u8fb9\u754c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece\u7eb3\u4ec0\u5747\u8861\u4ea4\u4e92\u4e2d\u6709\u6548\u6062\u590d\u7ea6\u675f\u3002", "method": "\u5f15\u5165\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\u7f16\u7801\u4ea4\u4e92\u667a\u80fd\u4f53\u7684KKT\u6761\u4ef6\uff0c\u901a\u8fc7\u9006\u52a8\u6001\u535a\u5f08\u65b9\u6cd5\u4ece\u7eb3\u4ec0\u5747\u8861\u4ea4\u4e92\u6f14\u793a\u4e2d\u6062\u590d\u53c2\u6570\u5316\u7ea6\u675f\u3002", "result": "\u7406\u8bba\u4fdd\u8bc1\u5b66\u4e60\u5230\u771f\u5b9e\u5b89\u5168\u96c6\u548c\u975e\u5b89\u5168\u96c6\u7684\u5185\u8fd1\u4f3c\uff0c\u80fd\u591f\u5904\u7406\u51f8\u548c\u975e\u51f8\u7ea6\u675f\uff0c\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u6210\u529f\u63a8\u65ad\u7ea6\u675f\u5e76\u8bbe\u8ba1\u6ee1\u8db3\u5e95\u5c42\u7ea6\u675f\u7684\u4ea4\u4e92\u8fd0\u52a8\u89c4\u5212\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u975e\u7ebf\u6027\u52a8\u6001\u667a\u80fd\u4f53\u7684\u7eb3\u4ec0\u5747\u8861\u4ea4\u4e92\u6f14\u793a\u4e2d\u6709\u6548\u5b66\u4e60\u5404\u7c7b\u7ea6\u675f\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u7ea6\u675f\u5b66\u4e60\u548c\u5b89\u5168\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19390", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19390", "abs": "https://arxiv.org/abs/2508.19390", "authors": ["Jana Weber", "Marcel Weber", "Juan Miguel Lopez Alcaraz"], "title": "Depression diagnosis from patient interviews using multimodal machine learning", "comment": "15 pages, 4 figures, source code under\n  https://github.com/UOLMDA2025/Depression", "summary": "Background: Depression is a major public health concern, affecting an\nestimated five percent of the global population. Early and accurate diagnosis\nis essential to initiate effective treatment, yet recognition remains\nchallenging in many clinical contexts. Speech, language, and behavioral cues\ncollected during patient interviews may provide objective markers that support\nclinical assessment.\n  Methods: We developed a diagnostic approach that integrates features derived\nfrom patient interviews, including speech patterns, linguistic characteristics,\nand structured clinical information. Separate models were trained for each\nmodality and subsequently combined through multimodal fusion to reflect the\ncomplexity of real-world psychiatric assessment. Model validity was assessed\nwith established performance metrics, and further evaluated using calibration\nand decision-analytic approaches to estimate potential clinical utility.\n  Results: The multimodal model achieved superior diagnostic accuracy compared\nto single-modality models, with an AUROC of 0.88 and an F1-score of 0.75.\nImportantly, the fused model demonstrated good calibration and offered higher\nnet clinical benefit compared to baseline strategies, highlighting its\npotential to assist clinicians in identifying patients with depression more\nreliably.\n  Conclusion: Multimodal analysis of patient interviews using machine learning\nmay serve as a valuable adjunct to psychiatric evaluation. By combining speech,\nlanguage, and clinical features, this approach provides a robust framework that\ncould enhance early detection of depressive disorders and support\nevidence-based decision-making in mental healthcare.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u878d\u5408\u7684\u6291\u90c1\u75c7\u8bca\u65ad\u65b9\u6cd5\uff0c\u6574\u5408\u8bed\u97f3\u3001\u8bed\u8a00\u548c\u4e34\u5e8a\u7279\u5f81\uff0c\u76f8\u6bd4\u5355\u6a21\u6001\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u6291\u90c1\u75c7\u662f\u5168\u7403\u91cd\u5927\u516c\u5171\u536b\u751f\u95ee\u9898\uff0c\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u5bf9\u6709\u6548\u6cbb\u7597\u81f3\u5173\u91cd\u8981\u3002\u4e34\u5e8a\u8bc6\u522b\u5b58\u5728\u6311\u6218\uff0c\u60a3\u8005\u8bbf\u8c08\u4e2d\u7684\u8bed\u97f3\u3001\u8bed\u8a00\u548c\u884c\u4e3a\u7ebf\u7d22\u53ef\u80fd\u63d0\u4f9b\u5ba2\u89c2\u7684\u8bca\u65ad\u6807\u8bb0\u3002", "method": "\u5f00\u53d1\u591a\u6a21\u6001\u8bca\u65ad\u65b9\u6cd5\uff0c\u6574\u5408\u60a3\u8005\u8bbf\u8c08\u4e2d\u7684\u8bed\u97f3\u6a21\u5f0f\u3001\u8bed\u8a00\u7279\u5f81\u548c\u7ed3\u6784\u5316\u4e34\u5e8a\u4fe1\u606f\u3002\u4e3a\u6bcf\u4e2a\u6a21\u6001\u8bad\u7ec3\u5355\u72ec\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u591a\u6a21\u6001\u878d\u5408\u53cd\u6620\u771f\u5b9e\u7cbe\u795e\u75c5\u5b66\u8bc4\u4f30\u7684\u590d\u6742\u6027\u3002\u4f7f\u7528\u6027\u80fd\u6307\u6807\u3001\u6821\u51c6\u548c\u51b3\u7b56\u5206\u6790\u65b9\u6cd5\u8bc4\u4f30\u6a21\u578b\u6709\u6548\u6027\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u76f8\u6bd4\u5355\u6a21\u6001\u6a21\u578b\u83b7\u5f97\u66f4\u4f18\u8bca\u65ad\u51c6\u786e\u6027\uff0cAUROC\u4e3a0.88\uff0cF1\u5206\u6570\u4e3a0.75\u3002\u878d\u5408\u6a21\u578b\u8868\u73b0\u51fa\u826f\u597d\u6821\u51c6\u6027\uff0c\u76f8\u6bd4\u57fa\u7ebf\u7b56\u7565\u63d0\u4f9b\u66f4\u9ad8\u7684\u4e34\u5e8a\u51c0\u6536\u76ca\u3002", "conclusion": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u60a3\u8005\u8bbf\u8c08\u591a\u6a21\u6001\u5206\u6790\u53ef\u4f5c\u4e3a\u7cbe\u795e\u75c5\u5b66\u8bc4\u4f30\u7684\u6709\u4ef7\u503c\u8f85\u52a9\u5de5\u5177\u3002\u901a\u8fc7\u6574\u5408\u8bed\u97f3\u3001\u8bed\u8a00\u548c\u4e34\u5e8a\u7279\u5f81\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u589e\u5f3a\u6291\u90c1\u75c7\u65e9\u671f\u68c0\u6d4b\u548c\u652f\u6301\u7cbe\u795e\u533b\u7597\u5faa\u8bc1\u51b3\u7b56\u7684\u7a33\u5065\u6846\u67b6\u3002"}}
{"id": "2508.19344", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19344", "abs": "https://arxiv.org/abs/2508.19344", "authors": ["Daniil Zelezetsky", "Egor Cherepanov", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "Re:Frame -- Retrieving Experience From Associative Memory", "comment": "11 pages, 3 figures", "summary": "Offline reinforcement learning (RL) often deals with suboptimal data when\ncollecting large expert datasets is unavailable or impractical. This limitation\nmakes it difficult for agents to generalize and achieve high performance, as\nthey must learn primarily from imperfect or inconsistent trajectories. A\ncentral challenge is therefore how to best leverage scarce expert\ndemonstrations alongside abundant but lower-quality data. We demonstrate that\nincorporating even a tiny amount of expert experience can substantially improve\nRL agent performance. We introduce Re:Frame (Retrieving Experience From\nAssociative Memory), a plug-in module that augments a standard offline RL\npolicy (e.g., Decision Transformer) with a small external Associative Memory\nBuffer (AMB) populated by expert trajectories drawn from a separate dataset.\nDuring training on low-quality data, the policy learns to retrieve expert data\nfrom the Associative Memory Buffer (AMB) via content-based associations and\nintegrate them into decision-making; the same AMB is queried at evaluation.\nThis requires no environment interaction and no modifications to the backbone\narchitecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories\n(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a\nstrong Decision Transformer baseline in three of four settings, with gains up\nto +10.7 normalized points. These results show that Re:Frame offers a simple\nand data-efficient way to inject scarce expert knowledge and substantially\nimprove offline RL from low-quality datasets.", "AI": {"tldr": "Re:Frame\u662f\u4e00\u4e2a\u63d2\u4ef6\u6a21\u5757\uff0c\u901a\u8fc7\u5173\u8054\u8bb0\u5fc6\u7f13\u51b2\u533a\u6574\u5408\u5c11\u91cf\u4e13\u5bb6\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u4f4e\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u901a\u5e38\u9762\u4e34\u6b21\u4f18\u6570\u636e\u95ee\u9898\uff0c\u96be\u4ee5\u83b7\u5f97\u5927\u89c4\u6a21\u4e13\u5bb6\u6570\u636e\u96c6\u3002\u6838\u5fc3\u6311\u6218\u662f\u5982\u4f55\u6709\u6548\u5229\u7528\u7a00\u7f3a\u4e13\u5bb6\u6f14\u793a\u548c\u4e30\u5bcc\u4f46\u4f4e\u8d28\u91cf\u6570\u636e", "method": "\u5f15\u5165Re:Frame\u6a21\u5757\uff0c\u5305\u542b\u5173\u8054\u8bb0\u5fc6\u7f13\u51b2\u533a(AMB)\u5b58\u50a8\u4e13\u5bb6\u8f68\u8ff9\u3002\u7b56\u7565\u901a\u8fc7\u5185\u5bb9\u5173\u8054\u68c0\u7d22\u4e13\u5bb6\u6570\u636e\u5e76\u6574\u5408\u5230\u51b3\u7b56\u4e2d\uff0c\u65e0\u9700\u73af\u5883\u4ea4\u4e92\u6216\u4fee\u6539\u4e3b\u5e72\u67b6\u6784", "result": "\u5728D4RL MuJoCo\u4efb\u52a1\u4e2d\uff0c\u4ec5\u4f7f\u752860\u6761\u4e13\u5bb6\u8f68\u8ff9(\u6570\u636e\u96c6\u76840.1%)\uff0cRe:Frame\u57284\u4e2a\u8bbe\u7f6e\u4e2d\u76843\u4e2a\u4e0a\u663e\u8457\u4f18\u4e8eDecision Transformer\u57fa\u7ebf\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe+10.7\u6807\u51c6\u5316\u5206\u6570", "conclusion": "Re:Frame\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u4e14\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6ce8\u5165\u7a00\u7f3a\u4e13\u5bb6\u77e5\u8bc6\uff0c\u5927\u5e45\u6539\u5584\u4ece\u4f4e\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60"}}
{"id": "2508.19897", "categories": ["stat.ML", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19897", "abs": "https://arxiv.org/abs/2508.19897", "authors": ["Luca Ambrogioni"], "title": "The Information Dynamics of Generative Diffusion", "comment": null, "summary": "Generative diffusion models have emerged as a powerful class of models in\nmachine learning, yet a unified theoretical understanding of their operation is\nstill developing. This perspective paper provides an integrated perspective on\ngenerative diffusion by connecting their dynamic, information-theoretic, and\nthermodynamic properties under a unified mathematical framework. We demonstrate\nthat the rate of conditional entropy production during generation (i.e. the\ngenerative bandwidth) is directly governed by the expected divergence of the\nscore function's vector field. This divergence, in turn, is linked to the\nbranching of trajectories and generative bifurcations, which we characterize as\nsymmetry-breaking phase transitions in the energy landscape. This synthesis\noffers a powerful insight: the process of generation is fundamentally driven by\nthe controlled, noise-induced breaking of (approximate) symmetries, where peaks\nin information transfer correspond to critical transitions between possible\noutcomes. The score function acts as a dynamic non-linear filter that regulates\nthe bandwidth of the noise by suppressing fluctuations that are incompatible\nwith the data.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u751f\u6210\u6269\u6563\u6a21\u578b\u7684\u52a8\u6001\u3001\u4fe1\u606f\u8bba\u548c\u70ed\u529b\u5b66\u7279\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u63ed\u793a\u4e86\u751f\u6210\u8fc7\u7a0b\u7531\u53d7\u63a7\u7684\u566a\u58f0\u8bf1\u5bfc\u5bf9\u79f0\u6027\u7834\u7f3a\u9a71\u52a8\u3002", "motivation": "\u867d\u7136\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5bf9\u5176\u8fd0\u884c\u673a\u5236\u7684\u7edf\u4e00\u7406\u8bba\u7406\u89e3\u4ecd\u5728\u53d1\u5c55\u4e2d\uff0c\u9700\u8981\u5efa\u7acb\u6574\u5408\u7684\u6570\u5b66\u6846\u67b6\u6765\u89e3\u91ca\u5176\u6838\u5fc3\u7279\u6027\u3002", "method": "\u901a\u8fc7\u8fde\u63a5\u6269\u6563\u6a21\u578b\u7684\u52a8\u6001\u7279\u6027\u3001\u4fe1\u606f\u8bba\u5c5e\u6027\u548c\u70ed\u529b\u5b66\u6027\u8d28\uff0c\u5efa\u7acb\u7edf\u4e00\u6570\u5b66\u6846\u67b6\uff0c\u5206\u6790\u6761\u4ef6\u71b5\u4ea7\u751f\u7387\u4e0e\u5f97\u5206\u51fd\u6570\u5411\u91cf\u573a\u6563\u5ea6\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\u751f\u6210\u5e26\u5bbd\u7531\u5f97\u5206\u51fd\u6570\u5411\u91cf\u573a\u7684\u671f\u671b\u6563\u5ea6\u76f4\u63a5\u63a7\u5236\uff0c\u8be5\u6563\u5ea6\u4e0e\u8f68\u8ff9\u5206\u652f\u548c\u751f\u6210\u5206\u53c9\u76f8\u5173\uff0c\u8868\u73b0\u4e3a\u80fd\u91cf\u666f\u89c2\u4e2d\u7684\u5bf9\u79f0\u6027\u7834\u7f3a\u76f8\u53d8\u3002", "conclusion": "\u751f\u6210\u8fc7\u7a0b\u672c\u8d28\u4e0a\u662f\u53d7\u63a7\u7684\u566a\u58f0\u8bf1\u5bfc\u5bf9\u79f0\u6027\u7834\u7f3a\u8fc7\u7a0b\uff0c\u5f97\u5206\u51fd\u6570\u4f5c\u4e3a\u52a8\u6001\u975e\u7ebf\u6027\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u6291\u5236\u4e0e\u6570\u636e\u4e0d\u517c\u5bb9\u7684\u6ce2\u52a8\u6765\u8c03\u8282\u566a\u58f0\u5e26\u5bbd\u3002"}}
{"id": "2508.19994", "categories": ["eess.SP", "cs.SY", "eess.SY", "q-fin.MF"], "pdf": "https://arxiv.org/pdf/2508.19994", "abs": "https://arxiv.org/abs/2508.19994", "authors": ["Noah Shore"], "title": "The Coherent Multiplex: Scalable Real-Time Wavelet Coherence Architecture", "comment": "Submitted to International Symposium for Signal Processing 2025", "summary": "The Coherent Multiplex is formalized and validated as a scalable, real-time\nsystem for identifying, analyzing, and visualizing coherence among multiple\ntime series. Its architecture comprises a fast spectral similarity layer based\non cosine similarity metrics of Fourier-transformed signals, and a sparse\ntime-frequency layer for wavelet coherence. The system constructs and evolves a\nmultilayer graph representing inter-signal relationships, enabling low-latency\ninference and monitoring. A simulation prototype demonstrates functionality\nacross 8 synthetic channels with a high similarity threshold for further\ncomputation, with additional opportunities for scaling the architecture up to\nsupport thousands of input signals with constrained hardware. Applications\ndiscussed include neuroscience, finance, and biomedical signal analysis.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86Coherent Multiplex\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u7cfb\u7edf\uff0c\u7528\u4e8e\u8bc6\u522b\u3001\u5206\u6790\u548c\u53ef\u89c6\u5316\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u4e4b\u95f4\u7684\u76f8\u5e72\u6027\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u548c\u5206\u6790\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u4e4b\u95f4\u76f8\u5e72\u6027\u7684\u53ef\u6269\u5c55\u7cfb\u7edf\uff0c\u5e94\u7528\u4e8e\u795e\u7ecf\u79d1\u5b66\u3001\u91d1\u878d\u548c\u751f\u7269\u533b\u5b66\u4fe1\u53f7\u5206\u6790\u7b49\u9886\u57df\u3002", "method": "\u7cfb\u7edf\u67b6\u6784\u5305\u542b\u57fa\u4e8e\u5085\u91cc\u53f6\u53d8\u6362\u4fe1\u53f7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u7684\u5feb\u901f\u9891\u8c31\u76f8\u4f3c\u6027\u5c42\uff0c\u4ee5\u53ca\u7528\u4e8e\u5c0f\u6ce2\u76f8\u5e72\u6027\u7684\u7a00\u758f\u65f6\u9891\u5c42\u3002\u6784\u5efa\u5e76\u6f14\u5316\u8868\u793a\u4fe1\u53f7\u95f4\u5173\u7cfb\u7684\u591a\u5c42\u56fe\uff0c\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u63a8\u7406\u548c\u76d1\u63a7\u3002", "result": "\u4eff\u771f\u539f\u578b\u57288\u4e2a\u5408\u6210\u901a\u9053\u4e0a\u5c55\u793a\u4e86\u529f\u80fd\uff0c\u4f7f\u7528\u9ad8\u76f8\u4f3c\u5ea6\u9608\u503c\u8fdb\u884c\u8fdb\u4e00\u6b65\u8ba1\u7b97\uff0c\u7cfb\u7edf\u67b6\u6784\u53ef\u6269\u5c55\u81f3\u652f\u6301\u6570\u5343\u4e2a\u8f93\u5165\u4fe1\u53f7\u3002", "conclusion": "Coherent Multiplex\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5b9e\u65f6\u76f8\u5e72\u6027\u5206\u6790\u7cfb\u7edf\uff0c\u5177\u6709\u5f88\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.19408", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.19408", "abs": "https://arxiv.org/abs/2508.19408", "authors": ["Vaclav Pavlicek", "Ayush Bhandari"], "title": "1-Bit Unlimited Sampling Beyond Fourier Domain: Low-Resolution Sampling of Quantization Noise", "comment": "20 pages, accepted to IEEE Journal of Selected Topics in Signal\n  Processing", "summary": "Analog-to-digital converters (ADCs) play a critical role in digital signal\nacquisition across various applications, but their performance is inherently\nconstrained by sampling rates and bit budgets. This bit budget imposes a\ntrade-off between dynamic range (DR) and digital resolution, with ADC energy\nconsumption scaling linearly with sampling rate and exponentially with bit\ndepth. To bypass this, numerous approaches, including oversampling with\nlow-resolution ADCs, have been explored. A prominent example is 1-Bit ADCs with\nSigma-Delta Quantization (SDQ), a widely used consumer-grade solution. However,\nSDQs suffer from overloading or saturation issues, limiting their ability to\nhandle inputs with arbitrary DR. The Unlimited Sensing Framework (USF)\naddresses this challenge by injecting modulo non-linearity in hardware,\nresulting in a new digital sensing technology. In this paper, we introduce a\nnovel 1-Bit sampling architecture that extends both conventional 1-Bit SDQ and\nUSF. Our contributions are twofold: (1) We generalize the concept of noise\nshaping beyond the Fourier domain, allowing the inclusion of non-bandlimited\nsignals in the Fourier domain but bandlimited in alternative transform domains.\n(2) Building on this generalization, we develop a new transform-domain recovery\nmethod for 1-Bit USF. When applied to the Fourier domain, our method\ndemonstrates superior performance compared to existing time-domain techniques,\noffering reduced oversampling requirements and improved robustness. Extensive\nnumerical experiments validate our findings, laying the groundwork for a\nbroader generalization of 1-Bit sampling systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u76841\u6bd4\u7279\u91c7\u6837\u67b6\u6784\uff0c\u901a\u8fc7\u5c06\u566a\u58f0\u5f62\u6210\u6982\u5ff5\u6269\u5c55\u5230\u590d\u6742\u53d8\u6362\u57df\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u91c7\u6837\u6548\u679c\u548c\u6062\u590d\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf1\u6bd4\u7279ADC\u548cSigma-Delta\u91c7\u6837\u5728\u5904\u7406\u4efb\u610f\u52a8\u6001\u8303\u56f4\u4fe1\u53f7\u65f6\u7684\u6fc0\u6d41\u95ee\u9898\uff0c\u7a81\u7834\u4f20\u7edfADC\u5728\u91c7\u6837\u7387\u3001\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u548c\u80fd\u8017\u4e4b\u95f4\u7684\u7279\u6027\u7ea6\u675f\u3002", "method": "\u901a\u8fc7\u5c06\u566a\u58f0\u5f62\u6210\u6982\u5ff5\u4ece\u5080\u91cc\u53f8\u57df\u6269\u5c55\u5230\u5176\u4ed6\u53d8\u6362\u57df\uff0c\u5f00\u53d1\u4e86\u65b0\u7684\u53d8\u6362\u57df\u6062\u590d\u65b9\u6cd5\uff0c\u6784\u5efa\u4e861\u6bd4\u7279\u65e0\u9650\u91c7\u6837\u6846\u67b6\u3002", "result": "\u5728\u5080\u91cc\u53f8\u57df\u5e94\u7528\u65f6\uff0c\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u73b0\u6709\u65f6\u57df\u6280\u672f\uff0c\u51cf\u5c11\u4e86\u8fc7\u91c7\u6837\u8981\u6c42\u5e76\u63d0\u9ad8\u4e86\u7a33\u5065\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u53d1\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a1\u6bd4\u7279\u91c7\u6837\u7cfb\u7edf\u7684\u66f4\u5e7f\u6cdb\u666e\u904d\u5316\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u53d8\u6362\u57df\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u4fe1\u53f7\u91c7\u96c6\u548c\u6062\u590d\u3002"}}
{"id": "2508.19352", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19352", "abs": "https://arxiv.org/abs/2508.19352", "authors": ["Adarsh Jamadandi", "Jing Xu", "Adam Dziedzic", "Franziska Boenisch"], "title": "Memorization in Graph Neural Networks", "comment": null, "summary": "Deep neural networks (DNNs) have been shown to memorize their training data,\nyet similar analyses for graph neural networks (GNNs) remain largely\nunder-explored. We introduce NCMemo (Node Classification Memorization), the\nfirst framework to quantify label memorization in semi-supervised node\nclassification. We first establish an inverse relationship between memorization\nand graph homophily, i.e., the property that connected nodes share similar\nlabels/features. We find that lower homophily significantly increases\nmemorization, indicating that GNNs rely on memorization to learn less\nhomophilic graphs. Secondly, we analyze GNN training dynamics. We find that the\nincreased memorization in low homophily graphs is tightly coupled to the GNNs'\nimplicit bias on using graph structure during learning. In low homophily\nregimes, this structure is less informative, hence inducing memorization of the\nnode labels to minimize training loss. Finally, we show that nodes with higher\nlabel inconsistency in their feature-space neighborhood are significantly more\nprone to memorization. Building on our insights into the link between graph\nhomophily and memorization, we investigate graph rewiring as a means to\nmitigate memorization. Our results demonstrate that this approach effectively\nreduces memorization without compromising model performance. Moreover, we show\nthat it lowers the privacy risk for previously memorized data points in\npractice. Thus, our work not only advances understanding of GNN learning but\nalso supports more privacy-preserving GNN deployment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86NCMemo\u6846\u67b6\uff0c\u9996\u6b21\u91cf\u5316\u534a\u76d1\u7763\u8282\u70b9\u5206\u7c7b\u4e2d\u7684\u6807\u7b7e\u8bb0\u5fc6\u73b0\u8c61\uff0c\u53d1\u73b0\u56fe\u540c\u914d\u6027\u4e0e\u8bb0\u5fc6\u5316\u5448\u8d1f\u76f8\u5173\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u56fe\u91cd\u8fde\u65b9\u6cd5\u6765\u51cf\u8f7b\u8bb0\u5fc6\u5316\u5e76\u4fdd\u62a4\u9690\u79c1\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5df2\u88ab\u8bc1\u660e\u4f1a\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u56fe\u795e\u7ecf\u7f51\u7edc(GNNs)\u7684\u8bb0\u5fc6\u5316\u5206\u6790\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u4f5c\u8005\u65e8\u5728\u91cf\u5316GNNs\u5728\u534a\u76d1\u7763\u8282\u70b9\u5206\u7c7b\u4e2d\u7684\u6807\u7b7e\u8bb0\u5fc6\u73b0\u8c61\uff0c\u5e76\u7814\u7a76\u5176\u4e0e\u56fe\u7ed3\u6784\u7279\u6027\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86NCMemo\u6846\u67b6\u6765\u5206\u6790\u8282\u70b9\u5206\u7c7b\u4e2d\u7684\u8bb0\u5fc6\u5316\u73b0\u8c61\u3002\u901a\u8fc7\u5206\u6790\u56fe\u540c\u914d\u6027(\u8282\u70b9\u8fde\u63a5\u76f8\u4f3c\u6027)\u4e0e\u8bb0\u5fc6\u5316\u7684\u5173\u7cfb\uff0c\u7814\u7a76GNN\u8bad\u7ec3\u52a8\u6001\uff0c\u5e76\u63a2\u7d22\u56fe\u91cd\u8fde\u4f5c\u4e3a\u51cf\u8f7b\u8bb0\u5fc6\u5316\u7684\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u8f83\u4f4e\u7684\u540c\u914d\u6027\u663e\u8457\u589e\u52a0\u8bb0\u5fc6\u5316\uff0cGNNs\u5728\u4f4e\u540c\u914d\u6027\u56fe\u4e2d\u4f9d\u8d56\u8bb0\u5fc6\u5316\u6765\u5b66\u4e60\u3002\u7279\u5f81\u7a7a\u95f4\u90bb\u57df\u4e2d\u6807\u7b7e\u4e0d\u4e00\u81f4\u6027\u9ad8\u7684\u8282\u70b9\u66f4\u5bb9\u6613\u88ab\u8bb0\u5fc6\u3002\u56fe\u91cd\u8fde\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u8bb0\u5fc6\u5316\u800c\u4e0d\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u589e\u8fdb\u4e86\u5bf9GNN\u5b66\u4e60\u673a\u5236\u7684\u7406\u89e3\uff0c\u8fd8\u652f\u6301\u4e86\u66f4\u9690\u79c1\u4fdd\u62a4\u7684GNN\u90e8\u7f72\u3002\u56fe\u540c\u914d\u6027\u4e0e\u8bb0\u5fc6\u5316\u7684\u5173\u7cfb\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u548c\u9690\u79c1\u53cb\u597d\u7684GNN\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.11693", "categories": ["eess.SP", "cs.AI", "cs.LG", "stat.ML", "68T05, 68T10", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.11693", "abs": "https://arxiv.org/abs/2508.11693", "authors": ["Francisco L\u00f3pez", "Eduardo Di Santi", "Cl\u00e9ment Lefebvre", "Nenad Mijatovic", "Michele Pugnaloni", "Victor Mart\u00edn", "Kenza Saiah"], "title": "Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data", "comment": "Peer-reviewed conference paper. Presented at ICROMA 2025\n  (International Conference on Railway Operations Modelling and Analysis),\n  Dresden, Germany", "summary": "Track Circuits (TC) are the main signalling devices used to detect the\npresence of a train on a rail track. It has been used since the 19th century\nand nowadays there are many types depending on the technology. As a general\nclassification, Track Circuits can be divided into 2 main groups, DC (Direct\nCurrent) and AC (Alternating Current) circuits. This work is focused on a\nparticular AC track circuit, called \"Smart Train Detection System\" (STDS),\ndesigned with both high and low-frequency bands. This approach uses STDS\ncurrent data applied to an SVM (support vector machine) classifier as a type of\nfailure identifier. The main purpose of this work consists on determine\nautomatically which is the component of the track that is failing to improve\nthe maintenance action. Model was trained to classify 15 different failures\nthat belong to 3 more general categories. The method was tested with field data\nfrom 10 different track circuits and validated by the STDS track circuit expert\nand maintainers. All use cases were correctly classified by the method.", "AI": {"tldr": "\u4f7f\u7528SVM\u5206\u7c7b\u5668\u5206\u6790STDS\u8f68\u9053\u7535\u8def\u7535\u6d41\u6570\u636e\uff0c\u81ea\u52a8\u8bc6\u522b15\u79cd\u6545\u969c\u7c7b\u578b\uff0c\u63d0\u9ad8\u7ef4\u62a4\u6548\u7387", "motivation": "\u8f68\u9053\u7535\u8def\u662f\u68c0\u6d4b\u5217\u8f66\u5b58\u5728\u7684\u4e3b\u8981\u4fe1\u53f7\u8bbe\u5907\uff0c\u4f20\u7edf\u7ef4\u62a4\u65b9\u5f0f\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u6545\u969c\u8bc6\u522b\u65b9\u6cd5\u6765\u63d0\u9ad8\u7ef4\u62a4\u6548\u7387", "method": "\u91c7\u7528\u652f\u6301\u5411\u91cf\u673a(SVM)\u5206\u7c7b\u5668\u5206\u6790\u667a\u80fd\u5217\u8f66\u68c0\u6d4b\u7cfb\u7edf(STDS)\u7684\u7535\u6d41\u6570\u636e\uff0c\u5c06\u6545\u969c\u5206\u4e3a3\u5927\u7c7b15\u79cd\u5177\u4f53\u7c7b\u578b", "result": "\u572810\u4e2a\u4e0d\u540c\u8f68\u9053\u7535\u8def\u7684\u73b0\u573a\u6570\u636e\u4e0a\u6d4b\u8bd5\uff0c\u6240\u6709\u7528\u4f8b\u90fd\u88ab\u6b63\u786e\u5206\u7c7b\uff0c\u5f97\u5230\u4e86STDS\u4e13\u5bb6\u548c\u7ef4\u62a4\u4eba\u5458\u7684\u9a8c\u8bc1", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u81ea\u52a8\u8bc6\u522b\u8f68\u9053\u7535\u8def\u7ec4\u4ef6\u6545\u969c\uff0c\u663e\u8457\u6539\u5584\u7ef4\u62a4\u884c\u52a8\u6548\u7387"}}
{"id": "2508.19439", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19439", "abs": "https://arxiv.org/abs/2508.19439", "authors": ["Jorge L. Gonzalez-Rios", "Eva Lagunas", "Hayder Al-Hraishawi", "Luis M. Garces-Socarras", "Symeon Chatzinotas"], "title": "In-Lab Carrier Aggregation Testbed for Satellite Communication Systems", "comment": null, "summary": "Carrier Aggregation (CA) is a technique used in 5G and previous cellular\ngenerations to temporarily increase the data rate of a specific user during\npeak demand periods or to reduce carrier congestion. CA is achieved by\ncombining two or more carriers and providing a virtual, wider overall bandwidth\nto high-demand users of the system. CA was introduced in the 4G/LTE wireless\nera and has been proven effective in 5G as well, where it is said to play a\nsignificant role in efficient network capacity management. Given this success,\nthe satellite communication (SatCom) community has put its attention into CA\nand the potential benefits it can bring in terms of better spectrum utilization\nand better meeting the user traffic demand. While the theoretical evaluation of\nCA for SatCom has already been presented in several works, this article\npresents the design and results obtained with an experimentation testbed based\non Software Defined Radio (SDR) and a satellite channel emulator. We first\npresent the detailed implementation design, which includes a Gateway (GW)\nmodule responsible for PDU-scheduling across the aggregated carriers, and a\nUser Terminal (UT) module responsible for aggregating the multiple received\nstreams. The second part of the article presents the experimental evaluation,\nincluding CA over a single Geostationary (GEO) satellite, CA over a single\nMedium Earth Orbit (MEO) satellite, and CA combining carriers sent over GEO and\nMEO satellites. A key contribution of this work is the explicit consideration\nof multi-orbit scenarios in the testbed design and validation. The testing\nresults show promising benefits of CA over SatCom systems, motivating potential\nupcoming testing on over-the-air systems.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7SDR\u548c\u536b\u661f\u901a\u9053\u6a21\u62df\u5668\u6784\u5efa\u4e86\u536b\u661f\u901a\u4fe1\u8f7d\u6ce2\u805a\u5408\u5b9e\u9a8c\u5e73\u53f0\uff0c\u9a8c\u8bc1\u4e86\u5728GEO\u3001MEO\u53ca\u6df7\u5408\u8f68\u9053\u573a\u666f\u4e0b\u7684CA\u6027\u80fd\u4f18\u52bf\u3002", "motivation": "\u8f7d\u6ce2\u805a\u5408\u6280\u672f\u5728\u5730\u9762\u7f51\u7edc\u4e2d\u5df2\u7ecf\u6210\u529f\u5e94\u7528\uff0c\u536b\u661f\u901a\u4fe1\u793e\u533a\u5e0c\u671b\u501f\u9274\u8be5\u6280\u672f\u6765\u63d0\u9ad8\u9891\u8c31\u5229\u7528\u7387\u548c\u6ee1\u8db3\u7528\u6237\u6d41\u91cf\u9700\u6c42\u3002\u867d\u7136\u7406\u8bba\u5206\u6790\u5df2\u7ecf\u5b58\u5728\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7279\u522b\u662f\u591a\u8f68\u9053\u573a\u666f\u4e0b\u7684\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u5e76\u6784\u5efa\u4e86\u57fa\u4e8e\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535(SDR)\u548c\u536b\u661f\u901a\u9053\u6a21\u62df\u5668\u7684\u5b9e\u9a8c\u6d4b\u8bd5\u5e73\u53f0\u3002\u5305\u542b\u7f51\u5173(GW)\u6a21\u5757\u8d1f\u8d23\u8de8\u805a\u5408\u8f7d\u6ce2\u7684PDU\u8c03\u5ea6\uff0c\u4ee5\u53ca\u7528\u6237\u7ec8\u7aef(UT)\u6a21\u5757\u8d1f\u8d23\u805a\u5408\u591a\u4e2a\u6536\u5230\u6d41\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u5305\u62ec\u5355\u4e2aGEO\u536b\u661f\u3001\u5355\u4e2aMEO\u536b\u661f\u4ee5\u53caGEO\u4e0eMEO\u536b\u661f\u7ec4\u5408\u7684CA\u573a\u666f\u3002\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u4e86CA\u5728\u536b\u661f\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf\uff0c\u7279\u522b\u662f\u5728\u591a\u8f68\u9053\u573a\u666f\u4e0b\u3002", "conclusion": "\u672c\u6587\u4e3a\u536b\u661f\u901a\u4fe1\u8f7d\u6ce2\u805a\u5408\u63d0\u4f9b\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u6280\u672f\u7684\u53ef\u884c\u6027\u548c\u6548\u76ca\uff0c\u4e3a\u672a\u6765\u8fc7\u7a7a\u4e2d\u7cfb\u7edf\u6d4b\u8bd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5de5\u4f5c\u7684\u4e3b\u8981\u8d21\u732e\u662f\u5728\u6d4b\u8bd5\u5e73\u53f0\u8bbe\u8ba1\u4e2d\u660e\u786e\u8003\u8651\u4e86\u591a\u8f68\u9053\u573a\u666f\u3002"}}
{"id": "2508.19353", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.19353", "abs": "https://arxiv.org/abs/2508.19353", "authors": ["Marcin Osial", "Bartosz W\u00f3jcik", "Bartosz Zieli\u0144ski", "Sebastian Cygert"], "title": "Efficient Multi-Source Knowledge Transfer by Model Merging", "comment": null, "summary": "While transfer learning is an advantageous strategy, it overlooks the\nopportunity to leverage knowledge from numerous available models online.\nAddressing this multi-source transfer learning problem is a promising path to\nboost adaptability and cut re-training costs. However, existing approaches are\ninherently coarse-grained, lacking the necessary precision for granular\nknowledge extraction and the aggregation efficiency required to fuse knowledge\nfrom either a large number of source models or those with high parameter\ncounts. We address these limitations by leveraging Singular Value Decomposition\n(SVD) to first decompose each source model into its elementary, rank-one\ncomponents. A subsequent aggregation stage then selects only the most salient\ncomponents from all sources, thereby overcoming the previous efficiency and\nprecision limitations. To best preserve and leverage the synthesized knowledge\nbase, our method adapts to the target task by fine-tuning only the principal\nsingular values of the merged matrix. In essence, this process only\nrecalibrates the importance of top SVD components. The proposed framework\nallows for efficient transfer learning, is robust to perturbations both at the\ninput level and in the parameter space (e.g., noisy or pruned sources), and\nscales well computationally.", "AI": {"tldr": "\u901a\u8fc7\u5947\u5f02\u503c\u5206\u89e3(SVD)\u5c06\u591a\u4e2a\u6e90\u6a21\u578b\u5206\u89e3\u4e3a\u57fa\u7840\u7ec4\u4ef6\uff0c\u7cbe\u51c6\u9009\u62e9\u548c\u805a\u5408\u6700\u91cd\u8981\u7684\u77e5\u8bc6\u7ec4\u4ef6\uff0c\u901a\u8fc7\u7f8e\u503c\u5fae\u8c03\u5b9e\u73b0\u9ad8\u6548\u591a\u6e90\u8fc1\u79fb\u5b66\u4e60", "motivation": "\u73b0\u6709\u591a\u6e90\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u63d0\u53d6\u80fd\u529b\u548c\u9ad8\u6548\u805a\u5408\u6548\u7387\uff0c\u65e0\u6cd5\u6fc0\u6d3b\u5229\u7528\u7f51\u4e0a\u4f17\u591a\u6a21\u578b\u7684\u77e5\u8bc6\u6f5c\u529b", "method": "\u4f7f\u7528SVD\u5c06\u6bcf\u4e2a\u6e90\u6a21\u578b\u5206\u89e3\u4e3a\u79e9\u4e00\u7ec4\u4ef6\uff0c\u9009\u62e9\u6700\u663e\u8457\u7684\u7ec4\u4ef6\u8fdb\u884c\u805a\u5408\uff0c\u901a\u8fc7\u5fae\u8c03\u5408\u5e76\u77e9\u9635\u7684\u4e3b\u5947\u5f02\u503c\u6765\u9002\u5e94\u76ee\u6807\u4efb\u52a1", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\uff0c\u5bf9\u8f93\u5165\u5c42\u548c\u53c2\u6570\u7a7a\u95f4\u7684\u5e72\u6270\u5177\u6709\u7a33\u5065\u6027\uff0c\u8ba1\u7b97\u6269\u5c55\u6027\u597d\uff0c\u9002\u7528\u4e8e\u5927\u91cf\u6e90\u6a21\u578b\u6216\u9ad8\u53c2\u6570\u6a21\u578b", "conclusion": "\u8be5\u6846\u67b6\u5145\u5206\u5229\u7528\u591a\u6e90\u6a21\u578b\u77e5\u8bc6\uff0c\u901a\u8fc7\u7cbe\u7ec6\u7684SVD\u5206\u89e3\u548c\u7ec4\u4ef6\u9009\u62e9\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u8fc1\u79fb\u5b66\u4e60\u7684\u6548\u7387\u548c\u6548\u679c\uff0c\u4e3a\u591a\u6e90\u77e5\u8bc6\u805a\u5408\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19522", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19522", "abs": "https://arxiv.org/abs/2508.19522", "authors": ["Si Wang", "Guoqiang Xiao"], "title": "Fourth-Order Hierarchical Array: A Novel Scheme for Sparse Array Design Based on Fourth-Order Difference Co-Array", "comment": "Sparse linear array, fourth-order cumulant, mutual coupling,\n  redundancy, direction of arrival estimation", "summary": "Conventional array designs based on circular fourth-order cumulant typically\nadopt a single expression form of the fourth-order difference co-array (FODCA),\nwhich limits the achievable degrees of freedom (DOFs) and neglects the impact\nof mutual coupling among physical sensors. To address above issues, this paper\nproposes a novel scheme to design arrays with increased DOFs by combining\ndifferent forms of FODCA while accounting for mutual coupling. A novel\nfourth-order hierarchical array (FOHA) based on different forms of FODCA is\nconstructed using an arbitrary generator set. The analytical expression between\nthe coupling leakage of the generator and the resulting FOHA is derived. Two\nspecific FOHA configurations are presented with closed-form sensor placements.\nThe arrays not only offer increased DOFs for resolving more sources in\ndirection of-arrival (DOA) estimation but also effectively suppress mutual\ncoupling. Moreover, the redundancy of FODCA is examined, and it is shown that\narrays based on the proposed scheme achieve lower redundancy compared to\nexisting arrays based on FODCA. Meanwhile, the necessary and sufficient\nconditions for signal reconstruction by FOHA are derived. Compared with\nexisting arrays based on FODCA, the proposed arrays provide enhanced DOFs and\nimproved robustness against mutual coupling. Numerical simulations verify that\nFOHAs achieve superior DOA estimation performance compared with other sparse\nlinear arrays.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e0d\u540c\u56db\u9636\u5dee\u8054\u5408\u9635\u5217\u5f62\u5f0f\u7684\u56db\u9636\u5206\u5c42\u9635\u5217\u8bbe\u8ba1\u65b9\u6848\uff0c\u5728\u8003\u8651\u4e92\u8026\u6548\u5e94\u7684\u540c\u65f6\u589e\u52a0\u81ea\u7531\u5ea6\u5e76\u964d\u4f4e\u5197\u4f59\u5ea6", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u56db\u9636\u7d2f\u79ef\u91cf\u7684\u9635\u5217\u8bbe\u8ba1\u91c7\u7528\u5355\u4e00\u7684\u56db\u9636\u5dee\u8054\u5408\u9635\u5217\u5f62\u5f0f\uff0c\u9650\u5236\u4e86\u53ef\u83b7\u5f97\u7684\u81ea\u7531\u5ea6\u4e14\u5ffd\u7565\u4e86\u7269\u7406\u4f20\u611f\u5668\u95f4\u4e92\u8026\u7684\u5f71\u54cd", "method": "\u4f7f\u7528\u4efb\u610f\u751f\u6210\u5668\u96c6\u6784\u5efa\u57fa\u4e8e\u4e0d\u540cFODCA\u5f62\u5f0f\u7684\u56db\u9636\u5206\u5c42\u9635\u5217(FOHA)\uff0c\u63a8\u5bfc\u751f\u6210\u5668\u8026\u5408\u6cc4\u6f0f\u4e0eFOHA\u4e4b\u95f4\u7684\u89e3\u6790\u5173\u7cfb\uff0c\u63d0\u51fa\u4e24\u79cd\u5177\u6709\u95ed\u5f0f\u4f20\u611f\u5668\u5e03\u5c40\u7684\u5177\u4f53FOHA\u914d\u7f6e", "result": "\u6240\u63d0\u9635\u5217\u4e0d\u4ec5\u80fd\u589e\u52a0DOA\u4f30\u8ba1\u4e2d\u7684\u81ea\u7531\u5ea6\u4ee5\u5206\u8fa8\u66f4\u591a\u4fe1\u6e90\uff0c\u8fd8\u80fd\u6709\u6548\u6291\u5236\u4e92\u8026\u6548\u5e94\uff0c\u4e14\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8eFODCA\u7684\u9635\u5217\u5177\u6709\u66f4\u4f4e\u7684\u5197\u4f59\u5ea6", "conclusion": "FOHA\u76f8\u6bd4\u5176\u4ed6\u7a00\u758f\u7ebf\u6027\u9635\u5217\u5177\u6709\u4f18\u8d8a\u7684DOA\u4f30\u8ba1\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u81ea\u7531\u5ea6\u548c\u6539\u8fdb\u7684\u6297\u4e92\u8026\u9c81\u68d2\u6027"}}
{"id": "2508.19356", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.19356", "abs": "https://arxiv.org/abs/2508.19356", "authors": ["Jos\u00e9 Manuel Barraza-Chavez", "Rana A. Barghout", "Ricardo Almada-Monter", "Benjamin Sanchez-Lengeling", "Adrian Jinich", "Radhakrishnan Mahadevan"], "title": "Graph Data Modeling: Molecules, Proteins, & Chemical Processes", "comment": "3 to 4 hours read time. 73 pages. 35 figures", "summary": "Graphs are central to the chemical sciences, providing a natural language to\ndescribe molecules, proteins, reactions, and industrial processes. They capture\ninteractions and structures that underpin materials, biology, and medicine.\nThis primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,\nintroduces graphs as mathematical objects in chemistry and shows how learning\nalgorithms (particularly graph neural networks) can operate on them. We outline\nthe foundations of graph design, key prediction tasks, representative examples\nacross chemical sciences, and the role of machine learning in graph-based\nmodeling. Together, these concepts prepare readers to apply graph methods to\nthe next generation of chemical discovery.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u56fe\u6570\u636e\u6a21\u578b\u5728\u5316\u5b66\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u5305\u62ec\u5206\u5b50\u3001\u86cb\u767d\u8d28\u548c\u5316\u5b66\u8fc7\u7a0b\u7684\u8868\u793a\uff0c\u4ee5\u53ca\u56fe\u795e\u7ecf\u7f51\u7edc\u7b49\u5b66\u4e60\u7b97\u6cd5\u5982\u4f55\u5728\u8fd9\u4e9b\u56fe\u4e0a\u8fdb\u884c\u64cd\u4f5c\u3002", "motivation": "\u56fe\u662f\u5316\u5b66\u79d1\u5b66\u4e2d\u7684\u6838\u5fc3\u8bed\u8a00\uff0c\u80fd\u591f\u81ea\u7136\u63cf\u8ff0\u5206\u5b50\u3001\u86cb\u767d\u8d28\u3001\u53cd\u5e94\u548c\u5de5\u4e1a\u8fc7\u7a0b\uff0c\u6355\u6349\u652f\u6491\u6750\u6599\u3001\u751f\u7269\u5b66\u548c\u533b\u5b66\u7684\u76f8\u4e92\u4f5c\u7528\u548c\u7ed3\u6784\u3002", "method": "\u8bba\u6587\u6982\u8ff0\u4e86\u56fe\u8bbe\u8ba1\u7684\u57fa\u7840\u3001\u5173\u952e\u9884\u6d4b\u4efb\u52a1\u3001\u5316\u5b66\u79d1\u5b66\u4e2d\u7684\u4ee3\u8868\u6027\u793a\u4f8b\uff0c\u4ee5\u53ca\u673a\u5668\u5b66\u4e60\u5728\u56fe\u5efa\u6a21\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5e94\u7528\u3002", "result": "\u901a\u8fc7\u4ecb\u7ecd\u56fe\u4f5c\u4e3a\u5316\u5b66\u4e2d\u7684\u6570\u5b66\u5bf9\u8c61\uff0c\u5c55\u793a\u4e86\u5b66\u4e60\u7b97\u6cd5\u5982\u4f55\u64cd\u4f5c\u8fd9\u4e9b\u56fe\uff0c\u4e3a\u8bfb\u8005\u5e94\u7528\u56fe\u65b9\u6cd5\u8fdb\u884c\u4e0b\u4e00\u4ee3\u5316\u5b66\u53d1\u73b0\u505a\u597d\u51c6\u5907\u3002", "conclusion": "\u8fd9\u4e9b\u6982\u5ff5\u5171\u540c\u4e3a\u8bfb\u8005\u63d0\u4f9b\u4e86\u5c06\u56fe\u65b9\u6cd5\u5e94\u7528\u4e8e\u4e0b\u4e00\u4ee3\u5316\u5b66\u53d1\u73b0\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u5316\u5b66\u79d1\u5b66\u4e2d\u56fe\u6570\u636e\u5efa\u6a21\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.19441", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19441", "abs": "https://arxiv.org/abs/2508.19441", "authors": ["Sanket Jantre", "Deepak Akhare", "Xiaoning Qian", "Nathan M. Urban"], "title": "Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models", "comment": null, "summary": "Partial differential equations (PDEs) underpin the modeling of many natural\nand engineered systems. It can be convenient to express such models as neural\nPDEs rather than using traditional numerical PDE solvers by replacing part or\nall of the PDE's governing equations with a neural network representation.\nNeural PDEs are often easier to differentiate, linearize, reduce, or use for\nuncertainty quantification than the original numerical solver. They are usually\ntrained on solution trajectories obtained by long time integration of the PDE\nsolver. Here we propose a more sample-efficient data-augmentation strategy for\ngenerating neural PDE training data from a computer model by space-filling\nsampling of local \"stencil\" states. This approach removes a large degree of\nspatiotemporal redundancy present in trajectory data and oversamples states\nthat may be rarely visited but help the neural PDE generalize across the state\nspace. We demonstrate that accurate neural PDE stencil operators can be learned\nfrom synthetic training data generated by the computational equivalent of 10\ntimesteps' worth of numerical simulation. Accuracy is further improved if we\nassume access to a single full-trajectory simulation from the computer model,\nwhich is typically available in practice. Across several PDE systems, we show\nthat our data-augmented synthetic stencil data yield better trained neural\nstencil operators, with clear performance gains compared with naively sampled\nstencil data from simulation trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f4\u586b\u5145\u91c7\u6837\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u7528\u4e8e\u4ece\u8ba1\u7b97\u673a\u6a21\u578b\u4e2d\u751f\u6210\u795e\u7ecfPDE\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u5c40\u90e8\"\u6a21\u677f\"\u72b6\u6001\u91c7\u6837\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecfPDE\u8bad\u7ec3\u9700\u8981\u957f\u65f6\u95f4\u79ef\u5206PDE\u6c42\u89e3\u5668\u83b7\u5f97\u8f68\u8ff9\u6570\u636e\uff0c\u5b58\u5728\u5927\u91cf\u65f6\u7a7a\u5197\u4f59\uff0c\u4e14\u53ef\u80fd\u9057\u6f0f\u67d0\u4e9b\u7f55\u89c1\u4f46\u91cd\u8981\u7684\u72b6\u6001\u3002", "method": "\u91c7\u7528\u7a7a\u95f4\u586b\u5145\u91c7\u6837\u65b9\u6cd5\u5bf9\u5c40\u90e8\u6a21\u677f\u72b6\u6001\u8fdb\u884c\u91c7\u6837\uff0c\u51cf\u5c11\u65f6\u7a7a\u5197\u4f59\uff0c\u5e76\u8fc7\u91c7\u6837\u53ef\u80fd\u5f88\u5c11\u8bbf\u95ee\u4f46\u6709\u52a9\u4e8e\u6cdb\u5316\u7684\u72b6\u6001\u3002\u53ef\u4ee5\u4ece\u76f8\u5f53\u4e8e10\u4e2a\u65f6\u95f4\u6b65\u957f\u7684\u6570\u503c\u6a21\u62df\u4e2d\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u5230\u51c6\u786e\u7684\u795e\u7ecfPDE\u6a21\u677f\u7b97\u5b50\uff0c\u5982\u679c\u80fd\u591f\u8bbf\u95ee\u5355\u4e2a\u5b8c\u6574\u8f68\u8ff9\u6a21\u62df\uff0c\u51c6\u786e\u5ea6\u4f1a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u3002\u5728\u591a\u4e2aPDE\u7cfb\u7edf\u4e2d\u90fd\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u91c7\u6837\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u795e\u7ecfPDE\u8bad\u7ec3\u7684\u6837\u672c\u6548\u7387\uff0c\u751f\u6210\u66f4\u597d\u7684\u795e\u7ecf\u6a21\u677f\u7b97\u5b50\uff0c\u76f8\u6bd4\u4f20\u7edf\u8f68\u8ff9\u91c7\u6837\u65b9\u6cd5\u6709\u660e\u663e\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.19540", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19540", "abs": "https://arxiv.org/abs/2508.19540", "authors": ["Haochen Li", "Ruikang Zhong", "Jiayi Lei", "Yuanwei Liu"], "title": "Pinching Antenna System for Integrated Sensing and Communications", "comment": "13 pages, 8 figures", "summary": "Recently, the pinching antenna system (PASS) has attracted considerable\nattention due to their advantages in flexible deployment and reduction of\nsignal propagation loss. In this work, a multiple waveguide PASS assisted\nintegrated sensing and communication (ISAC) system is proposed, where the base\nstation (BS) is equipped with transmitting pinching antennas (PAs) and\nreceiving uniform linear array (ULA) antennas. The full-duplex (FD) BS\ntransmits the communication and sensing signals through the PAs on waveguides\nand collects the echo sensing signals with the mounted ULA. Based on this\nconfiguration, a target sensing Cramer Rao Bound (CRB) minimization problem is\nformulated under communication quality-of-service (QoS) constraints, power\nbudget constraint, and PA deployment constraints. The alternating optimization\n(AO) method is employed to address the formulated non-convex optimization\nproblem. In each iteration, the overall optimization problem is decomposed into\na digital beamforming sub-problem and a pinching beamforming sub-problem. The\nsensing covariance matrix and communication beamforming matrix at the BS are\noptimized by solving the digital beamforming sub-problem with semidefinite\nrelaxation (SDR). The PA deployment is updated by solving the pinching\nbeamforming sub-problem with the successive convex approximation (SCA) method,\npenalty method, and element-wise optimization. Simulation results show that the\nproposed PASS assisted ISAC framework achieves superior performance over\nbenchmark schemes, is less affected by stringent communication constraints\ncompared to conventional MIMO-ISAC, and benefits further from increasing the\nnumber of waveguides and PAs per waveguide.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u6ce2\u5bfc\u94fe\u63a7\u5236\u5929\u7ebf\u7cfb\u7edf(PASS)\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u6700\u5c0f\u5316\u76ee\u6807\u611f\u77e5\u7684Cramer-Rao\u4e0b\u754c\uff0c\u5728\u6ee1\u8db3\u901a\u4fe1\u8d28\u91cf\u3001\u529f\u7387\u9884\u7b97\u548c\u5929\u7ebf\u90e8\u7f72\u7ea6\u675f\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u94fe\u63a7\u5929\u7ebf\u7cfb\u7edf(PASS)\u5177\u6709\u7075\u6d3b\u90e8\u7f72\u548c\u51cf\u5c11\u4fe1\u53f7\u4f20\u64ad\u635f\u8017\u7684\u4f18\u52bf\uff0c\u9002\u5408\u7528\u4e8e\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1(ISAC)\u7cfb\u7edf\u4ee5\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5168\u53cc\u5de5\u57fa\u7ad9\u914d\u7f6e\u53d1\u5c04\u94fe\u63a7\u5929\u7ebf\u548c\u63a5\u6536\u5747\u5300\u7ebf\u6027\u6570\u7ec4\u5929\u7ebf\uff0c\u901a\u8fc7\u4ea4\u66ff\u4f18\u5316(AO)\u65b9\u6cd5\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5305\u62ec\u6570\u5b57\u653e\u5927\u5b50\u95ee\u9898(\u4f7f\u7528\u534a\u6b63\u5b9a\u677e\u5f1b)\u548c\u94fe\u63a7\u653e\u5927\u5b50\u95ee\u9898(\u4f7f\u7528\u9010\u6b65\u51f8\u8fd1\u4f3c\u3001\u60e9\u7f5a\u6cd5\u548c\u5143\u7d20\u7ea7\u4f18\u5316)\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0c\u6240\u63d0\u65b9\u6848\u5728\u6027\u80fd\u4e0a\u8d85\u8fc7\u57fa\u51c6\u65b9\u6848\uff0c\u5bf9\u4e25\u683c\u901a\u4fe1\u7ea6\u675f\u7684\u6548\u679c\u5f71\u54cd\u8f83\u4f20\u7edfMIMO-ISAC\u66f4\u5c0f\uff0c\u5e76\u80fd\u591a\u4ece\u6ce2\u5bfc\u6570\u91cf\u548c\u6bcf\u6ce2\u5bfc\u5929\u7ebf\u6570\u91cf\u7684\u589e\u52a0\u4e2d\u83b7\u76ca\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86PASS\u8f85\u52a9ISAC\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\uff0c\u4e3a\u7075\u6d3b\u90e8\u7f72\u7684\u611f\u901a\u4e00\u4f53\u5316\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19361", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19361", "abs": "https://arxiv.org/abs/2508.19361", "authors": ["Yongbin Lee", "Ki H. Chon"], "title": "Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture", "comment": "4 pages, 2 figures, 4 table, IEEE-EMBS International Conference on\n  Body Sensor Networks (IEEE-EMBS BSN 2025)", "summary": "Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk\nof stroke, heart failure, and other cardiovascular complications. While AF\ndetection algorithms perform well in identifying persistent AF, early-stage\nprogression, such as paroxysmal AF (PAF), often goes undetected due to its\nsudden onset and short duration. However, undetected PAF can progress into\nsustained AF, increasing the risk of mortality and severe complications. Early\nprediction of AF offers an opportunity to reduce disease progression through\npreventive therapies, such as catecholamine-sparing agents or beta-blockers. In\nthis study, we propose a lightweight deep learning model using only RR\nIntervals (RRIs), combining a Temporal Convolutional Network (TCN) for\npositional encoding with Mamba, a selective state space model, to enable early\nprediction of AF through efficient parallel sequence modeling. In subject-wise\ntesting results, our model achieved a sensitivity of 0.908, specificity of\n0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our\nmethod demonstrates high computational efficiency, with only 73.5 thousand\nparameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural\nNetwork-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and\nmodel compactness. Notably, the model can predict AF up to two hours in advance\nusing just 30 minutes of input data, providing enough lead time for preventive\ninterventions.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4f7f\u7528RR\u95f4\u671f\u548cTCN-Mamba\u6df7\u5408\u7ed3\u6784\uff0c\u80fd\u591f\u63d0\u524d2\u5c0f\u65f6\u9884\u6d4b\u623f\u98a4\uff0c\u51c6\u786e\u7387\u9ad8\u4e14\u8ba1\u7b97\u6548\u7387\u4f18\u79c0\u3002", "motivation": "\u623f\u98a4\u662f\u6700\u5e38\u89c1\u7684\u5fc3\u5f8b\u5931\u5e38\uff0c\u7279\u522b\u662f\u53d1\u4f5c\u6027\u623f\u98a4\u56e0\u5176\u7a81\u7136\u53d1\u4f5c\u548c\u77ed\u6682\u6301\u7eed\u800c\u5bb9\u6613\u88ab\u6f0f\u8bc6\uff0c\u4f46\u672a\u88ab\u53d1\u73b0\u7684\u53d1\u4f5c\u6027\u623f\u98a4\u53ef\u8fdb\u5c55\u4e3a\u6301\u7eed\u6027\u623f\u98a4\uff0c\u589e\u52a0\u6b7b\u4ea1\u98ce\u9669\u3002\u65e9\u671f\u9884\u6d4b\u623f\u98a4\u53ef\u4ee5\u901a\u8fc7\u9884\u9632\u6027\u6cbb\u7597\u51cf\u7f13\u75be\u75c5\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528RR\u95f4\u671f\uff0c\u7ed3\u5408\u65f6\u5e8f\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\u8fdb\u884c\u4f4d\u7f6e\u7f16\u7801\u548cMamba\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u6548\u5e76\u884c\u5e8f\u5217\u5efa\u6a21\u5b9e\u73b0\u623f\u98a4\u7684\u65e9\u671f\u9884\u6d4b\u3002", "result": "\u5728\u4e3b\u4f53\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u8fbe\u5230\u654f\u611f\u5ea60.908\u3001\u7279\u5f02\u60270.933\u3001F1\u5f97\u52060.930\u3001AUROC 0.972\u548cAUPRC 0.932\u3002\u6a21\u578b\u53ea\u670973.5\u5343\u53c2\u6570\u548c38.3 MFLOPs\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u80fd\u591f\u4ec5\u4f7f\u752830\u5206\u949f\u8f93\u5165\u6570\u636e\u9884\u6d4b\u672a\u67652\u5c0f\u65f6\u7684\u623f\u98a4\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u51c6\u786e\u6027\u548c\u6a21\u578b\u7d27\u51d1\u6027\u65b9\u9762\u90fd\u8d85\u8fc7\u4f20\u7edf\u7684CNN-RNN\u65b9\u6cd5\uff0c\u4e3a\u623f\u98a4\u7684\u65e9\u671f\u9884\u6d4b\u63d0\u4f9b\u4e86\u5145\u8db3\u7684\u9884\u8b66\u65f6\u95f4\uff0c\u4ee5\u4fbf\u8fdb\u884c\u9884\u9632\u6027\u5e72\u9884\u6cbb\u7597\u3002"}}
{"id": "2508.19445", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19445", "abs": "https://arxiv.org/abs/2508.19445", "authors": ["Haozhe Jiang", "Nika Haghtalab"], "title": "On Surjectivity of Neural Networks: Can you elicit any behavior from your model?", "comment": null, "summary": "Given a trained neural network, can any specified output be generated by some\ninput? Equivalently, does the network correspond to a function that is\nsurjective? In generative models, surjectivity implies that any output,\nincluding harmful or undesirable content, can in principle be generated by the\nnetworks, raising concerns about model safety and jailbreak vulnerabilities. In\nthis paper, we prove that many fundamental building blocks of modern neural\narchitectures, such as networks with pre-layer normalization and\nlinear-attention modules, are almost always surjective. As corollaries, widely\nused generative frameworks, including GPT-style transformers and diffusion\nmodels with deterministic ODE solvers, admit inverse mappings for arbitrary\noutputs. By studying surjectivity of these modern and commonly used neural\narchitectures, we contribute a formalism that sheds light on their unavoidable\nvulnerability to a broad class of adversarial attacks.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08\u5982\u9884\u5c42\u5f52\u4e00\u5316\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u5757\uff09\u51e0\u4e4e\u603b\u662f\u6ee1\u5c04\u7684\uff0c\u8fd9\u610f\u5473\u7740\u4efb\u4f55\u8f93\u51fa\uff08\u5305\u62ec\u6709\u5bb3\u5185\u5bb9\uff09\u90fd\u53ef\u4ee5\u88ab\u751f\u6210\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5b89\u5168\u6027\u548c\u8d8a\u72f1\u6f0f\u6d1e\u7684\u56fa\u6709\u8106\u5f31\u6027\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u5177\u6709\u6ee1\u5c04\u6027\uff0c\u5373\u4efb\u4f55\u6307\u5b9a\u8f93\u51fa\u662f\u5426\u90fd\u80fd\u7531\u67d0\u4e2a\u8f93\u5165\u751f\u6210\uff0c\u8fd9\u5bf9\u4e8e\u7406\u89e3\u751f\u6210\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u8d8a\u72f1\u6f0f\u6d1e\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u5206\u6790\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u57fa\u672c\u6784\u5efa\u5757\uff0c\u5305\u62ec\u9884\u5c42\u5f52\u4e00\u5316\u7f51\u7edc\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u5757\uff0c\u8bc1\u660e\u5b83\u4eec\u51e0\u4e4e\u603b\u662f\u6ee1\u5c04\u7684\u3002", "result": "\u8bc1\u660e\u4e86GPT\u98ce\u683c\u53d8\u6362\u5668\u548c\u786e\u5b9a\u6027ODE\u6c42\u89e3\u5668\u7684\u6269\u6563\u6a21\u578b\u7b49\u5e7f\u6cdb\u4f7f\u7528\u7684\u751f\u6210\u6846\u67b6\u5bf9\u4efb\u610f\u8f93\u51fa\u90fd\u5b58\u5728\u9006\u6620\u5c04\u3002", "conclusion": "\u7814\u7a76\u4e3a\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u56fa\u6709\u8106\u5f31\u6027\u63d0\u4f9b\u4e86\u5f62\u5f0f\u5316\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5b83\u4eec\u5bf9\u4e00\u7c7b\u5e7f\u6cdb\u5bf9\u6297\u653b\u51fb\u7684\u4e0d\u53ef\u907f\u514d\u7684\u8106\u5f31\u6027\u3002"}}
{"id": "2508.19552", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19552", "abs": "https://arxiv.org/abs/2508.19552", "authors": ["Shuo Chang", "Rui Sun", "Jiashuo He", "Sai Huang", "Kan Yu", "Zhiyong Feng"], "title": "CSRD2025: A Large-Scale Synthetic Radio Dataset for Spectrum Sensing in Wireless Communications", "comment": null, "summary": "The development of Large AI Models (LAMs) for wireless communications,\nparticularly for complex tasks like spectrum sensing, is critically dependent\non the availability of vast, diverse, and realistic datasets. Addressing this\nneed, this paper introduces the ChangShuoRadioData (CSRD) framework, an\nopen-source, modular simulation platform designed for generating large-scale\nsynthetic radio frequency (RF) data. CSRD simulates the end-to-end transmission\nand reception process, incorporating an extensive range of modulation schemes\n(100 types, including analog, digital, OFDM, and OTFS), configurable channel\nmodels featuring both statistical fading and site-specific ray tracing using\nOpenStreetMap data, and detailed modeling of realistic RF front-end impairments\nfor various antenna configurations (SISO/MISO/MIMO). Using this framework, we\ncharacterize CSRD2025, a substantial dataset benchmark comprising over\n25,000,000 frames (approx. 200TB), which is approximately 10,000 times larger\nthan the widely used RML2018 dataset. CSRD2025 offers unprecedented signal\ndiversity and complexity, specifically engineered to bridge the Sim2Real gap.\nFurthermore, we provide processing pipelines to convert IQ data into\nspectrograms annotated in COCO format, facilitating object detection approaches\nfor time-frequency signal analysis. The dataset specification includes\nstandardized 8:1:1 training, validation, and test splits (via frame indices) to\nensure reproducible research. The CSRD framework is released at\nhttps://github.com/Singingkettle/ChangShuoRadioData to accelerate the\nadvancement of AI-driven spectrum sensing and management.", "AI": {"tldr": "CSRD\u6846\u67b6\u662f\u4e00\u4e2a\u5f00\u6e90\u6a21\u5757\u5316\u4eff\u771f\u5e73\u53f0\uff0c\u7528\u4e8e\u751f\u6210\u5927\u89c4\u6a21\u5408\u6210\u5c04\u9891\u6570\u636e\uff0c\u5305\u542b2500\u4e07\u5e27\u6570\u636e\uff08\u7ea6200TB\uff09\uff0c\u6bd4RML2018\u6570\u636e\u96c6\u592710000\u500d\uff0c\u65e8\u5728\u89e3\u51b3AI\u6a21\u578b\u8bad\u7ec3\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u5927\u578bAI\u6a21\u578b\u5728\u65e0\u7ebf\u901a\u4fe1\u9886\u57df\u7684\u5f00\u53d1\uff0c\u7279\u522b\u662f\u9891\u8c31\u611f\u77e5\u7b49\u590d\u6742\u4efb\u52a1\uff0c\u4e25\u91cd\u4f9d\u8d56\u5927\u91cf\u3001\u591a\u6837\u4e14\u771f\u5b9e\u7684\u6570\u636e\u96c6\u3002\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3AI\u6a21\u578b\u8bad\u7ec3\u9700\u6c42\u3002", "method": "\u5f00\u53d1CSRD\u6846\u67b6\u6a21\u62df\u7aef\u5230\u7aef\u4f20\u8f93\u63a5\u6536\u8fc7\u7a0b\uff0c\u5305\u542b100\u79cd\u8c03\u5236\u65b9\u6848\u3001\u53ef\u914d\u7f6e\u4fe1\u9053\u6a21\u578b\uff08\u7edf\u8ba1\u8870\u843d\u548c\u5c04\u7ebf\u8ffd\u8e2a\uff09\u3001RF\u524d\u7aef\u635f\u4f24\u5efa\u6a21\uff0c\u652f\u6301\u591a\u79cd\u5929\u7ebf\u914d\u7f6e\u3002\u63d0\u4f9b\u6570\u636e\u5904\u7406\u7ba1\u9053\u5c06IQ\u6570\u636e\u8f6c\u6362\u4e3aCOCO\u683c\u5f0f\u7684\u9891\u8c31\u56fe\u3002", "result": "\u751f\u6210CSRD2025\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc725,000,000\u5e27\u6570\u636e\uff08\u7ea6200TB\uff09\uff0c\u63d0\u4f9b\u524d\u6240\u672a\u6709\u7684\u4fe1\u53f7\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u6807\u51c6\u53168:1:1\u7684\u8bad\u7ec3/\u9a8c\u8bc1/\u6d4b\u8bd5\u5212\u5206\u3002", "conclusion": "CSRD\u6846\u67b6\u548c\u6570\u636e\u96c6\u80fd\u591f\u6709\u6548\u5f25\u5408\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u52a0\u901fAI\u9a71\u52a8\u7684\u9891\u8c31\u611f\u77e5\u548c\u7ba1\u7406\u6280\u672f\u53d1\u5c55\uff0c\u6846\u67b6\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2508.19366", "categories": ["cs.LG", "cs.AI", "53B21, 46E22 (Primary), 68R10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2508.19366", "abs": "https://arxiv.org/abs/2508.19366", "authors": ["Supratik Sarkar", "Swagatam Das"], "title": "Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs", "comment": "29 pages, 3 figures, 1 table", "summary": "Hallucinations in large language models (LLMs) remain a fundamental obstacle\nto trustworthy AI, particularly in high-stakes multimodal domains such as\nmedicine, law, and finance. Existing evaluation techniques are largely\nheuristic -- anchored in qualitative benchmarking or ad-hoc empirical\nmitigation -- providing neither principled quantification nor actionable\ntheoretical guarantees. This gap leaves a critical blind spot in understanding\nhow hallucinations arise, propagate, and interact across modalities. We\nintroduce the first (to our knowledge) rigorous information geometric framework\nin diffusion dynamics for quantifying hallucinations in multimodal LLMs\n(MLLMs), advancing the field from qualitative detection to mathematically\ngrounded measurement. Our approach represents MLLM outputs as the spectral\nembeddings over multimodal graph Laplacians and characterizes the manifold gaps\nof truth vs inconsistencies as the semantic distortion, enabling the tight\nRayleigh--Ritz bounds on the multimodal hallucination energy as a functional of\ntime-dependent temperature profiles. By leveraging eigenmode decompositions in\nReproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers\nmodality-aware, theoretically interpretable metrics that capture the evolution\nof hallucinations across time and input prompts through temperature annealing.\nThis work establishes a principled foundation for quantifying and bounding\nhallucinations, transforming them from a qualitative risk to a tractable,\nanalyzable phenomenon.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u606f\u51e0\u4f55\u548c\u6b63\u5219\u5316\u6838\u5e0c\u5c14\u4f2f\u7a7a\u95f4\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6e29\u5ea6\u9006\u9006\u6e29\u6cd5\u548c\u8c31\u5206\u89e3\u6765\u91cf\u5316\u5e7b\u89c9\u7684\u6f14\u5316\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u7684\u5e7b\u89c9\u8bc4\u4f30\u6280\u672f\u4e3b\u8981\u57fa\u4e8e\u7ecf\u9a8c\u6027\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u7406\u8bba\u57fa\u7840\u548c\u53ef\u884c\u4fdd\u8bc1\uff0c\u5728\u533b\u7597\u3001\u6cd5\u5f8b\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5b58\u5728\u91cd\u5927\u9690\u60d1\u3002", "method": "\u901a\u8fc7\u5c06MLLM\u8f93\u51fa\u8868\u793a\u4e3a\u591a\u6a21\u6001\u56fe\u62c9\u666e\u62c9\u65af\u77e9\u9635\u7684\u8c31\u5d4c\u5165\uff0c\u5e76\u4f7f\u7528\u6b63\u5219\u5316\u6838\u5e0c\u5c14\u4f2f\u7a7a\u95f4\u4e2d\u7684\u7279\u5f81\u6a21\u5f0f\u5206\u89e3\u6765\u91cf\u5316\u771f\u76f8\u4e0e\u4e0d\u4e00\u81f4\u6027\u4e4b\u95f4\u7684\u8bed\u4e49\u626d\u66f2\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u901a\u8fc7\u65f6\u95f4\u4f9d\u8d56\u7684\u6e29\u5ea6\u6e29\u51ac\u8fc7\u7a0b\u6765\u63d0\u4f9b\u6a21\u6001\u611f\u77e5\u3001\u7406\u8bba\u53ef\u89e3\u91ca\u7684\u6307\u6807\uff0c\u6355\u6349\u5e7b\u89c9\u5728\u65f6\u95f4\u548c\u8f93\u5165\u63d0\u793a\u4e2d\u7684\u6f14\u5316\u3002", "conclusion": "\u8fd9\u4e2a\u5de5\u4f5c\u4e3a\u91cf\u5316\u548c\u754c\u5b9a\u5e7b\u89c9\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5c06\u5e7b\u89c9\u4ece\u4e00\u79cd\u5b9a\u6027\u98ce\u9669\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u3001\u53ef\u5206\u6790\u7684\u73b0\u8c61\u3002"}}
{"id": "2508.19458", "categories": ["cs.LG", "cs.CR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19458", "abs": "https://arxiv.org/abs/2508.19458", "authors": ["Mahdi Haghifam", "Adam Smith", "Jonathan Ullman"], "title": "The Sample Complexity of Membership Inference and Privacy Auditing", "comment": "58 Pages", "summary": "A membership-inference attack gets the output of a learning algorithm, and a\ntarget individual, and tries to determine whether this individual is a member\nof the training data or an independent sample from the same distribution. A\nsuccessful membership-inference attack typically requires the attacker to have\nsome knowledge about the distribution that the training data was sampled from,\nand this knowledge is often captured through a set of independent reference\nsamples from that distribution. In this work we study how much information the\nattacker needs for membership inference by investigating the sample\ncomplexity-the minimum number of reference samples required-for a successful\nattack. We study this question in the fundamental setting of Gaussian mean\nestimation where the learning algorithm is given $n$ samples from a Gaussian\ndistribution $\\mathcal{N}(\\mu,\\Sigma)$ in $d$ dimensions, and tries to estimate\n$\\hat\\mu$ up to some error $\\mathbb{E}[\\|\\hat \\mu - \\mu\\|^2_{\\Sigma}]\\leq\n\\rho^2 d$. Our result shows that for membership inference in this setting,\n$\\Omega(n + n^2 \\rho^2)$ samples can be necessary to carry out any attack that\ncompetes with a fully informed attacker. Our result is the first to show that\nthe attacker sometimes needs many more samples than the training algorithm uses\nto train the model. This result has significant implications for practice, as\nall attacks used in practice have a restricted form that uses $O(n)$ samples\nand cannot benefit from $\\omega(n)$ samples. Thus, these attacks may be\nunderestimating the possibility of membership inference, and better attacks may\nbe possible when information about the distribution is easy to obtain.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u6210\u5458\u63a8\u7406\u653b\u51fb\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u53d1\u73b0\u5728\u9ad8\u65af\u5747\u503c\u4f30\u8ba1\u573a\u666f\u4e2d\uff0c\u653b\u51fb\u8005\u9700\u8981\u03a9(n + n\u00b2\u03c1\u00b2)\u4e2a\u53c2\u8003\u6837\u672c\u624d\u80fd\u4e0e\u5b8c\u5168\u77e5\u60c5\u653b\u51fb\u8005\u7ade\u4e89\uff0c\u8fd9\u6bd4\u8bad\u7ec3\u7b97\u6cd5\u4f7f\u7528\u7684\u6837\u672c\u6570\u91cf\u591a\u5f97\u591a\u3002", "motivation": "\u73b0\u6709\u6210\u5458\u63a8\u7406\u653b\u51fb\u901a\u5e38\u5047\u8bbe\u653b\u51fb\u8005\u62e5\u6709\u6765\u81ea\u76f8\u540c\u5206\u5e03\u7684\u53c2\u8003\u6837\u672c\uff0c\u4f46\u5b9e\u9645\u4e2d\u653b\u51fb\u8005\u80fd\u83b7\u53d6\u7684\u6837\u672c\u6570\u91cf\u6709\u9650\u3002\u672c\u6587\u65e8\u5728\u91cf\u5316\u653b\u51fb\u8005\u6210\u529f\u8fdb\u884c\u6210\u5458\u63a8\u7406\u6240\u9700\u7684\u6700\u5c0f\u53c2\u8003\u6837\u672c\u6570\u91cf\u3002", "method": "\u5728\u9ad8\u65af\u5747\u503c\u4f30\u8ba1\u7684\u57fa\u672c\u8bbe\u7f6e\u4e2d\uff0c\u5b66\u4e60\u7b97\u6cd5\u4eced\u7ef4\u9ad8\u65af\u5206\u5e03N(\u03bc,\u03a3)\u4e2d\u83b7\u53d6n\u4e2a\u6837\u672c\uff0c\u4f30\u8ba1\u5747\u503c\u03bc\u0302\u4f7f\u5f97\u671f\u671b\u8bef\u5deeE[\u2016\u03bc\u0302-\u03bc\u2016\u00b2_\u03a3]\u2264\u03c1\u00b2d\u3002\u7814\u7a76\u653b\u51fb\u8005\u9700\u8981\u591a\u5c11\u53c2\u8003\u6837\u672c\u624d\u80fd\u6709\u6548\u8fdb\u884c\u6210\u5458\u63a8\u7406\u3002", "result": "\u7814\u7a76\u663e\u793a\uff0c\u5bf9\u4e8e\u4efb\u4f55\u80fd\u4e0e\u5b8c\u5168\u77e5\u60c5\u653b\u51fb\u8005\u7ade\u4e89\u7684\u6210\u5458\u63a8\u7406\u653b\u51fb\uff0c\u653b\u51fb\u8005\u9700\u8981\u03a9(n + n\u00b2\u03c1\u00b2)\u4e2a\u53c2\u8003\u6837\u672c\u3002\u8fd9\u662f\u9996\u6b21\u8bc1\u660e\u653b\u51fb\u8005\u6709\u65f6\u9700\u8981\u6bd4\u8bad\u7ec3\u7b97\u6cd5\u4f7f\u7528\u7684\u6837\u672c\u6570\u91cf\u591a\u5f97\u591a\u7684\u6837\u672c\u3002", "conclusion": "\u5f53\u524d\u5b9e\u8df5\u4e2d\u4f7f\u7528\u7684\u653b\u51fb\uff08\u901a\u5e38\u4f7f\u7528O(n)\u4e2a\u6837\u672c\uff09\u53ef\u80fd\u4f4e\u4f30\u4e86\u6210\u5458\u63a8\u7406\u7684\u53ef\u80fd\u6027\u3002\u5f53\u5206\u5e03\u4fe1\u606f\u6613\u4e8e\u83b7\u53d6\u65f6\uff0c\u53ef\u80fd\u5b58\u5728\u66f4\u597d\u7684\u653b\u51fb\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.19566", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19566", "abs": "https://arxiv.org/abs/2508.19566", "authors": ["Chen Shang", "Jiadong Yu", "Dinh Thai Hoang"], "title": "Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks", "comment": "6 pages, 4 figures, conference paper", "summary": "This work proposes an energy-efficient, learning-based beamforming scheme for\nintegrated sensing and communication (ISAC)-enabled V2X networks. Specifically,\nwe first model the dynamic and uncertain nature of V2X environments as a Markov\nDecision Process. This formulation allows the roadside unit to generate\nbeamforming decisions based solely on current sensing information, thereby\neliminating the need for frequent pilot transmissions and extensive channel\nstate information acquisition. We then develop a deep reinforcement learning\n(DRL) algorithm to jointly optimize beamforming and power allocation, ensuring\nboth communication throughput and sensing accuracy in highly dynamic scenario.\nTo address the high energy demands of conventional learning-based schemes, we\nembed spiking neural networks (SNNs) into the DRL framework. Leveraging their\nevent-driven and sparsely activated architecture, SNNs significantly enhance\nenergy efficiency while maintaining robust performance. Simulation results\nconfirm that the proposed method achieves substantial energy savings and\nsuperior communication performance, demonstrating its potential to support\ngreen and sustainable connectivity in future V2X systems.", "AI": {"tldr": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u80fd\u6e90\u6548\u7387\u690d\u5165\u5f0f\u611f\u77e5\u4e0e\u901a\u4fe1\u805a\u7136\u6210\u578b\u65b9\u6848\uff0c\u4e3aV2X\u7f51\u7edc\u63d0\u4f9b\u9ad8\u6548\u80fd\u6e90\u6d88\u8017\u548c\u7a33\u5065\u6027\u80fd\u7684\u8fde\u63a5\u670d\u52a1", "motivation": "\u89e3\u51b3V2X\u7f51\u7edc\u4e2d\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u805a\u7136\u6210\u578b\u95ee\u9898\uff0c\u907f\u514d\u9891\u7e41\u7684\u5bfc\u9891\u4f20\u8f93\u548c\u6e90\u6d41\u6d88\u8017\u7684\u901a\u9053\u72b6\u6001\u4fe1\u606f\u83b7\u53d6", "method": "\u5c06V2X\u73af\u5883\u6a21\u578b\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5173\u805a\u7136\u6210\u578b\u548c\u529f\u7387\u5206\u914d\u4f18\u5316\uff0c\u5e76\u5d4c\u5165\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u63d0\u5347\u80fd\u6e90\u6548\u7387", "result": "\u6a21\u62df\u7ed3\u679c\u8bc1\u5b9e\u8be5\u65b9\u6848\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u80fd\u6e90\u8282\u7701\u548c\u4f18\u5f02\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u611f\u77e5\u51c6\u786e\u6027", "conclusion": "\u8be5\u65b9\u6848\u5177\u6709\u652f\u6301\u672a\u6765V2X\u7cfb\u7edf\u7eff\u8272\u53ef\u6301\u7eed\u8fde\u63a5\u7684\u6f5c\u529b\uff0c\u4e3a\u9ad8\u52a8\u6001\u573a\u666f\u4e0b\u7684\u80fd\u6e90\u6548\u7387\u548c\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19376", "categories": ["cs.LG", "cs.AI", "cs.CV", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.19376", "abs": "https://arxiv.org/abs/2508.19376", "authors": ["Dikshant Sagar", "Kaiwen Yu", "Alejandro Yankelevich", "Jianming Bian", "Pierre Baldi"], "title": "Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments", "comment": null, "summary": "Recent progress in large language models (LLMs) has shown strong potential\nfor multimodal reasoning beyond natural language. In this work, we explore the\nuse of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for\nclassifying neutrino interactions from pixelated detector images in high-energy\nphysics (HEP) experiments. We benchmark its performance against an established\nCNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as\nclassification accuracy, precision, recall, and AUC-ROC. Our results show that\nthe VLM not only matches or exceeds CNN performance but also enables richer\nreasoning and better integration of auxiliary textual or semantic context.\nThese findings suggest that VLMs offer a promising general-purpose backbone for\nevent classification in HEP, paving the way for multimodal approaches in\nexperimental neutrino physics.", "AI": {"tldr": "\u57fa\u4e8eLLaMA 3.2\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728HEP\u4e2d\u5fae\u5b50\u76f8\u4e92\u4f5c\u7528\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfCNN\uff0c\u652f\u6301\u591a\u6a21\u6001\u63a8\u7406", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u80fd\u7269\u7406\u5b9e\u9a8c\u4e2d\u7684\u4e2d\u5fae\u5b50\u76f8\u4e92\u4f5c\u7528\u5206\u7c7b\u5e94\u7528", "method": "\u4f7f\u7528\u57fa\u4e8eLLaMA 3.2\u7684\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e0eNOvA\u548cDUNE\u5b9e\u9a8c\u4e2d\u4f7f\u7528\u7684CNN\u57fa\u7ebf\u8fdb\u884c\u6027\u80fd\u5bf9\u6bd4\u8bc4\u4f30", "result": "VLM\u4e0d\u4ec5\u8fbe\u5230\u6216\u8d85\u8fc7CNN\u6027\u80fd\uff0c\u8fd8\u652f\u6301\u66f4\u4e30\u5bcc\u7684\u63a8\u7406\u548c\u8f85\u52a9\u6587\u672c/\u8bed\u4e49\u4e0a\u4e0b\u6587\u7684\u66f4\u597d\u96c6\u6210", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3aHEP\u4e8b\u4ef6\u5206\u7c7b\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u901a\u7528\u4e3b\u5e72\uff0c\u4e3a\u5b9e\u9a8c\u6027\u4e2d\u5fae\u5b50\u7269\u7406\u4e2d\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u94fa\u5e73\u4e86\u9053\u8def"}}
{"id": "2508.19563", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19563", "abs": "https://arxiv.org/abs/2508.19563", "authors": ["Hejia Liu", "Mochen Yang", "Gediminas Adomavicius"], "title": "Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting", "comment": null, "summary": "Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool.", "AI": {"tldr": "LLMs\u5728\u8868\u683c\u6570\u636e\u9884\u6d4b\u4e2d\u5b58\u5728\u4e25\u91cd\u8106\u5f31\u6027\uff0c\u4efb\u52a1\u65e0\u5173\u7684\u6570\u636e\u8868\u793a\u53d8\u5316\uff08\u5982\u53d8\u91cf\u540d\u66f4\u6539\uff09\u4f1a\u5bfc\u81f4\u9884\u6d4b\u7ed3\u679c\u5927\u5e45\u6ce2\u52a8\uff0c\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u8868\u683c\u57fa\u7840\u6a21\u578b\u4e5f\u65e0\u6cd5\u5b8c\u5168\u907f\u514d\u8fd9\u4e00\u95ee\u9898", "motivation": "\u968f\u7740LLMs\u88ab\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u79cd\u6570\u636e\u62df\u5408\u4efb\u52a1\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u4f5c\u4e3a\u6570\u636e\u62df\u5408\u5de5\u5177\u7684\u7a33\u5065\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4efb\u52a1\u65e0\u5173\u6570\u636e\u53d8\u5316\u7684\u654f\u611f\u6027", "method": "\u901a\u8fc7\u6539\u53d8\u53d8\u91cf\u540d\u7b49\u4efb\u52a1\u65e0\u5173\u7684\u6570\u636e\u8868\u793a\u65b9\u5f0f\uff0c\u6d4b\u8bd5LLMs\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u4e0b\u7684\u9884\u6d4b\u7a33\u5b9a\u6027\uff0c\u5e76\u5206\u6790\u6ce8\u610f\u529b\u6a21\u5f0f\u6765\u89e3\u91ca\u654f\u611f\u6027\u539f\u56e0", "result": "\u7b80\u5355\u7684\u53d8\u91cf\u540d\u66f4\u6539\u53ef\u4f7f\u9884\u6d4b\u8bef\u5dee\u6ce2\u52a8\u9ad8\u8fbe82%\uff0cLLMs\u5bf9\u4efb\u52a1\u65e0\u5173\u53d8\u5316\u8868\u73b0\u51fa\u663e\u8457\u654f\u611f\u6027\uff0c\u6ce8\u610f\u529b\u5206\u6790\u663e\u793a\u5b58\u5728\u975e\u5747\u5300\u6ce8\u610f\u529b\u6a21\u5f0f", "conclusion": "\u5c3d\u7ba1LLMs\u5177\u6709\u5f3a\u5927\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u57fa\u672c\u7684\u7a33\u5065\u6027\uff0c\u4e0d\u80fd\u4f5c\u4e3a\u53ef\u9760\u7684\u6570\u636e\u62df\u5408\u5de5\u5177\u4f7f\u7528"}}
{"id": "2508.19631", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19631", "abs": "https://arxiv.org/abs/2508.19631", "authors": ["Yubeen Jo", "Geon Choi", "Yongjune Kim", "Namyoon Lee"], "title": "Code-Weight Sphere Decoding", "comment": "5 pages, 6 figures", "summary": "Ultra-reliable low-latency communications (URLLC) demand high-performance\nerror-correcting codes and decoders in the finite blocklength regime. This\nletter introduces a novel two-stage near-maximum likelihood (near-ML) decoding\nframework applicable to any linear block code. Our approach first employs a\nlow-complexity initial decoder. If this initial stage fails a cyclic redundancy\ncheck, it triggers a second stage: the proposed code-weight sphere decoding\n(WSD). WSD iteratively refines the codeword estimate by exploring a localized\nsphere of candidates constructed from pre-computed low-weight codewords. This\nstrategy adaptively minimizes computational overhead at high signal-to-noise\nratios while achieving near-ML performance, especially for low-rate codes.\nExtensive simulations demonstrate that our two-stage decoder provides an\nexcellent trade-off between decoding reliability and complexity, establishing\nit as a promising solution for next-generation URLLC systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u8fd1\u6700\u5927\u4f3c\u7136\u89e3\u7801\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4efb\u4f55\u7ebf\u6027\u5206\u7ec4\u7801\uff0c\u901a\u8fc7\u521d\u59cb\u89e3\u7801\u548c\u7801\u91cd\u7403\u9762\u89e3\u7801\u76f8\u7ed3\u5408\uff0c\u5728\u6709\u9650\u7801\u957f\u4e0b\u5b9e\u73b0\u9ad8\u53ef\u9760\u6027\u4f4e\u5ef6\u8fdf\u901a\u4fe1\u3002", "motivation": "\u8d85\u53ef\u9760\u4f4e\u5ef6\u8fdf\u901a\u4fe1(URLLC)\u5728\u6709\u9650\u7801\u957f\u4e0b\u9700\u8981\u9ad8\u6027\u80fd\u7ea0\u9519\u7801\u548c\u89e3\u7801\u5668\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u5ea6\u548c\u6027\u80fd\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\u3002", "method": "\u4e24\u9636\u6bb5\u89e3\u7801\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u4f4e\u590d\u6742\u5ea6\u521d\u59cb\u89e3\u7801\u5668\uff0c\u82e5CRC\u6821\u9a8c\u5931\u8d25\u5219\u89e6\u53d1\u7b2c\u4e8c\u9636\u6bb5\u7801\u91cd\u7403\u9762\u89e3\u7801(WSD)\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u4f4e\u6743\u91cd\u7801\u5b57\u6784\u5efa\u5c40\u90e8\u5019\u9009\u96c6\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5927\u91cf\u4eff\u771f\u8868\u660e\u8be5\u89e3\u7801\u5668\u5728\u89e3\u7801\u53ef\u9760\u6027\u548c\u590d\u6742\u5ea6\u4e4b\u95f4\u63d0\u4f9b\u4e86\u4f18\u5f02\u5e73\u8861\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f4e\u7801\u7387\u7801\uff0c\u5728\u9ad8\u4fe1\u566a\u6bd4\u4e0b\u81ea\u9002\u5e94\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u8be5\u4e24\u9636\u6bb5\u89e3\u7801\u5668\u662f\u4e0b\u4e00\u4ee3URLLC\u7cfb\u7edf\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u8fd1\u6700\u5927\u4f3c\u7136\u6027\u80fd\u3002"}}
{"id": "2508.19381", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19381", "abs": "https://arxiv.org/abs/2508.19381", "authors": ["Jesus Lopez", "Saeefa Rubaiyet Nowmi", "Viviana Cadena", "Mohammad Saidur Rahman"], "title": "Towards Quantum Machine Learning for Malicious Code Analysis", "comment": "6 pages, 3 figures, 2 tables. Accepted at the International Workshop\n  on Quantum Computing and Reinforcement Learning (QCRL) @ IEEE Quantum Week\n  2025", "summary": "Classical machine learning (CML) has been extensively studied for malware\nclassification. With the emergence of quantum computing, quantum machine\nlearning (QML) presents a paradigm-shifting opportunity to improve malware\ndetection, though its application in this domain remains largely unexplored. In\nthis study, we investigate two hybrid quantum-classical models -- a Quantum\nMultilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),\nfor malware classification. Both models utilize angle embedding to encode\nmalware features into quantum states. QMLP captures complex patterns through\nfull qubit measurement and data re-uploading, while QCNN achieves faster\ntraining via quantum convolution and pooling layers that reduce active qubits.\nWe evaluate both models on five widely used malware datasets -- API-Graph,\nEMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and\nmulticlass classification tasks.\n  Our results show high accuracy for binary classification -- 95-96% on\nAPI-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass\nsettings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,\nand 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex\nmulticlass tasks, while QCNN offers improved training efficiency at the cost of\nreduced accuracy.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u4e24\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b(QMLP\u548cQCNN)\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4e2d\u7684\u5e94\u7528\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u5177\u4f53\u8868\u73b0\u968f\u4efb\u52a1\u590d\u6742\u5ea6\u800c\u5f02\u3002", "motivation": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60(QML)\u4e3a\u6076\u610f\u8f6f\u4ef6\u68c0\u6d4b\u5e26\u6765\u4e86\u65b0\u7684\u673a\u4f1a\uff0c\u4f46\u8be5\u9886\u57df\u4ecd\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u5f00\u521b\u6027\u5730\u5c06QML\u5e94\u7528\u4e8e\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\uff1a\u91cf\u5b50\u591a\u5c42\u611f\u77e5\u673a(QMLP)\u548c\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(QCNN)\u3002\u4f7f\u7528angle embedding\u6280\u672f\u5c06\u6076\u610f\u8f6f\u4ef6\u7279\u5f81\u7f16\u7801\u4e3a\u91cf\u5b50\u72b6\u6001\uff0c\u5e76\u57285\u4e2a\u5e38\u7528\u6076\u610f\u8f6f\u4ef6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e8c\u5143\u548c\u591a\u7c7b\u5206\u7c7b\u8bc4\u4f30\u3002", "result": "\u4e8c\u5143\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\u8fbe95-96%(API-Graph)\u300191-92%(AZ-Domain)\u548c77%(EMBER-Domain)\u3002\u591a\u7c7b\u5206\u7c7b\u51c6\u786e\u7387\u4ece41.7%\u523093.6%\u4e0d\u7b49\uff0cQMLP\u5728\u590d\u6742\u591a\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800cQCNN\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8\u4f46\u51c6\u786e\u7387\u7565\u4f4e\u3002", "conclusion": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u6076\u610f\u8f6f\u4ef6\u5206\u7c7b\u9886\u57df\u5177\u6709\u5f3a\u5927\u6f5c\u529b\uff0c\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\u80fd\u591f\u5728\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u3002QMLP\u9002\u5408\u590d\u6742\u5206\u7c7b\u4efb\u52a1\uff0cQCNN\u5219\u5728\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u6709\u4f18\u52bf\u3002"}}
{"id": "2508.19780", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.19780", "abs": "https://arxiv.org/abs/2508.19780", "authors": ["Ryoma Sato"], "title": "Interestingness First Classifiers", "comment": "14 pages", "summary": "Most machine learning models are designed to maximize predictive accuracy. In\nthis work, we explore a different goal: building classifiers that are\ninteresting. An ``interesting classifier'' is one that uses unusual or\nunexpected features, even if its accuracy is lower than the best possible\nmodel. For example, predicting room congestion from CO2 levels achieves\nnear-perfect accuracy but is unsurprising. In contrast, predicting room\ncongestion from humidity is less accurate yet more nuanced and intriguing. We\nintroduce EUREKA, a simple framework that selects features according to their\nperceived interestingness. Our method leverages large language models to rank\nfeatures by their interestingness and then builds interpretable classifiers\nusing only the selected interesting features. Across several benchmark\ndatasets, EUREKA consistently identifies features that are non-obvious yet\nstill predictive. For example, in the Occupancy Detection dataset, our method\nfavors humidity over CO2 levels and light intensity, producing classifiers that\nachieve meaningful accuracy while offering insights. In the Twin Papers\ndataset, our method discovers the rule that papers with a colon in the title\nare more likely to be cited in the future. We argue that such models can\nsupport new ways of knowledge discovery and communication, especially in\nsettings where moderate accuracy is sufficient but novelty and interpretability\nare valued.", "AI": {"tldr": "EUREKA\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6709\u8da3\u800c\u975e\u6700\u51c6\u786e\u7684\u7279\u5f81\u6765\u6784\u5efa\u5206\u7c7b\u5668\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u7279\u5f81\u6709\u8da3\u5ea6\uff0c\u751f\u6210\u5177\u6709\u65b0\u9896\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6a21\u578b", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8ffd\u6c42\u6700\u5927\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u4f46\u672c\u6587\u63a2\u7d22\u6784\u5efa\"\u6709\u8da3\u5206\u7c7b\u5668\"\u7684\u76ee\u6807\uff0c\u5373\u4f7f\u7528\u4e0d\u5bfb\u5e38\u6216\u610f\u5916\u7684\u7279\u5f81\uff0c\u5373\u4f7f\u51c6\u786e\u7387\u4f4e\u4e8e\u6700\u4f73\u6a21\u578b\uff0c\u4e5f\u80fd\u63d0\u4f9b\u65b0\u7684\u6d1e\u5bdf\u548c\u77e5\u8bc6\u53d1\u73b0", "method": "\u63d0\u51faEUREKA\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u7279\u5f81\u7684\u6709\u8da3\u5ea6\u8fdb\u884c\u6392\u5e8f\uff1b2\uff09\u4ec5\u9009\u62e9\u6709\u8da3\u7684\u7279\u5f81\u6784\u5efa\u53ef\u89e3\u91ca\u5206\u7c7b\u5668\uff1b3\uff09\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u65b9\u6cd5", "result": "\u5728Occupancy Detection\u6570\u636e\u96c6\u4e2d\u504f\u597d\u6e7f\u5ea6\u800c\u975eCO2\u548c\u5149\u7167\u5f3a\u5ea6\uff0c\u5728Twin Papers\u6570\u636e\u96c6\u4e2d\u53d1\u73b0\u6807\u9898\u542b\u5192\u53f7\u7684\u8bba\u6587\u66f4\u53ef\u80fd\u88ab\u5f15\u7528\u3002\u65b9\u6cd5\u80fd\u4e00\u81f4\u8bc6\u522b\u975e\u660e\u663e\u4f46\u4ecd\u5177\u9884\u6d4b\u6027\u7684\u7279\u5f81", "conclusion": "\u8fd9\u79cd\u6a21\u578b\u652f\u6301\u65b0\u7684\u77e5\u8bc6\u53d1\u73b0\u548c\u4f20\u64ad\u65b9\u5f0f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4e2d\u7b49\u51c6\u786e\u7387\u8db3\u591f\u4f46\u65b0\u9896\u6027\u548c\u53ef\u89e3\u91ca\u6027\u53d7\u5230\u91cd\u89c6\u7684\u573a\u666f"}}
{"id": "2508.19637", "categories": ["eess.SP", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19637", "abs": "https://arxiv.org/abs/2508.19637", "authors": ["Maha Shatta", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Georgios Panagopoulos", "Mehdi B. Tahoori", "Georgios Zervakis"], "title": "Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge", "comment": "Accepted at 2025 International Conference on Computer-Aided Design\n  (ICCAD)", "summary": "Flexible Electronics (FE) offer a promising alternative to rigid\nsilicon-based hardware for wearable healthcare devices, enabling lightweight,\nconformable, and low-cost systems. However, their limited integration density\nand large feature sizes impose strict area and power constraints, making\nML-based healthcare systems-integrating analog frontend, feature extraction and\nclassifier-particularly challenging. Existing FE solutions often neglect\npotential system-wide solutions and focus on the classifier, overlooking the\nsubstantial hardware cost of feature extraction and Analog-to-Digital\nConverters (ADCs)-both major contributors to area and power consumption. In\nthis work, we present a holistic mixed-signal feature-to-classifier co-design\nframework for flexible smart wearable systems. To the best of our knowledge, we\ndesign the first analog feature extractors in FE, significantly reducing\nfeature extraction cost. We further propose an hardware-aware NAS-inspired\nfeature selection strategy within ML training, enabling efficient,\napplication-specific designs. Our evaluation on healthcare benchmarks shows our\napproach delivers highly accurate, ultra-area-efficient flexible systems-ideal\nfor disposable, low-power wearable monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u67d4\u6027\u53ef\u7a7f\u6234\u7cfb\u7edf\u7684\u6df7\u5408\u4fe1\u53f7\u7279\u5f81-\u5206\u7c7b\u5668\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u9996\u6b21\u5728\u67d4\u6027\u7535\u5b50\u4e2d\u8bbe\u8ba1\u6a21\u62df\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u663e\u8457\u964d\u4f4e\u7279\u5f81\u63d0\u53d6\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u7684NAS\u7279\u5f81\u9009\u62e9\u7b56\u7565\u5b9e\u73b0\u9ad8\u6548\u5e94\u7528\u7279\u5b9a\u8bbe\u8ba1\u3002", "motivation": "\u67d4\u6027\u7535\u5b50\u5728\u53ef\u7a7f\u6234\u533b\u7597\u8bbe\u5907\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5176\u6709\u9650\u7684\u96c6\u6210\u5bc6\u5ea6\u548c\u5927\u7279\u5f81\u5c3a\u5bf8\u5e26\u6765\u4e86\u4e25\u683c\u7684\u9762\u79ef\u548c\u529f\u8017\u7ea6\u675f\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5f80\u5f80\u5ffd\u89c6\u7cfb\u7edf\u7ea7\u4f18\u5316\uff0c\u8fc7\u5ea6\u5173\u6ce8\u5206\u7c7b\u5668\u800c\u5ffd\u7565\u4e86\u7279\u5f81\u63d0\u53d6\u548cADC\u7684\u786c\u4ef6\u6210\u672c\u3002", "method": "1) \u8bbe\u8ba1\u9996\u4e2a\u67d4\u6027\u7535\u5b50\u4e2d\u7684\u6a21\u62df\u7279\u5f81\u63d0\u53d6\u5668\uff1b2) \u63d0\u51fa\u786c\u4ef6\u611f\u77e5\u7684NAS\u542f\u53d1\u5f0f\u7279\u5f81\u9009\u62e9\u7b56\u7565\uff1b3) \u5efa\u7acb\u6df7\u5408\u4fe1\u53f7\u7279\u5f81-\u5206\u7c7b\u5668\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\u3002", "result": "\u5728\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u3001\u8d85\u9762\u79ef\u6548\u7387\u7684\u67d4\u6027\u7cfb\u7edf\uff0c\u975e\u5e38\u9002\u5408\u4e00\u6b21\u6027\u4f4e\u529f\u8017\u53ef\u7a7f\u6234\u76d1\u6d4b\u5e94\u7528\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u7cfb\u7edf\u7ea7\u7684\u6df7\u5408\u4fe1\u53f7\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u67d4\u6027\u7535\u5b50\u5728\u673a\u5668\u5b66\u4e60\u533b\u7597\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7387\u7684\u53ef\u7a7f\u6234\u533b\u7597\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19389", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.19389", "abs": "https://arxiv.org/abs/2508.19389", "authors": ["Owais Ahmad", "Milad Ramezankhani", "Anirudh Deodhar"], "title": "DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting", "comment": null, "summary": "Accurate long-term traffic forecasting remains a critical challenge in\nintelligent transportation systems, particularly when predicting high-frequency\ntraffic phenomena such as shock waves and congestion boundaries over extended\nrollout horizons. Neural operators have recently gained attention as promising\ntools for modeling traffic flow. While effective at learning function space\nmappings, they inherently produce smooth predictions that fail to reconstruct\nhigh-frequency features such as sharp density gradients which results in rapid\nerror accumulation during multi-step rollout predictions essential for\nreal-time traffic management. To address these fundamental limitations, we\nintroduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)\narchitecture. DETNO leverages a transformer neural operator with\ncross-attention mechanisms, providing model expressivity and super-resolution,\ncoupled with a diffusion-based refinement component that iteratively\nreconstructs high-frequency traffic details through progressive denoising. This\novercomes the inherent smoothing limitations and rollout instability of\nstandard neural operators. Through comprehensive evaluation on chaotic traffic\ndatasets, our method demonstrates superior performance in extended rollout\npredictions compared to traditional and transformer-based neural operators,\npreserving high-frequency components and improving stability over long\nprediction horizons.", "AI": {"tldr": "\u63d0\u51faDETNO\u67b6\u6784\uff0c\u7ed3\u5408Transformer\u795e\u7ecf\u7b97\u5b50\u548c\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u4e2d\u9ad8\u9891\u7279\u5f81\u4e22\u5931\u548c\u957f\u671f\u9884\u6d4b\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7b97\u5b50\u5728\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u4e2d\u4f1a\u4ea7\u751f\u5e73\u6ed1\u9884\u6d4b\uff0c\u65e0\u6cd5\u91cd\u5efa\u9ad8\u9891\u7279\u5f81\uff08\u5982\u5bc6\u5ea6\u68af\u5ea6\uff09\uff0c\u5bfc\u81f4\u591a\u6b65\u9884\u6d4b\u65f6\u8bef\u5dee\u5feb\u901f\u7d2f\u79ef\uff0c\u5f71\u54cd\u5b9e\u65f6\u4ea4\u901a\u7ba1\u7406", "method": "\u91c7\u7528Transformer\u795e\u7ecf\u7b97\u5b50\u63d0\u4f9b\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u548c\u8d85\u5206\u8fa8\u7387\uff0c\u7ed3\u5408\u57fa\u4e8e\u6269\u6563\u7684\u7ec6\u5316\u7ec4\u4ef6\u901a\u8fc7\u6e10\u8fdb\u53bb\u566a\u8fed\u4ee3\u91cd\u5efa\u9ad8\u9891\u4ea4\u901a\u7ec6\u8282", "result": "\u5728\u6df7\u6c8c\u4ea4\u901a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u548c\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7b97\u5b50\uff0c\u8be5\u65b9\u6cd5\u5728\u957f\u671f\u9884\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u80fd\u4fdd\u6301\u9ad8\u9891\u6210\u5206\u5e76\u63d0\u9ad8\u9884\u6d4b\u7a33\u5b9a\u6027", "conclusion": "DETNO\u67b6\u6784\u6709\u6548\u514b\u670d\u4e86\u6807\u51c6\u795e\u7ecf\u7b97\u5b50\u7684\u5e73\u6ed1\u9650\u5236\u548c\u9884\u6d4b\u4e0d\u7a33\u5b9a\u6027\uff0c\u4e3a\u957f\u671f\u4ea4\u901a\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19657", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19657", "abs": "https://arxiv.org/abs/2508.19657", "authors": ["Jorge L. Gonz\u00e1lez-Rios", "Liz Mart\u00ednez Marrero", "Juan Duncan", "Luis M. Garc\u00e9s-Socarr\u00e1s", "Raudel Cuiman Marquez", "Juan A. V\u00e1squez Peralvo", "Jevgenij Krivochiza", "Symeon Chatzinotas", "Bj\u00f6rn Ottersten"], "title": "Demonstrator Testbed for Effective Precoding in MEO Multibeam Satellites", "comment": null, "summary": "The use of communication satellites in medium Earth orbit (MEO) is foreseen\nto provide quasi-global broadband Internet connectivity in the coming\nnetworking ecosystems. Multi-user multiple-input single-output (MU-MISO)\ndigital signal processing techniques, such as precoding, emerge as appealing\ntechnological enablers in the forward link of multi-beam satellite systems\noperating in full frequency reuse (FFR). However, the orbit dynamics of MEO\nsatellites pose additional challenges that must be carefully evaluated and\naddressed. This work presents the design of an in-lab testbed based on\nsoftware-defined radio (SDR) platforms and the corresponding adaptations\nrequired for efficient precoding in a MEO scenario. The setup incorporates a\nprecise orbit model and the radiation pattern of a custom-designed direct\nradiating array (DRA). We analyze the main impairments affecting precoding\nperformance, including Doppler shifts and payload phase noise, and propose a\nsynchronization loop to mitigate these effects. Preliminary experimental\nresults validate the feasibility and effectiveness of the proposed solution.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8f6f\u4ef6\u5b9a\u4e49\u65e0\u7ebf\u7535(SDR)\u7684\u5b9e\u9a8c\u5e8a\uff0c\u7528\u4e8e\u5728\u4e2d\u5730\u8f68(MEO)\u536b\u661f\u573a\u666f\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684\u9884\u7f16\u7801\u6280\u672f\uff0c\u89e3\u51b3\u8f68\u9053\u52a8\u529b\u5b66\u5e26\u6765\u7684\u6311\u6218\u3002", "motivation": "\u4e2d\u5730\u8f68(MEO)\u901a\u4fe1\u536b\u661f\u5c06\u63d0\u4f9b\u51e0\u4e4e\u5168\u7403\u7684\u5bbd\u5e26\u4e92\u8054\u7f51\u8fde\u63a5\uff0c\u4f46\u8f68\u9053\u52a8\u529b\u5b66\u7ed9\u591a\u7528\u6237MISO\u9884\u7f16\u7801\u6280\u672f\u5e26\u6765\u4e86\u65b0\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8eSDR\u5e73\u53f0\u7684\u5b9e\u9a8c\u5e8a\uff0c\u5305\u542b\u7cbe\u786e\u8f68\u9053\u6a21\u578b\u548c\u81ea\u5b9a\u4e49\u76f4\u63a5\u8f90\u5c04\u5927\u7ebf\u9635\u5217\u7684\u8f90\u5c04\u56fe\u6848\uff0c\u5e76\u63d0\u51fa\u540c\u6b65\u5faa\u73af\u6765\u51cf\u5c11\u591a\u666e\u52d2\u79fb\u9891\u548c\u6709\u6548\u8d28\u91cf\u566a\u58f0\u7684\u5f71\u54cd\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6848\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u8be5SDR\u57fa\u5b9e\u9a8c\u5e8a\u6210\u529f\u5730\u89e3\u51b3\u4e86MEO\u536b\u661f\u8f68\u9053\u52a8\u529b\u5b66\u5e26\u6765\u7684\u6311\u6218\uff0c\u4e3a\u5b9e\u73b0\u9ad8\u6548\u9884\u7f16\u7801\u6280\u672f\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19394", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19394", "abs": "https://arxiv.org/abs/2508.19394", "authors": ["Afrar Jahin", "Yi Pan", "Yingfeng Wang", "Tianming Liu", "Wei Zhang"], "title": "Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding", "comment": null, "summary": "Although recent advances in quantum machine learning (QML) offer significant\npotential for enhancing generative models, particularly in molecular design, a\nlarge array of classical approaches still face challenges in achieving high\nfidelity and validity. In particular, the integration of QML with\nsequence-based tasks, such as Simplified Molecular Input Line Entry System\n(SMILES) string reconstruction, remains underexplored and usually suffers from\nfidelity degradation. In this work, we propose a hybrid quantum-classical\narchitecture for SMILES reconstruction that integrates quantum encoding with\nclassical sequence modeling to improve quantum fidelity and classical\nsimilarity. Our approach achieves a quantum fidelity of approximately 84% and a\nclassical reconstruction similarity of 60%, surpassing existing quantum\nbaselines. Our work lays a promising foundation for future QML applications,\nstriking a balance between expressive quantum representations and classical\nsequence models and catalyzing broader research on quantum-aware sequence\nmodels for molecular and drug discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\u7528\u4e8eSMILES\u5b57\u7b26\u4e32\u91cd\u6784\uff0c\u901a\u8fc7\u91cf\u5b50\u7f16\u7801\u4e0e\u7ecf\u5178\u5e8f\u5217\u5efa\u6a21\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e8684%\u7684\u91cf\u5b50\u4fdd\u771f\u5ea6\u548c60%\u7684\u7ecf\u5178\u91cd\u6784\u76f8\u4f3c\u5ea6\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u91cf\u5b50\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u5206\u5b50\u8bbe\u8ba1\u7b49\u751f\u6210\u6a21\u578b\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728SMILES\u5e8f\u5217\u91cd\u6784\u4efb\u52a1\u4e2d\u4ecd\u9762\u4e34\u4fdd\u771f\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u91cf\u5b50\u4e0e\u5e8f\u5217\u4efb\u52a1\u7684\u7ed3\u5408\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\uff0c\u5c06\u91cf\u5b50\u7f16\u7801\u6280\u672f\u4e0e\u7ecf\u5178\u5e8f\u5217\u5efa\u6a21\u76f8\u7ed3\u5408\uff0c\u65e8\u5728\u540c\u65f6\u63d0\u5347\u91cf\u5b50\u4fdd\u771f\u5ea6\u548c\u7ecf\u5178\u76f8\u4f3c\u5ea6\u3002", "result": "\u5b9e\u73b0\u4e86\u7ea684%\u7684\u91cf\u5b50\u4fdd\u771f\u5ea6\u548c60%\u7684\u7ecf\u5178\u91cd\u6784\u76f8\u4f3c\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u91cf\u5b50\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u672a\u6765\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5e94\u7528\u5960\u5b9a\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\uff0c\u5728\u91cf\u5b50\u8868\u793a\u548c\u7ecf\u5178\u5e8f\u5217\u6a21\u578b\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u63a8\u52a8\u4e86\u91cf\u5b50\u611f\u77e5\u5e8f\u5217\u6a21\u578b\u5728\u5206\u5b50\u548c\u836f\u7269\u53d1\u73b0\u9886\u57df\u7684\u66f4\u5e7f\u6cdb\u7814\u7a76\u3002"}}
{"id": "2508.19660", "categories": ["eess.SP", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.19660", "abs": "https://arxiv.org/abs/2508.19660", "authors": ["Vojtech Mrazek", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Zdenek Vasicek", "Mehdi B. Tahoori", "Georgios Zervakis"], "title": "Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation", "comment": "Accepted at IEEE Transactions on Circuits and Systems for Artificial\n  Intelligence", "summary": "Printed electronics offer a promising alternative for applications beyond\nsilicon-based systems, requiring properties like flexibility, stretchability,\nconformality, and ultra-low fabrication costs. Despite the large feature sizes\nin printed electronics, printed neural networks have attracted attention for\nmeeting target application requirements, though realizing complex circuits\nremains challenging. This work bridges the gap between classification accuracy\nand area efficiency in printed neural networks, covering the entire\nprocessing-near-sensor system design and co-optimization from the\nanalog-to-digital interface-a major area and power bottleneck-to the digital\nclassifier. We propose an automated framework for designing printed Ternary\nNeural Networks with arbitrary input precision, utilizing multi-objective\noptimization and holistic approximation. Our circuits outperform existing\napproximate printed neural networks by 17x in area and 59x in power on average,\nbeing the first to enable printed-battery-powered operation with under 5%\naccuracy loss while accounting for analog-to-digital interfacing costs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\u6765\u8bbe\u8ba1\u5370\u5237\u4e09\u5143\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u548c\u6574\u4f53\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u5206\u7c7b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u9762\u79ef\u548c\u529f\u7387\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u5370\u5237\u7535\u6c60\u4f9b\u7535\u7684\u64cd\u4f5c\u3002", "motivation": "\u5370\u5237\u7535\u5b50\u5728\u67d4\u6027\u3001\u53ef\u62c9\u4f38\u6027\u3001\u8d85\u4f4e\u5236\u9020\u6210\u672c\u7b49\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5b9e\u73b0\u590d\u6742\u7535\u8def\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u5370\u5237\u795e\u7ecf\u7f51\u7edc\u5728\u5206\u7c7b\u7cbe\u5ea6\u548c\u9762\u79ef\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u6a21\u6570\u63a5\u53e3\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u8bbe\u8ba1\u4efb\u610f\u8f93\u5165\u7cbe\u5ea6\u7684\u5370\u5237\u4e09\u5143\u795e\u7ecf\u7f51\u7edc\uff0c\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316\u548c\u6574\u4f53\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u4ece\u6a21\u6570\u63a5\u53e3\u5230\u6570\u5b57\u5206\u7c7b\u5668\u8fdb\u884c\u5168\u7cfb\u7edf\u534f\u540c\u4f18\u5316\u8bbe\u8ba1\u3002", "result": "\u6240\u8bbe\u8ba1\u7684\u7535\u8def\u5728\u9762\u79ef\u4e0a\u6bd4\u73b0\u6709\u8fd1\u4f3c\u5370\u5237\u795e\u7ecf\u7f51\u7edc\u5e73\u5747\u63d0\u534717\u500d\uff0c\u5728\u529f\u7387\u4e0a\u63d0\u534759\u500d\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5370\u5237\u7535\u6c60\u4f9b\u7535\u64cd\u4f5c\uff0c\u7cbe\u5ea6\u635f\u5931\u4f4e\u4e8e5%\uff0c\u540c\u65f6\u8003\u8651\u4e86\u6a21\u6570\u63a5\u53e3\u6210\u672c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u6210\u529f\u89e3\u51b3\u4e86\u5370\u5237\u795e\u7ecf\u7f51\u7edc\u4e2d\u5206\u7c7b\u7cbe\u5ea6\u4e0e\u9762\u79ef\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u5370\u5237\u7535\u5b50\u5728\u4f4e\u529f\u8017\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19410", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.19410", "abs": "https://arxiv.org/abs/2508.19410", "authors": ["Zongyu Wu", "Ruichen Xu", "Luoyao Chen", "Georgios Kementzidis", "Siyao Wang", "Yuefan Deng"], "title": "Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks", "comment": "Comments: 8 pages, 6 figures. Accepted at IJCNN 2025 (to appear in\n  IEEE/IJCNN proceedings). This arXiv submission corresponds to the\n  camera-ready version with minor editorial clarifications; results unchanged", "summary": "We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural\nNetwork (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with\nunivariate transformations. While Hamiltonian Neural Networks (HNNs) ensure\nenergy conservation by learning Hamiltonian functions directly from data,\nexisting implementations, often relying on MLPs, cause hypersensitivity to the\nhyperparameters while exploring complex energy landscapes. Our approach\nexploits the localized function approximations to better capture high-frequency\nand multi-scale dynamics, reducing energy drift and improving long-term\npredictive stability. The networks preserve the symplectic form of Hamiltonian\nsystems, and thus maintain interpretability and physical consistency. After\nassessing KAR-HNN on four benchmark problems including spring-mass, simple\npendulum, two- and three-body problem, we foresee its effectiveness for\naccurate and stable modeling of realistic physical processes often at high\ndimensions and with few known parameters.", "AI": {"tldr": "KAR-HNN\u4f7f\u7528Kolmogorov-Arnold\u8868\u793a\u66ff\u4ee3MLP\uff0c\u901a\u8fc7\u5355\u53d8\u91cf\u53d8\u6362\u6539\u8fdb\u54c8\u5bc6\u987f\u795e\u7ecf\u7f51\u7edc\uff0c\u51cf\u5c11\u80fd\u91cf\u6f02\u79fb\u5e76\u63d0\u9ad8\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMLP\u7684\u54c8\u5bc6\u987f\u795e\u7ecf\u7f51\u7edc\u5728\u63a2\u7d22\u590d\u6742\u80fd\u91cf\u666f\u89c2\u65f6\u5bf9\u8d85\u53c2\u6570\u8fc7\u4e8e\u654f\u611f\uff0c\u9700\u8981\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u6355\u83b7\u9ad8\u9891\u548c\u591a\u5c3a\u5ea6\u52a8\u529b\u5b66\u3002", "method": "\u4f7f\u7528Kolmogorov-Arnold\u8868\u793a\u7684\u5355\u53d8\u91cf\u53d8\u6362\u66ff\u4ee3MLP\uff0c\u5229\u7528\u5c40\u90e8\u51fd\u6570\u903c\u8fd1\u6765\u66f4\u597d\u5730\u6355\u83b7\u52a8\u529b\u5b66\u7279\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u54c8\u5bc6\u987f\u7cfb\u7edf\u7684\u8f9b\u5f62\u5f0f\u3002", "result": "\u5728\u5f39\u7c27\u8d28\u91cf\u3001\u5355\u6446\u3001\u4e8c\u4f53\u548c\u4e09\u4f53\u95ee\u9898\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u51cf\u5c11\u4e86\u80fd\u91cf\u6f02\u79fb\uff0c\u63d0\u9ad8\u4e86\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "conclusion": "KAR-HNN\u6709\u671b\u5728\u9ad8\u7ef4\u5ea6\u548c\u53c2\u6570\u7a00\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u73b0\u5b9e\u7269\u7406\u8fc7\u7a0b\u63d0\u4f9b\u51c6\u786e\u7a33\u5b9a\u7684\u5efa\u6a21\u3002"}}
{"id": "2508.19739", "categories": ["eess.SP", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.19739", "abs": "https://arxiv.org/abs/2508.19739", "authors": ["Sebastian Lotter", "Marco Seiter", "Maryam Pirmoradi", "Lukas Brand", "Dagmar Fischer", "Robert Schober"], "title": "MC for Gastroretentive Drug Delivery", "comment": "4 pages, 2 figures, This paper has been submitted to IEEE\n  Transactions on Molecular, Biological, and Multi-Scale Communications as\n  Transactions Letter", "summary": "Recently, bacterial nanocellulose (BNC), a biological material produced by\nnon-pathogenic bacteria that possesses excellent material properties for\nvarious medical applications, has received increased interest as a carrier\nsystem for drug delivery. However, the vast majority of existing studies on\ndrug release from BNC are feasibility studies with modeling and design aspects\nremaining largely unexplored. To narrow this research gap, this paper proposes\na novel model for the drug release from BNC. Specifically, the drug delivery\nsystem considered in this paper consists of a BNC fleece coated with a polymer.\nThe polymer coating is used as an additional diffusion barrier, enabling the\ncontrolled release of an active pharmaceutical ingredient. The proposed\nphysics-based model reflects the geometry of the BNC and incorporates the\nimpact of the polymer coating on the drug release. Hence, it can be useful for\ndesigning BNC-based drug delivery systems in the future. The accuracy of the\nmodel is validated with experimental data obtained in wet lab experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u7ec6\u83cc\u7eb3\u7c73\u7ea4\u7ef4\u7d20\uff08BNC\uff09\u836f\u7269\u91ca\u653e\u65b0\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u8003\u8651\u4e86BNC\u51e0\u4f55\u5f62\u72b6\u548c\u805a\u5408\u7269\u6d82\u5c42\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u867d\u7136\u7ec6\u83cc\u7eb3\u7c73\u7ea4\u7ef4\u7d20\uff08BNC\uff09\u4f5c\u4e3a\u836f\u7269\u9012\u9001\u8f7d\u4f53\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u591a\u4e3a\u53ef\u884c\u6027\u7814\u7a76\uff0c\u7f3a\u4e4f\u5efa\u6a21\u548c\u8bbe\u8ba1\u65b9\u9762\u7684\u6df1\u5165\u63a2\u7d22\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7269\u7406\u57fa\u7840\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u53cd\u6620\u4e86BNC\u7684\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u7eb3\u5165\u4e86\u805a\u5408\u7269\u6d82\u5c42\u5bf9\u836f\u7269\u91ca\u653e\u7684\u5f71\u54cd\u3002\u6a21\u578b\u8003\u8651\u4e86\u7531\u805a\u5408\u7269\u6d82\u5c42\u7684BNC\u7d6e\u7247\u6784\u6210\u7684\u836f\u7269\u9012\u9001\u7cfb\u7edf\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u51c6\u786e\u63cf\u8ff0\u836f\u7269\u4eceBNC\u7cfb\u7edf\u4e2d\u7684\u91ca\u653e\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u6e7f\u5b9e\u9a8c\u5ba4\u5b9e\u9a8c\u6570\u636e\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7269\u7406\u57fa\u7840\u6a21\u578b\u53ef\u7528\u4e8e\u672a\u6765\u8bbe\u8ba1\u57fa\u4e8eBNC\u7684\u836f\u7269\u9012\u9001\u7cfb\u7edf\uff0c\u4e3a\u53ef\u63a7\u836f\u7269\u91ca\u653e\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5efa\u6a21\u5de5\u5177\u3002"}}
{"id": "2508.19414", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19414", "abs": "https://arxiv.org/abs/2508.19414", "authors": ["Gustavo Sandoval"], "title": "Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention", "comment": "9 pages", "summary": "We present a mechanistic case study of a format-dependent reasoning failure\nin Llama-3.1-8B-Instruct, where the model incorrectly judges \"9.11\" as larger\nthan \"9.8\" in chat or Q&A formats, but answers correctly in simple format.\nThrough systematic intervention, we discover transformers implement even/odd\nattention head specialization: even indexed heads handle numerical comparison,\nwhile odd heads serve incompatible functions. The bug requires exactly 8 even\nheads at Layer 10 for perfect repair. Any combination of 8+ even heads\nsucceeds, while 7 or fewer completely fails, revealing sharp computational\nthresholds with perfect redundancy among the 16 even heads. SAE analysis\nreveals the mechanism: format representations separate (10% feature overlap at\nLayer 7), then re-entangle with different weightings (80% feature overlap at\nLayer 10), with specific features showing 1.5x amplification in failing\nformats. We achieve perfect repair using only 25% of attention heads and\nidentify a 60% pattern replacement threshold, demonstrating that apparent\nfull-module requirements hide sophisticated substructure with implications for\ninterpretability and efficiency. All of our code is available at\nhttps://github.com/gussand/surgeon.", "AI": {"tldr": "Llama-3.1-8B-Instruct\u5728\u804a\u5929\u683c\u5f0f\u4e2d\u9519\u8bef\u5224\u65ad\"9.11\"\u6bd4\"9.8\"\u5927\uff0c\u4f46\u5728\u7b80\u5355\u683c\u5f0f\u4e2d\u6b63\u786e\u3002\u7814\u7a76\u53d1\u73b0transformer\u5b58\u5728\u5947\u5076\u6ce8\u610f\u529b\u5934\u4e13\u95e8\u5316\u673a\u5236\uff0c\u5076\u6570\u5934\u8d1f\u8d23\u6570\u503c\u6bd4\u8f83\uff0c\u9700\u8981\u81f3\u5c118\u4e2a\u5076\u6570\u5934\u624d\u80fd\u4fee\u590d\u6b64bug\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u683c\u5f0f\u4f9d\u8d56\u7684\u63a8\u7406\u5931\u8d25\u673a\u5236\uff0c\u7279\u522b\u662f\u6570\u503c\u6bd4\u8f83\u4efb\u52a1\u4e2d\u51fa\u73b0\u7684\u683c\u5f0f\u654f\u611f\u6027\u9519\u8bef\uff0c\u4ee5\u63ed\u793a\u6a21\u578b\u5185\u90e8\u7684\u8ba1\u7b97\u7ed3\u6784\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u5e72\u9884\u5b9e\u9a8c\uff0c\u5206\u6790transformer\u6ce8\u610f\u529b\u5934\u7684\u5947\u5076\u7d22\u5f15\u4e13\u95e8\u5316\uff0c\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u8fdb\u884c\u7279\u5f81\u5206\u6790\uff0c\u5e76\u6d4b\u8bd5\u4e0d\u540c\u6570\u91cf\u6ce8\u610f\u529b\u5934\u7ec4\u5408\u7684\u4fee\u590d\u6548\u679c\u3002", "result": "\u53d1\u73b0\u9700\u8981\u81f3\u5c118\u4e2a\u5076\u6570\u6ce8\u610f\u529b\u5934\u624d\u80fd\u5b8c\u7f8e\u4fee\u590dbug\uff0c\u5076\u6570\u5934\u4e4b\u95f4\u5b58\u5728\u5b8c\u7f8e\u5197\u4f59\u6027\u3002\u683c\u5f0f\u8868\u793a\u5728Layer 7\u5206\u79bb(10%\u7279\u5f81\u91cd\u53e0)\uff0c\u5728Layer 10\u91cd\u65b0\u7ea0\u7f20(80%\u7279\u5f81\u91cd\u53e0)\uff0c\u5931\u8d25\u683c\u5f0f\u4e2d\u7279\u5b9a\u7279\u5f81\u653e\u59271.5\u500d\u3002", "conclusion": "\u6a21\u578b\u8868\u9762\u4e0a\u7684\u5168\u6a21\u5757\u9700\u6c42\u9690\u85cf\u4e86\u7cbe\u5bc6\u7684\u5b50\u7ed3\u6784\uff0c\u4ec5\u970025%\u7684\u6ce8\u610f\u529b\u5934\u5373\u53ef\u5b9e\u73b0\u5b8c\u7f8e\u4fee\u590d\uff0c\u8fd9\u5bf9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u4f18\u5316\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.19822", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19822", "abs": "https://arxiv.org/abs/2508.19822", "authors": ["Chunxuan Shi", "Yongzhe Li", "Ran Tao"], "title": "On Minimization/Maximization of the Generalized Multi-Order Complex Quadratic Form With Constant-Modulus Constraints", "comment": "14 pages, 3 figures (16 subfigures)", "summary": "In this paper, we study the generalized problem that minimizes or maximizes a\nmulti-order complex quadratic form with constant-modulus constraints on all\nelements of its optimization variable. Such a mathematical problem is commonly\nencountered in various applications of signal processing. We term it as the\nconstant-modulus multi-order complex quadratic programming (CMCQP) in this\npaper. In general, the CMCQP is non-convex and difficult to solve. Its\nobjective function typically relates to metrics such as signal-to-noise ratio,\nCram\\'er-Rao bound, integrated sidelobe level, etc., and constraints normally\ncorrespond to requirements on similarity to desired aspects,\npeak-to-average-power ratio, or constant-modulus property in practical\nscenarios. In order to find efficient solutions to the CMCQP, we first\nreformulate it into an unconstrained optimization problem with respect to phase\nvalues of the studied variable only. Then, we devise a steepest descent/ascent\nmethod with fast determinations on its optimal step sizes. Specifically, we\nconvert the step-size searching problem into a polynomial form that leads to\nclosed-form solutions of high accuracy, wherein the third-order Taylor\nexpansion of the search function is conducted. Our major contributions also lie\nin investigating the effect of the order and specific form of matrices embedded\nin the CMCQP, for which two representative cases are identified. Examples of\nrelated applications associated with the two cases are also provided for\ncompleteness. The proposed methods are summarized into algorithms, whose\nconvergence speeds are verified to be fast by comprehensive simulations and\ncomparisons to existing methods. The accuracy of our proposed fast step-size\ndetermination is also evaluated.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6052\u5b9a\u6a21\u591a\u9636\u590d\u4e8c\u6b21\u89c4\u5212(CMCQP)\u95ee\u9898\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u901a\u8fc7\u76f8\u4f4d\u91cd\u6784\u548c\u6700\u4f18\u6b65\u957f\u5feb\u901f\u786e\u5b9a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u6536\u655b\u548c\u9ad8\u7cbe\u5ea6\u6c42\u89e3\u3002", "motivation": "CMCQP\u95ee\u9898\u5728\u4fe1\u53f7\u5904\u7406\u4e2d\u5e7f\u6cdb\u5b58\u5728\uff0c\u4f46\u901a\u5e38\u662f\u975e\u51f8\u4e14\u96be\u4ee5\u6c42\u89e3\u7684\u3002\u73b0\u6709\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u6162\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6c42\u89e3\u7b97\u6cd5\u3002", "method": "\u5c06CMCQP\u91cd\u6784\u4e3a\u4ec5\u5173\u4e8e\u76f8\u4f4d\u53d8\u91cf\u7684\u65e0\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u8bbe\u8ba1\u6700\u901f\u4e0b\u964d/\u4e0a\u5347\u6cd5\uff0c\u901a\u8fc7\u4e09\u9636\u6cf0\u52d2\u5c55\u5f00\u5c06\u6b65\u957f\u641c\u7d22\u8f6c\u5316\u4e3a\u591a\u9879\u5f0f\u5f62\u5f0f\uff0c\u83b7\u5f97\u95ed\u5f0f\u89e3\u3002", "result": "\u7b97\u6cd5\u6536\u655b\u901f\u5ea6\u5feb\uff0c\u6b65\u957f\u786e\u5b9a\u7cbe\u5ea6\u9ad8\uff0c\u901a\u8fc7\u7efc\u5408\u4eff\u771f\u9a8c\u8bc1\u4e86\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3aCMCQP\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u6c42\u89e3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4fe1\u53f7\u5904\u7406\u4e2d\u7684\u5404\u79cd\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.19419", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19419", "abs": "https://arxiv.org/abs/2508.19419", "authors": ["Harun Ur Rashid", "Aleksandra Pachalieva", "Daniel O'Malley"], "title": "Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management", "comment": null, "summary": "Accurate subsurface reservoir pressure control is extremely challenging due\nto geological heterogeneity and multiphase fluid-flow dynamics. Predicting\nbehavior in this setting relies on high-fidelity physics-based simulations that\nare computationally expensive. Yet, the uncertain, heterogeneous properties\nthat control these flows make it necessary to perform many of these expensive\nsimulations, which is often prohibitive. To address these challenges, we\nintroduce a physics-informed machine learning workflow that couples a fully\ndifferentiable multiphase flow simulator, which is implemented in the DPFEHM\nframework with a convolutional neural network (CNN). The CNN learns to predict\nfluid extraction rates from heterogeneous permeability fields to enforce\npressure limits at critical reservoir locations. By incorporating transient\nmultiphase flow physics into the training process, our method enables more\npractical and accurate predictions for realistic injection-extraction scenarios\ncompare to previous works. To speed up training, we pretrain the model on\nsingle-phase, steady-state simulations and then fine-tune it on full multiphase\nscenarios, which dramatically reduces the computational cost. We demonstrate\nthat high-accuracy training can be achieved with fewer than three thousand\nfull-physics multiphase flow simulations -- compared to previous estimates\nrequiring up to ten million. This drastic reduction in the number of\nsimulations is achieved by leveraging transfer learning from much less\nexpensive single-phase simulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u591a\u76f8\u6d41\u6a21\u62df\u5668\u548cCNN\uff0c\u5927\u5e45\u51cf\u5c11\u9ad8\u4fdd\u771f\u6a21\u62df\u9700\u6c42\uff0c\u4ece\u5343\u4e07\u6b21\u964d\u81f3\u6570\u5343\u6b21", "motivation": "\u5730\u4e0b\u50a8\u5c42\u538b\u529b\u63a7\u5236\u9762\u4e34\u5730\u8d28\u5f02\u8d28\u6027\u548c\u591a\u76f8\u6d41\u52a8\u529b\u5b66\u7684\u6311\u6218\uff0c\u4f20\u7edf\u9ad8\u4fdd\u771f\u7269\u7406\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\uff0c\u4e14\u9700\u8981\u5927\u91cf\u6a21\u62df\u6765\u5904\u7406\u4e0d\u786e\u5b9a\u6027", "method": "\u4f7f\u7528\u5b8c\u5168\u53ef\u5fae\u5206\u591a\u76f8\u6d41\u6a21\u62df\u5668\uff08DPFEHM\u6846\u67b6\uff09\u4e0e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8026\u5408\uff0c\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5148\u9884\u8bad\u7ec3\u5355\u76f8\u7a33\u6001\u6a21\u62df\uff0c\u518d\u5fae\u8c03\u591a\u76f8\u573a\u666f", "result": "\u4ec5\u9700\u4e0d\u52303000\u6b21\u5168\u7269\u7406\u591a\u76f8\u6d41\u6a21\u62df\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bad\u7ec3\uff0c\u76f8\u6bd4\u4e4b\u524d\u9700\u8981\u4e0a\u5343\u4e07\u6b21\u6a21\u62df\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7269\u7406\u4fe1\u606f\u673a\u5668\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u76f8\u6d41\u50a8\u5c42\u538b\u529b\u9884\u6d4b\u7684\u5b9e\u7528\u6027\u548c\u8ba1\u7b97\u6548\u7387"}}
{"id": "2508.19910", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19910", "abs": "https://arxiv.org/abs/2508.19910", "authors": ["Sergio Hernandez", "Christophe Peucheret", "Francesco Da Ros", "Darko Zibar"], "title": "Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission", "comment": "10 pages, 10 figures, submitted to journal of lightwave technology", "summary": "Directly modulated lasers (DMLs) are an attractive technology for short-reach\nintensity modulation and direct detection communication systems. However, their\ncomplex nonlinear dynamics make the modeling and optimization of DML-based\nsystems challenging. In this paper, we study the end-to-end optimization of\nDML-based systems based on a data-driven surrogate model trained on\nexperimental data. The end-to-end optimization includes the pulse shaping and\nequalizer filters, the bias current and the modulation radio-frequency (RF)\npower applied to the laser. The performance of the end-to-end optimization\nscheme is tested on the experimental setup and compared to 4 different\nbenchmark schemes based on linear and nonlinear receiver-side equalization. The\nresults show that the proposed end-to-end scheme is able to deliver better\nperformance throughout the studied symbol rates and transmission distances\nwhile employing lower modulation RF power, fewer filter taps and utilizing a\nsmaller signal bandwidth.", "AI": {"tldr": "\u57fa\u4e8e\u5b9e\u9a8c\u6570\u636e\u8bad\u7ec3\u7684\u6570\u636e\u9a71\u52a8\u4ee3\u6a21\u578b\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u76f4\u63a5\u8c03\u5236\u6fc0\u5149\u5668\u7cfb\u7edf\uff0c\u5728\u4f4e\u8c03\u5236\u529f\u8017\u3001\u5c11\u6ee4\u6ce2\u5668\u6c34\u6ef4\u548c\u5c0f\u4fe1\u53f7\u5e26\u5bbd\u4e0b\u5b9e\u73b0\u66f4\u4f18\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u76f4\u63a5\u8c03\u5236\u6fc0\u5149\u5668\u5728\u77ed\u8ddd\u79bb\u901a\u4fe1\u7cfb\u7edf\u4e2d\u5f88\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5176\u590d\u6742\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\u4f7f\u5f97\u5efa\u6a21\u548c\u4f18\u5316\u9762\u4e34\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5b9e\u9a8c\u6570\u636e\u8bad\u7ec3\u7684\u6570\u636e\u9a71\u52a8\u4ee3\u6a21\u578b\uff0c\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u5305\u62ec\u810f\u6b63\u5f62\u3001\u5747\u8861\u5668\u6ee4\u6ce2\u3001\u504f\u7f6e\u7535\u6d41\u548c\u8c03\u5236\u5f04\u9891\u529f\u7387\u7b49\u53c2\u6570\u3002", "result": "\u5728\u5404\u79cd\u7b26\u53f7\u901f\u7387\u548c\u4f20\u8f93\u8ddd\u79bb\u4e0b\uff0c\u7aef\u5230\u7aef\u4f18\u5316\u65b9\u6848\u90fd\u663e\u793a\u51fa\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u540c\u65f6\u9700\u8981\u66f4\u4f4e\u7684\u8c03\u5236\u5f04\u9891\u529f\u7387\u3001\u66f4\u5c11\u7684\u6ee4\u6ce2\u5668\u6c34\u6ef4\u548c\u66f4\u5c0f\u7684\u4fe1\u53f7\u5e26\u5bbd\u3002", "conclusion": "\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u7aef\u5230\u7aef\u4f18\u5316\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u63d0\u5347\u76f4\u63a5\u8c03\u5236\u6fc0\u5149\u5668\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u77ed\u8ddd\u79bb\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19424", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19424", "abs": "https://arxiv.org/abs/2508.19424", "authors": ["Yifan Dou", "Adam Khadre", "Ruben C Petreaca", "Golrokh Mirzaei"], "title": "MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification", "comment": null, "summary": "Motivation. Understanding the pan-cancer mutational landscape offers critical\ninsights into the molecular mechanisms underlying tumorigenesis. While\npatient-level machine learning techniques have been widely employed to identify\ntumor subtypes, cohort-level clustering, where entire cancer types are grouped\nbased on shared molecular features, has largely relied on classical statistical\nmethods.\n  Results. In this study, we introduce a novel unsupervised contrastive\nlearning framework to cluster 43 cancer types based on coding mutation data\nderived from the COSMIC database. For each cancer type, we construct two\ncomplementary mutation signatures: a gene-level profile capturing nucleotide\nsubstitution patterns across the most frequently mutated genes, and a\nchromosome-level profile representing normalized substitution frequencies\nacross chromosomes. These dual views are encoded using TabNet encoders and\noptimized via a multi-scale contrastive learning objective (NT-Xent loss) to\nlearn unified cancer-type embeddings. We demonstrate that the resulting latent\nrepresentations yield biologically meaningful clusters of cancer types,\naligning with known mutational processes and tissue origins. Our work\nrepresents the first application of contrastive learning to cohort-level cancer\nclustering, offering a scalable and interpretable framework for mutation-driven\ncancer subtyping.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u5bf943\u79cd\u764c\u75c7\u7c7b\u578b\u8fdb\u884c\u805a\u7c7b\u5206\u6790\uff0c\u4f7f\u7528\u57fa\u56e0\u6c34\u5e73\u548c\u67d3\u8272\u4f53\u6c34\u5e73\u7684\u53cc\u91cd\u7a81\u53d8\u7279\u5f81\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u83b7\u5f97\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u764c\u75c7\u805a\u7c7b", "motivation": "\u7406\u89e3\u6cdb\u764c\u7a81\u53d8\u666f\u89c2\u5bf9\u80bf\u7624\u53d1\u751f\u673a\u5236\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u60a3\u8005\u7ea7\u673a\u5668\u5b66\u4e60\u5df2\u5e7f\u6cdb\u7528\u4e8e\u8bc6\u522b\u80bf\u7624\u4e9a\u578b\uff0c\u4f46\u57fa\u4e8e\u5171\u4eab\u5206\u5b50\u7279\u5f81\u7684\u961f\u5217\u7ea7\u764c\u75c7\u805a\u7c7b\u4ecd\u4e3b\u8981\u4f9d\u8d56\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5", "method": "\u4f7f\u7528COSMIC\u6570\u636e\u5e93\u7684\u7f16\u7801\u7a81\u53d8\u6570\u636e\uff0c\u4e3a\u6bcf\u79cd\u764c\u75c7\u6784\u5efa\u57fa\u56e0\u6c34\u5e73\uff08\u6838\u82f7\u9178\u66ff\u6362\u6a21\u5f0f\uff09\u548c\u67d3\u8272\u4f53\u6c34\u5e73\uff08\u6807\u51c6\u5316\u66ff\u6362\u9891\u7387\uff09\u7684\u53cc\u91cd\u7a81\u53d8\u7279\u5f81\u3002\u91c7\u7528TabNet\u7f16\u7801\u5668\u548c\u591a\u5c3a\u5ea6\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff08NT-Xent\u635f\u5931\uff09\u5b66\u4e60\u7edf\u4e00\u7684\u764c\u75c7\u7c7b\u578b\u5d4c\u5165\u8868\u793a", "result": "\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u8868\u793a\u4ea7\u751f\u4e86\u5177\u6709\u751f\u7269\u5b66\u610f\u4e49\u7684\u764c\u75c7\u7c7b\u578b\u805a\u7c7b\uff0c\u4e0e\u5df2\u77e5\u7684\u7a81\u53d8\u8fc7\u7a0b\u548c\u7ec4\u7ec7\u8d77\u6e90\u76f8\u4e00\u81f4", "conclusion": "\u8fd9\u662f\u5bf9\u6bd4\u5b66\u4e60\u5728\u961f\u5217\u7ea7\u764c\u75c7\u805a\u7c7b\u4e2d\u7684\u9996\u6b21\u5e94\u7528\uff0c\u4e3a\u7a81\u53d8\u9a71\u52a8\u7684\u764c\u75c7\u4e9a\u578b\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6"}}
{"id": "2508.19931", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.19931", "abs": "https://arxiv.org/abs/2508.19931", "authors": ["Isabella W. G. da Silva", "Zahra Mobini", "Hien Quoc Ngo", "Michail Matthaiou"], "title": "Cell-Free Massive MIMO-Based Physical-Layer Authentication", "comment": null, "summary": "In this paper, we exploit the cell-free massive multiple-input\nmultiple-output (CF-mMIMO) architecture to design a physical-layer\nauthentication (PLA) framework that can simultaneously authenticate multiple\ndistributed users across the coverage area. Our proposed scheme remains\neffective even in the presence of active adversaries attempting impersonation\nattacks to disrupt the authentication process. Specifically, we introduce a\ntag-based PLA CFmMIMO system, wherein the access points (APs) first estimate\ntheir channels with the legitimate users during an uplink training phase.\nSubsequently, a unique secret key is generated and securely shared between each\nuser and the APs. We then formulate a hypothesis testing problem and derive a\nclosed-form expression for the probability of detection for each user in the\nnetwork. Numerical results validate the effectiveness of the proposed approach,\ndemonstrating that it maintains a high detection probability even as the number\nof users in the system increases.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7684\u7269\u7406\u5c42\u8ba4\u8bc1\u6846\u67b6\uff0c\u53ef\u540c\u65f6\u8ba4\u8bc1\u591a\u4e2a\u5206\u5e03\u5f0f\u7528\u6237\uff0c\u6709\u6548\u62b5\u5fa1\u4e3b\u52a8\u653b\u51fb\u8005\u7684\u5192\u5145\u653b\u51fb", "motivation": "\u4f20\u7edf\u8ba4\u8bc1\u65b9\u6cd5\u5728\u5206\u5e03\u5f0f\u591a\u7528\u6237\u73af\u5883\u4e2d\u9762\u4e34\u5b89\u5168\u6311\u6218\uff0c\u9700\u8981\u8bbe\u8ba1\u80fd\u591f\u540c\u65f6\u8ba4\u8bc1\u591a\u4e2a\u7528\u6237\u4e14\u80fd\u62b5\u6297\u4e3b\u52a8\u653b\u51fb\u7684\u7269\u7406\u5c42\u8ba4\u8bc1\u65b9\u6848", "method": "\u91c7\u7528\u6807\u7b7e\u5f0f\u7269\u7406\u5c42\u8ba4\u8bc1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0a\u884c\u8bad\u7ec3\u9636\u6bb5\u4f30\u8ba1\u4fe1\u9053\uff0c\u751f\u6210\u5e76\u5b89\u5168\u5171\u4eab\u552f\u4e00\u5bc6\u94a5\uff0c\u5efa\u7acb\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\u5e76\u63a8\u5bfc\u68c0\u6d4b\u6982\u7387\u95ed\u5f0f\u8868\u8fbe\u5f0f", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u68c0\u6d4b\u6982\u7387\uff0c\u4e14\u968f\u7740\u7528\u6237\u6570\u91cf\u589e\u52a0\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u6027\u80fd", "conclusion": "\u6240\u63d0\u51fa\u7684\u65e0\u8702\u7a9d\u5927\u89c4\u6a21MIMO\u7269\u7406\u5c42\u8ba4\u8bc1\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u591a\u7528\u6237\u540c\u65f6\u8ba4\u8bc1\uff0c\u5177\u5907\u62b5\u6297\u4e3b\u52a8\u653b\u51fb\u7684\u80fd\u529b\uff0c\u7cfb\u7edf\u6269\u5c55\u6027\u826f\u597d"}}
{"id": "2508.19443", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19443", "abs": "https://arxiv.org/abs/2508.19443", "authors": ["Paimon Goulart", "Shaan Pakala", "Evangelos Papalexakis"], "title": "Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization", "comment": null, "summary": "Producing large complex simulation datasets can often be a time and resource\nconsuming task. Especially when these experiments are very expensive, it is\nbecoming more reasonable to generate synthetic data for downstream tasks.\nRecently, these methods may include using generative machine learning models\nsuch as Generative Adversarial Networks or diffusion models. As these\ngenerative models improve efficiency in producing useful data, we introduce an\ninternal tensor decomposition to these generative models to even further reduce\ncosts. More specifically, for multidimensional data, or tensors, we generate\nthe smaller tensor factors instead of the full tensor, in order to\nsignificantly reduce the model's output and overall parameters. This reduces\nthe costs of generating complex simulation data, and our experiments show the\ngenerated data remains useful. As a result, tensor decomposition has the\npotential to improve efficiency in generative models, especially when\ngenerating multidimensional data, or tensors.", "AI": {"tldr": "\u63d0\u51fa\u5728\u751f\u6210\u6a21\u578b\u4e2d\u5f15\u5165\u5f20\u91cf\u5206\u89e3\u6280\u672f\uff0c\u901a\u8fc7\u751f\u6210\u8f83\u5c0f\u7684\u5f20\u91cf\u56e0\u5b50\u800c\u975e\u5b8c\u6574\u5f20\u91cf\u6765\u663e\u8457\u964d\u4f4e\u6a21\u578b\u8f93\u51fa\u548c\u53c2\u6570\u6570\u91cf\uff0c\u4ece\u800c\u51cf\u5c11\u590d\u6742\u6a21\u62df\u6570\u636e\u7684\u751f\u6210\u6210\u672c\u3002", "motivation": "\u5927\u578b\u590d\u6742\u6a21\u62df\u6570\u636e\u96c6\u7684\u751f\u6210\u901a\u5e38\u8017\u65f6\u8017\u8d44\u6e90\uff0c\u7279\u522b\u662f\u5728\u5b9e\u9a8c\u6210\u672c\u9ad8\u6602\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u751f\u6210\u5408\u6210\u6570\u636e\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u5728\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u6216\u6269\u6563\u6a21\u578b\u7b49\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u5f15\u5165\u5185\u90e8\u5f20\u91cf\u5206\u89e3\uff0c\u9488\u5bf9\u591a\u7ef4\u6570\u636e\uff08\u5f20\u91cf\uff09\u751f\u6210\u8f83\u5c0f\u7684\u5f20\u91cf\u56e0\u5b50\u800c\u975e\u5b8c\u6574\u5f20\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u6a21\u578b\u8f93\u51fa\u548c\u6574\u4f53\u53c2\u6570\u6570\u91cf\uff0c\u51cf\u5c11\u4e86\u590d\u6742\u6a21\u62df\u6570\u636e\u7684\u751f\u6210\u6210\u672c\uff0c\u540c\u65f6\u751f\u6210\u7684\u6570\u636e\u4ecd\u7136\u4fdd\u6301\u5b9e\u7528\u6027\u3002", "conclusion": "\u5f20\u91cf\u5206\u89e3\u6280\u672f\u6709\u6f5c\u529b\u63d0\u9ad8\u751f\u6210\u6a21\u578b\u7684\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u751f\u6210\u591a\u7ef4\u6570\u636e\u6216\u5f20\u91cf\u65f6\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2508.19466", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19466", "abs": "https://arxiv.org/abs/2508.19466", "authors": ["Sourav Chakraborty", "Amit Kiran Rege", "Claire Monteleoni", "Lijun Chen"], "title": "Incentivized Lipschitz Bandits", "comment": null, "summary": "We study incentivized exploration in multi-armed bandit (MAB) settings with\ninfinitely many arms modeled as elements in continuous metric spaces. Unlike\nclassical bandit models, we consider scenarios where the decision-maker\n(principal) incentivizes myopic agents to explore beyond their greedy choices\nthrough compensation, but with the complication of reward drift--biased\nfeedback arising due to the incentives. We propose novel incentivized\nexploration algorithms that discretize the infinite arm space uniformly and\ndemonstrate that these algorithms simultaneously achieve sublinear cumulative\nregret and sublinear total compensation. Specifically, we derive regret and\ncompensation bounds of $\\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the\ncovering dimension of the metric space. Furthermore, we generalize our results\nto contextual bandits, achieving comparable performance guarantees. We validate\nour theoretical findings through numerical simulations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.19479", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.19479", "abs": "https://arxiv.org/abs/2508.19479", "authors": ["Serena Hughes", "Timothy Hamilton", "Tom Kolokotrones", "Eric J. Deeds"], "title": "DeepAtlas: a tool for effective manifold learning", "comment": "38 pages, 7 main text figures, 16 supplementary figures", "summary": "Manifold learning builds on the \"manifold hypothesis,\" which posits that data\nin high-dimensional datasets are drawn from lower-dimensional manifolds.\nCurrent tools generate global embeddings of data, rather than the local maps\nused to define manifolds mathematically. These tools also cannot assess whether\nthe manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,\nan algorithm that generates lower-dimensional representations of the data's\nlocal neighborhoods, then trains deep neural networks that map between these\nlocal embeddings and the original data. Topological distortion is used to\ndetermine whether a dataset is drawn from a manifold and, if so, its\ndimensionality. Application to test datasets indicates that DeepAtlas can\nsuccessfully learn manifold structures. Interestingly, many real datasets,\nincluding single-cell RNA-sequencing, do not conform to the manifold\nhypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a\nmodel that can be used generatively and promises to allow the application of\npowerful tools from differential geometry to a variety of datasets.", "AI": {"tldr": "DeepAtlas\u7b97\u6cd5\u901a\u8fc7\u751f\u6210\u6570\u636e\u7684\u5c40\u90e8\u4f4e\u7ef4\u8868\u793a\u5e76\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6765\u9a8c\u8bc1\u6d41\u5f62\u5047\u8bbe\uff0c\u53d1\u73b0\u8bb8\u591a\u771f\u5b9e\u6570\u636e\u96c6\u5e76\u4e0d\u7b26\u5408\u6d41\u5f62\u5047\u8bbe\uff0c\u4f46\u5728\u7b26\u5408\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u6784\u5efa\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u6d41\u5f62\u5b66\u4e60\u5de5\u5177\u53ea\u80fd\u751f\u6210\u5168\u5c40\u5d4c\u5165\uff0c\u65e0\u6cd5\u9a8c\u8bc1\u6d41\u5f62\u5047\u8bbe\u662f\u5426\u6210\u7acb\uff0c\u4e5f\u65e0\u6cd5\u63d0\u4f9b\u6570\u5b66\u5b9a\u4e49\u4e2d\u7684\u5c40\u90e8\u6620\u5c04\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc4\u4f30\u6570\u636e\u96c6\u662f\u5426\u6765\u81ea\u6d41\u5f62\u5e76\u786e\u5b9a\u5176\u7ef4\u5ea6\u7684\u5de5\u5177\u3002", "method": "DeepAtlas\u7b97\u6cd5\u9996\u5148\u751f\u6210\u6570\u636e\u5c40\u90e8\u90bb\u57df\u7684\u4f4e\u7ef4\u8868\u793a\uff0c\u7136\u540e\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8fd9\u4e9b\u5c40\u90e8\u5d4c\u5165\u548c\u539f\u59cb\u6570\u636e\u4e4b\u95f4\u8fdb\u884c\u6620\u5c04\u3002\u4f7f\u7528\u62d3\u6251\u5931\u771f\u6765\u8bc4\u4f30\u6570\u636e\u96c6\u662f\u5426\u6765\u81ea\u6d41\u5f62\u5e76\u786e\u5b9a\u5176\u7ef4\u5ea6\u3002", "result": "\u5728\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\uff0cDeepAtlas\u6210\u529f\u5b66\u4e60\u4e86\u6d41\u5f62\u7ed3\u6784\u3002\u6709\u8da3\u7684\u662f\uff0c\u8bb8\u591a\u771f\u5b9e\u6570\u636e\u96c6\uff08\u5305\u62ec\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\uff09\u5e76\u4e0d\u7b26\u5408\u6d41\u5f62\u5047\u8bbe\u3002\u5728\u6570\u636e\u6765\u81ea\u6d41\u5f62\u7684\u60c5\u51b5\u4e0b\uff0cDeepAtlas\u53ef\u4ee5\u6784\u5efa\u751f\u6210\u6a21\u578b\u3002", "conclusion": "DeepAtlas\u80fd\u591f\u9a8c\u8bc1\u6d41\u5f62\u5047\u8bbe\u5e76\u786e\u5b9a\u6d41\u5f62\u7ef4\u5ea6\uff0c\u4e3a\u7b26\u5408\u6d41\u5f62\u5047\u8bbe\u7684\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u751f\u6210\u6a21\u578b\uff0c\u4f7f\u5f97\u5fae\u5206\u51e0\u4f55\u7684\u5f3a\u5927\u5de5\u5177\u53ef\u4ee5\u5e94\u7528\u4e8e\u5404\u79cd\u6570\u636e\u96c6\u3002"}}
{"id": "2508.19486", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19486", "abs": "https://arxiv.org/abs/2508.19486", "authors": ["Wangyang Ying", "Nanxu Gong", "Dongjie Wang", "Xinyuan Wang", "Arun Vignesh Malarkkan", "Vivek Gupta", "Chandan K. Reddy", "Yanjie Fu"], "title": "Distribution Shift Aware Neural Tabular Learning", "comment": null, "summary": "Tabular learning transforms raw features into optimized spaces for downstream\ntasks, but its effectiveness deteriorates under distribution shifts between\ntraining and testing data. We formalize this challenge as the Distribution\nShift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature\nTransformation (SAFT) framework to address it. SAFT reframes tabular learning\nfrom a discrete search task into a continuous representation-generation\nparadigm, enabling differentiable optimization over transformed feature sets.\nSAFT integrates three mechanisms to ensure robustness: (i) shift-resistant\nrepresentation via embedding decorrelation and sample reweighting, (ii)\nflatness-aware generation through suboptimal embedding averaging, and (iii)\nnormalization-based alignment between training and test distributions.\nExtensive experiments show that SAFT consistently outperforms prior tabular\nlearning methods in terms of robustness, effectiveness, and generalization\nability under diverse real-world distribution shifts.", "AI": {"tldr": "\u63d0\u51fa\u4e86SAFT\u6846\u67b6\u6765\u89e3\u51b3\u8868\u683c\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u8fde\u7eed\u8868\u793a\u751f\u6210\u8303\u5f0f\u5b9e\u73b0\u53ef\u5fae\u5206\u4f18\u5316\uff0c\u5305\u542b\u4e09\u4e2a\u9c81\u68d2\u6027\u673a\u5236\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02", "motivation": "\u8868\u683c\u5b66\u4e60\u5728\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u5b58\u5728\u5206\u5e03\u504f\u79fb\u65f6\u6548\u679c\u4f1a\u663e\u8457\u4e0b\u964d\uff0c\u9700\u8981\u89e3\u51b3\u5206\u5e03\u504f\u79fb\u8868\u683c\u5b66\u4e60(DSTL)\u95ee\u9898", "method": "SAFT\u6846\u67b6\u5c06\u8868\u683c\u5b66\u4e60\u91cd\u6784\u4e3a\u8fde\u7eed\u8868\u793a\u751f\u6210\u8303\u5f0f\uff0c\u5305\u542b\u4e09\u4e2a\u673a\u5236\uff1a\u5d4c\u5165\u53bb\u76f8\u5173\u548c\u6837\u672c\u91cd\u52a0\u6743\u7684\u6297\u504f\u79fb\u8868\u793a\u3001\u6b21\u4f18\u5d4c\u5165\u5e73\u5747\u7684\u5e73\u5766\u611f\u77e5\u751f\u6210\u3001\u8bad\u7ec3\u6d4b\u8bd5\u5206\u5e03\u95f4\u7684\u5f52\u4e00\u5316\u5bf9\u9f50", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eSAFT\u5728\u9c81\u68d2\u6027\u3001\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u8868\u683c\u5b66\u4e60\u65b9\u6cd5", "conclusion": "SAFT\u901a\u8fc7\u8fde\u7eed\u4f18\u5316\u548c\u591a\u91cd\u9c81\u68d2\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8868\u683c\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5177\u6709\u5f88\u597d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2508.19487", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19487", "abs": "https://arxiv.org/abs/2508.19487", "authors": ["Wangyang Ying", "Jinghan Zhang", "Haoyue Bai", "Nanxu Gong", "Xinyuan Wang", "Kunpeng Liu", "Chandan K. Reddy", "Yanjie Fu"], "title": "Data-Efficient Symbolic Regression via Foundation Model Distillation", "comment": null, "summary": "Discovering interpretable mathematical equations from observed data (a.k.a.\nequation discovery or symbolic regression) is a cornerstone of scientific\ndiscovery, enabling transparent modeling of physical, biological, and economic\nsystems. While foundation models pre-trained on large-scale equation datasets\noffer a promising starting point, they often suffer from negative transfer and\npoor generalization when applied to small, domain-specific datasets. In this\npaper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer\nEmbeddings), a data-efficient fine-tuning framework that adapts foundation\nmodels for symbolic equation discovery in low-data regimes via distillation.\nEQUATE combines symbolic-numeric alignment with evaluator-guided embedding\noptimization, enabling a principled embedding-search-generation paradigm. Our\napproach reformulates discrete equation search as a continuous optimization\ntask in a shared embedding space, guided by data-equation fitness and\nsimplicity. Experiments across three standard public benchmarks (Feynman,\nStrogatz, and black-box datasets) demonstrate that EQUATE consistently\noutperforms state-of-the-art baselines in both accuracy and robustness, while\npreserving low complexity and fast inference. These results highlight EQUATE as\na practical and generalizable solution for data-efficient symbolic regression\nin foundation model distillation settings.", "AI": {"tldr": "EQUATE\u662f\u4e00\u4e2a\u901a\u8fc7\u8d28\u91cf\u5bf9\u9f50\u8f6c\u79fb\u5d4c\u5165\u7684\u65b9\u7a0b\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4f4e\u6570\u636e\u6761\u4ef6\u4e0b\u901a\u8fc7\u84b8\u998f\u65b9\u6cd5\u5fae\u8c03\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u7b26\u53f7\u65b9\u7a0b\u53d1\u73b0\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\u5728\u5927\u89c4\u6a21\u65b9\u7a0b\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u540e\uff0c\u5728\u5e94\u7528\u4e8e\u5c0f\u578b\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u65f6\u7ecf\u5e38\u51fa\u73b0\u8d1f\u8fc1\u79fb\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u6570\u636e\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u3002", "method": "EQUATE\u7ed3\u5408\u7b26\u53f7-\u6570\u503c\u5bf9\u9f50\u548c\u8bc4\u4f30\u5668\u5f15\u5bfc\u7684\u5d4c\u5165\u4f18\u5316\uff0c\u5c06\u79bb\u6563\u65b9\u7a0b\u641c\u7d22\u91cd\u65b0\u8868\u8ff0\u4e3a\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8fde\u7eed\u4f18\u5316\u4efb\u52a1\uff0c\u901a\u8fc7\u6570\u636e-\u65b9\u7a0b\u9002\u5e94\u6027\u548c\u7b80\u6d01\u6027\u8fdb\u884c\u6307\u5bfc\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\uff08Feynman\u3001Strogatz\u548c\u9ed1\u76d2\u6570\u636e\u96c6\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEQUATE\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u548c\u5feb\u901f\u63a8\u7406\u3002", "conclusion": "EQUATE\u4e3a\u57fa\u7840\u6a21\u578b\u84b8\u998f\u8bbe\u7f6e\u4e2d\u7684\u6570\u636e\u9ad8\u6548\u7b26\u53f7\u56de\u5f52\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u63a8\u5e7f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19488", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.19488", "abs": "https://arxiv.org/abs/2508.19488", "authors": ["Xavier Cadet", "Simona Boboila", "Sie Hendrata Dharmawan", "Alina Oprea", "Peter Chin"], "title": "PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense", "comment": "Accepted at GameSec 2025", "summary": "Cyber defense requires automating defensive decision-making under stealthy,\ndeceptive, and continuously evolving adversarial strategies. The FlipIt game\nprovides a foundational framework for modeling interactions between a defender\nand an advanced adversary that compromises a system without being immediately\ndetected. In FlipIt, the attacker and defender compete to control a shared\nresource by performing a Flip action and paying a cost. However, the existing\nFlipIt frameworks rely on a small number of heuristics or specialized learning\ntechniques, which can lead to brittleness and the inability to adapt to new\nattacks. To address these limitations, we introduce PoolFlip, a multi-agent gym\nenvironment that extends the FlipIt game to allow efficient learning for\nattackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent\nreinforcement learning (MARL) approach that leverages population-based training\nto train defender agents equipped to generalize against a range of unknown,\npotentially adaptive opponents. Our empirical results suggest that Flip-PSRO\ndefenders are $2\\times$ more effective than baselines to generalize to a\nheuristic attack not exposed in training. In addition, our newly designed\nownership-based utility functions ensure that Flip-PSRO defenders maintain a\nhigh level of control while optimizing performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86PoolFlip\u73af\u5883\u548cFlip-PSRO\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9632\u5fa1\u8005\u5bf9\u6297\u672a\u77e5\u653b\u51fb\u7b56\u7565\uff0c\u6548\u679c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53472\u500d", "motivation": "\u73b0\u6709FlipIt\u6846\u67b6\u4f9d\u8d56\u5c11\u91cf\u542f\u53d1\u5f0f\u6216\u4e13\u95e8\u5b66\u4e60\u6280\u672f\uff0c\u5b58\u5728\u8106\u5f31\u6027\u548c\u65e0\u6cd5\u9002\u5e94\u65b0\u653b\u51fb\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u81ea\u9002\u5e94\u9632\u5fa1\u65b9\u6cd5", "method": "\u5f00\u53d1PoolFlip\u591a\u667a\u80fd\u4f53\u73af\u5883\u6269\u5c55FlipIt\u6e38\u620f\uff0c\u63d0\u51faFlip-PSRO\u65b9\u6cd5\u5229\u7528\u57fa\u4e8e\u7fa4\u4f53\u7684\u8bad\u7ec3\u6765\u8bad\u7ec3\u9632\u5fa1\u8005\u667a\u80fd\u4f53\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u6240\u6709\u6743\u7684\u6548\u7528\u51fd\u6570", "result": "Flip-PSRO\u9632\u5fa1\u8005\u5bf9\u8bad\u7ec3\u4e2d\u672a\u66b4\u9732\u7684\u542f\u53d1\u5f0f\u653b\u51fb\u7684\u6cdb\u5316\u80fd\u529b\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad82\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6c34\u5e73\u63a7\u5236", "conclusion": "Flip-PSRO\u4e3a\u7f51\u7edc\u9632\u5fa1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u9002\u5e94\u51b3\u7b56\u6846\u67b6\uff0c\u80fd\u591f\u5bf9\u6297\u4e0d\u65ad\u6f14\u5316\u7684\u5bf9\u6297\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u9632\u5fa1\u6548\u679c"}}
{"id": "2508.19506", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19506", "abs": "https://arxiv.org/abs/2508.19506", "authors": ["Zhiyi Kuang", "Ryan Rong", "YuCheng Yuan", "Allen Nie"], "title": "Learning Game-Playing Agents with Generative Code Optimization", "comment": "ICML 2025 Workshop on Programmatic Representations for Agent\n  Learning, Vancouver, Canada", "summary": "We present a generative optimization approach for learning game-playing\nagents, where policies are represented as Python programs and refined using\nlarge language models (LLMs). Our method treats decision-making policies as\nself-evolving code, with current observation as input and an in-game action as\noutput, enabling agents to self-improve through execution traces and natural\nlanguage feedback with minimal human intervention. Applied to Atari games, our\ngame-playing Python program achieves performance competitive with deep\nreinforcement learning (RL) baselines while using significantly less training\ntime and much fewer environment interactions. This work highlights the promise\nof programmatic policy representations for building efficient, adaptable agents\ncapable of complex, long-horizon reasoning.", "AI": {"tldr": "\u4f7f\u7528Python\u7a0b\u5e8f\u4f5c\u4e3a\u7b56\u7565\u8868\u793a\uff0c\u901a\u8fc7LLM\u4f18\u5316\u751f\u6210\u6e38\u620f\u73a9\u5bb6\u4ee3\u7406\uff0c\u5728Atari\u6e38\u620f\u4e2d\u8fbe\u5230\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u4f46\u8017\u8d39\u66f4\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u73af\u5883\u4ea4\u4e92", "motivation": "\u63a2\u7d22\u7a0b\u5e8f\u5316\u7b56\u7565\u8868\u793a\u5728\u6784\u5efa\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u6e38\u620f\u73a9\u5bb6\u4ee3\u7406\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u81ea\u6211\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u590d\u6742\u957f\u671f\u671b\u7684\u7406\u6027\u51b3\u7b56\u80fd\u529b", "method": "\u5c06\u51b3\u7b56\u7b56\u7565\u8868\u793a\u4e3aPython\u7a0b\u5e8f\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u751f\u6210\u5f0f\u4f18\u5316\uff0c\u901a\u8fc7\u6267\u884c\u8e2a\u8ff9\u548c\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u5b9e\u73b0\u81ea\u6211\u6539\u8fdb\uff0c\u6700\u5c0f\u5316\u4eba\u5de5\u5e2e\u52a9", "result": "\u5728Atari\u6e38\u620f\u4e2d\u8fbe\u5230\u4e0e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u76f8\u7ade\u4e89\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u540c\u65f6\u4f7f\u7528\u663e\u8457\u66f4\u5c11\u7684\u8bad\u7ec3\u65f6\u95f4\u548c\u73af\u5883\u4ea4\u4e92", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u544a\u8bc9\u4e86\u7a0b\u5e8f\u5316\u7b56\u7565\u8868\u793a\u5728\u6784\u5efa\u9ad8\u6548\u3001\u9002\u5e94\u6027\u5f3a\u7684\u4ee3\u7406\u65b9\u9762\u7684\u524d\u666f\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u7684\u957f\u671f\u671b\u7406\u6027\u4efb\u52a1"}}
{"id": "2508.19554", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19554", "abs": "https://arxiv.org/abs/2508.19554", "authors": ["Haruki Yonekura", "Ren Ozeki", "Tatsuya Amano", "Hamada Rizk", "Hirozumi Yamaguchi"], "title": "MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data", "comment": "Accepted to The 33rd ACM International Conference on Advances in\n  Geographic Information Systems(SIGSPATIAL '25) as a short paper in the Short\n  Paper Track", "summary": "Modern mobility platforms have stored vast streams of GPS trajectories,\ntemporal metadata, free-form textual notes, and other unstructured data.\nPrivacy statutes such as the GDPR require that any individual's contribution be\nunlearned on demand, yet retraining deep models from scratch for every request\nis untenable. We introduce MobText-SISA, a scalable machine-unlearning\nframework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)\ntraining to heterogeneous spatio-temporal data. MobText-SISA first embeds each\ntrip's numerical and linguistic features into a shared latent space, then\nemploys similarity-aware clustering to distribute samples across shards so that\nfuture deletions touch only a single constituent model while preserving\ninter-shard diversity. Each shard is trained incrementally; at inference time,\nconstituent predictions are aggregated to yield the output. Deletion requests\ntrigger retraining solely of the affected shard from its last valid checkpoint,\nguaranteeing exact unlearning. Experiments on a ten-month real-world mobility\nlog demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,\nand (ii) consistently outperforms random sharding in both error and convergence\nspeed. These results establish MobText-SISA as a practical foundation for\nprivacy-compliant analytics on multimodal mobility data at urban scale.", "AI": {"tldr": "MobText-SISA\u662f\u4e00\u4e2a\u9488\u5bf9\u5f02\u6784\u65f6\u7a7a\u6570\u636e\u7684\u673a\u5668\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u76f8\u4f3c\u6027\u611f\u77e5\u805a\u7c7b\u548c\u5206\u7247\u8bad\u7ec3\u5b9e\u73b0\u9ad8\u6548\u5220\u9664\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u6ee1\u8db3GDPR\u9690\u79c1\u8981\u6c42\u3002", "motivation": "\u73b0\u4ee3\u79fb\u52a8\u5e73\u53f0\u5b58\u50a8\u4e86\u5927\u91cfGPS\u8f68\u8ff9\u548c\u6587\u672c\u6570\u636e\uff0cGDPR\u7b49\u9690\u79c1\u6cd5\u89c4\u8981\u6c42\u80fd\u591f\u6309\u9700\u5220\u9664\u4e2a\u4eba\u6570\u636e\uff0c\u4f46\u4e3a\u6bcf\u4e2a\u5220\u9664\u8bf7\u6c42\u91cd\u65b0\u8bad\u7ec3\u6df1\u5ea6\u6a21\u578b\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u6269\u5c55SISA\u8bad\u7ec3\u65b9\u6cd5\uff0c\u9996\u5148\u5c06\u884c\u7a0b\u7684\u6570\u503c\u548c\u8bed\u8a00\u7279\u5f81\u5d4c\u5165\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u4f7f\u7528\u76f8\u4f3c\u6027\u611f\u77e5\u805a\u7c7b\u5c06\u6837\u672c\u5206\u5e03\u5230\u5206\u7247\u4e2d\uff0c\u6bcf\u4e2a\u5206\u7247\u589e\u91cf\u8bad\u7ec3\uff0c\u5220\u9664\u65f6\u4ec5\u9700\u91cd\u65b0\u8bad\u7ec3\u53d7\u5f71\u54cd\u7684\u5206\u7247\u3002", "result": "\u572810\u4e2a\u6708\u7684\u771f\u5b9e\u79fb\u52a8\u65e5\u5fd7\u5b9e\u9a8c\u4e2d\uff0cMobText-SISA\u4fdd\u6301\u4e86\u57fa\u7ebf\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5728\u9519\u8bef\u7387\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u968f\u673a\u5206\u7247\u65b9\u6cd5\u3002", "conclusion": "MobText-SISA\u4e3a\u57ce\u5e02\u89c4\u6a21\u7684\u591a\u6a21\u6001\u79fb\u52a8\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u9690\u79c1\u5408\u89c4\u57fa\u7840\u6846\u67b6\uff0c\u80fd\u591f\u4fdd\u8bc1\u7cbe\u786e\u9057\u5fd8\u540c\u65f6\u7ef4\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.19564", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19564", "abs": "https://arxiv.org/abs/2508.19564", "authors": ["Yuhang Liu", "Tao Li", "Zhehao Huang", "Zuopeng Yang", "Xiaolin Huang"], "title": "Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models", "comment": null, "summary": "Fine-tuning large-scale pre-trained models with limited data presents\nsignificant challenges for generalization. While Sharpness-Aware Minimization\n(SAM) has proven effective in improving generalization by seeking flat minima,\nits substantial extra memory and computation overhead make it impractical for\nlarge models. Integrating SAM with parameter-efficient fine-tuning methods like\nLow-Rank Adaptation (LoRA) is a promising direction. However, we find that\ndirectly applying SAM to LoRA parameters limits the sharpness optimization to a\nrestricted subspace, hindering its effectiveness. To address this limitation,\nwe propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an\nauxiliary LoRA module to model SAM's adversarial weight perturbations. It\ndecouples SAM's weight perturbations from LoRA optimization: the primary LoRA\nmodule adapts to specific tasks via standard gradient descent, while the\nauxiliary module captures the sharpness of the loss landscape through gradient\nascent. Such dual-module design enables Bi-LoRA to capture broader sharpness\nfor achieving flatter minima while remaining memory-efficient. Another\nimportant benefit is that the dual design allows for simultaneous optimization\nand perturbation, eliminating SAM's doubled training costs. Extensive\nexperiments across diverse tasks and architectures demonstrate Bi-LoRA's\nefficiency and effectiveness in enhancing generalization.", "AI": {"tldr": "Bi-LoRA\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u8f85\u52a9LoRA\u6a21\u5757\u6765\u6a21\u62dfSAM\u7684\u5bf9\u6297\u6027\u6743\u91cd\u6270\u52a8\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u5e73\u5766\u7684\u6700\u5c0f\u503c\uff0c\u6d88\u9664\u4e86SAM\u7684\u53cc\u500d\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "SAM\u867d\u7136\u80fd\u901a\u8fc7\u5bfb\u627e\u5e73\u5766\u6700\u5c0f\u503c\u6765\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5176\u5de8\u5927\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u5f00\u9500\u4f7f\u5176\u4e0d\u9002\u7528\u4e8e\u5927\u578b\u6a21\u578b\u3002\u76f4\u63a5\u5bf9LoRA\u53c2\u6570\u5e94\u7528SAM\u4f1a\u9650\u5236\u9510\u5ea6\u4f18\u5316\u5230\u53d7\u9650\u5b50\u7a7a\u95f4\uff0c\u5f71\u54cd\u6548\u679c\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u4f4e\u79e9\u9002\u5e94(Bi-LoRA)\uff0c\u5f15\u5165\u8f85\u52a9LoRA\u6a21\u5757\u6765\u5efa\u6a21SAM\u7684\u5bf9\u6297\u6743\u91cd\u6270\u52a8\u3002\u4e3bLoRA\u6a21\u5757\u901a\u8fc7\u6807\u51c6\u68af\u5ea6\u4e0b\u964d\u9002\u5e94\u4efb\u52a1\uff0c\u8f85\u52a9\u6a21\u5757\u901a\u8fc7\u68af\u5ea6\u4e0a\u5347\u6355\u6349\u635f\u5931\u666f\u89c2\u7684\u9510\u5ea6\u3002", "result": "\u5728\u591a\u79cd\u4efb\u52a1\u548c\u67b6\u6784\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86Bi-LoRA\u5728\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u6548\u7387\u548c\u6709\u6548\u6027\u3002", "conclusion": "Bi-LoRA\u901a\u8fc7\u53cc\u6a21\u5757\u8bbe\u8ba1\u6210\u529f\u89e3\u51b3\u4e86SAM\u5728\u5927\u578b\u6a21\u578b\u4e0a\u7684\u5e94\u7528\u9650\u5236\uff0c\u5728\u4fdd\u6301\u5185\u5b58\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2508.19567", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19567", "abs": "https://arxiv.org/abs/2508.19567", "authors": ["Sheryl Mathew", "N Harshit"], "title": "Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning", "comment": null, "summary": "In reinforcement learning with human feedback (RLHF), reward models can\nefficiently learn and amplify latent biases within multimodal datasets, which\ncan lead to imperfect policy optimization through flawed reward signals and\ndecreased fairness. Bias mitigation studies have often applied passive\nconstraints, which can fail under causal confounding. Here, we present a\ncounterfactual reward model that introduces causal inference with multimodal\nrepresentation learning to provide an unsupervised, bias-resilient reward\nsignal. The heart of our contribution is the Counterfactual Trust Score, an\naggregated score consisting of four components: (1) counterfactual shifts that\ndecompose political framing bias from topical bias; (2) reconstruction\nuncertainty during counterfactual perturbations; (3) demonstrable violations of\nfairness rules for each protected attribute; and (4) temporal reward shifts\naligned with dynamic trust measures. We evaluated the framework on a multimodal\nfake versus true news dataset, which exhibits framing bias, class imbalance,\nand distributional drift. Following methodologies similar to unsupervised drift\ndetection from representation-based distances [1] and temporal robustness\nbenchmarking in language models [2], we also inject synthetic bias across\nsequential batches to test robustness. The resulting system achieved an\naccuracy of 89.12% in fake news detection, outperforming the baseline reward\nmodels. More importantly, it reduced spurious correlations and unfair\nreinforcement signals. This pipeline outlines a robust and interpretable\napproach to fairness-aware RLHF, offering tunable bias reduction thresholds and\nincreasing reliability in dynamic real-time policy making.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\u7684\u591a\u6a21\u6001\u53cd\u4e8b\u5b9e\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7Counterfactual Trust Score\u6765\u51cf\u5c11RLHF\u4e2d\u7684\u504f\u89c1\uff0c\u5728\u5047\u65b0\u95fb\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u523089.12%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "RLHF\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u5bb9\u6613\u5b66\u4e60\u548c\u653e\u5927\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\uff0c\u5bfc\u81f4\u6709\u7f3a\u9677\u7684\u5956\u52b1\u4fe1\u53f7\u548c\u516c\u5e73\u6027\u4e0b\u964d\u3002\u73b0\u6709\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u5f80\u5f80\u91c7\u7528\u88ab\u52a8\u7ea6\u675f\uff0c\u5728\u56e0\u679c\u6df7\u6dc6\u60c5\u51b5\u4e0b\u4f1a\u5931\u6548\u3002", "method": "\u5f00\u53d1\u4e86\u53cd\u4e8b\u5b9e\u5956\u52b1\u6a21\u578b\uff0c\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u548c\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\uff0c\u63d0\u51faCounterfactual Trust Score\u5305\u542b\u56db\u4e2a\u7ec4\u4ef6\uff1a\u53cd\u4e8b\u5b9e\u504f\u79fb\u3001\u91cd\u6784\u4e0d\u786e\u5b9a\u6027\u3001\u516c\u5e73\u89c4\u5219\u8fdd\u53cd\u68c0\u6d4b\u548c\u65f6\u95f4\u5956\u52b1\u504f\u79fb\u3002", "result": "\u5728\u591a\u6a21\u6001\u771f\u5047\u65b0\u95fb\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u51c6\u786e\u7387\u8fbe\u523089.12%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u5956\u52b1\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u865a\u5047\u76f8\u5173\u6027\u548c\u4e0d\u516c\u5e73\u7684\u5f3a\u5316\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u516c\u5e73\u611f\u77e5\u7684RLHF\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u53ef\u8c03\u8282\u7684\u504f\u89c1\u51cf\u5c11\u9608\u503c\uff0c\u63d0\u9ad8\u4e86\u52a8\u6001\u5b9e\u65f6\u7b56\u7565\u5236\u5b9a\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.19570", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19570", "abs": "https://arxiv.org/abs/2508.19570", "authors": ["Dawei Li", "Yue Huang", "Ming Li", "Tianyi Zhou", "Xiangliang Zhang", "Huan Liu"], "title": "Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era", "comment": "Accepted by CIKM 2025 Tutorial", "summary": "Generative models such as Large Language Models, Diffusion Models, and\ngenerative adversarial networks have recently revolutionized the creation of\nsynthetic data, offering scalable solutions to data scarcity, privacy, and\nannotation challenges in data mining. This tutorial introduces the foundations\nand latest advances in synthetic data generation, covers key methodologies and\npractical frameworks, and discusses evaluation strategies and applications.\nAttendees will gain actionable insights into leveraging generative synthetic\ndata to enhance data mining research and practice. More information can be\nfound on our website: https://syndata4dm.github.io/.", "AI": {"tldr": "\u672c\u6559\u7a0b\u4ecb\u7ecd\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u9762\u7684\u57fa\u7840\u77e5\u8bc6\u548c\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u5173\u952e\u65b9\u6cd5\u3001\u5b9e\u8df5\u6846\u67b6\u3001\u8bc4\u4f30\u7b56\u7565\u548c\u5e94\u7528\uff0c\u5e2e\u52a9\u89e3\u51b3\u6570\u636e\u6316\u6398\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u3001\u9690\u79c1\u548c\u6807\u6ce8\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u6316\u6398\u4e2d\u9762\u4e34\u7684\u6570\u636e\u7a00\u7f3a\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u6807\u6ce8\u6210\u672c\u9ad8\u7b49\u6311\u6218\uff0c\u5229\u7528\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4ecb\u7ecd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u6269\u6563\u6a21\u578b\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7b49\u751f\u6210\u6a21\u578b\u7684\u57fa\u7840\u77e5\u8bc6\u548c\u6700\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u5b9e\u7528\u6846\u67b6\u548c\u8bc4\u4f30\u7b56\u7565\u3002", "result": "\u4e3a\u53c2\u4e0e\u8005\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5229\u7528\u751f\u6210\u5f0f\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u6570\u636e\u6316\u6398\u7814\u7a76\u548c\u5b9e\u8df5\u3002", "conclusion": "\u751f\u6210\u6a21\u578b\u4e3a\u5408\u6210\u6570\u636e\u751f\u6210\u63d0\u4f9b\u4e86\u9769\u547d\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6570\u636e\u6316\u6398\u4e2d\u7684\u591a\u79cd\u6311\u6218\uff0c\u5177\u6709\u91cd\u8981\u7684\u7814\u7a76\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.19571", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19571", "abs": "https://arxiv.org/abs/2508.19571", "authors": ["Yunlong Lin", "Chao Lu", "Tongshuai Wu", "Xiaocong Zhao", "Guodong Du", "Yanwei Sun", "Zirui Li", "Jianwei Gong"], "title": "Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal", "comment": "Official code: https://github.com/BIT-Jack/SyReM", "summary": "Deep neural networks (DNN) have achieved remarkable success in motion\nforecasting. However, most DNN-based methods suffer from catastrophic\nforgetting and fail to maintain their performance in previously learned\nscenarios after adapting to new data. Recent continual learning (CL) studies\naim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the\nability to retain learned knowledge. Yet, excessive emphasis on the memory\nstability often impairs learning plasticity, i.e., the capacity of DNN to\nacquire new information effectively. To address such stability-plasticity\ndilemma, this study proposes a novel CL method, synergetic memory rehearsal\n(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory\nbuffer to represent learned knowledge. To ensure memory stability, it employs\nan inequality constraint that limits increments in the average loss over the\nmemory buffer. Synergistically, a selective memory rehearsal mechanism is\ndesigned to enhance learning plasticity by selecting samples from the memory\nbuffer that are most similar to recently observed data. This selection is based\non an online-measured cosine similarity of loss gradients, ensuring targeted\nmemory rehearsal. Since replayed samples originate from learned scenarios, this\nmemory rehearsal mechanism avoids compromising memory stability. We validate\nSyReM under an online CL paradigm where training samples from diverse scenarios\narrive as a one-pass stream. Experiments on 11 naturalistic driving datasets\nfrom INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM\nsignificantly mitigates catastrophic forgetting in past scenarios while\nimproving forecasting accuracy in new ones. The implementation is publicly\navailable at https://github.com/BIT-Jack/SyReM.", "AI": {"tldr": "\u63d0\u51faSyReM\u65b9\u6cd5\u89e3\u51b3\u8fd0\u52a8\u9884\u6d4b\u4e2d\u6301\u7eed\u5b66\u4e60\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u901a\u8fc7\u7ea6\u675f\u635f\u5931\u589e\u91cf\u4fdd\u8bc1\u7a33\u5b9a\u6027\uff0c\u57fa\u4e8e\u68af\u5ea6\u76f8\u4f3c\u6027\u9009\u62e9\u8bb0\u5fc6\u6837\u672c\u63d0\u5347\u53ef\u5851\u6027", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8fd0\u52a8\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u4f46\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u73b0\u6709\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u8fc7\u5ea6\u5f3a\u8c03\u8bb0\u5fc6\u7a33\u5b9a\u6027\u4f1a\u635f\u5bb3\u5b66\u4e60\u53ef\u5851\u6027", "method": "\u7ef4\u62a4\u7d27\u51d1\u8bb0\u5fc6\u7f13\u51b2\u533a\uff0c\u4f7f\u7528\u4e0d\u7b49\u5f0f\u7ea6\u675f\u9650\u5236\u5e73\u5747\u635f\u5931\u589e\u91cf\u786e\u4fdd\u7a33\u5b9a\u6027\uff0c\u57fa\u4e8e\u635f\u5931\u68af\u5ea6\u4f59\u5f26\u76f8\u4f3c\u5ea6\u9009\u62e9\u76f8\u4f3c\u6837\u672c\u8fdb\u884c\u9488\u5bf9\u6027\u8bb0\u5fc6\u91cd\u653e", "result": "\u572811\u4e2a\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u975eCL\u548cCL\u57fa\u7ebf\u65b9\u6cd5\uff0cSyReM\u663e\u8457\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u5e76\u63d0\u5347\u65b0\u573a\u666f\u9884\u6d4b\u7cbe\u5ea6", "conclusion": "SyReM\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u5728\u8fd0\u52a8\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u5e73\u8861"}}
{"id": "2508.19589", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19589", "abs": "https://arxiv.org/abs/2508.19589", "authors": ["Arshia Hemmat", "Afsaneh Fatemi"], "title": "Delta-Audit: Explaining What Changes When Models Change", "comment": "7 pages, 1 figure, 4 tables", "summary": "Model updates (new hyperparameters, kernels, depths, solvers, or data) change\nperformance, but the \\emph{reason} often remains opaque. We introduce\n\\textbf{Delta-Attribution} (\\mbox{$\\Delta$-Attribution}), a model-agnostic\nframework that explains \\emph{what changed} between versions $A$ and $B$ by\ndifferencing per-feature attributions: $\\Delta\\phi(x)=\\phi_B(x)-\\phi_A(x)$. We\nevaluate $\\Delta\\phi$ with a \\emph{$\\Delta$-Attribution Quality Suite} covering\nmagnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,\nJensen--Shannon divergence), behavioural alignment (Delta Conservation Error,\nDCE; Behaviour--Attribution Coupling, BAC; CO$\\Delta$F), and robustness (noise,\nbaseline sensitivity, grouped occlusion).\n  Instantiated via fast occlusion/clamping in standardized space with a\nclass-anchored margin and baseline averaging, we audit 45 settings: five\nclassical families (Logistic Regression, SVC, Random Forests, Gradient\nBoosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B\npairs per family. \\textbf{Findings.} Inductive-bias changes yield large,\nbehaviour-aligned deltas (e.g., SVC poly$\\!\\rightarrow$rbf on Breast Cancer:\nBAC$\\approx$0.998, DCE$\\approx$6.6; Random Forest feature-rule swap on Digits:\nBAC$\\approx$0.997, DCE$\\approx$7.5), while ``cosmetic'' tweaks (SVC\n\\texttt{gamma=scale} vs.\\ \\texttt{auto}, $k$NN search) show\nrank-overlap@10$=1.0$ and DCE$\\approx$0. The largest redistribution appears for\ndeeper GB on Breast Cancer (JSD$\\approx$0.357). $\\Delta$-Attribution offers a\nlightweight update audit that complements accuracy by distinguishing benign\nchanges from behaviourally meaningful or risky reliance shifts.", "AI": {"tldr": "Delta-Attribution\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u7248\u672c\u6a21\u578b\u7684\u7279\u5f81\u5f52\u56e0\u5dee\u5f02\u6765\u89e3\u91ca\u6a21\u578b\u66f4\u65b0\u7684\u53d8\u5316\u539f\u56e0\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u7684\u66f4\u65b0\u5ba1\u8ba1\u65b9\u6cd5\u3002", "motivation": "\u6a21\u578b\u66f4\u65b0\uff08\u5982\u8d85\u53c2\u6570\u3001\u6838\u51fd\u6570\u3001\u6df1\u5ea6\u3001\u6c42\u89e3\u5668\u6216\u6570\u636e\u53d8\u5316\uff09\u4f1a\u6539\u53d8\u6027\u80fd\uff0c\u4f46\u53d8\u5316\u7684\u539f\u56e0\u5f80\u5f80\u4e0d\u900f\u660e\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u89e3\u91ca\u6a21\u578b\u7248\u672c\u95f4\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51faDelta-Attribution\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u7279\u5f81\u5f52\u56e0\u5dee\u5f02\u0394\u03c6(x)=\u03c6_B(x)-\u03c6_A(x)\u6765\u89e3\u91ca\u6a21\u578b\u53d8\u5316\u3002\u4f7f\u7528\u6807\u51c6\u5316\u7a7a\u95f4\u4e2d\u7684\u5feb\u901f\u906e\u6321/\u94b3\u4f4d\u65b9\u6cd5\uff0c\u914d\u5408\u7c7b\u522b\u951a\u5b9a\u8fb9\u754c\u548c\u57fa\u7ebf\u5e73\u5747\uff0c\u572845\u4e2a\u8bbe\u7f6e\u4e2d\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5f52\u7eb3\u504f\u7f6e\u53d8\u5316\u4ea7\u751f\u5927\u7684\u884c\u4e3a\u5bf9\u9f50\u5dee\u5f02\uff08\u5982SVC\u591a\u9879\u5f0f\u2192RBF\u6838\uff1aBAC\u22480.998\uff0cDCE\u22486.6\uff09\uff0c\u800c\u8868\u9762\u8c03\u6574\uff08\u5982SVC gamma\u53c2\u6570\u53d8\u5316\uff09\u663e\u793arank-overlap@10=1.0\u548cDCE\u22480\u3002\u6700\u660e\u663e\u7684\u91cd\u65b0\u5206\u5e03\u51fa\u73b0\u5728\u6df1\u5ea6\u68af\u5ea6\u63d0\u5347\u4e0a\uff08JSD\u22480.357\uff09\u3002", "conclusion": "Delta-Attribution\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u66f4\u65b0\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u533a\u5206\u826f\u6027\u53d8\u5316\u4e0e\u884c\u4e3a\u4e0a\u6709\u610f\u4e49\u6216\u98ce\u9669\u4f9d\u8d56\u8f6c\u79fb\u7684\u53d8\u5316\uff0c\u8865\u5145\u4e86\u51c6\u786e\u6027\u7684\u8bc4\u4f30\u3002"}}
{"id": "2508.19597", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19597", "abs": "https://arxiv.org/abs/2508.19597", "authors": ["Zirui Li", "Yunlong Lin", "Guodong Du", "Xiaocong Zhao", "Cheng Gong", "Chen Lv", "Chao Lu", "Jianwei Gong"], "title": "Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities", "comment": "19 pages, 6 figures", "summary": "Artificial intelligence underpins most smart city services, yet deep neural\nnetwork (DNN) that forecasts vehicle motion still struggle with catastrophic\nforgetting, the loss of earlier knowledge when models are updated. Conventional\nfixes enlarge the training set or replay past data, but these strategies incur\nhigh data collection costs, sample inefficiently and fail to balance long- and\nshort-term experience, leaving them short of human-like continual learning.\nHere we introduce Dual-LS, a task-free, online continual learning paradigm for\nDNN-based motion forecasting that is inspired by the complementary learning\nsystem of the human brain. Dual-LS pairs two synergistic memory rehearsal\nreplay mechanisms to accelerate experience retrieval while dynamically\ncoordinating long-term and short-term knowledge representations. Tests on\nnaturalistic data spanning three countries, over 772,000 vehicles and\ncumulative testing mileage of 11,187 km show that Dual-LS mitigates\ncatastrophic forgetting by up to 74.31\\% and reduces computational resource\ndemand by up to 94.02\\%, markedly boosting predictive stability in vehicle\nmotion forecasting without inflating data requirements. Meanwhile, it endows\nDNN-based vehicle motion forecasting with computation efficient and human-like\ncontinual learning adaptability fit for smart cities.", "AI": {"tldr": "Dual-LS\u662f\u4e00\u79cd\u53d7\u4eba\u7c7b\u5927\u8111\u4e92\u8865\u5b66\u4e60\u7cfb\u7edf\u542f\u53d1\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u53cc\u8bb0\u5fc6\u91cd\u653e\u673a\u5236\u89e3\u51b3DNN\u8f66\u8f86\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7a33\u5b9a\u6027\u5e76\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u667a\u80fd\u57ce\u5e02\u670d\u52a1\u4f9d\u8d56AI\uff0c\u4f46\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u8f66\u8f86\u8fd0\u52a8\u9884\u6d4b\u4e2d\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u3001\u6837\u672c\u6548\u7387\u4f4e\uff0c\u4e14\u65e0\u6cd5\u5e73\u8861\u957f\u77ed\u671f\u7ecf\u9a8c\uff0c\u65e0\u6cd5\u5b9e\u73b0\u7c7b\u4eba\u7684\u6301\u7eed\u5b66\u4e60\u3002", "method": "\u63d0\u51faDual-LS\u8303\u5f0f\uff0c\u91c7\u7528\u4e24\u79cd\u534f\u540c\u7684\u8bb0\u5fc6\u91cd\u653e\u673a\u5236\uff0c\u52a0\u901f\u7ecf\u9a8c\u68c0\u7d22\u5e76\u52a8\u6001\u534f\u8c03\u957f\u77ed\u671f\u77e5\u8bc6\u8868\u793a\uff0c\u5b9e\u73b0\u4efb\u52a1\u65e0\u5173\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u56fd\u5bb6\u8d85\u8fc777.2\u4e07\u8f86\u8f66\u8f86\u3001\u7d2f\u8ba1\u6d4b\u8bd5\u91cc\u7a0b11,187\u516c\u91cc\u7684\u81ea\u7136\u6570\u636e\u6d4b\u8bd5\u4e2d\uff0cDual-LS\u5c06\u707e\u96be\u6027\u9057\u5fd8\u51cf\u5c1174.31%\uff0c\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u964d\u4f4e94.02%\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7a33\u5b9a\u6027\u3002", "conclusion": "Dual-LS\u4e3a\u57fa\u4e8eDNN\u7684\u8f66\u8f86\u8fd0\u52a8\u9884\u6d4b\u63d0\u4f9b\u4e86\u8ba1\u7b97\u9ad8\u6548\u3001\u7c7b\u4eba\u7684\u6301\u7eed\u5b66\u4e60\u9002\u5e94\u6027\uff0c\u9002\u5408\u667a\u80fd\u57ce\u5e02\u5e94\u7528\uff0c\u4e14\u4e0d\u589e\u52a0\u6570\u636e\u9700\u6c42\u3002"}}
{"id": "2508.19598", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19598", "abs": "https://arxiv.org/abs/2508.19598", "authors": ["Zhiwei Li", "Yong Hu", "Wenqing Wang"], "title": "Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning", "comment": null, "summary": "The functionality of Large Language Model (LLM) agents is primarily\ndetermined by two capabilities: action planning and answer summarization. The\nformer, action planning, is the core capability that dictates an agent's\nperformance. However, prevailing training paradigms employ end-to-end,\nmulti-objective optimization that jointly trains both capabilities. This\nparadigm faces two critical challenges: imbalanced optimization objective\nallocation and scarcity of verifiable data, making it difficult to enhance the\nagent's planning capability. To address these challenges, we propose\nReinforcement Learning with Tool-use Rewards (RLTR), a novel framework that\ndecouples the training process to enable a focused, single-objective\noptimization of the planning module. Crucially, RLTR introduces a reward signal\nbased on tool-use completeness to directly evaluate the quality of tool\ninvocation sequences. This method offers a more direct and reliable training\nsignal than assessing the final response content, thereby obviating the need\nfor verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%\nimprovement in planning performance compared to end-to-end baselines. Moreover,\nthis enhanced planning capability, in turn, translates to a 5%-6% increase in\nthe final response quality of the overall agent system.", "AI": {"tldr": "\u63d0\u51fa\u4e86RLTR\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u4f7f\u7528\u5b8c\u6574\u6027\u5956\u52b1\u4fe1\u53f7\u89e3\u8026\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4e13\u6ce8\u4e8e\u89c4\u5212\u6a21\u5757\u7684\u5355\u76ee\u6807\u4f18\u5316\uff0c\u76f8\u6bd4\u7aef\u5230\u7aef\u65b9\u6cd5\u63d0\u5347\u4e868-12%\u7684\u89c4\u5212\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u8bad\u7ec3\u91c7\u7528\u7aef\u5230\u7aef\u591a\u76ee\u6807\u4f18\u5316\uff0c\u5b58\u5728\u76ee\u6807\u5206\u914d\u4e0d\u5e73\u8861\u548c\u53ef\u9a8c\u8bc1\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u6709\u6548\u63d0\u5347\u89c4\u5212\u80fd\u529b\u3002", "method": "RLTR\u6846\u67b6\u89e3\u8026\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u57fa\u4e8e\u5de5\u5177\u4f7f\u7528\u5b8c\u6574\u6027\u8bbe\u8ba1\u5956\u52b1\u4fe1\u53f7\uff0c\u76f4\u63a5\u8bc4\u4f30\u5de5\u5177\u8c03\u7528\u5e8f\u5217\u8d28\u91cf\uff0c\u5b9e\u73b0\u89c4\u5212\u6a21\u5757\u7684\u5355\u76ee\u6807\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRLTR\u5728\u89c4\u5212\u6027\u80fd\u4e0a\u6bd4\u7aef\u5230\u7aef\u57fa\u7ebf\u63d0\u53478-12%\uff0c\u589e\u5f3a\u7684\u89c4\u5212\u80fd\u529b\u4f7f\u6574\u4f53\u7cfb\u7edf\u6700\u7ec8\u54cd\u5e94\u8d28\u91cf\u63d0\u53475-6%\u3002", "conclusion": "RLTR\u901a\u8fc7\u89e3\u8026\u8bad\u7ec3\u548c\u5de5\u5177\u4f7f\u7528\u5956\u52b1\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u89c4\u5212\u80fd\u529b\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.19609", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2508.19609", "abs": "https://arxiv.org/abs/2508.19609", "authors": ["Zhuohang Zhu", "Haodong Chen", "Qiang Qu", "Vera Chung"], "title": "FinCast: A Foundation Model for Financial Time-Series Forecasting", "comment": null, "summary": "Financial time-series forecasting is critical for maintaining economic\nstability, guiding informed policymaking, and promoting sustainable investment\npractices. However, it remains challenging due to various underlying pattern\nshifts. These shifts arise primarily from three sources: temporal\nnon-stationarity (distribution changes over time), multi-domain diversity\n(distinct patterns across financial domains such as stocks, commodities, and\nfutures), and varying temporal resolutions (patterns differing across\nper-second, hourly, daily, or weekly indicators). While recent deep learning\nmethods attempt to address these complexities, they frequently suffer from\noverfitting and typically require extensive domain-specific fine-tuning. To\novercome these limitations, we introduce FinCast, the first foundation model\nspecifically designed for financial time-series forecasting, trained on\nlarge-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot\nperformance, effectively capturing diverse patterns without domain-specific\nfine-tuning. Comprehensive empirical and qualitative evaluations demonstrate\nthat FinCast surpasses existing state-of-the-art methods, highlighting its\nstrong generalization capabilities.", "AI": {"tldr": "FinCast\u662f\u9996\u4e2a\u4e13\u95e8\u4e3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8bbe\u8ba1\u7684\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u91d1\u878d\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u5728\u96f6\u6837\u672c\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u5373\u53ef\u6355\u6349\u591a\u6837\u5316\u6a21\u5f0f\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5bf9\u7ecf\u6d4e\u7a33\u5b9a\u3001\u653f\u7b56\u5236\u5b9a\u548c\u53ef\u6301\u7eed\u6295\u8d44\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u65f6\u95f4\u975e\u5e73\u7a33\u6027\u3001\u591a\u9886\u57df\u591a\u6837\u6027\u548c\u4e0d\u540c\u65f6\u95f4\u5206\u8fa8\u7387\u7b49\u6a21\u5f0f\u53d8\u5316\u6311\u6218\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5bb9\u6613\u8fc7\u62df\u5408\u4e14\u9700\u8981\u5927\u91cf\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u3002", "method": "\u5f00\u53d1FinCast\u57fa\u7840\u6a21\u578b\uff0c\u5728\u5927\u89c4\u6a21\u91d1\u878d\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u4e13\u95e8\u9488\u5bf9\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8bbe\u8ba1\uff0c\u5177\u5907\u96f6\u6837\u672c\u9884\u6d4b\u80fd\u529b\u3002", "result": "FinCast\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u591a\u6837\u5316\u6a21\u5f0f\u800c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0c\u5728\u7efc\u5408\u5b9e\u8bc1\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FinCast\u4f5c\u4e3a\u9996\u4e2a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u7840\u6a21\u578b\uff0c\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u91d1\u878d\u9884\u6d4b\u4e2d\u7684\u6a21\u5f0f\u53d8\u5316\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19613", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19613", "abs": "https://arxiv.org/abs/2508.19613", "authors": ["Chenzhi Liu", "Mahsa Baktashmotlagh", "Yanran Tang", "Zi Huang", "Ruihong Qiu"], "title": "ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation", "comment": "Accepted to BMVC 2025, Oral", "summary": "Estimating model accuracy on unseen, unlabeled datasets is crucial for\nreal-world machine learning applications, especially under distribution shifts\nthat can degrade performance. Existing methods often rely on predicted class\nprobabilities (softmax scores) or data similarity metrics. While softmax-based\napproaches benefit from representing predictions on the standard simplex,\ncompressing logits into probabilities leads to information loss. Meanwhile,\nsimilarity-based methods can be computationally expensive and domain-specific,\nlimiting their broader applicability. In this paper, we introduce ALSA (Anchors\nin Logit Space for Accuracy estimation), a novel framework that preserves\nricher information by operating directly in the logit space. Building on\ntheoretical insights and empirical observations, we demonstrate that the\naggregation and distribution of logits exhibit a strong correlation with the\npredictive performance of the model. To exploit this property, ALSA employs an\nanchor-based modeling strategy: multiple learnable anchors are initialized in\nlogit space, each assigned an influence function that captures subtle\nvariations in the logits. This allows ALSA to provide robust and accurate\nperformance estimates across a wide range of distribution shifts. Extensive\nexperiments on vision, language, and graph benchmarks demonstrate ALSA's\nsuperiority over both softmax- and similarity-based baselines. Notably, ALSA's\nrobustness under significant distribution shifts highlights its potential as a\npractical tool for reliable model evaluation.", "AI": {"tldr": "ALSA\u662f\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u7cbe\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u76f4\u63a5\u5728logit\u7a7a\u95f4\u64cd\u4f5c\uff0c\u901a\u8fc7\u951a\u70b9\u5efa\u6a21\u7b56\u7565\u6765\u4f30\u8ba1\u6a21\u578b\u5728\u672a\u89c1\u672a\u6807\u8bb0\u6570\u636e\u96c6\u4e0a\u7684\u7cbe\u5ea6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5206\u5e03\u504f\u79fb\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56softmax\u6982\u7387\u6216\u6570\u636e\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u524d\u8005\u5b58\u5728\u4fe1\u606f\u635f\u5931\uff0c\u540e\u8005\u8ba1\u7b97\u6602\u8d35\u4e14\u9886\u57df\u7279\u5b9a\u3002\u9700\u8981\u4e00\u79cd\u80fd\u4fdd\u7559\u66f4\u4e30\u5bcc\u4fe1\u606f\u3001\u8ba1\u7b97\u9ad8\u6548\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u7cbe\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "ALSA\u5728logit\u7a7a\u95f4\u521d\u59cb\u5316\u591a\u4e2a\u53ef\u5b66\u4e60\u951a\u70b9\uff0c\u6bcf\u4e2a\u951a\u70b9\u5206\u914d\u5f71\u54cd\u51fd\u6570\u6765\u6355\u6349logits\u7684\u7ec6\u5fae\u53d8\u5316\u3002\u57fa\u4e8e\u7406\u8bba\u548c\u5b9e\u8bc1\u89c2\u5bdf\uff0c\u5229\u7528logits\u7684\u805a\u5408\u548c\u5206\u5e03\u4e0e\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u7684\u5f3a\u76f8\u5173\u6027\u3002", "result": "\u5728\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u56fe\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0cALSA\u4f18\u4e8e\u57fa\u4e8esoftmax\u548c\u76f8\u4f3c\u6027\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u663e\u8457\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "ALSA\u662f\u4e00\u4e2a\u5b9e\u7528\u7684\u53ef\u9760\u6a21\u578b\u8bc4\u4f30\u5de5\u5177\uff0c\u901a\u8fc7\u76f4\u63a5\u5728logit\u7a7a\u95f4\u64cd\u4f5c\u907f\u514d\u4e86\u4fe1\u606f\u635f\u5931\uff0c\u63d0\u4f9b\u4e86\u8de8\u591a\u79cd\u5206\u5e03\u504f\u79fb\u7684\u51c6\u786e\u6027\u80fd\u4f30\u8ba1\u3002"}}
{"id": "2508.19621", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19621", "abs": "https://arxiv.org/abs/2508.19621", "authors": ["Tiandi Ye", "Wenyan Liu", "Kai Yao", "Lichun Li", "Shangchao Su", "Cen Chen", "Xiang Li", "Shan Yin", "Ming Gao"], "title": "Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning", "comment": "Accepted by CIKM2025", "summary": "Federated learning (FL) is a privacy-preserving machine learning paradigm\nthat enables collaborative model training across multiple distributed clients\nwithout disclosing their raw data. Personalized federated learning (pFL) has\ngained increasing attention for its ability to address data heterogeneity.\nHowever, most existing pFL methods assume that each client's data follows a\nsingle distribution and learn one client-level personalized model for each\nclient. This assumption often fails in practice, where a single client may\npossess data from multiple sources or domains, resulting in significant\nintra-client heterogeneity and suboptimal performance. To tackle this\nchallenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework\nbased on visual prompt tuning. Specifically, we formulate instance-wise prompt\ngeneration from a Bayesian perspective and model the prompt posterior as an\nimplicit distribution to capture diverse visual semantics. We derive a\nvariational training objective under the semi-implicit variational inference\nframework. Extensive experiments on benchmark datasets demonstrate that\npFedBayesPT consistently outperforms existing pFL methods under both feature\nand label heterogeneity settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86pFedBayesPT\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u5185\u90e8\u6570\u636e\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u5b9e\u4f8b\u7ea7\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60", "motivation": "\u73b0\u6709\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5047\u8bbe\u6bcf\u4e2a\u5ba2\u6237\u7aef\u6570\u636e\u9075\u5faa\u5355\u4e00\u5206\u5e03\uff0c\u4f46\u5b9e\u9645\u4e2d\u5355\u4e2a\u5ba2\u6237\u7aef\u53ef\u80fd\u5305\u542b\u6765\u81ea\u591a\u4e2a\u6e90\u6216\u57df\u7684\u6570\u636e\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u5ba2\u6237\u7aef\u5185\u90e8\u5f02\u8d28\u6027\u548c\u6b21\u4f18\u6027\u80fd", "method": "\u57fa\u4e8e\u89c6\u89c9\u63d0\u793a\u8c03\u4f18\u7684\u7ec6\u7c92\u5ea6\u5b9e\u4f8b\u7ea7pFL\u6846\u67b6\uff0c\u4ece\u8d1d\u53f6\u65af\u89d2\u5ea6\u5236\u5b9a\u5b9e\u4f8b\u7ea7\u63d0\u793a\u751f\u6210\uff0c\u5c06\u63d0\u793a\u540e\u9a8c\u5efa\u6a21\u4e3a\u9690\u5f0f\u5206\u5e03\u4ee5\u6355\u6349\u591a\u6837\u5316\u89c6\u89c9\u8bed\u4e49\uff0c\u5728\u534a\u9690\u5f0f\u53d8\u5206\u63a8\u65ad\u6846\u67b6\u4e0b\u63a8\u5bfc\u53d8\u5206\u8bad\u7ec3\u76ee\u6807", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cpFedBayesPT\u5728\u7279\u5f81\u548c\u6807\u7b7e\u5f02\u8d28\u6027\u8bbe\u7f6e\u4e0b\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709pFL\u65b9\u6cd5", "conclusion": "pFedBayesPT\u6709\u6548\u89e3\u51b3\u4e86\u5ba2\u6237\u7aef\u5185\u90e8\u6570\u636e\u5f02\u8d28\u6027\u6311\u6218\uff0c\u4e3a\u4e2a\u6027\u5316\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u5b9e\u4f8b\u7ea7\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19659", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19659", "abs": "https://arxiv.org/abs/2508.19659", "authors": ["Ri Su", "Zhao Chen", "Caleb Chen Cao", "Nan Tang", "Lei Chen"], "title": "SCAR: A Characterization Scheme for Multi-Modal Dataset", "comment": "6 pages, 3 figures", "summary": "Foundation models exhibit remarkable generalization across diverse tasks,\nlargely driven by the characteristics of their training data. Recent\ndata-centric methods like pruning and compression aim to optimize training but\noffer limited theoretical insight into how data properties affect\ngeneralization, especially the data characteristics in sample scaling.\nTraditional perspectives further constrain progress by focusing predominantly\non data quantity and training efficiency, often overlooking structural aspects\nof data quality. In this study, we introduce SCAR, a principled scheme for\ncharacterizing the intrinsic structural properties of datasets across four key\nmeasures: Scale, Coverage, Authenticity, and Richness. Unlike prior\ndata-centric measures, SCAR captures stable characteristics that remain\ninvariant under dataset scaling, providing a robust and general foundation for\ndata understanding. Leveraging these structural properties, we introduce\nFoundation Data-a minimal subset that preserves the generalization behavior of\nthe full dataset without requiring model-specific retraining. We model\nsingle-modality tasks as step functions and estimate the distribution of the\nfoundation data size to capture step-wise generalization bias across modalities\nin the target multi-modal dataset. Finally, we develop a SCAR-guided data\ncompletion strategy based on this generalization bias, which enables efficient,\nmodality-aware expansion of modality-specific characteristics in multimodal\ndatasets. Experiments across diverse multi-modal datasets and model\narchitectures validate the effectiveness of SCAR in predicting data utility and\nguiding data acquisition. Code is available at https://github.com/McAloma/SCAR.", "AI": {"tldr": "SCAR\u662f\u4e00\u4e2a\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7Scale\u3001Coverage\u3001Authenticity\u3001Richness\u56db\u4e2a\u7ef4\u5ea6\u91cf\u5316\u6570\u636e\u96c6\u7684\u7ed3\u6784\u7279\u6027\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51faFoundation Data\u6982\u5ff5\u548c\u6a21\u6001\u611f\u77e5\u7684\u6570\u636e\u8865\u5168\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6570\u636e\u91cf\u548c\u8bad\u7ec3\u6548\u7387\uff0c\u7f3a\u4e4f\u5bf9\u6570\u636e\u8d28\u91cf\u7ed3\u6784\u7279\u6027\u7684\u7406\u8bba\u7406\u89e3\uff0c\u7279\u522b\u662f\u5728\u6837\u672c\u7f29\u653e\u65f6\u6570\u636e\u7279\u6027\u5982\u4f55\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSCAR\u6846\u67b6\u91cf\u5316\u6570\u636e\u96c6\u7684\u56db\u4e2a\u7ed3\u6784\u7279\u6027\uff0c\u5efa\u7acbFoundation Data\u6700\u5c0f\u5b50\u96c6\u4fdd\u6301\u6cdb\u5316\u884c\u4e3a\uff0c\u5efa\u6a21\u5355\u6a21\u6001\u4efb\u52a1\u4e3a\u9636\u8dc3\u51fd\u6570\uff0c\u5f00\u53d1\u57fa\u4e8e\u6cdb\u5316\u504f\u89c1\u7684SCAR\u5f15\u5bfc\u6570\u636e\u8865\u5168\u7b56\u7565\u3002", "result": "\u5728\u591a\u79cd\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SCAR\u5728\u9884\u6d4b\u6570\u636e\u6548\u7528\u548c\u6307\u5bfc\u6570\u636e\u91c7\u96c6\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "SCAR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u6570\u636e\u7406\u89e3\u57fa\u7840\uff0c\u80fd\u591f\u6355\u6349\u6570\u636e\u96c6\u7f29\u653e\u65f6\u4fdd\u6301\u7a33\u5b9a\u7684\u5185\u5728\u7ed3\u6784\u7279\u6027\uff0c\u4e3a\u6570\u636e\u4f18\u5316\u548c\u6269\u5c55\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2508.19661", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.19661", "abs": "https://arxiv.org/abs/2508.19661", "authors": ["Florentia Afentaki", "Sri Sai Rakesh Nakkilla", "Konstantinos Balaskas", "Paula Carolina Lozano Duarte", "Shiyi Jiang", "Georgios Zervakis", "Farshad Firouzi", "Krishnendu Chakrabarty", "Mehdi B. Tahoori"], "title": "Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables", "comment": "Accepted for publication at the IEEE/ACM International Symposium on\n  Low Power Electronics and Design} (ISLPED 2025)", "summary": "Conventional stress monitoring relies on episodic, symptom-focused\ninterventions, missing the need for continuous, accessible, and cost-efficient\nsolutions. State-of-the-art approaches use rigid, silicon-based wearables,\nwhich, though capable of multitasking, are not optimized for lightweight,\nflexible wear, limiting their practicality for continuous monitoring. In\ncontrast, flexible electronics (FE) offer flexibility and low manufacturing\ncosts, enabling real-time stress monitoring circuits. However, implementing\ncomplex circuits like machine learning (ML) classifiers in FE is challenging\ndue to integration and power constraints. Previous research has explored\nflexible biosensors and ADCs, but classifier design for stress detection\nremains underexplored. This work presents the first comprehensive design space\nexploration of low-power, flexible stress classifiers. We cover various ML\nclassifiers, feature selection, and neural simplification algorithms, with over\n1200 flexible classifiers. To optimize hardware efficiency, fully customized\ncircuits with low-precision arithmetic are designed in each case. Our\nexploration provides insights into designing real-time stress classifiers that\noffer higher accuracy than current methods, while being low-cost, conformable,\nand ensuring low power and compact size.", "AI": {"tldr": "\u9996\u6b21\u5168\u9762\u63a2\u7d22\u4f4e\u529f\u8017\u67d4\u6027\u538b\u529b\u5206\u7c7b\u5668\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u6db5\u76d6\u591a\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u3001\u7279\u5f81\u9009\u62e9\u548c\u795e\u7ecf\u7b80\u5316\u7b97\u6cd5\uff0c\u8bbe\u8ba1\u4e861200\u591a\u4e2a\u67d4\u6027\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u538b\u529b\u76d1\u6d4b", "motivation": "\u4f20\u7edf\u538b\u529b\u76d1\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u95f4\u6b47\u6027\u3001\u75c7\u72b6\u5bfc\u5411\u7684\u5e72\u9884\uff0c\u7f3a\u4e4f\u8fde\u7eed\u3001\u53ef\u53ca\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002\u73b0\u6709\u521a\u6027\u7845\u57fa\u53ef\u7a7f\u6234\u8bbe\u5907\u867d\u80fd\u591a\u4efb\u52a1\u5904\u7406\uff0c\u4f46\u4e0d\u591f\u8f7b\u4fbf\u7075\u6d3b\uff0c\u9650\u5236\u4e86\u8fde\u7eed\u76d1\u6d4b\u7684\u5b9e\u7528\u6027", "method": "\u91c7\u7528\u67d4\u6027\u7535\u5b50\u6280\u672f\uff0c\u8bbe\u8ba1\u4f4e\u529f\u8017\u67d4\u6027\u538b\u529b\u5206\u7c7b\u5668\u3002\u63a2\u7d22\u591a\u79cd\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u3001\u7279\u5f81\u9009\u62e9\u7b97\u6cd5\u548c\u795e\u7ecf\u7b80\u5316\u6280\u672f\uff0c\u8bbe\u8ba1\u5b8c\u5168\u5b9a\u5236\u5316\u7684\u4f4e\u7cbe\u5ea6\u7b97\u672f\u7535\u8def\uff0c\u4f18\u5316\u786c\u4ef6\u6548\u7387", "result": "\u5f00\u53d1\u4e86\u8d85\u8fc71200\u4e2a\u67d4\u6027\u5206\u7c7b\u5668\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u5177\u5907\u4f4e\u6210\u672c\u3001\u53ef\u8d34\u5408\u3001\u4f4e\u529f\u8017\u548c\u5c0f\u5c3a\u5bf8\u7684\u4f18\u52bf", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8bbe\u8ba1\u5b9e\u65f6\u538b\u529b\u5206\u7c7b\u5668\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u67d4\u6027\u7535\u5b50\u5728\u5b9e\u73b0\u9ad8\u6548\u3001\u5b9e\u7528\u538b\u529b\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u7684\u6f5c\u529b"}}
{"id": "2508.19672", "categories": ["cs.LG", "cs.IT", "cs.NA", "math.IT", "math.NA", "33F05, 41A20, 41A25, 26C15"], "pdf": "https://arxiv.org/pdf/2508.19672", "abs": "https://arxiv.org/abs/2508.19672", "authors": ["Erion Morina", "Martin Holler"], "title": "$\\mathcal{C}^1$-approximation with rational functions and rational neural networks", "comment": null, "summary": "We show that suitably regular functions can be approximated in the\n$\\mathcal{C}^1$-norm both with rational functions and rational neural networks,\nincluding approximation rates with respect to width and depth of the network,\nand degree of the rational functions. As consequence of our results, we further\nobtain $\\mathcal{C}^1$-approximation results for rational neural networks with\nthe $\\text{EQL}^\\div$ and ParFam architecture, both of which are important in\nparticular in the context of symbolic regression for physical law learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u9002\u5f53\u6b63\u5219\u51fd\u6570\u53ef\u4ee5\u901a\u8fc7\u6709\u7406\u51fd\u6570\u548c\u6709\u7406\u795e\u7ecf\u7f51\u7edc\u5728C\u00b9\u8303\u6570\u4e0b\u8fdb\u884c\u903c\u8fd1\uff0c\u5e76\u7ed9\u51fa\u4e86\u5173\u4e8e\u7f51\u7edc\u5bbd\u5ea6\u3001\u6df1\u5ea6\u4ee5\u53ca\u6709\u7406\u51fd\u6570\u6b21\u6570\u7684\u903c\u8fd1\u901f\u7387\u3002", "motivation": "\u7814\u7a76\u6709\u7406\u51fd\u6570\u548c\u6709\u7406\u795e\u7ecf\u7f51\u7edc\u5728C\u00b9\u8303\u6570\u4e0b\u7684\u903c\u8fd1\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u7b26\u53f7\u56de\u5f52\u548c\u7269\u7406\u5b9a\u5f8b\u5b66\u4e60\u7b49\u5e94\u7528\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f7f\u7528\u6709\u7406\u51fd\u6570\u548c\u6709\u7406\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u51fd\u6570\u903c\u8fd1\uff0c\u5206\u6790\u4e86EQL\u207a\u548cParFam\u67b6\u6784\u5728C\u00b9\u8303\u6570\u4e0b\u7684\u903c\u8fd1\u6027\u80fd\u3002", "result": "\u83b7\u5f97\u4e86\u5173\u4e8e\u7f51\u7edc\u5bbd\u5ea6\u3001\u6df1\u5ea6\u4ee5\u53ca\u6709\u7406\u51fd\u6570\u6b21\u6570\u7684\u5177\u4f53\u903c\u8fd1\u901f\u7387\uff0c\u8bc1\u660e\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u5728C\u00b9\u8303\u6570\u4e0b\u7684\u6709\u6548\u903c\u8fd1\u80fd\u529b\u3002", "conclusion": "\u6709\u7406\u51fd\u6570\u548c\u6709\u7406\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u6709\u6548\u903c\u8fd1\u9002\u5f53\u6b63\u5219\u51fd\u6570\uff0c\u4e3a\u7b26\u53f7\u56de\u5f52\u548c\u7269\u7406\u5b9a\u5f8b\u5b66\u4e60\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2508.19709", "categories": ["cs.LG", "math.FA", "26A16"], "pdf": "https://arxiv.org/pdf/2508.19709", "abs": "https://arxiv.org/abs/2508.19709", "authors": ["R. Arnau", "A. Gonz\u00e1lez Cort\u00e9s", "E. A. S\u00e1nchez P\u00e9rez", "S. Sanjuan"], "title": "Metric spaces of walks and Lipschitz duality on graphs", "comment": "31 pages, 3 figures", "summary": "We study the metric structure of walks on graphs, understood as Lipschitz\nsequences. To this end, a weighted metric is introduced to handle sequences,\nenabling the definition of distances between walks based on stepwise vertex\ndistances and weighted norms. We analyze the main properties of these metric\nspaces, which provides the foundation for the analysis of weaker forms of\ninstruments to measure relative distances between walks: proximities. We\nprovide some representation formulas for such proximities under different\nassumptions and provide explicit constructions for these cases. The resulting\nmetric framework allows the use of classical tools from metric modeling, such\nas the extension of Lipschitz functions from subspaces of walks, which permits\nextending proximity functions while preserving fundamental properties via the\nmentioned representations. Potential applications include the estimation of\nproximities and the development of reinforcement learning strategies based on\nexploratory walks, offering a robust approach to Lipschitz regression on\nnetwork structures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u56fe\u4e0a\u6e38\u8d70\u7684\u5ea6\u91cf\u7ed3\u6784\uff0c\u5f15\u5165\u52a0\u6743\u5ea6\u91cf\u5904\u7406\u5e8f\u5217\uff0c\u5b9a\u4e49\u57fa\u4e8e\u9010\u6b65\u9876\u70b9\u8ddd\u79bb\u548c\u52a0\u6743\u8303\u6570\u7684\u6e38\u8d70\u95f4\u8ddd\u79bb\uff0c\u5206\u6790\u5ea6\u91cf\u7a7a\u95f4\u6027\u8d28\uff0c\u63d0\u4f9b\u90bb\u8fd1\u5ea6\u7684\u8868\u793a\u516c\u5f0f\u548c\u663e\u5f0f\u6784\u9020\uff0c\u652f\u6301\u5ea6\u91cf\u5efa\u6a21\u7ecf\u5178\u5de5\u5177\u7684\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u56fe\u4e0a\u6e38\u8d70\u7684\u5ea6\u91cf\u7ed3\u6784\uff0c\u4e3a\u5206\u6790\u6d4b\u91cf\u6e38\u8d70\u95f4\u76f8\u5bf9\u8ddd\u79bb\u7684\u8f83\u5f31\u5f62\u5f0f\uff08\u90bb\u8fd1\u5ea6\uff09\u63d0\u4f9b\u57fa\u7840\uff0c\u652f\u6301\u5728\u7f51\u7edc\u4e0a\u8fdb\u884cLipschitz\u56de\u5f52\u7b49\u5e94\u7528\u3002", "method": "\u5f15\u5165\u52a0\u6743\u5ea6\u91cf\u5904\u7406\u5e8f\u5217\uff0c\u5b9a\u4e49\u57fa\u4e8e\u9010\u6b65\u9876\u70b9\u8ddd\u79bb\u548c\u52a0\u6743\u8303\u6570\u7684\u6e38\u8d70\u95f4\u8ddd\u79bb\uff0c\u5206\u6790\u5ea6\u91cf\u7a7a\u95f4\u6027\u8d28\uff0c\u63d0\u4f9b\u90bb\u8fd1\u5ea6\u7684\u8868\u793a\u516c\u5f0f\u548c\u663e\u5f0f\u6784\u9020\u3002", "result": "\u5efa\u7acb\u4e86\u6e38\u8d70\u7684\u5ea6\u91cf\u6846\u67b6\uff0c\u652f\u6301\u7ecf\u5178\u5ea6\u91cf\u5efa\u6a21\u5de5\u5177\u7684\u4f7f\u7528\uff0c\u5982\u4ece\u6e38\u8d70\u5b50\u7a7a\u95f4\u6269\u5c55Lipschitz\u51fd\u6570\uff0c\u901a\u8fc7\u8868\u793a\u4fdd\u6301\u57fa\u672c\u6027\u8d28\u6269\u5c55\u90bb\u8fd1\u5ea6\u51fd\u6570\u3002", "conclusion": "\u63d0\u51fa\u7684\u5ea6\u91cf\u6846\u67b6\u4e3a\u4f30\u8ba1\u90bb\u8fd1\u5ea6\u548c\u57fa\u4e8e\u63a2\u7d22\u6027\u6e38\u8d70\u5f00\u53d1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u63d0\u4f9b\u4e86\u7a33\u5065\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7f51\u7edc\u7ed3\u6784\u4e0a\u7684Lipschitz\u56de\u5f52\u3002"}}
{"id": "2508.19733", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19733", "abs": "https://arxiv.org/abs/2508.19733", "authors": ["Theodoros Athanasiadis", "Steven Adriaensen", "Samuel M\u00fcller", "Frank Hutter"], "title": "Tune My Adam, Please!", "comment": "Accepted as a short paper at the non-archival content track of AutoML\n  2025", "summary": "The Adam optimizer remains one of the most widely used optimizers in deep\nlearning, and effectively tuning its hyperparameters is key to optimizing\nperformance. However, tuning can be tedious and costly. Freeze-thaw Bayesian\nOptimization (BO) is a recent promising approach for low-budget hyperparameter\ntuning, but is limited by generic surrogates without prior knowledge of how\nhyperparameters affect learning. We propose Adam-PFN, a new surrogate model for\nFreeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from\nTaskSet, together with a new learning curve augmentation method, CDF-augment,\nwhich artificially increases the number of available training examples. Our\napproach improves both learning curve extrapolation and accelerates\nhyperparameter optimization on TaskSet evaluation tasks, with strong\nperformance on out-of-distribution (OOD) tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86Adam-PFN\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u4ee3\u7406\u6a21\u578b\u548c\u65b0\u7684\u5b66\u4e60\u66f2\u7ebf\u589e\u5f3a\u6280\u672fCDF-augment\uff0c\u6539\u8fdbAdam\u4f18\u5316\u5668\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u6548\u7387", "motivation": "Adam\u4f18\u5316\u5668\u5e7f\u6cdb\u4f7f\u7528\u4f46\u8d85\u53c2\u6570\u8c03\u4f18\u8017\u65f6\uff0c\u73b0\u6709Freeze-thaw BO\u65b9\u6cd5\u53d7\u9650\u4e8e\u901a\u7528\u4ee3\u7406\u6a21\u578b\uff0c\u7f3a\u4e4f\u8d85\u53c2\u6570\u5bf9\u5b66\u4e60\u5f71\u54cd\u7684\u5148\u9a8c\u77e5\u8bc6", "method": "\u5f00\u53d1Adam-PFN\u4ee3\u7406\u6a21\u578b\uff0c\u5728TaskSet\u5b66\u4e60\u66f2\u7ebf\u4e0a\u9884\u8bad\u7ec3\uff0c\u5e76\u63d0\u51faCDF-augment\u5b66\u4e60\u66f2\u7ebf\u589e\u5f3a\u65b9\u6cd5\u589e\u52a0\u8bad\u7ec3\u6837\u672c", "result": "\u5728TaskSet\u8bc4\u4f30\u4efb\u52a1\u4e0a\u6539\u8fdb\u4e86\u5b66\u4e60\u66f2\u7ebf\u5916\u63a8\u80fd\u529b\uff0c\u52a0\u901f\u4e86\u8d85\u53c2\u6570\u4f18\u5316\uff0c\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02", "conclusion": "Adam-PFN\u7ed3\u5408\u9884\u8bad\u7ec3\u4ee3\u7406\u6a21\u578b\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u4e3aAdam\u8d85\u53c2\u6570\u8c03\u4f18\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19737", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.19737", "abs": "https://arxiv.org/abs/2508.19737", "authors": ["Meng Qin", "Weihua Li", "Jinqiang Cui", "Sen Pei"], "title": "InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections", "comment": null, "summary": "Graph partitioning (GP), a.k.a. community detection, is a classic problem\nthat divides nodes of a graph into densely-connected blocks. From a perspective\nof graph signal processing, we find that graph Laplacian with a negative\ncorrection can derive graph frequencies beyond the conventional range $[0, 2]$.\nTo explore whether the low-frequency information beyond this range can encode\nmore informative properties about community structures, we propose InfraredGP.\nIt (\\romannumeral1) adopts a spectral GNN as its backbone combined with\nlow-pass filters and a negative correction mechanism, (\\romannumeral2) only\nfeeds random inputs to this backbone, (\\romannumeral3) derives graph embeddings\nvia one feed-forward propagation (FFP) without any training, and\n(\\romannumeral4) obtains feasible GP results by feeding the derived embeddings\nto BIRCH. Surprisingly, our experiments demonstrate that based solely on the\nnegative correction mechanism that amplifies low-frequency information beyond\n$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard\nclustering modules (e.g., BIRCH) and obtain high-quality results for GP without\nany training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate\nInfraredGP for both static and streaming GP, where InfraredGP can achieve much\nbetter efficiency (e.g., 16x-23x faster) and competitive quality over various\nbaselines. We have made our code public at\nhttps://github.com/KuroginQin/InfraredGP", "AI": {"tldr": "InfraredGP\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u5206\u533a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d1f\u6821\u6b63\u673a\u5236\u6269\u5c55\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u7684\u9891\u7387\u8303\u56f4\uff0c\u4ec5\u4f7f\u7528\u968f\u673a\u8f93\u5165\u548c\u4e00\u6b21\u524d\u5411\u4f20\u64ad\u5373\u53ef\u751f\u6210\u53ef\u533a\u5206\u7684\u56fe\u5d4c\u5165\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u4f20\u7edf\u56fe\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u7684\u9891\u7387\u8303\u56f4\u9650\u5236\u5728[0,2]\u4e4b\u95f4\uff0c\u4f5c\u8005\u53d1\u73b0\u901a\u8fc7\u8d1f\u6821\u6b63\u53ef\u4ee5\u83b7\u5f97\u8d85\u51fa\u6b64\u8303\u56f4\u7684\u4f4e\u9891\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u53ef\u80fd\u5305\u542b\u66f4\u4e30\u5bcc\u7684\u793e\u533a\u7ed3\u6784\u7279\u5f81\u3002", "method": "\u91c7\u7528\u8c31GNN\u4f5c\u4e3a\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408\u4f4e\u901a\u6ee4\u6ce2\u5668\u548c\u8d1f\u6821\u6b63\u673a\u5236\uff1b\u4ec5\u8f93\u5165\u968f\u673a\u4fe1\u53f7\uff1b\u901a\u8fc7\u4e00\u6b21\u524d\u5411\u4f20\u64ad\u751f\u6210\u56fe\u5d4c\u5165\u800c\u4e0d\u9700\u8981\u8bad\u7ec3\uff1b\u4f7f\u7528BIRCH\u805a\u7c7b\u7b97\u6cd5\u83b7\u5f97\u6700\u7ec8\u5206\u533a\u7ed3\u679c\u3002", "result": "\u5728IEEE HPEC\u56fe\u6311\u6218\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cInfraredGP\u5728\u9759\u6001\u548c\u6d41\u5f0f\u56fe\u5206\u533a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e8616-23\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u5206\u533a\u8d28\u91cf\u3002", "conclusion": "\u8d1f\u6821\u6b63\u673a\u5236\u80fd\u591f\u6709\u6548\u63d0\u53d6\u8d85\u51fa\u4f20\u7edf\u9891\u7387\u8303\u56f4\u7684\u4f4e\u9891\u4fe1\u606f\uff0c\u4f7f\u5f97\u65e0\u9700\u8bad\u7ec3\u7684\u56fe\u5206\u533a\u65b9\u6cd5\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u90fd\u8fbe\u5230\u4e86\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2508.19752", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19752", "abs": "https://arxiv.org/abs/2508.19752", "authors": ["Muhammad Moeeze Hassan", "R\u00e9gis Cottereau", "Filippo Gatti", "Patryk Dec"], "title": "Fast 3D Diffusion for Scalable Granular Media Synthesis", "comment": null, "summary": "Simulating granular media, using Discrete Element Method is a computationally\nintensive task. This is especially true during initialization phase, which\ndominates total simulation time because of large displacements involved and\nassociated kinetic energy. We overcome this bottleneck with a novel generative\npipeline based on 3D diffusion models that directly synthesizes arbitrarily\nlarge granular assemblies in their final and physically realistic\nconfigurations. The approach frames the problem as a 3D generative modeling\ntask, consisting of a two-stage pipeline. First a diffusion model is trained to\ngenerate independent 3D voxel grids representing granular media. Second, a 3D\ninpainting model, adapted from 2D inpainting techniques using masked inputs,\nstitches these grids together seamlessly, enabling synthesis of large samples\nwith physically realistic structure. The inpainting model explores several\nmasking strategies for the inputs to the underlying UNets by training the\nnetwork to infer missing portions of voxel grids from a concatenation of noised\ntensors, masks, and masked tensors as input channels. The model also adapts a\n2D repainting technique of re-injecting noise scheduler output with ground\ntruth to provide a strong guidance to the 3D model. This along with weighted\nlosses ensures long-term coherence over generation of masked regions. Both\nmodels are trained on the same binarized 3D occupancy grids extracted from\nsmall-scale DEM simulations, achieving linear scaling of computational time\nwith respect to sample size. Quantitatively, a 1.2 m long ballasted rail track\nsynthesis equivalent to a 3-hour DEM simulation, was completed under 20\nseconds. The generated voxel grids can also be post-processed to extract grain\ngeometries for DEM-compatibility as well, enabling physically coherent,\nreal-time, scalable granular media synthesis for industrial applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e3D\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u7ba1\u9053\uff0c\u76f4\u63a5\u5408\u6210\u4efb\u610f\u5927\u5c0f\u7684\u7269\u7406\u771f\u5b9e\u9897\u7c92\u4ecb\u8d28\uff0c\u89e3\u51b3\u79bb\u6563\u5143\u6cd5\u521d\u59cb\u5316\u9636\u6bb5\u8ba1\u7b97\u74f6\u9888\u95ee\u9898", "motivation": "\u79bb\u6563\u5143\u6cd5\u6a21\u62df\u9897\u7c92\u4ecb\u8d28\u65f6\uff0c\u521d\u59cb\u5316\u9636\u6bb5\u56e0\u5927\u4f4d\u79fb\u548c\u52a8\u80fd\u5bfc\u81f4\u8ba1\u7b97\u65f6\u95f4\u8fc7\u957f\uff0c\u6210\u4e3a\u4e3b\u8981\u74f6\u9888", "method": "\u4e24\u9636\u6bb5\u7ba1\u9053\uff1a\u9996\u5148\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u751f\u6210\u72ec\u7acb3D\u4f53\u7d20\u7f51\u683c\uff0c\u7136\u540e\u4f7f\u7528\u57fa\u4e8e\u63a9\u7801\u8f93\u5165\u76843D\u4fee\u590d\u6a21\u578b\u65e0\u7f1d\u62fc\u63a5\u7f51\u683c\uff0c\u91c7\u7528\u566a\u58f0\u8c03\u5ea6\u5668\u8f93\u51fa\u4e0e\u771f\u5b9e\u6570\u636e\u91cd\u65b0\u6ce8\u5165\u76842D\u91cd\u7ed8\u6280\u672f", "result": "\u751f\u6210\u957f1.2\u7c73\u7684\u9053\u78b4\u8f68\u9053\u7b49\u6548\u4e8e3\u5c0f\u65f6DEM\u6a21\u62df\uff0c\u572820\u79d2\u5185\u5b8c\u6210\uff0c\u8ba1\u7b97\u65f6\u95f4\u4e0e\u6837\u672c\u5927\u5c0f\u5448\u7ebf\u6027\u5173\u7cfb", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u7269\u7406\u4e00\u81f4\u3001\u5b9e\u65f6\u3001\u53ef\u6269\u5c55\u7684\u9897\u7c92\u4ecb\u8d28\u5408\u6210\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u5e94\u7528"}}
{"id": "2508.19839", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.19839", "abs": "https://arxiv.org/abs/2508.19839", "authors": ["Kehao Zhang", "Shaolei Zhang", "Yang Feng"], "title": "PSO-Merging: Merging Models Based on Particle Swarm Optimization", "comment": null, "summary": "Model merging has emerged as an efficient strategy for constructing multitask\nmodels by integrating the strengths of multiple available expert models,\nthereby reducing the need to fine-tune a pre-trained model for all the tasks\nfrom scratch. Existing data-independent methods struggle with performance\nlimitations due to the lack of data-driven guidance. Data-driven approaches\nalso face key challenges: gradient-based methods are computationally expensive,\nlimiting their practicality for merging large expert models, whereas existing\ngradient-free methods often fail to achieve satisfactory results within a\nlimited number of optimization steps. To address these limitations, this paper\nintroduces PSO-Merging, a novel data-driven merging method based on the\nParticle Swarm Optimization (PSO). In this approach, we initialize the particle\nswarm with a pre-trained model, expert models, and sparsified expert models. We\nthen perform multiple iterations, with the final global best particle serving\nas the merged model. Experimental results on different language models show\nthat PSO-Merging generally outperforms baseline merging methods, offering a\nmore efficient and scalable solution for model merging.", "AI": {"tldr": "PSO-Merging\u662f\u4e00\u79cd\u57fa\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316\u7684\u6570\u636e\u9a71\u52a8\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u521d\u59cb\u5316\u7c92\u5b50\u7fa4\u5e76\u8fdb\u884c\u591a\u8f6e\u8fed\u4ee3\uff0c\u5728\u8bed\u8a00\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u6570\u636e\u65e0\u5173\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u6570\u636e\u6307\u5bfc\u800c\u6027\u80fd\u53d7\u9650\uff0c\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4e2d\u68af\u5ea6\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\uff0c\u65e0\u68af\u5ea6\u65b9\u6cd5\u5728\u6709\u9650\u4f18\u5316\u6b65\u6570\u5185\u6548\u679c\u4e0d\u4f73", "method": "\u57fa\u4e8e\u7c92\u5b50\u7fa4\u4f18\u5316(PSO)\uff0c\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u3001\u4e13\u5bb6\u6a21\u578b\u548c\u7a00\u758f\u5316\u4e13\u5bb6\u6a21\u578b\u521d\u59cb\u5316\u7c92\u5b50\u7fa4\uff0c\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u83b7\u5f97\u6700\u7ec8\u878d\u5408\u6a21\u578b", "result": "\u5728\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPSO-Merging\u901a\u5e38\u4f18\u4e8e\u57fa\u7ebf\u878d\u5408\u65b9\u6cd5", "conclusion": "PSO-Merging\u4e3a\u6a21\u578b\u878d\u5408\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19842", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19842", "abs": "https://arxiv.org/abs/2508.19842", "authors": ["S\u00fcleyman Y\u0131ld\u0131z", "Konrad Janik", "Peter Benner"], "title": "Symplectic convolutional neural networks", "comment": null, "summary": "We propose a new symplectic convolutional neural network (CNN) architecture\nby leveraging symplectic neural networks, proper symplectic decomposition, and\ntensor techniques. Specifically, we first introduce a mathematically equivalent\nform of the convolution layer and then, using symplectic neural networks, we\ndemonstrate a way to parameterize the layers of the CNN to ensure that the\nconvolution layer remains symplectic. To construct a complete autoencoder, we\nintroduce a symplectic pooling layer. We demonstrate the performance of the\nproposed neural network on three examples: the wave equation, the nonlinear\nSchr\\\"odinger (NLS) equation, and the sine-Gordon equation. The numerical\nresults indicate that the symplectic CNN outperforms the linear symplectic\nautoencoder obtained via proper symplectic decomposition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f9b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u8f9b\u795e\u7ecf\u7f51\u7edc\u3001\u9002\u5f53\u8f9b\u5206\u89e3\u548c\u5f20\u91cf\u6280\u672f\u6765\u6784\u5efa\u4fdd\u6301\u8f9b\u7ed3\u6784\u7684\u5377\u79ef\u5c42\u548c\u6c60\u5316\u5c42", "motivation": "\u4f20\u7edf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5728\u7269\u7406\u7cfb\u7edf\u5efa\u6a21\u4e2d\u65e0\u6cd5\u4fdd\u6301\u8f9b\u7ed3\u6784\uff0c\u800c\u8f9b\u7ed3\u6784\u5bf9\u4e8e\u54c8\u5bc6\u987f\u7cfb\u7edf\u7684\u65f6\u95f4\u6f14\u5316\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4fdd\u6301\u8fd9\u79cd\u51e0\u4f55\u7ed3\u6784\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784", "method": "\u9996\u5148\u5f15\u5165\u5377\u79ef\u5c42\u7684\u6570\u5b66\u7b49\u4ef7\u5f62\u5f0f\uff0c\u7136\u540e\u4f7f\u7528\u8f9b\u795e\u7ecf\u7f51\u7edc\u53c2\u6570\u5316CNN\u5c42\u4ee5\u786e\u4fdd\u5377\u79ef\u5c42\u4fdd\u6301\u8f9b\u6027\uff0c\u5e76\u5f15\u5165\u8f9b\u6c60\u5316\u5c42\u6765\u6784\u5efa\u5b8c\u6574\u7684\u81ea\u7f16\u7801\u5668", "result": "\u5728\u6ce2\u52a8\u65b9\u7a0b\u3001\u975e\u7ebf\u6027\u859b\u5b9a\u8c14\u65b9\u7a0b\u548c\u6b63\u5f26-\u6208\u767b\u65b9\u7a0b\u4e09\u4e2a\u793a\u4f8b\u4e0a\u6d4b\u8bd5\uff0c\u6570\u503c\u7ed3\u679c\u8868\u660e\u8f9bCNN\u4f18\u4e8e\u901a\u8fc7\u9002\u5f53\u8f9b\u5206\u89e3\u83b7\u5f97\u7684\u7ebf\u6027\u8f9b\u81ea\u7f16\u7801\u5668", "conclusion": "\u63d0\u51fa\u7684\u8f9b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u80fd\u591f\u6709\u6548\u4fdd\u6301\u7269\u7406\u7cfb\u7edf\u7684\u8f9b\u7ed3\u6784\uff0c\u5728\u54c8\u5bc6\u987f\u7cfb\u7edf\u5efa\u6a21\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u7269\u7406\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.19847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19847", "abs": "https://arxiv.org/abs/2508.19847", "authors": ["Erdi Kara", "Panos Stinis"], "title": "Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources", "comment": null, "summary": "We present a hybrid framework that couples finite element methods (FEM) with\nphysics-informed DeepONet to model fluid transport in porous media from sharp,\nlocalized Gaussian sources. The governing system consists of a steady-state\nDarcy flow equation and a time-dependent convection-diffusion equation. Our\napproach solves the Darcy system using FEM and transfers the resulting velocity\nfield to a physics-informed DeepONet, which learns the mapping from source\nfunctions to solute concentration profiles. This modular strategy preserves\nFEM-level accuracy in the flow field while enabling fast inference for\ntransport dynamics. To handle steep gradients induced by sharp sources, we\nintroduce an adaptive sampling strategy for trunk collocation points. Numerical\nexperiments demonstrate that our method is in good agreement with the reference\nsolutions while offering orders of magnitude speedups over traditional solvers,\nmaking it suitable for practical applications in relevant scenarios.\nImplementation of our proposed method is available at\nhttps://github.com/erkara/fem-pi-deeponet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u5c06\u6709\u9650\u5143\u65b9\u6cd5\u4e0e\u7269\u7406\u4fe1\u606fDeepONet\u7ed3\u5408\uff0c\u7528\u4e8e\u6a21\u62df\u591a\u5b54\u4ecb\u8d28\u4e2d\u6765\u81ea\u5c16\u9510\u9ad8\u65af\u6e90\u7684\u6d41\u4f53\u8f93\u8fd0\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u5feb\u901f\u63a8\u65ad\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u591a\u5b54\u4ecb\u8d28\u4e2d\u5c16\u9510\u6e90\u5f15\u8d77\u7684\u9661\u5ced\u68af\u5ea6\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u7cbe\u5ea6\u53c8\u80fd\u5927\u5e45\u52a0\u901f\u8ba1\u7b97\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528FEM\u6c42\u89e3\u8fbe\u897f\u6d41\u52a8\u65b9\u7a0b\u83b7\u5f97\u901f\u5ea6\u573a\uff0c\u7136\u540e\u5c06\u901f\u5ea6\u573a\u8f93\u5165\u7269\u7406\u4fe1\u606fDeepONet\u6765\u5b66\u4e60\u4ece\u6e90\u51fd\u6570\u5230\u6eb6\u8d28\u6d53\u5ea6\u5206\u5e03\u7684\u6620\u5c04\uff0c\u5e76\u91c7\u7528\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u5904\u7406\u9661\u5ced\u68af\u5ea6\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4e0e\u53c2\u8003\u89e3\u543b\u5408\u826f\u597d\uff0c\u76f8\u6bd4\u4f20\u7edf\u6c42\u89e3\u5668\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u52a0\u901f\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86FEM\u7684\u7cbe\u5ea6\u548cDeepONet\u7684\u5feb\u901f\u63a8\u65ad\u4f18\u52bf\uff0c\u4e3a\u591a\u5b54\u4ecb\u8d28\u6d41\u4f53\u8f93\u8fd0\u6a21\u62df\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19857", "categories": ["cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.19857", "abs": "https://arxiv.org/abs/2508.19857", "authors": ["Omar Bacarreza", "Thorin Farnsworth", "Alexander Makarovskiy", "Hugo Wallner", "Tessa Hicks", "Santiago Sempere-Llagostera", "John Price", "Robert J. A. Francis-Jones", "William R. Clements"], "title": "Quantum latent distributions in deep generative models", "comment": null, "summary": "Many successful families of generative models leverage a low-dimensional\nlatent distribution that is mapped to a data distribution. Though simple latent\ndistributions are commonly used, it has been shown that more sophisticated\ndistributions can improve performance. For instance, recent work has explored\nusing the distributions produced by quantum processors and found empirical\nimprovements. However, when latent space distributions produced by quantum\nprocessors can be expected to improve performance, and whether these\nimprovements are reproducible, are open questions that we investigate in this\nwork. We prove that, under certain conditions, these \"quantum latent\ndistributions\" enable generative models to produce data distributions that\nclassical latent distributions cannot efficiently produce. We also provide\nactionable intuitions to identify when such quantum advantages may arise in\nreal-world settings. We perform benchmarking experiments on both a synthetic\nquantum dataset and the QM9 molecular dataset, using both simulated and real\nphotonic quantum processors. Our results demonstrate that quantum latent\ndistributions can lead to improved generative performance in GANs compared to a\nrange of classical baselines. We also explore diffusion and flow matching\nmodels, identifying architectures compatible with quantum latent distributions.\nThis work confirms that near-term quantum processors can expand the\ncapabilities of deep generative models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u5904\u7406\u5668\u4ea7\u751f\u7684\u6f5c\u5728\u5206\u5e03\u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u4f18\u52bf\uff0c\u8bc1\u660e\u4e86\u5728\u67d0\u4e9b\u6761\u4ef6\u4e0b\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u80fd\u591f\u4ea7\u751f\u7ecf\u5178\u5206\u5e03\u65e0\u6cd5\u9ad8\u6548\u751f\u6210\u7684\u6570\u636e\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u5728GAN\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\u6a21\u578b\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u867d\u7136\u7b80\u5355\u6f5c\u5728\u5206\u5e03\u5728\u751f\u6210\u6a21\u578b\u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u66f4\u590d\u6742\u7684\u5206\u5e03\u53ef\u4ee5\u63d0\u5347\u6027\u80fd\u3002\u91cf\u5b50\u5904\u7406\u5668\u4ea7\u751f\u7684\u5206\u5e03\u5df2\u88ab\u8bc1\u660e\u80fd\u5e26\u6765\u7ecf\u9a8c\u6027\u6539\u8fdb\uff0c\u4f46\u91cf\u5b50\u4f18\u52bf\u4f55\u65f6\u51fa\u73b0\u4ee5\u53ca\u8fd9\u4e9b\u6539\u8fdb\u662f\u5426\u53ef\u91cd\u73b0\u4ecd\u662f\u5f00\u653e\u95ee\u9898\u3002", "method": "\u7406\u8bba\u8bc1\u660e\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u76f4\u89c9\u5224\u65ad\u6807\u51c6\uff0c\u5728\u5408\u6210\u91cf\u5b50\u6570\u636e\u96c6\u548cQM9\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u6a21\u62df\u548c\u771f\u5b9e\u7684\u5149\u5b50\u91cf\u5b50\u5904\u7406\u5668\uff0c\u63a2\u7d22GAN\u3001\u6269\u6563\u6a21\u578b\u548c\u6d41\u5339\u914d\u6a21\u578b\u7b49\u67b6\u6784\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4e00\u7cfb\u5217\u7ecf\u5178\u57fa\u7ebf\u76f8\u6bd4\uff0c\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u80fd\u591f\u63d0\u5347GAN\u7684\u751f\u6210\u6027\u80fd\uff0c\u5e76\u786e\u5b9a\u4e86\u4e0e\u91cf\u5b50\u6f5c\u5728\u5206\u5e03\u517c\u5bb9\u7684\u6269\u6563\u548c\u6d41\u5339\u914d\u6a21\u578b\u67b6\u6784\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u5b9e\u4e86\u8fd1\u671f\u7684\u91cf\u5b50\u5904\u7406\u5668\u53ef\u4ee5\u6269\u5c55\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7684\u80fd\u529b\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u5728\u751f\u6210\u5efa\u6a21\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u9a8c\u652f\u6301\u3002"}}
{"id": "2508.19884", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19884", "abs": "https://arxiv.org/abs/2508.19884", "authors": ["Mingyue Kong", "Yinglong Zhang", "Chengda Xu", "Xuewen Xia", "Xing Xu"], "title": "Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks", "comment": "50 pages, 6 figures", "summary": "Graph Neural Networks (GNNs) have shown remarkable performance in structured\ndata modeling tasks such as node classification. However, mainstream approaches\ngenerally rely on a large number of trainable parameters and fixed aggregation\nrules, making it difficult to adapt to graph data with strong structural\nheterogeneity and complex feature distributions. This often leads to\nover-smoothing of node representations and semantic degradation. To address\nthese issues, this paper proposes a parameter-free graph neural network\nframework based on structural diversity, namely SDGNN (Structural-Diversity\nGraph Neural Network). The framework is inspired by structural diversity theory\nand designs a unified structural-diversity message passing mechanism that\nsimultaneously captures the heterogeneity of neighborhood structures and the\nstability of feature semantics, without introducing additional trainable\nparameters. Unlike traditional parameterized methods, SDGNN does not rely on\ncomplex model training, but instead leverages complementary modeling from both\nstructure-driven and feature-driven perspectives, thereby effectively improving\nadaptability across datasets and scenarios. Experimental results show that on\neight public benchmark datasets and an interdisciplinary PubMed citation\nnetwork, SDGNN consistently outperforms mainstream GNNs under challenging\nconditions such as low supervision, class imbalance, and cross-domain transfer.\nThis work provides a new theoretical perspective and general approach for the\ndesign of parameter-free graph neural networks, and further validates the\nimportance of structural diversity as a core signal in graph representation\nlearning. To facilitate reproducibility and further research, the full\nimplementation of SDGNN has been released at:\nhttps://github.com/mingyue15694/SGDNN/tree/main", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u7ed3\u6784\u591a\u6837\u6027\u7684\u65e0\u53c2\u6570\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6SDGNN\uff0c\u901a\u8fc7\u7ed3\u6784\u591a\u6837\u6027\u6d88\u606f\u4f20\u9012\u673a\u5236\u540c\u65f6\u6355\u83b7\u90bb\u57df\u7ed3\u6784\u5f02\u8d28\u6027\u548c\u7279\u5f81\u8bed\u4e49\u7a33\u5b9a\u6027\uff0c\u65e0\u9700\u8bad\u7ec3\u53c2\u6570\uff0c\u5728\u591a\u4e2a\u6311\u6218\u6027\u573a\u666f\u4e0b\u4f18\u4e8e\u4e3b\u6d41GNN\u65b9\u6cd5", "motivation": "\u4f20\u7edfGNN\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u548c\u56fa\u5b9a\u805a\u5408\u89c4\u5219\uff0c\u96be\u4ee5\u9002\u5e94\u7ed3\u6784\u5f02\u8d28\u6027\u5f3a\u7684\u56fe\u6570\u636e\uff0c\u5bb9\u6613\u5bfc\u81f4\u8282\u70b9\u8868\u793a\u8fc7\u5e73\u6ed1\u548c\u8bed\u4e49\u9000\u5316\u95ee\u9898", "method": "\u57fa\u4e8e\u7ed3\u6784\u591a\u6837\u6027\u7406\u8bba\u8bbe\u8ba1\u7edf\u4e00\u7684\u7ed3\u6784\u591a\u6837\u6027\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u4ece\u7ed3\u6784\u9a71\u52a8\u548c\u7279\u5f81\u9a71\u52a8\u4e24\u4e2a\u89d2\u5ea6\u8fdb\u884c\u4e92\u8865\u5efa\u6a21\uff0c\u4e0d\u5f15\u5165\u989d\u5916\u53ef\u8bad\u7ec3\u53c2\u6570", "result": "\u57288\u4e2a\u516c\u5171\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8de8\u5b66\u79d1PubMed\u5f15\u6587\u7f51\u7edc\u4e0a\uff0cSDGNN\u5728\u4f4e\u76d1\u7763\u3001\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8de8\u57df\u8fc1\u79fb\u7b49\u6311\u6218\u6761\u4ef6\u4e0b consistently\u4f18\u4e8e\u4e3b\u6d41GNN", "conclusion": "\u4e3a\u65e0\u53c2\u6570\u56fe\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u548c\u901a\u7528\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u7ed3\u6784\u591a\u6837\u6027\u4f5c\u4e3a\u56fe\u8868\u793a\u5b66\u4e60\u6838\u5fc3\u4fe1\u53f7\u7684\u91cd\u8981\u6027"}}
{"id": "2508.19896", "categories": ["cs.LG", "cs.CV", "I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.19896", "abs": "https://arxiv.org/abs/2508.19896", "authors": ["Davorin Mili\u010devi\u0107", "Ratko Grbi\u0107"], "title": "NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs", "comment": "13 pages, 4 figures. Submitted to Elsevier Neurocomputing, under\n  review", "summary": "Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often\nrely on purely global, gradient-based optimisation, which can lead to\noverfitting, redundant filters, and reduced interpretability. To address these\nlimitations, we propose NM-Hebb, a two-phase training framework that integrates\nneuro-inspired local plasticity with distance-aware supervision. Phase 1\nextends standard supervised training by jointly optimising a cross-entropy\nobjective with two biologically inspired mechanisms: (i) a Hebbian regulariser\nthat aligns the spatial mean of activations with the mean of the corresponding\nconvolutional filter weights, encouraging structured, reusable primitives; and\n(ii) a learnable neuromodulator that gates an elastic-weight-style\nconsolidation loss, preserving beneficial parameters without freezing the\nnetwork. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,\nexplicitly compressing intra-class distances and enlarging inter-class margins\nin the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet\nacross five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,\nDenseNet-121), NM-Hebb achieves consistent gains over baseline and other\nmethods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp\n(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual\nInformation (NMI) increased by up to +0.15. Qualitative visualisations and\nfilter-level analyses further confirm that NM-Hebb produces more structured and\nselective features, yielding tighter and more interpretable class clusters.\nOverall, coupling local Hebbian plasticity with metric-based fine-tuning yields\nCNNs that are not only more accurate but also more interpretable, offering\npractical benefits for resource-constrained and safety-critical AI deployments.", "AI": {"tldr": "NM-Hebb\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u542f\u53d1\u7684\u5c40\u90e8\u53ef\u5851\u6027\u548c\u8ddd\u79bb\u611f\u77e5\u76d1\u7763\uff0c\u901a\u8fc7Hebbian\u6b63\u5219\u5316\u548c\u53ef\u5b66\u4e60\u795e\u7ecf\u8c03\u8282\u5668\u63d0\u5347CNN\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u9aa8\u5e72\u7f51\u7edc\u4e0a\u5b9e\u73b0\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u548c\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfCNN\u57fa\u4e8e\u5168\u5c40\u68af\u5ea6\u4f18\u5316\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u3001\u5197\u4f59\u8fc7\u6ee4\u5668\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u7269\u542f\u53d1\u673a\u5236\u63d0\u5347\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u7ed3\u5408\u4ea4\u53c9\u71b5\u635f\u5931\u3001Hebbian\u6b63\u5219\u5316\uff08\u6fc0\u6d3b\u7a7a\u95f4\u5747\u503c\u4e0e\u6ee4\u6ce2\u5668\u6743\u91cd\u5747\u503c\u5bf9\u9f50\uff09\u548c\u53ef\u5b66\u4e60\u795e\u7ecf\u8c03\u8282\u5668\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6210\u5bf9\u5ea6\u91cf\u5b66\u4e60\u635f\u5931\u8fdb\u884c\u5fae\u8c03\uff0c\u538b\u7f29\u7c7b\u5185\u8ddd\u79bb\u5e76\u6269\u5927\u7c7b\u95f4\u95f4\u9694\u3002", "result": "\u5728CIFAR-10\u3001CIFAR-100\u548cTinyImageNet\u4e0a\uff0cTop-1\u51c6\u786e\u7387\u63d0\u53472.0-10.0\u4e2a\u767e\u5206\u70b9\uff0cNMI\u63d0\u5347\u6700\u9ad80.15\uff0c\u4ea7\u751f\u66f4\u7ed3\u6784\u5316\u3001\u9009\u62e9\u6027\u7684\u7279\u5f81\u548c\u66f4\u7d27\u5bc6\u7684\u7c7b\u7c07\u3002", "conclusion": "\u7ed3\u5408\u5c40\u90e8Hebbian\u53ef\u5851\u6027\u548c\u57fa\u4e8e\u5ea6\u91cf\u7684\u5fae\u8c03\uff0c\u4f7fCNN\u4e0d\u4ec5\u66f4\u51c6\u786e\u800c\u4e14\u66f4\u53ef\u89e3\u91ca\uff0c\u5bf9\u8d44\u6e90\u53d7\u9650\u548c\u5b89\u5168\u5173\u952e\u7684AI\u90e8\u7f72\u5177\u6709\u5b9e\u9645\u76ca\u5904\u3002"}}
{"id": "2508.19900", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19900", "abs": "https://arxiv.org/abs/2508.19900", "authors": ["Tan Jing", "Xiaorui Li", "Chao Yao", "Xiaojuan Ban", "Yuetong Fang", "Renjing Xu", "Zhaolin Yuan"], "title": "Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning (RL) enables learning effective policies from\nfixed datasets without any environment interaction. Existing methods typically\nemploy policy constraints to mitigate the distribution shift encountered during\noffline RL training. However, because the scale of the constraints varies\nacross tasks and datasets of differing quality, existing methods must\nmeticulously tune hyperparameters to match each dataset, which is\ntime-consuming and often impractical. We propose Adaptive Scaling of Policy\nConstraints (ASPC), a second-order differentiable framework that dynamically\nbalances RL and behavior cloning (BC) during training. We theoretically analyze\nits performance improvement guarantee. In experiments on 39 datasets across\nfour D4RL domains, ASPC using a single hyperparameter configuration outperforms\nother adaptive constraint methods and state-of-the-art offline RL algorithms\nthat require per-dataset tuning while incurring only minimal computational\noverhead. The code will be released at https://github.com/Colin-Jing/ASPC.", "AI": {"tldr": "ASPC\u662f\u4e00\u4e2a\u4e8c\u9636\u53ef\u5fae\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861\u5f3a\u5316\u5b66\u4e60\u548c\u884c\u4e3a\u514b\u9686\u6765\u89e3\u51b3\u79bb\u7ebfRL\u4e2d\u7b56\u7565\u7ea6\u675f\u7684\u7f29\u653e\u95ee\u9898\uff0c\u65e0\u9700\u9488\u5bf9\u4e0d\u540c\u6570\u636e\u96c6\u8fdb\u884c\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebfRL\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u4efb\u52a1\u548c\u6570\u636e\u96c6\u8d28\u91cf\u7cbe\u5fc3\u8c03\u6574\u7b56\u7565\u7ea6\u675f\u7684\u8d85\u53c2\u6570\uff0c\u8fd9\u65e2\u8017\u65f6\u53c8\u4e0d\u5b9e\u7528\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u7b56\u7565\u7ea6\u675f\u7f29\u653e(ASPC)\u6846\u67b6\uff0c\u4f7f\u7528\u4e8c\u9636\u53ef\u5fae\u65b9\u6cd5\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5e73\u8861RL\u76ee\u6807\u548c\u884c\u4e3a\u514b\u9686\u76ee\u6807\u3002", "result": "\u57284\u4e2aD4RL\u9886\u57df\u768439\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cASPC\u4f7f\u7528\u5355\u4e00\u8d85\u53c2\u6570\u914d\u7f6e\u5c31\u4f18\u4e8e\u5176\u4ed6\u81ea\u9002\u5e94\u7ea6\u675f\u65b9\u6cd5\u548c\u9700\u8981\u9010\u6570\u636e\u96c6\u8c03\u4f18\u7684\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "ASPC\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684\u79bb\u7ebfRL\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u81ea\u52a8\u9002\u5e94\u4e0d\u540c\u6570\u636e\u96c6\uff0c\u907f\u514d\u4e86\u7e41\u7410\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u8fc7\u7a0b\u3002"}}
{"id": "2508.19907", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.19907", "abs": "https://arxiv.org/abs/2508.19907", "authors": ["Hewen Wang", "Renchi Yang", "Xiaokui Xiao"], "title": "GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs", "comment": "11 pages. Paper accepted to CIKM 2025", "summary": "Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,\nthe goal of link sign prediction is to predict the signs of potential links\nconnecting U and V based on known positive and negative edges in G. The\nmajority of existing solutions towards link sign prediction mainly focus on\nunipartite signed graphs, which are sub-optimal due to the neglect of node\nheterogeneity and unique bipartite characteristics of SBGs. To this end, recent\nstudies adapt graph neural networks to SBGs by introducing message-passing\nschemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node\npairs. However, the fundamental spectral convolutional operators were\noriginally designed for positive links in unsigned graphs, and thus, are not\noptimal for inferring missing positive or negative links from known ones in\nSBGs.\n  Motivated by this, this paper proposes GegenNet, a novel and effective\nspectral convolutional neural network model for link sign prediction in SBGs.\nIn particular, GegenNet achieves enhanced model capacity and high predictive\naccuracy through three main technical contributions: (i) fast and theoretically\ngrounded spectral decomposition techniques for node feature initialization;\n(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and\n(iii) multi-layer sign-aware spectral convolutional networks alternating\nGegenbauer polynomial filters with positive and negative edges. Our extensive\nempirical studies reveal that GegenNet can achieve significantly superior\nperformance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign\nprediction compared to 11 strong competitors over 6 benchmark SBG datasets.", "AI": {"tldr": "GegenNet\u662f\u4e00\u4e2a\u7528\u4e8e\u7b26\u53f7\u4e8c\u5206\u56fe\u94fe\u63a5\u7b26\u53f7\u9884\u6d4b\u7684\u65b0\u578b\u8c31\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7Gegenbauer\u591a\u9879\u5f0f\u57fa\u6ee4\u6ce2\u5668\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u5206\u56fe\uff0c\u5ffd\u7565\u4e86\u4e8c\u5206\u56fe\u7684\u8282\u70b9\u5f02\u8d28\u6027\u548c\u72ec\u7279\u7279\u5f81\uff0c\u4e14\u4f20\u7edf\u8c31\u5377\u79ef\u7b97\u5b50\u4e0d\u9002\u5408\u4ece\u5df2\u77e5\u6b63\u8d1f\u94fe\u63a5\u63a8\u65ad\u7f3a\u5931\u94fe\u63a5", "method": "\u91c7\u7528\u5feb\u901f\u8c31\u5206\u89e3\u6280\u672f\u521d\u59cb\u5316\u8282\u70b9\u7279\u5f81\uff0c\u57fa\u4e8eGegenbauer\u591a\u9879\u5f0f\u57fa\u7684\u65b0\u8c31\u56fe\u6ee4\u6ce2\u5668\uff0c\u4ee5\u53ca\u591a\u5c42\u7b26\u53f7\u611f\u77e5\u8c31\u5377\u79ef\u7f51\u7edc\u4ea4\u66ff\u5904\u7406\u6b63\u8d1f\u8fb9", "result": "\u57286\u4e2a\u57fa\u51c6SBG\u6570\u636e\u96c6\u4e0a\u76f8\u6bd411\u4e2a\u5f3a\u7ade\u4e89\u5bf9\u624b\uff0cAUC\u63d0\u5347\u6700\u9ad84.28%\uff0cF1\u5206\u6570\u63d0\u5347\u6700\u9ad811.69%", "conclusion": "GegenNet\u901a\u8fc7\u521b\u65b0\u7684\u8c31\u5377\u79ef\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u7b26\u53f7\u4e8c\u5206\u56fe\u94fe\u63a5\u7b26\u53f7\u9884\u6d4b\u95ee\u9898\uff0c\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u8d8a\u7684\u6027\u80fd"}}
{"id": "2508.19915", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19915", "abs": "https://arxiv.org/abs/2508.19915", "authors": ["Felix N\u00fctzel", "Mischa Dombrowski", "Bernhard Kainz"], "title": "Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling", "comment": "10 pages, 3 figures, Preprint (submitted version, de-anonymized).\n  Accepted at MLMI (MICCAI Workshop) 2025. Version of Record to appear in\n  Springer LNCS; This preprint has not undergone peer review or any\n  post-submission improvements or corrections", "summary": "Retrieval-augmented learning based on radiology reports has emerged as a\npromising direction to improve performance on long-tail medical imaging tasks,\nsuch as rare disease detection in chest X-rays. Most existing methods rely on\ncomparing high-dimensional text embeddings from models like CLIP or CXR-BERT,\nwhich are often difficult to interpret, computationally expensive, and not\nwell-aligned with the structured nature of medical knowledge. We propose a\nnovel, ontology-driven alternative for comparing radiology report texts based\non clinically grounded concepts from the Unified Medical Language System\n(UMLS). Our method extracts standardised medical entities from free-text\nreports using an enhanced pipeline built on RadGraph-XL and SapBERT. These\nentities are linked to UMLS concepts (CUIs), enabling a transparent,\ninterpretable set-based representation of each report. We then define a\ntask-adaptive similarity measure based on a modified and weighted version of\nthe Tversky Index that accounts for synonymy, negation, and hierarchical\nrelationships between medical entities. This allows efficient and semantically\nmeaningful similarity comparisons between reports. We demonstrate that our\napproach outperforms state-of-the-art embedding-based retrieval methods in a\nradiograph classification task on MIMIC-CXR, particularly in long-tail\nsettings. Additionally, we use our pipeline to generate ontology-backed disease\nlabels for MIMIC-CXR, offering a valuable new resource for downstream learning\ntasks. Our work provides more explainable, reliable, and task-specific\nretrieval strategies in clinical AI systems, especially when interpretability\nand domain knowledge integration are essential. Our code is available at\nhttps://github.com/Felix-012/ontology-concept-distillation", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eUMLS\u672c\u4f53\u6982\u5ff5\u7684\u653e\u5c04\u5b66\u62a5\u544a\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u533b\u5b66\u5b9e\u4f53\u63d0\u53d6\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u5728\u80f8\u90e8X\u5149\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u5d4c\u5165\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u957f\u5c3e\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eCLIP\u6216CXR-BERT\u7b49\u9ad8\u7ef4\u6587\u672c\u5d4c\u5165\u7684\u68c0\u7d22\u65b9\u6cd5\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0e\u533b\u5b66\u77e5\u8bc6\u7ed3\u6784\u5316\u7279\u6027\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u533b\u5b66\u62a5\u544a\u6bd4\u8f83\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528RadGraph-XL\u548cSapBERT\u589e\u5f3a\u6d41\u6c34\u7ebf\u4ece\u81ea\u7531\u6587\u672c\u62a5\u544a\u4e2d\u63d0\u53d6\u6807\u51c6\u5316\u533b\u5b66\u5b9e\u4f53\uff0c\u94fe\u63a5\u5230UMLS\u6982\u5ff5(CUIs)\uff0c\u7136\u540e\u57fa\u4e8e\u6539\u8fdb\u7684\u52a0\u6743Tversky\u6307\u6570\u5b9a\u4e49\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u76f8\u4f3c\u5ea6\u5ea6\u91cf\uff0c\u8003\u8651\u540c\u4e49\u8bcd\u3001\u5426\u5b9a\u548c\u5c42\u6b21\u5173\u7cfb\u3002", "result": "\u5728MIMIC-CXR\u7684\u653e\u5c04\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u5d4c\u5165\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u957f\u5c3e\u8bbe\u7f6e\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e3aMIMIC-CXR\u751f\u6210\u4e86\u672c\u4f53\u652f\u6301\u7684\u65b0\u75be\u75c5\u6807\u7b7e\u8d44\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e34\u5e8aAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u89e3\u91ca\u3001\u53ef\u9760\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u68c0\u7d22\u7b56\u7565\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u53ef\u89e3\u91ca\u6027\u548c\u9886\u57df\u77e5\u8bc6\u6574\u5408\u7684\u573a\u666f\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2508.19924", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19924", "abs": "https://arxiv.org/abs/2508.19924", "authors": ["Liming Liu", "Ruoyu Li", "Qing Li", "Meijia Hou", "Yong Jiang", "Mingwei Xu"], "title": "FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification", "comment": null, "summary": "Network traffic classification using pre-training models has shown promising\nresults, but existing methods struggle to capture packet structural\ncharacteristics, flow-level behaviors, hierarchical protocol semantics, and\ninter-packet contextual relationships. To address these challenges, we propose\nFlowletFormer, a BERT-based pre-training model specifically designed for\nnetwork traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware\nTraffic Representation Model for segmenting traffic into semantically\nmeaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture\nmultilayer protocol semantics, and Field-Specific and Context-Aware Pretraining\nTasks to enhance both inter-packet and inter-flow learning. Experimental\nresults demonstrate that FlowletFormer significantly outperforms existing\nmethods in the effectiveness of traffic representation, classification\naccuracy, and few-shot learning capability. Moreover, by effectively\nintegrating domain-specific network knowledge, FlowletFormer shows better\ncomprehension of the principles of network transmission (e.g., stateful\nconnections of TCP), providing a more robust and trustworthy framework for\ntraffic analysis.", "AI": {"tldr": "FlowletFormer\u662f\u4e00\u4e2a\u57fa\u4e8eBERT\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u7f51\u7edc\u6d41\u91cf\u5206\u6790\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6d41\u91cf\u8868\u793a\u3001\u534f\u8bae\u8bed\u4e49\u5d4c\u5165\u548c\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d41\u91cf\u5206\u7c7b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u6d41\u91cf\u5206\u7c7b\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u6570\u636e\u5305\u7ed3\u6784\u7279\u5f81\u3001\u6d41\u7ea7\u884c\u4e3a\u3001\u5206\u5c42\u534f\u8bae\u8bed\u4e49\u548c\u5305\u95f4\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u9700\u8981\u66f4\u4e13\u4e1a\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faFlowletFormer\u6a21\u578b\uff0c\u5305\u542b\uff1a1) \u8fde\u8d2f\u884c\u4e3a\u611f\u77e5\u6d41\u91cf\u8868\u793a\u6a21\u578b\uff1b2) \u534f\u8bae\u6808\u5bf9\u9f50\u5d4c\u5165\u5c42\uff1b3) \u5b57\u6bb5\u7279\u5b9a\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u4ee5\u589e\u5f3a\u5305\u95f4\u548c\u6d41\u95f4\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFlowletFormer\u5728\u6d41\u91cf\u8868\u793a\u6548\u679c\u3001\u5206\u7c7b\u51c6\u786e\u6027\u548c\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u7406\u89e3\u7f51\u7edc\u4f20\u8f93\u539f\u7406\u3002", "conclusion": "FlowletFormer\u901a\u8fc7\u6709\u6548\u6574\u5408\u9886\u57df\u7279\u5b9a\u7684\u7f51\u7edc\u77e5\u8bc6\uff0c\u4e3a\u6d41\u91cf\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u4fe1\u7684\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2508.19955", "categories": ["cs.LG", "cs.IT", "math.IT", "62M10 (primary), 94A17 (secondary)"], "pdf": "https://arxiv.org/pdf/2508.19955", "abs": "https://arxiv.org/abs/2508.19955", "authors": ["Abhijeet Avhale", "Joscha Diehl", "Niraj Velankar", "Emanuele Verri"], "title": "Global Permutation Entropy", "comment": "12 pages, 10 figures", "summary": "Permutation Entropy, introduced by Bandt and Pompe, is a widely used\ncomplexity measure for real-valued time series that is based on the relative\norder of values within consecutive segments of fixed length. After\nstandardizing each segment to a permutation and computing the frequency\ndistribution of these permutations, Shannon Entropy is then applied to quantify\nthe series' complexity. We introduce Global Permutation Entropy (GPE), a novel\nindex that considers all possible patterns of a given length, including\nnon-consecutive ones. Its computation relies on recently developed algorithms\nthat enable the efficient extraction of full permutation profiles. We\nillustrate some properties of GPE and demonstrate its effectiveness through\nexperiments on synthetic datasets, showing that it reveals structural\ninformation not accessible through standard permutation entropy. We provide a\nJulia package for the calculation of GPE at\n`https://github.com/AThreeH1/Global-Permutation-Entropy'.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u5168\u5c40\u6392\u5217\u71b5(GPE)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u7684\u590d\u6742\u5ea6\u6307\u6807\uff0c\u4e0d\u4ec5\u8003\u8651\u8fde\u7eed\u6bb5\u843d\u7684\u6392\u5217\u6a21\u5f0f\uff0c\u8fd8\u5305\u542b\u6240\u6709\u53ef\u80fd\u957f\u5ea6\u7684\u975e\u8fde\u7eed\u6a21\u5f0f\uff0c\u901a\u8fc7\u9ad8\u6548\u7b97\u6cd5\u63d0\u53d6\u5b8c\u6574\u6392\u5217\u5206\u5e03\uff0c\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u4f18\u4e8e\u6807\u51c6\u6392\u5217\u71b5\u7684\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u6392\u5217\u71b5\u53ea\u8003\u8651\u8fde\u7eed\u6bb5\u843d\u7684\u76f8\u5bf9\u987a\u5e8f\u6a21\u5f0f\uff0c\u53ef\u80fd\u5ffd\u7565\u4e86\u65f6\u95f4\u5e8f\u5217\u4e2d\u91cd\u8981\u7684\u975e\u8fde\u7eed\u7ed3\u6784\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6355\u6349\u66f4\u5168\u9762\u6392\u5217\u6a21\u5f0f\u7684\u590d\u6742\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u65b0\u5f00\u53d1\u7684\u9ad8\u6548\u7b97\u6cd5\u63d0\u53d6\u5b8c\u6574\u6392\u5217\u5206\u5e03\uff0c\u8003\u8651\u6240\u6709\u7ed9\u5b9a\u957f\u5ea6\u7684\u53ef\u80fd\u6a21\u5f0f\uff08\u5305\u62ec\u975e\u8fde\u7eed\u6a21\u5f0f\uff09\uff0c\u7136\u540e\u5e94\u7528\u9999\u519c\u71b5\u6765\u91cf\u5316\u590d\u6742\u5ea6\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGPE\u80fd\u591f\u63ed\u793a\u6807\u51c6\u6392\u5217\u71b5\u65e0\u6cd5\u8bbf\u95ee\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u590d\u6742\u5ea6\u5206\u6790\u3002", "conclusion": "\u5168\u5c40\u6392\u5217\u71b5(GPE)\u662f\u4e00\u4e2a\u6709\u6548\u7684\u590d\u6742\u5ea6\u6307\u6807\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u6392\u5217\u71b5\u7684\u6982\u5ff5\uff0c\u80fd\u591f\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u4e2d\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u4e3a\u6b64\u63d0\u4f9b\u4e86\u5f00\u6e90\u7684Julia\u5b9e\u73b0\u5305\u3002"}}
{"id": "2508.19974", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19974", "abs": "https://arxiv.org/abs/2508.19974", "authors": ["Khaled M. A. Alghtus", "Aiyad Gannan", "Khalid M. Alhajri", "Ali L. A. Al Jubouri", "Hassan A. I. Al-Janahi"], "title": "Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning", "comment": null, "summary": "This study presents a machine learning framework for forecasting short-term\nfaults in industrial centrifugal pumps using real-time sensor data. The\napproach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in\nadvance based on patterns extracted from historical operation. Two lookback\nperiods, 60 minutes and 120 minutes, were evaluated using a sliding window\napproach. For each window, statistical features including mean, standard\ndeviation, minimum, maximum, and linear trend were extracted, and class\nimbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost\nclassifiers were trained and tested on the labeled dataset. Results show that\nthe Random Forest model achieved the best short-term forecasting performance\nwith a 60-minute window, reaching recall scores of 69.2\\% at 5 minutes, 64.9\\%\nat 15 minutes, and 48.6\\% at 30 minutes. With a 120-minute window, the Random\nForest model achieved 57.6\\% recall at 5 minutes, and improved predictive\naccuracy of 65.6\\% at both 15 and 30 minutes. XGBoost displayed similar but\nslightly lower performance. These findings highlight that optimal history\nlength depends on the prediction horizon, and that different fault patterns may\nevolve at different timescales. The proposed method offers an interpretable and\nscalable solution for integrating predictive maintenance into real-time\nindustrial monitoring systems.", "AI": {"tldr": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u5de5\u4e1a\u79bb\u5fc3\u6cf5\u77ed\u671f\u6545\u969c\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528\u968f\u673a\u68ee\u6797\u548cXGBoost\u6a21\u578b\uff0c\u901a\u8fc760\u5206\u949f\u548c120\u5206\u949f\u6ed1\u52a8\u7a97\u53e3\u63d0\u53d6\u7edf\u8ba1\u7279\u5f81\uff0c\u5b9e\u73b0\u4e865-30\u5206\u949f\u524d\u7684\u6545\u969c\u9884\u8b66\u3002", "motivation": "\u5de5\u4e1a\u79bb\u5fc3\u6cf5\u7684\u6545\u969c\u9884\u6d4b\u5bf9\u9884\u9632\u6027\u7ef4\u62a4\u81f3\u5173\u91cd\u8981\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u77ed\u671f\u6545\u969c\u9884\u8b66\uff0c\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u5b9e\u65f6\u4f20\u611f\u5668\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6ed1\u52a8\u7a97\u53e3\u65b9\u6cd5\uff0860\u5206\u949f\u548c120\u5206\u949f\uff09\u63d0\u53d6\u7edf\u8ba1\u7279\u5f81\uff08\u5747\u503c\u3001\u6807\u51c6\u5dee\u3001\u6700\u5c0f\u503c\u3001\u6700\u5927\u503c\u3001\u7ebf\u6027\u8d8b\u52bf\uff09\uff0c\u91c7\u7528SMOTE\u7b97\u6cd5\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u8bad\u7ec3\u968f\u673a\u68ee\u6797\u548cXGBoost\u5206\u7c7b\u5668\u8fdb\u884c\u6545\u969c\u9884\u6d4b\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u572860\u5206\u949f\u7a97\u53e3\u4e0b\u8868\u73b0\u6700\u4f73\uff1a5\u5206\u949f\u524d\u53ec\u56de\u738769.2%\uff0c15\u5206\u949f\u524d64.9%\uff0c30\u5206\u949f\u524d48.6%\u3002120\u5206\u949f\u7a97\u53e3\u4e0b\uff0c15\u548c30\u5206\u949f\u524d\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u81f365.6%\u3002", "conclusion": "\u9884\u6d4b\u6027\u80fd\u53d6\u51b3\u4e8e\u5386\u53f2\u6570\u636e\u957f\u5ea6\u548c\u9884\u6d4b\u65f6\u95f4\u8de8\u5ea6\uff0c\u4e0d\u540c\u6545\u969c\u6a21\u5f0f\u5177\u6709\u4e0d\u540c\u7684\u65f6\u95f4\u5c3a\u5ea6\u7279\u5f81\u3002\u8be5\u65b9\u6cd5\u4e3a\u5de5\u4e1a\u5b9e\u65f6\u76d1\u63a7\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.19979", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19979", "abs": "https://arxiv.org/abs/2508.19979", "authors": ["Behafarid Hemmatpour", "Javad Dogani", "Nikolaos Laoutaris"], "title": "Reducing Street Parking Search Time via Smart Assignment Strategies", "comment": "Please cite the ACM SIGSPATIAL'25 version of this paper", "summary": "In dense metropolitan areas, searching for street parking adds to traffic\ncongestion. Like many other problems, real-time assistants based on mobile\nphones have been proposed, but their effectiveness is understudied. This work\nquantifies how varying levels of user coordination and information availability\nthrough such apps impact search time and the probability of finding street\nparking. Through a data-driven simulation of Madrid's street parking ecosystem,\nwe analyze four distinct strategies: uncoordinated search (Unc-Agn),\ncoordinated parking without awareness of non-users (Cord-Agn), an idealized\noracle system that knows the positions of all non-users (Cord-Oracle), and our\nnovel/practical Cord-Approx strategy that estimates non-users' behavior\nprobabilistically. The Cord-Approx strategy, instead of requiring knowledge of\nhow close non-users are to a certain spot in order to decide whether to\nnavigate toward it, uses past occupancy distributions to elongate physical\ndistances between system users and alternative parking spots, and then solves a\nHungarian matching problem to dispatch accordingly. In high-fidelity\nsimulations of Madrid's parking network with real traffic data, users of\nCord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes\nfor non-users without an app. A zone-level snapshot shows that Cord-Approx\nreduces search time for system users by 72% (range = 67-76%) in central hubs,\nand up to 73% in residential areas, relative to non-users.", "AI": {"tldr": "\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u6a21\u62df\u5206\u6790\u4e86\u4e0d\u540c\u505c\u8f66\u641c\u7d22\u7b56\u7565\u7684\u6548\u679c\uff0cCord-Approx\u7b56\u7565\u5c06\u7528\u6237\u505c\u8f66\u641c\u7d22\u65f6\u95f4\u4ece19.98\u5206\u949f\u964d\u81f36.69\u5206\u949f\uff0c\u51cf\u5c1172-73%", "motivation": "\u57ce\u5e02\u5bc6\u96c6\u533a\u57df\u7684\u8def\u8fb9\u505c\u8f66\u641c\u7d22\u52a0\u5267\u4e86\u4ea4\u901a\u62d5\u585e\uff0c\u9700\u8981\u91cf\u5316\u5206\u6790\u79fb\u52a8\u8bbe\u5907\u534f\u540c\u505c\u8f66\u7cfb\u7edf\u7684\u5b9e\u9645\u6548\u679c", "method": "\u4f7f\u7528\u9a6c\u5fb7\u91cc\u5b9e\u9645\u4ea4\u901a\u6570\u636e\u8fdb\u884c\u9ad8\u4fdd\u771f\u6a21\u62df\uff0c\u6bd4\u8f83\u56db\u79cd\u505c\u8f66\u7b56\u7565\uff1a\u65e0\u534f\u8c03\u641c\u7d22\u3001\u534f\u8c03\u65e0\u975e\u7528\u6237\u77e5\u8bc6\u3001\u7406\u60f3\u795e\u8c15\u7cfb\u7edf\u548c\u65b0\u9898Cord-Approx\u7b56\u7565\uff08\u5229\u7528\u5386\u53f2\u5360\u7528\u5206\u5e03\u4f30\u8ba1\u975e\u7528\u6237\u884c\u4e3a\uff09", "result": "Cord-Approx\u7528\u6237\u5e73\u5747\u641c\u7d22\u65f6\u95f46.69\u5206\u949f\uff0c\u8f83\u65e0\u5e94\u7528\u7a0b\u5e0c\u7684\u7528\u6237\uff0819.98\u5206\u949f\uff09\u51cf\u5c1172%\uff0c\u5728\u4e2d\u5fc3\u533a\u57df\u548c\u4f4f\u5b85\u533a\u5747\u53ef\u5b9e\u73b0\u8fd973%\u7684\u641c\u7d22\u65f6\u95f4\u7f29\u51cf", "conclusion": "Cord-Approx\u7b56\u7565\u901a\u8fc7\u6982\u7387\u6027\u4f30\u8ba1\u975e\u7528\u6237\u884c\u4e3a\uff0c\u5728\u4e0d\u4f9d\u8d56\u5b8c\u6574\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u663e\u8457\u63d0\u5347\u505c\u8f66\u641c\u7d22\u6548\u7387\uff0c\u4e3a\u57ce\u5e02\u505c\u8f66\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.19980", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.19980", "abs": "https://arxiv.org/abs/2508.19980", "authors": ["Dylan Sam", "Alexander Robey", "Andy Zou", "Matt Fredrikson", "J. Zico Kolter"], "title": "Evaluating Language Model Reasoning about Confidential Information", "comment": "20 pages", "summary": "As language models are increasingly deployed as autonomous agents in\nhigh-stakes settings, ensuring that they reliably follow user-defined rules has\nbecome a critical safety concern. To this end, we study whether language models\nexhibit contextual robustness, or the capability to adhere to context-dependent\nsafety specifications. For this analysis, we develop a benchmark (PasswordEval)\nthat measures whether language models can correctly determine when a user\nrequest is authorized (i.e., with a correct password). We find that current\nopen- and closed-source models struggle with this seemingly simple task, and\nthat, perhaps surprisingly, reasoning capabilities do not generally improve\nperformance. In fact, we find that reasoning traces frequently leak\nconfidential information, which calls into question whether reasoning traces\nshould be exposed to users in such applications. We also scale the difficulty\nof our evaluation along multiple axes: (i) by adding adversarial user pressure\nthrough various jailbreaking strategies, and (ii) through longer multi-turn\nconversations where password verification is more challenging. Overall, our\nresults suggest that current frontier models are not well-suited to handling\nconfidential information, and that reasoning capabilities may need to be\ntrained in a different manner to make them safer for release in high-stakes\nsettings.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u5bc6\u7801\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63a8\u7406\u80fd\u529b\u53cd\u800c\u4f1a\u6cc4\u9732\u673a\u5bc6\u4fe1\u606f\uff0c\u5f53\u524d\u524d\u6cbf\u6a21\u578b\u4e0d\u9002\u5408\u5904\u7406\u654f\u611f\u4fe1\u606f", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5728\u5173\u952e\u573a\u666f\u4e2d\u4f5c\u4e3a\u81ea\u4e3b\u4ee3\u7406\u90e8\u7f72\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u9075\u5faa\u7528\u6237\u5b9a\u4e49\u89c4\u5219\u5df2\u6210\u4e3a\u5173\u952e\u5b89\u5168\u95ee\u9898\uff0c\u9700\u8981\u7814\u7a76\u6a21\u578b\u662f\u5426\u5177\u5907\u4e0a\u4e0b\u6587\u9c81\u68d2\u6027", "method": "\u5f00\u53d1PasswordEval\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6d4b\u91cf\u6a21\u578b\u80fd\u5426\u6b63\u786e\u5224\u65ad\u7528\u6237\u8bf7\u6c42\u662f\u5426\u88ab\u6388\u6743\uff08\u901a\u8fc7\u5bc6\u7801\u9a8c\u8bc1\uff09\uff0c\u5e76\u6cbf\u591a\u4e2a\u7ef4\u5ea6\u6269\u5c55\u96be\u5ea6\uff1a\u6dfb\u52a0\u5bf9\u6297\u6027\u7528\u6237\u538b\u529b\u548c\u591a\u8f6e\u5bf9\u8bdd", "result": "\u5f53\u524d\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u5728\u8fd9\u4e2a\u770b\u4f3c\u7b80\u5355\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u63a8\u7406\u80fd\u529b\u901a\u5e38\u4e0d\u4f1a\u6539\u5584\u6027\u80fd\uff0c\u53cd\u800c\u7ecf\u5e38\u6cc4\u9732\u673a\u5bc6\u4fe1\u606f", "conclusion": "\u5f53\u524d\u524d\u6cbf\u6a21\u578b\u4e0d\u9002\u5408\u5904\u7406\u673a\u5bc6\u4fe1\u606f\uff0c\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u9700\u8981\u4ee5\u4e0d\u540c\u65b9\u5f0f\u8bad\u7ec3\u624d\u80fd\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e2d\u5b89\u5168\u90e8\u7f72"}}
{"id": "2508.19990", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19990", "abs": "https://arxiv.org/abs/2508.19990", "authors": ["Xiaodong Cui", "A F M Saif", "Brian Kingsbury", "Tianyi Chen"], "title": "Self-Supervised Pre-Training with Equilibrium Constraints", "comment": null, "summary": "Self-supervised pre-training using unlabeled data is widely used in machine\nlearning. In this paper, we propose a new self-supervised pre-training approach\nto dealing with heterogeneous data. Instead of mixing all the data and\nminimizing the averaged global loss in the conventional way, we impose\nadditional equilibrium constraints to ensure that the models optimizes each\nsource of heterogeneous data to its local optima after $K$-step gradient\ndescent initialized from the model. We formulate this as a bilevel optimization\nproblem, and use the first-order approximation method to solve the problem. We\ndiscuss its connection to model-agnostic meta learning (MAML). Experiments are\ncarried out on self-supervised pre-training using multi-domain and multilingual\ndatasets, demonstrating that the proposed approach can significantly improve\nthe adaptivity of the self-supervised pre-trained model for the downstream\nsupervised fine-tuning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u548c\u5747\u8861\u7ea6\u675f\u5904\u7406\u5f02\u6784\u6570\u636e\uff0c\u63d0\u9ad8\u6a21\u578b\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027", "motivation": "\u4f20\u7edf\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u65f6\u7b80\u5355\u6df7\u5408\u6240\u6709\u6570\u636e\u5e76\u6700\u5c0f\u5316\u5168\u5c40\u5e73\u5747\u635f\u5931\uff0c\u65e0\u6cd5\u786e\u4fdd\u6a21\u578b\u5bf9\u6bcf\u4e2a\u5f02\u6784\u6570\u636e\u6e90\u90fd\u8fbe\u5230\u5c40\u90e8\u6700\u4f18", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7K\u6b65\u68af\u5ea6\u4e0b\u964d\u786e\u4fdd\u6a21\u578b\u4ece\u521d\u59cb\u70b9\u51fa\u53d1\u80fd\u4e3a\u6bcf\u4e2a\u5f02\u6784\u6570\u636e\u6e90\u627e\u5230\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u4f7f\u7528\u4e00\u9636\u8fd1\u4f3c\u65b9\u6cd5\u6c42\u89e3", "result": "\u5728\u591a\u9886\u57df\u548c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u4e0b\u6e38\u76d1\u7763\u5fae\u8c03\u4efb\u52a1\u4e2d\u7684\u9002\u5e94\u6027", "conclusion": "\u63d0\u51fa\u7684\u5747\u8861\u7ea6\u675f\u548c\u53cc\u5c42\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u6570\u636e\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u95ee\u9898\uff0c\u4e0eMAML\u6709\u7406\u8bba\u8054\u7cfb\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027"}}
{"id": "2508.19999", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.19999", "abs": "https://arxiv.org/abs/2508.19999", "authors": ["Ziniu Zhang", "Zhenshuo Zhang", "Dongyue Li", "Lu Wang", "Jennifer Dy", "Hongyang R. Zhang"], "title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation", "comment": "19 pages. To appear in EMNLP'25", "summary": "This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n$\\mathbf{1}\\%$ error across six datasets. This allows us to scale up subset\nselection that would otherwise run full inference by up to\n$\\mathbf{37.7}\\times$ on models with up to $34$ billion parameters, and\noutperform existing selection methods based on input embeddings by\n$\\mathbf{11}\\%$ on average.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u793a\u4f8b\u9009\u62e9\u7b97\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u4f30\u8ba1\u6765\u9ad8\u6548\u9009\u62e9\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u6700\u4f73\u793a\u4f8b\u5b50\u96c6\uff0c\u5927\u5e45\u63d0\u5347\u9009\u62e9\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5728\u56fa\u5b9a\u6a21\u578b\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u9ad8\u6548\u9009\u62e9\u6700\u4f73\u7684k\u4e2a\u793a\u4f8b\u6765\u4f18\u5316\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u95ee\u9898\uff0c\u8fd9\u5728\u63d0\u793a\u8c03\u6574\u548c\u94fe\u5f0f\u601d\u7eea\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u4f30\u8ba1\u8f93\u51fa\uff0c\u91c7\u7528\u4e00\u9636\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u591a\u6b21\u968f\u673a\u91c7\u6837\u5b50\u96c6\u5e76\u805a\u5408\u5f71\u54cd\u5206\u6570\uff0c\u6700\u7ec8\u9009\u62e9\u6700\u76f8\u5173\u7684k\u4e2a\u793a\u4f8b\u3002\u7b97\u6cd5\u53ea\u9700\u9884\u8ba1\u7b97\u6a21\u578b\u8f93\u51fa\u548c\u68af\u5ea6\u4e00\u6b21\uff0c\u590d\u6742\u5ea6\u4e3a\u7ebf\u6027\u3002", "result": "\u57286\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u5c0f\u4e8e1%\uff0c\u9009\u62e9\u901f\u5ea6\u63d0\u534737.7\u500d\uff0c\u6027\u80fd\u8d85\u8fc7\u73b0\u6709\u57fa\u4e8e\u8f93\u5165\u5d4c\u5165\u7684\u65b9\u6cd511%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u793a\u4f8b\u9009\u62e9\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20013", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.20013", "abs": "https://arxiv.org/abs/2508.20013", "authors": ["Lotte Gross", "Rebecca Walter", "Nicole Zoppi", "Adrien Justus", "Alessandro Gambetti", "Qiwei Han", "Maximilian Kaiser"], "title": "Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach", "comment": "10 pages, 5 figures, 3 tables", "summary": "This study addresses critical industrial challenges in e-commerce product\ncategorization, namely platform heterogeneity and the structural limitations of\nexisting taxonomies, by developing and deploying a multimodal hierarchical\nclassification framework. Using a dataset of 271,700 products from 40\ninternational fashion e-commerce platforms, we integrate textual features\n(RoBERTa), visual features (ViT), and joint vision--language representations\n(CLIP). We investigate fusion strategies, including early, late, and\nattention-based fusion within a hierarchical architecture enhanced by dynamic\nmasking to ensure taxonomic consistency. Results show that CLIP embeddings\ncombined via an MLP-based late-fusion strategy achieve the highest hierarchical\nF1 (98.59\\%), outperforming unimodal baselines. To address shallow or\ninconsistent categories, we further introduce a self-supervised ``product\nrecategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which\ndiscovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with\ncluster purities above 86\\%. Cross-platform experiments reveal a\ndeployment-relevant trade-off: complex late-fusion methods maximize accuracy\nwith diverse training data, while simpler early-fusion methods generalize more\neffectively to unseen platforms. Finally, we demonstrate the framework's\nindustrial scalability through deployment in EURWEB's commercial transaction\nintelligence platform via a two-stage inference pipeline, combining a\nlightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance\ncost and accuracy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u5c42\u6b21\u5206\u7c7b\u6846\u67b6\u6765\u89e3\u51b3\u7535\u5546\u4ea7\u54c1\u5206\u7c7b\u4e2d\u7684\u5e73\u53f0\u5f02\u8d28\u6027\u548c\u73b0\u6709\u5206\u7c7b\u6cd5\u7ed3\u6784\u9650\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u878d\u5408\u6587\u672c\u3001\u89c6\u89c9\u548c\u89c6\u89c9-\u8bed\u8a00\u7279\u5f81\uff0c\u5728\u65f6\u5c1a\u7535\u5546\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e8698.59%\u7684\u5c42\u6b21F1\u5206\u6570\uff0c\u5e76\u5c55\u793a\u4e86\u5de5\u4e1a\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u89e3\u51b3\u7535\u5546\u4ea7\u54c1\u5206\u7c7b\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u5de5\u4e1a\u6311\u6218\uff1a\u5e73\u53f0\u5f02\u8d28\u6027\u548c\u73b0\u6709\u5206\u7c7b\u6cd5\u7684\u7ed3\u6784\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u591a\u5e73\u53f0\u6570\u636e\u5e76\u4fdd\u6301\u5206\u7c7b\u4e00\u81f4\u6027\u7684\u5206\u7c7b\u6846\u67b6\u3002", "method": "\u4f7f\u7528271,700\u4e2a\u4ea7\u54c1\u7684\u6570\u636e\u96c6\uff0c\u6574\u5408RoBERTa\u6587\u672c\u7279\u5f81\u3001ViT\u89c6\u89c9\u7279\u5f81\u548cCLIP\u89c6\u89c9-\u8bed\u8a00\u7279\u5f81\uff0c\u7814\u7a76\u65e9\u671f\u878d\u5408\u3001\u665a\u671f\u878d\u5408\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u878d\u5408\u7b56\u7565\uff0c\u91c7\u7528\u52a8\u6001\u63a9\u7801\u7684\u5c42\u6b21\u67b6\u6784\u786e\u4fdd\u5206\u7c7b\u4e00\u81f4\u6027\uff0c\u5e76\u5f15\u5165\u81ea\u76d1\u7763\u4ea7\u54c1\u91cd\u5206\u7c7b\u6d41\u7a0b\u3002", "result": "CLIP\u5d4c\u5165\u901a\u8fc7MLP\u665a\u671f\u878d\u5408\u7b56\u7565\u83b7\u5f97\u6700\u9ad8\u5c42\u6b21F1\u5206\u6570(98.59%)\uff1b\u81ea\u76d1\u7763\u91cd\u5206\u7c7b\u6d41\u7a0b\u53d1\u73b0\u65b0\u7684\u7ec6\u7c92\u5ea6\u7c7b\u522b\uff0c\u805a\u7c7b\u7eaf\u5ea6\u8d85\u8fc786%\uff1b\u8de8\u5e73\u53f0\u5b9e\u9a8c\u663e\u793a\u665a\u671f\u878d\u5408\u65b9\u6cd5\u5728\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u4e0a\u51c6\u786e\u7387\u6700\u9ad8\uff0c\u800c\u65e9\u671f\u878d\u5408\u65b9\u6cd5\u5bf9\u672a\u89c1\u5e73\u53f0\u6cdb\u5316\u66f4\u597d\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u5c42\u6b21\u5206\u7c7b\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u7535\u5546\u4ea7\u54c1\u5206\u7c7b\u7684\u5de5\u4e1a\u6311\u6218\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u63a8\u7406\u7ba1\u9053\u5b9e\u73b0\u4e86\u5de5\u4e1a\u53ef\u6269\u5c55\u6027\u90e8\u7f72\uff0c\u5e73\u8861\u4e86\u6210\u672c\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u5b9e\u9645\u5546\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20015", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20015", "abs": "https://arxiv.org/abs/2508.20015", "authors": ["Julian Arnold", "Niels L\u00f6rch"], "title": "Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment", "comment": "11+25 pages, 4+11 figures", "summary": "Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is\nbroadly misaligned with respect to human values. To understand when and how\nthis emergent misalignment occurs, we develop a comprehensive framework for\ndetecting and characterizing rapid transitions during fine-tuning using both\ndistributional change detection methods as well as order parameters that are\nformulated in plain English and evaluated by an LLM judge. Using an objective\nstatistical dissimilarity measure, we quantify how the phase transition that\noccurs during fine-tuning affects multiple aspects of the model. In particular,\nwe assess what percentage of the total distributional change in model outputs\nis captured by different aspects, such as alignment or verbosity, providing a\ndecomposition of the overall transition. We also find that the actual\nbehavioral transition occurs later in training than indicated by the peak in\nthe gradient norm alone. Our framework enables the automated discovery and\nquantification of language-based order parameters, which we demonstrate on\nexamples ranging from knowledge questions to politics and ethics.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u68c0\u6d4b\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5feb\u901f\u8f6c\u53d8\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u5206\u5e03\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u548c\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u5e8f\u53c2\u91cf\u6765\u91cf\u5316LLM\u5fae\u8c03\u4e2d\u7684\u76f8\u53d8\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76LLM\u5728\u72ed\u7a84\u6709\u5bb3\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u65f6\u5982\u4f55\u51fa\u73b0\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5e7f\u6cdb\u4e0d\u5339\u914d\u7684\u884c\u4e3a\uff0c\u7406\u89e3\u8fd9\u79cd\u7a81\u53d1\u6027\u4e0d\u5339\u914d\u4f55\u65f6\u4ee5\u53ca\u5982\u4f55\u53d1\u751f\u3002", "method": "\u7ed3\u5408\u5206\u5e03\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u548c\u7531LLM\u8bc4\u4f30\u7684\u82f1\u6587\u5e8f\u53c2\u91cf\uff0c\u4f7f\u7528\u5ba2\u89c2\u7edf\u8ba1\u5dee\u5f02\u5ea6\u91cf\u6765\u91cf\u5316\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u76f8\u53d8\u5bf9\u6a21\u578b\u591a\u65b9\u9762\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5b9e\u9645\u884c\u4e3a\u8f6c\u53d8\u53d1\u751f\u5728\u8bad\u7ec3\u540e\u671f\uff0c\u6bd4\u68af\u5ea6\u8303\u6570\u5cf0\u503c\u6307\u793a\u7684\u65f6\u95f4\u66f4\u665a\uff1b\u80fd\u591f\u81ea\u52a8\u5316\u53d1\u73b0\u548c\u91cf\u5316\u57fa\u4e8e\u8bed\u8a00\u7684\u5e8f\u53c2\u91cf\uff0c\u9002\u7528\u4e8e\u77e5\u8bc6\u95ee\u7b54\u3001\u653f\u6cbb\u548c\u4f26\u7406\u7b49\u591a\u4e2a\u9886\u57df\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u8868\u5f81\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5feb\u901f\u8f6c\u53d8\uff0c\u4e3a\u7406\u89e3LLM\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u76f8\u53d8\u73b0\u8c61\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u5206\u6790\u65b9\u6cd5\u3002"}}
{"id": "2508.20019", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.20019", "abs": "https://arxiv.org/abs/2508.20019", "authors": ["Ji Wang", "Kashing Chen", "Xinyuan Song", "Ke Zhang", "Lynn Ai", "Eric Yang", "Bill Shi"], "title": "Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence", "comment": null, "summary": "Most existing Large Language Model (LLM)-based agent frameworks rely on\ncentralized orchestration, incurring high deployment costs, rigid communication\ntopologies, and limited adaptability. To address these challenges, we introduce\nSymphony, a decentralized multi-agent system which enables lightweight LLMs on\nconsumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:\n(1) a decentralized ledger that records capabilities, (2) a Beacon-selection\nprotocol for dynamic task allocation, and (3) weighted result voting based on\nCoTs. This design forms a privacy-saving, scalable, and fault-tolerant\norchestration with low overhead. Empirically, Symphony outperforms existing\nbaselines on reasoning benchmarks, achieving substantial accuracy gains and\ndemonstrating robustness across models of varying capacities.", "AI": {"tldr": "Symphony\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u8d26\u672c\u3001\u4fe1\u6807\u9009\u62e9\u534f\u8bae\u548c\u52a0\u6743\u6295\u7968\u673a\u5236\uff0c\u4f7f\u6d88\u8d39\u7ea7GPU\u4e0a\u7684\u8f7b\u91cf\u7ea7LLM\u80fd\u591f\u534f\u540c\u5de5\u4f5c\uff0c\u89e3\u51b3\u4e86\u96c6\u4e2d\u5f0f\u7f16\u6392\u7684\u9ad8\u6210\u672c\u3001\u62d3\u6251\u50f5\u5316\u548c\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u667a\u80fd\u4f53\u6846\u67b6\u5927\u591a\u91c7\u7528\u96c6\u4e2d\u5f0f\u7f16\u6392\uff0c\u5b58\u5728\u90e8\u7f72\u6210\u672c\u9ad8\u3001\u901a\u4fe1\u62d3\u6251\u50f5\u5316\u3001\u9002\u5e94\u6027\u6709\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u5206\u5e03\u5f0f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSymphony\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u673a\u5236\uff1a1\uff09\u53bb\u4e2d\u5fc3\u5316\u8d26\u672c\u8bb0\u5f55\u80fd\u529b\uff1b2\uff09\u4fe1\u6807\u9009\u62e9\u534f\u8bae\u8fdb\u884c\u52a8\u6001\u4efb\u52a1\u5206\u914d\uff1b3\uff09\u57fa\u4e8e\u601d\u7ef4\u94fe\u7684\u52a0\u6743\u7ed3\u679c\u6295\u7968\u3002", "result": "\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u5e76\u5728\u4e0d\u540c\u5bb9\u91cf\u6a21\u578b\u95f4\u5c55\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "Symphony\u63d0\u4f9b\u4e86\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u3001\u53ef\u6269\u5c55\u3001\u5bb9\u9519\u4e14\u4f4e\u5f00\u9500\u7684\u7f16\u6392\u65b9\u6848\uff0c\u8bc1\u660e\u4e86\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\u5728LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.20021", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20021", "abs": "https://arxiv.org/abs/2508.20021", "authors": ["Felix M\u00f6hrlein", "Martin K\u00e4ppel", "Julian Neuberger", "Sven Weinzierl", "Lars Ackermann", "Martin Matzner", "Stefan Jablonski"], "title": "FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring", "comment": "Proceedings of the Best BPM Dissertation Award, Doctoral Consortium,\n  and Demonstrations & Resources Forum co-located with 23rd International\n  Conference on Business Process Management (BPM 2025), Seville, Spain, August\n  31st to September 5th, 2025", "summary": "Sensitive attributes like gender or age can lead to unfair predictions in\nmachine learning tasks such as predictive business process monitoring,\nparticularly when used without considering context. We present FairLoop1, a\ntool for human-guided bias mitigation in neural network-based prediction\nmodels. FairLoop distills decision trees from neural networks, allowing users\nto inspect and modify unfair decision logic, which is then used to fine-tune\nthe original model towards fairer predictions. Compared to other approaches to\nfairness, FairLoop enables context-aware bias removal through human\ninvolvement, addressing the influence of sensitive attributes selectively\nrather than excluding them uniformly.", "AI": {"tldr": "FairLoop\u662f\u4e00\u4e2a\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6a21\u578b\u4e2d\u4eba\u5de5\u5f15\u5bfc\u504f\u5dee\u7f13\u89e3\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u4ece\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u53d6\u51b3\u7b56\u6811\u8ba9\u7528\u6237\u68c0\u67e5\u548c\u4fee\u6539\u4e0d\u516c\u5e73\u51b3\u7b56\u903b\u8f91\uff0c\u7136\u540e\u5fae\u8c03\u539f\u59cb\u6a21\u578b\u4ee5\u83b7\u5f97\u66f4\u516c\u5e73\u7684\u9884\u6d4b", "motivation": "\u654f\u611f\u5c5e\u6027\uff08\u5982\u6027\u522b\u6216\u5e74\u9f84\uff09\u5728\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u53ef\u80fd\u5bfc\u81f4\u4e0d\u516c\u5e73\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u4e0d\u8003\u8651\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\u4f7f\u7528\u65f6\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u7edf\u4e00\u6392\u9664\u654f\u611f\u5c5e\u6027\uff0c\u4f46\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u504f\u5dee\u79fb\u9664", "method": "FairLoop\u4ece\u795e\u7ecf\u7f51\u7edc\u4e2d\u63d0\u53d6\u51b3\u7b56\u6811\uff0c\u5141\u8bb8\u7528\u6237\u68c0\u67e5\u548c\u4fee\u6539\u4e0d\u516c\u5e73\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u7136\u540e\u4f7f\u7528\u4fee\u6539\u540e\u7684\u903b\u8f91\u6765\u5fae\u8c03\u539f\u59cb\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u504f\u5dee\u79fb\u9664", "result": "\u76f8\u6bd4\u5176\u4ed6\u516c\u5e73\u6027\u65b9\u6cd5\uff0cFairLoop\u901a\u8fc7\u4eba\u5de5\u53c2\u4e0e\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u504f\u5dee\u79fb\u9664\uff0c\u80fd\u591f\u9009\u62e9\u6027\u5730\u5904\u7406\u654f\u611f\u5c5e\u6027\u7684\u5f71\u54cd\uff0c\u800c\u4e0d\u662f\u7edf\u4e00\u6392\u9664\u5b83\u4eec", "conclusion": "FairLoop\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4eba\u5de5\u5f15\u5bfc\u504f\u5dee\u7f13\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6a21\u578b\u4e2d\u5f15\u5165\u4eba\u7c7b\u76d1\u7763\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u7ec6\u548c\u6709\u6548\u7684\u516c\u5e73\u6027\u4fdd\u8bc1"}}
{"id": "2508.20024", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20024", "abs": "https://arxiv.org/abs/2508.20024", "authors": ["Deddy Jobson", "Muktti Shukla", "Phuong Dinh", "Julio Christian Young", "Nick Pitton", "Nina Chen", "Ryan Ginstrom"], "title": "Using item recommendations and LLMs in marketing email titles", "comment": "Accepted to The Second Workshop on Generative AI for E-commerce\n  (GenAIECommerce '25), held September 22, 2025, in Prague, Czech Republic. 3\n  figures", "summary": "E-commerce marketplaces make use of a number of marketing channels like\nemails, push notifications, etc. to reach their users and stimulate purchases.\nPersonalized emails especially are a popular touch point for marketers to\ninform users of latest items in stock, especially for those who stopped\nvisiting the marketplace. Such emails contain personalized recommendations\ntailored to each user's interests, enticing users to buy relevant items. A\ncommon limitation of these emails is that the primary entry point, the title of\nthe email, tends to follow fixed templates, failing to inspire enough interest\nin the contents. In this work, we explore the potential of large language\nmodels (LLMs) for generating thematic titles that reflect the personalized\ncontent of the emails. We perform offline simulations and conduct online\nexperiments on the order of millions of users, finding our techniques useful in\nimproving the engagement between customers and our emails. We highlight key\nfindings and learnings as we productionize the safe and automated generation of\nemail titles for millions of users.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u4e2a\u6027\u5316\u63a8\u8350\u90ae\u4ef6\u751f\u6210\u4e3b\u9898\u6807\u9898\uff0c\u901a\u8fc7\u79bb\u7ebf\u548c\u5728\u7ebf\u5b9e\u9a8c\u8bc1\u660e\u80fd\u6709\u6548\u63d0\u5347\u7528\u6237\u53c2\u4e0e\u5ea6", "motivation": "\u7535\u5546\u5e73\u53f0\u4e2a\u6027\u5316\u90ae\u4ef6\u7684\u6807\u9898\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u6a21\u677f\uff0c\u65e0\u6cd5\u5145\u5206\u6fc0\u53d1\u7528\u6237\u5bf9\u90ae\u4ef6\u5185\u5bb9\u7684\u5174\u8da3\uff0c\u9650\u5236\u4e86\u8425\u9500\u6548\u679c", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u6620\u90ae\u4ef6\u4e2a\u6027\u5316\u5185\u5bb9\u7684\u4e3b\u9898\u6807\u9898\uff0c\u8fdb\u884c\u79bb\u7ebf\u6a21\u62df\u548c\u6570\u767e\u4e07\u7528\u6237\u89c4\u6a21\u7684\u5728\u7ebf\u5b9e\u9a8c", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6280\u672f\u80fd\u6709\u6548\u6539\u5584\u5ba2\u6237\u4e0e\u90ae\u4ef6\u4e4b\u95f4\u7684\u4e92\u52a8\u53c2\u4e0e\u5ea6", "conclusion": "\u6210\u529f\u5b9e\u73b0\u4e86\u4e3a\u767e\u4e07\u7ea7\u7528\u6237\u5b89\u5168\u81ea\u52a8\u5316\u751f\u6210\u90ae\u4ef6\u6807\u9898\u7684\u751f\u4ea7\u5316\u90e8\u7f72\uff0c\u5e76\u603b\u7ed3\u4e86\u5173\u952e\u53d1\u73b0\u548c\u7ecf\u9a8c"}}
{"id": "2508.20032", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20032", "abs": "https://arxiv.org/abs/2508.20032", "authors": ["Santosh Chapagain", "Shah Muhammad Hamdi", "Soukaina Filali Boubrahimi"], "title": "Pruning Strategies for Backdoor Defense in LLMs", "comment": "Accepted in CIKM '25: The 34th ACM International Conference on\n  Information and Knowledge Management Proceedings", "summary": "Backdoor attacks are a significant threat to the performance and integrity of\npre-trained language models. Although such models are routinely fine-tuned for\ndownstream NLP tasks, recent work shows they remain vulnerable to backdoor\nattacks that survive vanilla fine-tuning. These attacks are difficult to defend\nbecause end users typically lack knowledge of the attack triggers. Such attacks\nconsist of stealthy malicious triggers introduced through subtle syntactic or\nstylistic manipulations, which can bypass traditional detection and remain in\nthe model, making post-hoc purification essential. In this study, we explore\nwhether attention-head pruning can mitigate these threats without any knowledge\nof the trigger or access to a clean reference model. To this end, we design and\nimplement six pruning-based strategies: (i) gradient-based pruning, (ii)\nlayer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2\nsparsification, (iv) randomized ensemble pruning, (v)\nreinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.\nEach method iteratively removes the least informative heads while monitoring\nvalidation accuracy to avoid over-pruning. Experimental evaluation shows that\ngradient-based pruning performs best while defending the syntactic triggers,\nwhereas reinforcement learning and Bayesian pruning better withstand stylistic\nattacks.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u6ce8\u610f\u529b\u5934\u526a\u679d\u6765\u9632\u5fa1\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u540e\u95e8\u653b\u51fb\uff0c\u63d0\u51fa\u4e86\u516d\u79cd\u526a\u679d\u7b56\u7565\uff0c\u5b9e\u9a8c\u8868\u660e\u4e0d\u540c\u7b56\u7565\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u653b\u51fb\u89e6\u53d1\u65b9\u5f0f\u6548\u679c\u4e0d\u540c\u3002", "motivation": "\u540e\u95e8\u653b\u51fb\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u5b8c\u6574\u6027\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u5373\u4f7f\u7ecf\u8fc7\u5fae\u8c03\u540e\u8fd9\u4e9b\u653b\u51fb\u4ecd\u7136\u5b58\u5728\u3002\u7531\u4e8e\u7ec8\u7aef\u7528\u6237\u901a\u5e38\u4e0d\u4e86\u89e3\u653b\u51fb\u89e6\u53d1\u5668\uff0c\u4f20\u7edf\u68c0\u6d4b\u96be\u4ee5\u9632\u5fa1\uff0c\u56e0\u6b64\u9700\u8981\u4e8b\u540e\u51c0\u5316\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u516d\u79cd\u57fa\u4e8e\u526a\u679d\u7684\u7b56\u7565\uff1a\u68af\u5ea6\u526a\u679d\u3001\u5c42\u95f4\u65b9\u5dee\u526a\u679d\u3001\u7ed3\u6784\u5316\u7a00\u758f\u526a\u679d\u3001\u968f\u673a\u96c6\u6210\u526a\u679d\u3001\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u526a\u679d\u548c\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u526a\u679d\u3002\u8fd9\u4e9b\u65b9\u6cd5\u8fed\u4ee3\u79fb\u9664\u4fe1\u606f\u91cf\u6700\u5c0f\u7684\u6ce8\u610f\u529b\u5934\uff0c\u540c\u65f6\u76d1\u63a7\u9a8c\u8bc1\u51c6\u786e\u7387\u4ee5\u907f\u514d\u8fc7\u5ea6\u526a\u679d\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0c\u68af\u5ea6\u526a\u679d\u5728\u9632\u5fa1\u8bed\u6cd5\u89e6\u53d1\u653b\u51fb\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u548c\u8d1d\u53f6\u65af\u526a\u679d\u5728\u5e94\u5bf9\u98ce\u683c\u653b\u51fb\u65b9\u9762\u6548\u679c\u66f4\u597d\u3002", "conclusion": "\u6ce8\u610f\u529b\u5934\u526a\u679d\u53ef\u4ee5\u6709\u6548\u7f13\u89e3\u540e\u95e8\u653b\u51fb\u5a01\u80c1\uff0c\u4e0d\u540c\u526a\u679d\u7b56\u7565\u9002\u7528\u4e8e\u4e0d\u540c\u7c7b\u578b\u7684\u653b\u51fb\u89e6\u53d1\u65b9\u5f0f\uff0c\u4e3a\u540e\u95e8\u9632\u5fa1\u63d0\u4f9b\u4e86\u65e0\u9700\u89e6\u53d1\u5668\u77e5\u8bc6\u6216\u5e72\u51c0\u53c2\u8003\u6a21\u578b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.20056", "categories": ["cs.LG", "90-08, 90B35, 90C59, 90C99, 68T20, 90C27"], "pdf": "https://arxiv.org/pdf/2508.20056", "abs": "https://arxiv.org/abs/2508.20056", "authors": ["Vil\u00e9m Heinz", "Petr Vil\u00edm", "Zden\u011bk Hanz\u00e1lek"], "title": "Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks", "comment": null, "summary": "Failure-Directed Search (FDS) is a significant complete generic search\nalgorithm used in Constraint Programming (CP) to efficiently explore the search\nspace, proven particularly effective on scheduling problems. This paper\nanalyzes FDS's properties, showing that minimizing the size of its search tree\nguided by ranked branching decisions is closely related to the Multi-armed\nbandit (MAB) problem. Building on this insight, MAB reinforcement learning\nalgorithms are applied to FDS, extended with problem-specific refinements and\nparameter tuning, and evaluated on the two most fundamental scheduling\nproblems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained\nProject Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best\nextended MAB algorithm and configuration, performs 1.7 times faster on the JSSP\nand 2.1 times faster on the RCPSP benchmarks compared to the original\nimplementation in a new solver called OptalCP, while also being 3.5 times\nfaster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the\ncurrent state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,\nusing only a 900-second time limit per instance, the enhanced FDS improved the\nexisting state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP\nstandard open benchmark instances while also completely closing a few of them.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5c06\u591a\u81c2\u8001\u864e\u673a\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5e94\u7528\u4e8eFailure-Directed Search (FDS)\uff0c\u5728\u8c03\u5ea6\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u6bd4\u539f\u59cbFDS\u5feb1.7-2.1\u500d\uff0c\u6bd4IBM CP Optimizer\u5feb2.1-3.5\u500d\uff0c\u5e76\u6539\u8fdb\u4e86\u591a\u4e2a\u57fa\u51c6\u5b9e\u4f8b\u7684\u6700\u4f18\u4e0b\u754c\u3002", "motivation": "FDS\u662f\u7ea6\u675f\u89c4\u5212\u4e2d\u91cd\u8981\u7684\u5b8c\u5168\u901a\u7528\u641c\u7d22\u7b97\u6cd5\uff0c\u5728\u8c03\u5ea6\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u7814\u7a76\u53d1\u73b0FDS\u7684\u641c\u7d22\u6811\u6700\u5c0f\u5316\u4e0e\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\u5bc6\u5207\u76f8\u5173\uff0c\u8fd9\u4e3a\u5e94\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5c06\u591a\u81c2\u8001\u864e\u673a\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5e94\u7528\u4e8eFDS\uff0c\u5e76\u8fdb\u884c\u4e86\u95ee\u9898\u7279\u5b9a\u7684\u6539\u8fdb\u548c\u53c2\u6570\u8c03\u4f18\u3002\u5728\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u95ee\u9898(JSSP)\u548c\u8d44\u6e90\u7ea6\u675f\u9879\u76ee\u8c03\u5ea6\u95ee\u9898(RCPSP)\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u589e\u5f3a\u7248FDS\u5728JSSP\u4e0a\u6bd4\u539f\u59cb\u5b9e\u73b0\u5feb1.7\u500d\uff0c\u5728RCPSP\u4e0a\u5feb2.1\u500d\uff1b\u6bd4IBM CP Optimizer 22.1\u4e2d\u7684FDS\u7b97\u6cd5\u5728JSSP\u4e0a\u5feb3.5\u500d\uff0c\u5728RCPSP\u4e0a\u5feb2.1\u500d\u3002\u5728900\u79d2\u65f6\u95f4\u9650\u5236\u5185\uff0c\u6539\u8fdb\u4e8684\u4e2aJSSP\u5b9e\u4f8b\u4e2d\u768478\u4e2a\u548c393\u4e2aRCPSP\u5b9e\u4f8b\u4e2d\u7684226\u4e2a\u7684\u6700\u4f18\u4e0b\u754c\u3002", "conclusion": "\u591a\u81c2\u8001\u864e\u673a\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0eFDS\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u8c03\u5ea6\u95ee\u9898\u7684\u6c42\u89e3\u6548\u7387\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u7ea6\u675f\u89c4\u5212\u641c\u7d22\u7b97\u6cd5\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.11692", "categories": ["eess.SP", "cs.AI", "cs.LG", "68T07, 68T05", "I.2.6; I.5.1; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.11692", "abs": "https://arxiv.org/abs/2508.11692", "authors": ["Eduardo Di Santi", "Ruixiang Ci", "Cl\u00e9ment Lefebvre", "Nenad Mijatovic", "Michele Pugnaloni", "Jonathan Brown", "Victor Mart\u00edn", "Kenza Saiah"], "title": "Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning", "comment": "Peer-reviewed conference paper. Presented at ICROMA 2025, Dresden,\n  Germany. Conference: https://tu-dresden.de/raildresden2025. Book of\n  abstracts: https://tu-dresden.de/raildresden2025/BoA.pdf. 8 pages, 6 figures,\n  1 table", "summary": "The Point Machine (PM) is a critical piece of railway equipment that switches\ntrain routes by diverting tracks through a switchblade. As with any critical\nsafety equipment, a failure will halt operations leading to service\ndisruptions; therefore, pre-emptive maintenance may avoid unnecessary\ninterruptions by detecting anomalies before they become failures. Previous work\nrelies on several inputs and crafting custom features by segmenting the signal.\nThis not only adds additional requirements for data collection and processing,\nbut it is also specific to the PM technology, the installed locations and\noperational conditions limiting scalability. Based on the available maintenance\nrecords, the main failure causes for PM are obstacles, friction, power source\nissues and misalignment. Those failures affect the energy consumption pattern\nof PMs, altering the usual (or healthy) shape of the power signal during the PM\nmovement. In contrast to the current state-of-the-art, our method requires only\none input. We apply a deep learning model to the power signal pattern to\nclassify if the PM is nominal or associated with any failure type, achieving\n>99.99\\% precision, <0.01\\% false positives and negligible false negatives. Our\nmethodology is generic and technology-agnostic, proven to be scalable on\nseveral electromechanical PM types deployed in both real-world and test bench\nenvironments. Finally, by using conformal prediction the maintainer gets a\nclear indication of the certainty of the system outputs, adding a confidence\nlayer to operations and making the method compliant with the ISO-17359\nstandard.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u9053\u5c94\u673a\u6545\u969c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u9700\u7535\u529b\u4fe1\u53f7\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6545\u969c\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u8d85\u8fc799.99%\uff0c\u5177\u6709\u6280\u672f\u65e0\u5173\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u9053\u5c94\u673a\u662f\u94c1\u8def\u5173\u952e\u5b89\u5168\u8bbe\u5907\uff0c\u6545\u969c\u4f1a\u5bfc\u81f4\u8fd0\u8425\u4e2d\u65ad\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u591a\u8f93\u5165\u548c\u5b9a\u5236\u7279\u5f81\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002\u4e3b\u8981\u6545\u969c\u539f\u56e0\uff08\u969c\u788d\u7269\u3001\u6469\u64e6\u3001\u7535\u6e90\u95ee\u9898\u3001\u9519\u4f4d\uff09\u4f1a\u5f71\u54cd\u80fd\u8017\u6a21\u5f0f\uff0c\u4e3a\u57fa\u4e8e\u7535\u529b\u4fe1\u53f7\u7684\u6545\u969c\u68c0\u6d4b\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5206\u6790\u9053\u5c94\u673a\u8fd0\u52a8\u671f\u95f4\u7684\u7535\u529b\u4fe1\u53f7\u6a21\u5f0f\uff0c\u4ec5\u9700\u5355\u4e00\u8f93\u5165\u3002\u91c7\u7528\u4fdd\u5f62\u9884\u6d4b\u4e3a\u7ef4\u62a4\u4eba\u5458\u63d0\u4f9b\u7cfb\u7edf\u8f93\u51fa\u7684\u786e\u5b9a\u6027\u6307\u793a\uff0c\u589e\u52a0\u7f6e\u4fe1\u5c42\u3002", "result": "\u5b9e\u73b0\u4e86>99.99%\u7684\u7cbe\u786e\u5ea6\uff0c<0.01%\u7684\u8bef\u62a5\u7387\u548c\u53ef\u5ffd\u7565\u7684\u6f0f\u62a5\u7387\u3002\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u548c\u6d4b\u8bd5\u53f0\u73af\u5883\u4e2d\u7684\u591a\u79cd\u673a\u7535\u9053\u5c94\u673a\u7c7b\u578b\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6280\u672f\u65e0\u5173\u3001\u53ef\u6269\u5c55\uff0c\u7b26\u5408ISO-17359\u6807\u51c6\uff0c\u80fd\u591f\u901a\u8fc7\u5355\u4e00\u7535\u529b\u4fe1\u53f7\u8f93\u5165\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u9053\u5c94\u673a\u6545\u969c\u68c0\u6d4b\uff0c\u4e3a\u9884\u9632\u6027\u7ef4\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
