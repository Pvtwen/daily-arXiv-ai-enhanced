<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 6]
- [cs.LG](#cs.LG) [Total: 101]
- [stat.ML](#stat.ML) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Optimizing ISAC MIMO Systems with Reconfigurable Pixel Antennas](https://arxiv.org/abs/2512.03319)
*Ataher Sams,Yu-Cheng Hsiao,Muhammad Talha,Besma Smida,Ashutosh Sabharwal*

Main category: eess.SP

TL;DR: 提出基于可重构像素天线的ISAC MIMO框架，通过电磁域和数字域联合波束成形优化，在满足通信速率约束下最大化感知速率，相比传统阵列显著提升性能并减少天线数量。


<details>
  <summary>Details</summary>
Motivation: 感知与通信的融合需要能灵活利用空间和电磁自由度的架构。传统方法在电磁域自由度利用不足，需要新的架构来同时优化感知和通信性能。

Method: 提出基于可重构像素天线(RPixA)的ISAC MIMO框架，通过二进制天线编码器开关网络引入电磁域自由度。采用电磁域和数字域联合波束成形架构，建立非凸联合优化问题，使用交替优化框架结合遗传算法优化像素天线端口状态，半定松弛优化数字波束成形。

Result: 数值结果表明，所提出的电磁感知设计相比传统阵列显著提高感知速率，并在等效ISAC性能下实现可观的天线数量减少。

Conclusion: 可重构像素天线有潜力实现高效、可扩展的电磁感知ISAC系统，为未来6G网络提供有前景的解决方案。

Abstract: The integration of sensing and communication demands architectures that can flexibly exploit spatial and electromagnetic (EM) degrees of freedom (DoF). This paper proposes an Integrated Sensing and Communication (ISAC) MIMO framework that uses Reconfigurable Pixel Antenna (RPixA), which introduces additional EM-domain DoF that are electronically controlled through binary antenna coder switch networks. We introduce a beamforming architecture combining this EM and digital precoding to jointly optimize Sensing and Communication. Based on full-wave simulation of pixel antenna, we formulate a non-convex joint optimization problem to maximize sensing rate under user-specific constraints on communication rate. We utilize an Alternating Optimization framework incorporating genetic algorithm for port states of Pixel antennas, and semi-definite relaxation (SDR) for digital beamforming. Numerical results demonstrate that the proposed EM-aware design achieves considerably higher sensing rate compared to conventional arrays and enables considerable antenna reduction for equivalent ISAC performance. These findings highlight the potential of reconfigurable pixel antennas to realize efficient and scalable EM-aware ISAC systems for future 6G networks.

</details>


### [2] [A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses](https://arxiv.org/abs/2512.03458)
*Maryam Maghsoudi,Mohsen Rezaeizadeh,Shihab Shamma*

Main category: eess.SP

TL;DR: 研究人员使用MEG记录训练有素的音乐家在想象和聆听音乐/诗歌刺激时的大脑活动，开发了一种CNN模型，能够将想象的神经活动转化为类似感知的响应，为脑机接口应用奠定基础。


<details>
  <summary>Details</summary>
Motivation: 解码想象言语涉及复杂的神经过程，但由于时间不确定性和想象响应数据集的有限性，难以解释。本研究旨在探索想象和感知大脑响应之间的关系，为脑机接口应用提供基础。

Method: 收集训练有素的音乐家的MEG数据，记录他们想象和聆听音乐/诗歌刺激时的大脑活动。使用滑动窗口岭回归模型在个体水平映射想象响应到感知响应，并在群体水平开发带有主体特定校准层的编码器-解码器卷积神经网络。

Result: 想象和感知的大脑响应都包含一致的、条件特定的信息。CNN模型在几乎所有保留主体上都显著优于零模型，产生预测和真实感知响应之间更高的相关性，表明想象神经活动可以转化为类似感知的响应。

Conclusion: 想象神经活动可以转化为类似感知的响应，这为涉及想象言语和音乐的脑机接口应用提供了基础。开发的CNN模型能够产生稳定且可泛化的映射，在群体水平上表现良好。

Abstract: Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.

</details>


### [3] [Resource Allocation for Pinching-Antenna Systems (PASS)-enabled NOMA Communications](https://arxiv.org/abs/2512.03502)
*Songtao Xue,Jingjing Zhao,Kaiquan Cai,Xidong Mu,Zhenyu Xiao,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出基于Pinching-天线系统(PASS)的多用户非正交多址(NOMA)框架，利用波导分割传输特性，通过联合优化功率分配、pinching波束成形和用户调度来最大化系统和速率。


<details>
  <summary>Details</summary>
Motivation: 传统固定位置天线系统在动态重构无线传播环境方面能力有限，需要新的技术来提升多用户通信性能。Pinching-天线系统(PASS)因其能够动态重构无线传播环境而成为有前景的技术，但如何将其与多用户NOMA结合以提升系统性能仍需研究。

Method: 1) 提出基于PASS的NOMA框架，利用波导分割传输特性，每个NOMA用户簇由一个专用波导服务；2) 使用pinching波束成形增强簇内性能并抑制簇间干扰；3) 建立系统和速率最大化问题，联合优化功率分配、pinching波束成形和用户调度；4) 开发两步算法：a) 使用惩罚对偶分解(PDD)算法解决联合功率分配和pinching波束成形设计，b) 开发低复杂度匹配算法解决用户调度问题。

Result: 仿真结果表明：1) 所提出的基于PASS的NOMA框架在波导分割传输结构下，相比传统固定位置天线系统和正交多址方案实现了显著的和速率增益；2) 所提出的基于匹配的用户调度算法以低计算复杂度实现了接近最优的用户-波导关联。

Conclusion: 提出的基于PASS的NOMA框架通过有效利用波导分割传输特性和pinching波束成形，显著提升了多用户通信系统的性能。两步优化算法能够有效解决复杂的联合优化问题，为动态可重构天线系统在多用户通信中的应用提供了有前景的解决方案。

Abstract: Pinching-antenna systems (PASS) have emerged as a promising technology due to their ability to dynamically reconfigure wireless propagation environments. A novel PASS-based multi-user non-orthogonal multiple access (NOMA) framework is proposed by exploiting the waveguide-division (WD) transmission characteristic. Specifically, each NOMA user cluster is served by one dedicated waveguide, and the corresponding pinching beamforming is exploited to enhance the intra-cluster performance while mitigating the inter-cluster interference. Based on this framework, a sum-rate maximization problem is formulated for jointly optimizing power allocation, pinching beamforming, and user scheduling. To solve this problem, a two-step algorithm is developed, which decomposes the original problem into two subproblems. For the joint power allocation and pinching beamforming design, a penalty dual decomposition (PDD) algorithm is proposed to obtain the locally optimal solutions. Specifically, the coupling constraints are alleviated through augmented Lagrangian relaxation, and the resulting augmented Lagrangian (AL) problem is decomposed into four subproblems, which are solved by the block coordinate descent (BCD) method. For the user scheduling, a low-complexity matching algorithm is developed to solve the user-to-waveguide assignment problem. Simulation results demonstrate that 1) the proposed PASS-based NOMA framework under the WD transmission structure achieves significant sum-rate gain over conventional fixed-position antenna systems and orthogonal multiple access (OMA) scheme; and 2) the proposed matching-based user scheduling algorithm achieves near-optimal user-waveguide association with low computational complexity.

</details>


### [4] [A Comprehensive Survey of 3GPP Release 19 ISAC Channel Modeling: From Empirical Features to Unified Methodology and Standardized Simulator](https://arxiv.org/abs/2512.03506)
*Yameng Liu,Yuxiang Zhang,Jianhua Zhang,Yuanpeng Pei,Changsheng Zhao,Shilin Luo,Lei Tian,Yingyang Li,Wei Hong,Jianming Wu,Guangyi Liu,Yan Li,Tao Jiang,Chuangxin Jiang,Junchen Liu,Yongqiang Fei,Woo-Suk Ko,Jing Xu,Bin Liang,Takahiro Tomie*

Main category: eess.SP

TL;DR: 本文系统综述了3GPP Release 19中集成感知与通信（ISAC）信道的标准化工作，提出了扩展几何随机模型（E-GBSM）并开发了标准化仿真器，为6G ISAC技术评估和标准化提供支持。


<details>
  <summary>Details</summary>
Motivation: ISAC被确定为6G关键应用，信道测量与建模是系统设计的前提。尽管3GPP Release 19已完成ISAC信道建模规范，但缺乏从经验信道特征到建模方法和标准化仿真器的系统综述。

Method: 首先分析ISAC信道研究的关键需求和挑战，系统梳理3GPP Release 19标准化流程。深入分析ISAC信道的关键方面（物理对象、目标信道、背景信道等），提出扩展几何随机模型（E-GBSM），开发标准化仿真器，并进行两阶段校准验证。

Result: 建立了统一的ISAC信道建模框架E-GBSM，开发了标准化仿真器，通过两阶段校准验证与工业参考结果高度一致，为ISAC技术评估和未来6G标准化提供了系统支持。

Conclusion: 本文系统综述了3GPP Release 19 ISAC信道标准化，为新特征表征、统一建模方法和标准化仿真器实施提供了最佳实践见解，能有效支持ISAC技术评估和未来6G标准化。

Abstract: Integrated Sensing and Communication (ISAC) has been identified as a key 6G application by ITU and 3GPP. Channel measurement and modeling is a prerequisite for ISAC system design and has attracted widespread attention from both academia and industry. 3GPP Release 19 initiated the ISAC channel study item in December 2023 and finalized its modeling specification in May 2025 after extensive technical discussions. However, a comprehensive survey that provides a systematic overview,from empirical channel features to modeling methodologies and standardized simulators,remains unavailable. In this paper, the key requirements and challenges in ISAC channel research are first analyzed, followed by a structured overview of the standardization workflow throughout the 3GPP Release 19 process. Then, critical aspects of ISAC channels, including physical objects, target channels, and background channels, are examined in depth, together with additional features such as spatial consistency, environment objects, Doppler characteristics, and shared clusters, supported by measurement-based analysis. To establish a unified ISAC channel modeling framework, an Extended Geometry-based Stochastic Model (E-GBSM) is proposed, incorporating all the aforementioned ISAC channel characteristics. Finally, a standardized simulator is developed based on E-GBSM, and a two-phase calibration procedure aligned with 3GPP Release 19 is conducted to validate both the model and the simulator, demonstrating close agreement with industrial reference results. Overall, this paper provides a systematic survey of 3GPP Release 19 ISAC channel standardization and offers insights into best practices for new feature characterization, unified modeling methodology, and standardized simulator implementation, which can effectively supporting ISAC technology evaluation and future 6G standardization.

</details>


### [5] [Scalable Pixel-based Reconfigurable Beamforming Networks for Designing Fluid Antenna Systems](https://arxiv.org/abs/2512.03703)
*Jichen Zhang,Junhui Rao,Tianqu Kang,Zhaoyang Ming,Yijun Chen,Alikhan Umirbayev,Chi-Yuk Chiu,Ross Murch*

Main category: eess.SP

TL;DR: 提出一种新型可扩展像素化可重构波束成形网络(PRBFN)，用于构建流体天线系统(FAS)，称为PRBFN-FAS，解决了传统机械可重构天线惯性大、速度慢的问题。


<details>
  <summary>Details</summary>
Motivation: 流体天线系统(FAS)是6G无线系统的有前景技术，但现有实现多依赖机械可重构天线，存在惯性大、速度慢的局限性，需要更快的电子可重构方案。

Method: 基于天线物理位置变化等效于多端口天线激励电流向量切换的洞察，设计可扩展PRBFN，通过选择所需电流向量和级联波束成形网络构建PRBFN-FAS。

Result: 设计了0.5和1.5波长等效物理移动的PRBFN-FAS实例，测量显示在所需带宽内具有良好的匹配特性和Bessel相关性，满足FAS要求，系统级实验证实了实际通信场景中的可行性。

Conclusion: PRBFN-FAS提供了一种可扩展的电子可重构解决方案，克服了机械FAS的速度限制，为6G无线系统中的流体天线技术提供了实用实现途径。

Abstract: A novel scalable pixel-based reconfigurable beamforming network (PRBFN) that can be used to form a Fluid Antenna System (FAS), referred to as a PRBFN-FAS, is introduced. The concept of FAS has emerged as an attractive new technology for use in sixth-generation (6G) wireless systems, but most implementations of FAS rely on mechanically reconfigurable antennas which are hindered by inertia, and are therefore too slow to be useful. Using the insight that changing an antenna's physical position is equivalent to switching the excitation current vector of a multi-port antenna, a novel beamformer is proposed with a scalable methodology for incorporating into FAS to form a PRBFN-FAS. Key novelties in creating the PRBFN include selecting the required current vectors and concatenating beamforming networks together to form the desired PRBFN. Two PRBFN-FAS design examples are demonstrated with FAS equivalent physical movements of 0.5 and 1.5 wavelengths. Measurements demonstrate that the PRBFN-FAS provides good matching and Bessel correlation across the desired bandwidth, satisfying FAS requirements. System-level experiments confirm the viability of PRBFN-FAS in practical communication scenarios.

</details>


### [6] [Doppler Robust Vortex Wavefront Design for Integrated Sensing and Communication](https://arxiv.org/abs/2512.03802)
*Yuan Liu,Wen-Xuan Long,M. R. Bhavani Shankar,Marco Moretti,Rui Chen,Björn Ottersten*

Main category: eess.SP

TL;DR: 提出一种多普勒鲁棒的集成感知与通信框架，采用先感知后通信模式，通过VCM-EM算法解决多普勒引起的模式间干扰，优化感知通信资源分配


<details>
  <summary>Details</summary>
Motivation: 在动态场景中，多普勒频移会同时降低感知和通信性能，特别是对波束敏感的轨道角动量波前影响更严重。现有ISAC系统在动态环境下性能受限，需要解决多普勒引起的干扰问题

Method: 1. 采用先感知后通信的ISAC框架；2. 感知阶段使用码分模式复用同时传输多个涡旋模式；3. 提出VCM-EM算法联合解码感知矩阵并估计目标参数；4. 通信阶段基于估计的CSI配置发射波束成形和接收波束导向；5. 量化感知通信资源分配权衡

Result: 仿真结果表明，所提VCM-EM算法和ISAC设计在动态场景下比基准方案获得更高的感知精度和通信频谱效率

Conclusion: 提出的多普勒鲁棒ISAC框架有效解决了动态场景中的多普勒干扰问题，通过先感知后通信的策略和VCM-EM算法，实现了高精度的多目标参数估计和高效的通信性能

Abstract: Integrated sensing and communication (ISAC) is a promising paradigm for future wireless systems due to spectrum reuse, hardware sharing, and joint waveform design. In dynamic scenes, Doppler shifts degrade both sensing and communication, which is particularly critical for beam-sensitive orbital angular momentum (OAM) wavefronts. To address this, we propose a Doppler-robust ISAC framework, which first senses and then communicates. Specifically, in the sensing phase, multiple vortex modes are simultaneously transmitted via code-division mode-multiplexing (CDMM). To solve Doppler-induced inter-mode interference, we propose a velocity-consistency matching (VCM)-expectation maximization (EM) algorithm that jointly decodes the sensing matrix and estimates range, azimuth, elevation, and velocity for multiple moving targets. In the communication phase, the joint transmitter (Tx) beamforming and receiver (Rx) beam steering are configured from the estimated channel state information (CSI). We further quantify the sensing-communication allocation trade-off by evaluating how pilot length affects estimation accuracy, beam alignment, and spectral efficiency (SE). Simulation results show that the proposed VCM-EM and ISAC designs achieve higher sensing accuracy and communication SE than baseline schemes in dynamic scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [7] [Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling](https://arxiv.org/abs/2512.03050)
*Peter Hedström,Victor Lamelas Cubero,Jón Sigurdsson,Viktor Österberg,Satish Kolli,Joakim Odqvist,Ziyong Hou,Wangzhong Mu,Viswanadh Gowtham Arigela*

Main category: cs.LG

TL;DR: 本文提出一个结合物理洞察与机器学习的计算框架，用于开发钢材的物理信息连续冷却转变（CCT）模型，该模型在4100个图表数据集上训练，能高效生成CCT图并展示良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在材料科学中已广泛应用于新化合物发现和制造过程优化，但将其应用于钢铁等复杂工业材料仍面临挑战。主要障碍在于准确捕捉化学成分、加工参数与最终微观结构和性能之间的复杂关系。

Method: 引入一个结合物理洞察与机器学习的计算框架，开发物理信息连续冷却转变（CCT）模型。该模型在4100个CCT图数据集上训练，并针对合金钢进行验证。

Result: 模型计算效率高，能在5秒内生成包含100条冷却曲线的完整CCT图。相分类F1分数在所有相上都超过88%，相变温度回归的平均绝对误差（MAE）除贝氏体（27°C）外均低于20°C。

Conclusion: 该框架可通过添加通用和定制ML模型扩展为通用的热处理数字孪生平台。与补充模拟工具和针对性实验的集成将进一步支持加速材料设计工作流程。

Abstract: Machine learning (ML) has emerged as a powerful tool for accelerating the computational design and production of materials. In materials science, ML has primarily supported large-scale discovery of novel compounds using first-principles data and digital twin applications for optimizing manufacturing processes. However, applying general-purpose ML frameworks to complex industrial materials such as steel remains a challenge. A key obstacle is accurately capturing the intricate relationship between chemical composition, processing parameters, and the resulting microstructure and properties. To address this, we introduce a computational framework that combines physical insights with ML to develop a physics-informed continuous cooling transformation (CCT) model for steels. Our model, trained on a dataset of 4,100 diagrams, is validated against literature and experimental data. It demonstrates high computational efficiency, generating complete CCT diagrams with 100 cooling curves in under 5 seconds. It also shows strong generalizability across alloy steels, achieving phase classification F1 scores above 88% for all phases. For phase transition temperature regression, it attains mean absolute errors (MAE) below 20 °C across all phases except bainite, which shows a slightly higher MAE of 27 °C. This framework can be extended with additional generic and customized ML models to establish a universal digital twin platform for heat treatment. Integration with complementary simulation tools and targeted experiments will further support accelerated materials design workflows.

</details>


### [8] [Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation](https://arxiv.org/abs/2512.03053)
*Andrew S. Cassidy,Guillaume Garreau,Jay Sivagnaname,Mike Grassi,Bernard Brezzo,John V. Arthur,Dharmendra S. Modha*

Main category: cs.LG

TL;DR: 使用LLM作为无损编码器/解码器，通过双向转换验证生成代码的正确性，减少幻觉和遗漏问题


<details>
  <summary>Details</summary>
Motivation: 解决LLM在代码生成任务中常见的幻觉和遗漏问题，提高生成代码的可靠性和准确性

Method: 提出双向转换方法：先用LLM将源域数据（如LCTs）无损编码到目标域（如HDL代码），再用LLM将生成的代码解码回源域，通过比较原始和重建的源数据来验证正确性

Result: 成功生成了二维网络芯片路由器的完整HDL代码（13个单元，1500-2000行代码），通过双向验证显著提高了生产力，不仅能确认正确生成的逻辑，还能检测错误生成的逻辑，并帮助开发者发现设计规范错误

Conclusion: 基于信息论无损压缩思想的LLM双向转换方法能有效缓解LLM的幻觉和遗漏问题，在可逆问题中实现可靠的代码生成和验证

Abstract: We show for invertible problems that transform data from a source domain (for example, Logic Condition Tables (LCTs)) to a destination domain (for example, Hardware Description Language (HDL) code), an approach of using Large Language Models (LLMs) as a lossless encoder from source to destination followed by as a lossless decoder back to the source, comparable to lossless compression in information theory, can mitigate most of the LLM drawbacks of hallucinations and omissions. Specifically, using LCTs as inputs, we generate the full HDL for a two-dimensional network-on-chip router (13 units, 1500-2000 lines of code) using seven different LLMs, reconstruct the LCTs from the auto-generated HDL, and compare the original and reconstructed LCTs. This approach yields significant productivity improvements, not only confirming correctly generated LLM logic and detecting incorrectly generated LLM logic but also assisting developers in finding design specification errors.

</details>


### [9] [Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research](https://arxiv.org/abs/2512.03054)
*Ciro Benito Raggio,Lucia Migliorelli,Nils Skupien,Mathias Krohmer Zabaleta,Oliver Blanck,Francesco Cicone,Giuseppe Lucio Cascini,Paolo Zaffino,Maria Francesca Spadea*

Main category: cs.LG

TL;DR: 提出一种面向绿色AI的自适应层冻结策略，用于联邦学习中的MRI-to-CT转换任务，在保持模型性能的同时显著降低能耗和碳排放


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然能促进医疗平等，但其高资源需求会排除计算基础设施有限的机构，加剧医疗不平等。需要开发节能的联邦学习方法，确保气候、社会和经济的可持续性

Method: 提出自适应层冻结策略：基于轮次间编码器权重的相对差异选择性冻结编码器权重，采用基于耐心的机制确保仅在更新持续较小时才进行冻结。使用CodeCarbon库跟踪能耗和碳排放

Result: 相比非冻结方法，训练时间、总能耗和CO2eq排放降低高达23%。MRI-to-CT转换性能保持稳定，MAE仅有小幅变化。5种架构中3种无显著差异，2种有显著改进

Conclusion: 该工作为满足临床需求同时确保气候、社会和经济可持续性的DL框架提供了研究范式，为推进AI医疗中的隐私、公平和正义奠定了基础

Abstract: Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.

</details>


### [10] [Globally optimized SVD compression of LLMs via Fermi-function-based rank selection and gauge fixing](https://arxiv.org/abs/2512.03062)
*Roman Rausch,David Jansen,Sukhbinder Singh,Román Orús*

Main category: cs.LG

TL;DR: 提出两种基于物理启发的SVD LLM压缩改进方法：FermiGrad算法通过费米函数将离散奇异值截断松弛为连续优化来确定全局最优层秩；PivGa利用参数化中的规范自由度对低秩因子进行无损压缩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对计算资源需求极高，低秩分解（如SVD）是LLM压缩的有前景方法，但面临层秩选择和参数冗余等实际挑战。

Method: 1. FermiGrad：基于梯度下降算法，使用费米函数将离散奇异值截断松弛为连续优化，确定全局最优层秩。2. PivGa：利用低秩因子参数化中的规范自由度，对低秩因子进行额外的无损压缩。

Result: 论文提出了两种改进SVD LLM压缩的物理启发方法，解决了传统方法中的层秩选择困难和参数冗余问题。

Conclusion: 通过FermiGrad和PivGa两种方法，显著改进了基于SVD的LLM压缩效果，为高效LLM部署提供了新思路。

Abstract: Large Language Models (LLMs) are very demanding in terms of their computational resources. Low-rank decompositions of LLM weights, e.g. via Singular Value Decomposition (SVD), is a promising approach for LLM compression, but presents several practical hurdles, e.g. selecting appropriate layer-wise ranks and getting rid of its parameter redundancy. In this work, we present two physics-inspired improvements to SVD LLM compression: (1) \textbf{FermiGrad}, a gradient-descent algorithm that determines globally optimal layer-wise ranks by relaxing the discrete singular-value truncation into a continuous optimization using the Fermi function; (2) \textbf{PivGa}, an additional \textit{lossless} compression of the low-rank factors that exploits the intrinsic gauge freedom in their parametrization.

</details>


### [11] [Physics-informed self-supervised learning for predictive modeling of coronary artery digital twins](https://arxiv.org/abs/2512.03055)
*Xiaowu Sun,Thabo Mahendiran,Ortal Senouf,Denise Auberson,Bernard De Bruyne,Stephane Fournier,Olivier Muller,Pascal Frossard,Emmanuel Abbe,Dorina Thanou*

Main category: cs.LG

TL;DR: PINS-CAD：基于物理信息自监督学习的框架，通过预训练图神经网络在合成冠状动脉数字孪生上预测血流压力，无需CFD或标记数据，在临床数据上微调后可预测心血管事件，优于传统风险评分。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，冠状动脉疾病（CAD）是最常见形式，需要早期风险预测。传统基于3D冠状动脉数字孪生的分析依赖计算流体动力学（CFD），计算量大且难以扩展；数据驱动方法受限于标记数据稀缺且缺乏生理学先验知识。

Method: 提出PINS-CAD框架：1）在20万个合成冠状动脉数字孪生上预训练图神经网络，预测压力和血流；2）使用1D Navier-Stokes方程和压力降定律作为物理约束指导；3）无需CFD或标记数据；4）在FAME2研究的635名患者临床数据上微调。

Result: 1）预测未来心血管事件的AUC达到0.73，优于临床风险评分和数据驱动基线；2）物理信息预训练提高了样本效率并产生生理学有意义的表示；3）生成空间分辨的压力和血流储备分数曲线，提供可解释的生物标志物。

Conclusion: 通过将物理先验嵌入几何深度学习，PINS-CAD将常规血管造影转化为无需仿真、具有生理感知的框架，为可扩展的预防性心脏病学提供了新途径。

Abstract: Cardiovascular disease is the leading global cause of mortality, with coronary artery disease (CAD) as its most prevalent form, necessitating early risk prediction. While 3D coronary artery digital twins reconstructed from imaging offer detailed anatomy for personalized assessment, their analysis relies on computationally intensive computational fluid dynamics (CFD), limiting scalability. Data-driven approaches are hindered by scarce labeled data and lack of physiological priors. To address this, we present PINS-CAD, a physics-informed self-supervised learning framework. It pre-trains graph neural networks on 200,000 synthetic coronary digital twins to predict pressure and flow, guided by 1D Navier-Stokes equations and pressure-drop laws, eliminating the need for CFD or labeled data. When fine-tuned on clinical data from 635 patients in the multicenter FAME2 study, PINS-CAD predicts future cardiovascular events with an AUC of 0.73, outperforming clinical risk scores and data-driven baselines. This demonstrates that physics-informed pretraining boosts sample efficiency and yields physiologically meaningful representations. Furthermore, PINS-CAD generates spatially resolved pressure and fractional flow reserve curves, providing interpretable biomarkers. By embedding physical priors into geometric deep learning, PINS-CAD transforms routine angiography into a simulation-free, physiology-aware framework for scalable, preventive cardiology.

</details>


### [12] [Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%](https://arxiv.org/abs/2512.03107)
*Mainak Singha*

Main category: cs.LG

TL;DR: ECLIPSE框架通过结合语义熵估计和困惑度分解来检测LLM幻觉，将幻觉视为模型语义熵与可用证据容量之间的不匹配，在金融QA数据集上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型会产生流畅但无根据的幻觉回答，这限制了其在高风险领域的部署。需要一种能够检测幻觉的机制。

Method: 提出ECLIPSE框架，将幻觉视为模型语义熵与可用证据容量之间的不匹配。结合多样本聚类的熵估计和新的困惑度分解方法，测量模型如何使用检索到的证据。

Result: 在金融QA数据集上，ECLIPSE达到ROC AUC 0.89和平均精度0.90，显著优于仅使用语义熵的基线（AUC 0.50）。实验证明ECLIPSE是一个依赖于校准的token级不确定性的logprob原生机制。

Conclusion: ECLIPSE通过结合语义熵和证据利用测量，有效检测LLM幻觉。困惑度分解特征具有最大的学习系数，证实证据利用是幻觉检测的核心。该研究是一个受控机制研究，需要在更多领域和自然幻觉上进行验证。

Abstract: Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.

</details>


### [13] [Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models](https://arxiv.org/abs/2512.03056)
*Zhidong Gao,Zimeng Pan,Yuhang Yao,Chenyue Xie,Wei Wei*

Main category: cs.LG

TL;DR: Delta Sampling (DS) 是一种无需原始训练数据、在推理时实现跨架构扩散模型知识迁移的方法，通过利用模型适应前后的预测差异来指导新基础模型的去噪过程。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型生态系统中的参数高效适配器（如LoRA、LyCORIS、ControlNet）与特定基础模型紧密耦合，当基础模型升级时（如从SD 1.x到2.x），由于模型参数和架构的显著变化，这些适配组件难以重用。

Method: Delta Sampling (DS) 完全在推理时操作，通过利用"delta"（模型适应前后的预测差异）来指导新基础模型的去噪过程。该方法不需要访问原始训练数据，能够实现跨不同架构基础模型的知识迁移。

Result: DS在不同Stable Diffusion版本上进行了评估，在各种采样策略下都能一致地改善期望效果（如视觉风格、语义概念和结构）的创建，证明了其作为即插即用知识迁移机制的有效性。

Conclusion: Delta Sampling 为扩散模型生态系统提供了一种有效的、即插即用的知识迁移机制，解决了适配组件与基础模型紧密耦合的问题，使得知识能够在不同架构的模型间有效转移。

Abstract: Diffusion models like Stable Diffusion (SD) drive a vibrant open-source ecosystem including fully fine-tuned checkpoints and parameter-efficient adapters such as LoRA, LyCORIS, and ControlNet. However, these adaptation components are tightly coupled to a specific base model, making them difficult to reuse when the base model is upgraded (e.g., from SD 1.x to 2.x) due to substantial changes in model parameters and architecture. In this work, we propose Delta Sampling (DS), a novel method that enables knowledge transfer across base models with different architectures, without requiring access to the original training data. DS operates entirely at inference time by leveraging the delta: the difference in model predictions before and after the adaptation of a base model. This delta is then used to guide the denoising process of a new base model. We evaluate DS across various SD versions, demonstrating that DS achieves consistent improvements in creating desired effects (e.g., visual styles, semantic concepts, and structures) under different sampling strategies. These results highlight DS as an effective, plug-and-play mechanism for knowledge transfer in diffusion-based image synthesis. Code:~ https://github.com/Zhidong-Gao/DeltaSampling

</details>


### [14] [E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing](https://arxiv.org/abs/2512.03109)
*Shuvom Sadhuka,Drew Prinster,Clara Fannjiang,Gabriele Scalia,Aviv Regev,Hanchen Wang*

Main category: cs.LG

TL;DR: 提出e-valuator方法，将任意黑盒验证器分数转换为具有可证明误报率控制的决策规则，用于评估智能体轨迹成功率。


<details>
  <summary>Details</summary>
Motivation: 现有智能体验证器（如LLM法官和过程奖励模型）使用启发式评分评估智能体轨迹质量，但这些评分缺乏正确性保证，无法可靠判断智能体是否会产生成功输出。

Method: 将成功与失败轨迹区分问题构建为序列假设检验问题，基于e-process工具开发序列假设检验方法，在智能体轨迹的每一步保持统计有效性，支持在线监控任意长动作序列。

Result: 在六个数据集和三种智能体上，e-valuator相比其他策略提供更强的统计功效和更好的误报率控制，并能快速终止问题轨迹以节省token使用。

Conclusion: e-valuator提供轻量级、模型无关的框架，将验证器启发式方法转换为具有统计保证的决策规则，实现更可靠的智能体系统部署。

Abstract: Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.

</details>


### [15] [Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding](https://arxiv.org/abs/2512.03058)
*Duy-Tung Pham,An The Nguyen,Viet-Hoang Tran,Nhan-Phu Chung,Xin T. Tong,Tan M. Nguyen,Thieu N. Vo*

Main category: cs.LG

TL;DR: 研究预训练Transformer模型中token的动态特性，分析其连续时间极限下的动力系统，提出改进Transformer架构的简单方法。


<details>
  <summary>Details</summary>
Motivation: 探索预训练Transformer模型中token的动态行为，理解token随时间相互靠近或远离的机制，以及不同位置编码如何影响这些动态模式，为改进Transformer模型提供理论基础。

Method: 分析预训练模型的连续时间极限动力系统，表征解的渐近行为；基于模型参数提供token收敛到零或发散到无穷的充分条件；研究绝对位置编码和旋转位置编码对动态模式的影响；提出减轻收敛行为的Transformer架构改进方法。

Result: 建立了比先前工作更广泛适用的条件来识别token收敛或发散的情景；发现收敛情景会损害模型性能；针对绝对和旋转位置编码提出了能减轻收敛行为的简单架构改进。

Conclusion: 对Transformer模型中token动态特性的分析为改进模型提供了理论基础和设计原则，提出的简单架构改进能有效减轻收敛行为对性能的不利影响。

Abstract: This paper investigates the dynamical properties of tokens in pre-trained Transformer models and explores their application to improving Transformers. To this end, we analyze the dynamical system governing the continuous-time limit of the pre-trained model and characterize the asymptotic behavior of its solutions. Specifically, we characterize when tokens move closer to or farther from one another over time, depending on the model parameters. We provide sufficient conditions, based on these parameters, to identify scenarios where tokens either converge to zero or diverge to infinity. Unlike prior works, our conditions are broader in scope and more applicable to real-world models. Furthermore, we investigate how different forms of positional encoding -- specifically absolute and rotary -- affect these dynamical regimes. Empirical evidence reveals that the convergence scenario adversely impacts model performance. Motivated by these insights, we propose simple refinements to Transformer architectures that mitigate convergence behavior in models with absolute or rotary positional encoding. These findings support theoretical foundations and design principles for improving Transformer models.

</details>


### [16] [Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability](https://arxiv.org/abs/2512.03112)
*Jialai She*

Main category: cs.LG

TL;DR: SISR框架统一解决Shapley值的两个核心问题：通过单调变换恢复可加性，同时施加L0稀疏约束提高高维计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统Shapley值假设可加性，但现实中的收益函数常因非高斯分布、重尾、特征依赖或领域特定损失尺度而违反此假设，导致归因失真。同时，在高维空间中计算密集Shapley值后采用临时阈值处理既昂贵又不一致。

Method: 提出稀疏单调Shapley回归(SISR)：同时学习单调变换以恢复可加性（无需封闭形式指定），并对Shapley向量施加L0稀疏约束。优化算法利用相邻池化违规者进行高效单调回归，以及归一化硬阈值进行支持选择。

Result: SISR能在多种场景下恢复真实变换，即使在高噪声下也能实现强支持恢复。实验表明，SISR能稳定不同收益方案下的归因，正确过滤无关特征，而标准Shapley值则遭受严重的排名和符号失真。

Conclusion: 通过统一非线性变换估计与稀疏性追求，SISR推进了非线性可解释性的前沿，提供了理论扎实且实用的归因框架，首次证明无关特征和特征间依赖可导致真实收益变换显著偏离线性。

Abstract: Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.

</details>


### [17] [Deep Unfolding: Recent Developments, Theory, and Design Guidelines](https://arxiv.org/abs/2512.03768)
*Nir Shlezinger,Santiago Segarra,Yi Zhang,Dvir Avrahami,Zohar Davidov,Tirza Routtenberg,Yonina C. Eldar*

Main category: cs.LG

TL;DR: 深度展开（Deep Unfolding）是一种将迭代优化算法转化为结构化可训练机器学习架构的框架，它结合了传统优化的理论保证与机器学习的数据驱动能力。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法在信号处理中具有理论保证和可解释性，但通常需要代理目标函数、精心调参且计算延迟高；而机器学习虽然数据驱动能力强，但缺乏优化驱动推理所需的结构、透明度和效率。深度展开旨在桥接这两个范式。

Method: 深度展开通过系统地将迭代优化算法转化为结构化、可训练的机器学习架构。文章介绍了四种代表性的设计范式，讨论了从迭代特性衍生的独特训练方案，并提供了将优化求解器转换为机器学习模型的统一方法论视角。

Result: 文章综述了深度展开的理论进展，包括收敛性和泛化性保证，并提供了比较性定性和实证研究，展示了在复杂度、可解释性和鲁棒性方面的权衡。

Conclusion: 深度展开是一个有前景的框架，它结合了传统优化的理论严谨性和机器学习的数据驱动能力，为优化驱动的推理提供了新的方法论，并在复杂度、可解释性和鲁棒性之间提供了有价值的权衡。

Abstract: Optimization methods play a central role in signal processing, serving as the mathematical foundation for inference, estimation, and control. While classical iterative optimization algorithms provide interpretability and theoretical guarantees, they often rely on surrogate objectives, require careful hyperparameter tuning, and exhibit substantial computational latency. Conversely, machine learning (ML ) offers powerful data-driven modeling capabilities but lacks the structure, transparency, and efficiency needed for optimization-driven inference. Deep unfolding has recently emerged as a compelling framework that bridges these two paradigms by systematically transforming iterative optimization algorithms into structured, trainable ML architectures. This article provides a tutorial-style overview of deep unfolding, presenting a unified perspective of methodologies for converting optimization solvers into ML models and highlighting their conceptual, theoretical, and practical implications. We review the foundations of optimization for inference and for learning, introduce four representative design paradigms for deep unfolding, and discuss the distinctive training schemes that arise from their iterative nature. Furthermore, we survey recent theoretical advances that establish convergence and generalization guarantees for unfolded optimizers, and provide comparative qualitative and empirical studies illustrating their relative trade-offs in complexity, interpretability, and robustness.

</details>


### [18] [Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL](https://arxiv.org/abs/2512.03059)
*Jiaju Qi,Lei Lei,Thorsteinn Jonsson,Dusit Niyato*

Main category: cs.LG

TL;DR: 提出一种安全的分层深度强化学习框架，用于解决多源不确定性下的电动公交车充电调度问题，通过双层优化实现成本最小化和安全约束满足。


<details>
  <summary>Details</summary>
Motivation: 电动公交车与光伏等可再生能源结合是实现低碳公共交通的重要途径，但在实际运营中面临光伏发电不确定性、动态电价、可变行驶时间和有限充电设施等多重挑战，需要开发能够同时优化成本并确保安全运行的调度方法。

Method: 将问题建模为带约束的马尔可夫决策过程，提出DAC-MAPPO-Lagrangian分层深度强化学习算法。高层采用集中式PPO-Lagrangian学习安全的充电桩分配策略，低层采用MAPPO-Lagrangian在CTDE范式下学习分散式充电功率决策。

Result: 基于真实数据的实验表明，该方法在成本最小化和安全合规性方面优于现有基线方法，同时保持了较快的收敛速度。

Conclusion: 提出的安全分层深度强化学习框架能够有效解决多源不确定性下的电动公交车充电调度问题，为可持续公共交通系统提供了实用的优化解决方案。

Abstract: The integration of Electric Buses (EBs) with renewable energy sources such as photovoltaic (PV) panels is a promising approach to promote sustainable and low-carbon public transportation. However, optimizing EB charging schedules to minimize operational costs while ensuring safe operation without battery depletion remains challenging - especially under real-world conditions, where uncertainties in PV generation, dynamic electricity prices, variable travel times, and limited charging infrastructure must be accounted for. In this paper, we propose a safe Hierarchical Deep Reinforcement Learning (HDRL) framework for solving the EB Charging Scheduling Problem (EBCSP) under multi-source uncertainties. We formulate the problem as a Constrained Markov Decision Process (CMDP) with options to enable temporally abstract decision-making. We develop a novel HDRL algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization Lagrangian (DAC-MAPPO-Lagrangian), which integrates Lagrangian relaxation into the Double Actor-Critic (DAC) framework. At the high level, we adopt a centralized PPO-Lagrangian algorithm to learn safe charger allocation policies. At the low level, we incorporate MAPPO-Lagrangian to learn decentralized charging power decisions under the Centralized Training and Decentralized Execution (CTDE) paradigm. Extensive experiments with real-world data demonstrate that the proposed approach outperforms existing baselines in both cost minimization and safety compliance, while maintaining fast convergence speed.

</details>


### [19] [Single-Round Scalable Analytic Federated Learning](https://arxiv.org/abs/2512.03336)
*Alan T. L. Bacellar,Mustafa Munir,Felipe M. G. França,Priscila M. V. Lima,Radu Marculescu,Lizy K. John*

Main category: cs.LG

TL;DR: SAFLe框架通过引入结构化头部和稀疏分组嵌入，实现非线性表达能力，同时保持单轮联邦学习的优势，在联邦视觉任务中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临通信开销高和异构数据性能下降两大挑战。分析式联邦学习（AFL）提供单轮、数据分布不变的解决方案，但仅限于线性模型。后续的非线性方法如DeepAFL恢复了准确性，但牺牲了单轮优势。需要打破这种权衡。

Method: 提出SAFLe框架，通过引入结构化头部（分桶特征）和稀疏分组嵌入实现可扩展的非线性表达能力。证明这种非线性架构在数学上等价于高维线性回归，从而能够使用AFL的单轮不变聚合法则进行求解。

Result: SAFLe在分析式联邦学习中建立了新的最先进水平，在所有基准测试中显著优于线性AFL和多轮DeepAFL的准确性，为联邦视觉提供了高效可扩展的解决方案。

Conclusion: SAFLe成功打破了联邦学习中非线性表达能力与单轮通信之间的权衡，通过数学等价性将非线性模型转化为可单轮求解的线性问题，实现了高效且性能优越的联邦学习框架。

Abstract: Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.

</details>


### [20] [A Large Scale Heterogeneous Treatment Effect Estimation Framework and Its Applications of Users' Journey at Snap](https://arxiv.org/abs/2512.03060)
*Jing Pan,Li Shi,Paul Lo*

Main category: cs.LG

TL;DR: 大规模工业框架利用数百个实验数据估计异质处理效应，通过组合多个实验揭示潜在用户特征，在广告影响力和敏感性应用中取得显著效果


<details>
  <summary>Details</summary>
Motivation: 传统方法假设所有用户的处理效应相同，但实际中不同用户对干预的反应存在差异。需要开发能够在大规模工业环境中估计异质处理效应的框架，以更精准地理解用户特征和优化商业决策。

Method: 构建大规模工业框架，整合数百个实验的数亿用户数据，包括实验选择、基础学习器设计和增量训练等核心组件。通过组合多个实验结果来揭示潜在用户特征，并生成稳定的处理效应估计。

Result: 框架成功应用于两个场景：用户对广告的影响力和用户对广告的敏感性。使用影响力分数进行定向的在线A/B测试显示，关键业务指标的提升效果超过通常认为显著水平的六倍以上。

Conclusion: 该大规模工业框架能够有效估计异质处理效应，通过整合多个实验数据揭示先前无法测量的用户潜在特征，为精准营销和个性化干预提供了强有力的工具，在实际应用中取得了显著的业务效果提升。

Abstract: Heterogeneous Treatment Effect (HTE) and Conditional Average Treatment Effect (CATE) models relax the assumption that treatment effects are the same for every user. We present a large scale industrial framework for estimating HTE using experimental data from hundreds of millions of Snapchat users. By combining results across many experiments, the framework uncovers latent user characteristics that were previously unmeasurable and produces stable treatment effect estimates at scale.
  We describe the core components that enabled this system, including experiment selection, base learner design, and incremental training. We also highlight two applications: user influenceability to ads and user sensitivity to ads. An online A/B test using influenceability scores for targeting showed an improvement on key business metrics that is more than six times larger than what is typically considered significant.

</details>


### [21] [Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions](https://arxiv.org/abs/2512.03354)
*Hongseon Yeom,Jaeyoul Shin,Soojin Min,Jeongmin Yoon,Seunghak Yu,Dongyeop Kang*

Main category: cs.LG

TL;DR: 提出首个用于确定性广告拍卖的离策略评估框架，通过重新利用出价景观模型近似倾向得分，实现稳定评估，在工业平台测试中达到92%的平均方向准确率。


<details>
  <summary>Details</summary>
Motivation: 在线A/B测试消耗大量工程资源且存在收入损失风险，而传统离策略评估方法不适用于确定性拍卖环境（最高出价者获胜，非获胜广告曝光概率为零）。

Method: 重新利用出价景观模型近似倾向得分，推导稳健的近似倾向分数，使自归一化逆倾向评分等稳定估计器能在确定性拍卖环境中使用。

Result: 在AuctionNet模拟基准和工业平台2周在线A/B测试中验证，CTR预测达到92%的平均方向准确率，显著优于参数基线，与在线结果高度一致。

Conclusion: 提出了首个实用且经过验证的确定性拍卖环境离策略评估框架，为昂贵且有风险的在线实验提供了高效替代方案。

Abstract: Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.

</details>


### [22] [Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization](https://arxiv.org/abs/2512.03393)
*Lakshmi Jayalal,Sheetal Kalyani*

Main category: cs.LG

TL;DR: 提出一种基于隐式正则化的免调参框架，用于多测量向量（MMV）中的联合稀疏信号恢复，无需先验稀疏度或噪声方差信息。


<details>
  <summary>Details</summary>
Motivation: 传统MMV方法（如M-OMP、M-FOCUSS）需要仔细调参或先验知识（信号稀疏度、噪声方差），限制了实际应用。需要一种免调参的鲁棒方法。

Method: 采用隐式正则化（IR）框架，通过过参数化将估计矩阵重参数化为因子，分离共享行支持与个体向量条目。对标准最小二乘目标应用梯度下降，优化动态自然促进行稀疏结构。

Result: 理论证明：在足够小且平衡的初始化下，优化动态呈现"动量效应"，真实支持中的行范数增长远快于其他行，保证解轨迹收敛到理想行稀疏解。实证结果：性能与现有方法相当，无需先验信息或调参。

Conclusion: 提出的隐式正则化框架为MMV问题提供了一种免调参解决方案，通过过参数化和梯度下降的优化动态自然实现行稀疏恢复，无需先验知识，具有实际应用价值。

Abstract: Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a "momentum-like" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.

</details>


### [23] [Optimizing Life Sciences Agents in Real-Time using Reinforcement Learning](https://arxiv.org/abs/2512.03065)
*Nihir Chadderwala*

Main category: cs.LG

TL;DR: 提出结合AWS Strands Agents与Thompson Sampling上下文bandits的框架，让AI代理仅从用户反馈中学习最优决策策略，在生命科学查询中提升15-30%用户满意度。


<details>
  <summary>Details</summary>
Motivation: 生命科学中的生成式AI代理面临关键挑战：如何为从简单事实问题到复杂机制推理的多样化查询确定最优方法。传统方法依赖固定规则或昂贵的标注训练数据，都无法适应变化条件或用户偏好。

Method: 结合AWS Strands Agents与Thompson Sampling上下文bandits的框架，优化三个关键维度：生成策略选择（直接vs.思维链）、工具选择（文献搜索、药物数据库等）和领域路由（药理学、分子生物学、临床专家）。系统仅从用户反馈中学习，无需标注数据。

Result: 在生命科学查询的实证评估中，相比随机基线，用户满意度提升15-30%。经过20-30个查询后出现清晰的学习模式。方法无需真实标注，能持续适应用户偏好。

Conclusion: 该方法为代理式AI系统中的探索-利用困境提供了原则性解决方案，无需标注数据，能持续适应用户偏好，在生命科学AI代理中实现显著性能提升。

Abstract: Generative AI agents in life sciences face a critical challenge: determining the optimal approach for diverse queries ranging from simple factoid questions to complex mechanistic reasoning. Traditional methods rely on fixed rules or expensive labeled training data, neither of which adapts to changing conditions or user preferences. We present a novel framework that combines AWS Strands Agents with Thompson Sampling contextual bandits to enable AI agents to learn optimal decision-making strategies from user feedback alone. Our system optimizes three key dimensions: generation strategy selection (direct vs. chain-of-thought), tool selection (literature search, drug databases, etc.), and domain routing (pharmacology, molecular biology, clinical specialists). Through empirical evaluation on life science queries, we demonstrate 15-30\% improvement in user satisfaction compared to random baselines, with clear learning patterns emerging after 20-30 queries. Our approach requires no ground truth labels, adapts continuously to user preferences, and provides a principled solution to the exploration-exploitation dilemma in agentic AI systems.

</details>


### [24] [GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test](https://arxiv.org/abs/2512.03428)
*Ziyi Ding,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出GaussDetect-LiNGAM，一种新的双变量因果发现方法，通过利用前向模型噪声的高斯性与反向模型残差独立性之间的等价关系，消除了显式高斯性检验的需求。


<details>
  <summary>Details</summary>
Motivation: 传统LiNGAM方法依赖脆弱且对样本敏感的高斯性检验，这限制了其在实际应用中的可靠性和可访问性。需要一种更稳健的方法来替代这些检验。

Method: 基于理论证明：在线性、无环性和外生性的标准LiNGAM假设下，前向模型噪声的高斯性等价于反向模型中回归变量与残差的独立性。利用这一等价关系，用稳健的核基独立性检验替代高斯性检验。

Result: 实验验证了所提出的等价关系，并证明GaussDetect-LiNGAM在不同噪声类型和样本量下保持高一致性，同时减少了每个决策所需的检验次数（TPD）。

Conclusion: 该方法提高了因果推断的效率和实际适用性，使LiNGAM在现实场景中更加可访问和可靠。

Abstract: We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.

</details>


### [25] [Hierarchical clustering of complex energy systems using pretopology](https://arxiv.org/abs/2512.03069)
*Loup-Noe Levy,Jeremie Bosom,Guillaume Guerard,Soufian Ben Amor,Marc Bui,Hai Tran*

Main category: cs.LG

TL;DR: 使用预拓扑学建模建筑能耗分布，开发多准则层次分类算法，通过Python库实现，在合成数据和真实能耗数据上验证效果良好。


<details>
  <summary>Details</summary>
Motivation: 大规模分布式建筑能耗管理需要自动化方法，因为逐个深入审计数千栋建筑耗时耗力且需要大量专业人员。

Method: 采用预拓扑学建模站点能耗分布，开发基于预拓扑空间特性的多准则层次分类算法，并实现为Python库。

Result: 在合成点数据集上能识别空间位置和尺寸的聚类；在合成时间序列上使用皮尔逊相关系数获得ARI=1的完美聚类；在400个真实能耗站点数据上验证有效。

Conclusion: 提出的预拓扑建模和多准则层次分类方法能够有效识别能耗分布模式，为大规模建筑能耗优化管理提供自动化解决方案。

Abstract: This article attempts answering the following problematic: How to model and classify energy consumption profiles over a large distributed territory to optimize the management of buildings' consumption?
  Doing case-by-case in depth auditing of thousands of buildings would require a massive amount of time and money as well as a significant number of qualified people. Thus, an automated method must be developed to establish a relevant and effective recommendations system.
  To answer this problematic, pretopology is used to model the sites' consumption profiles and a multi-criterion hierarchical classification algorithm, using the properties of pretopological space, has been developed in a Python library.
  To evaluate the results, three data sets are used: A generated set of dots of various sizes in a 2D space, a generated set of time series and a set of consumption time series of 400 real consumption sites from a French Energy company.
  On the point data set, the algorithm is able to identify the clusters of points using their position in space and their size as parameter. On the generated time series, the algorithm is able to identify the time series clusters using Pearson's correlation with an Adjusted Rand Index (ARI) of 1.

</details>


### [26] [Parameter-Efficient Augment Plugin for Class-Incremental Learning](https://arxiv.org/abs/2512.03537)
*Zhiming Xu,Baile Xu,Jian Zhao,Furao Shen,Suorong Yang*

Main category: cs.LG

TL;DR: 提出DLC方法，通过LoRA插件扩展范式解决类增量学习中的遗忘问题，仅需少量参数即可显著提升准确率


<details>
  <summary>Details</summary>
Motivation: 现有类增量学习方法存在遗忘问题或稳定性-可塑性困境，扩展方法虽然准确率高但参数增加显著。需要一种高效且参数友好的解决方案。

Method: 提出DLC方法：将基于重放或蒸馏训练的特征提取器作为基础模型，为每个任务使用LoRA注入任务特定残差，推理时聚合这些表示，并引入轻量级加权单元减少非目标插件的干扰。

Result: 在ImageNet-100上，仅用ResNet-18 4%的参数就实现了8%的准确率提升，在固定内存预算下超越现有最优方法。

Conclusion: DLC方法作为一种即插即用的增强方案，能够高效扩展基础方法，在类增量学习中实现高准确率和参数效率的平衡。

Abstract: Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.

</details>


### [27] [Mixed Data Clustering Survey and Challenges](https://arxiv.org/abs/2512.03070)
*Guillaume Guerard,Sonia Djebali*

Main category: cs.LG

TL;DR: 提出一种基于预拓扑空间的混合数据聚类方法，用于处理大数据环境下的数值和分类变量混合数据集


<details>
  <summary>Details</summary>
Motivation: 大数据时代带来了前所未有的数据量、速度和多样性，混合数据聚类成为关键挑战。传统聚类方法通常针对同质数据集设计，难以有效处理数值和分类变量混合的复杂数据，需要专门针对这种场景的方法。

Method: 提出一种基于预拓扑空间的聚类方法，该方法能够有效处理混合数据类型（数值和分类变量），并提供层次化和可解释的聚类结果。

Result: 通过与经典数值聚类算法和现有预拓扑方法进行基准测试，评估了所提出方法的性能和有效性，验证了其在大数据范式下的适用性。

Conclusion: 基于预拓扑空间的聚类方法为解决大数据环境下的混合数据聚类问题提供了有效途径，能够提供结构化、可解释的聚类结果，支持明智的决策制定。

Abstract: The advent of the big data paradigm has transformed how industries manage and analyze information, ushering in an era of unprecedented data volume, velocity, and variety. Within this landscape, mixed-data clustering has become a critical challenge, requiring innovative methods that can effectively exploit heterogeneous data types, including numerical and categorical variables. Traditional clustering techniques, typically designed for homogeneous datasets, often struggle to capture the additional complexity introduced by mixed data, underscoring the need for approaches specifically tailored to this setting. Hierarchical and explainable algorithms are particularly valuable in this context, as they provide structured, interpretable clustering results that support informed decision-making. This paper introduces a clustering method grounded in pretopological spaces. In addition, benchmarking against classical numerical clustering algorithms and existing pretopological approaches yields insights into the performance and effectiveness of the proposed method within the big data paradigm.

</details>


### [28] [Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction](https://arxiv.org/abs/2512.03899)
*Janis Keck,Lukas Silvester Barth,Fatemeh,Fahimi,Parvaneh Joharinad,Jürgen Jost*

Main category: cs.LG

TL;DR: 该论文为模糊单纯集提供了一个概率框架，将其解释为单纯集上概率测度的边际分布，从而为UMAP等降维方法建立了统一的理论基础。


<details>
  <summary>Details</summary>
Motivation: 模糊单纯集在降维和流形学习中很重要（如UMAP），但其代数拓扑定义缺乏清晰的概率解释，脱离了这些领域常用的理论框架。

Method: 引入一个框架，将模糊单纯集解释为单纯集上概率测度的边际分布。具体展示了UMAP的模糊权重源于一个生成模型，该模型在随机尺度上采样Vietoris-Rips滤过，产生成对距离的累积分布函数。

Result: 该框架将模糊单纯集连接到面偏序集上的概率模型，澄清了Kullback-Leibler散度与模糊交叉熵的关系，并通过底层单纯集的布尔运算恢复了标准t-范数和t-余范数。还展示了如何从这个框架推导新的嵌入方法。

Conclusion: 这个概率视角为模糊单纯集提供了统一的概率理论基础，阐明了UMAP在该框架中的作用，并能够系统性地推导新的降维方法。

Abstract: Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using Čech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.

</details>


### [29] [PretopoMD: Pretopology-based Mixed Data Hierarchical Clustering](https://arxiv.org/abs/2512.03071)
*Loup-Noe Levy,Guillaume Guerard,Sonia Djebali,Soufian Ben Amor*

Main category: cs.LG

TL;DR: 提出一种基于预拓扑的新算法，用于混合数据聚类，无需降维，使用析取范式构建可定制逻辑规则和可调超参数，实现用户定义的分层聚类。


<details>
  <summary>Details</summary>
Motivation: 解决混合数据聚类中的挑战，避免传统降维技术导致的信息损失，提高聚类结果的可解释性，克服聚类数据可解释性问题。

Method: 基于预拓扑的算法，利用析取范式构建可定制的逻辑规则和可调超参数，允许用户定义分层聚类结构，直接从原始数据构建聚类，保持数据完整性。

Result: 通过分层树状图分析和比较聚类指标，该方法表现出优越性能，能够准确且可解释地从原始数据中划分聚类，构建有意义的聚类结构。

Conclusion: 该工作创新性地避免了传统降维技术，使用逻辑规则增强聚类形成和清晰度，为混合数据聚类领域做出了重要贡献，特别是在可解释性方面。

Abstract: This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm's robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.

</details>


### [30] [Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics](https://arxiv.org/abs/2512.04006)
*Connall Garrod,Jonathan P. Keating,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: 本文分析了交叉熵损失在两层线性神经网络中的优化动力学，首次证明了梯度流会收敛到神经坍缩几何，并构建了李雅普诺夫函数来保证全局收敛。


<details>
  <summary>Details</summary>
Motivation: 交叉熵损失在深度学习中广泛应用，但现有理论常使用平方损失或凸模型进行简化，无法捕捉其真实动态。需要研究交叉熵在非凸优化中的本质行为。

Method: 分析标准基向量作为输入的两层线性神经网络（最简单的非凸扩展），利用Hadamard初始化对角化softmax算子，冻结权重矩阵的奇异向量，将动态完全简化为奇异值动态。

Result: 首次证明了梯度流在交叉熵损失下会收敛到神经坍缩几何，构建了显式李雅普诺夫函数来保证全局收敛，尽管存在虚假临界点。

Conclusion: 该分析技术为研究交叉熵训练动态开辟了新途径，超越了特定设置的限制，对理解深度学习中交叉熵优化的本质行为有重要意义。

Abstract: Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.

</details>


### [31] [Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information](https://arxiv.org/abs/2512.03074)
*Mahdi Tavassoli Kejani,Fadi Dornaika,Jean-Michel Loubes*

Main category: cs.LG

TL;DR: 提出了一种用于图神经网络的新型公平性正则化框架，适用于敏感属性仅部分可用的现实场景，在保持分类性能的同时显著减少偏见。


<details>
  <summary>Details</summary>
Motivation: 现有公平性GNN方法依赖敏感属性完全可用的强假设，这在隐私和数据收集限制下不现实。需要解决敏感属性仅部分可用场景下的公平性问题。

Method: 提出模型无关的公平性正则化框架，将机会均等和统计奇偶性作为可微正则化项集成到目标函数中，适用于敏感属性部分可用的情况。

Result: 在五个真实世界基准数据集上的评估表明，该方法在关键公平性指标上显著减少偏见，同时保持竞争力的节点分类性能，在公平性-准确性权衡方面优于基线模型。

Conclusion: 该框架为敏感属性部分可用的现实场景提供了有效的公平性解决方案，在减少偏见的同时最小化预测准确性损失，具有良好的公平性-准确性权衡。

Abstract: Graph Neural Networks (GNNs) have demonstrated exceptional efficacy in relational learning tasks, including node classification and link prediction. However, their application raises significant fairness concerns, as GNNs can perpetuate and even amplify societal biases against protected groups defined by sensitive attributes such as race or gender. These biases are often inherent in the node features, structural topology, and message-passing mechanisms of the graph itself. A critical limitation of existing fairness-aware GNN methods is their reliance on the strong assumption that sensitive attributes are fully available for all nodes during training--a condition that poses a practical impediment due to privacy concerns and data collection constraints. To address this gap, we propose a novel, model-agnostic fairness regularization framework designed for the realistic scenario where sensitive attributes are only partially available. Our approach formalizes a fairness-aware objective function that integrates both equal opportunity and statistical parity as differentiable regularization terms. Through a comprehensive empirical evaluation across five real-world benchmark datasets, we demonstrate that the proposed method significantly mitigates bias across key fairness metrics while maintaining competitive node classification performance. Results show that our framework consistently outperforms baseline models in achieving a favorable fairness-accuracy trade-off, with minimal degradation in predictive accuracy. The datasets and source code will be publicly released at https://github.com/mtavassoli/GNN-FC.

</details>


### [32] [Risk-Entropic Flow Matching](https://arxiv.org/abs/2512.03078)
*Vahid R. Ramezani,Benjamin Englard*

Main category: cs.LG

TL;DR: 本文提出将倾斜（熵）风险应用于流匹配（FM），通过log-exponential变换增强对罕见或高损失事件的关注，改进对数据几何结构的恢复能力。


<details>
  <summary>Details</summary>
Motivation: 标准FM使用均方误差损失，将所有到达同一时空点的速度目标压缩为单一条件均值，忽略了高阶条件信息（方差、偏度、多模态），这些信息编码了数据流形的精细几何结构和少数分支。

Method: 将标准风险敏感（log-exponential）变换应用于条件FM损失，得到倾斜风险损失，该损失是每个时空点上有意义的条件熵FM目标的上界。对梯度进行小阶展开得到两个可解释的一阶修正：FM残差的协方差预处理，以及偏好不对称或罕见分支的偏尾项。

Result: 在专门设计用于探测模糊性和尾部的合成数据上，风险敏感损失相比标准整流FM改善了统计指标，并更忠实地恢复了几何结构。

Conclusion: 倾斜风险损失为FM提供了一种自然框架，通过强调罕见事件和高损失区域，更好地捕捉数据流形的精细几何特征，特别是多模态和少数分支结构。

Abstract: Tilted (entropic) risk, obtained by applying a log-exponential transform to a base loss, is a well established tool in statistics and machine learning for emphasizing rare or high loss events while retaining a tractable optimization problem. In this work, our aim is to interpret its structure for Flow Matching (FM). FM learns a velocity field that transports samples from a simple source distribution to data by integrating an ODE. In rectified FM, training pairs are obtained by linearly interpolating between a source sample and a data sample, and a neural velocity field is trained to predict the straight line displacement using a mean squared error loss. This squared loss collapses all velocity targets that reach the same space-time point into a single conditional mean, thereby ignoring higher order conditional information (variance, skewness, multi-modality) that encodes fine geometric structure about the data manifold and minority branches. We apply the standard risk-sensitive (log-exponential) transform to the conditional FM loss and show that the resulting tilted risk loss is a natural upper-bound on a meaningful conditional entropic FM objective defined at each space-time point. Furthermore, we show that a small order expansion of the gradient of this conditional entropic objective yields two interpretable first order corrections: covariance preconditioning of the FM residual, and a skew tail term that favors asymmetric or rare branches. On synthetic data designed to probe ambiguity and tails, the resulting risk-sensitive loss improves statistical metrics and recovers geometric structure more faithfully than standard rectified FM.

</details>


### [33] [ALARM: Automated MLLM-Based Anomaly Detection in Complex-EnviRonment Monitoring with Uncertainty Quantification](https://arxiv.org/abs/2512.03101)
*Congjing Zhang,Feng Lin,Xinyi Zhao,Pei Guo,Wei Li,Lin Chen,Chaoyue Zhao,Shuai Huang*

Main category: cs.LG

TL;DR: ALARM是一个基于多模态大语言模型的视觉异常检测框架，集成了不确定性量化、推理链、自我反思和模型集成等技术，在复杂环境中实现可靠决策


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，异常通常具有高度上下文相关性和模糊性，因此不确定性量化成为MLLM视觉异常检测系统成功的关键能力

Method: ALARM框架将不确定性量化与推理链、自我反思、MLLM集成等质量保证技术相结合，基于严格的概率推理流程和计算过程设计

Result: 在真实世界智能家居基准数据和伤口图像分类数据上的广泛实证评估显示，ALARM具有优越性能，并在不同领域具有通用适用性

Conclusion: ALARM框架通过集成不确定性量化与质量保证技术，为复杂环境中的视觉异常检测提供了可靠决策支持，具有跨领域的通用应用潜力

Abstract: The advance of Large Language Models (LLMs) has greatly stimulated research interest in developing multi-modal LLM (MLLM)-based visual anomaly detection (VAD) algorithms that can be deployed in complex environments. The challenge is that in these complex environments, the anomalies are sometimes highly contextual and also ambiguous, and thereby, uncertainty quantification (UQ) is a crucial capacity for an MLLM-based VAD system to succeed. In this paper, we introduce our UQ-supported MLLM-based VAD framework called ALARM. ALARM integrates UQ with quality-assurance techniques like reasoning chain, self-reflection, and MLLM ensemble for robust and accurate performance and is designed based on a rigorous probabilistic inference pipeline and computational process. Extensive empirical evaluations are conducted using the real-world smart-home benchmark data and wound image classification data, which shows ALARM's superior performance and its generic applicability across different domains for reliable decision-making.

</details>


### [34] [Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration](https://arxiv.org/abs/2512.03102)
*Yiwei Shi,Hongnan Ma,Mengyue Yang,Cunjia Liu,Weiru Liu*

Main category: cs.LG

TL;DR: 提出扩散驱动的贝叶斯探索框架，用于实时纠正早期状态估计错误，解决粒子滤波中的后验支持不变性问题


<details>
  <summary>Details</summary>
Motivation: 在应急响应等高风险应用中，基于有限或偏差信息的早期状态估计可能与现实严重不符，导致灾难性后果。传统粒子滤波存在"平稳性诱导的后验支持不变性"问题，初始先验排除的区域将永远无法探索，即使新证据与当前信念相矛盾也无法纠正。

Method: 提出扩散驱动的贝叶斯探索框架：1) 通过熵正则化采样和协方差缩放扩散扩展后验支持；2) 使用Metropolis-Hastings检查验证提议；3) 保持推理对意外证据的自适应性。

Result: 在危险气体定位任务中：1) 当先验正确时，性能与强化学习和规划基线相当；2) 在先验错配情况下，显著优于经典SMC扰动和基于RL的方法；3) 提供理论保证证明DEPF能解决S-PSI问题并保持统计严谨性。

Conclusion: 提出的扩散驱动贝叶斯探索框架能够原则性地实时纠正早期状态估计错误，解决了粒子滤波中的后验支持不变性问题，在高风险应用中具有重要价值。

Abstract: In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor.

</details>


### [35] [Temporal Graph Neural Networks for Early Anomaly Detection and Performance Prediction via PV System Monitoring Data](https://arxiv.org/abs/2512.03114)
*Srijani Mukherjee,Laurent Vuillon,Liliane Bou Nassif,Stéphanie Giroux-Julien,Hervé Pabiou,Denys Dutykh,Ionnasis Tsanakas*

Main category: cs.LG

TL;DR: 提出基于时间图神经网络的方法，利用环境与运行参数预测光伏系统输出功率并检测异常


<details>
  <summary>Details</summary>
Motivation: 光伏系统快速增长需要先进的性能监控和异常检测方法以确保最佳运行

Method: 采用时间图神经网络，利用辐照度、模块温度、环境温度等关键参数之间的图基时间关系来预测电功率输出

Result: 基于法国里昂屋顶户外设施收集的数据，包括光伏模块功率测量和气象参数

Conclusion: 时间图神经网络方法能够有效预测光伏输出功率并检测异常，为光伏系统性能监控提供新方案

Abstract: The rapid growth of solar photovoltaic (PV) systems necessitates advanced methods for performance monitoring and anomaly detection to ensure optimal operation. In this study, we propose a novel approach leveraging Temporal Graph Neural Network (Temporal GNN) to predict solar PV output power and detect anomalies using environmental and operational parameters. The proposed model utilizes graph-based temporal relationships among key PV system parameters, including irradiance, module and ambient temperature to predict electrical power output. This study is based on data collected from an outdoor facility located on a rooftop in Lyon (France) including power measurements from a PV module and meteorological parameters.

</details>


### [36] [Real-Time Structural Health Monitoring with Bayesian Neural Networks: Distinguishing Aleatoric and Epistemic Uncertainty for Digital Twin Frameworks](https://arxiv.org/abs/2512.03115)
*Hanbin Cho,Jecheon Yu,Hyeonbin Moon,Jiyoung Yoon,Junhyeong Lee,Giyoung Kim,Jinhyoung Park,Seunghwa Ryu*

Main category: cs.LG

TL;DR: 提出集成PCA、贝叶斯神经网络和HMC推理的SHM框架，从稀疏应变测量重建全场应变分布并量化不确定性，在CFRP试件上验证了准确重建和实时不确定性场生成。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测需要可靠的全场不确定性量化来支持可信决策，但现有方法难以同时获得空间分辨的偶然和认知不确定性。

Method: 结合主成分分析(PCA)、贝叶斯神经网络(BNN)和哈密顿蒙特卡洛(HMC)推理，将稀疏应变测量映射到主要PCA模式以重建全场应变分布。

Result: 在碳纤维增强聚合物试件循环四点弯曲试验中验证，实现准确应变场重建(R²>0.9)，同时产生实时不确定性场，能区分数据固有问题和模型限制。

Conclusion: 该框架推进了结构健康监测向可信数字孪生部署和风险感知结构诊断的发展，通过全场不确定性量化支持可靠决策。

Abstract: Reliable real-time analysis of sensor data is essential for structural health monitoring (SHM) of high-value assets, yet a major challenge is to obtain spatially resolved full-field aleatoric and epistemic uncertainties for trustworthy decision-making. We present an integrated SHM framework that combines principal component analysis (PCA), a Bayesian neural network (BNN), and Hamiltonian Monte Carlo (HMC) inference, mapping sparse strain gauge measurements onto leading PCA modes to reconstruct full-field strain distributions with uncertainty quantification. The framework was validated through cyclic four-point bending tests on carbon fiber reinforced polymer (CFRP) specimens with varying crack lengths, achieving accurate strain field reconstruction (R squared value > 0.9) while simultaneously producing real-time uncertainty fields. A key contribution is that the BNN yields robust full-field strain reconstructions from noisy experimental data with crack-induced strain singularities, while also providing explicit representations of two complementary uncertainty fields. Considered jointly in full-field form, the aleatoric and epistemic uncertainty fields make it possible to diagnose at a local level, whether low-confidence regions are driven by data-inherent issues or by model-related limitations, thereby supporting reliable decision-making. Collectively, the results demonstrate that the proposed framework advances SHM toward trustworthy digital twin deployment and risk-aware structural diagnostics.

</details>


### [37] [Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models](https://arxiv.org/abs/2512.03125)
*Xiwen Wei,Mustafa Munir,Radu Marculescu*

Main category: cs.LG

TL;DR: 提出MoDE架构解决统一多模态生成模型中的模态间遗忘问题，通过解耦模态特定更新和知识蒸馏来缓解梯度冲突


<details>
  <summary>Details</summary>
Motivation: 统一多模态生成模型在持续学习中面临严重的灾难性遗忘问题，包括模态内遗忘和模态间遗忘。模态间遗忘问题尚未得到充分研究，作者发现其根源在于模态间的梯度冲突

Method: 提出模态解耦专家架构，通过隔离模态特定更新来缓解梯度冲突，利用知识蒸馏防止灾难性遗忘并保留预训练能力

Result: 在多样化基准测试中，MoDE显著缓解了模态间和模态内遗忘，在统一多模态生成设置中优于先前的持续学习基线方法

Conclusion: MoDE通过解耦模态更新有效解决了统一多模态生成模型中的模态间遗忘问题，为多模态持续学习提供了轻量级且可扩展的解决方案

Abstract: Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git

</details>


### [38] [Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra](https://arxiv.org/abs/2512.03127)
*Ziyu Xiong,Yichi Zhang,Foyez Alauddin,Chu Xin Cheng,Joon Soo An,Mohammad R. Seyedsayamdost,Ellen D. Zhong*

Main category: cs.LG

TL;DR: ChefNMR：首个端到端框架，仅使用1D NMR谱和化学式直接预测未知分子结构，对天然产物结构预测准确率超过65%


<details>
  <summary>Details</summary>
Motivation: NMR谱解析是确定小分子结构的关键技术，但传统方法耗时且需要大量专业知识。天然产物和临床治疗药物的发现急需自动化结构解析方法。

Method: 将结构解析构建为条件生成问题，使用非等变transformer架构的原子扩散模型。创建了包含11.1万+天然产物的模拟1D NMR谱数据集。

Result: ChefNMR在挑战性天然产物化合物结构预测中达到超过65%的准确率，显著优于现有方法，是自动化小分子结构解析的重要进展。

Conclusion: 该工作向自动化小分子结构解析迈出重要一步，展示了深度学习在加速分子发现中的潜力，为天然产物研究提供了强大工具。

Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.

</details>


### [39] [Contrastive Deep Learning for Variant Detection in Wastewater Genomic Sequencing](https://arxiv.org/abs/2512.03158)
*Adele Chinda,Richmond Azumah,Hemanth Demakethepalli Venkateswara*

Main category: cs.LG

TL;DR: 基于VQ-VAE的无监督病毒变异检测框架，用于废水基因组监测，无需参考基因组或变异标签，在SARS-CoV-2数据上实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 废水基因组监测面临高测序噪声、低病毒覆盖率、片段化读取和缺乏变异标注等计算挑战，传统基于参考的变异检测方法难以处理新突变且计算资源需求大。

Method: 使用向量量化变分自编码器(VQ-VAE)框架，通过k-mer标记化序列学习基因组模式的离散码本，无需参考基因组或变异标签。扩展了基础VQ-VAE架构，加入掩码重建预训练以增强对缺失数据的鲁棒性，以及对比学习以获得高判别性嵌入。

Result: 在约10万条SARS-CoV-2废水测序数据上，VQ-VAE达到99.52%的平均标记级准确率和56.33%的精确序列匹配率，同时保持19.73%的码本利用率(512个码中101个活跃)。对比微调显著改善聚类效果：64维嵌入的轮廓系数提升35%(0.31到0.42)，128维嵌入提升42%(0.31到0.44)。

Conclusion: 该无参考框架为基因组监测提供了可扩展、可解释的方法，可直接应用于公共卫生监测，展示了嵌入维度对变异判别能力的重要影响。

Abstract: Wastewater-based genomic surveillance has emerged as a powerful tool for population-level viral monitoring, offering comprehensive insights into circulating viral variants across entire communities. However, this approach faces significant computational challenges stemming from high sequencing noise, low viral coverage, fragmented reads, and the complete absence of labeled variant annotations. Traditional reference-based variant calling pipelines struggle with novel mutations and require extensive computational resources. We present a comprehensive framework for unsupervised viral variant detection using Vector-Quantized Variational Autoencoders (VQ-VAE) that learns discrete codebooks of genomic patterns from k-mer tokenized sequences without requiring reference genomes or variant labels. Our approach extends the base VQ-VAE architecture with masked reconstruction pretraining for robustness to missing data and contrastive learning for highly discriminative embeddings. Evaluated on SARS-CoV-2 wastewater sequencing data comprising approximately 100,000 reads, our VQ-VAE achieves 99.52% mean token-level accuracy and 56.33% exact sequence match rate while maintaining 19.73% codebook utilization (101 of 512 codes active), demonstrating efficient discrete representation learning. Contrastive fine-tuning with different projection dimensions yields substantial clustering improvements: 64-dimensional embeddings achieve +35% Silhouette score improvement (0.31 to 0.42), while 128-dimensional embeddings achieve +42% improvement (0.31 to 0.44), clearly demonstrating the impact of embedding dimensionality on variant discrimination capability. Our reference-free framework provides a scalable, interpretable approach to genomic surveillance with direct applications to public health monitoring.

</details>


### [40] [Plantain: Plan-Answer Interleaved Reasoning](https://arxiv.org/abs/2512.03176)
*Anthony Liang,Jonathan Berant,Adam Fisch,Abhimanyu Goyal,Kalpesh Krishna,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 论文提出交错推理(IR)方法，让模型在推理过程中交替输出中间结果，替代传统的"先思考后回答"模式，从而减少用户感知延迟并允许早期干预。


<details>
  <summary>Details</summary>
Motivation: 当前推理模型在生成可见响应前需要长时间思考，不给用户任何提示或干预机会，导致用户时间浪费在错误的推理前提上。相比之下，人类对话中会进行轻量级、增量的确认行为来确保对话参与者理解一致。

Method: 提出交错推理(IR)方法，模型在思考过程中交替输出中间响应。进一步提出Plantain(计划-思考-答案交错)方法，其中第一个中间响应是执行任务的明确、逐步计划，允许用户干预并为后续推理步骤提供早期反馈。

Result: Plantain在多个具有挑战性的数学推理和编码基准测试中，pass@1提高了约6%，同时相对于"先思考后回答"基线，首次响应时间减少了60%以上。

Conclusion: 交错推理方法能够在不影响最终响应质量的情况下减少用户感知延迟，而Plantain的计划优先策略通过允许用户干预和早期反馈，进一步提高了推理性能。

Abstract: Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard "think-then-answer" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.

</details>


### [41] [Neighborhood density estimation using space-partitioning based hashing schemes](https://arxiv.org/abs/2512.03187)
*Aashi Jindal*

Main category: cs.LG

TL;DR: FiRE/FiRE.1 是一种基于草图技术的异常检测算法，用于快速识别大规模单细胞RNA测序数据中的稀有细胞亚群；Enhash 是一种快速且资源高效的集成学习器，使用投影哈希检测流数据中的概念漂移。


<details>
  <summary>Details</summary>
Motivation: 需要在大规模单细胞RNA测序数据中快速识别稀有细胞亚群，以及在流数据中高效检测概念漂移，现有方法在速度和资源效率方面存在不足。

Method: FiRE/FiRE.1采用草图技术进行异常检测；Enhash使用投影哈希构建集成学习器来检测概念漂移。

Result: FiRE/FiRE.1在性能上优于现有最先进技术；Enhash在各种漂移类型中在时间和准确性方面都表现出高度竞争力。

Conclusion: 提出的两种方法分别在单细胞RNA测序异常检测和流数据概念漂移检测方面提供了高效且性能优越的解决方案。

Abstract: This work introduces FiRE/FiRE.1, a novel sketching-based algorithm for anomaly detection to quickly identify rare cell sub-populations in large-scale single-cell RNA sequencing data. This method demonstrated superior performance against state-of-the-art techniques. Furthermore, the thesis proposes Enhash, a fast and resource-efficient ensemble learner that uses projection hashing to detect concept drift in streaming data, proving highly competitive in time and accuracy across various drift types.

</details>


### [42] [Scaling Internal-State Policy-Gradient Methods for POMDPs](https://arxiv.org/abs/2512.03204)
*Douglas Aberdeen,Jonathan Baxter*

Main category: cs.LG

TL;DR: 本文改进了在部分可观测环境中学习带记忆策略的算法，包括已知模型和模拟两种情况，并在大型POMDP问题上进行了测试。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在部分可观测环境中学习时，对于无记忆策略表现良好，但在需要记忆的情况下效果不佳。本文旨在改进带记忆策略的学习算法。

Method: 开发了多种改进算法，用于在无限时域设置中学习带记忆策略：当环境模型已知时直接学习，否则通过模拟学习。

Result: 在大型POMDP问题上进行了比较测试，包括噪声机器人导航和多智能体问题。

Conclusion: 提出了改进的带记忆策略学习算法，能够更好地处理部分可观测环境中需要记忆的任务。

Abstract: Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.

</details>


### [43] [A Multi-Agent, Policy-Gradient approach to Network Routing](https://arxiv.org/abs/2512.03211)
*Nigel Tao,Jonathan Baxter,Lex Weaver*

Main category: cs.LG

TL;DR: OLPOMDP强化学习算法成功应用于模拟网络路由，多个分布式路由器智能体无需显式通信即可学习协作行为，避免个体最优但群体有害的行为，通过奖励塑形显著提升收敛速度。


<details>
  <summary>Details</summary>
Motivation: 网络路由是一个分布式决策问题，具有自然的数值性能度量（如平均包传输时间）。研究如何让多个分布式路由器智能体学习协作行为，避免个体最优但群体有害的行为，提高整体网络性能。

Method: 使用OLPOMDP（一种策略梯度强化学习算法）应用于模拟网络路由，多个分布式路由器作为智能体，通过奖励塑形技术（明确惩罚次优行为模式）来引导学习过程。

Result: OLPOMDP在多种网络模型下成功应用于网络路由，分布式路由器智能体无需显式通信即可学习协作行为，避免了"个体理性但群体有害"的行为，奖励塑形显著提高了收敛速度。

Conclusion: OLPOMDP强化学习算法能够有效解决分布式网络路由问题，使多个智能体学习协作行为，奖励塑形是提高学习效率的关键技术，为分布式决策系统提供了有前景的解决方案。

Abstract: Network routing is a distributed decision problem which naturally admits numerical performance measures, such as the average time for a packet to travel from source to destination. OLPOMDP, a policy-gradient reinforcement learning algorithm, was successfully applied to simulated network routing under a number of network models. Multiple distributed agents (routers) learned co-operative behavior without explicit inter-agent communication, and they avoided behavior which was individually desirable, but detrimental to the group's overall performance. Furthermore, shaping the reward signal by explicitly penalizing certain patterns of sub-optimal behavior was found to dramatically improve the convergence rate.

</details>


### [44] [Perch 2.0 transfers 'whale' to underwater tasks](https://arxiv.org/abs/2512.03219)
*Andrea Burns,Lauren Harrell,Bart van Merriënboer,Vincent Dumoulin,Jenny Hamer,Tom Denton*

Main category: cs.LG

TL;DR: Perch 2.0作为生物声学基础模型，在海洋哺乳动物音频任务上通过少样本迁移学习表现出色，优于其他预训练模型。


<details>
  <summary>Details</summary>
Motivation: 尽管Perch 2.0在训练数据中几乎不包含海洋哺乳动物音频或类别，但研究希望评估其在海洋哺乳动物和水下音频任务上的迁移学习能力，为少标签样本的分类任务提供有效解决方案。

Method: 使用Perch 2.0生成的嵌入进行线性探测（linear probing），通过少样本迁移学习评估其在海洋哺乳动物音频任务上的性能，并与多个开源生物声学模型（包括Perch 1.0、SurfPerch、AVES-bio等）进行比较。

Result: Perch 2.0的嵌入在少样本迁移学习中表现出持续高性能，在大多数任务上优于其他嵌入模型，特别是在海洋哺乳动物分类任务中。

Conclusion: Perch 2.0模型在开发海洋哺乳动物分类的线性分类器时，特别是在只有少量标记样本的情况下，是推荐的选择，其嵌入特征具有优越的迁移学习能力。

Abstract: Perch 2.0 is a supervised bioacoustics foundation model pretrained on 14,597 species, including birds, mammals, amphibians, and insects, and has state-of-the-art performance on multiple benchmarks. Given that Perch 2.0 includes almost no marine mammal audio or classes in the training data, we evaluate Perch 2.0 performance on marine mammal and underwater audio tasks through few-shot transfer learning. We perform linear probing with the embeddings generated from this foundation model and compare performance to other pretrained bioacoustics models. In particular, we compare Perch 2.0 with previous multispecies whale, Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3 models, which have open-source tools for transfer-learning and agile modeling. We show that the embeddings from the Perch 2.0 model have consistently high performance for few-shot transfer learning, generally outperforming alternative embedding models on the majority of tasks, and thus is recommended when developing new linear classifiers for marine mammal classification with few labeled examples.

</details>


### [45] [SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning](https://arxiv.org/abs/2512.03244)
*Salman Rahman,Sruthi Gorantla,Arpit Gupta,Swastik Roy,Nanyun Peng,Yang Liu*

Main category: cs.LG

TL;DR: SPARK框架通过三阶段方法生成合成训练数据来训练过程奖励模型，无需昂贵的步骤级标注或真实答案参考，在数学推理任务上超越了基于真实答案的方法。


<details>
  <summary>Details</summary>
Motivation: 过程奖励模型需要昂贵的步骤级标注或真实答案参考，限制了其应用。作者希望开发一种无需真实答案参考的方法来训练高质量的过程奖励模型。

Method: 三阶段框架：1) 生成器产生多样解，验证器通过并行扩展（自一致性）和序列扩展（元批判）进行评估；2) 用验证输出作为合成数据微调生成式过程奖励模型；3) 将PRM-CoT作为奖励模型用于强化学习，并引入格式约束防止奖励攻击。

Result: 在ProcessBench上达到67.5 F1，优于参考引导训练的66.4和GPT-4o的61.9；在六个数学推理基准上平均准确率达到47.4%，优于基于真实答案的RLVR的43.9%。

Conclusion: SPARK框架实现了无需真实答案参考的强化学习训练，性能超越基于真实答案的方法，为缺乏可验证答案或真实答案的领域开辟了新可能性。

Abstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.

</details>


### [46] [Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval](https://arxiv.org/abs/2512.03276)
*Constantin Venhoff,Ashkan Khakzar,Sonia Joseph,Philip Torr,Neel Nanda*

Main category: cs.LG

TL;DR: VLMs在视觉输入的事实回忆任务上表现不如其LLM骨干，原因是实体表示形成太晚，无法有效复用LLM已有的知识回忆机制。


<details>
  <summary>Details</summary>
Motivation: 许多视觉语言模型（VLMs）在事实回忆任务上表现不如其原始的大型语言模型（LLM）骨干，这引发了一个问题：多模态微调在将LLM现有机制扩展到视觉输入方面效果如何？

Method: 对14种不同架构、规模和训练设置的VLMs进行基准测试，使用归因修补、激活修补和探测技术分析性能差异，并尝试两种性能恢复方法：从LLM骨干修补实体表示和使用思维链推理提示。

Result: 14个模型中11个表现出事实回忆性能下降；性能差的VLMs在计算过程中太晚形成实体表示，无法有效复用LLM的事实回忆机制；而高性能VLMs能早期形成实体表示；通过修补实体表示和思维链提示可以恢复性能。

Conclusion: 早期实体表示形成的速度决定了VLMs能否有效复用LLM的预存机制；机制分析可以解释多模态对齐中的系统性失败，并为改进VLMs设计提供指导。

Abstract: Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.

</details>


### [47] [BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark](https://arxiv.org/abs/2512.03280)
*Nicholas Sung,Steven Spreizer,Mohamed Elrefaie,Matthew C. Jones,Faez Ahmed*

Main category: cs.LG

TL;DR: BlendedNet++：一个大规模混合翼体飞机气动数据集和基准，包含12,000+几何形状的CFD模拟结果，提供前向代理预测和逆向设计基准


<details>
  <summary>Details</summary>
Motivation: 现有机器学习气动代理模型受限于大规模场解析数据集的缺乏，阻碍了点态预测精度和可重复逆向设计的进展

Method: 构建包含12,000+混合翼体飞机几何形状的CFD数据集，建立前向代理基准（6种模型），并设计基于条件扩散模型的逆向设计任务

Result: 提供了统一的前向和逆向协议，包含多模型基线，支持跨架构和优化范式的公平、可重复比较

Conclusion: BlendedNet++将促进场级气动学和逆向设计的可重复研究，数据集和基准将在论文接受后发布

Abstract: Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.

</details>


### [48] [Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors](https://arxiv.org/abs/2512.03287)
*Dario Fenoglio,Mohan Li,Davide Casnici,Matias Laporte,Shkurta Gashi,Silvia Santini,Martin Gjoreski,Marc Langheinrich*

Main category: cs.LG

TL;DR: 提出多频联邦学习用于头戴设备活动识别，解决隐私问题和设备采样频率差异


<details>
  <summary>Details</summary>
Motivation: 传统活动识别需要集中上传用户数据，存在隐私风险；同时不同设备的采样频率不同，需要统一处理

Method: 采用多频联邦学习框架，支持隐私保护的机器学习，并能够联合学习不同采样频率设备的模型

Result: 在两个数据集上相比频率特定方法有改进，表明多频联邦学习在活动识别任务中具有前景

Conclusion: 多频联邦学习为头戴设备活动识别提供了隐私保护且能处理频率差异的解决方案，代码已开源供进一步研究

Abstract: Human Activity Recognition (HAR) benefits various application domains, including health and elderly care. Traditional HAR involves constructing pipelines reliant on centralized user data, which can pose privacy concerns as they necessitate the uploading of user data to a centralized server. This work proposes multi-frequency Federated Learning (FL) to enable: (1) privacy-aware ML; (2) joint ML model learning across devices with varying sampling frequency. We focus on head-worn devices (e.g., earbuds and smart glasses), a relatively unexplored domain compared to traditional smartwatch- or smartphone-based HAR. Results have shown improvements on two datasets against frequency-specific approaches, indicating a promising future in the multi-frequency FL-HAR task. The proposed network's implementation is publicly available for further research and development.

</details>


### [49] [ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics](https://arxiv.org/abs/2512.03290)
*Julian Evan Chrisnanto,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: 提出ASPEN架构，通过自适应谱层解决PINNs在处理刚性、多尺度非线性系统时的频谱偏差问题，成功求解复杂Ginzburg-Landau方程。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在处理刚性、多尺度和非线性系统时存在严重局限性，主要原因是标准MLP架构固有的频谱偏差，无法充分表示高频分量，导致求解失败。

Method: 提出ASPEN架构，在网络输入阶段集成具有可学习傅里叶特征的自适应谱层，使模型能够在训练过程中动态调整自身的谱基，高效学习和表示解所需的精确频率内容。

Result: 在复杂Ginzburg-Landau方程上，标准PINN架构完全失败，而ASPEN成功求解，预测解与高分辨率真实解视觉上无法区分，物理残差中位数低至5.10×10^-3，正确捕捉了自由能快速弛豫和畴壁前沿长期稳定性等物理特性。

Conclusion: 通过引入自适应谱基，ASPEN为复杂动力系统提供了稳健且物理一致的求解器，在标准PINNs失败的挑战性物理领域中开辟了机器学习的新途径。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.

</details>


### [50] [Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction](https://arxiv.org/abs/2512.03298)
*Echo Diyun LU,Charles Findling,Marianne Clausel,Alessandro Leite,Wei Gong,Pierric Kersaudy*

Main category: cs.LG

TL;DR: 该研究将自适应共形推理（ACI）与深度切换状态空间模型结合，为非平稳时间序列中的切换机制预测提供分布自由的置信区间，并在多种基准模型上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的机制转换会破坏平稳性，使得校准的不确定性变得与点预测精度同等重要。需要为机制切换预测提供分布自由的置信区间保证。

Method: 将深度切换状态空间模型与自适应共形推理（ACI）及其聚合变体（AgACI）结合，并开发了一个统一的共形包装器，可应用于S4、MC-Dropout GRU、稀疏高斯过程和变点局部模型等序列基线模型。

Result: 在合成和真实数据集上，共形化预测器实现了接近标称的覆盖率，具有竞争力的准确性，并且通常提高了区间效率。

Conclusion: 该方法能够在非平稳性和模型误设条件下，为在线预测提供有限样本的边际保证，是处理机制切换时间序列预测的有效解决方案。

Abstract: Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.

</details>


### [51] [HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction](https://arxiv.org/abs/2512.03300)
*Pengfei Hu,Fan Ming,Xiaoxue Han,Chang Lu,Yue Ning,Dan Lu*

Main category: cs.LG

TL;DR: 提出HydroDCM框架，通过伪域标签和对抗学习解决多水库流入预测中的域泛化问题，利用空间元数据进行轻量级适应


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在水库流入预测中表现良好，但在不同水库间应用时因域偏移问题性能下降。传统域泛化方法在多水库水文系统中适用性有限，因为每个水库有独特流入模式，且空间元数据等观测外信息有间接但重要影响。

Method: 提出HydroDCM框架：1) 利用水库空间元数据构建伪域标签指导对抗学习，提取不变时序特征；2) 推理时通过轻量级条件层结合目标水库元数据进行特征适应，平衡域不变性与位置特定适应性。

Result: 在科罗拉多河上游流域30个真实水库上的实验表明，该方法在多域条件下显著优于最先进的域泛化基线方法，且保持计算高效性。

Conclusion: HydroDCM通过结合伪域标签引导的对抗学习和元数据驱动的轻量级适应，有效解决了多水库水文系统中的域泛化问题，实现了跨水库流入预测的准确性和可扩展性。

Abstract: Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.

</details>


### [52] [Robust Tabular Foundation Models](https://arxiv.org/abs/2512.03307)
*Matthew Peroni,Franck Le,Vadim Sheinin*

Main category: cs.LG

TL;DR: RTFM提出了一种对抗性训练框架，通过参数化合成数据生成器来强调对模型具有挑战性的数据集，从而提升表格基础模型的鲁棒性和性能。


<details>
  <summary>Details</summary>
Motivation: 表格基础模型(TFMs)在结构化数据上展现出超越传统ML方法的潜力，但现有研究主要关注设计高质量的先验生成器来提升预训练性能。本文的洞察是：参数化生成器分布可以从对抗鲁棒性的角度出发，在训练过程中调整生成器以强调对模型特别具有挑战性的数据集。

Method: 提出了RTFM（鲁棒表格基础模型）框架，引入最优性间隙度量（TFM性能与XGBoost、CatBoost、随机森林等强基线最佳可达性能之间的差异）。基于此，采用模型无关的对抗训练方法，在训练过程中自适应调整合成数据生成器，重点关注模型表现不佳的数据集。

Result: 在TabPFN V2分类器上应用RTFM，相比原始TabPFN和其他基线算法，平均归一化AUC提升了高达6%，且仅需不到10万个额外的合成数据集。这表明通过针对性对抗训练仅使用合成数据就能有效微调TFMs。

Conclusion: RTFM展示了一种有前景的新方向：仅使用合成数据进行针对性对抗训练和微调表格基础模型。参数化生成器分布并采用对抗鲁棒性视角，能够有效提升模型性能，为TFMs的训练方法开辟了新途径。

Abstract: The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.

</details>


### [53] [Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates](https://arxiv.org/abs/2512.03309)
*Aniruddha Bora,Shixuan Zhang,Khemraj Shukla,Bryce Harrop,George Em. Karniadakis,L. Ruby Leung*

Main category: cs.LG

TL;DR: 提出一个算子学习框架，通过在线应用偏差修正趋势来改进地球系统模型预测，使用U-Net架构变体捕获多尺度非线性特征，在混合E3SM模拟中实现稳定且可扩展的偏差减少。


<details>
  <summary>Details</summary>
Motivation: 传统的数据同化偏差修正方法在模型自由运行时效果有限，需要一种能够在模型集成过程中在线应用偏差修正的新方法，以解决地球系统模型因分辨率、参数化和初始状态不确定性导致的预测限制。

Method: 开发基于U-Net的算子学习框架，包括Inception U-Net (IUNet)和多尺度网络(M&M)两种架构，结合不同上采样和感受野来捕获多尺度非线性特征。使用两年E3SM模拟向ERA5再分析数据进行训练，在线应用偏差修正趋势。

Result: 两种架构在离线测试中都优于标准U-Net基线，表明功能丰富性而非参数数量驱动性能。在在线混合E3SM运行中，M&M在变量和垂直层次上提供最一致的偏差减少。ML增强配置在多年模拟中保持稳定且计算可行。

Conclusion: 该框架强调长期稳定性、可移植性和更新频率限制，展示了表达性ML算子在学习和结构化跨尺度关系以及改造传统ESMs方面的实用性，为可扩展混合建模提供了实用途径。

Abstract: Coarse resolution, imperfect parameterizations, and uncertain initial states and forcings limit Earth-system model (ESM) predictions. Traditional bias correction via data assimilation improves constrained simulations but offers limited benefit once models run freely. We introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies and applies them online during integration. Building on a U-Net backbone, we develop two operator architectures Inception U-Net (IUNet) and a multi-scale network (M\&M) that combine diverse upsampling and receptive fields to capture multiscale nonlinear features under Energy Exascale Earth System Model (E3SM) runtime constraints. Trained on two years E3SM simulations nudged toward ERA5 reanalysis, the operators generalize across height levels and seasons. Both architectures outperform standard U-Net baselines in offline tests, indicating that functional richness rather than parameter count drives performance. In online hybrid E3SM runs, M\&M delivers the most consistent bias reductions across variables and vertical levels. The ML-augmented configurations remain stable and computationally feasible in multi-year simulations, providing a practical pathway for scalable hybrid modeling. Our framework emphasizes long-term stability, portability, and cadence-limited updates, demonstrating the utility of expressive ML operators for learning structured, cross-scale relationships and retrofitting legacy ESMs.

</details>


### [54] [Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs](https://arxiv.org/abs/2512.03324)
*Ngoc Bui,Shubham Sharma,Simran Lamba,Saumitra Mishra,Rex Ying*

Main category: cs.LG

TL;DR: TRIM-KV：通过轻量级保留门学习token内在重要性，在内存受限时智能淘汰低重要性token，实现高效长序列推理


<details>
  <summary>Details</summary>
Motivation: 解决长序列LLM推理中内存和计算瓶颈问题，现有方法（量化、卸载、启发式KV淘汰）要么协调成本高，要么依赖不可靠的注意力重要性代理

Method: 提出TRIM-KV方法，通过轻量级保留门在token创建时学习其内在重要性，预测随时间衰减的保留分数，内存超限时淘汰低分token，仅需门微调且推理开销可忽略

Result: 在数学推理、过程生成、对话长记忆、长上下文理解等基准测试中，TRIM-KV始终优于强淘汰和可学习检索基线，在低内存场景尤其突出，甚至在某些设置中超越全缓存模型

Conclusion: 选择性保留可作为正则化形式，抑制无信息token的噪声；保留分数与人类直觉一致，自然恢复sink token、滑动窗口、要点压缩等启发式方法，为LLM可解释性提供新途径

Abstract: Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.

</details>


### [55] [A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning](https://arxiv.org/abs/2512.03363)
*Shanika Iroshi Nanayakkara,Shiva Raj Pokhrel*

Main category: cs.LG

TL;DR: 提出A2G框架，通过几何增益和QoS增益自适应聚合，解决量子联邦学习中客户端质量不均、量子隐形传态保真度随机、设备不稳定等问题，在混合量子经典测试床上表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 量子联邦学习面临客户端质量不均、量子隐形传态保真度随机、设备不稳定、局部与全局模型几何不匹配等问题，传统基于欧几里得拓扑和均匀通信可靠性的聚合规则不适用于新兴量子联邦系统。

Method: 提出A2G（自适应双增益聚合）框架，包含几何增益（调节几何混合）和QoS增益（基于隐形传态保真度、延迟和不稳定性调节客户端重要性），开发A2G更新规则，在平滑性和有界方差假设下建立收敛保证。

Result: A2G能够恢复FedAvg、QoS感知平均和基于流形的聚合作为特例，在量子经典混合测试床上实验表明，在异构和噪声条件下提高了稳定性和准确性。

Conclusion: A2G框架有效解决了量子联邦学习中的聚合挑战，通过双增益机制自适应处理几何和通信质量问题，为量子联邦系统提供了更鲁棒的聚合方案。

Abstract: Federated learning (FL) deployed over quantum enabled and heterogeneous classical networks faces significant performance degradation due to uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. Classical aggregation rules assume euclidean topology and uniform communication reliability, limiting their suitability for emerging quantum federated systems. This paper introduces A2G (Adaptive Aggregation with Two Gains), a dual gain framework that jointly regulates geometric blending through a geometry gain and modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability. We develop the A2G update rule, establish convergence guarantees under smoothness and bounded variance assumptions, and show that A2G recovers FedAvg, QoS aware averaging, and manifold based aggregation as special cases. Experiments on a quantum classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.

</details>


### [56] [MAGE-ID: A Multimodal Generative Framework for Intrusion Detection Systems](https://arxiv.org/abs/2512.03375)
*Mahdi Arab Loodaricheh,Mohammad Hossein Manshaei,Anita Raja*

Main category: cs.LG

TL;DR: MAGE-ID是一个基于扩散的多模态攻击生成框架，用于入侵检测系统的数据增强，通过联合训练Transformer和CNN编码器，将表格流量特征与转换图像耦合，解决数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现代入侵检测系统面临异构网络流量、不断演变的网络威胁以及良性流量与攻击流量之间严重数据不平衡的挑战。现有生成模型仅限于单模态，无法捕捉跨域依赖关系。

Method: 提出MAGE-ID（多模态攻击生成器），这是一个基于扩散的生成框架，通过统一的潜在先验将表格流量特征与其转换图像耦合。联合训练基于Transformer和CNN的变分编码器与EDM风格去噪器，实现平衡且连贯的多模态合成。

Result: 在CIC-IDS-2017和NSL-KDD数据集上的评估显示，MAGE-ID在保真度、多样性和下游检测性能方面显著优于TabSyn和TabDDPM，证明了其多模态IDS增强的有效性。

Conclusion: MAGE-ID通过多模态生成方法有效解决了入侵检测中的数据不平衡问题，为IDS数据增强提供了更有效的解决方案，在保真度、多样性和检测性能方面均优于现有方法。

Abstract: Modern Intrusion Detection Systems (IDS) face severe challenges due to heterogeneous network traffic, evolving cyber threats, and pronounced data imbalance between benign and attack flows. While generative models have shown promise in data augmentation, existing approaches are limited to single modalities and fail to capture cross-domain dependencies. This paper introduces MAGE-ID (Multimodal Attack Generator for Intrusion Detection), a diffusion-based generative framework that couples tabular flow features with their transformed images through a unified latent prior. By jointly training Transformer and CNN-based variational encoders with an EDM style denoiser, MAGE-ID achieves balanced and coherent multimodal synthesis. Evaluations on CIC-IDS-2017 and NSL-KDD demonstrate significant improvements in fidelity, diversity, and downstream detection performance over TabSyn and TabDDPM, highlighting the effectiveness of MAGE-ID for multimodal IDS augmentation.

</details>


### [57] [UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs](https://arxiv.org/abs/2512.03383)
*Hung-Yueh Chiang,Chi-Chih Chang,Yu-Chen Lu,Chien-Yu Lin,Kai-Chiang Wu,Mohamed S. Abdelfattah,Diana Marculescu*

Main category: cs.LG

TL;DR: UniQL是一个统一的边缘LLM后训练量化和低秩压缩框架，支持Transformer、SSM和混合模型，通过权重排序、量化感知SVD等技术实现4-5.7倍内存减少和2.7-3.4倍吞吐提升，精度损失在5%以内。


<details>
  <summary>Details</summary>
Motivation: 在移动平台上部署大型语言模型面临内存有限和计算资源受限的挑战，且设备工作负载直接影响资源可用性，增加了模型部署的不确定性。

Method: 提出UniQL统一框架，集成量化和低秩压缩，包含高效结构化权重排序方法（加速20倍）、量化感知SVD、SSM状态感知权重排序、剪枝模型融合RoPE核，支持云端单次工作流和端侧可配置剪枝率（最高35%）。

Result: 量化剪枝模型实现4-5.7倍内存减少和2.7-3.4倍token吞吐提升，在15%剪枝率下，Transformer（Llama3、Qwen2.5）、SSM（Mamba2）和混合模型（Nemotron-H、Bamba-v2）的精度损失控制在5%以内。

Conclusion: UniQL为边缘LLM部署提供了有效的统一压缩框架，显著减少内存占用并提升推理速度，同时保持模型精度，支持多种模型架构的端侧部署。

Abstract: Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.

</details>


### [58] [VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing](https://arxiv.org/abs/2512.03394)
*Hamed Poursiami,Shay Snyder,Guojing Cong,Thomas Potok,Maryam Parsa*

Main category: cs.LG

TL;DR: VS-Graph：基于向量符号架构的图学习框架，通过尖峰扩散机制和关联消息传递，在保持高维向量空间计算的同时，实现了与GNNs竞争的性能，训练速度提升高达450倍。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在图分类任务中表现出色，但计算成本高，限制了其在资源受限设备上的部署。超维计算（HDC）提供了轻量级替代方案，但现有HDC方法难以匹配GNNs的预测性能。需要一种既能保持HDC效率又能接近GNNs表达能力的方法。

Method: 提出VS-Graph框架：1）尖峰扩散机制（Spike Diffusion）用于拓扑驱动的节点识别；2）关联消息传递方案（Associative Message Passing）用于在高维向量空间内实现多跳邻域聚合。无需基于梯度的优化或反向传播。

Result: 在MUTAG和DD等标准基准测试中，比先前的HDC基线提升4-5%的准确率；与GNN基线相比，在多个数据集上达到相当或更好的性能；训练速度提升高达450倍；即使将超向量维度降至D=128仍保持高准确率。

Conclusion: VS-Graph成功缩小了HDC效率与消息传递表达能力之间的差距，展示了在边缘和神经形态硬件上实现超高效执行的潜力，为资源受限环境下的图学习提供了有前景的解决方案。

Abstract: Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.

</details>


### [59] [Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value](https://arxiv.org/abs/2512.03399)
*Joe Edelman,Tan Zhi-Xuan,Ryan Lowe,Oliver Klingefjord,Vincent Wang-Mascianica,Matija Franklin,Ryan Othniel Kearns,Ellie Hain,Atrisha Sarkar,Michiel Bakker,Fazl Barez,David Duvenaud,Jakob Foerster,Iason Gabriel,Joseph Gubbels,Bryce Goodman,Andreas Haupt,Jobst Heitzig,Julian Jara-Ettinger,Atoosa Kasirzadeh,James Ravi Kirkpatrick,Andrew Koh,W. Bradley Knox,Philipp Koralus,Joel Lehman,Sydney Levine,Samuele Marro,Manon Revel,Toby Shorin,Morgan Sutherland,Michael Henry Tessler,Ivan Vendrov,James Wilken-Smith*

Main category: cs.LG

TL;DR: 论文提出"全栈对齐"概念，认为仅对齐AI系统与操作者意图不足以保证良好社会结果，需要同时对齐AI系统和塑造它们的机构与人类价值观。建议采用"厚价值模型"来更好地区分价值观与其他信号，支持规范性推理和集体利益建模。


<details>
  <summary>Details</summary>
Motivation: 当前AI对齐方法存在局限：即使AI系统完美对齐其操作组织的意图，如果该组织的目标与其他机构和个体不匹配，仍可能导致不良社会结果。现有价值表示方法（如效用函数、偏好排序、非结构化文本）难以有效区分价值观与其他信号，支持规范性推理，以及建模集体利益。

Method: 提出"厚价值模型"方法，结构化地表示价值观和规范，使系统能够区分持久价值观与短暂偏好，建模个体选择的社会嵌入性，并在新领域中进行规范性推理。在五个领域展示该方法：AI价值管理、规范性能力代理、双赢谈判系统、意义保持的经济机制和民主监管机构。

Result: 论文展示了厚价值模型在五个应用领域的潜力，表明这种方法能够更好地处理AI对齐中的复杂问题，包括区分价值观与其他信号、支持规范性推理、建模集体利益等。

Conclusion: 需要全栈对齐——同时对齐AI系统和塑造它们的机构与人类价值观，而厚价值模型是实现这一目标的有效方法，能够克服现有价值表示方法的局限性，促进AI系统更好地服务于社会利益。

Abstract: Beneficial societal outcomes cannot be guaranteed by aligning individual AI systems with the intentions of their operators or users. Even an AI system that is perfectly aligned to the intentions of its operating organization can lead to bad outcomes if the goals of that organization are misaligned with those of other institutions and individuals. For this reason, we need full-stack alignment, the concurrent alignment of AI systems and the institutions that shape them with what people value. This can be done without imposing a particular vision of individual or collective flourishing. We argue that current approaches for representing values, such as utility functions, preference orderings, or unstructured text, struggle to address these and other issues effectively. They struggle to distinguish values from other signals, to support principled normative reasoning, and to model collective goods. We propose thick models of value will be needed. These structure the way values and norms are represented, enabling systems to distinguish enduring values from fleeting preferences, to model the social embedding of individual choices, and to reason normatively, applying values in new domains. We demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.

</details>


### [60] [Better World Models Can Lead to Better Post-Training Performance](https://arxiv.org/abs/2512.03400)
*Prakhar Gupta,Henry Conklin,Sarah-Jane Leslie,Andrew Lee*

Main category: cs.LG

TL;DR: 研究显示，在Transformer中显式训练世界模型能提升状态表征的线性可解码性和因果可操控性，进而增强强化学习后训练的效果，特别是在复杂任务状态下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 探究显式世界建模目标如何影响Transformer在不同训练阶段的内在表征和下游能力，特别关注世界模型质量对强化学习后训练性能的影响。

Method: 使用2x2x2魔方作为测试环境，比较标准的下一个token预测与两种显式世界建模策略：(i)状态预测预训练，(ii)状态预测+下一个token联合目标。后训练采用Group Relative Policy Optimization (GRPO)，通过线性探针和因果干预评估表征质量。

Result: 显式世界建模产生更线性可解码和因果可操控的状态表征，这些改进的状态表征显著提升了GRPO的性能增益，特别是在更难的魔方状态下表现更突出。

Conclusion: 通过锐化状态表征可以提升序列规划任务的后训练效果，显式世界建模有助于改善模型在复杂任务中的表现。

Abstract: In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.

</details>


### [61] [Grokked Models are Better Unlearners](https://arxiv.org/abs/2512.03437)
*Yuanbang Liang,Yang Li*

Main category: cs.LG

TL;DR: 论文研究发现，在模型完成"顿悟"（grokking）阶段后进行机器遗忘，相比早期停止的模型，能实现更高效的遗忘、更少的副作用和更稳定的更新。


<details>
  <summary>Details</summary>
Motivation: 探索"顿悟"（延迟泛化现象）是否有助于机器遗忘问题，即在不完全重新训练的情况下移除特定数据的影响。

Method: 在视觉（CNN/ResNet在CIFAR、SVHN、ImageNet）和语言（Transformer在TOFU风格设置）任务上，比较在顿悟前后应用标准遗忘方法的效果。

Result: 从顿悟后的检查点开始进行遗忘，能实现：(1) 更高效的遗忘（达到目标遗忘水平需要更少的更新），(2) 更少的附带损害（在保留数据和测试集上的性能下降更小），(3) 跨种子的更新更稳定。

Conclusion: 顿悟后的模型学习到更模块化的表示，减少了遗忘子集和保留子集之间的梯度对齐，这有助于选择性遗忘。训练时机（顿悟前vs后）是改进现有遗忘方法的正交杠杆。

Abstract: Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.

</details>


### [62] [Multi-Modal Opinion Integration for Financial Sentiment Analysis using Cross-Modal Attention](https://arxiv.org/abs/2512.03464)
*Yujing Liu,Chen Yang*

Main category: cs.LG

TL;DR: 提出一个端到端深度学习框架，整合金融意见的时效性模态和流行性模态，通过跨模态注意力机制进行金融情感分析，在837家公司数据集上达到83.5%准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效整合多样化的意见模态并捕捉模态间的细粒度交互，而金融情感分析对市场预测和风险评估越来越重要。

Method: 使用BERT（中文wwm-ext）进行特征嵌入，提出金融多头交叉注意力（FMHCA）结构促进不同意见模态间的信息交换，通过Transformer层优化特征，使用多模态分解双线性池化进行分类。

Result: 在覆盖837家公司的综合数据集上，该方法达到83.5%的准确率，显著优于包括BERT+Transformer在内的基线方法21个百分点。

Conclusion: 该框架展示了整合时效性和流行性金融意见模态的潜力，能够支持更准确的金融决策和风险管理。

Abstract: In recent years, financial sentiment analysis of public opinion has become increasingly important for market forecasting and risk assessment. However, existing methods often struggle to effectively integrate diverse opinion modalities and capture fine-grained interactions across them. This paper proposes an end-to-end deep learning framework that integrates two distinct modalities of financial opinions: recency modality (timely opinions) and popularity modality (trending opinions), through a novel cross-modal attention mechanism specifically designed for financial sentiment analysis. While both modalities consist of textual data, they represent fundamentally different information channels: recency-driven market updates versus popularity-driven collective sentiment. Our model first uses BERT (Chinese-wwm-ext) for feature embedding and then employs our proposed Financial Multi-Head Cross-Attention (FMHCA) structure to facilitate information exchange between these distinct opinion modalities. The processed features are optimized through a transformer layer and fused using multimodal factored bilinear pooling for classification into negative, neutral, and positive sentiment. Extensive experiments on a comprehensive dataset covering 837 companies demonstrate that our approach achieves an accuracy of 83.5%, significantly outperforming baselines including BERT+Transformer by 21 percent. These results highlight the potential of our framework to support more accurate financial decision-making and risk management.

</details>


### [63] [Bayesian Event-Based Model for Disease Subtype and Stage Inference](https://arxiv.org/abs/2512.03467)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯子类型事件模型（BEBMS），在疾病进展子类型推断任务上显著优于现有方法SuStaIn，并在阿尔茨海默病数据上得到更符合科学共识的结果。


<details>
  <summary>Details</summary>
Motivation: 慢性疾病在不同患者中的进展存在异质性，通常表现为少数几种进展子类型。SuStaIn方法虽然被广泛应用，但其鲁棒性尚未得到充分验证。本文旨在开发更鲁棒的贝叶斯子类型事件模型，并与SuStaIn进行系统性比较。

Method: 开发了基于贝叶斯框架的子类型事件模型（BEBMS），通过合成数据实验系统比较BEBMS与SuStaIn在不同模型误设程度下的性能，包括排序、分期和子类型分配任务。最后将两种方法应用于真实世界的阿尔茨海默病数据集。

Result: 在合成数据实验中，BEBMS在排序、分期和子类型分配任务上均显著优于SuStaIn。在阿尔茨海默病数据应用中，BEBMS得到的结果比SuStaIn更符合该疾病进展的科学共识。

Conclusion: BEBMS作为一种新的贝叶斯子类型事件模型，在疾病进展子类型推断方面比SuStaIn更鲁棒、更准确，为理解疾病异质性提供了更可靠的工具。

Abstract: Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.

</details>


### [64] [SweetDeep: A Wearable AI Solution for Real-Time Non-Invasive Diabetes Screening](https://arxiv.org/abs/2512.03471)
*Ian Henriques,Lynda Elhassar,Sarvesh Relekar,Denis Walrave,Shayan Hassantabar,Vishu Ghanakota,Adel Laoui,Mahmoud Aich,Rafia Tir,Mohamed Zerguine,Samir Louafi,Moncef Kimouche,Emmanuel Cosson,Niraj K Jha*

Main category: cs.LG

TL;DR: SweetDeep：基于三星手表生理数据的轻量级神经网络，在自由生活条件下实现82.5%的2型糖尿病检测准确率


<details>
  <summary>Details</summary>
Motivation: 2型糖尿病全球增长需要可扩展、低成本的筛查方法。现有生化检测方法具有侵入性和成本高的缺点，而基于消费级可穿戴设备的机器学习检测方法此前仅限于受控环境。

Method: 开发SweetDeep轻量神经网络，使用欧盟和中东地区285名参与者的生理和人口统计数据，通过三星Galaxy Watch 7在自由生活条件下收集6天数据，每人每天多个2分钟传感器记录，总计约20个记录/人。模型参数少于3000个。

Result: 在三折交叉验证下达到82.5%患者级准确率（82.1% macro-F1，79.7%灵敏度，84.6%特异性），预期校准误差5.5%。对低置信度预测（<10%）允许弃权后，剩余患者准确率达84.5%。

Conclusion: 结合工程特征与轻量架构可在真实世界可穿戴设备环境中实现准确、快速、可泛化的2型糖尿病检测，为大规模筛查提供可行方案。

Abstract: The global rise in type 2 diabetes underscores the need for scalable and cost-effective screening methods. Current diagnosis requires biochemical assays, which are invasive and costly. Advances in consumer wearables have enabled early explorations of machine learning-based disease detection, but prior studies were limited to controlled settings. We present SweetDeep, a compact neural network trained on physiological and demographic data from 285 (diabetic and non-diabetic) participants in the EU and MENA regions, collected using Samsung Galaxy Watch 7 devices in free-living conditions over six days. Each participant contributed multiple 2-minute sensor recordings per day, totaling approximately 20 recordings per individual. Despite comprising fewer than 3,000 parameters, SweetDeep achieves 82.5% patient-level accuracy (82.1% macro-F1, 79.7% sensitivity, 84.6% specificity) under three-fold cross-validation, with an expected calibration error of 5.5%. Allowing the model to abstain on less than 10% of low-confidence patient predictions yields an accuracy of 84.5% on the remaining patients. These findings demonstrate that combining engineered features with lightweight architectures can support accurate, rapid, and generalizable detection of type 2 diabetes in real-world wearable settings.

</details>


### [65] [Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression](https://arxiv.org/abs/2512.03475)
*Hongtao Hao,Joseph L. Austerweil*

Main category: cs.LG

TL;DR: JPM是一个概率框架，用于从横断面数据中推断混合神经退行性疾病的联合进展，将单病轨迹视为部分排序并构建联合进展的先验分布。


<details>
  <summary>Details</summary>
Motivation: 标准事件模型假设个体只有单一疾病，但神经退行性疾病中混合病理很常见，需要能够处理多种疾病同时进展的模型。

Method: 提出联合进展模型(JPM)，将单病轨迹视为部分排序，构建联合进展的先验分布。研究了四种变体：Pairwise、Bradley-Terry、Plackett-Luce和Mallows，分析了校准性、分离性和锐度三个特性。

Result: 所有JPM变体都具有校准性，分离性接近完美；锐度因变体而异，可通过输入部分排序的简单特征预测。在合成实验中，JPM比强基线SA-EBM提高了约21%的排序准确性。在NACC数据中，Mallows变体和基线模型的结果与AD和VaD混合病理进展的先前文献更一致。

Conclusion: JPM为从横断面数据中推断混合神经退行性疾病的联合进展提供了一个有效的概率框架，能够处理现实世界中常见的多病理共存情况。

Abstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.

</details>


### [66] [ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms](https://arxiv.org/abs/2512.03476)
*Juan Diego Toscano,Daniel T. Chen,George Em Karniadakis*

Main category: cs.LG

TL;DR: ATHENA是一个自主代理框架，用于管理端到端的计算研究生命周期，通过知识驱动的诊断过程（HENA循环）实现超人类性能，在科学计算和科学机器学习中达到10^{-14}的验证误差。


<details>
  <summary>Details</summary>
Motivation: 解决理论概念化与计算实现之间的差距，这是科学计算和科学机器学习中的主要瓶颈。当前标准自动化工具无法自主识别数学对称性、推导稳定数值求解器或处理病态问题。

Method: 提出ATHENA框架，核心是HENA循环（知识驱动的诊断过程），将其建模为上下文赌博机问题。系统作为在线学习器，分析先前试验，从组合空间中选择结构动作，通过专家蓝图（如通用逼近、物理信息约束）指导，将动作转换为可执行代码以生成科学奖励。

Result: 在科学计算中自主识别数学对称性获得精确解析解，在基础模型失败时推导稳定数值求解器；在科学机器学习中执行深度诊断处理病态问题，结合混合符号-数值工作流解决多物理问题。框架达到超人类性能，验证误差为10^{-14}。通过"人在回路"协作干预，结果提升一个数量级。

Conclusion: ATHENA实现了从实现机制到方法创新的范式转变，加速科学发现。该框架超越了标准自动化，通过知识驱动的自主决策和人类协作，解决了科学计算和科学机器学习中的关键瓶颈。

Abstract: Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' ($A_n$) from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code ($S_n$) to generate scientific rewards ($R_n$). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of $10^{-14}$. Furthermore, collaborative ``human-in-the-loop" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.

</details>


### [67] [Modal Logical Neural Networks](https://arxiv.org/abs/2512.03491)
*Antonin Sulc*

Main category: cs.LG

TL;DR: 提出Modal Logical Neural Networks (MLNNs)，一种将深度学习与模态逻辑形式语义结合的神经符号框架，能够进行必然性和可能性推理，作为可微分的"逻辑护栏"。


<details>
  <summary>Details</summary>
Motivation: 需要将深度学习与形式逻辑推理相结合，创建能够进行模态逻辑推理（关于必然性和可能性）的神经符号系统，同时保持端到端的可微分性，以增强模型的逻辑一致性和可解释性。

Method: 基于Kripke语义学，引入专门用于模态运算符□和◇的神经元，这些神经元在一组可能世界上操作。框架允许用户固定世界间的可达关系来强制执行已知规则，或者通过神经网络参数化作为归纳特征来学习逻辑系统的关系结构。

Result: 在四个案例研究中展示了MLNNs：语法护栏、未知的公理检测、多智能体认知信任、以及自然语言谈判中的建设性欺骗检测。实验表明，强制执行或学习可达关系可以提高逻辑一致性和可解释性，而不改变底层任务架构。

Conclusion: MLNNs提供了一个灵活、可微分的神经符号框架，能够整合深度学习和模态逻辑推理，既可以强制执行已知逻辑规则，也可以从数据中学习逻辑结构，在保持任务架构不变的同时提高逻辑一致性和模型可解释性。

Abstract: We propose Modal Logical Neural Networks (MLNNs), a neurosymbolic framework that integrates deep learning with the formal semantics of modal logic, enabling reasoning about necessity and possibility. Drawing on Kripke semantics, we introduce specialized neurons for the modal operators $\Box$ and $\Diamond$ that operate over a set of possible worlds, enabling the framework to act as a differentiable ``logical guardrail.'' The architecture is highly flexible: the accessibility relation between worlds can either be fixed by the user to enforce known rules or, as an inductive feature, be parameterized by a neural network. This allows the model to optionally learn the relational structure of a logical system from data while simultaneously performing deductive reasoning within that structure.
  This versatile construction is designed for flexibility. The entire framework is differentiable from end to end, with learning driven by minimizing a logical contradiction loss. This not only makes the system resilient to inconsistent knowledge but also enables it to learn nonlinear relationships that can help define the logic of a problem space. We illustrate MLNNs on four case studies: grammatical guardrailing, axiomatic detection of the unknown, multi-agent epistemic trust, and detecting constructive deception in natural language negotiation. These experiments demonstrate how enforcing or learning accessibility can increase logical consistency and interpretability without changing the underlying task architecture.

</details>


### [68] [Physics-Driven Learning Framework for Tomographic Tactile Sensing](https://arxiv.org/abs/2512.03512)
*Xuanxuan Yang,Xiuyang Zhang,Haofeng Chen,Gang Ma,Xiaojie Wang*

Main category: cs.LG

TL;DR: PhyDNN：一种将EIT前向模型嵌入学习目标的物理驱动深度重建框架，用于解决电阻抗层析成像中的非线性逆问题，提高触觉传感的重建质量。


<details>
  <summary>Details</summary>
Motivation: 电阻抗层析成像（EIT）因其布线简单和形状灵活而成为大面积触觉传感的有吸引力方案，但其非线性逆问题常导致严重伪影和不准确的接触重建。现有方法存在黑盒性质，物理一致性差，泛化能力有限。

Method: 提出PhyDNN框架，将EIT前向模型直接嵌入学习目标，联合最小化预测与真实电导率图的差异，并强制与正向偏微分方程的一致性。设计了可微分的前向算子网络来准确近似非线性EIT响应，实现快速的物理引导训练。

Result: 在16电极软传感器上的大量仿真和真实触觉实验表明，PhyDNN在重建接触形状、位置和压力分布方面始终优于NOSER、TV和标准DNN方法，产生更少伪影、更清晰边界和更高度量分数。

Conclusion: PhyDNN通过嵌入物理模型减少了深度网络的黑盒性质，提高了物理合理性和泛化能力，为高质量层析触觉传感提供了有效解决方案。

Abstract: Electrical impedance tomography (EIT) provides an attractive solution for large-area tactile sensing due to its minimal wiring and shape flexibility, but its nonlinear inverse problem often leads to severe artifacts and inaccurate contact reconstruction. This work presents PhyDNN, a physics-driven deep reconstruction framework that embeds the EIT forward model directly into the learning objective. By jointly minimizing the discrepancy between predicted and ground-truth conductivity maps and enforcing consistency with the forward PDE, PhyDNN reduces the black-box nature of deep networks and improves both physical plausibility and generalization. To enable efficient backpropagation, we design a differentiable forward-operator network that accurately approximates the nonlinear EIT response, allowing fast physics-guided training. Extensive simulations and real tactile experiments on a 16-electrode soft sensor show that PhyDNN consistently outperforms NOSER, TV, and standard DNNs in reconstructing contact shape, location, and pressure distribution. PhyDNN yields fewer artifacts, sharper boundaries, and higher metric scores, demonstrating its effectiveness for high-quality tomographic tactile sensing.

</details>


### [69] [Adaptive sampling using variational autoencoder and reinforcement learning](https://arxiv.org/abs/2512.03525)
*Adil Rasheed,Mikael Aleksander Jansen Shahly,Muhammad Faisal Aftab*

Main category: cs.LG

TL;DR: 提出自适应稀疏感知框架，结合变分自编码器先验和强化学习进行顺序测量选择，优于传统压缩感知、最优传感器布置和基于生成模型的方法。


<details>
  <summary>Details</summary>
Motivation: 传统压缩感知使用通用基和随机测量，效率和质量受限；最优传感器布置使用历史数据但固定线性基无法适应非线性或样本特异性变化；基于生成模型的压缩感知使用深度生成先验但仍采用次优随机采样。

Method: 提出自适应稀疏感知框架，将变分自编码器先验与强化学习耦合，通过强化学习顺序选择测量位置，利用生成模型作为先验知识指导采样策略。

Result: 实验表明该方法在稀疏测量下的重建性能优于传统压缩感知、最优传感器布置和基于生成模型的重建方法。

Conclusion: 自适应稀疏感知框架通过结合生成模型先验和强化学习的顺序测量选择，实现了更高效的稀疏采样和更高质量的重建。

Abstract: Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.

</details>


### [70] [Towards Irreversible Machine Unlearning for Diffusion Models](https://arxiv.org/abs/2512.03564)
*Xun Yuan,Zilong Zhao,Jiayu Li,Aryan Pasikhani,Prosanta Gope,Biplab Sikdar*

Main category: cs.LG

TL;DR: 本文提出了一种针对扩散模型遗忘攻击的新攻击方法DiMRA，以及相应的防御方法DiMUM。DiMRA能够逆转基于微调的遗忘方法，而DiMUM通过记忆替代数据来防止生成目标内容。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成合成图像方面表现出色，但存在安全、隐私和版权问题，需要机器遗忘技术来让模型忘记特定训练数据。然而，现有的基于微调的遗忘方法存在安全漏洞，可能被攻击者逆转。

Method: 1. 提出DiMRA攻击方法：在不知道遗忘元素的情况下，通过在辅助数据集上优化已遗忘的扩散模型来逆转遗忘过程，使模型重新生成先前遗忘的元素。
2. 提出DiMUM防御方法：通过记忆替代数据或特征来替换目标遗忘数据或特征，而不是简单地遗忘，从而防止生成这些元素。

Result: 1. DiMRA成功逆转了最先进的基于微调的扩散模型遗忘方法，暴露了这类技术的重大漏洞。
2. DiMUM在保持扩散模型生成性能的同时，显著增强了对DiMRA攻击的鲁棒性。

Conclusion: 现有的基于微调的扩散模型遗忘方法存在被逆转的安全风险，需要更鲁棒的解决方案。DiMUM通过记忆替代数据的策略，在保持模型性能的同时提供了更好的安全性。

Abstract: Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.

</details>


### [71] [When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate](https://arxiv.org/abs/2512.03578)
*Florent Forest,Amaury Wei,Olga Fink*

Main category: cs.LG

TL;DR: MAGNETS是一个用于时间序列外生回归任务的可解释神经网络架构，它通过学习一组可理解的概念来实现透明预测，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 时间序列外生回归任务在医疗、金融等领域需要准确预测和可信推理，但现有模型多为黑箱，难以理解驱动决策的时间模式。后验可解释技术通常产生粗糙、嘈杂或不稳定的解释，而现有的内在可解释方法需要概念监督、无法捕捉特征交互、表达能力有限且难以扩展到高维数据。

Method: 提出MAGNETS架构，通过掩码聚合网络学习一组紧凑的人类可理解概念，无需任何标注。每个概念对应基于掩码的输入特征聚合，明确揭示哪些特征驱动预测以及何时重要。预测通过透明的加法结构组合这些学习到的概念。

Result: MAGNETS能够学习紧凑的可理解概念，明确显示驱动预测的特征及其在时间序列中的重要性，提供对模型决策过程的清晰洞察。

Conclusion: MAGNETS为时间序列外生回归任务提供了一个内在可解释的解决方案，克服了现有方法的局限性，实现了透明且可理解的预测。

Abstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.
  To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.

</details>


### [72] [Optimal Transportation and Alignment Between Gaussian Measures](https://arxiv.org/abs/2512.03579)
*Sanjit Dandapanthula,Aleksandr Podkopaev,Shiva Prasad Kasiviswanathan,Aaditya Ramdas,Ziv Goldfeld*

Main category: cs.LG

TL;DR: 该论文为高斯分布下的最优传输和Gromov-Wasserstein对齐提供了全面的封闭形式解，解决了文献中的多个空白，并应用于知识蒸馏和异构聚类。


<details>
  <summary>Details</summary>
Motivation: 最优传输和Gromov-Wasserstein对齐是数据科学中比较、转换和聚合异构数据集的重要几何框架，但计算成本高。现有方法主要依赖高斯分布在二次成本下的封闭形式解，但文献中存在多个未解决的问题限制了应用范围。

Method: 1. 针对未中心化高斯分布的内积GW对齐，给出了封闭形式表达式（需在酉算子上进行二次优化），并推导了紧致的解析上下界。2. 当至少一个高斯分布中心化时，提供完全封闭形式解。3. 将高斯多边际OT简化为可处理的优化问题，并提出使用秩不足约束的高效算法。

Result: 1. 解决了可分离希尔伯特空间上未中心化高斯分布的IGW对齐开放问题。2. 为中心化高斯分布提供了完全封闭形式的IGW对齐解和IGW重心解析解。3. 实现了高斯多边际OT的高效算法。4. 在合成和真实数据集上展示了知识蒸馏和异构聚类的应用效果。

Conclusion: 该工作填补了高斯分布下OT和GW对齐理论的多个空白，提供了更广泛的封闭形式解，显著扩展了这些几何框架在数据科学和机器学习中的实际应用范围。

Abstract: Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.

</details>


### [73] [Federated Learning and Trajectory Compression for Enhanced AIS Coverage](https://arxiv.org/abs/2512.03584)
*Thomas Gräupl,Andreas Reisenbauer,Marcel Hecko,Anil Rasouli,Anita Graser,Melitta Dragaschnig,Axel Weissenfeld,Gilles Dejaegere,Mahmoud Sakr*

Main category: cs.LG

TL;DR: VesselEdge系统利用联邦学习和带宽受限轨迹压缩技术，将船舶转变为移动传感器，通过扩展AIS覆盖范围来增强海上态势感知能力。


<details>
  <summary>Details</summary>
Motivation: 解决海上AIS覆盖范围有限的问题，提高海上态势感知能力，特别是在低带宽连接环境下实现实时异常检测。

Method: 结合联邦学习（M3fed模型）和带宽受限轨迹压缩（BWC-DR-A算法），将船舶转变为移动传感器，优先传输异常数据。

Result: 初步结果表明VesselEdge系统能有效提高AIS覆盖范围和海上态势感知能力，使用历史数据验证了其有效性。

Conclusion: VesselEdge系统通过联邦学习和轨迹压缩技术，成功扩展了AIS覆盖范围，增强了海上实时异常检测和态势感知能力。

Abstract: This paper presents the VesselEdge system, which leverages federated learning and bandwidth-constrained trajectory compression to enhance maritime situational awareness by extending AIS coverage. VesselEdge transforms vessels into mobile sensors, enabling real-time anomaly detection and efficient data transmission over low-bandwidth connections. The system integrates the M3fed model for federated learning and the BWC-DR-A algorithm for trajectory compression, prioritizing anomalous data. Preliminary results demonstrate the effectiveness of VesselEdge in improving AIS coverage and situational awareness using historical data.

</details>


### [74] [Observation-driven correction of numerical weather prediction for marine winds](https://arxiv.org/abs/2512.03606)
*Matteo Peduto,Qidong Yang,Jonathan Giezendanner,Devis Tuia,Sherrie Wang*

Main category: cs.LG

TL;DR: 提出基于Transformer的深度学习架构，通过同化最新观测数据来修正全球天气预报系统(GFS)输出，实现海洋风速预测的改进


<details>
  <summary>Details</summary>
Motivation: 海洋风速预测对航行安全、船舶路径规划和能源运营至关重要，但由于海洋观测数据稀疏、异构且随时间变化，准确预测具有挑战性

Method: 将风速预测重新定义为观测信息驱动的全球数值天气预报(NWP)模型修正，使用基于Transformer的深度学习架构，通过掩码和基于集合的注意力机制处理不规则和时变的观测数据，利用交叉注意力将预测条件化于最近的观测-预报对，采用循环时间嵌入和坐标感知位置表示实现任意空间坐标的单次推理

Result: 在大西洋区域使用ICOADS观测数据评估，模型在所有48小时以内的预报时效上都降低了GFS 10米风速的RMSE，1小时预报时效改善45%，48小时预报时效改善13%。空间分析显示在海岸线和航运路线等观测数据丰富的区域改进最显著

Conclusion: 该架构自然适应异构观测平台（船舶、浮标、潮汐计和海岸站），在单次前向传递中同时产生站点特定预测和流域尺度网格产品，展示了一种实用的低延迟后处理方法，通过学习修正系统性预报误差来补充NWP

Abstract: Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.

</details>


### [75] [CoGraM: Context-sensitive granular optimization method with rollback for robust model fusion](https://arxiv.org/abs/2512.03610)
*Julius Lenz*

Main category: cs.LG

TL;DR: CoGraM是一种多阶段、上下文敏感、基于损失的迭代优化方法，用于合并神经网络而无需重新训练，解决了联邦学习中权重平均和Fisher合并等方法精度损失和不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习和分布式学习中需要在不重新训练的情况下合并神经网络，但现有方法如权重平均或Fisher合并通常会导致精度损失且在不同随机种子下不稳定。

Method: CoGraM是一种多阶段、上下文敏感、基于损失的迭代优化方法，在层、神经元和权重级别进行操作，通过决策对齐损失差异和阈值，并通过回滚机制防止有害更新。

Result: CoGraM能够显著改善合并后的网络性能，解决了Fisher等方法存在的弱点。

Conclusion: CoGraM为联邦学习和分布式学习中的神经网络合并问题提供了一种有效的优化解决方案，相比传统方法具有更好的稳定性和精度。

Abstract: Merging neural networks without retraining is central to federated and distributed learning. Common methods such as weight averaging or Fisher merging often lose accuracy and are unstable across seeds. CoGraM (Contextual Granular Merging) is a multi-stage, context-sensitive, loss-based, and iterative optimization method across layers, neurons, and weight levels that aligns decisions with loss differences and thresholds and prevents harmful updates through rollback. CoGraM is an optimization method that addresses the weaknesses of methods such as Fisher and can significantly improve the merged network.

</details>


### [76] [The promising potential of vision language models for the generation of textual weather forecasts](https://arxiv.org/abs/2512.03623)
*Edward C. C. Steele,Dinesh Mane,Emilio Monti,Luis Orus,Rebecca Chantrill-Cheyette,Matthew Couch,Kirstine I. Dale,Simon Eaton,Govindarajan Rangarajan,Amir Majlesi,Steven Ramsdale,Michael Sharpe,Craig Smith,Jonathan Smith,Rebecca Yates,Holly Ellis,Charles Ewen*

Main category: cs.LG

TL;DR: 使用视觉语言模型从视频编码的网格天气数据直接生成航运预报文本的早期探索


<details>
  <summary>Details</summary>
Motivation: 尽管多模态基础模型能力强大，但在气象产品和服务生成方面的应用仍处于起步阶段。需要加速这些技术在气象领域的应用和采纳。

Method: 探索使用视觉语言模型，直接从视频编码的网格天气数据生成标志性的航运预报文本。

Result: 早期结果显示，这种方法在提高生产效率和促进服务创新方面具有可扩展的技术机会。

Conclusion: 这项研究为气象企业及其他领域展示了有前景的技术应用方向，能够提升生产效率和推动服务创新。

Abstract: Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.

</details>


### [77] [Conditional updates of neural network weights for increased out of training performance](https://arxiv.org/abs/2512.03653)
*Jan Saynisch-Wagner,Saran Rajendran Sari*

Main category: cs.LG

TL;DR: 提出一种三步法增强神经网络在训练数据与应用数据不相似时的性能，包括重训练获取权重异常、建立预测器回归关系、外推权重到应用数据


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在训练数据与应用数据不相似时的性能问题，包括分布外问题、模式转换和机制变化等场景

Method: 三步法：1) 对训练数据集的合理子集重训练神经网络，记录权重异常；2) 选择合理预测器，建立预测器与权重异常的回归关系；3) 将权重外推到应用数据，从而外推神经网络

Result: 在气候科学的三个用例中展示了该方法，成功实现了神经网络的时间、空间和跨域外推

Conclusion: 该方法能有效增强神经网络在训练数据与应用数据不相似场景下的性能，为处理分布外问题提供了实用解决方案

Abstract: This study proposes a method to enhance neural network performance when training data and application data are not very similar, e.g., out of distribution problems, as well as pattern and regime shifts. The method consists of three main steps: 1) Retrain the neural network towards reasonable subsets of the training data set and note down the resulting weight anomalies. 2) Choose reasonable predictors and derive a regression between the predictors and the weight anomalies. 3) Extrapolate the weights, and thereby the neural network, to the application data. We show and discuss this method in three use cases from the climate sciences, which include successful temporal, spatial and cross-domain extrapolations of neural networks.

</details>


### [78] [Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting](https://arxiv.org/abs/2512.03656)
*Salim Khazem,Houssam Kanso*

Main category: cs.LG

TL;DR: 提出一个结合循环时间编码与混合LSTM-CNN架构的统一深度学习框架，用于多步能源预测，通过实验验证了其在多个预测时间窗口上的性能提升。


<details>
  <summary>Details</summary>
Motivation: 准确的电力消费预测对于需求管理和智能电网运营至关重要。现有方法在同时考虑循环时间编码、日历特征和混合集成架构方面存在不足，需要统一的短期能源预测框架。

Method: 使用正弦余弦编码系统性地转换基于日历的属性以保持周期结构；采用包含LSTM、CNN和针对每个预测时间窗口专门设计的MLP回归器元学习器的集成模型；使用一年的全国消费数据集进行实验，包括消融分析和与基准方法的比较。

Result: 在所有七个预测时间窗口上都取得了持续的改进，混合模型在RMSE和MAE指标上优于单个架构和先前方法，验证了循环时间表示与互补深度学习结构结合的优势。

Conclusion: 结合循环时间编码与混合深度学习架构能有效提升短期能源预测性能，这是首个在统一框架中同时评估时间编码、日历特征和混合集成架构的工作。

Abstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.

</details>


### [79] [Dynamically Scaled Activation Steering](https://arxiv.org/abs/2512.03661)
*Alex Ferrando,Xavier Suau,Jordi Gonzàlez,Pau Rodriguez*

Main category: cs.LG

TL;DR: DSAS是一种动态缩放激活引导的框架，能够根据输入内容自适应调整引导强度，在需要时进行强干预，不需要时保持模型性能，实现毒性与效用的更好平衡。


<details>
  <summary>Details</summary>
Motivation: 现有激活引导方法对所有输入统一应用干预，导致在不需要引导时模型性能下降。需要一种能自适应判断何时引导以及如何引导的方法。

Method: DSAS将"何时引导"与"如何引导"解耦，通过计算上下文相关的缩放因子，动态调整任何现有引导方法在各层和输入上的强度。支持端到端联合优化，计算开销小。

Result: DSAS与现有引导方法结合能持续改进Pareto前沿，在毒性缓解和效用保持间实现更好平衡。在文本到图像扩散模型中也能有效调制特定概念，同时提高可解释性。

Conclusion: DSAS提供了一种方法无关的动态引导框架，能自适应调整引导强度，在保持模型性能的同时有效控制不良行为，具有通用性和实用性。

Abstract: Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.

</details>


### [80] [Feature-aware Modulation for Learning from Temporal Tabular Data](https://arxiv.org/abs/2512.03678)
*Hao-Run Cai,Han-Jia Ye*

Main category: cs.LG

TL;DR: 提出一种特征感知的时间调制机制，通过调节特征的统计属性来对齐跨时间的特征语义，从而平衡表格数据中的泛化性和适应性


<details>
  <summary>Details</summary>
Motivation: 表格机器学习在现实部署中面临时间分布偏移的挑战，静态模型假设固定映射以确保泛化，而自适应模型可能过度拟合瞬态模式，需要在鲁棒性和适应性之间取得平衡

Method: 提出特征感知的时间调制机制，基于时间上下文调节特征表示，调制统计属性如尺度和偏度，通过对齐跨时间的特征语义实现轻量级但强大的适应

Result: 基准评估验证了该方法在处理表格数据时间偏移方面的有效性

Conclusion: 通过分析特征语义演化和特征转换策略，提出的特征感知时间调制机制能够有效平衡泛化性和适应性，处理表格数据中的时间分布偏移问题

Abstract: While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics-particularly objective and subjective meanings-introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data.

</details>


### [81] [Quantum Topological Graph Neural Networks for Detecting Complex Fraud Patterns](https://arxiv.org/abs/2512.03696)
*Mohammad Doost,Mohammad Manthouri*

Main category: cs.LG

TL;DR: QTGNN框架结合量子嵌入、变分图卷积和拓扑数据分析，用于大规模金融网络中的欺诈交易检测，在NISQ设备上实现稳定训练并提供可解释的决策。


<details>
  <summary>Details</summary>
Motivation: 传统欺诈检测方法难以捕捉大规模金融网络中复杂的交易动态和结构异常，需要结合量子机器学习、图理论和拓扑分析的新方法。

Method: 1) 量子数据嵌入与纠缠增强；2) 具有非线性动态的变分量子图卷积；3) 高阶拓扑不变量提取；4) 自适应优化的混合量子-经典异常学习；5) 基于拓扑归因的可解释决策；6) NISQ硬件优化（电路简化和图采样）。

Result: 在PaySim和Elliptic等金融数据集上，QTGNN在ROC-AUC、精确度和误报率等指标上优于经典和量子基线方法，消融研究验证了量子嵌入、拓扑特征、非线性通道和混合学习的贡献。

Conclusion: QTGNN为金融欺诈检测提供了理论严谨、可解释且实用的解决方案，成功桥接了量子机器学习、图理论和拓扑分析，适用于NISQ设备并具有良好的扩展性。

Abstract: We propose a novel QTGNN framework for detecting fraudulent transactions in large-scale financial networks. By integrating quantum embedding, variational graph convolutions, and topological data analysis, QTGNN captures complex transaction dynamics and structural anomalies indicative of fraud. The methodology includes quantum data embedding with entanglement enhancement, variational quantum graph convolutions with non-linear dynamics, extraction of higher-order topological invariants, hybrid quantum-classical anomaly learning with adaptive optimization, and interpretable decision-making via topological attribution. Rigorous convergence guarantees ensure stable training on noisy intermediate-scale quantum (NISQ) devices, while stability of topological signatures provides robust fraud detection. Optimized for NISQ hardware with circuit simplifications and graph sampling, the framework scales to large transaction networks. Simulations on financial datasets, such as PaySim and Elliptic, benchmark QTGNN against classical and quantum baselines, using metrics like ROC-AUC, precision, and false positive rate. An ablation study evaluates the contributions of quantum embeddings, topological features, non-linear channels, and hybrid learning. QTGNN offers a theoretically sound, interpretable, and practical solution for financial fraud detection, bridging quantum machine learning, graph theory, and topological analysis.

</details>


### [82] [Unlocking the Invisible Urban Traffic Dynamics under Extreme Weather: A New Physics-Constrained Hamiltonian Learning Algorithm](https://arxiv.org/abs/2512.03744)
*Xuhui Lin,Qiuchen Lu*

Main category: cs.LG

TL;DR: 提出一种物理约束的哈密顿学习算法，通过"结构不可逆性检测"和"能量景观重构"来区分城市交通系统的真实恢复与虚假恢复，揭示传统指标无法检测的隐藏结构损伤。


<details>
  <summary>Details</summary>
Motivation: 当前城市交通系统韧性评估方法依赖表面恢复指标，无法检测隐藏的结构损伤，无法区分真实恢复与"虚假恢复"（交通指标正常化但系统动力学永久退化）。

Method: 开发物理约束的哈密顿学习算法，结合结构不可逆性检测和能量景观重构：提取低维状态表示，通过物理约束优化识别准哈密顿结构，通过能量景观比较量化结构变化。

Result: 应用于伦敦2021年极端降雨事件分析，发现表面指标完全恢复时，算法检测到64.8%的结构损伤被传统监测方法遗漏。

Conclusion: 该框架为主动结构风险评估提供工具，使基础设施投资能够基于真实的系统健康状况而非误导性的表面指标。

Abstract: Urban transportation systems face increasing resilience challenges from extreme weather events, but current assessment methods rely on surface-level recovery indicators that miss hidden structural damage. Existing approaches cannot distinguish between true recovery and "false recovery," where traffic metrics normalize, but the underlying system dynamics permanently degrade. To address this, a new physics-constrained Hamiltonian learning algorithm combining "structural irreversibility detection" and "energy landscape reconstruction" has been developed. Our approach extracts low-dimensional state representations, identifies quasi-Hamiltonian structures through physics-constrained optimization, and quantifies structural changes via energy landscape comparison. Analysis of London's extreme rainfall in 2021 demonstrates that while surface indicators were fully recovered, our algorithm detected 64.8\% structural damage missed by traditional monitoring. Our framework provides tools for proactive structural risk assessment, enabling infrastructure investments based on true system health rather than misleading surface metrics.

</details>


### [83] [Universally Converging Representations of Matter Across Scientific Foundation Models](https://arxiv.org/abs/2512.03750)
*Sathya Edamadaka,Soojung Yang,Ju Li,Rafael Gómez-Bombarelli*

Main category: cs.LG

TL;DR: 研究发现近60个科学模型（涵盖字符串、图、3D原子和蛋白质等不同模态）在化学系统上学习到的表示高度对齐，表明基础模型学习到了物理现实的共同底层表示，但当前模型仍受限于训练数据和归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 不同模态和架构的机器学习模型被用于预测分子、材料和蛋白质行为，但尚不清楚它们是否学习到相似的内部表示。理解其潜在结构对于构建能够可靠泛化到训练域之外的科学基础模型至关重要。

Method: 分析了近60个科学模型（包括字符串、图、3D原子和蛋白质等不同模态）的表示对齐情况，研究了在不同化学系统上的表示相似性，并考察了模型在不同输入条件下的表示行为。

Result: 1）不同数据集训练的模型对小分子有高度相似的表示；2）机器学习原子间势能模型在性能提升时表示空间收敛；3）发现两种不同的表示机制：在类似训练输入时，高性能模型对齐紧密而弱模型在表示空间中发散；在完全不同结构时，几乎所有模型都坍缩到低信息表示。

Conclusion: 表示对齐可作为科学基础模型泛化能力的定量基准。当前模型仍受限于训练数据和归纳偏置，尚未编码真正的通用结构。该工作可追踪模型扩展时通用表示的出现，并帮助选择跨模态、物质域和科学任务转移性最好的模型。

Abstract: Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.

</details>


### [84] [Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition](https://arxiv.org/abs/2512.03755)
*Stephen Law,Tao Yang,Nanjiang Chen,Xuhui Lin*

Main category: cs.LG

TL;DR: 提出条件轨迹编码器，联合学习空间与运动表示，解决轨迹分析中时空分离、方向不对称和过度依赖辅助数据的问题，量化城市形态造成的认知不平等。


<details>
  <summary>Details</summary>
Motivation: 当前城市轨迹分析方法存在三个主要问题：1) 空间与时间表示分离训练；2) 忽略导航方向不对称性（A→B ≠ B→A）；3) 过度依赖POI、图像等辅助数据而非城市空间基本几何特性。需要一种能联合学习时空表示并保持方向不对称性的方法。

Method: 提出条件轨迹编码器框架，使用双向LSTM处理可见性比率和曲率等几何特征，通过可学习的起点嵌入进行条件化。将表示分解为共享的城市模式和起点特定特征，采用对比学习方法。在六个合成城市和北京西城区进行验证。

Result: 实验证明城市形态会造成系统性认知不平等。该方法能够量化不同起点的认知不对称性，为城市规划和建筑设计提供评估体验公平性的工具。

Conclusion: 该框架为城市规划者提供了评估体验公平性的量化工具，为建筑师提供了布局决策认知影响的洞察，并为导航系统实现了起点感知的分析能力。

Abstract: Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation ($A \to B \ne B \to A$), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.

</details>


### [85] [Forensic Activity Classification Using Digital Traces from iPhones: A Machine Learning-based Approach](https://arxiv.org/abs/2512.03786)
*Conor McCarthy,Jan Peter van Zandwijk,Marcel Worring,Zeno Geradts*

Main category: cs.LG

TL;DR: 基于智能手机和智能手表运动传感器数据，开发机器学习方法将数字痕迹转化为不同身体活动的似然比，用于法医调查中活动识别和活动时间线重建。


<details>
  <summary>Details</summary>
Motivation: 智能手机和智能手表在日常生活中的普及提供了丰富的用户行为信息，特别是手机内置运动传感器产生的数字痕迹，为法医调查人员了解个人身体活动提供了机会。

Method: 提出基于机器学习的方法，将数字痕迹转化为不同身体活动的似然比(LR)，使用新数据集NFI_FARED（包含四种iPhone的19种活动标记数据），并扩展方法同时分析多个活动或活动组，创建活动时间线。

Result: 在171种可能的活动配对中，该方法能够为167种配对生成有用的似然比系统，成功区分不同活动类型，并能够创建活动时间线辅助法医调查。

Conclusion: 该方法能够有效利用智能手机传感器数据进行法医活动识别，公开数据集和代码以促进该领域进一步研究，为法医调查的早期和后期阶段提供支持。

Abstract: Smartphones and smartwatches are ever-present in daily life, and provide a rich source of information on their users' behaviour. In particular, digital traces derived from the phone's embedded movement sensors present an opportunity for a forensic investigator to gain insight into a person's physical activities. In this work, we present a machine learning-based approach to translate digital traces into likelihood ratios (LRs) for different types of physical activities. Evaluating on a new dataset, NFI\_FARED, which contains digital traces from four different types of iPhones labelled with 19 activities, it was found that our approach could produce useful LR systems to distinguish 167 out of a possible 171 activity pairings. The same approach was extended to analyse likelihoods for multiple activities (or groups of activities) simultaneously and create activity timelines to aid in both the early and latter stages of forensic investigations. The dataset and all code required to replicate the results have also been made public to encourage further research on this topic.

</details>


### [86] [Adaptive Identification and Modeling of Clinical Pathways with Process Mining](https://arxiv.org/abs/2512.03787)
*Francesco Vitale,Nicola Mazzocca*

Main category: cs.LG

TL;DR: 提出基于过程挖掘的两阶段临床路径建模方法，利用一致性检查扩展临床路径知识库，针对疾病变体或组合创建更具体的模型。


<details>
  <summary>Details</summary>
Motivation: 临床路径的手动建模困难且难以反映不同疾病变体或组合的最佳实践，需要自动化的方法来扩展和优化临床路径知识库。

Method: 两阶段过程挖掘方法：第一阶段从历史数据中提取治疗过程模型；第二阶段将新数据与参考模型进行一致性检查，基于检查结果扩展知识库，为新的疾病变体或组合创建更具体的模型。

Result: 使用Synthea基准数据集（模拟SARS-CoV-2感染及COVID-19并发症）验证，方法能以95.62%的AUC精度扩展临床路径知识库，同时保持67.11%的弧度简单性。

Conclusion: 提出的两阶段过程挖掘方法能有效扩展临床路径知识库，为不同疾病变体和组合创建更具体的模型，提高临床路径的适应性和实用性。

Abstract: Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.

</details>


### [87] [EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification](https://arxiv.org/abs/2512.03804)
*Hanhui Deng,Xinglin Li,Jie Luo,Zhanpeng Jin,Di Wu*

Main category: cs.LG

TL;DR: 提出基于EfficientNet的轻量级ECG分类模型EfficientECG，以及跨注意力多特征融合模型，用于准确快速的心电图诊断


<details>
  <summary>Details</summary>
Motivation: 心电图作为快速、无创且信息丰富的诊断信号，在心脏异常检测中有广泛应用。现有ECG模型误诊率高，需要开发能够自动提取特征、准确快速诊断的深度学习模型，减轻医疗工作者负担。

Method: 1. 基于EfficientNet设计EfficientECG模型，用于处理高频长序列多导联ECG数据；2. 提出跨注意力特征融合模型，整合多导联ECG数据及患者性别、年龄等多特征信息。

Result: 在代表性ECG数据集上的评估表明，所提模型在精度、多特征融合能力和轻量化方面优于现有最先进方法。

Conclusion: 提出的EfficientECG及其跨注意力特征融合模型能够有效处理ECG数据，实现准确快速的诊断，为心电图分析提供了高效的深度学习解决方案。

Abstract: Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.

</details>


### [88] [Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($λ$,$λ$))-GA](https://arxiv.org/abs/2512.03805)
*Tai Nguyen,Phong Le,André Biedenkapp,Carola Doerr,Nguyen Dang*

Main category: cs.LG

TL;DR: 本文系统研究深度强化学习在动态算法配置中的应用，针对(1+(λ,λ))-GA算法在OneMax问题上的种群规模参数控制，发现DDQN和PPO存在可扩展性下降和学习不稳定两大挑战，并提出自适应奖励偏移机制解决探索不足问题。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在动态算法配置中展现出潜力，但实际应用面临挑战且需要大量领域专业知识。本文旨在通过系统研究深度强化学习算法在DAC中的表现，识别关键问题并提供解决方案。

Method: 以控制(1+(λ,λ))-GA在OneMax实例上的种群规模参数为案例，系统分析DDQN和PPO算法。针对发现的探索不足问题，提出自适应奖励偏移机制；针对规划视野覆盖问题，采用无折扣学习。同时分析PPO的超参数依赖性。

Result: 发现DDQN和PPO存在可扩展性下降和学习不稳定两大挑战，主要源于探索不足和规划视野覆盖问题。自适应奖励偏移机制能有效增强DDQN探索，无需实例特定超参数调优。DDQN配合该策略达到与理论推导策略相当的性能，样本效率提升数个数量级。

Conclusion: 深度强化学习在DAC中面临可扩展性和稳定性挑战，但通过针对性解决方案（如自适应奖励偏移和无折扣学习）可有效克服。DDQN配合自适应奖励偏移策略在DAC中表现优异，显著优于现有方法，为强化学习在算法配置中的应用提供了实用指导。

Abstract: Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($λ$,$λ$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.

</details>


### [89] [Log Probability Tracking of LLM APIs](https://arxiv.org/abs/2512.03816)
*Timothée Chauvin,Erwan Le Merrer,François Taïani,Gilles Tredan*

Main category: cs.LG

TL;DR: 提出一种基于logprobs的廉价LLM API监控方法，仅需单个token输出即可检测微小模型变化，比现有方法敏感1000倍


<details>
  <summary>Details</summary>
Motivation: LLM API提供商需要保持模型一致性以确保下游应用可靠性和研究可复现性，但现有审计方法成本过高无法定期监控广泛可用的LLM API

Method: 利用LLM log probabilities的统计特性，基于每个token logprob的平均值设计简单统计测试，仅需请求单个token输出

Result: 该方法能检测小至单步微调的模型变化，比现有方法更敏感且成本降低1000倍，并引入TinyChange基准评估审计方法灵敏度

Conclusion: logprobs可作为经济有效的LLM API连续监控基础，为模型更新提供实用且敏感的检测方案

Abstract: When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.

</details>


### [90] [Transmit Weights, Not Features: Orthogonal-Basis Aided Wireless Point-Cloud Transmission](https://arxiv.org/abs/2512.03819)
*Junlin Chang,Yubo Han,Hnag Yue,John S Thompson,Rongke Liu*

Main category: cs.LG

TL;DR: 提出基于深度联合信源信道编码的语义无线传输框架，通过预测接收端语义正交特征池的组合权重实现紧凑表示和鲁棒重建，在带宽受限场景下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着深度传感器的普及，点云获取门槛降低，但传统传输方法在带宽受限时性能不佳。需要一种能实现紧凑表示和鲁棒重建的语义传输框架。

Method: 基于DeepJSCC的语义无线传输框架：1）发送端预测接收端语义正交特征池的组合权重而非原始特征；2）采用折叠式解码器将2D网格变形为3D点云，保持流形连续性和几何保真度；3）使用Chamfer距离和正交正则化器进行训练。

Result: 在ModelNet40数据集上测试不同信噪比和带宽：1）高带宽时性能与SEPT相当；2）带宽受限时明显优于SEPT；3）PSNR和CD指标均有持续改进；4）消融实验验证正交化和折叠先验的有效性。

Conclusion: 提出的语义传输框架通过正交特征池和折叠解码器，在保持几何保真度的同时实现了高效的点云传输，特别在带宽受限场景下表现出显著优势。

Abstract: The widespread adoption of depth sensors has substantially lowered the barrier to point-cloud acquisition. This letter proposes a semantic wireless transmission framework for three dimension (3D) point clouds built on Deep Joint Source - Channel Coding (DeepJSCC). Instead of sending raw features, the transmitter predicts combination weights over a receiver-side semantic orthogonal feature pool, enabling compact representations and robust reconstruction. A folding-based decoder deforms a 2D grid into 3D, enforcing manifold continuity while preserving geometric fidelity. Trained with Chamfer Distance (CD) and an orthogonality regularizer, the system is evaluated on ModelNet40 across varying Signal-to-Noise Ratios (SNRs) and bandwidths. Results show performance on par with SEmantic Point cloud Transmission (SEPT) at high bandwidth and clear gains in bandwidth-constrained regimes, with consistent improvements in both Peak Signal-to-Noise Ratio (PSNR) and CD. Ablation experiments confirm the benefits of orthogonalization and the folding prior.

</details>


### [91] [DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training](https://arxiv.org/abs/2512.03847)
*Dingwei Zhu,Zhiheng Xi,Shihan Dou,Yuhui Wang,Sixian Li,Junjie Ye,Honglin Guo,Shichun Liu,Chenhao Huang,Yajie Yang,Junlin Shang,Senjie Jin,Ming Zhang,Jiazheng Zhang,Caishuang Huang,Yunke Zhang,Demei Yan,Yuran Wang,Tao Gui*

Main category: cs.LG

TL;DR: DVPO是一个结合条件风险理论和分布价值建模的RL框架，通过学习token级价值分布和应用非对称风险正则化，在噪声监督下平衡鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的LLM后训练常面临噪声或不完整的监督信号，现有方法（如RFQI、CQL、PPO、GRPO）要么忽略泛化性，要么产生过于保守的策略，导致在不同真实场景中表现不均。

Method: DVPO结合条件风险理论和分布价值建模：1）学习token级价值分布以提供细粒度监督；2）应用非对称风险正则化来塑造分布尾部：收缩下尾以抑制噪声负偏差，扩展上尾以保持探索多样性。

Result: 在多轮对话、数学推理和科学QA的广泛实验中，DVPO在噪声监督下始终优于PPO、GRPO和基于鲁棒Bellman的PPO，展示了其在现实世界LLM后训练中的潜力。

Conclusion: DVPO通过分布价值建模和风险感知策略优化，有效平衡了鲁棒性和泛化性，为现实世界噪声监督下的LLM后训练提供了有前景的解决方案。

Abstract: Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.

</details>


### [92] [Scalable Decision Focused Learning via Online Trainable Surrogates](https://arxiv.org/abs/2512.03861)
*Gaetano Signorelli,Michele Lombardi*

Main category: cs.LG

TL;DR: 提出一种基于无偏估计器的替代方法，加速决策聚焦学习，减少昂贵求解器调用，同时保持解质量


<details>
  <summary>Details</summary>
Motivation: 传统训练的估计器在决策支持系统中可能导致次优解，而决策聚焦学习虽然能解决此问题，但训练时计算成本高、可扩展性差

Method: 使用无偏估计器构建高效的替代损失函数，替代昂贵的实际决策成本评估；该方法适用于黑盒设置，能补偿优化模型简化并考虑补救行动

Result: 该方法显著减少了昂贵的内部求解器调用次数，同时解质量与其他最先进技术相当

Conclusion: 提出的加速方法有效解决了决策聚焦学习的可扩展性问题，通过无偏替代损失函数在保持解质量的同时大幅降低计算成本

Abstract: Decision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques.

</details>


### [93] [Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment](https://arxiv.org/abs/2512.03864)
*Danny Hoang,Anandkumar Patel,Ruimen Chen,Rajiv Malhotra,Farhad Imani*

Main category: cs.LG

TL;DR: 该研究比较了智能加工中AI模型的能耗，提出超维计算（HDC）作为替代方案，在保持精度的同时大幅降低能耗和计算时间。


<details>
  <summary>Details</summary>
Motivation: 智能制造虽然能提高效率和降低能耗，但AI模型的高能耗可能抵消这些优势。需要寻找既能保持预测精度又能显著降低能耗的AI解决方案。

Method: 使用基于原位传感的几何质量预测来比较常见AI模型的能耗、精度和速度。引入超维计算（HDC）作为替代方案，并与传统模型进行对比分析。

Result: HDC在精度上与常规模型相当，但能耗大幅降低：训练能耗降低200倍，推理能耗降低175-1000倍。训练时间减少200倍，推理时间减少300-600倍。

Conclusion: 超维计算（HDC）在智能加工中展现出巨大潜力，能够在保持预测精度的同时显著降低能耗和计算时间，为实现节能的智能制造提供了有前景的解决方案。

Abstract: Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\times$ for training and 175 to 1000$\times$ for inference. Furthermore, HDC reduces training times by 200$\times$ and inference times by 300 to 600$\times$, showcasing its potential for energy-efficient smart manufacturing.

</details>


### [94] [Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models](https://arxiv.org/abs/2512.03882)
*Haidong Kang,Wei Wu,Hanling Wang*

Main category: cs.LG

TL;DR: 本文提出ACraft方法，利用大语言模型自动生成针对少样本类增量学习(FSCIL)的攻击方法，无需人工专家参与，显著降低了攻击成本并提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: FSCIL是持续学习中更具现实性和挑战性的范式，但现有研究主要关注提升FSCIL方法效果，而忽视了其安全问题。传统人工设计的攻击方法要么对基类攻击失败，要么依赖大量专家知识导致成本高昂，因此需要专门针对FSCIL的攻击方法。

Method: 提出ACraft方法：1) 利用大语言模型自动发现针对FSCIL的最优攻击方法；2) 引入基于近端策略优化的强化学习来优化LLMs与FSCIL之间的推理，通过建立正反馈使LLMs在下一代生成更好的攻击方法。

Result: 在主流基准测试中，ACraft方法显著降低了最先进FSCIL方法的性能，效果远超人工专家设计的攻击方法，同时保持了最低的攻击成本。

Conclusion: ACraft方法为FSCIL安全问题提供了有效的自动化攻击解决方案，通过LLMs和强化学习的结合，实现了无需人工专家参与的高效攻击生成，对FSCIL的安全性研究具有重要意义。

Abstract: Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.

</details>


### [95] [Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations](https://arxiv.org/abs/2512.03923)
*Xiang Rao,Yina Liu,Yuxuan Shen*

Main category: cs.LG

TL;DR: 提出一种离散变量量子-经典物理信息神经网络（DV-QCPINN），首次应用于三种典型油藏渗流模型，相比传统PINNs参数更少、精度更高。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法存在网格依赖误差和计算成本高的问题，经典物理信息神经网络（PINNs）在参数效率、高维表达和强非线性拟合方面存在瓶颈，需要新的解决方案。

Method: 提出离散变量量子-经典物理信息神经网络（DV-QCPINN），将经典预处理/后处理网络与离散变量量子核心集成，利用量子叠加和纠缠增强高维特征映射，同时嵌入物理约束确保解的一致性。测试了三种量子电路拓扑结构（级联、交叉网格、交替）。

Result: QCPINNs比经典PINNs使用更少参数实现高预测精度。在异质单相流和两相BL方程模拟中，交替拓扑表现最佳；在考虑吸附的对流-扩散-吸附耦合的组分流中，级联拓扑表现最优。

Conclusion: 验证了QCPINN在油藏工程应用中的可行性，弥合了量子计算研究与油气工程工业实践之间的差距。

Abstract: Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.

</details>


### [96] [Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization](https://arxiv.org/abs/2512.03928)
*Michele Alessi,Alessio Ansuini,Alex Rodriguez*

Main category: cs.LG

TL;DR: DiVAE是一种轻量级VAE正则化器，通过将VAE对数先验概率与数据估计的对数密度对齐，改善潜在空间分布对齐、先验覆盖和OOD不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 标准VAE将潜在变量匹配到简单先验分布，忽略了数据空间中的密度结构。这导致编码器无法根据数据空间密度合理分配后验质量，先验分布也无法反映数据的高密度区域。

Method: DiVAE在ELBO中添加了一个稳健的、精度加权的惩罚项，将VAE的对数先验概率与从数据估计的对数密度对齐。该方法鼓励编码器根据数据空间密度按比例分配后验质量，并在先验可学习时推动先验向高密度区域靠拢。

Result: 在合成数据集上，DiVAE：(i) 改善了潜在对数密度与其实对应物的分布对齐；(ii) 提高了先验覆盖；(iii) 获得了更好的OOD不确定性校准。在MNIST上，DiVAE改善了先验与外部密度估计的对齐，提供了更好的可解释性，并提高了可学习先验的OOD检测性能。

Conclusion: DiVAE是一种计算开销可忽略的轻量级正则化方法，通过将VAE先验与数据密度对齐，显著改善了潜在空间分布对齐、先验覆盖和OOD性能，为VAE提供了更好的可解释性和不确定性校准能力。

Abstract: We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.

</details>


### [97] [Technical Report on Text Dataset Distillation](https://arxiv.org/abs/2512.03967)
*Keith Ando Ogawa,Bruno Lopes Yamamoto,Lucas Lauton de Alcantara,Victor Zacarias,Edson Bollis,Lucas Pellicer,Rosimeire Pereira Costa,Anna Helena Reali Costa,Artur Jordao*

Main category: cs.LG

TL;DR: 本文综述了文本数据集蒸馏领域的发展历程，从最初借鉴视觉领域方法到形成独立研究方向，涵盖关键里程碑、现有挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 文本数据集蒸馏研究相对视觉领域较少，但文本模态的特殊性（如离散性）带来了独特挑战，需要专门的研究方法和评估标准。

Method: 综述性研究，系统回顾文本数据集蒸馏的发展历程，包括：1）从视觉方法迁移到独立分支；2）使用Transformer模型的方法；3）生成离散合成文本的技术；4）扩展到10亿参数以上的解码器模型。

Result: 识别了文本数据集蒸馏的关键里程碑：Transformer模型应用、离散合成文本生成、大规模解码器模型扩展。同时指出了领域仍处于成熟阶段，存在多个改进空间。

Conclusion: 文本数据集蒸馏领域虽取得重要进展，但在基准标准化、处理文本离散性、复杂任务处理、实际应用示例等方面仍有改进空间，需要进一步研究和发展。

Abstract: In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.

</details>


### [98] [Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning](https://arxiv.org/abs/2512.03973)
*Franki Nguimatsia Tiofack,Théotime Le Hellard,Fabian Schramm,Nicolas Perrin-Gilbert,Justin Carpentier*

Main category: cs.LG

TL;DR: GFP通过结合多步流匹配策略和蒸馏一步行动器，区分数据集中的高价值和低价值动作，在离线强化学习中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 传统离线强化学习中的行为正则化方法对所有状态-动作对进行无差别模仿，无法区分高价值和低价值动作，限制了策略性能

Method: 提出引导流策略(GFP)，耦合多步流匹配策略和蒸馏一步行动器。行动器通过加权行为克隆指导流策略专注于克隆数据集中的高价值动作，而流策略则约束行动器保持与数据集最佳转移对齐的同时最大化批评者

Result: 在OGBench、Minari和D4RL基准测试的144个状态和像素任务中实现最先进性能，在次优数据集和挑战性任务上取得显著提升

Conclusion: GFP通过行动器和流策略的相互引导机制，有效区分数据集中的高价值和低价值动作，解决了传统行为正则化的局限性，在离线强化学习中实现了卓越性能

Abstract: Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/

</details>


### [99] [Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs](https://arxiv.org/abs/2512.03994)
*Oren Rachmil,Roy Betser,Itay Gershon,Omer Hofman,Nitay Yakoby,Yuval Meron,Idan Yankelev,Asaf Shabtai,Yuval Elovici,Roman Vainshtein*

Main category: cs.LG

TL;DR: 提出一种无需训练的高效方法，将策略违规检测视为分布外检测问题，通过白化技术处理隐藏激活，使用欧几里得范数作为合规分数，在策略基准上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在敏感领域（法律、金融、医疗）的部署，组织需要可靠的机制来检测内部策略违规，现有方法（护栏、LLM-as-a-judge、微调）存在延迟高、可解释性差、缺乏鲁棒性等问题。

Method: 将策略违规检测视为分布外检测问题，采用白化技术对模型隐藏激活进行线性变换（去相关、标准化为零均值和单位方差），在变换空间中使用欧几里得范数作为合规分数，仅需策略文本和少量示例样本。

Result: 在具有挑战性的策略基准测试中取得最先进的结果，超越了现有护栏和微调推理模型，方法轻量且易于部署。

Conclusion: 为组织提供了一个实用且统计基础的政策感知监督框架，推进了可部署AI治理的广泛目标。

Abstract: Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection

</details>


### [100] [Physics-Embedded Gaussian Process for Traffic State Estimation](https://arxiv.org/abs/2512.04004)
*Yanlin Chen,Kehua Chen,Yinhai Wang*

Main category: cs.LG

TL;DR: 提出PEGP框架，将经典交通流模型嵌入高斯过程中，通过线性化微分算子构建多输出核函数，在稀疏观测下提升交通状态估计的可靠性


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：纯数据驱动方法缺乏物理解释且在稀疏数据下泛化能力差；物理模型难以整合不确定性和捕捉真实交通复杂性；现有结合方法依赖惩罚调参且缺乏原则性不确定性校准

Method: 提出物理嵌入高斯过程(PEGP)框架，基于经典交通流模型(LWR和ARZ)设计两种多输出核函数，通过线性化微分算子的显式应用构建，将物理结构作为硬约束而非软约束

Result: 在HighD和NGSIM数据集上实验显示：PEGP-ARZ在稀疏观测下更可靠，PEGP-LWR在密集观测下误差更低；消融研究表明PEGP-ARZ残差与物理模型更一致且产生可解释的不确定性，PEGP-LWR残差更正交且产生近似恒定方差场

Conclusion: PEGP框架成功整合物理先验和不确定性量化，为交通状态估计提供可靠支持，解决了现有方法对模型误设敏感和缺乏原则性不确定性校准的问题

Abstract: Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.

</details>


### [101] [Efficient Public Verification of Private ML via Regularization](https://arxiv.org/abs/2512.04008)
*Zoë Ruha Bell,Anvith Thudi,Olive Franzese-McLaughlin,Nicolas Papernot,Shafi Goldwasser*

Main category: cs.LG

TL;DR: 提出首个差分隐私算法，其隐私保证验证成本远低于训练成本，在DP-SCO中实现近最优的隐私-效用权衡


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私算法的验证成本与训练成本相当，数据提供者和公众缺乏高效验证DP保证的方法，需要降低验证成本

Method: 通过私有化最小化一系列正则化目标函数，仅使用标准DP组合边界，在DP-SCO中实现近最优隐私-效用权衡

Result: 获得了紧致的隐私-效用权衡，且验证成本显著低于训练成本，特别是对于大型数据集验证成本大幅降低

Conclusion: 首次设计了验证成本低于训练成本的DP-SCO算法，为实际部署中的DP验证提供了实用解决方案

Abstract: Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.

</details>


### [102] [Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions](https://arxiv.org/abs/2512.04034)
*Hong Yang,Devroop Kar,Qi Yu,Alex Ororbia,Travis Desell*

Main category: cs.LG

TL;DR: 该论文从信息论角度解释了为什么在单域数据集上训练的模型会出现OOD检测灾难性失败，提出了"域特征坍缩"理论，并通过域过滤方法验证了理论。


<details>
  <summary>Details</summary>
Motivation: 解释为什么最先进的OOD检测方法在单域数据集训练时会表现出灾难性失败，这一现象在现有文献中缺乏理论解释。

Method: 1. 从信息论角度分析，证明单域监督学习必然导致域特征坍缩(I(x_d; z)=0)；2. 使用Fano不等式量化实际场景中的部分坍缩；3. 构建Domain Bench基准测试；4. 通过域过滤方法(使用预训练表示)保持I(x_d; z)>0来验证理论。

Result: 1. 理论证明单域训练会导致模型完全丢弃域特定信息；2. 在MNIST等数据集上OOD检测性能极差(仅53% FPR@95)；3. 域过滤方法能有效解决该失败模式，为理论提供实证支持。

Conclusion: 单域监督学习存在根本性限制，会导致域特征坍缩，从而无法检测域外样本。该工作不仅解释了令人困惑的经验现象，还对迁移学习以及何时微调与冻结预训练模型具有更广泛的意义。

Abstract: Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.

</details>


### [103] [MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking](https://arxiv.org/abs/2512.04044)
*Yizhou Zhao,Zhiwei Steven Wu,Adam Block*

Main category: cs.LG

TL;DR: MarkTune：一种基于策略的微调框架，通过将水印信号作为奖励进行优化，在保持文本质量的同时提高水印检测能力，解决了开源模型水印的质量-可检测性权衡问题。


<details>
  <summary>Details</summary>
Motivation: 开源语言模型对水印技术提出了严峻挑战，因为一旦模型权重公开，就无法强制执行推理时干预。现有技术如GaussMark通过微调权重嵌入水印，但通常需要在检测能力和生成质量之间进行权衡。

Method: 提出MarkTune框架，将GaussMark水印信号作为奖励函数，同时通过正则化防止文本质量下降。该方法在模型的表示空间中进行更精细的、水印感知的权重更新。

Result: MarkTune显著改善了GaussMark的质量-可检测性权衡，接近推理时水印的性能水平。对转述和微调攻击具有鲁棒性，并在未见数据集上表现出良好的泛化能力。

Conclusion: MarkTune为开源语言模型嵌入鲁棒、高质量水印提供了一种通用策略，在保持生成质量的同时实现了强大的水印检测能力。

Abstract: Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.

</details>


### [104] [Convergence for Discrete Parameter Updates](https://arxiv.org/abs/2512.04051)
*Paul Wilson,Fabio Zanasi,George Constantinides*

Main category: cs.LG

TL;DR: 提出一种离散更新规则的量化训练方法，避免传统量化训练中对连续更新的离散化，为具有固有离散结构的模型开辟了新的高效训练途径。


<details>
  <summary>Details</summary>
Motivation: 现代深度学习模型需要巨大的计算资源，这推动了低精度训练的研究。传统的量化训练通过将训练组件表示为低比特整数来解决问题，但通常依赖于对实值更新的离散化。

Method: 引入一种替代方法，其中更新规则本身是离散的，通过设计避免对连续更新的量化。建立了一类此类离散方案的收敛保证，并以多项更新规则作为具体示例，并进行了实证评估。

Result: 为一般类别的离散方案建立了收敛保证，并通过实证评估支持了多项更新规则的有效性。

Conclusion: 这种离散更新规则的视角为高效训练开辟了新途径，特别适用于具有固有离散结构的模型。

Abstract: Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.

</details>


### [105] [Eval Factsheets: A Structured Framework for Documenting AI Evaluations](https://arxiv.org/abs/2512.04062)
*Florian Bordes,Candace Ross,Justine T Kao,Evangelia Spiliopoulou,Adina Williams*

Main category: cs.LG

TL;DR: Eval Factsheets：为AI系统评估方法设计的结构化文档框架，通过五维度分类和问卷形式系统记录评估特征，提升评估的透明度、可复现性和可比性。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域基准测试激增，但评估方法缺乏系统化的文档标准，导致可复现性、透明度和决策制定困难。与数据集和模型已有Datasheets和Model Cards等文档框架不同，评估方法缺乏类似的结构化文档标准。

Method: 提出Eval Factsheets框架，通过五维度分类（Context、Scope、Structure、Method、Alignment）组织评估特征，并实现为包含强制和推荐元素的实用问卷。该框架能够捕捉从传统基准测试到LLM-as-judge方法等多种评估范式。

Result: 通过多个基准测试的案例研究证明，Eval Factsheets能够有效捕捉多样化的评估范式，同时保持一致性和可比性。该框架适用于现有和新发布的评估框架。

Conclusion: Eval Factsheets应被纳入现有和新发布的评估框架中，以促进AI系统评估的透明度、可复现性和可比性，改善当前评估方法文档标准缺失的问题。

Abstract: The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.

</details>


### [106] [Fare Comparison App of Uber, Ola and Rapido](https://arxiv.org/abs/2512.04065)
*Ashlesha Gopinath Sawant,Sahil S. Jadhav,Vidhan R. Jain,Shriraj S. Jagtap,Prachi Jadhav,Soham Jadhav,Ichha Raina*

Main category: cs.LG

TL;DR: 开发一个比较Ola、Uber和Rapido价格的Web应用，帮助用户选择最经济高效的网约车服务


<details>
  <summary>Details</summary>
Motivation: 用户在选择网约车服务时面临困难，难以找到既经济又省时的最佳选项，需要提高服务透明度和用户体验

Method: 构建Web应用程序，使用Python后端通过API获取不同平台的价格数据，进行实时比价，并利用Android Studio模拟器、Appium等工具解决数据访问和位置比较的技术挑战

Result: 开发了一个能够为用户提供网约车服务价格比较的Web应用，帮助用户选择最经济高效的出行方案

Conclusion: 该项目提高了网约车服务的透明度，增强了用户体验和出行效率，解决了用户在选择服务时面临的信息不对称问题

Abstract: In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.

</details>


### [107] [Learning Steerable Clarification Policies with Collaborative Self-play](https://arxiv.org/abs/2512.04068)
*Jonathan Berant,Maximillian Chen,Adam Fisch,Reza Aghajani,Fantine Huot,Mirella Lapata,Jacob Eisenstein*

Main category: cs.LG

TL;DR: 训练可调控的AI助手策略，通过自博弈学习在不确定情况下决定何时猜测用户意图、枚举多种可能或提问澄清，策略可根据不同成本设置灵活调整


<details>
  <summary>Details</summary>
Motivation: AI助手需要处理模糊查询，但现有策略缺乏对不同上下文（如用户偏好、屏幕尺寸、语音环境）的适应性。例如在小屏幕或语音环境中枚举多种意图会显得繁琐，需要一种可根据不同成本因素灵活调整的策略

Method: 使用自博弈方法训练可调控策略：两个代理分别模拟用户和AI助手，生成对话场景。模型输入澄清问题成本和生成单词成本，通过强化自训练（ReST）最大化最终奖励（成本惩罚后的准确率），使策略能根据提供的成本预测性地调整行为

Result: 训练出的策略能够根据成本设置预测性地改变行为，获得更高的奖励和准确率。更重要的是，该方法还能泛化到训练时未观察到的成本数值

Conclusion: 通过自博弈和强化自训练可以学习可调控的不确定性管理策略，使AI助手能够根据上下文成本因素灵活调整响应方式，且具有良好的泛化能力

Abstract: To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [108] [A note on the impossibility of conditional PAC-efficient reasoning in large language models](https://arxiv.org/abs/2512.03057)
*Hao Zeng*

Main category: stat.ML

TL;DR: 证明了在大语言模型中条件性PAC高效推理的不可能性结果：在无分布设置下，任何实现条件性PAC高效的算法都必须是平凡的，即对几乎所有输入都以至少1-α的概率依赖专家模型。


<details>
  <summary>Details</summary>
Motivation: 最近的研究建立了组合模型（在昂贵的专家模型和更便宜的快速模型之间切换）的边缘PAC效率保证，但条件性（逐点）保证是否可能尚不清楚。本文旨在探索在无分布设置下条件性PAC高效推理的理论可能性。

Method: 采用理论证明方法，通过数学分析证明在非原子输入空间上，任何实现条件性PAC效率的算法都必须是平凡的。具体来说，证明了算法必须以至少1-α的概率对几乎所有输入依赖专家模型。

Result: 证明了条件性PAC高效推理在无分布设置下的不可能性定理：对于非原子输入空间，任何满足条件性PAC效率的算法都必须是平凡的，即对几乎所有输入都以至少1-α的概率使用专家模型。

Conclusion: 条件性（逐点）PAC效率保证在无分布设置下是不可能的，这为理解大语言模型中高效推理的理论限制提供了重要见解，表明边缘效率保证可能是更现实的追求目标。

Abstract: We prove an impossibility result for conditional Probably Approximately Correct (PAC)-efficient reasoning in large language models. While recent work has established marginal PAC efficiency guarantees for composite models that switch between expensive expert models and cheaper fast models, we show that conditional (pointwise) guarantees are impossible in the distribution-free setting. Specifically, for non-atomic input spaces, any algorithm achieving conditional PAC efficiency must be trivial in the sense that it defers to the expert model with probability at least $1-α$ for almost every input.

</details>


### [109] [Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback](https://arxiv.org/abs/2512.03208)
*Pangpang Liu,Junwei Lu,Will Wei Sun*

Main category: stat.ML

TL;DR: 该论文研究大语言模型对齐中奖励模型的估计与统计推断，提出处理人类反馈异质性的方法，建立理论保证并应用于不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型对齐中的强化学习人类反馈存在人类偏好异质性问题，这给可靠的奖励学习带来挑战，需要开发能够处理异质性并量化不确定性的方法。

Method: 采用异质性偏好框架联合建模答案的潜在奖励和人类理性，通过交替梯度下降算法解决双凸优化问题，建立估计器的理论保证和渐近分布。

Result: 建立了估计器的收敛性和渐近分布理论，能够构建奖励估计的置信区间，并将不确定性量化结果应用于奖励统计比较和最佳N策略框架。

Conclusion: 该方法能有效处理人类反馈异质性，为LLM对齐中的奖励建模提供可靠的不确定性量化，在模拟和实际数据中验证了其实际价值。

Abstract: We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of-$N$ (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.

</details>


### [110] [Iterative Tilting for Diffusion Fine-Tuning](https://arxiv.org/abs/2512.03234)
*Jean Pachebat,Giovanni Conforti,Alain Durmus,Yazid Janati*

Main category: stat.ML

TL;DR: 提出了一种无需梯度的迭代倾斜方法，用于微调扩散模型朝向奖励倾斜分布


<details>
  <summary>Details</summary>
Motivation: 现有方法需要反向传播通过采样链，计算成本高。需要一种更高效的方法来调整扩散模型以适应奖励倾斜分布

Method: 将大的奖励倾斜分解为多个顺序的小倾斜，每个小倾斜通过一阶泰勒展开获得可处理的分数更新，仅需奖励函数的前向评估

Result: 在具有线性奖励的二维高斯混合模型上验证，该模型的精确倾斜分布有闭式解

Conclusion: 迭代倾斜方法提供了一种无需梯度、计算高效的扩散模型微调方法，避免了反向传播通过采样链

Abstract: We introduce iterative tilting, a gradient-free method for fine-tuning diffusion models toward reward-tilted distributions. The method decomposes a large reward tilt $\exp(λr)$ into $N$ sequential smaller tilts, each admitting a tractable score update via first-order Taylor expansion. This requires only forward evaluations of the reward function and avoids backpropagating through sampling chains. We validate on a two-dimensional Gaussian mixture with linear reward, where the exact tilted distribution is available in closed form.

</details>


### [111] [Novelty detection on path space](https://arxiv.org/abs/2512.03243)
*Ioannis Gasteratos,Antoine Jacquier,Maud Lemercier,Terry Lyons,Cristopher Salvi*

Main category: stat.ML

TL;DR: 论文提出了一种基于路径空间签名的异常检测方法，通过假设检验框架和运输成本不等式获得错误率边界，并推导出CVaR的精确公式用于一类SVM算法，最后在合成和真实数据上验证了统计性能。


<details>
  <summary>Details</summary>
Motivation: 在路径空间上进行异常检测是一个重要问题，但现有方法在处理非高斯测度和计算风险度量方面存在局限。需要开发能够处理一般随机微分方程解、提供统计保证并优化风险度量的新方法。

Method: 1. 将路径空间异常检测构建为假设检验问题，使用基于签名的检验统计量；2. 利用运输成本不等式获得错误率边界；3. 通过洗牌积推导CVaR的精确公式；4. 设计优化平滑CVaR目标的一类SVM算法；5. 建立第二类错误的下界；6. 在合成和真实数据上进行数值评估。

Result: 1. 获得了超出高斯测度的错误率边界，适用于具有光滑有界向量场的RDE解；2. 推导出CVaR平滑代理的精确公式；3. 开发了新的优化平滑CVaR的一类SVM算法；4. 建立了第二类错误的下界；5. 数值实验验证了基于签名的检验统计量的第一类错误和统计功效。

Conclusion: 该研究提出了一种基于签名的路径空间异常检测框架，具有理论保证和实际应用价值。方法能够处理一般随机微分方程，提供风险度量的精确计算，并在合成和真实数据上表现出良好的统计性能。

Abstract: We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type-$\mathrm{II}$ error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type-$\mathrm{I}$ error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data.

</details>


### [112] [Colored Markov Random Fields for Probabilistic Topological Modeling](https://arxiv.org/abs/2512.03727)
*Lorenzo Marinucci,Leonardo Di Nino,Gabriele D'Acunto,Mario Edoardo Pandolfo,Paolo Di Lorenzo,Sergio Barbarossa*

Main category: stat.ML

TL;DR: 提出彩色马尔可夫随机场（CMRF），通过引入链接着色机制扩展经典高斯马尔可夫随机场，同时建模条件依赖和边缘依赖，特别适用于拓扑空间上的高斯边变量分析。


<details>
  <summary>Details</summary>
Motivation: 传统概率图模型在处理拓扑空间上的变量时表达能力有限，因为底层拓扑结构会塑造统计关系。拓扑信号处理的最新进展凸显了在拓扑空间上定义变量的重要性，需要新的模型来更好地捕捉这些关系。

Method: 引入彩色马尔可夫随机场（CMRF），基于Hodge理论，在拓扑空间上建模高斯边变量的条件依赖和边缘依赖。通过链接着色机制：连接性编码条件独立性，颜色编码边缘独立性，扩展经典高斯马尔可夫随机场。

Result: 在物理网络上的分布式估计案例研究中，CMRF相比具有不同拓扑先验水平的基线方法表现出优势，验证了其在处理拓扑空间变量时的有效性。

Conclusion: CMRF为分析拓扑空间上的复杂系统提供了更强大的建模框架，能够同时捕捉条件依赖和边缘依赖关系，在需要拓扑先验的应用领域具有重要价值。

Abstract: Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior.

</details>


### [113] [Comparison of neural network training strategies for the simulation of dynamical systems](https://arxiv.org/abs/2512.03851)
*Paul Strasser,Andreas Pfeffer,Jakob Weber,Markus Gurtner,Andreas Körner*

Main category: stat.ML

TL;DR: 并行训练在神经网络动态系统建模中比当前主流的串行-并行训练具有更好的长期预测精度，应作为默认训练策略。


<details>
  <summary>Details</summary>
Motivation: 神经网络已成为非线性动态系统建模的常用工具，但训练策略选择仍是关键设计决策，特别是对于仿真任务。当前实践中串行-并行训练占主导地位，但缺乏对不同训练策略性能的系统比较。

Method: 通过实证分析比较两种主要训练策略：并行训练和串行-并行训练。研究涵盖五种神经网络架构和两个实际案例：气动阀门测试台和工业机器人基准测试。同时澄清文献中术语不一致的问题，并将两种策略与系统辨识概念联系起来。

Result: 研究发现，尽管串行-并行训练在当前实践中占主导地位，但并行训练在长期预测精度方面始终表现更好。实证分析结果支持这一结论。

Conclusion: 并行训练应被视为基于神经网络的动态系统仿真的默认训练策略。研究还澄清了相关术语，为领域提供了更清晰的框架。

Abstract: Neural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems.

</details>
