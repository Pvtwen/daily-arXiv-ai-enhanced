<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 21]
- [cs.LG](#cs.LG) [Total: 155]
- [stat.ML](#stat.ML) [Total: 11]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [UniFi: Combining Irregularly Sampled CSI from Diverse Communication Packets and Frequency Bands for Wi-Fi Sensing](https://arxiv.org/abs/2512.22143)
*Gaofeng Dong,Kang Yang,Mani Srivastava*

Main category: eess.SP

TL;DR: UniFi是首个完全消除侵入式数据包注入的Wi-Fi集成感知与通信框架，通过直接利用多频段通信数据包的不规则采样CSI实现感知，同时保持通信吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有Wi-Fi感知系统依赖高频率探测包提取CSI，导致通信性能下降和部署困难。虽然ISAC是很有前景的方向，但现有方案仍需辅助包注入，因为它们仅利用数据帧的CSI。

Method: UniFi框架包含CSI净化管道来协调异构数据包并去除突发冗余，以及时间感知注意力模型直接从非均匀CSI序列学习而无需重采样。还创建了首个包含真实世界双频通信流量不规则采样CSI的数据集CommCSI-HAR。

Result: 在CommCSI-HAR数据集和四个公开基准测试上的广泛评估表明，UniFi以紧凑的模型大小实现了最先进的准确性，同时完全保持了通信吞吐量。

Conclusion: UniFi是首个完全消除侵入式包注入的Wi-Fi ISAC框架，通过直接利用通信数据包的不规则采样CSI，在保持通信性能的同时实现了高精度感知。

Abstract: Existing Wi-Fi sensing systems rely on injecting high-rate probing packets to extract channel state information (CSI), leading to communication degradation and poor deployability. Although Integrated Sensing and Communication (ISAC) is a promising direction, existing solutions still rely on auxiliary packet injection because they exploit only CSI from data frames. We present UniFi, the first Wi-Fi-based ISAC framework that fully eliminates intrusive packet injection by directly exploiting irregularly sampled CSI from diverse communication packets across multiple frequency bands. UniFi integrates a CSI sanitization pipeline to harmonize heterogeneous packets and remove burst-induced redundancy, together with a time-aware attention model that learns directly from non-uniform CSI sequences without resampling. We further introduce CommCSI-HAR, the first dataset with irregularly sampled CSI from real-world dual-band communication traffic. Extensive evaluations on this dataset and four public benchmarks show that UniFi achieves state-of-the-art accuracy with a compact model size, while fully preserving communication throughput.

</details>


### [2] [EEG-to-Voice Decoding of Spoken and Imagined speech Using Non-Invasive EEG](https://arxiv.org/abs/2512.22146)
*Hanbeot Park,Yunjeong Cho,Hunhee Kim*

Main category: eess.SP

TL;DR: 提出EEG-to-Voice框架，直接从非侵入式脑电信号重建语音，无需动态时间规整或显式时间对齐，实现了口语和想象语音的稳定重建。


<details>
  <summary>Details</summary>
Motivation: 基于EEG的语音重建面临空间分辨率有限、易受噪声影响、想象语音缺乏时间对齐声学目标等挑战，需要开发无需时间对齐的直接重建方法。

Method: 使用主体特定生成器从EEG生成梅尔频谱图，结合预训练声码器和ASR模块合成语音波形和解码文本；采用迁移学习进行领域适应，从口语预训练到想象语音；可选语言模型校正模块修正ASR错误。

Result: 在2秒和4秒语音条件下，口语和想象语音均实现稳定的声学重建和可比的语言准确性；声学相似度随语句增长下降，但文本解码性能保持；语言模型校正持续降低CER和WER而不引入语义失真。

Conclusion: 证明了无需显式时间对齐的直接开环EEG-to-Voice重建对于口语和想象语音的可行性，为脑机接口语音通信提供了新范式。

Abstract: Restoring speech communication from neural signals is a central goal of brain-computer interface research, yet EEG-based speech reconstruction remains challenging due to limited spatial resolution, susceptibility to noise, and the absence of temporally aligned acoustic targets in imagined speech. In this study, we propose an EEG-to-Voice paradigm that directly reconstructs speech from non-invasive EEG signals without dynamic time warping (DTW) or explicit temporal alignment. The proposed pipeline generates mel-spectrograms from EEG in an open-loop manner using a subject-specific generator, followed by pretrained vocoder and automatic speech recognition (ASR) modules to synthesize speech waveforms and decode text. Separate generators were trained for spoken speech and imagined speech, and transfer learning-based domain adaptation was applied by pretraining on spoken speech and adapting to imagined speech. A minimal language model-based correction module was optionally applied to correct limited ASR errors while preserving semantic structure. The framework was evaluated under 2 s and 4 s speech conditions using acoustic-level metrics (PCC, RMSE, MCD) and linguistic-level metrics (CER, WER). Stable acoustic reconstruction and comparable linguistic accuracy were observed for both spoken speech and imagined speech. While acoustic similarity decreased for longer utterances, text-level decoding performance was largely preserved, and word-position analysis revealed a mild increase in decoding errors toward later parts of sentences. The language model-based correction consistently reduced CER and WER without introducing semantic distortion. These results demonstrate the feasibility of direct, open-loop EEG-to-Voice reconstruction for spoken speech and imagined speech without explicit temporal alignment.

</details>


### [3] [Machine Learning-Based Basil Yield Prediction in IoT-Enabled Indoor Vertical Hydroponic Farms](https://arxiv.org/abs/2512.22151)
*Emna Bouzid,Noura Baccar,Kamran Iqbal,Yassine Chaouch,Fares Ben Youssef,Amine Regayeg,Sarra Toumi,Houda Nsir,Amina Mseddi,Leila Costelle*

Main category: eess.SP

TL;DR: 本研究将室内垂直水培与机器学习结合，开发了基于IoT传感器数据的罗勒产量预测系统，比较了LR、LSTM和DNN三种模型，发现DNN在准确率（98%）与计算效率间取得最佳平衡，适合实际农业部署。


<details>
  <summary>Details</summary>
Motivation: 农业面临水资源短缺压力，特别是在突尼斯等地区，急需创新、资源高效的解决方案。本研究旨在通过室内垂直水培与机器学习技术结合，优化罗勒产量同时节约水资源。

Method: 开发预测系统，使用三种机器学习模型（线性回归LR、长短期记忆网络LSTM、深度神经网络DNN），基于IoT传感器收集的CO2、光照等环境参数数据（10k数据点）进行训练和评估。实验设置包括21株罗勒作物，使用树莓派和Arduino。

Result: LSTM准确率最高（99%），但执行时间是LR的10倍，RAM使用量是DNN的3倍；DNN准确率98%，在计算速度与预测质量间取得良好平衡；LR执行时间仅11秒，适合低复杂度或资源有限应用。

Conclusion: DNN模型在准确率与计算效率间达到最佳平衡，适合构建响应迅速、高精度的农业决策支持系统，未来适合边缘设备部署，为水资源短缺地区的精准农业提供有效解决方案。

Abstract: As agriculture faces increasing pressure from water scarcity, especially in regions like Tunisia, innovative, resource-efficient solutions are urgently needed. This work explores the integration of indoor vertical hydroponics with Machine Learning (ML) techniques to optimize basil yield while saving water. This research develops a prediction system that uses different ML models and assesses their performance. The models were systematically trained and tested using data collected from IoT sensors of various environmental parameters like CO2, light. The experimental setup features 21 basil crops and uses Raspberry Pi and Arduino. 10k data points were collected and used to train and evaluate three ML models: Linear Regression (LR), Long Short-Term Memory (LSTM), and Deep Neural Networks (DNN). The comparative analysis of the performance of each model revealed that, while LSTM showed high predictive capability and accuracy of 99%, its execution time was 10 times longer than LR and its RAM usage was about 3 times higher than DNN's when simulated on a standard CPU environment. Conversely, the DNN model had an accuracy rate of 98%. This proves an efficient balance between computational speed and prediction quality, which makes this model well-suited for real-life deployment. Moreover, LR excelled in fast processing of basic prediction with an execution time of 11 seconds. This makes the LR model more suitable for low-complexity or resource-limited applications. These performance trade-offs highlight the potential of DNN-based solutions for building responsive, high-accuracy decision-support systems tailored to agricultural environments, making it suitable for future edge-device deployment.

</details>


### [4] [PaperNet: Efficient Temporal Convolutions and Channel Residual Attention for EEG Epilepsy Detection](https://arxiv.org/abs/2512.22172)
*Md Shahriar Sajid,Abhijit Kumar Ghosh,Fariha Nusrat*

Main category: eess.SP

TL;DR: PaperNet：结合时间卷积、通道注意力与轻量双向循环模块的紧凑混合架构，用于EEG癫痫检测，在BEED数据集上达到0.96宏F1分数，仅需约60万参数。


<details>
  <summary>Details</summary>
Motivation: EEG信号具有丰富的时频结构但难以建模，现有轻量深度学习模型要么仅依赖局部卷积，要么需要重型循环模块。需要一种既能捕获多尺度动态又保持计算效率的紧凑架构。

Method: 提出PaperNet混合架构：结合时间卷积、通道级残差注意力模块和轻量双向循环块（用于短窗口分类）。在BEED数据集上使用明确定义的主体独立训练协议进行评估。

Result: 在测试集上达到0.96宏F1分数，仅约0.6M参数，所有四类性能均衡。消融研究验证了各组件贡献，通道注意力权重提供了电极相关性洞察，计算分析显示适合资源受限系统部署。

Conclusion: 精心结合时间滤波、通道重加权和循环上下文建模可以在不过度增加计算成本的情况下获得强大的EEG分类性能，表明这种混合方法对实际部署具有实用价值。

Abstract: Electroencephalography (EEG) signals contain rich temporal-spectral structure but are difficult to model due to noise, subject variability, and multi-scale dynamics. Lightweight deep learning models have shown promise, yet many either rely solely on local convolutions or require heavy recurrent modules. This paper presents PaperNet, a compact hybrid architecture that combines temporal convolutions, a channel-wise residual attention module, and a lightweight bidirectional recurrent block which is used for short-window classification. Using the publicly available BEED: Bangalore EEG Epilepsy Dataset, we evaluate PaperNet under a clearly defined subject-independent training protocol and compare it against established and widely used lightweight baselines. The model achieves a macro-F1 of 0.96 on the held-out test set with approximately 0.6M parameters, while maintaining balanced performance across all four classes. An ablation study demonstrates the contribution of temporal convolutions, residual attention, and recurrent aggregation. Channel-wise attention weights further offer insights into electrode relevance. Computational profiling shows that PaperNet remains efficient enough for practical deployment on resource-constrained systems through out the whole process. These results indicate that carefully combining temporal filtering, channel reweighting, and recurrent context modeling can yield strong EEG classification performance without excessive computational cost.

</details>


### [5] [Simultaneous Source Separation, Synchronization, Localization and Mapping for 6G Systems](https://arxiv.org/abs/2512.22393)
*Alexander Venus,Erik Leitinger,Klaus Witrisal*

Main category: eess.SP

TL;DR: 本文提出一种用于非完美同步基站场景下的多路径SLAM方法，通过联合贝叶斯框架处理源分离、同步和建图问题，在5G系统中保持与传统正交同步方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 现有协作多路径SLAM方法通常假设基站完美同步且传输序列正交，忽略基站间干扰。然而实际5G系统中基站采用静默模式仍会导致定位性能下降，需要解决非理想同步下的源分离、同步和建图问题。

Method: 提出基站依赖的数据关联和同步偏差模型，集成到联合贝叶斯框架中，通过因子图上的和积算法进行推断，实现同时源分离、同步和建图。

Result: 在各种系统配置下分析了联合同步和源分离的影响，统计分析显示与传统假设正交同步基站的协作MP-SLAM相比，没有显著性能下降。

Conclusion: 所提方法能够有效处理实际5G系统中的非完美同步问题，在保持性能的同时放宽了对基站同步和正交性的假设，为未来6G网络中的多路径SLAM提供了更实用的解决方案。

Abstract: Multipath-based simultaneous localization and mapping (MP-SLAM) is a promising approach for future 6G networks to jointly estimate the positions of transmitters and receivers together with the propagation environment. In cooperative MP-SLAM, information collected by multiple mobile terminals (MTs) is fused to enhance accuracy and robustness. Existing methods, however, typically assume perfectly synchronized base stations (BSs) and orthogonal transmission sequences, rendering inter-BS interference at the MTs negligible. In this work, we relax these assumptions and address simultaneous source separation, synchronization, and mapping. A relevant example arises in modern 5G systems, where BSs employ muting patterns to mitigate interference, yet localization performance still degrades. We propose a novel BS-dependent data association and synchronization bias model, integrated into a joint Bayesian framework and inferred via the sum-product algorithm on a factor graph. The impact of joint synchronization and source separation is analyzed under various system configurations. Compared with state-of-the-art cooperative MP-SLAM assuming orthogonal and synchronized BSs, our statistical analysis shows no significant performance degradation.

</details>


### [6] [FARIS: Fluid-Active-RIS](https://arxiv.org/abs/2512.22479)
*Hong-Bae Jeon*

Main category: eess.SP

TL;DR: 提出流体主动可重构智能表面(FARIS)，结合流体端口重定位和单元主动放大，通过交替优化框架最大化6G网络遍历速率


<details>
  <summary>Details</summary>
Motivation: 传统FRIS/ARIS在6G网络中的性能有限，需要同时优化端口位置和主动放大能力来提升网络性能

Method: 提出FARIS架构，结合流体端口重定位和单元主动放大；建立遍历速率最大化问题，采用交替优化(AO)框架联合优化主动放大反射向量和流体主动单元离散选择

Result: FARIS在多种环境下始终优于传统FRIS/ARIS，即使使用更少主动单元或更小物理孔径也能获得更高速率

Conclusion: FARIS通过流体端口重定位和主动放大的协同优化，显著提升6G网络性能，为未来智能表面设计提供新方向

Abstract: In this paper, we introduce a fluid-active reconfigurable intelligent surface (FARIS) that combines fluid-based port repositioning with per-element active amplification to enhance the performance of 6G network. To characterize the performance, we formulate an ergodic-rate maximization problem that jointly optimizes both the active amplification-reflection vector and the discrete selection of fluid active elements under practical hardware constraints. The problem is addressed via an alternating optimization (AO) framework, which progressively improves the rate. Complexity and convergence analyses that follow furnish deeper insight into the algorithmic operation and performance enhancement. Numerical results confirm that the proposed FARIS with AO framework consistently outperforms conventional FRIS/ARIS, delivering higher rates across diverse environments, often even when using fewer active elements or a smaller physical aperture.

</details>


### [7] [CoDS: Collaborative Perception via Digital Semantic Communication](https://arxiv.org/abs/2512.22513)
*Jipeng Gan,Le Liang,Hua Zhang,Chongtao Guo,Shi Jin*

Main category: eess.SP

TL;DR: CoDS：基于数字语义通信的协作感知框架，解决现有语义通信与数字V2X网络不兼容的问题，通过语义压缩编解码器、语义模数转换器和不确定性感知网络实现高效可靠的数字语义传输。


<details>
  <summary>Details</summary>
Motivation: 现有语义通信方法主要依赖模拟传输模型，与现代车联网(V2X)的数字架构不兼容，阻碍了实际部署。需要一种能在实际数字通信系统中实现语义级传输效率的解决方案。

Method: 1) 语义压缩编解码器：提取和压缩任务导向的语义特征，保持下游感知精度；2) 语义模数转换器：将连续语义特征转换为离散比特流，与现有数字通信管道集成；3) 不确定性感知网络：评估接收特征的可靠性，丢弃解码失败的特征，缓解低信噪比下的悬崖效应。

Result: CoDS显著优于现有语义通信和传统数字通信方案，在保证与实际数字V2X系统兼容性的同时，实现了最先进的感知性能。

Conclusion: CoDS成功将语义通信与数字V2X网络结合，解决了实际部署的关键障碍，为自动驾驶协作感知系统提供了高效可靠的数字语义通信解决方案。

Abstract: Semantic communication has been introduced into collaborative perception systems for autonomous driving, offering a promising approach to enhancing data transmission efficiency and robustness. Despite its potential, existing semantic communication approaches predominantly rely on analog transmission models, rendering these systems fundamentally incompatible with the digital architecture of modern vehicle-to-everything (V2X) networks and posing a significant barrier to real-world deployment. To bridge this critical gap, we propose CoDS, a novel collaborative perception framework based on digital semantic communication, designed to realize semantic-level transmission efficiency within practical digital communication systems. Specifically, we develop a semantic compression codec that extracts and compresses task-oriented semantic features while preserving downstream perception accuracy. Building on this, we propose a novel semantic analog-to-digital converter that converts these continuous semantic features into a discrete bitstream, ensuring integration with existing digital communication pipelines. Furthermore, we develop an uncertainty-aware network (UAN) that assesses the reliability of each received feature and discards those corrupted by decoding failures, thereby mitigating the cliff effect of conventional channel coding schemes under low signal-to-noise ratio (SNR) conditions. Extensive experiments demonstrate that CoDS significantly outperforms existing semantic communication and traditional digital communication schemes, achieving state-of-the-art perception performance while ensuring compatibility with practical digital V2X systems.

</details>


### [8] [Compressive Toeplitz Covariance Estimation From Few-Bit Quantized Measurements With Applications to DOA Estimation](https://arxiv.org/abs/2512.22527)
*Hongwei Xu,Weichao Zheng,Zai Yang*

Main category: eess.SP

TL;DR: 提出针对稀疏观测和粗量化硬件约束下的Hermitian Toeplitz协方差矩阵估计方法，包括Q-TSCM和2k-TSCM估计器，以及基于协方差拟合的Q-SPA方法。


<details>
  <summary>Details</summary>
Motivation: 解决实际硬件约束（稀疏观测和粗量化）下的Hermitian Toeplitz协方差矩阵估计问题，这些约束在雷达、通信等应用中常见。

Method: 1. 在三角抖动量化框架下提出Q-TSCM估计器补偿量化偏差；2. 提出2k-TSCM作为有限比特版本；3. 基于协方差拟合准则提出Q-SPA方法，通过半定规划强制正半定性。

Result: 推导了复数高斯假设下的非渐近误差界，显示误差与量化水平的二次依赖关系，并通过覆盖系数捕捉稀疏采样模式的影响。数值实验验证了理论结果，并在波达方向估计应用中展示了有效性。

Conclusion: 提出的估计器能有效处理稀疏观测和粗量化约束下的协方差矩阵估计问题，Q-SPA通过半定规划进一步提升了性能，为实际硬件受限系统提供了实用解决方案。

Abstract: This paper addresses the problem of estimating the Hermitian Toeplitz covariance matrix under practical hardware constraints of sparse observations and coarse quantization. Within the triangular-dithered quantization framework, we propose an estimator called Toeplitz-projected sample covariance matrix (Q-TSCM) to compensate for the quantization-induced bias, together with its finite-bit counterpart termed the $2k$-bit Toeplitz-projected sample covariance matrix ($2k$-TSCM), obtained by truncating the pre-quantization observations. Under the complex Gaussian assumption, we derive non-asymptotic error bounds of the estimators that reveal a quadratic dependence on the quantization level and capture the effect of sparse sampling patterns through the so-called coverage coefficient. To further improve performance, we propose the quantized sparse and parametric approach (Q-SPA) based on a covariance-fitting criterion, which enforces additionally positive semidefiniteness at the cost of solving a semidefinite program. Numerical experiments are presented that corroborate our theoretical findings and demonstrate the effectiveness of the proposed estimators in the application to direction-of-arrival estimation.

</details>


### [9] [A Novel Geometry-Aware GPR-Based Energy-Efficient and Low-Overhead Channel Estimation Scheme](https://arxiv.org/abs/2512.22578)
*Syed Luqman Shah,Nurul Huda Mahmood*

Main category: eess.SP

TL;DR: 提出基于天线几何结构的谱混合协方差函数（GB-SMCF）建模无线信道，通过高斯过程回归（GPR）框架实现从少量噪声观测中准确估计信道状态信息（CSI），显著降低导频开销和训练能耗。


<details>
  <summary>Details</summary>
Motivation: 传统信道估计方法需要大量导频传输，导致高开销和能耗。现有方法未能充分利用天线阵列的几何结构信息，限制了在少量观测下的估计精度。

Method: 将无线信道建模为复值高斯过程，提出GB-SMCF协方差函数捕获天线阵列空间结构。开发基于GPR的信道估计框架，使用GB-SMCF作为先验协方差模型，并在线优化超参数。仅从少量发射天线传输导频，接收端处理有限观测数据。

Result: 仿真结果表明，提出的GB-SMCF估计器优于基线方法，相比传统方案可将导频开销和训练能耗降低高达50%。

Conclusion: 基于天线几何结构的谱混合协方差函数结合高斯过程回归，能够有效利用天线阵列空间结构信息，在少量观测下实现准确信道估计，显著降低系统开销。

Abstract: In this work, we model the wireless channel as a complex-valued Gaussian process (GP) over the transmit and receive antenna arrays. The channel covariance is characterized using an antenna-geometry-based spectral mixture covariance function (GB-SMCF), which captures the spatial structure of the antenna arrays. To address the problem of accurate channel state information (CSI) estimation from very few noisy observations, we develop a Gaussian process regression (GPR)-based channel estimation framework that employs the GB-SMCF as a prior covariance model with online hyperparameter optimization. In the proposed scheme, the full channel is learned by transmitting pilots from only a small subset of transmit antennas while receiving them at all receive antennas, resulting in noisy partial CSI at the receiver. These limited observations are then processed by the GPR framework, which updates the GB-SMCF hyperparameters online from incoming measurements and reconstructs the full CSI in real time. Simulation results demonstrate that the proposed GB-SMCF-based estimator outperforms baseline methods while reducing pilot overhead and training energy by up to 50$\%$ compared to conventional schemes.

</details>


### [10] [Real-Time Multi-Target Detection and Tracking with mmWave 5G NR Waveforms on RFSoC](https://arxiv.org/abs/2512.22582)
*Xinyang Li,Hian Zing Voon,Vlad C. Andrei,Alexander Sessler,Nunzio Sciammetta,Ullrich J. Mönich,Dominic A. Schupke,Holger Boche*

Main category: eess.SP

TL;DR: 基于5G NR PDSCH波形实现实时多目标检测与跟踪的毫米波雷达系统，采用400MHz带宽28GHz载频，在RFSoC平台上实现硬件加速处理


<details>
  <summary>Details</summary>
Motivation: 开发一个基于5G NR波形的实时多目标检测与跟踪系统，利用5G基础设施实现感知功能，探索通信与感知融合的可能性

Method: 使用RFSoC 4x2板卡连接毫米波波束成形器，在可编程逻辑部分实现完整的感知收发处理和快速波束控制；采用自适应背景减除、CA-CFAR检测、DBSCAN聚类和EKF滤波算法进行目标检测与跟踪

Result: 成功实现了基于5G NR PDSCH波形的实时多目标检测与跟踪系统，系统集成了CPU、GPU和FPGA等异构计算资源，提供设计灵活性

Conclusion: 该研究展示了5G NR波形在感知应用中的可行性，为通信感知一体化提供了硬件加速的实时实现方案，具有低延迟和灵活性优势

Abstract: We demonstrate a real-time implementation of multi-target detection and tracking using 5G New Radio (NR) physical downlink shared channel (PDSCH) waveform with 400 MHz bandwidth at 28 GHz carrier frequency. The hardware platform is built on a radio frequency system-on-chip (RFSoC) 4x2 board connected with a pair of Sivers EVK02001 mmWave beamformers for transmission and reception. The entire sensing transceiver processing and fast beam control are realized purely in the programmable logic (PL) part of the RFSoC, enabling low-latency and fully hardware-accelerated operation. The continuously acquired sensing data constitute 3D range-angle (RA) tensors, which are processed on a host PC using adaptive background subtraction, cell-averaging constant false alarm rate (CA-CFAR) detection with density-based spatial clustering of applications with noise (DBSCAN) clustering, and extended Kalman filtering (EKF), to detect and track targets in the environment. Our software-defined radio (SDR) testbed integrates heterogeneous computing resources, including CPUs, GPUs, and FPGAs, thereby providing design flexibility for a wide range of tasks.

</details>


### [11] [Synthesis of signal processing algorithms with constraints on minimal parallelism and memory space](https://arxiv.org/abs/2512.22676)
*Sergey Salishev*

Main category: eess.SP

TL;DR: 开发低功耗硬件信号处理算法，通过最小并行度和内存优化提升能效，包括功耗模型、函数近似方法、FFT内存调度和Toeplitz系统求解分析。


<details>
  <summary>Details</summary>
Motivation: 针对低功耗计算硬件的能效提升需求，在最小并行度和内存空间约束下开发信号处理算法，为专用加速器提供理论和方法支持。

Method: 1) 建立时钟CMOS逻辑功耗/能耗模型以选择最优并行度；2) 整数友好初等函数近似方法，通过约束分段多项式减少查找表大小；3) 混合基流式FFT的无冲突数据放置和执行顺序，包括自排序FFT变体；4) 快速Schur算法并行度/内存分析，针对Toeplitz系统求解。

Result: 提供了构造性定理、调度方案和设计权衡，能够实现高效的专用加速器，在低功耗硬件上优化信号处理算法的能效。

Conclusion: 该论文提出了一套完整的低功耗信号处理算法框架，通过系统化的功耗建模、函数近似、内存调度和并行分析，为能效优化的专用硬件设计提供了理论基础和实用方法。

Abstract: This thesis develops signal-processing algorithms and implementation schemes under constraints of minimal parallelism and memory space, with the goal of improving energy efficiency of low-power computing hardware. We propose (i) a power/energy consumption model for clocked CMOS logic that supports selecting optimal parallelism, (ii) integer-friendly approximation methods for elementary functions that reduce lookup-table size via constrained piecewise-polynomial (quasi-spline) constructions with accuracy guarantees, (iii) provably conflict-free data placement and execution order for mixed-radix streaming FFT on multi-bank and single-port memories, including a self-sorting FFT variant, and (iv) a parallelism/memory analysis of the fast Schur algorithm for superfast Toeplitz system solving, motivated by echo-cancellation workloads. The results provide constructive theorems, schedules, and design trade-offs enabling efficient specialized accelerators.

</details>


### [12] [Multistatic Radar Performance in the Presence of Distributed Wireless Synchronization](https://arxiv.org/abs/2512.22686)
*Kumar Sai Bondada,Daniel J. Jakubisin,R. Michael Buehrer*

Main category: eess.SP

TL;DR: 提出一种采用分布式无线同步协议的多基地雷达系统，通过双音波形交换实现频率同步，双向波形交换实现时间同步，无需GPS。开发了BCRLB框架量化同步偏移对目标定位和速度估计的影响，仿真表明优化同步链路参数可使系统性能超越单基地雷达并接近理想情况。


<details>
  <summary>Details</summary>
Motivation: 多基地雷达系统需要精确的时间和频率同步，传统依赖GPS的同步方法存在局限性。本文旨在开发一种不依赖GPS的无线同步协议，并量化分析同步偏移对系统性能的影响，为实际系统设计提供理论指导。

Method: 1) 提出分布式无线同步协议：使用双音波形交换进行频率同步，双向波形交换进行时间同步；2) 建立贝叶斯克拉美罗下界(BCRLB)框架，量化同步偏移对联合时延和多普勒估计的影响；3) 通过仿真分析同步链路参数对系统性能的影响。

Result: 仿真结果表明：1) 同步偏移会降低多基地雷达的性能；2) 同步链路性能主要取决于信道和传输参数；3) 优化这些参数可使多基地雷达性能超越单基地雷达并接近理想情况；4) 模拟的同步链路参数表明实际实现是可行的。

Conclusion: 提出的无线同步协议为多基地雷达系统提供了一种不依赖GPS的同步解决方案。BCRLB框架为系统设计提供了量化分析工具，仿真结果表明通过优化同步链路参数，多基地雷达系统可以实现优于单基地雷达的性能，且实际实现可行。

Abstract: This paper proposes a multistatic radar (MSR) system utilizing a distributed wireless synchronization protocol. The wireless synchronization protocol uses a two-tone waveform exchange for frequency synchronization and a bi-directional waveform exchange for time synchronization, independent of GPS. A Bayesian Cramer-Rao lower bound (BCRLB) framework is developed to quantify the impact of synchronization offsets on joint delay and Doppler estimation, and consequently, on target localization and velocity estimation accuracy. Simulation results derived from the analytical expressions establish the extent to which the residual synchronization offsets degrade the MSR's performance. The performance of the synchronization links primarily depends on the synchronization-link channel and transmit parameters; optimizing these parameters enables the MSR configuration to surpass the monostatic performance and approach the ideal case. Furthermore, the simulated synchronization-link parameters suggest that practical implementation is feasible.

</details>


### [13] [Instance Communication System for Intelligent Connected Vehicles: Bridging the Gap from Semantic to Instance-Level Transmission](https://arxiv.org/abs/2512.22693)
*Daiqi Zhang,Bizhu Wang,Wenqi Zhang,Chen Sun,Xiaodong Xu*

Main category: eess.SP

TL;DR: InsCom（实例通信）通过从语义级提升到实例级通信，识别图像中的关键实例并过滤非关键实例，显著减少智能网联车辆的数据冗余，提升传输效率。


<details>
  <summary>Details</summary>
Motivation: 智能网联车辆需要高速数据传输，但无线资源稀缺。现有语义通信系统虽然能提取任务相关信息，但仍有冗余，因为同一语义类别下的不同实例对下游任务的重要性不同。

Method: 提出实例通信（InsCom）：1）使用场景图生成模型识别图像中的所有实例及其相互关系；2）基于用户可配置的任务关键性标准（主体语义和关系-对象对）过滤识别的实例；3）仅传输任务关键实例。

Result: 在多种数据集和无线信道条件下，InsCom相比最先进的语义通信系统：数据量减少超过7.82倍，质量提升1.75-14.03 dB。

Conclusion: InsCom通过从语义级提升到实例级通信，有效解决了语义通信中的剩余冗余问题，显著提高了智能网联车辆在有限无线资源下的传输效率。

Abstract: Intelligent Connected Vehicles (ICVs) rely on high-speed data transmission for efficient and safety-critical services. However, the scarcity of wireless resources limits the capabilities of ICVs. Semantic Communication (SemCom) systems can alleviate this issue by extracting and transmitting task-relevant information, termed semantic information, instead of the entire raw data. Despite this, we reveal that residual redundancy persists within SemCom systems, where not all instances under the same semantic category are equally critical for downstream tasks. To tackle this issue, we introduce Instance Communication (InsCom), which elevates communication from the semantic level to the instance level for ICVs. Specifically, InsCom uses a scene graph generation model to identify all image instances and analyze their inter-relationships, thus distinguishing between semantically identical instances. Additionally, it applies user-configurable, task-critical criteria based on subject semantics and relation-object pairs to filter recognized instances. Consequently, by transmitting only task-critical instances, InsCom significantly reduces data redundancy, substantially enhancing transmission efficiency within limited wireless resources. Evaluations across various datasets and wireless channel conditions show that InsCom achieves a data volume reduction of over 7.82 times and a quality improvement ranging from 1.75 to 14.03 dB compared to the state-of-the-art SemCom systems.

</details>


### [14] [On the Impact of Phase Errors in Phase-Dependent Amplitudes of Near-Field RISs](https://arxiv.org/abs/2512.22825)
*Ke Wang,Chan-Tong Lam,Benjamin K. Ng,Yue Liu*

Main category: eess.SP

TL;DR: 该论文研究了近场可重构智能表面中相位相关幅度与设计相移之间的相互耦合，提出了剩余功率指标量化RIS反射信号功率，并推导了频谱效率的闭式上界。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将相位误差和相位相关幅度分开处理，忽略了它们之间的耦合效应，这导致理论预测与实测结果存在差异。本文旨在建立更准确的模型来捕捉这种耦合关系。

Method: 提出剩余功率指标量化RIS反射信号功率，利用扩展的Glivenko-Cantelli定理证明其渐近收敛性；建立四种像素反射模型；基于Friis传输公式和投影孔径提出包含耦合效应的近场信道模型；使用泰勒展开、柯西-施瓦茨不等式和黎曼和推导闭式上界。

Result: 发现当RIS相移接近其范围两端时，独立同分布相位误差下的剩余功率小于完全相关相位误差下的剩余功率，而在相移接近范围中间时关系相反；忽略相位相关幅度中的相位误差会导致高估RIS性能增益。

Conclusion: 相位相关幅度与设计相移之间的耦合效应不可忽略，忽略这种耦合会导致理论预测与实测结果不符。提出的模型和上界为RIS性能分析提供了更准确的理论框架。

Abstract: This paper investigates mutual coupling between phase-dependent amplitudes (PDAs) and designed phase shifts within pixels of near-field (NF) reconfigurable intelligent surfaces (RISs) in the presence of phase errors (PEs). In contrast to existing research that treats phase shifts with errors (PSEs) and the PDAs separately, we introduce a remaining power (RP) metric to quantify the proportion of power preserved in the signals reflected by the RIS, and we prove its asymptotic convergence to theoretical values by leveraging extended Glivenko-Cantelli theorem. Then, the RP of signals passing through RIS pixels is jointly examined under combined phase and amplitude uncertainties. In addition, we propose four pixel reflection models to capture practical conditions, and we derive approximate polynomial upper bounds for the RP with error terms by applying Taylor expansion. Furthermore, based on Friis transmission formula and projected aperture, we propose a general NF channel model that incorporates the coupling between the PSEs and the PDAs. By using Cauchy-Bunyakovsky-Schwarz inequality and Riemann sums, we derive a closed-form upper bound on spectral efficiency, and the bound becomes tighter as the pixel area decreases. We reveal that as the RIS phase shifts approach the ends of their range, the RP under independent and identically distributed PEs is smaller than that under fully correlated PEs, whereas this relationship reverses when the phase shifts are near the middle of their range. Neglecting the PEs in the PDAs leads to an overestimation of the RIS performance gain, explaining the discrepancies between theoretical and measured results.

</details>


### [15] [Generalizable Learning for Massive MIMO CSI Feedback in Unseen Environments](https://arxiv.org/abs/2512.22840)
*Haoyu Wang,Zhi Sun,Shuangfeng Han,Xiaoyun Wang,Zhaocheng Wang*

Main category: eess.SP

TL;DR: 提出基于物理解释的CSI反馈泛化方法EG-CsiNet，通过多簇解耦和细粒度对齐解决分布偏移问题，在多种环境下比现有方法降低3dB以上泛化误差。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的CSI反馈算法在未见环境中泛化性无法保证，导致部署成本高。需要提升深度学习CSI反馈的泛化能力。

Method: 1) 建模基于簇的信道分布偏移；2) 提出基于物理的分布对齐方法，包括多簇解耦和细粒度对齐；3) 基于EYM定理设计高效多簇解耦算法，设计混合准则估计簇数；4) 提出EG-CsiNet学习框架。

Result: 在广泛仿真和模拟到真实实验中，EG-CsiNet能鲁棒地将泛化误差降低超过3dB，相比现有最优方法。

Conclusion: 通过物理解释提升深度学习CSI反馈的泛化性，EG-CsiNet在多种条件下表现优异，显著降低部署成本。

Abstract: Deep learning is promising to enhance the accuracy and reduce the overhead of channel state information (CSI) feedback, which can boost the capacity of frequency division duplex (FDD) massive multiple-input multiple-output (MIMO) systems. Nevertheless, the generalizability of current deep learning-based CSI feedback algorithms cannot be guaranteed in unseen environments, which induces a high deployment cost. In this paper, the generalizability of deep learning-based CSI feedback is promoted with physics interpretation. Firstly, the distribution shift of the cluster-based channel is modeled, which comprises the multi-cluster structure and single-cluster response. Secondly, the physics-based distribution alignment is proposed to effectively address the distribution shift of the cluster-based channel, which comprises multi-cluster decoupling and fine-grained alignment. Thirdly, the efficiency and robustness of physics-based distribution alignment are enhanced. Explicitly, an efficient multi-cluster decoupling algorithm is proposed based on the Eckart-Young-Mirsky (EYM) theorem to support real-time CSI feedback. Meanwhile, a hybrid criterion to estimate the number of decoupled clusters is designed, which enhances the robustness against channel estimation error. Fourthly, environment-generalizable neural network for CSI feedback (EG-CsiNet) is proposed as a novel learning framework with physics-based distribution alignment. Based on extensive simulations and sim-to-real experiments in various conditions, the proposed EG-CsiNet can robustly reduce the generalization error by more than 3 dB compared to the state-of-the-arts.

</details>


### [16] [Confidence analysis-based hybrid heartbeat detection for ballistocardiogram using template matching and deep learning](https://arxiv.org/abs/2512.22926)
*Dongli Cai,Xihe Chen,Yaosheng Chen,Hong Xian,Baoxian Yu,Han Zhang*

Main category: eess.SP

TL;DR: 提出基于置信度分析的混合心跳检测方法，结合模板匹配和深度学习，在BCG信号中实现更准确的心跳间隔检测。


<details>
  <summary>Details</summary>
Motivation: 传统的心跳检测方法各有局限：模板匹配基于相邻心跳事件的相似性，深度学习基于鲁棒的时空特征，但两者在不同情况下的表现不稳定。需要结合两者的优势来提高检测的准确性和鲁棒性。

Method: 提出置信度分析的混合方法，结合模板匹配和深度学习。通过信号形态一致性和心跳间隔变异性评估检测结果的置信度，保留高置信度结果。形态一致性用每个心跳事件与检测模板的平均相关性衡量，间隔变异性用归一化标准差衡量。

Result: 在包含34名受试者、924,235个心跳的临床BCG数据集上，混合方法的平均绝对间隔误差为20.73毫秒，比单独使用模板匹配减少29.28毫秒，比单独使用深度学习减少10.13毫秒。案例研究显示混合方法能结合两者的互补优势。

Conclusion: 提出的混合方法结合了模板匹配和深度学习的优势，在个体差异和信号质量变化的情况下表现出更好的鲁棒性，在实际BCG监测场景中具有优越性。

Abstract: Heartbeat interval can be detected from ballistocardiogram (BCG) signals in a non-contact manner. Conventional methods achieved heartbeat detection from different perspectives, where template matching (TM) and deep learning (DL) were based on the similarity of neighboring heartbeat episodes and robust spatio-temporal characteristics, respectively, and thus, performed varied from case to case. Inspired by the above facts, we propose confidence analysis-based hybrid heartbeat detection using both TM and DL, and further explore the advantages of both methods in various scenarios. To be specific, the confidence of the heartbeat detection results was evaluated by the consistency of signal morphology and the variability of the detected heartbeat intervals, which could be formulated by the averaged correlation between each heartbeat episode and the detected template and the normalized standard deviation among detected heartbeat intervals, respectively, where the results with higher confidence were remained. In order to validate the effectiveness of the proposed hybrid method, we conducted experiments using practical clinical BCG dataset with 34 subjects including 924,235 heartbeats. Numerical results showed that the proposed hybrid method achieved an average absolute interval error of 20.73 ms, yielding a reduction of 29.28 ms and 10.13 ms compared to solo TM and DL methods, respectively. Besides, case study showed the robustness of heartbeat detection of TM and DL to individual differences and signal quality, respectively, and in turn, validated that the hybrid method could benefit from the complementary advantages of both methods, which demonstrated the superiority of the proposed hybrid method in practical BCG monitoring scenarios.

</details>


### [17] [Masked Sequence Autoencoding for Enhanced Defect Visualization in Active Infrared Thermography](https://arxiv.org/abs/2512.23000)
*Mohammed Salah,Eman Ouda,Stefano Sfarra,Davor Svetinovic,Yusra Abdulrahman*

Main category: eess.SP

TL;DR: 提出AIRT-Masked-CAAE方法，结合卷积特征提取与注意力机制，通过掩码序列自编码策略提升红外热成像缺陷检测性能，减少训练时间30倍。


<details>
  <summary>Details</summary>
Motivation: 传统红外热成像自动编码器难以分离细微缺陷特征与主导背景响应，导致在不同材料和检测条件下缺陷分析效果不佳。

Method: 提出掩码CNN-注意力自动编码器(AIRT-Masked-CAAE)，整合卷积特征提取与注意力机制，采用掩码序列自编码策略，让网络从上下文线索推断缺失的热响应。

Result: 在PVC、CFRP和PLA样本上评估，AIRT-Masked-CAAE在对比度、信噪比和神经网络指标上超越现有最佳红外热成像自动编码器。

Conclusion: AIRT-Masked-CAAE能有效捕捉局部热模式和全局上下文依赖，抑制背景冗余，仅需部分热序列训练即可获得泛化性强的潜在表示，训练时间减少30倍。

Abstract: Active infrared thermography (AIRT) became a crucial tool in aerospace non-destructive testing (NDT), enabling the detection of hidden defects and anomalies in materials by capturing thermal responses over time. In AIRT, autoencoders are widely used to enhance defect detection by reducing the dimensionality of thermal data and improving the signal-to-noise ratio. However, traditional AIRT autoencoders often struggle to disentangle subtle defect features from dominant background responses, leading to suboptimal defect analysis under varying material and inspection conditions. To overcome this challenge, this work proposes a Masked CNN-Attention Autoencoder (AIRT-Masked-CAAE) that integrates convolutional feature extraction with attention mechanisms to capture both local thermal patterns and global contextual dependencies. The AIRT-Masked-CAAE introduces a masked sequence autoencoding strategy, where the network learns to infer missing thermal responses from surrounding contextual cues, while suppressing background redundancy. In addition, the proposed masked sequence autoencoding approach enables training on only a subset of the thermal sequence, while providing generalizable latent representations and reducing training time by a factor of 30. The AIRT-Masked-CAAE framework was evaluated using specimens made of PVC, CFRP, and PLA. The results demonstrate that the AIRT-Masked-CAAE surpasses state-of-the-art AIRT autoencoders in terms of contrast, signal-to-noise ratio (SNR), and metrics based on neural networks.

</details>


### [18] [Flexible Intelligent Metasurface for Downlink Communications under Statistical CSI](https://arxiv.org/abs/2512.23045)
*Vaibhav Kumar,Anastasios Papazafeiropoulos,Pandelis Kourtessis,John Senior,Marwa Chafii,Dimitra I. Kaklamani,Iakovos S. Venieris*

Main category: eess.SP

TL;DR: FIM辅助发射机在统计CSI下通过形态变化优化空间相关性，在强空间信道相关场景中显著优于传统刚性天线阵列


<details>
  <summary>Details</summary>
Motivation: 柔性智能超表面是6G无线系统的突破性硬件技术，相比传统刚性天线阵列，FIM可以通过形态变化动态改变物理表面，为系统性能提升提供新的自由度。现有研究多依赖瞬时信道状态信息，本文研究在统计CSI下最大化平均和频谱效率的问题。

Method: 首先推导FIM辅助发射机的空间相关矩阵，然后基于梯度投影法提出迭代FIM优化算法，在统计CSI下优化FIM形态以最大化平均和频谱效率。

Result: 仿真结果表明，在统计CSI下，当信道具有强空间相关性时，FIM辅助系统相比传统刚性天线阵列系统提供显著性能增益；当信道弱相关时，增益减小。

Conclusion: FIM技术在统计CSI下仍能有效提升系统性能，特别是在空间信道相关性强的场景中，这为6G无线系统中FIM的实际部署提供了理论支持。

Abstract: Flexible intelligent metasurface (FIM) is a recently developed, groundbreaking hardware technology with promising potential for 6G wireless systems. Unlike conventional rigid antenna array (RAA)-based transmitters, FIM-assisted transmitters can dynamically alter their physical surface through morphing, offering new degrees of freedom to enhance system performance. In this letter, we depart from prior works that rely on instantaneous channel state information (CSI) and instead address the problem of average sum spectral efficiency maximization under statistical CSI in a FIM-assisted downlink multiuser multiple-input single-output setting. To this end, we first derive the spatial correlation matrix for the FIM-aided transmitter and then propose an iterative FIM optimization algorithm based on the gradient projection method. Simulation results show that with statistical CSI, the FIM-aided system provides a significant performance gain over its RAA-based counterpart in scenarios with strong spatial channel correlation, whereas the gain diminishes when the channels are weakly correlated.

</details>


### [19] [Unscented and Higher-Order Linear Covariance Fidelity Checks and Measures of Non-Gaussianity](https://arxiv.org/abs/2512.23152)
*Jackson Kulik,Braden Hastings,Keith A. LeGrand*

Main category: eess.SP

TL;DR: 提出用于评估线性协方差(LinCov)方法性能的计算技术，通过高阶统计、约束优化和无迹变换来量化线性化近似的准确性


<details>
  <summary>Details</summary>
Motivation: 线性协方差方法在航天器导航性能初步研究中广泛应用，虽然计算效率优于蒙特卡洛方法，但依赖线性化近似。需要理解这些近似的准确性，特别是在处理高度非线性系统和大状态不确定性时，这对航天器导航和任务规划至关重要

Method: 提出新的LinCov保真度度量方法，基于高阶统计、约束优化和无迹变换来评估线性协方差性能

Result: 开发了多种计算技术来量化线性化近似的准确性，帮助识别线性协方差方法在哪些情况下会失效

Conclusion: 这些新的保真度度量方法能够有效评估线性协方差方法的性能，特别是在高度非线性系统和大不确定性情况下，为航天器导航和任务规划提供更可靠的性能评估工具

Abstract: Linear covariance (LinCov) techniques have gained widespread traction in the modeling of uncertainty, including in the preliminary study of spacecraft navigation performance. While LinCov methods offer improved computational efficiency compared to Monte Carlo based uncertainty analysis, they inherently rely on linearization approximations. Understanding the fidelity of these approximations and identifying when they are deficient is critically important for spacecraft navigation and mission planning, especially when dealing with highly nonlinear systems and large state uncertainties. This work presents a number of computational techniques for assessing linear covariance performance. These new LinCov fidelity measures are formulated using higher-order statistics, constrained optimization, and the unscented transform.

</details>


### [20] [Ultra-Massive MIMO with Orthogonal Chirp Division Multiplexing for Near-Field Sensing and Communication Integration](https://arxiv.org/abs/2512.23246)
*Ziwei Wan,Zhen Gao,Fabien Heliot,Qu Luo,Pei Xiao,Haiyang Zhang,Christos Masouros,Yonina C. Eldar,Sheng Chen*

Main category: eess.SP

TL;DR: 论文提出将超大规模MIMO与正交啁啾分复用波形结合，用于解决近场集成感知与通信问题，通过专用感知子载波和虚拟双基地感知实现高精度目标定位和三维速度测量。


<details>
  <summary>Details</summary>
Motivation: 解决近场集成感知与通信的挑战性问题，通过结合UM-MIMO和OCDM技术，在简化硬件复杂度的同时提高感知精度和通信性能。

Method: 采用UM-MIMO基站结合OCDM波形通信，感知接收器采用FMCW检测原理。设计专用感知子载波和天线选择方案，通过虚拟双基地感知利用空间多样性进行目标定位和速度测量。

Result: 仿真结果表明，所提出的ISAC方案能提高感知精度，同时改善通信性能，在空间非平稳和不相关多径环境中具有鲁棒性。

Conclusion: UM-MIMO与OCDM波形结合为近场ISAC提供了有效的解决方案，通过虚拟双基地感知实现了高精度目标定位，并增强了通信系统的信道估计性能。

Abstract: This paper integrates the emerging ultra-massive multiple-input multiple-output (UM-MIMO) technique with orthogonal chirp division multiplexing (OCDM) waveform to tackle the challenging near-field integrated sensing and communication (ISAC) problem. Specifically, we conceive a comprehensive ISAC architecture, where an UM-MIMO base station adopts OCDM waveform for communications and a co-located sensing receiver adopts the frequency-modulated continuous wave (FMCW) detection principle to simplify the associated hardware. For sensing tasks, several OCDM subcarriers, namely, dedicated sensing subcarriers (DSSs), are each transmitted through a dedicated sensing antenna (DSA) within the transmit antenna array. By judiciously designing the DSS selection scheme and optimizing receiver parameters, the FMCW-based sensing receiver can decouple the echo signals from different DSAs with significantly reduced hardware complexity. This setup enables the estimation of ranges and velocities of near-field targets in an antenna-pairwise manner. Moreover, by leveraging the spatial diversity of UM-MIMO, we introduce the concept of virtual bistatic sensing (VIBS), which incorporates the estimates from multiple antenna pairs to achieve high-accuracy target positioning and three-dimensional velocity measurement. The VIBS paradigm is immune to hostile channel environments characterized by spatial non-stationarity and uncorrelated multipath environment. Furthermore, the channel estimation of UM-MIMO OCDM systems enhanced by the sensing results is investigated. Simulation results demonstrate that the proposed ISAC scheme enhances sensing accuracy, and also benefits communication performance.

</details>


### [21] [On Signal Peak Power Constraint of Over-the-Air Federated Learning](https://arxiv.org/abs/2512.23381)
*Lorenz Bielefeld,Paul Zheng,Oner Hanay,Yao Zhu,Yulin Hu,Anke Schmeink*

Main category: eess.SP

TL;DR: 本文指出现有AirComp-FL忽略了射频功率放大器的瞬时峰值功率约束，分析了限幅滤波方法的影响，发现在多载波OFDM系统中限幅滤波会导致显著的性能下降。


<details>
  <summary>Details</summary>
Motivation: 现有AirComp-FL框架忽略了射频功率放大器的非线性特性带来的瞬时峰值功率约束这一关键实际问题，这在实际部署中可能导致性能下降。

Method: 研究了处理峰值功率约束的经典方法——限幅与滤波，分析了单载波和多载波OFDM系统中瞬时峰值功率约束对AirComp-FL的影响，特别关注了梯度值分布导致的更高PAPR问题。

Result: 仿真结果表明，在实际设置中，瞬时发射功率经常超过功率放大器限制；通过应用限幅和滤波，FL性能会下降，尤其是在多载波OFDM系统中，由于限幅和滤波引起的带内失真，性能下降更为显著。

Conclusion: AirComp-FL中的瞬时峰值功率约束是一个重要实际问题，现有解决方案（限幅滤波）在多载波系统中会导致显著性能下降，需要新的方法来平衡功率约束和FL性能。

Abstract: Federated learning (FL) has been considered a promising privacy preserving distributed edge learning framework. Over-the-air computation (AirComp) technique leveraging analog transmission enables the aggregation of local updates directly over-the-air by exploiting the superposition properties of wireless multiple-access channel, thereby drastically reducing the communication bottleneck issues of FL compared with digital transmission schemes. This work points out that existing AirComp-FL overlooks a key practical constraint, the instantaneous peak-power constraints imposed by the non-linearity of radiofrequency power amplifiers. We present and analyze the effect of the classic method to deal with this issue, amplitude clipping combined with filtering. We investigate the effect of instantaneous peak-power constraints in AirComp-FL for both single-carrier and multi-carrier orthogonal frequency-division multiplexing (OFDM) systems. We highlight the specificity of AirComp-FL: the samples depend on the gradient value distribution, leading to a higher peak-to-average power ratio (PAPR) than that observed for uniformly distributed signals. Simulation results demonstrate that, in practical settings, the instantaneous transmit power regularly exceeds the power-amplifier limit; however, by applying clipping and filtering, the FL performance can be degraded. The degradation becomes pronounced especially in multi-carrier OFDM systems due to the in-band distortions caused by clipping and filtering.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Predictive Modeling of Power Outages during Extreme Events: Integrating Weather and Socio-Economic Factors](https://arxiv.org/abs/2512.22699)
*Antar Kumar Biswas,Masoud H. Nazari*

Main category: cs.LG

TL;DR: 提出基于机器学习的极端天气停电预测框架，整合多源数据，LSTM模型表现最佳，经济条件和基础设施发展程度与停电风险负相关


<details>
  <summary>Details</summary>
Motivation: 针对低概率高影响的极端天气停电事件，传统预测方法存在局限，需要更准确的风险评估框架来整合天气、社会经济、基础设施等多维度因素

Method: 整合EAGLE-I停电记录（2014-2024）与天气、社会经济、基础设施和季节性事件数据，评估四种机器学习模型：随机森林、支持向量机、自适应提升和长短期记忆网络

Result: 在密歇根州下半岛县级数据集上验证，LSTM网络预测误差最低；经济条件越强、基础设施越发达的地区停电发生率越低

Conclusion: 提出的学习框架能有效预测极端天气导致的停电风险，LSTM模型表现最佳，社会经济和基础设施因素对停电风险有显著影响

Abstract: This paper presents a novel learning-based framework for predicting power outages caused by extreme events. The proposed approach specifically targets low-probability, high-consequence outage scenarios and leverages a comprehensive set of features derived from publicly available data sources. We integrate EAGLE-I outage records (2014-2024) with weather, socio-economic, infrastructure, and seasonal event data. Incorporating social and demographic indicators reveals underlying patterns of community vulnerability and provides a clearer understanding of outage risk during extreme conditions. Four machine learning models (Random Forest (RF), Support Vector Machine (SVM), Adaptive Boosting (AdaBoost), and Long Short-Term Memory (LSTM)) are evaluated. Experimental validation is performed on a large-scale dataset covering counties in the lower peninsula of Michigan. Among all models tested, the LSTM network achieves the lowest prediction error. Additionally, the results demonstrate that stronger economic conditions and more developed infrastructure are associated with lower outage occurrence.

</details>


### [23] [Pruning Graphs by Adversarial Robustness Evaluation to Strengthen GNN Defenses](https://arxiv.org/abs/2512.22128)
*Yongyu Wang*

Main category: cs.LG

TL;DR: 提出基于对抗鲁棒性评估的图剪枝框架，通过识别并移除图中脆弱或有害的边来增强GNN的防御能力


<details>
  <summary>Details</summary>
Motivation: GNN虽然能有效利用图结构和节点特征，但消息传递机制会放大结构或特征中的扰动和噪声，使得GNN对对抗攻击和虚假连接高度脆弱

Method: 提出一个剪枝框架，利用对抗鲁棒性评估来显式识别并移除图中的脆弱或有害组件。通过鲁棒性分数作为指导，选择性剪除最可能降低模型可靠性的边，从而获得更干净、更具韧性的图表示

Result: 在三种代表性GNN架构上进行实验，结果表明该方法在高扰动情况下能显著增强GNN的防御能力

Conclusion: 通过对抗鲁棒性指导的图剪枝可以有效提升GNN在面对对抗攻击时的可靠性，为构建更稳健的图学习模型提供了新思路

Abstract: Graph Neural Networks (GNNs) have emerged as a dominant paradigm for learning on graph-structured data, thanks to their ability to jointly exploit node features and relational information encoded in the graph topology. This joint modeling, however, also introduces a critical weakness: perturbations or noise in either the structure or the features can be amplified through message passing, making GNNs highly vulnerable to adversarial attacks and spurious connections. In this work, we introduce a pruning framework that leverages adversarial robustness evaluation to explicitly identify and remove fragile or detrimental components of the graph. By using robustness scores as guidance, our method selectively prunes edges that are most likely to degrade model reliability, thereby yielding cleaner and more resilient graph representations. We instantiate this framework on three representative GNN architectures and conduct extensive experiments on benchmarks. The experimental results show that our approach can significantly enhance the defense capability of GNNs in the high-perturbation regime.

</details>


### [24] [Towards Unsupervised Causal Representation Learning via Latent Additive Noise Model Causal Autoencoders](https://arxiv.org/abs/2512.22150)
*Hans Jarett J. Ong,Brian Godwin S. Lim,Dominic Dayta,Renzo Roel P. Tan,Kazushi Ikeda*

Main category: cs.LG

TL;DR: LANCA提出了一种基于加性噪声模型的无监督因果发现方法，通过确定性编码和显式优化残差独立性，在合成和真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无监督表示学习通常依赖统计独立性，但难以捕捉因果依赖关系。因果变量从观测数据中解耦存在可识别性问题，需要监督、辅助信号或强归纳偏置。

Method: 提出LANCA（潜在加性噪声模型因果自编码器），将加性噪声模型作为强归纳偏置。采用确定性Wasserstein自编码器结合可微ANM层，将残差独立性从被动假设转变为显式优化目标。

Result: 理论上证明ANM约束将可容许变换从任意微分同胚限制到仿射类。实验上在合成物理基准（Pendulum, Flow）和真实环境（CANDLE）上优于现有方法，对复杂背景中的虚假相关性具有更强鲁棒性。

Conclusion: LANCA通过将加性噪声模型作为显式归纳偏置，成功解决了无监督因果发现中的可识别性问题，为从观测数据中学习因果表示提供了有效框架。

Abstract: Unsupervised representation learning seeks to recover latent generative factors, yet standard methods relying on statistical independence often fail to capture causal dependencies. A central challenge is identifiability: as established in disentangled representation learning and nonlinear ICA literature, disentangling causal variables from observational data is impossible without supervision, auxiliary signals, or strong inductive biases. In this work, we propose the Latent Additive Noise Model Causal Autoencoder (LANCA) to operationalize the Additive Noise Model (ANM) as a strong inductive bias for unsupervised discovery. Theoretically, we prove that while the ANM constraint does not guarantee unique identifiability in the general mixing case, it resolves component-wise indeterminacy by restricting the admissible transformations from arbitrary diffeomorphisms to the affine class. Methodologically, arguing that the stochastic encoding inherent to VAEs obscures the structural residuals required for latent causal discovery, LANCA employs a deterministic Wasserstein Auto-Encoder (WAE) coupled with a differentiable ANM Layer. This architecture transforms residual independence from a passive assumption into an explicit optimization objective. Empirically, LANCA outperforms state-of-the-art baselines on synthetic physics benchmarks (Pendulum, Flow), and on photorealistic environments (CANDLE), where it demonstrates superior robustness to spurious correlations arising from complex background scenes.

</details>


### [25] [SoliReward: Mitigating Susceptibility to Reward Hacking and Annotation Noise in Video Generation Reward Models](https://arxiv.org/abs/2512.22170)
*Jiesong Lian,Ruizhe Zhong,Zixiang Zhou,Xiaoyue Mi,Yixue Hao,Yuan Zhou,Qinglin Lu,Long Hu,Junchi Yan*

Main category: cs.LG

TL;DR: SoliReward：一个用于视频生成模型奖励模型训练的系统框架，通过高质量数据收集、层次化注意力机制和改进的损失函数来解决当前奖励模型训练中的问题。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型的后训练对齐需要有效的奖励模型，但当前方法面临数据标注噪声、架构设计不足和奖励黑客攻击等挑战。

Method: 1) 使用单项目二元标注收集高质量数据，并通过跨提示配对策略构建偏好对；2) 采用层次化渐进查询注意力机制增强特征聚合；3) 引入改进的Bradley-Terry损失函数，显式处理赢-平局场景，正则化奖励分数分布。

Result: 在评估物理合理性、主体变形和语义对齐的基准测试中，该方法在直接奖励模型评估指标和后训练视频生成模型效果方面均表现出改进。

Conclusion: SoliReward框架通过系统性方法解决了视频奖励模型训练的关键挑战，为视频生成模型的后训练对齐提供了更有效的解决方案。

Abstract: Post-training alignment of video generation models with human preferences is a critical goal. Developing effective Reward Models (RMs) for this process faces significant methodological hurdles. Current data collection paradigms, reliant on in-prompt pairwise annotations, suffer from labeling noise. Concurrently, the architectural design of VLM-based RMs, particularly their output mechanisms, remains underexplored. Furthermore, RM is susceptible to reward hacking in post-training. To mitigate these limitations, we propose SoliReward, a systematic framework for video RM training. Our framework first sources high-quality, cost-efficient data via single-item binary annotations, then constructs preference pairs using a cross-prompt pairing strategy. Architecturally, we employ a Hierarchical Progressive Query Attention mechanism to enhance feature aggregation. Finally, we introduce a modified BT loss that explicitly accommodates win-tie scenarios. This approach regularizes the RM's score distribution for positive samples, providing more nuanced preference signals to alleviate over-focus on a small number of top-scoring samples. Our approach is validated on benchmarks evaluating physical plausibility, subject deformity, and semantic alignment, demonstrating improvements in direct RM evaluation metrics and in the efficacy of post-training on video generation models. Code and benchmark will be publicly available.

</details>


### [26] [Wireless Traffic Prediction with Large Language Model](https://arxiv.org/abs/2512.22178)
*Chuanting Zhang,Haixia Zhang,Jingping Qiao,Zongzhang Li,Mohamed-Slim Alouini*

Main category: cs.LG

TL;DR: TIDES是一个基于LLM的无线流量预测框架，通过聚类识别区域异质流量模式、提示工程将统计特征转化为结构化输入、以及DeepSeek模块实现跨域空间对齐，显著提升了城市级无线流量预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需要智能、自适应的资源管理，而现有深度学习和基础模型在无线流量预测中大多忽略了城市尺度流量动态的空间依赖性，这限制了预测的准确性和可扩展性。

Method: 1) 通过聚类机制识别区域异质流量模式，为每个区域训练个性化模型；2) 引入提示工程方案将统计流量特征嵌入为结构化输入；3) 设计DeepSeek模块通过跨域注意力实现空间对齐；4) 仅微调轻量级组件而冻结核心LLM层。

Result: 在真实世界蜂窝流量数据集上的实验表明，TIDES在预测准确性和鲁棒性方面显著优于最先进的基线方法，证明了将空间感知集成到LLM预测器中的有效性。

Conclusion: 将空间感知集成到基于LLM的预测器中是解锁未来6G系统中可扩展和智能网络管理的关键，TIDES框架为此提供了有效的解决方案。

Abstract: The growing demand for intelligent, adaptive resource management in next-generation wireless networks has underscored the importance of accurate and scalable wireless traffic prediction. While recent advancements in deep learning and foundation models such as large language models (LLMs) have demonstrated promising forecasting capabilities, they largely overlook the spatial dependencies inherent in city-scale traffic dynamics. In this paper, we propose TIDES (Traffic Intelligence with DeepSeek-Enhanced Spatial-temporal prediction), a novel LLM-based framework that captures spatial-temporal correlations for urban wireless traffic prediction. TIDES first identifies heterogeneous traffic patterns across regions through a clustering mechanism and trains personalized models for each region to balance generalization and specialization. To bridge the domain gap between numerical traffic data and language-based models, we introduce a prompt engineering scheme that embeds statistical traffic features as structured inputs. Furthermore, we design a DeepSeek module that enables spatial alignment via cross-domain attention, allowing the LLM to leverage information from spatially related regions. By fine-tuning only lightweight components while freezing core LLM layers, TIDES achieves efficient adaptation to domain-specific patterns without incurring excessive training overhead. Extensive experiments on real-world cellular traffic datasets demonstrate that TIDES significantly outperforms state-of-the-art baselines in both prediction accuracy and robustness. Our results indicate that integrating spatial awareness into LLM-based predictors is the key to unlocking scalable and intelligent network management in future 6G systems.

</details>


### [27] [Latent Sculpting for Zero-Shot Generalization: A Manifold Learning Approach to Out-of-Distribution Anomaly Detection](https://arxiv.org/abs/2512.22179)
*Rajeeb Thapa Chhetri,Zhixiong Chen,Saurab Thapa*

Main category: cs.LG

TL;DR: 提出Latent Sculpting框架，通过两阶段表示学习解决高维表格数据中监督学习的"泛化崩溃"问题，在零样本异常检测上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 监督深度学习在高维表格数据中存在"泛化崩溃"问题：模型能学习已知分布的精确决策边界，但在面对分布外数据时表现灾难性失败。作者认为这源于潜在空间缺乏拓扑约束，导致流形扩散，使得新异常与正常数据统计上无法区分。

Method: 提出Latent Sculpting框架：第一阶段使用混合1D-CNN和Transformer编码器，配合新颖的双中心紧凑性损失（DCCL），将正常流量"雕刻"成低熵的超球面簇；第二阶段基于这个预结构化流形，使用掩码自回归流（MAF）学习精确的密度估计。

Result: 在CIC-IDS-2017基准测试中，监督基线在未见分布偏移上表现灾难性崩溃（F1约0.30），最强无监督基线仅达0.76，而本框架在严格零样本异常检测上达到F1分数0.87。在"渗透"场景中检测率达到88.89%，而最先进的监督模型准确率为0.00%。

Conclusion: 显式的流形雕刻是鲁棒零样本泛化的前提条件。将结构学习与密度估计解耦为广义异常检测提供了可扩展的路径。

Abstract: A fundamental limitation of supervised deep learning in high-dimensional tabular domains is "Generalization Collapse": models learn precise decision boundaries for known distributions but fail catastrophically when facing Out-of-Distribution (OOD) data. We hypothesize that this failure stems from the lack of topological constraints in the latent space, resulting in diffuse manifolds where novel anomalies remain statistically indistinguishable from benign data. To address this, we propose Latent Sculpting, a hierarchical two-stage representation learning framework. Stage 1 utilizes a hybrid 1D-CNN and Transformer Encoder trained with a novel Dual-Centroid Compactness Loss (DCCL) to actively "sculpt" benign traffic into a low-entropy, hyperspherical cluster. Unlike standard contrastive losses that rely on triplet mining, DCCL optimizes global cluster centroids to enforce absolute manifold density. Stage 2 conditions a Masked Autoregressive Flow (MAF) on this pre-structured manifold to learn an exact density estimate. We evaluate this methodology on the rigorous CIC-IDS-2017 benchmark, treating it as a proxy for complex, non-stationary data streams. Empirical results demonstrate that explicit manifold sculpting is a prerequisite for robust zero-shot generalization. While supervised baselines suffered catastrophic performance collapse on unseen distribution shifts (F1 approx 0.30) and the strongest unsupervised baseline achieved only 0.76, our framework achieved an F1-Score of 0.87 on strictly zero-shot anomalies. Notably, we report an 88.89% detection rate on "Infiltration" scenarios--a complex distributional shift where state-of-the-art supervised models achieved 0.00% accuracy. These findings suggest that decoupling structure learning from density estimation provides a scalable path toward generalized anomaly detection.

</details>


### [28] [Learning Tennis Strategy Through Curriculum-Based Dueling Double Deep Q-Networks](https://arxiv.org/abs/2512.22186)
*Vishnu Mohan*

Main category: cs.LG

TL;DR: 使用DDQN与课程学习的强化学习框架优化网球策略，在模拟环境中实现高胜率，但策略偏向防守而非进攻


<details>
  <summary>Details</summary>
Motivation: 网球策略优化是一个复杂的序列决策问题，涉及分层计分、随机结果、长时程信用分配、身体疲劳和对手技能适应等挑战。现有方法难以有效处理这些问题。

Method: 构建自定义网球模拟环境，包含完整计分系统（分、局、盘）、10个离散战术动作类别、对称疲劳动态和连续对手技能参数。采用Dueling Double Deep Q-Network(DDQN)架构，通过课程学习逐步提升对手难度（从0.40到0.50）。

Result: 训练后的智能体对平衡对手胜率达98-100%，发球效率63.0-67.5%，回球效率52.8-57.1%。消融研究表明，dueling架构和课程学习对稳定收敛都至关重要，标准DQN基线无法学习有效策略。

Conclusion: 尽管性能强劲，但战术分析显示学习到的策略有明显的防守偏向，优先考虑避免失误和延长回合而非积极得分。这凸显了在简化体育模拟中仅以胜率为驱动的优化局限性，强调了奖励设计对现实体育强化学习的重要性。

Abstract: Tennis strategy optimization is a challenging sequential decision-making problem involving hierarchical scoring, stochastic outcomes, long-horizon credit assignment, physical fatigue, and adaptation to opponent skill. I present a reinforcement learning framework that integrates a custom tennis simulation environment with a Dueling Double Deep Q-Network(DDQN) trained using curriculum learning. The environment models complete tennis scoring at the level of points, games, and sets, rally-level tactical decisions across ten discrete action categories, symmetric fatigue dynamics, and a continuous opponent skill parameter. The dueling architecture decomposes action-value estimation into state-value and advantage components, while double Q-learning reduces overestimation bias and improves training stability in this long-horizon stochastic domain. Curriculum learning progressively increases opponent difficulty from 0.40 to 0.50, enabling robust skill acquisition without the training collapse observed under fixed opponents. Across extensive evaluations, the trained agent achieves win rates between 98 and 100 percent against balanced opponents and maintains strong performance against more challenging opponents. Serve efficiency ranges from 63.0 to 67.5 percent, and return efficiency ranges from 52.8 to 57.1 percent. Ablation studies demonstrate that both the dueling architecture and curriculum learning are necessary for stable convergence, while a standard DQN baseline fails to learn effective policies. Despite strong performance, tactical analysis reveals a pronounced defensive bias, with the learned policy prioritizing error avoidance and prolonged rallies over aggressive point construction. These results highlight a limitation of win-rate driven optimization in simplified sports simulations and emphasize the importance of reward design for realistic sports reinforcement learning.

</details>


### [29] [Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part II: Physics-Informed Neural Networks and Uncertainty Quantification](https://arxiv.org/abs/2512.22189)
*Jose I. Aizpurua*

Main category: cs.LG

TL;DR: 本文是系列论文的第二部分，重点研究将物理知识和不确定性量化集成到变压器健康评估的机器学习模型中，介绍了物理信息神经网络及其贝叶斯扩展。


<details>
  <summary>Details</summary>
Motivation: 随着基于物理的知识与机器学习模型在电力变压器监测、诊断和预测中的融合日益重要，需要开发能够集成物理约束并量化不确定性的可靠学习框架，特别是在数据稀疏的情况下。

Method: 1. 介绍物理信息神经网络(PINNs)基础，应用于时空热建模和固体绝缘老化；2. 提出贝叶斯PINNs作为量化认知不确定性的原则框架；3. 概述新兴研究方向。

Result: 提出了一个集成物理知识和不确定性量化的综合框架，能够在稀疏数据条件下提供稳健预测，为关键电力资产的物理感知和可信机器学习奠定基础。

Conclusion: 物理感知和可信的机器学习方法对于关键电力资产的健康评估具有重要潜力，贝叶斯PINNs为稀疏数据下的稳健预测提供了有前景的解决方案，并指明了未来研究方向。

Abstract: The integration of physics-based knowledge with machine learning models is increasingly shaping the monitoring, diagnostics, and prognostics of electrical transformers. In this two-part series, the first paper introduced the foundations of Neural Networks (NNs) and their variants for health assessment tasks. This second paper focuses on integrating physics and uncertainty into the learning process. We begin with the fundamentals of Physics-Informed Neural Networks (PINNs), applied to spatiotemporal thermal modeling and solid insulation ageing. Building on this, we present Bayesian PINNs as a principled framework to quantify epistemic uncertainty and deliver robust predictions under sparse data. Finally, we outline emerging research directions that highlight the potential of physics-aware and trustworthy machine learning for critical power assets.

</details>


### [30] [Physics-Informed Machine Learning for Transformer Condition Monitoring -- Part I: Basic Concepts, Neural Networks, and Variants](https://arxiv.org/abs/2512.22190)
*Jose I. Aizpurua*

Main category: cs.LG

TL;DR: 本文是两篇系列论文的第一部分，探讨神经网络在电力变压器状态监测和健康管理中的应用，介绍了CNN用于多模态数据监测以及RL用于决策控制。


<details>
  <summary>Details</summary>
Motivation: 电力变压器是电网关键资产，其可靠性直接影响电网韧性和稳定性。传统基于规则或纯物理模型的状态监测方法在处理不确定性、数据有限性和现代运行条件复杂性方面存在困难。

Method: 本文首先介绍神经网络基本概念，重点探讨卷积神经网络（CNN）用于多模态数据的状态监测，并讨论在强化学习（RL）框架中集成神经网络概念用于决策和控制。

Result: 机器学习方法特别是神经网络能够补充和扩展传统方法，实现更准确的诊断、预测和控制能力。

Conclusion: 神经网络及其扩展在变压器状态监测和健康管理中具有重要作用，本文为后续研究提供了基础框架和新兴研究方向展望。

Abstract: Power transformers are critical assets in power networks, whose reliability directly impacts grid resilience and stability. Traditional condition monitoring approaches, often rule-based or purely physics-based, struggle with uncertainty, limited data availability, and the complexity of modern operating conditions. Recent advances in machine learning (ML) provide powerful tools to complement and extend these methods, enabling more accurate diagnostics, prognostics, and control. In this two-part series, we examine the role of Neural Networks (NNs) and their extensions in transformer condition monitoring and health management tasks. This first paper introduces the basic concepts of NNs, explores Convolutional Neural Networks (CNNs) for condition monitoring using diverse data modalities, and discusses the integration of NN concepts within the Reinforcement Learning (RL) paradigm for decision-making and control. Finally, perspectives on emerging research directions are also provided.

</details>


### [31] [Frequency Regularization: Unveiling the Spectral Inductive Bias of Deep Neural Networks](https://arxiv.org/abs/2512.22192)
*Jiahao Lu*

Main category: cs.LG

TL;DR: 本文从信号处理角度研究CNN正则化机制，发现L2正则化通过抑制高频权重能量实现低通滤波效果，揭示了正则化在频率选择上的物理机制及其对精度-鲁棒性权衡的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管L2正则化和Dropout等正则化技术是训练深度神经网络的基础，但其在特征频率选择方面的物理机制仍不清楚。本文旨在从频谱偏置角度理解正则化如何影响卷积神经网络的频率特性。

Method: 提出视觉诊断框架跟踪训练过程中权重频率的动态演化，引入谱抑制比(SSR)量化不同正则器的"低通滤波"强度。针对小卷积核(如3x3)的混叠问题，采用离散径向剖面分析，在ResNet-18和CIFAR-10上进行实证研究。

Result: L2正则化相比无正则化基线将高频能量积累抑制了3倍以上。揭示了关键的精度-鲁棒性权衡：L2模型因过度专注于低频而对宽带高斯噪声敏感，但在高频信息丢失(如低分辨率)场景中表现出色，在模糊场景中优于基线>6%。

Conclusion: 正则化强制了强烈的谱归纳偏置，使网络偏向低频结构。这为理解泛化提供了信号处理视角，证实正则化通过频谱选择机制影响模型性能。

Abstract: Regularization techniques such as L2 regularization (Weight Decay) and Dropout are fundamental to training deep neural networks, yet their underlying physical mechanisms regarding feature frequency selection remain poorly understood. In this work, we investigate the Spectral Bias of modern Convolutional Neural Networks (CNNs). We introduce a Visual Diagnostic Framework to track the dynamic evolution of weight frequencies during training and propose a novel metric, the Spectral Suppression Ratio (SSR), to quantify the "low-pass filtering" intensity of different regularizers. By addressing the aliasing issue in small kernels (e.g., 3x3) through discrete radial profiling, our empirical results on ResNet-18 and CIFAR-10 demonstrate that L2 regularization suppresses high-frequency energy accumulation by over 3x compared to unregularized baselines. Furthermore, we reveal a critical Accuracy-Robustness Trade-off: while L2 models are sensitive to broadband Gaussian noise due to over-specialization in low frequencies, they exhibit superior robustness against high-frequency information loss (e.g., low resolution), outperforming baselines by >6% in blurred scenarios. This work provides a signal-processing perspective on generalization, confirming that regularization enforces a strong spectral inductive bias towards low-frequency structures.

</details>


### [32] [Emotion-Inspired Learning Signals (EILS): A Homeostatic Framework for Adaptive Autonomous Agents](https://arxiv.org/abs/2512.22200)
*Dhruv Tiwari*

Main category: cs.LG

TL;DR: 论文提出情感启发学习信号(EILS)框架，用类似生物情感的稳态控制机制替代传统静态奖励函数，以提升AI在开放环境中的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统依赖外部定义的静态奖励函数，在封闭环境中表现优异但在开放、非平稳的真实世界中表现脆弱。标准智能体缺乏内部自主性，难以在没有密集反馈的情况下探索，无法适应分布变化，且需要大量手动调参。

Method: 提出情感启发学习信号(EILS)框架，将情感建模为连续的稳态评估信号（如好奇心、压力、信心），这些信号从交互历史中推导出的向量值内部状态，动态调节智能体的优化景观：好奇心调节熵防止模式崩溃，压力调节可塑性克服不活跃，信心调整信任区域稳定收敛。

Result: 论文假设这种闭环稳态调节能使EILS智能体在样本效率和非平稳适应方面优于标准基线方法，但具体实验结果未在摘要中提供。

Conclusion: 生物情感启发的内部稳态控制机制是解决AI在开放环境中鲁棒自主性的关键，EILS框架为替代传统分散优化启发式方法提供了统一的生物启发内部反馈引擎。

Abstract: The ruling method in modern Artificial Intelligence spanning from Deep Reinforcement Learning (DRL) to Large Language Models (LLMs) relies on a surge of static, externally defined reward functions. While this "extrinsic maximization" approach has rendered superhuman performance in closed, stationary fields, it produces agents that are fragile in open-ended, real-world environments. Standard agents lack internal autonomy: they struggle to explore without dense feedback, fail to adapt to distribution shifts (non-stationarity), and require extensive manual tuning of static hyperparameters. This paper proposes that the unaddressed factor in robust autonomy is a functional analog to biological emotion, serving as a high-level homeostatic control mechanism. We introduce Emotion-Inspired Learning Signals (EILS), a unified framework that replaces scattered optimization heuristics with a coherent, bio-inspired internal feedback engine. Unlike traditional methods that treat emotions as semantic labels, EILS models them as continuous, homeostatic appraisal signals such as Curiosity, Stress, and Confidence. We formalize these signals as vector-valued internal states derived from interaction history. These states dynamically modulate the agent's optimization landscape in real time: curiosity regulates entropy to prevent mode collapse, stress modulates plasticity to overcome inactivity, and confidence adapts trust regions to stabilize convergence. We hypothesize that this closed-loop homeostatic regulation can enable EILS agents to outperform standard baselines in terms of sample efficiency and non-stationary adaptation.

</details>


### [33] [Transformer Reconstructed with Dynamic Value Attention](https://arxiv.org/abs/2512.22212)
*Xiaowei Wang*

Main category: cs.LG

TL;DR: 提出动态值注意力（DVA）方法，用单头注意力替代多头注意力，动态为每个查询生成值，减少冗余计算，提升训练效率37.6%


<details>
  <summary>Details</summary>
Motivation: Transformer的多头注意力机制存在固有局限：每个头使用相同的静态值，且头数受复杂度限制。多头注意力试图解决单头限制，但增加了冗余计算。

Method: 提出动态值注意力（DVA），为每个查询动态决定值，消除多头冗余，仅保留单头。由于每个修订后的嵌入已获取足够有用信息，后续前馈网络也可完全去除。

Result: DVA相比原始Transformer可节省37.6%的训练时间，同时提升学习能力。单头DVA即可满足Transformer需求。

Conclusion: 动态值注意力（DVA）是Transformer的有效优化方案，通过动态值分配和单头设计，在减少计算成本的同时提升性能。

Abstract: Since transformer was firstly published in 2017, several works have been proposed to optimize it. However, the major structure of transformer remains unchanged, ignoring one of its main intrinsic limitations, which is the same static value is used for every query in a head. Transformer itself tries to solve this problem by implementing multi-head attentions, yet the number of heads is limited by complexity. I propose a method to decide a value for each query dynamically, which could cut down all the redundant heads, keeping only one. Consequently, the following feed forward network could be cut down entirely, as each revised embedding has already fetched enough useful values far beyond the context. As a result, a single-head Dynamic Value Attention (DVA) is all you need in a transformer. According to the experiment, DVA may save 37.6% training time than the original transformer meanwhile increasing the learning capability.

</details>


### [34] [On the Existence and Behaviour of Secondary Attention Sinks](https://arxiv.org/abs/2512.22213)
*Jeffrey T. H. Wong,Cheng Zhang,Louis Mahon,Wayne Luk,Anton Isopoussu,Yiren Zhao*

Main category: cs.LG

TL;DR: 论文发现并分析了"次级注意力汇"现象，这是一种与先前研究的"主注意力汇"不同的注意力机制特征，主要出现在中间层，由特定MLP模块形成，影响注意力分布。


<details>
  <summary>Details</summary>
Motivation: 先前研究主要关注BOS等"主注意力汇"（primary sinks），但作者发现存在性质不同的"次级注意力汇"（secondary sinks），需要系统分析其特性、形成机制和对注意力机制的影响。

Method: 通过对11个模型家族进行广泛实验，分析次级注意力汇的出现位置、特性、形成机制和影响。特别研究了特定中间层MLP模块如何将token表示映射到与主注意力汇方向对齐的向量，以及这些向量的ℓ₂范数如何决定次级汇的分数和持续时间。

Result: 发现：(1) 次级汇由特定中间层MLP模块形成，这些MLP将token表示映射到与当前层主汇方向对齐的向量；(2) 这些向量的ℓ₂范数决定次级汇的分数和持续时间，从而影响注意力机制；(3) 主汇在中间层减弱，与次级汇的出现重合。在更大规模模型中，汇的位置和持续时间（统称为"汇层级"）以更确定和频繁的方式出现，如在QwQ-32B中识别出3个层级，在Qwen3-14B中识别出6个层级。

Conclusion: 论文系统识别和分析了与主注意力汇性质不同的次级注意力汇现象，揭示了特定MLP模块在形成这些汇中的关键作用，以及它们对注意力机制的动态影响，为理解大规模语言模型的注意力机制提供了新视角。

Abstract: Attention sinks are tokens, often the beginning-of-sequence (BOS) token, that receive disproportionately high attention despite limited semantic relevance. In this work, we identify a class of attention sinks, which we term secondary sinks, that differ fundamentally from the sinks studied in prior works, which we term primary sinks. While prior works have identified that tokens other than BOS can sometimes become sinks, they were found to exhibit properties analogous to the BOS token. Specifically, they emerge at the same layer, persist throughout the network and draw a large amount of attention mass. Whereas, we find the existence of secondary sinks that arise primarily in middle layers and can persist for a variable number of layers, and draw a smaller, but still significant, amount of attention mass. Through extensive experiments across 11 model families, we analyze where these secondary sinks appear, their properties, how they are formed, and their impact on the attention mechanism. Specifically, we show that: (1) these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink of that layer. (2) The $\ell_2$-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for, thereby leading to different impacts on the attention mechanisms accordingly. (3) The primary sink weakens in middle layers, coinciding with the emergence of secondary sinks. We observe that in larger-scale models, the location and lifetime of the sinks, together referred to as sink levels, appear in a more deterministic and frequent manner. Specifically, we identify three sink levels in QwQ-32B and six levels in Qwen3-14B.

</details>


### [35] [Predicting Mycotoxin Contamination in Irish Oats Using Deep and Transfer Learning](https://arxiv.org/abs/2512.22243)
*Alan Inglis,Fiona Doohan,Subramani Natarajan,Breige McNulty,Chris Elliott,Anne Nugent,Julie Meneely,Brett Greer,Stephen Kildea,Diana Bucur,Martin Danaher,Melissa Di Rocco,Lisa Black,Adam Gauley,Naoise McKenna,Andrew Parnell*

Main category: cs.LG

TL;DR: 该研究使用神经网络和迁移学习模型预测爱尔兰燕麦作物中的霉菌毒素污染，发现TabPFN迁移学习方法表现最佳，收获前90天的天气模式和种子含水量是最重要的预测因子。


<details>
  <summary>Details</summary>
Motivation: 霉菌毒素污染对谷物作物质量、食品安全和农业生产率构成重大风险。准确预测霉菌毒素水平可以支持早期干预策略并减少经济损失。

Method: 研究评估了五种建模方法：基线多层感知器（MLP）、带预训练的MLP，以及三种迁移学习模型（TabPFN、TabNet和FT-Transformer）。使用爱尔兰燕麦样本数据集，包含环境、农艺和地理预测因子。通过回归（RMSE、R²）和分类（AUC、F1）指标评估性能，并进行基于排列的变量重要性分析。

Result: 迁移学习方法TabPFN提供了整体最佳性能，其次是基线MLP。变量重要性分析显示，收获前90天的天气历史模式是最重要的预测因子，其次是种子含水量。

Conclusion: 迁移学习模型特别是TabPFN在预测霉菌毒素污染方面表现优异，收获前天气条件和种子含水量是关键影响因素，这为早期干预提供了重要指导。

Abstract: Mycotoxin contamination poses a significant risk to cereal crop quality, food safety, and agricultural productivity. Accurate prediction of mycotoxin levels can support early intervention strategies and reduce economic losses. This study investigates the use of neural networks and transfer learning models to predict mycotoxin contamination in Irish oat crops as a multi-response prediction task. Our dataset comprises oat samples collected in Ireland, containing a mix of environmental, agronomic, and geographical predictors. Five modelling approaches were evaluated: a baseline multilayer perceptron (MLP), an MLP with pre-training, and three transfer learning models; TabPFN, TabNet, and FT-Transformer. Model performance was evaluated using regression (RMSE, $R^2$) and classification (AUC, F1) metrics, with results reported per toxin and on average. Additionally, permutation-based variable importance analysis was conducted to identify the most influential predictors across both prediction tasks. The transfer learning approach TabPFN provided the overall best performance, followed by the baseline MLP. Our variable importance analysis revealed that weather history patterns in the 90-day pre-harvest period were the most important predictors, alongside seed moisture content.

</details>


### [36] [Interpretable and Adaptive Node Classification on Heterophilic Graphs via Combinatorial Scoring and Hybrid Learning](https://arxiv.org/abs/2512.22221)
*Soroush Vahidi*

Main category: cs.LG

TL;DR: 提出一个可解释的自适应框架用于半监督节点分类，基于组合推理而非深度消息传递，通过置信度排序的贪心算法分配标签，在异质性和过渡性基准上表现与GNNs相当，但具有更好的可解释性、可调性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在同性图上表现良好，但在异质性图上表现不佳，需要一种可解释且能自适应处理不同同质/异质程度的节点分类方法。

Method: 1) 使用置信度排序的贪心算法分配标签，基于加性评分函数整合类别先验、邻居统计、特征相似性和训练得到的标签兼容性；2) 引入验证门控混合策略，仅在验证性能提升时将组合预测作为先验注入轻量级神经网络；3) 所有自适应信号仅从训练数据计算，确保无泄漏评估。

Result: 在异质性和过渡性基准测试中，该方法与现代GNNs表现相当，同时在可解释性、可调性和计算效率方面具有优势。

Conclusion: 该方法提供了一种可解释、自适应且计算高效的半监督节点分类框架，能够平滑适应不同同质/异质程度的图结构，在保持性能的同时提供更好的透明度和可控性。

Abstract: Graph neural networks (GNNs) achieve strong performance on homophilic graphs but often struggle under heterophily, where adjacent nodes frequently belong to different classes. We propose an interpretable and adaptive framework for semi-supervised node classification based on explicit combinatorial inference rather than deep message passing. Our method assigns labels using a confidence-ordered greedy procedure driven by an additive scoring function that integrates class priors, neighborhood statistics, feature similarity, and training-derived label-label compatibility. A small set of transparent hyperparameters controls the relative influence of these components, enabling smooth adaptation between homophilic and heterophilic regimes.
  We further introduce a validation-gated hybrid strategy in which combinatorial predictions are optionally injected as priors into a lightweight neural model. Hybrid refinement is applied only when it improves validation performance, preserving interpretability when neuralization is unnecessary. All adaptation signals are computed strictly from training data, ensuring a leakage-free evaluation protocol. Experiments on heterophilic and transitional benchmarks demonstrate competitive performance with modern GNNs while offering advantages in interpretability, tunability, and computational efficiency.

</details>


### [37] [Completed Hyperparameter Transfer across Modules, Width, Depth, Batch and Duration](https://arxiv.org/abs/2512.22382)
*Bruno Mlodozeniec,Pierre Ablin,Louis Béthune,Dan Busbridge,Michal Klein,Jason Ramapuram,Marco Cuturi*

Main category: cs.LG

TL;DR: 论文提出Complete(d)参数化方法，统一处理宽度、深度、批大小和训练时长的缩放，并研究模块级超参数优化与迁移，显著提升大语言模型的训练速度。


<details>
  <summary>Details</summary>
Motivation: 超参数调优对大规模模型的训练稳定性和最终性能有巨大影响。现有方法如μP虽然支持跨模型大小的全局超参数迁移，但未能处理深度缩放和模块级超参数优化问题。

Method: 提出Complete(d)参数化方法，基于CompleteP的改进，统一处理宽度、深度、批大小和训练时长的缩放。研究模块级超参数优化，分析高维超参数空间的挑战，并提出实用优化指南。

Result: 实验证明，通过合适的参数化方法，模块级超参数迁移是可行的。研究覆盖了现代模型的各种优化超参数，包括学习率、AdamW参数、权重衰减、初始化尺度和残差块乘数。使用迁移的模块级超参数显著提升了大语言模型的训练速度。

Conclusion: Complete(d)参数化方法能够统一处理多个维度的缩放，模块级超参数优化与迁移在大规模模型训练中具有实际价值，能够显著提升训练效率。

Abstract: Hyperparameter tuning can dramatically impact training stability and final performance of large-scale models. Recent works on neural network parameterisations, such as $μ$P, have enabled transfer of optimal global hyperparameters across model sizes. These works propose an empirical practice of search for optimal global base hyperparameters at a small model size, and transfer to a large size. We extend these works in two key ways. To handle scaling along most important scaling axes, we propose the Complete$^{(d)}$ Parameterisation that unifies scaling in width and depth -- using an adaptation of CompleteP -- as well as in batch-size and training duration. Secondly, with our parameterisation, we investigate per-module hyperparameter optimisation and transfer. We characterise the empirical challenges of navigating the high-dimensional hyperparameter landscape, and propose practical guidelines for tackling this optimisation problem. We demonstrate that, with the right parameterisation, hyperparameter transfer holds even in the per-module hyperparameter regime. Our study covers an extensive range of optimisation hyperparameters of modern models: learning rates, AdamW parameters, weight decay, initialisation scales, and residual block multipliers. Our experiments demonstrate significant training speed improvements in Large Language Models with the transferred per-module hyperparameters.

</details>


### [38] [Müntz-Szász Networks: Neural Architectures with Learnable Power-Law Bases](https://arxiv.org/abs/2512.22222)
*Gnankan Landry Regis N'guessan*

Main category: cs.LG

TL;DR: 提出Müntz-Szász Networks (MSN)，用可学习的分数幂基函数替代固定激活函数，专门用于逼近具有奇异性或分数幂行为的函数，在物理问题中表现优异。


<details>
  <summary>Details</summary>
Motivation: 标准神经网络使用固定激活函数（ReLU、tanh、sigmoid）不适合逼近具有奇异性或分数幂行为的函数，这类函数在物理问题中普遍存在，如边界层、断裂力学和角点奇异性。

Method: 引入Müntz-Szász Networks (MSN)，用可学习的分数幂基函数替代固定激活函数。每个MSN边计算φ(x) = Σa_k|x|^μ_k + Σb_k sign(x)|x|^λ_k，其中指数{μ_k, λ_k}与系数一同学习。

Result: MSN继承了Müntz-Szász定理的通用逼近性质，对于|x|^α类函数，MSN使用单个学习指数可实现O(|μ-α|^2)误差，而标准MLP需要O(ε^{-1/α})神经元。在奇异性目标函数的监督回归中，MSN误差降低5-8倍且参数减少10倍。在PINN基准测试中，MSN实现3-6倍改进，并能学习到与已知解结构匹配的可解释指数。

Conclusion: 理论指导的架构设计可以为科学驱动的函数类带来显著改进，MSN在处理奇异性函数逼近方面优于传统神经网络。

Abstract: Standard neural network architectures employ fixed activation functions (ReLU, tanh, sigmoid) that are poorly suited for approximating functions with singular or fractional power behavior, a structure that arises ubiquitously in physics, including boundary layers, fracture mechanics, and corner singularities. We introduce Müntz-Szász Networks (MSN), a novel architecture that replaces fixed smooth activations with learnable fractional power bases grounded in classical approximation theory. Each MSN edge computes $φ(x) = \sum_k a_k |x|^{μ_k} + \sum_k b_k \mathrm{sign}(x)|x|^{λ_k}$, where the exponents $\{μ_k, λ_k\}$ are learned alongside the coefficients. We prove that MSN inherits universal approximation from the Müntz-Szász theorem and establish novel approximation rates: for functions of the form $|x|^α$, MSN achieves error $\mathcal{O}(|μ- α|^2)$ with a single learned exponent, whereas standard MLPs require $\mathcal{O}(ε^{-1/α})$ neurons for comparable accuracy. On supervised regression with singular target functions, MSN achieves 5-8x lower error than MLPs with 10x fewer parameters. Physics-informed neural networks (PINNs) represent a particularly demanding application for singular function approximation; on PINN benchmarks including a singular ODE and stiff boundary-layer problems, MSN achieves 3-6x improvement while learning interpretable exponents that match the known solution structure. Our results demonstrate that theory-guided architectural design can yield dramatic improvements for scientifically-motivated function classes.

</details>


### [39] [BLISS: Bandit Layer Importance Sampling Strategy for Efficient Training of Graph Neural Networks](https://arxiv.org/abs/2512.22388)
*Omar Alsaqa,Linh Thi Hoang,Muhammed Fatih Balin*

Main category: cs.LG

TL;DR: BLISS使用多臂老虎机动态选择GNN每层中最具信息量的节点，解决大图计算瓶颈，在保持精度的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在处理大图时面临计算成本高的问题，需要处理每个节点的所有邻居导致内存和计算瓶颈。现有静态采样方法无法适应节点重要性的动态变化。

Method: 提出BLISS（Bandit Layer Importance Sampling Strategy），使用多臂老虎机框架，在GNN的每一层动态选择最具信息量的节点。该方法平衡探索与利用，适应不同GNN架构（如GCN和GAT）的聚合机制。

Result: 实验表明BLISS能够保持或超过全批次训练的精度，同时显著提升计算效率，展现出良好的泛化能力。

Conclusion: BLISS通过动态自适应采样策略有效解决了GNN在大图上的计算瓶颈问题，为大规模图学习提供了高效解决方案。

Abstract: Graph Neural Networks (GNNs) are powerful tools for learning from graph-structured data, but their application to large graphs is hindered by computational costs. The need to process every neighbor for each node creates memory and computational bottlenecks. To address this, we introduce BLISS, a Bandit Layer Importance Sampling Strategy. It uses multi-armed bandits to dynamically select the most informative nodes at each layer, balancing exploration and exploitation to ensure comprehensive graph coverage. Unlike existing static sampling methods, BLISS adapts to evolving node importance, leading to more informed node selection and improved performance. It demonstrates versatility by integrating with both Graph Convolutional Networks (GCNs) and Graph Attention Networks (GATs), adapting its selection policy to their specific aggregation mechanisms. Experiments show that BLISS maintains or exceeds the accuracy of full-batch training.

</details>


### [40] [ReGAIN: Retrieval-Grounded AI Framework for Network Traffic Analysis](https://arxiv.org/abs/2512.22223)
*Shaghayegh Shajarian,Kennedy Marsh,James Benson,Sajad Khorsandroo,Mahmoud Abdelsalam*

Main category: cs.LG

TL;DR: ReGAIN是一个结合流量摘要、检索增强生成和LLM推理的多阶段网络流量分析框架，通过证据引用提供透明准确的分析，在真实流量数据集上达到95.95%-98.82%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统网络流量分析系统（基于规则或机器学习）存在高误报率和缺乏可解释性的问题，限制了分析师的信任度，需要更透明准确的解决方案。

Method: ReGAIN采用多阶段框架：1) 从网络流量生成自然语言摘要；2) 嵌入到多集合向量数据库；3) 使用分层检索管道（元数据过滤、MMR采样、两阶段交叉编码器重排序、弃权机制）来为LLM响应提供证据引用；4) 减少幻觉并确保基于证据的推理。

Result: 在真实世界流量数据集的ICMP ping flood和TCP SYN flood跟踪上，ReGAIN在不同攻击类型和评估基准上实现了95.95%到98.82%的准确率，结果通过数据集真实标签和人类专家评估双重验证。

Conclusion: ReGAIN超越了基于规则、传统机器学习和深度学习的基线方法，同时通过可信、可验证的响应提供了独特的可解释性，为网络流量分析提供了透明准确的解决方案。

Abstract: Modern networks generate vast, heterogeneous traffic that must be continuously analyzed for security and performance. Traditional network traffic analysis systems, whether rule-based or machine learning-driven, often suffer from high false positives and lack interpretability, limiting analyst trust. In this paper, we present ReGAIN, a multi-stage framework that combines traffic summarization, retrieval-augmented generation (RAG), and Large Language Model (LLM) reasoning for transparent and accurate network traffic analysis. ReGAIN creates natural-language summaries from network traffic, embeds them into a multi-collection vector database, and utilizes a hierarchical retrieval pipeline to ground LLM responses with evidence citations. The pipeline features metadata-based filtering, MMR sampling, a two-stage cross-encoder reranking mechanism, and an abstention mechanism to reduce hallucinations and ensure grounded reasoning. Evaluated on ICMP ping flood and TCP SYN flood traces from the real-world traffic dataset, it demonstrates robust performance, achieving accuracy between 95.95% and 98.82% across different attack types and evaluation benchmarks. These results are validated against two complementary sources: dataset ground truth and human expert assessments. ReGAIN also outperforms rule-based, classical ML, and deep learning baselines while providing unique explainability through trustworthy, verifiable responses.

</details>


### [41] [Causality-Inspired Safe Residual Correction for Multivariate Time Series](https://arxiv.org/abs/2512.22428)
*Jianxiang Xie,Yuncheng Hua*

Main category: cs.LG

TL;DR: CRC是一个因果启发的安全残差校正框架，通过解耦自变量和交叉变量动态，结合四重安全机制，确保预测性能不退化


<details>
  <summary>Details</summary>
Motivation: 现有Transformer和GNN等多元预测器存在系统性误差，缺乏部署性能保障；传统后处理残差校正方法虽然可能提高平均精度，但可能"错误地帮助"，在未见场景中导致局部失败

Method: 采用分而治之策略：1) 因果启发编码器通过解耦自变量和交叉变量动态暴露方向感知结构；2) 混合校正器建模残差误差；3) 关键的四重安全机制控制校正过程，防止有害更新

Result: 在多个数据集和预测骨干上的实验表明，CRC持续提高准确性；消融研究证实其核心安全机制确保极高的非退化率(NDR)，适合安全可靠部署

Conclusion: CRC是一个即插即用的安全残差校正框架，解决了预测模型部署中的"安全差距"，确保性能不退化，适合实际应用

Abstract: While modern multivariate forecasters such as Transformers and GNNs achieve strong benchmark performance, they often suffer from systematic errors at specific variables or horizons and, critically, lack guarantees against performance degradation in deployment. Existing post-hoc residual correction methods attempt to fix these errors, but are inherently greedy: although they may improve average accuracy, they can also "help in the wrong way" by overcorrecting reliable predictions and causing local failures in unseen scenarios.
  To address this critical "safety gap," we propose CRC (Causality-inspired Safe Residual Correction), a plug-and-play framework explicitly designed to ensure non-degradation. CRC follows a divide-and-conquer philosophy: it employs a causality-inspired encoder to expose direction-aware structure by decoupling self- and cross-variable dynamics, and a hybrid corrector to model residual errors. Crucially, the correction process is governed by a strict four-fold safety mechanism that prevents harmful updates.
  Experiments across multiple datasets and forecasting backbones show that CRC consistently improves accuracy, while an in-depth ablation study confirms that its core safety mechanisms ensure exceptionally high non-degradation rates (NDR), making CRC a correction framework suited for safe and reliable deployment.

</details>


### [42] [DiRL: An Efficient Post-Training Framework for Diffusion Language Models](https://arxiv.org/abs/2512.22234)
*Ying Zhu,Jiaxin Wan,Xiaoran Liu,Siyanag He,Qiqi Wang,Xu Guo,Tianyi Liang,Zengfeng Huang,Ziwei He,Xipeng Qiu*

Main category: cs.LG

TL;DR: DiRL是一个针对扩散语言模型的高效后训练框架，结合了FlexAttention加速的块状训练和LMDeploy优化的推理，通过两阶段后训练（监督微调+强化学习）提升数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型作为自回归模型的替代方案，其后训练方法存在计算效率低下、训练与推理目标不匹配等问题，限制了在复杂推理任务（如数学）上的性能。

Method: 提出DiRL框架：1）整合FlexAttention加速的块状训练和LMDeploy优化的推理，实现高效的在线模型更新循环；2）提出DiPO（首个针对dLLMs的无偏Group Relative Policy Optimization实现）；3）采用两阶段后训练（监督微调+强化学习）。

Result: DiRL-8B-Instruct在高质量数学数据上训练，在dLLMs中达到最先进的数学性能，并在多个基准测试中超越Qwen2.5系列的可比模型。

Conclusion: DiRL框架有效解决了dLLMs后训练的计算效率和目标匹配问题，显著提升了复杂推理任务的性能，为扩散语言模型的后训练提供了高效解决方案。

Abstract: Diffusion Language Models (dLLMs) have emerged as promising alternatives to Auto-Regressive (AR) models. While recent efforts have validated their pre-training potential and accelerated inference speeds, the post-training landscape for dLLMs remains underdeveloped. Existing methods suffer from computational inefficiency and objective mismatches between training and inference, severely limiting performance on complex reasoning tasks such as mathematics. To address this, we introduce DiRL, an efficient post-training framework that tightly integrates FlexAttention-accelerated blockwise training with LMDeploy-optimized inference. This architecture enables a streamlined online model update loop, facilitating efficient two-stage post-training (Supervised Fine-Tuning followed by Reinforcement Learning). Building on this framework, we propose DiPO, the first unbiased Group Relative Policy Optimization (GRPO) implementation tailored for dLLMs. We validate our approach by training DiRL-8B-Instruct on high-quality math data. Our model achieves state-of-the-art math performance among dLLMs and surpasses comparable models in the Qwen2.5 series on several benchmarks.

</details>


### [43] [The Bayesian Geometry of Transformer Attention](https://arxiv.org/abs/2512.22471)
*Naman Aggarwal,Siddhartha R. Dalal,Vishal Misra*

Main category: cs.LG

TL;DR: 小Transformer在受控环境中能精确执行贝叶斯推理，而容量匹配的MLP则失败，揭示了注意力机制在实现贝叶斯推理中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 验证Transformer是否真正执行贝叶斯推理一直很困难：自然数据缺乏解析后验，大模型混淆了推理和记忆。需要创建受控环境来严格验证。

Method: 构建"贝叶斯风洞"——后验分布已知且记忆被证明不可能的受控环境。在两个任务（双射消除和隐马尔可夫模型状态跟踪）中分析小Transformer的内部机制。

Result: 小Transformer以10^-3-10^-4比特精度重现贝叶斯后验，而容量匹配的MLP则失败数个数量级。Transformer通过几何机制实现贝叶斯推理：残差流作为信念基底，前馈网络执行后验更新，注意力提供内容寻址路由。

Conclusion: 分层注意力通过几何设计实现贝叶斯推理，解释了注意力机制的必要性和平面架构的失败。贝叶斯风洞为连接小可验证系统与大语言模型中的推理现象提供了基础。

Abstract: Transformers often appear to perform Bayesian reasoning in context, but verifying this rigorously has been impossible: natural data lack analytic posteriors, and large models conflate reasoning with memorization. We address this by constructing \emph{Bayesian wind tunnels} -- controlled environments where the true posterior is known in closed form and memorization is provably impossible. In these settings, small transformers reproduce Bayesian posteriors with $10^{-3}$-$10^{-4}$ bit accuracy, while capacity-matched MLPs fail by orders of magnitude, establishing a clear architectural separation.
  Across two tasks -- bijection elimination and Hidden Markov Model (HMM) state tracking -- we find that transformers implement Bayesian inference through a consistent geometric mechanism: residual streams serve as the belief substrate, feed-forward networks perform the posterior update, and attention provides content-addressable routing. Geometric diagnostics reveal orthogonal key bases, progressive query-key alignment, and a low-dimensional value manifold parameterized by posterior entropy. During training this manifold unfurls while attention patterns remain stable, a \emph{frame-precision dissociation} predicted by recent gradient analyses.
  Taken together, these results demonstrate that hierarchical attention realizes Bayesian inference by geometric design, explaining both the necessity of attention and the failure of flat architectures. Bayesian wind tunnels provide a foundation for mechanistically connecting small, verifiable systems to reasoning phenomena observed in large language models.

</details>


### [44] [Masking Teacher and Reinforcing Student for Distilling Vision-Language Models](https://arxiv.org/abs/2512.22238)
*Byung-Kwan Lee,Yu-Chiang Frank Wang,Ryo Hachiuma*

Main category: cs.LG

TL;DR: 提出Masters框架，通过掩码渐进式强化学习蒸馏，解决大教师模型与小学生模型之间的尺寸差距问题，实现高效知识迁移


<details>
  <summary>Details</summary>
Motivation: 大规模视觉语言模型虽然能力强但体积过大，难以部署到移动或边缘设备。需要紧凑但能力强的VLM，但大教师模型向小学生模型的知识蒸馏面临尺寸差距大、学生难以复现教师复杂表示、学习不稳定和性能下降等挑战

Method: 提出Masters框架：1) 掩码教师非主导权重降低复杂度；2) 渐进式恢复教师容量；3) 离线强化学习阶段，结合准确性奖励（衡量生成响应的正确性）和蒸馏奖励（量化教师到学生响应的转移难度）；4) 利用掩码教师预生成响应提供高效指导

Result: 学生模型能够以平滑稳定的方式从教师学习更丰富的表示，无需在线思考-回答过程，实现强大性能

Conclusion: Masters框架通过掩码渐进式强化学习蒸馏，有效解决了大教师模型向小学生模型知识迁移的挑战，实现了高效稳定的知识蒸馏，为部署紧凑视觉语言模型提供了可行方案

Abstract: Large-scale vision-language models (VLMs) have recently achieved remarkable multimodal understanding, but their massive size makes them impractical for deployment on mobile or edge devices. This raises the need for compact yet capable VLMs that can efficiently learn from powerful large teachers. However, distilling knowledge from a large teacher to a small student remains challenging due to their large size gap: the student often fails to reproduce the teacher's complex, high-dimensional representations, leading to unstable learning and degraded performance. To address this, we propose Masters (Masking Teacher and Reinforcing Student), a mask-progressive reinforcement learning (RL) distillation framework. Masters first masks non-dominant weights of the teacher to reduce unnecessary complexity, then progressively restores the teacher by gradually increasing its capacity during training. This strategy allows the student to learn richer representations from the teacher in a smooth and stable manner. To further refine knowledge transfer, Masters integrates an offline RL stage with two complementary rewards: an accuracy reward that measures the correctness of the generated responses, and a distillation reward that quantifies the ease of transferring responses from teacher to student. Unlike online think-answer RL paradigms that are computationally expensive and generate lengthy responses, our offline RL leverages pre-generated responses from masked teachers. These provide rich yet efficient guidance, enabling students to achieve strong performance without requiring the think-answer process.

</details>


### [45] [On Admissible Rank-based Input Normalization Operators](https://arxiv.org/abs/2512.22587)
*Taeyun Kim*

Main category: cs.LG

TL;DR: 该论文指出现有可微排序/排名算子在单调变换、批次变化和小扰动下不稳定，提出了三个公理来定义秩归一化算子应满足的稳定性条件，并构造了满足这些条件的算子。


<details>
  <summary>Details</summary>
Motivation: 秩归一化在机器学习中被广泛使用，因其对尺度、单调变换和批次变化的鲁棒性。然而，现有可微排序和排名算子在这些变换下不稳定，其结构条件从未被正式确定。

Method: 提出了三个公理来形式化秩归一化算子应满足的最小不变性和稳定性要求。证明满足这些公理的算子必须分解为特征级秩表示和单调Lipschitz连续标量化映射。构造了一个满足这些标准的最小算子。

Result: 证明了现有可微排序/排名算子因依赖值间隙和批次级成对交互而内在不稳定。提出的公理和构造的算子为有效的秩归一化算子划定了清晰的设计空间。

Conclusion: 该工作形式化地区分了有效的秩归一化算子和现有的基于连续松弛的排序方法，为设计稳定可靠的秩归一化算子提供了理论基础。

Abstract: Rank-based input normalization is a workhorse of modern machine learning, prized for its robustness to scale, monotone transformations, and batch-to-batch variation. In many real systems, the ordering of feature values matters far more than their raw magnitudes - yet the structural conditions that a rank-based normalization operator must satisfy to remain stable under these invariances have never been formally pinned down.
  We show that widely used differentiable sorting and ranking operators fundamentally fail these criteria. Because they rely on value gaps and batch-level pairwise interactions, they are intrinsically unstable under strictly monotone transformations, shifts in mini-batch composition, and even tiny input perturbations. Crucially, these failures stem from the operators' structural design, not from incidental implementation choices.
  To address this, we propose three axioms that formalize the minimal invariance and stability properties required of rank-based input normalization. We prove that any operator satisfying these axioms must factor into (i) a feature-wise rank representation and (ii) a scalarization map that is both monotone and Lipschitz-continuous. We then construct a minimal operator that meets these criteria and empirically show that the resulting constraints are non-trivial in realistic setups. Together, our results sharply delineate the design space of valid rank-based normalization operators and formally separate them from existing continuous-relaxation-based sorting methods.

</details>


### [46] [EvoXplain: When Machine Learning Models Agree on Predictions but Disagree on Why -- Measuring Mechanistic Multiplicity Across Training Runs](https://arxiv.org/abs/2512.22240)
*Chama Bensmail*

Main category: cs.LG

TL;DR: EvoXplain框架揭示高精度模型可能存在多种解释模式，即使性能相同，不同训练可能产生竞争性解释机制


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型评估过于依赖预测性能，假设高精度模型的解释是正确可信的，但忽略了不同模型可能通过不同内部逻辑达到相同结果的问题

Method: 提出EvoXplain诊断框架，将解释视为随机优化过程中的样本，分析重复训练中解释的稳定性，检测是否存在多种解释模式而非单一解释

Result: 在乳腺癌和COMPAS数据集上测试逻辑回归和随机森林，发现即使模型精度高，解释经常呈现明显多模态，甚至逻辑回归也会产生多个分离的解释盆地

Conclusion: EvoXplain使解释不稳定性可见且可量化，将可解释性重新定义为模型类在重复实例化下的属性，而非单个训练模型的属性

Abstract: Machine learning models are primarily judged by predictive performance, especially in applied settings. Once a model reaches high accuracy, its explanation is often assumed to be correct and trustworthy. However, this assumption raises an overlooked question: when two models achieve high accuracy, do they rely on the same internal logic, or do they reach the same outcome via different -- and potentially competing -- mechanisms? We introduce EvoXplain, a diagnostic framework that measures the stability of model explanations across repeated training. Rather than analysing a single trained model, EvoXplain treats explanations as samples drawn from the stochastic optimisation process itself -- without aggregating predictions or constructing ensembles -- and examines whether these samples form a single coherent explanation or separate into multiple, distinct explanatory modes. We evaluate EvoXplain on the Breast Cancer and COMPAS datasets using two widely deployed model classes: Logistic Regression and Random Forests. Although all models achieve high predictive accuracy, their explanations frequently exhibit clear multimodality. Even models commonly assumed to be stable, such as Logistic Regression, can produce multiple well-separated explanatory basins under repeated training on the same data split. These differences are not explained by hyperparameter variation or simple performance trade-offs. EvoXplain does not attempt to select a 'correct' explanation. Instead, it makes explanatory instability visible and quantifiable, revealing when single-instance or averaged explanations obscure the existence of multiple underlying mechanisms. More broadly, EvoXplain reframes interpretability as a property of a model class under repeated instantiation, rather than of any single trained model.

</details>


### [47] [Understanding the Mechanisms of Fast Hyperparameter Transfer](https://arxiv.org/abs/2512.22768)
*Nikhil Ghosh,Denny Wu,Alberto Bietti*

Main category: cs.LG

TL;DR: 该论文研究了深度学习模型规模扩大时超参数优化的可转移性问题，提出了一个理论框架来分析跨规模超参数转移的效率，并解释了μP参数化在实践中表现良好的机制。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型规模不断增大，使得标准的超参数优化变得极其昂贵。需要找到一种方法，能够将小规模网格搜索得到的最优超参数直接转移到大规模模型上，同时保持性能损失最小。

Method: 提出了一个通用的概念框架来分析跨规模超参数转移，将"快速转移"定义为转移引起的次优性渐近消失速度比有限规模性能差距更快。证明了快速转移对于计算最优网格搜索等同于有用转移。通过分解优化轨迹来分析损失减少的两个组成部分：宽度稳定组件和宽度敏感组件。

Result: 形式化证明了快速转移等价于有用转移，意味着转移在渐近意义上比直接调参更计算高效。发现μP参数化的快速转移特性取决于问题结构，在某些合成设置中转移可能提供计算优势，而在其他情况下即使使用μP也可能失败。提供了经验证据支持优化轨迹分解假设。

Conclusion: 跨规模超参数转移的有效性取决于问题结构，μP参数化在实践中表现良好是因为优化轨迹可以分解为决定最优超参数的宽度稳定组件和仅微弱扰动超参数最优值的宽度敏感组件。这一理解有助于设计更高效的大规模模型超参数优化策略。

Abstract: The growing scale of deep learning models has rendered standard hyperparameter (HP) optimization prohibitively expensive. A promising solution is the use of scale-aware hyperparameters, which can enable direct transfer of optimal HPs from small-scale grid searches to large models with minimal performance loss. To understand the principles governing such transfer strategy, we develop a general conceptual framework for reasoning about HP transfer across scale, characterizing transfer as fast when the suboptimality it induces vanishes asymptotically faster than the finite-scale performance gap. We show formally that fast transfer is equivalent to useful transfer for compute-optimal grid search, meaning that transfer is asymptotically more compute-efficient than direct tuning. While empirical work has found that the Maximal Update Parameterization ($μ$P) exhibits fast transfer when scaling model width, the mechanisms remain poorly understood. We show that this property depends critically on problem structure by presenting synthetic settings where transfer either offers provable computational advantage or fails to outperform direct tuning even under $μ$P. To explain the fast transfer observed in practice, we conjecture that decomposing the optimization trajectory reveals two contributions to loss reduction: (1) a width-stable component that determines the optimal HPs, and (2) a width-sensitive component that improves with width but weakly perturbs the HP optimum. We present empirical evidence for this hypothesis across various settings, including large language model pretraining.

</details>


### [48] [Enhanced geometry prediction in laser directed energy deposition using meta-learning](https://arxiv.org/abs/2512.22241)
*Abdul Malik Al Mardhouf Al Saadi,Amrita Basak*

Main category: cs.LG

TL;DR: 提出基于元学习的跨数据集知识转移模型，用于激光定向能量沉积中的焊道几何形状预测，能够在少量数据下快速适应新沉积条件。


<details>
  <summary>Details</summary>
Motivation: 激光定向能量沉积中，由于不同材料、机器配置和工艺参数下实验数据稀缺且异质性高，准确预测焊道几何形状存在困难。

Method: 采用两种基于梯度的元学习算法（MAML和Reptile），构建跨数据集知识转移框架，使用来自文献和内部实验的多数据集，涵盖粉末送料、线材送料和混合送料L-DED工艺。

Result: MAML和Reptile在仅需3-9个训练样本的情况下，就能在未见目标任务上实现准确的焊道高度预测，性能优于传统前馈神经网络。R²值可达约0.9，平均绝对误差在0.03-0.08mm之间。

Conclusion: 元学习方法在异质性L-DED设置中实现了有效的知识转移，能够在数据稀缺条件下实现强泛化性能，为L-DED焊道几何预测提供了有前景的解决方案。

Abstract: Accurate bead geometry prediction in laser-directed energy deposition (L-DED) is often hindered by the scarcity and heterogeneity of experimental datasets collected under different materials, machine configurations, and process parameters. To address this challenge, a cross-dataset knowledge transfer model based on meta-learning for predicting deposited track geometry in L-DED is proposed. Specifically, two gradient-based meta-learning algorithms, i.e., Model-Agnostic Meta-Learning (MAML) and Reptile, are investigated to enable rapid adaptation to new deposition conditions with limited data. The proposed framework is performed using multiple experimental datasets compiled from peer-reviewed literature and in-house experiments and evaluated across powder-fed, wire-fed, and hybrid wire-powder L-DED processes. Results show that both MAML and Reptile achieve accurate bead height predictions on unseen target tasks using as few as three to nine training examples, consistently outperforming conventional feedforward neural networks trained under comparable data constraints. Across multiple target tasks representing different printing conditions, the meta-learning models achieve strong generalization performance, with R-squared values reaching up to approximately 0.9 and mean absolute errors between 0.03-0.08 mm, demonstrating effective knowledge transfer across heterogeneous L-DED settings.

</details>


### [49] [Fundamental Novel Consistency Theory: $H$-Consistency Bounds](https://arxiv.org/abs/2512.22880)
*Yutao Zhong*

Main category: cs.LG

TL;DR: 该论文提出了H-一致性边界理论，为机器学习中代理损失函数与目标损失函数之间的估计误差提供理论保证，比贝叶斯一致性或H-校准更强，比超额误差边界更信息丰富。


<details>
  <summary>Details</summary>
Motivation: 机器学习训练中优化的损失函数常与定义任务性能的目标损失不同（由于计算不可行性或不可微性），需要理论分析代理损失与目标损失之间的估计误差关系。

Method: 提出H-一致性边界理论框架，首先在二分类中建立紧致的分布依赖和独立边界，分析凸代理损失（线性模型和神经网络）和对抗设置；扩展到多分类，为max、sum和约束损失提供H-一致性边界；研究comp-sum损失（如交叉熵、MAE）并引入平滑对抗变体；分析边界增长率和最小化差距。

Result: 建立了首个针对各种代理损失的H-一致性边界理论体系，包括二分类和多分类的凸损失、对抗损失、comp-sum损失等；发现某些情况下非平凡H-一致性边界不可达；证明了平滑代理损失在二分类和多分类任务中具有普适的平方根增长率。

Conclusion: H-一致性边界为代理损失选择提供了强有力的理论指导，比现有一致性概念更具信息性，最小化差距分析有助于选择最优代理损失函数，为鲁棒学习算法设计奠定了基础。

Abstract: In machine learning, the loss functions optimized during training often differ from the target loss that defines task performance due to computational intractability or lack of differentiability. We present an in-depth study of the target loss estimation error relative to the surrogate loss estimation error. Our analysis leads to $H$-consistency bounds, which are guarantees accounting for the hypothesis set $H$. These bounds offer stronger guarantees than Bayes-consistency or $H$-calibration and are more informative than excess error bounds.
  We begin with binary classification, establishing tight distribution-dependent and -independent bounds. We provide explicit bounds for convex surrogates (including linear models and neural networks) and analyze the adversarial setting for surrogates like $ρ$-margin and sigmoid loss. Extending to multi-class classification, we present the first $H$-consistency bounds for max, sum, and constrained losses, covering both non-adversarial and adversarial scenarios. We demonstrate that in some cases, non-trivial $H$-consistency bounds are unattainable. We also investigate comp-sum losses (e.g., cross-entropy, MAE), deriving their first $H$-consistency bounds and introducing smooth adversarial variants that yield robust learning algorithms.
  We develop a comprehensive framework for deriving these bounds across various surrogates, introducing new characterizations for constrained and comp-sum losses. Finally, we examine the growth rates of $H$-consistency bounds, establishing a universal square-root growth rate for smooth surrogates in binary and multi-class tasks, and analyze minimizability gaps to guide surrogate selection.

</details>


### [50] [Fairness Evaluation of Risk Estimation Models for Lung Cancer Screening](https://arxiv.org/abs/2512.22242)
*Shaurya Gaur,Michel Vitale,Alessa Hering,Johan Kwisthout,Colin Jacobs,Lena Philipp,Fennie van der Graaf*

Main category: cs.LG

TL;DR: 该研究使用JustEFAB框架评估了两种深度学习肺癌风险预测模型（Sybil和Venkadesh21）及传统逻辑回归模型在不同人口亚组中的性能差异，发现存在统计学显著的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 肺癌是全球癌症相关死亡的主要原因，低剂量CT筛查可早期发现但可能加重放射科医生负担。AI模型在肺癌风险评估中显示出潜力，但不同人口群体的性能差异和公平性问题尚未明确。

Method: 基于JustEFAB框架，评估Sybil和Venkadesh21两种深度学习模型以及PanCan2b逻辑回归模型。使用美国国家肺癌筛查试验(NLST)数据训练，在保留验证集上评估。分析不同人口亚组（性别、种族）的AUROC、敏感性和特异性，并探索临床混杂因素的影响。

Result: Sybil模型在女性（AUROC 0.88）和男性（AUROC 0.81）间存在统计学显著差异（p<0.001）。Venkadesh21模型在90%特异性下，黑人参与者敏感性（0.39）显著低于白人参与者（0.69）。这些差异无法用现有临床混杂因素解释，根据JustEFAB框架可归类为不公平偏差。

Conclusion: 研究强调了在肺癌筛查中改进和监测模型在不同亚组中性能的重要性，需要进一步研究算法公平性，确保AI模型在不同人口群体中的公平应用。

Abstract: Lung cancer is the leading cause of cancer-related mortality in adults worldwide. Screening high-risk individuals with annual low-dose CT (LDCT) can support earlier detection and reduce deaths, but widespread implementation may strain the already limited radiology workforce. AI models have shown potential in estimating lung cancer risk from LDCT scans. However, high-risk populations for lung cancer are diverse, and these models' performance across demographic groups remains an open question. In this study, we drew on the considerations on confounding factors and ethically significant biases outlined in the JustEFAB framework to evaluate potential performance disparities and fairness in two deep learning risk estimation models for lung cancer screening: the Sybil lung cancer risk model and the Venkadesh21 nodule risk estimator. We also examined disparities in the PanCan2b logistic regression model recommended in the British Thoracic Society nodule management guideline. Both deep learning models were trained on data from the US-based National Lung Screening Trial (NLST), and assessed on a held-out NLST validation set. We evaluated AUROC, sensitivity, and specificity across demographic subgroups, and explored potential confounding from clinical risk factors. We observed a statistically significant AUROC difference in Sybil's performance between women (0.88, 95% CI: 0.86, 0.90) and men (0.81, 95% CI: 0.78, 0.84, p < .001). At 90% specificity, Venkadesh21 showed lower sensitivity for Black (0.39, 95% CI: 0.23, 0.59) than White participants (0.69, 95% CI: 0.65, 0.73). These differences were not explained by available clinical confounders and thus may be classified as unfair biases according to JustEFAB. Our findings highlight the importance of improving and monitoring model performance across underrepresented subgroups, and further research on algorithmic fairness, in lung cancer screening.

</details>


### [51] [Theory and Algorithms for Learning with Multi-Class Abstention and Multi-Expert Deferral](https://arxiv.org/abs/2512.22886)
*Anqi Mao*

Main category: cs.LG

TL;DR: 该论文系统研究了多专家延迟学习问题，包括分类中的弃权和回归中的延迟，提出了具有强一致性保证的新损失函数和算法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型面临幻觉和高推理成本挑战，通过将不确定输入延迟给更强大的专家可以提高可靠性，将简单查询路由到更小的蒸馏模型可以提高效率，这激发了多专家延迟学习问题的研究。

Method: 1) 针对分类中的弃权问题，提出基于分数和预测器-拒绝器的新损失函数族；2) 针对多专家延迟分类，设计单阶段和两阶段场景的损失函数；3) 针对回归延迟，提出支持多专家和多种成本结构的通用框架。

Result: 证明了所提损失函数具有强H-一致性保证，在CIFAR-10、CIFAR-100和SVHN等数据集上的实验表明算法性能优越，特别是在两阶段场景中，对于常数成本函数实现了可实现的H-一致性。

Conclusion: 该论文为多专家延迟学习提供了全面的理论框架和实用算法，解决了现有开放性问题，在分类和回归任务中都实现了强一致性保证和优越的实证性能。

Abstract: Large language models (LLMs) have achieved remarkable performance but face critical challenges: hallucinations and high inference costs. Leveraging multiple experts offers a solution: deferring uncertain inputs to more capable experts improves reliability, while routing simpler queries to smaller, distilled models enhances efficiency. This motivates the problem of learning with multiple-expert deferral. This thesis presents a comprehensive study of this problem and the related problem of learning with abstention, supported by strong consistency guarantees.
  First, for learning with abstention (a special case of deferral), we analyze score-based and predictor-rejector formulations in multi-class classification. We introduce new families of surrogate losses and prove strong non-asymptotic, hypothesis set-specific consistency guarantees, resolving two existing open questions. We analyze both single-stage and practical two-stage settings, with experiments on CIFAR-10, CIFAR-100, and SVHN demonstrating the superior performance of our algorithms.
  Second, we address general multi-expert deferral in classification. We design new surrogate losses for both single-stage and two-stage scenarios and prove they benefit from strong $H$-consistency bounds. For the two-stage scenario, we show that our surrogate losses are realizable $H$-consistent for constant cost functions, leading to effective new algorithms.
  Finally, we introduce a novel framework for regression with deferral to address continuous label spaces. Our versatile framework accommodates multiple experts and various cost structures, supporting both single-stage and two-stage methods. It subsumes recent work on regression with abstention. We propose new surrogate losses with proven $H$-consistency and demonstrate the empirical effectiveness of the resulting algorithms.

</details>


### [52] [Trust Region Masking for Long-Horizon LLM Reinforcement Learning](https://arxiv.org/abs/2512.23075)
*Yingru Li,Jiacai Liu,Jiawei Xu,Yuxuan Tong,Ziniu Li,Baoxiang Wang*

Main category: cs.LG

TL;DR: 论文提出了Trust Region Masking (TRM)方法，通过排除违反信任区域的整个序列来提供长序列LLM强化学习的非空单调改进保证。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的策略梯度方法存在离策略不匹配问题，传统信任区域边界随序列长度T呈O(T^2)增长，对于长序列任务变得无效，需要更紧的边界来保证单调改进。

Method: 推导了两个更紧的边界：Pinsker-Marginal边界(O(T^{3/2}))和Mixed边界(O(T))，并提出Trust Region Masking (TRM)方法，当序列中任何令牌违反信任区域时排除整个序列的梯度计算。

Result: TRM方法提供了第一个非空单调改进保证，适用于长序列LLM强化学习任务，解决了传统PPO裁剪等方法无法控制的序列级KL散度问题。

Conclusion: 论文通过更紧的信任区域边界和TRM方法，解决了长序列LLM强化学习中的离策略近似误差问题，为实际应用提供了理论保证。

Abstract: Policy gradient methods for large language models optimize a surrogate objective computed from samples of a rollout policy $π_{\text{roll}}$. When $π_{\text{roll}} \ne π_θ$, there is approximation error between the surrogate and the true objective. Prior work has shown that this off-policy mismatch is unavoidable in modern LLM-RL due to implementation divergence, mixture-of-experts routing discontinuities, and distributed training staleness. Classical trust region bounds on the resulting error scale as $O(T^2)$ with sequence length $T$, rendering them vacuous for long-horizon tasks. We derive two tighter bounds: a Pinsker-Marginal bound scaling as $O(T^{3/2})$ and a Mixed bound scaling as $O(T)$. Crucially, both bounds depend on $D_{kl}^{tok,max}$ -- the maximum token-level KL divergence across all positions in a sequence. This is inherently a sequence-level quantity: it requires examining the entire trajectory to compute, and therefore cannot be controlled by token-independent methods like PPO clipping. We propose Trust Region Masking (TRM), which excludes entire sequences from gradient computation if any token violates the trust region, providing the first non-vacuous monotonic improvement guarantees for long-horizon LLM-RL.

</details>


### [53] [Communication Compression for Distributed Learning with Aggregate and Server-Guided Feedback](https://arxiv.org/abs/2512.22623)
*Tomas Ortega,Chun-Yin Huang,Xiaoxiao Li,Hamid Jafarkhani*

Main category: cs.LG

TL;DR: 提出两种无需客户端状态或控制变量的有偏压缩框架CAFe和CAFe-S，解决联邦学习中通信瓶颈和隐私问题，在非凸场景下证明优于现有压缩方案。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临客户端到服务器上行传输的通信瓶颈，现有有偏压缩技术需要误差反馈机制，但标准误差反馈依赖客户端特定控制变量，这违反用户隐私且与大规模FL中常见的无状态客户端不兼容。

Method: 提出两种新框架：1) CAFe使用上一轮全局聚合更新作为所有客户端的共享控制变量；2) CAFe-S在服务器拥有小型私有数据集时，生成服务器引导的候选更新作为更准确的预测器。基于分布式梯度下降进行分析。

Result: 在非凸场景和有界梯度差异下，理论证明CAFe优于分布式压缩梯度下降；CAFe-S收敛到平稳点，且收敛速率随服务器数据代表性增强而提升。FL场景实验验证了方法优于现有压缩方案。

Conclusion: CAFe和CAFe-S框架有效解决了联邦学习中有偏压缩的通信瓶颈和隐私问题，无需客户端状态，在理论和实验上均表现出优越性，为大规模FL部署提供了实用解决方案。

Abstract: Distributed learning, particularly Federated Learning (FL), faces a significant bottleneck in the communication cost, particularly the uplink transmission of client-to-server updates, which is often constrained by asymmetric bandwidth limits at the edge. Biased compression techniques are effective in practice, but require error feedback mechanisms to provide theoretical guarantees and to ensure convergence when compression is aggressive. Standard error feedback, however, relies on client-specific control variates, which violates user privacy and is incompatible with stateless clients common in large-scale FL. This paper proposes two novel frameworks that enable biased compression without client-side state or control variates. The first, Compressed Aggregate Feedback (CAFe), uses the globally aggregated update from the previous round as a shared control variate for all clients. The second, Server-Guided Compressed Aggregate Feedback (CAFe-S), extends this idea to scenarios where the server possesses a small private dataset; it generates a server-guided candidate update to be used as a more accurate predictor. We consider Distributed Gradient Descent (DGD) as a representative algorithm and analytically prove CAFe's superiority to Distributed Compressed Gradient Descent (DCGD) with biased compression in the non-convex regime with bounded gradient dissimilarity. We further prove that CAFe-S converges to a stationary point, with a rate that improves as the server's data become more representative. Experimental results in FL scenarios validate the superiority of our approaches over existing compression schemes.

</details>


### [54] [Calibrating LLM Judges: Linear Probes for Fast and Reliable Uncertainty Estimation](https://arxiv.org/abs/2512.22245)
*Bhaktipriya Radharapu,Eshika Saxena,Kenneth Li,Chenxi Whitehouse,Adina Williams,Nicola Cancedda*

Main category: cs.LG

TL;DR: 使用线性探针从LLM推理法官的隐藏状态获取校准的不确定性估计，相比现有方法节省约10倍计算成本，提供更好的校准效果


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的法官在工业应用中变得重要，需要高效获取良好校准的不确定性估计以支持生产部署。现有方法（如语言化置信度和多生成方法）要么校准效果差，要么计算成本高。

Method: 引入基于Brier分数的损失函数训练的线性探针，从推理法官的隐藏状态提供校准的不确定性估计，无需额外的模型训练

Result: 探针在客观任务（推理、数学、事实性、编码）和主观人类偏好判断上都表现出优于现有方法的校准效果，节省约10倍计算成本，对未见评估域具有鲁棒泛化能力，在高置信度预测上提供更高准确性

Conclusion: 基于可解释性的不确定性估计为LLM法官提供了实用、可扩展的即插即用解决方案，虽然会产生保守估计（在简单数据集上表现不佳），但可能有利于优先考虑低误报率的安全关键部署

Abstract: As LLM-based judges become integral to industry applications, obtaining well-calibrated uncertainty estimates efficiently has become critical for production deployment. However, existing techniques, such as verbalized confidence and multi-generation methods, are often either poorly calibrated or computationally expensive. We introduce linear probes trained with a Brier score-based loss to provide calibrated uncertainty estimates from reasoning judges' hidden states, requiring no additional model training. We evaluate our approach on both objective tasks (reasoning, mathematics, factuality, coding) and subjective human preference judgments. Our results demonstrate that probes achieve superior calibration compared to existing methods with $\approx10$x computational savings, generalize robustly to unseen evaluation domains, and deliver higher accuracy on high-confidence predictions. However, probes produce conservative estimates that underperform on easier datasets but may benefit safety-critical deployments prioritizing low false-positive rates. Overall, our work demonstrates that interpretability-based uncertainty estimation provides a practical and scalable plug-and-play solution for LLM judges in production.

</details>


### [55] [Taming the Tail: Stable LLM Reinforcement Learning via Dynamic Vocabulary Pruning](https://arxiv.org/abs/2512.23087)
*Yingru Li,Jiawei Xu,Jiacai Liu,Yuxuan Tong,Ziniu Li,Tianle Cai,Ge Zhang,Qian Liu,Baoxiang Wang*

Main category: cs.LG

TL;DR: 论文提出通过动态剪枝词汇表来解决LLM强化学习中训练-推理不匹配问题，排除极端低概率token以稳定训练


<details>
  <summary>Details</summary>
Motivation: 大型语言模型强化学习面临训练-推理不匹配问题：高吞吐量推理引擎和数值精确训练系统从相同参数产生不同的概率分布，这种不匹配对低概率token影响更大，导致梯度估计不稳定

Method: 提出将RL目标约束到动态剪枝的"安全"词汇表，排除极端尾部的token，用小的有界优化偏差换取大的系统性不匹配

Result: 方法实现了稳定的训练，理论上界定了词汇表剪枝引入的优化偏差

Conclusion: 通过动态剪枝词汇表排除极端低概率token，可以有效解决训练-推理不匹配问题，实现稳定的强化学习训练

Abstract: Reinforcement learning for large language models (LLMs) faces a fundamental tension: high-throughput inference engines and numerically-precise training systems produce different probability distributions from the same parameters, creating a training-inference mismatch. We prove this mismatch has an asymmetric effect: the bound on log-probability mismatch scales as $(1-p)$ where $p$ is the token probability. For high-probability tokens, this bound vanishes, contributing negligibly to sequence-level mismatch. For low-probability tokens in the tail, the bound remains large, and moreover, when sampled, these tokens exhibit systematically biased mismatches that accumulate over sequences, destabilizing gradient estimation. Rather than applying post-hoc corrections, we propose constraining the RL objective to a dynamically-pruned ``safe'' vocabulary that excludes the extreme tail. By pruning such tokens, we trade large, systematically biased mismatches for a small, bounded optimization bias. Empirically, our method achieves stable training; theoretically, we bound the optimization bias introduced by vocabulary pruning.

</details>


### [56] [SNM-Net: A Universal Framework for Robust Open-Set Gas Recognition via Spherical Normalization and Mahalanobis Distance](https://arxiv.org/abs/2512.22792)
*Shuai Chen,Chen Wang,Ziran Wang*

Main category: cs.LG

TL;DR: 提出SNM-Net框架，通过几何解耦和马氏距离解决电子鼻开放集气体识别中的特征漂移和未知干扰问题，在Vergara数据集上达到接近理论极限的性能。


<details>
  <summary>Details</summary>
Motivation: 电子鼻系统在开放集气体识别中面临双重挑战：信号漂移导致特征分布偏移，以及未知干扰引起的决策失败。现有方法主要依赖欧氏距离，未能充分考虑气体特征分布的各向异性和动态信号强度变化。

Method: 提出SNM-Net深度学习框架，核心创新包括：1）通过级联批归一化和L2归一化实现几何解耦机制，将高维特征投影到单位超球面以消除信号强度波动；2）引入马氏距离作为评分机制，利用类统计信息构建自适应椭球决策边界。该框架与CNN、RNN和Transformer骨干网络兼容。

Result: 在Vergara数据集上，Transformer+SNM配置达到接近理论极限的性能：AUROC为0.9977，未知气体检测率为99.57%（5% FPR下的TPR）。相比最先进方法，AUROC提升3.0%，标准差降低91.0%（相比Class Anchor Clustering）。在不同传感器位置下表现出卓越鲁棒性，标准差低于0.0028。

Conclusion: SNM-Net有效解决了准确性与稳定性之间的权衡问题，为工业电子鼻部署提供了坚实的技术基础。该框架具有架构无关性，能显著提升开放集气体识别性能。

Abstract: Electronic nose (E-nose) systems face dual challenges in open-set gas recognition: feature distribution shifts caused by signal drift and decision failures induced by unknown interference. Existing methods predominantly rely on Euclidean distance, failing to adequately account for anisotropic gas feature distributions and dynamic signal intensity variations. To address these issues, this study proposes SNM-Net, a universal deep learning framework for open-set gas recognition. The core innovation lies in a geometric decoupling mechanism achieved through cascaded batch normalization and L2 normalization, which projects high-dimensional features onto a unit hypersphere to eliminate signal intensity fluctuations. Additionally, Mahalanobis distance is introduced as the scoring mechanism, utilizing class-wise statistics to construct adaptive ellipsoidal decision boundaries. SNM-Net is architecture-agnostic and seamlessly integrates with CNN, RNN, and Transformer backbones. Systematic experiments on the Vergara dataset demonstrate that the Transformer+SNM configuration attains near-theoretical performance, achieving an AUROC of 0.9977 and an unknown gas detection rate of 99.57% (TPR at 5% FPR). This performance significantly outperforms state-of-the-art methods, showing a 3.0% improvement in AUROC and a 91.0% reduction in standard deviation compared to Class Anchor Clustering. The framework exhibits exceptional robustness across sensor positions with standard deviations below 0.0028. This work effectively resolves the trade-off between accuracy and stability, providing a solid technical foundation for industrial E-nose deployment.

</details>


### [57] [The Affine Divergence: Aligning Activation Updates Beyond Normalisation](https://arxiv.org/abs/2512.22247)
*George Bird*

Main category: cs.LG

TL;DR: 论文提出激活更新在梯度下降中存在系统性不匹配问题，通过数学分析推导出归一化的新理论框架，并提出替代传统归一化的新方法


<details>
  <summary>Details</summary>
Motivation: 激活更新在梯度下降中未能沿最陡下降方向更新，而激活作为更直接影响损失的量，其优化应优先考虑。传统归一化方法缺乏理论依据，需要从第一性原理重新理解

Method: 通过数学分析揭示激活更新的非理想缩放问题，从最陡下降原则推导归一化形式，提出两种新方法：1）替代仿射映射的尺度非不变方案；2）卷积层的"PatchNorm"组合不可分归一化

Result: 新方法在多个测试中优于传统归一化器，提供了归一化的替代机制框架，将归一化器重新解释为具有参数化缩放的类激活函数映射

Conclusion: 从理论原则出发建立了归一化的新理解框架，提出了实证有效的新函数，对传统"仿射+非线性"模型构建方法提出质疑，为优化中表示优先提供了新视角

Abstract: A systematic mismatch exists between mathematically ideal and effective activation updates during gradient descent. As intended, parameters update in their direction of steepest descent. However, activations are argued to constitute a more directly impactful quantity to prioritise in optimisation, as they are closer to the loss in the computational graph and carry sample-dependent information through the network. Yet their propagated updates do not take the optimal steepest-descent step. These quantities exhibit non-ideal sample-wise scaling across affine, convolutional, and attention layers. Solutions to correct for this are trivial and, entirely incidentally, derive normalisation from first principles despite motivational independence. Consequently, such considerations offer a fresh and conceptual reframe of normalisation's action, with auxiliary experiments bolstering this mechanistically. Moreover, this analysis makes clear a second possibility: a solution that is functionally distinct from modern normalisations, without scale-invariance, yet remains empirically successful, outperforming conventional normalisers across several tests. This is presented as an alternative to the affine map. This generalises to convolution via a new functional form, "PatchNorm", a compositionally inseparable normaliser. Together, these provide an alternative mechanistic framework that adds to, and counters some of, the discussion of normalisation. Further, it is argued that normalisers are better decomposed into activation-function-like maps with parameterised scaling, thereby aiding the prioritisation of representations during optimisation. Overall, this constitutes a theoretical-principled approach that yields several new functions that are empirically validated and raises questions about the affine + nonlinear approach to model creation.

</details>


### [58] [How Much Data Is Enough? Uniform Convergence Bounds for Generative & Vision-Language Models under Low-Dimensional Structure](https://arxiv.org/abs/2512.23109)
*Paul M. Thompson*

Main category: cs.LG

TL;DR: 研究生成模型和视觉语言模型在生物医学应用中如何实现均匀准确的预测和校准，而非仅平均表现良好，特别关注有限样本下的理论保证。


<details>
  <summary>Details</summary>
Motivation: 生物医学决策支持需要模型预测既准确又校准良好，但当前模型在罕见条件和特定群体上可能表现不佳，即使整体损失较低。需要理解在什么条件下模型能实现均匀可靠的预测。

Method: 分析通过提示或语义嵌入在受限表示空间中诱导的分类器族。假设模型输出对低维语义表示具有平滑依赖性，使用经典均匀收敛工具推导有限样本保证。

Result: 在提示嵌入的Lipschitz稳定性条件下，给出了VLM诱导分类器的准确性和校准泛函的有限样本均匀收敛界。样本复杂度取决于内在/有效维度而非环境嵌入维度，并推导了谱依赖界显示特征值衰减如何控制数据需求。

Conclusion: 该研究为数据有限的生物医学建模提供了理论依据，阐明了当前数据集大小何时能支持均匀可靠的预测，以及为什么平均校准指标可能遗漏最坏情况下的校准错误。

Abstract: Modern generative and vision-language models (VLMs) are increasingly used in scientific and medical decision support, where predicted probabilities must be both accurate and well calibrated. Despite strong empirical results with moderate data, it remains unclear when such predictions generalize uniformly across inputs, classes, or subpopulations, rather than only on average-a critical issue in biomedicine, where rare conditions and specific groups can exhibit large errors even when overall loss is low.
  We study this question from a finite-sample perspective and ask: under what structural assumptions can generative and VLM-based predictors achieve uniformly accurate and calibrated behavior with practical sample sizes? Rather than analyzing arbitrary parameterizations, we focus on induced families of classifiers obtained by varying prompts or semantic embeddings within a restricted representation space. When model outputs depend smoothly on a low-dimensional semantic representation-an assumption supported by spectral structure in text and joint image-text embeddings-classical uniform convergence tools yield meaningful non-asymptotic guarantees.
  Our main results give finite-sample uniform convergence bounds for accuracy and calibration functionals of VLM-induced classifiers under Lipschitz stability with respect to prompt embeddings. The implied sample complexity depends on intrinsic/effective dimension, not ambient embedding dimension, and we further derive spectrum-dependent bounds that make explicit how eigenvalue decay governs data requirements. We conclude with implications for data-limited biomedical modeling, including when current dataset sizes can support uniformly reliable predictions and why average calibration metrics may miss worst-case miscalibration.

</details>


### [59] [Amortized Inference for Model Rocket Aerodynamics: Learning to Estimate Physical Parameters from Simulation](https://arxiv.org/abs/2512.22248)
*Rohit Pandey,Rohan Pandey*

Main category: cs.LG

TL;DR: 使用基于模拟的摊销推理方法，通过神经网络在合成飞行数据上训练，无需真实数据即可预测模型火箭的气动参数，实现从模拟到现实的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 传统模型火箭飞行性能预测方法依赖计算流体动力学或经验相关性，而数据驱动方法需要大量昂贵的真实飞行数据。需要一种无需真实训练数据就能准确预测气动参数的方法。

Method: 提出基于模拟的摊销推理方法：1）使用物理模拟器生成10,000次合成飞行数据；2）训练神经网络学习逆向物理模型，从单次远地点测量值结合发动机和配置特征直接预测阻力系数和推力修正因子；3）将学习到的模型直接应用于真实飞行，无需微调。

Result: 在8次真实飞行测试中，远地点预测的平均绝对误差为12.3米，相比OpenRocket基线预测减少了误差。分析显示预测存在系统性正偏差，量化了理想物理模型与现实飞行条件之间的差距。

Conclusion: 该方法成功实现了从模拟到现实的零样本迁移，仅使用合成数据训练就能准确预测真实火箭飞行性能，为业余火箭社区提供了可复现的解决方案，并揭示了物理模型与真实条件之间的量化差距。

Abstract: Accurate prediction of model rocket flight performance requires estimating aerodynamic parameters that are difficult to measure directly. Traditional approaches rely on computational fluid dynamics or empirical correlations, while data-driven methods require extensive real flight data that is expensive and time-consuming to collect. We present a simulation-based amortized inference approach that trains a neural network on synthetic flight data generated from a physics simulator, then applies the learned model to real flights without any fine-tuning. Our method learns to invert the forward physics model, directly predicting drag coefficient and thrust correction factor from a single apogee measurement combined with motor and configuration features. In this proof-of-concept study, we train on 10,000 synthetic flights and evaluate on 8 real flights, achieving a mean absolute error of 12.3 m in apogee prediction - demonstrating promising sim-to-real transfer with zero real training examples. Analysis reveals a systematic positive bias in predictions, providing quantitative insight into the gap between idealized physics and real-world flight conditions. We additionally compare against OpenRocket baseline predictions, showing that our learned approach reduces apogee prediction error. Our implementation is publicly available to support reproducibility and adoption in the amateur rocketry community.

</details>


### [60] [Principled Algorithms for Optimizing Generalized Metrics in Binary Classification](https://arxiv.org/abs/2512.23133)
*Anqi Mao,Mehryar Mohri,Yutao Zhong*

Main category: cs.LG

TL;DR: 提出METRO算法，用于优化类别不平衡或成本不对称场景下的广义度量指标（如Fβ、AM、Jaccard等），通过将度量优化转化为广义成本敏感学习问题，设计具有H一致性的代理损失函数，并提供有限样本性能保证。


<details>
  <summary>Details</summary>
Motivation: 在类别不平衡或成本不对称的应用中，传统二元分类损失不适用，而Fβ、AM、Jaccard等度量指标更合适。但优化这些指标存在计算和统计挑战，现有方法通常基于贝叶斯最优分类器，使用阈值方法先估计类别概率再寻找最优阈值，导致算法不适用于受限假设集且缺乏有限样本性能保证。

Method: 将度量优化重新表述为广义成本敏感学习问题，设计具有可证明H一致性保证的新型代理损失函数。基于此框架开发METRO算法，提供H一致性和有限样本泛化边界理论保证。

Result: 实验结果表明，METRO方法相比先前基线方法更有效。算法具有强理论性能保证，包括H一致性和有限样本泛化边界。

Conclusion: 提出了一种原则性的广义度量优化方法METRO，通过将度量优化转化为成本敏感学习问题，设计了具有理论保证的代理损失函数，解决了现有方法在受限假设集和有限样本性能方面的局限性。

Abstract: In applications with significant class imbalance or asymmetric costs, metrics such as the $F_β$-measure, AM measure, Jaccard similarity coefficient, and weighted accuracy offer more suitable evaluation criteria than standard binary classification loss. However, optimizing these metrics present significant computational and statistical challenges. Existing approaches often rely on the characterization of the Bayes-optimal classifier, and use threshold-based methods that first estimate class probabilities and then seek an optimal threshold. This leads to algorithms that are not tailored to restricted hypothesis sets and lack finite-sample performance guarantees. In this work, we introduce principled algorithms for optimizing generalized metrics, supported by $H$-consistency and finite-sample generalization bounds. Our approach reformulates metric optimization as a generalized cost-sensitive learning problem, enabling the design of novel surrogate loss functions with provable $H$-consistency guarantees. Leveraging this framework, we develop new algorithms, METRO (Metric Optimization), with strong theoretical performance guarantees. We report the results of experiments demonstrating the effectiveness of our methods compared to prior baselines.

</details>


### [61] [Temporal Visual Semantics-Induced Human Motion Understanding with Large Language Models](https://arxiv.org/abs/2512.22249)
*Zheng Xing,Weibing Zhao*

Main category: cs.LG

TL;DR: 本文提出了一种结合时间视觉语义和子空间聚类的人体运动分割方法，利用大语言模型从连续帧中提取文本运动信息，并通过时间正则化提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 传统的人体运动分割方法忽视了时间语义探索的重要性。本文旨在利用大语言模型的图像到文本能力，从人体运动序列中提取时间视觉语义，以增强子空间聚类的性能。

Method: 通过LLM查询连续帧是否描述相同运动，学习时间相邻信息；开发TVS集成子空间聚类方法，包含时间正则化的子空间嵌入；引入反馈框架，基于分割输出持续优化子空间嵌入。

Result: 在四个基准人体运动数据集上的实验结果表明，所提方法优于现有的最先进方法。

Conclusion: 通过结合时间视觉语义和子空间聚类，本文提出的方法能够有效提升无监督人体运动分割的性能，证明了时间语义信息在运动分割中的重要性。

Abstract: Unsupervised human motion segmentation (HMS) can be effectively achieved using subspace clustering techniques. However, traditional methods overlook the role of temporal semantic exploration in HMS. This paper explores the use of temporal vision semantics (TVS) derived from human motion sequences, leveraging the image-to-text capabilities of a large language model (LLM) to enhance subspace clustering performance. The core idea is to extract textual motion information from consecutive frames via LLM and incorporate this learned information into the subspace clustering framework. The primary challenge lies in learning TVS from human motion sequences using LLM and integrating this information into subspace clustering. To address this, we determine whether consecutive frames depict the same motion by querying the LLM and subsequently learn temporal neighboring information based on its response. We then develop a TVS-integrated subspace clustering approach, incorporating subspace embedding with a temporal regularizer that induces each frame to share similar subspace embeddings with its temporal neighbors. Additionally, segmentation is performed based on subspace embedding with a temporal constraint that induces the grouping of each frame with its temporal neighbors. We also introduce a feedback-enabled framework that continuously optimizes subspace embedding based on the segmentation output. Experimental results demonstrate that the proposed method outperforms existing state-of-the-art approaches on four benchmark human motion datasets.

</details>


### [62] [A Simple, Optimal and Efficient Algorithm for Online Exp-Concave Optimization](https://arxiv.org/abs/2512.23190)
*Yi-Han Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: LightONS算法通过延迟昂贵的马氏投影，将OXO计算复杂度从O(d^ωT)降低到O(d²T + d^ω√T)，同时保持最优O(d log T)遗憾界，解决了SXO的COLT'13开放问题。


<details>
  <summary>Details</summary>
Motivation: 在线指数凹优化(OXO)的标准算法ONS存在计算瓶颈，其马氏投影步骤成本高达Ω(d^ω)次算术运算，导致总运行时间达到O(d^ωT)。对于随机指数凹优化(SXO)，使用在线到批量转换的ONS需要O(d^{ω+1}/ε)运行时间，这促使了COLT'13开放问题的提出。

Method: 提出LightONS算法，这是ONS的简单变体。通过利用参数无关在线学习中的域转换技术，引入滞后机制，延迟昂贵的马氏投影直到必要时才执行。保留了ONS的优雅结构，同时显著降低计算复杂度。

Result: LightONS将总运行时间减少到O(d²T + d^ω√T log T)，同时保持最优的O(d log T)遗憾界。对于SXO问题，实现了O(d³/ε)的运行时间，解决了COLT'13开放问题。

Conclusion: LightONS算法通过巧妙的滞后机制设计，在保持最优统计性能的同时大幅降低计算复杂度，不仅解决了SXO的开放问题，还能作为ONS的高效替代方案应用于更广泛的场景，包括梯度范数自适应遗憾、参数化随机赌博机和内存高效在线学习。

Abstract: Online eXp-concave Optimization (OXO) is a fundamental problem in online learning. The standard algorithm, Online Newton Step (ONS), balances statistical optimality and computational practicality, guaranteeing an optimal regret of $O(d \log T)$, where $d$ is the dimension and $T$ is the time horizon. ONS faces a computational bottleneck due to the Mahalanobis projections at each round. This step costs $Ω(d^ω)$ arithmetic operations for bounded domains, even for the unit ball, where $ω\in (2,3]$ is the matrix-multiplication exponent. As a result, the total runtime can reach $\tilde{O}(d^ωT)$, particularly when iterates frequently oscillate near the domain boundary. For Stochastic eXp-concave Optimization (SXO), computational cost is also a challenge. Deploying ONS with online-to-batch conversion for SXO requires $T = \tilde{O}(d/ε)$ rounds to achieve an excess risk of $ε$, and thereby necessitates an $\tilde{O}(d^{ω+1}/ε)$ runtime. A COLT'13 open problem posed by Koren [2013] asks for an SXO algorithm with runtime less than $\tilde{O}(d^{ω+1}/ε)$.
  This paper proposes a simple variant of ONS, LightONS, which reduces the total runtime to $O(d^2 T + d^ω\sqrt{T \log T})$ while preserving the optimal $O(d \log T)$ regret. LightONS implies an SXO method with runtime $\tilde{O}(d^3/ε)$, thereby answering the open problem. Importantly, LightONS preserves the elegant structure of ONS by leveraging domain-conversion techniques from parameter-free online learning to introduce a hysteresis mechanism that delays expensive Mahalanobis projections until necessary. This design enables LightONS to serve as an efficient plug-in replacement of ONS in broader scenarios, even beyond regret minimization, including gradient-norm adaptive regret, parametric stochastic bandits, and memory-efficient online learning.

</details>


### [63] [Interpretable Perturbation Modeling Through Biomedical Knowledge Graphs](https://arxiv.org/abs/2512.22251)
*Pascal Passigan,Kevin zhu,Angelina Ning*

Main category: cs.LG

TL;DR: 该研究构建了一个融合生物医学知识图谱与药物-细胞系数据的异质图，使用图注意力网络预测药物对基因表达的扰动效应，超越了传统二元药物-疾病关联任务。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架主要关注知识图谱链接预测和二元药物-疾病关联任务，而药物对基因表达的扰动效应能更深入揭示药物作用机制，但目前缺乏相关研究。

Method: 构建融合PrimeKG++知识图谱和LINCS L1000药物-细胞系数据的异质图，使用MolFormerXL和BioBERT等基础模型生成多模态嵌入，训练图注意力网络预测978个标志基因的表达变化。

Result: 该框架在支架分割和随机分割下均优于多层感知机基线，边洗牌和节点特征随机化消融实验证明生物医学知识图谱的边信息能增强扰动水平预测。

Conclusion: 该研究提供了一种机制性药物建模路径，从二元药物-疾病关联任务转向更精细的转录效应预测，有助于理解药物作用机制和发现药物再利用机会。

Abstract: Understanding how small molecules perturb gene expression is essential for uncovering drug mechanisms, predicting off-target effects, and identifying repurposing opportunities. While prior deep learning frameworks have integrated multimodal embeddings into biomedical knowledge graphs (BKGs) and further improved these representations through graph neural network message-passing paradigms, these models have been applied to tasks such as link prediction and binary drug-disease association, rather than the task of gene perturbation, which may unveil more about mechanistic transcriptomic effects. To address this gap, we construct a merged biomedical graph that integrates (i) PrimeKG++, an augmentation of PrimeKG containing semantically rich embeddings for nodes with (ii) LINCS L1000 drug and cell line nodes, initialized with multimodal embeddings from foundation models such as MolFormerXL and BioBERT. Using this heterogeneous graph, we train a graph attention network (GAT) with a downstream prediction head that learns the delta expression profile of over 978 landmark genes for a given drug-cell pair. Our results show that our framework outperforms MLP baselines for differentially expressed genes (DEG) -- which predict the delta expression given a concatenated embedding of drug features, target features, and baseline cell expression -- under the scaffold and random splits. Ablation experiments with edge shuffling and node feature randomization further demonstrate that the edges provided by biomedical KGs enhance perturbation-level prediction. More broadly, our framework provides a path toward mechanistic drug modeling: moving beyond binary drug-disease association tasks to granular transcriptional effects of therapeutic intervention.

</details>


### [64] [On the Sample Complexity of Learning for Blind Inverse Problems](https://arxiv.org/abs/2512.23405)
*Nathan Buskulic,Luca Calatroni,Lorenzo Rosasco,Silvia Villa*

Main category: cs.LG

TL;DR: 该论文研究了盲逆问题中的学习理论，在线性最小均方误差估计器框架下，提供了闭式最优解、与Tikhonov正则化的等价关系、收敛性分析和有限样本误差界，并通过数值实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 盲逆问题在许多实验设置中出现，其中前向算子部分或完全未知。现有的数据驱动方法虽然表现出强大的经验性能，但缺乏可解释性和严格的理论保证，限制了其在成像等应用领域的可靠性。

Method: 在线性最小均方误差估计器(LMMSE)的简化框架下进行研究，推导最优估计器的闭式表达式，建立与Tikhonov正则化公式的等价关系，其中正则化项明确依赖于未知信号、噪声和随机前向算子的分布。在适当的源条件假设下证明收敛结果，并推导严格的有限样本误差界。

Result: 提供了闭式最优估计器表达式，扩展了经典结果。建立了与Tikhonov正则化公式的等价关系。证明了在源条件下的收敛性。推导了有限样本误差界，明确量化了算子随机性的影响，并揭示了当随机性消失时的收敛速率。

Conclusion: 该工作为盲逆问题中的学习提供了深入的理论分析，建立了严格的理论保证，并通过数值实验验证了理论预测的收敛行为，为数据驱动方法在盲逆问题中的应用提供了可靠的理论基础。

Abstract: Blind inverse problems arise in many experimental settings where the forward operator is partially or entirely unknown. In this context, methods developed for the non-blind case cannot be adapted in a straightforward manner. Recently, data-driven approaches have been proposed to address blind inverse problems, demonstrating strong empirical performance and adaptability. However, these methods often lack interpretability and are not supported by rigorous theoretical guarantees, limiting their reliability in applied domains such as imaging inverse problems. In this work, we shed light on learning in blind inverse problems within the simplified yet insightful framework of Linear Minimum Mean Square Estimators (LMMSEs). We provide an in-depth theoretical analysis, deriving closed-form expressions for optimal estimators and extending classical results. In particular, we establish equivalences with suitably chosen Tikhonov-regularized formulations, where the regularization depends explicitly on the distributions of the unknown signal, the noise, and the random forward operators. We also prove convergence results under appropriate source condition assumptions. Furthermore, we derive rigorous finite-sample error bounds that characterize the performance of learned estimators as a function of the noise level, problem conditioning, and number of available samples. These bounds explicitly quantify the impact of operator randomness and reveal the associated convergence rates as this randomness vanishes. Finally, we validate our theoretical findings through illustrative numerical experiments that confirm the predicted convergence behavior.

</details>


### [65] [Graph Attention-based Adaptive Transfer Learning for Link Prediction](https://arxiv.org/abs/2512.22252)
*Huashen Lu,Wensheng Gan,Guoting Chen,Zhichao Huang,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出GAATNet图注意力自适应迁移网络，结合预训练和微调解决大规模稀疏图链接预测中的迁移学习问题，通过引入远邻嵌入和轻量适配器提升泛化能力和训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有GNN链接预测方法在处理大规模稀疏图和跨数据集迁移学习时面临挑战，特别是不同数据集间需要高度对齐。虽然自监督方法在图任务中取得成功，但先前研究忽视了迁移学习在不同图数据集间的泛化潜力。

Method: 提出Graph Attention Adaptive Transfer Network (GAATNet)，结合预训练和微调捕获不同规模数据集的全局节点嵌入信息。设计两个关键策略：1) 在自注意力模块中引入远邻嵌入作为偏置以捕获全局特征；2) 在微调阶段引入轻量自适配器模块提升训练效率。

Result: 在七个公开数据集上的综合实验表明，GAATNet在链接预测任务中达到了最先进的性能，为有效整合GNN与迁移学习提供了通用且可扩展的解决方案。

Conclusion: GAATNet通过创新的迁移学习框架解决了GNN链接预测中的关键挑战，在保持高效训练的同时实现了优异的跨数据集泛化能力，为图神经网络与迁移学习的结合提供了新思路。

Abstract: Graph neural networks (GNNs) have brought revolutionary advancements to the field of link prediction (LP), providing powerful tools for mining potential relationships in graphs. However, existing methods face challenges when dealing with large-scale sparse graphs and the need for a high degree of alignment between different datasets in transfer learning. Besides, although self-supervised methods have achieved remarkable success in many graph tasks, prior research has overlooked the potential of transfer learning to generalize across different graph datasets. To address these limitations, we propose a novel Graph Attention Adaptive Transfer Network (GAATNet). It combines the advantages of pre-training and fine-tuning to capture global node embedding information across datasets of different scales, ensuring efficient knowledge transfer and improved LP performance. To enhance the model's generalization ability and accelerate training, we design two key strategies: 1) Incorporate distant neighbor embeddings as biases in the self-attention module to capture global features. 2) Introduce a lightweight self-adapter module during fine-tuning to improve training efficiency. Comprehensive experiments on seven public datasets demonstrate that GAATNet achieves state-of-the-art performance in LP tasks. This study provides a general and scalable solution for LP tasks to effectively integrate GNNs with transfer learning. The source code and datasets are publicly available at https://github.com/DSI-Lab1/GAATNet

</details>


### [66] [ML Compass: Navigating Capability, Cost, and Compliance Trade-offs in AI Model Deployment](https://arxiv.org/abs/2512.23487)
*Vassilis Digalakis,Ramayya Krishnan,Gonzalo Martin Fernandez,Agni Orfanoudaki*

Main category: cs.LG

TL;DR: ML Compass框架将模型选择视为能力-成本前沿上的约束优化问题，帮助组织在考虑用户效用、部署成本和合规要求时做出更好的AI模型选择决策。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的能力排行榜不能直接转化为部署决策，存在能力-部署差距。组织在选择AI模型时需要综合考虑用户效用、部署成本和合规要求，但缺乏系统性的决策框架。

Method: 提出ML Compass框架，将模型选择视为能力-成本前沿上的约束优化问题。包括理论分析（参数化前沿下的最优配置特征）和实施流程（提取内部度量、估计经验前沿、学习效用函数、推荐模型）。

Result: 在通用对话（PRISM Alignment数据集）和医疗（HealthBench自定义数据集）两个案例研究中验证，ML Compass产生的推荐和部署感知排行榜与纯能力排名有显著差异，能更好地平衡能力、成本和安全性。

Conclusion: ML Compass框架能够有效弥合能力-部署差距，为组织提供系统化的AI模型选择方法，在考虑多种约束条件下优化部署价值。

Abstract: We study how organizations should select among competing AI models when user utility, deployment costs, and compliance requirements jointly matter. Widely used capability leaderboards do not translate directly into deployment decisions, creating a capability -- deployment gap; to bridge it, we take a systems-level view in which model choice is tied to application outcomes, operating constraints, and a capability-cost frontier. We develop ML Compass, a framework that treats model selection as constrained optimization over this frontier. On the theory side, we characterize optimal model configurations under a parametric frontier and show a three-regime structure in optimal internal measures: some dimensions are pinned at compliance minima, some saturate at maximum levels, and the remainder take interior values governed by frontier curvature. We derive comparative statics that quantify how budget changes, regulatory tightening, and technological progress propagate across capability dimensions and costs. On the implementation side, we propose a pipeline that (i) extracts low-dimensional internal measures from heterogeneous model descriptors, (ii) estimates an empirical frontier from capability and cost data, (iii) learns a user- or task-specific utility function from interaction outcome data, and (iv) uses these components to target capability-cost profiles and recommend models. We validate ML Compass with two case studies: a general-purpose conversational setting using the PRISM Alignment dataset and a healthcare setting using a custom dataset we build using HealthBench. In both environments, our framework produces recommendations -- and deployment-aware leaderboards based on predicted deployment value under constraints -- that can differ materially from capability-only rankings, and clarifies how trade-offs between capability, cost, and safety shape optimal model choice.

</details>


### [67] [Cardiac mortality prediction in patients undergoing PCI based on real and synthetic data](https://arxiv.org/abs/2512.22259)
*Daniil Burakov,Ivan Petrov,Dmitrii Khelimskii,Ivan Bessonov,Mikhail Lazarev*

Main category: cs.LG

TL;DR: 开发基于真实和合成数据的PCI术后心脏死亡风险预测模型，通过数据增强改善类别不平衡问题，识别出年龄、射血分数等关键影响因素。


<details>
  <summary>Details</summary>
Motivation: PCI术后患者状态、血管造影和手术特征包含预测长期预后的关键信号，但现有模型在处理类别不平衡时对少数类（心脏死亡）识别能力不足，需要开发更稳健的预测模型。

Method: 分析2044例分叉病变PCI患者数据，应用多种机器学习模型预测3年死亡率。为解决类别不平衡问题，生成500个合成样本加入训练集。使用置换特征重要性评估特征贡献，并通过移除非信息特征进行额外实验。

Result: 数据增强后，所有模型在保持高AUROC的同时显著提高少数类召回率，改善概率质量，对构建的严重病例产生更合理的风险估计。年龄、射血分数、外周动脉疾病和脑血管疾病被识别为最具影响力的四个特征。

Conclusion: 简单的数据增强方法能够暴露、量化和减少不平衡临床预测中的脆弱性，建议在报告模型性能时常规包含概率质量和压力测试指标。

Abstract: Patient status, angiographic and procedural characteristics encode crucial signals for predicting long-term outcomes after percutaneous coronary intervention (PCI). The aim of the study was to develop a predictive model for assessing the risk of cardiac death based on the real and synthetic data of patients undergoing PCI and to identify the factors that have the greatest impact on mortality. We analyzed 2,044 patients, who underwent a PCI for bifurcation lesions. The primary outcome was cardiac death at 3-year follow-up. Several machine learning models were applied to predict three-year mortality after PCI. To address class imbalance and improve the representation of the minority class, an additional 500 synthetic samples were generated and added to the training set. To evaluate the contribution of individual features to model performance, we applied permutation feature importance. An additional experiment was conducted to evaluate how the model's predictions would change after removing non-informative features from the training and test datasets. Without oversampling, all models achieve high overall accuracy (0.92-0.93), yet they almost completely ignore the minority class. Across models, augmentation consistently increases minority-class recall with minimal loss of AUROC, improves probability quality, and yields more clinically reasonable risk estimates on the constructed severe profiles. According to feature importance analysis, four features emerged as the most influential: Age, Ejection Fraction, Peripheral Artery Disease, and Cerebrovascular Disease. These results show that straightforward augmentation with realistic and extreme cases can expose, quantify, and reduce brittleness in imbalanced clinical prediction using only tabular records, and motivate routine reporting of probability quality and stress tests alongside headline metrics.

</details>


### [68] [Trustworthy Machine Learning under Distribution Shifts](https://arxiv.org/abs/2512.23524)
*Zhuo Huang*

Main category: cs.LG

TL;DR: 该研究专注于分布偏移下的可信机器学习，通过分析扰动偏移、域偏移和模态偏移三种常见分布偏移，从鲁棒性、可解释性和适应性三个维度提出解决方案，旨在提升AI系统的可靠性、通用性和安全性。


<details>
  <summary>Details</summary>
Motivation: 尽管机器学习在AI领域取得了显著进展，从ResNet到Transformer再到LLMs，但分布偏移问题仍然是ML系统的"阿喀琉斯之踵"，限制了系统的可靠性和通用性，并引发了AI的信任问题。研究旨在解决这一根本挑战。

Method: 将分布偏移系统分类为三种类型：扰动偏移、域偏移和模态偏移。从三个可信度维度（鲁棒性、可解释性、适应性）进行严格研究，针对每种偏移场景提出有效的解决方案和理论见解。

Result: 研究提出了针对不同分布偏移类型的可信机器学习框架，旨在增强机器学习的关键问题，如效率、适应性和安全性，为构建更可靠、更通用的AI系统提供理论基础和实践方案。

Conclusion: 分布偏移是限制ML系统可靠性和通用性的核心问题。通过系统研究三种分布偏移类型和三个可信度维度，该研究为构建更鲁棒、可解释和自适应的AI系统提供了重要方向，有助于推动可信机器学习的发展。

Abstract: Machine Learning (ML) has been a foundational topic in artificial intelligence (AI), providing both theoretical groundwork and practical tools for its exciting advancements. From ResNet for visual recognition to Transformer for vision-language alignment, the AI models have achieved superior capability to humans. Furthermore, the scaling law has enabled AI to initially develop general intelligence, as demonstrated by Large Language Models (LLMs). To this stage, AI has had an enormous influence on society and yet still keeps shaping the future for humanity. However, distribution shift remains a persistent ``Achilles' heel'', fundamentally limiting the reliability and general usefulness of ML systems. Moreover, generalization under distribution shift would also cause trust issues for AIs. Motivated by these challenges, my research focuses on \textit{Trustworthy Machine Learning under Distribution Shifts}, with the goal of expanding AI's robustness, versatility, as well as its responsibility and reliability. We carefully study the three common distribution shifts into: (1) Perturbation Shift, (2) Domain Shift, and (3) Modality Shift. For all scenarios, we also rigorously investigate trustworthiness via three aspects: (1) Robustness, (2) Explainability, and (3) Adaptability. Based on these dimensions, we propose effective solutions and fundamental insights, meanwhile aiming to enhance the critical ML problems, such as efficiency, adaptability, and safety.

</details>


### [69] [The Physics Constraint Paradox: When Removing Explicit Constraints Improves Physics-Informed Data for Machine Learning](https://arxiv.org/abs/2512.22261)
*Rahul D Ray*

Main category: cs.LG

TL;DR: 研究通过系统消融实验揭示物理约束的冗余性：显式能量守恒约束在物理一致方程中数学冗余，而法布里-珀罗振荡主导带宽变化，移除后可提升机器学习预测精度。


<details>
  <summary>Details</summary>
Motivation: 在科学领域中，真实数据稀缺时物理约束数据生成至关重要，但现有方法往往过度约束模型，未能识别哪些物理组件真正必要。需要系统评估不同物理约束对数据生成和机器学习性能的影响。

Method: 对物理信息光栅耦合器光谱生成器进行系统消融研究，选择性移除：1)显式能量守恒约束，2)法布里-珀罗振荡，3)带宽变化，4)噪声。生成器将5个几何参数映射到100点光谱响应，评估不同约束变体的性能。

Result: 1)显式能量守恒约束在物理一致方程中数学冗余，约束与非约束变体达到相同守恒精度(平均误差约7×10⁻⁹)；2)法布里-珀罗振荡主导带宽变化，移除后半高宽分布减少72%(从132.3nm降至37.4nm)；3)标准噪声添加-重归一化流程引入0.5%非物理负吸收值；4)生成器速度达200样本/秒，远快于全波求解器；5)下游机器学习显示物理可学习性权衡：移除法布里-珀罗振荡使带宽预测R²提升31.3%，RMSE降低73.8%。

Conclusion: 研究揭示了物理约束的冗余性，为物理信息数据集设计提供实用指导，并强调机器学习性能可作为评估约束相关性的诊断工具。显式能量守恒约束在物理一致模型中不必要，而法布里-珀罗振荡的移除可显著改善机器学习预测性能。

Abstract: Physics-constrained data generation is essential for machine learning in scientific domains where real data are scarce; however, existing approaches often over-constrain models without identifying which physical components are necessary. We present a systematic ablation study of a physics-informed grating coupler spectrum generator that maps five geometric parameters to 100-point spectral responses. By selectively removing explicit energy conservation enforcement, Fabry-Perot oscillations, bandwidth variation, and noise, we uncover a physics constraint paradox: explicit energy conservation enforcement is mathematically redundant when the underlying equations are physically consistent, with constrained and unconstrained variants achieving identical conservation accuracy (mean error approximately 7 x 10^-9). In contrast, Fabry-Perot oscillations dominate threshold-based bandwidth variability, accounting for a 72 percent reduction in half-maximum bandwidth spread when removed (with bandwidth spread reduced from 132.3 nm to 37.4 nm). We further identify a subtle pitfall: standard noise-addition-plus-renormalization pipelines introduce 0.5 percent unphysical negative absorption values. The generator operates at 200 samples per second, enabling high-throughput data generation and remaining orders of magnitude faster than typical full-wave solvers reported in the literature. Finally, downstream machine learning evaluation reveals a clear physics-learnability trade-off: while central wavelength prediction remains unaffected, removing Fabry-Perot oscillations improves bandwidth prediction accuracy by 31.3 percent in R-squared and reduces RMSE by 73.8 percent. These findings provide actionable guidance for physics-informed dataset design and highlight machine learning performance as a diagnostic tool for assessing constraint relevance.

</details>


### [70] [Le Cam Distortion: A Decision-Theoretic Framework for Robust Transfer Learning](https://arxiv.org/abs/2512.23617)
*Deniz Akdemir*

Main category: cs.LG

TL;DR: 本文提出Le Cam Distortion框架，通过方向性可模拟性替代对称不变性，解决传统无监督域适应中因强制特征对齐导致信息破坏和负迁移的问题。


<details>
  <summary>Details</summary>
Motivation: 传统无监督域适应方法强制源域和目标域特征对齐，但在域间信息量不均等时（如高质量vs降质传感器），这种对称不变性会导致信息破坏，引发负迁移甚至灾难性后果，特别是在安全关键应用中。

Method: 基于Le Cam统计实验理论，提出决策理论框架，用方向性可模拟性替代对称不变性。引入Le Cam Distortion（通过缺陷距离δ(E₁, E₂)量化）作为可模拟性条件下迁移风险的严格上界。学习从源域模拟目标域的核函数，实现无源域性能损失的迁移。

Result: 在五个实验中表现优异：1) HLA基因组学中实现近乎完美的频率估计（相关性r=0.999）；2) CIFAR-10图像分类中保持81.2%准确率，而CycleGAN下降34.7%；3) RL控制中实现安全策略迁移，而基于不变性的方法出现灾难性崩溃。

Conclusion: Le Cam Distortion为风险可控的迁移学习提供了首个理论框架，特别适用于医学影像、自主系统和精准医疗等负迁移不可接受的领域。

Abstract: Distribution shift is the defining challenge of real-world machine learning. The dominant paradigm--Unsupervised Domain Adaptation (UDA)--enforces feature invariance, aligning source and target representations via symmetric divergence minimization [Ganin et al., 2016]. We demonstrate that this approach is fundamentally flawed: when domains are unequally informative (e.g., high-quality vs degraded sensors), strict invariance necessitates information destruction, causing "negative transfer" that can be catastrophic in safety-critical applications [Wang et al., 2019].
  We propose a decision-theoretic framework grounded in Le Cam's theory of statistical experiments [Le Cam, 1986], using constructive approximations to replace symmetric invariance with directional simulability. We introduce Le Cam Distortion, quantified by the Deficiency Distance $δ(E_1, E_2)$, as a rigorous upper bound for transfer risk conditional on simulability. Our framework enables transfer without source degradation by learning a kernel that simulates the target from the source. Across five experiments (genomics, vision, reinforcement learning), Le Cam Distortion achieves: (1) near-perfect frequency estimation in HLA genomics (correlation $r=0.999$, matching classical methods), (2) zero source utility loss in CIFAR-10 image classification (81.2% accuracy preserved vs 34.7% drop for CycleGAN), and (3) safe policy transfer in RL control where invariance-based methods suffer catastrophic collapse. Le Cam Distortion provides the first principled framework for risk-controlled transfer learning in domains where negative transfer is unacceptable: medical imaging, autonomous systems, and precision medicine.

</details>


### [71] [LuxIA: A Lightweight Unitary matriX-based Framework Built on an Iterative Algorithm for Photonic Neural Network Training](https://arxiv.org/abs/2512.22264)
*Tzamn Melendez Carmona,Federico Marchesin,Marco P. Abrate,Peter Bienstman,Stefano Di Carlo,Alessandro Savino Senior*

Main category: cs.LG

TL;DR: 提出LuxIA框架和Slicing方法，通过高效传输矩阵计算解决光子神经网络训练中的可扩展性问题，显著降低内存和时间消耗


<details>
  <summary>Details</summary>
Motivation: 当前光子神经网络模拟工具在训练大规模网络时面临可扩展性挑战，传输矩阵计算导致高内存和时间消耗，限制了光子神经网络的发展和应用

Method: 提出Slicing方法作为高效的传输矩阵计算技术，兼容反向传播算法，并将其集成到统一的模拟和训练框架LuxIA中

Result: LuxIA在MNIST、Digits和Olivetti Faces等数据集上的实验表明，相比现有工具在速度和可扩展性方面有显著提升，能够支持更大规模光子神经网络的模拟和训练

Conclusion: LuxIA框架通过解决关键计算瓶颈，推动了光子神经网络模拟技术的发展，为探索和优化更复杂的光子架构铺平了道路，促进了光子AI硬件的创新和采用

Abstract: PNNs present promising opportunities for accelerating machine learning by leveraging the unique benefits of photonic circuits. However, current state of the art PNN simulation tools face significant scalability challenges when training large-scale PNNs, due to the computational demands of transfer matrix calculations, resulting in high memory and time consumption. To overcome these limitations, we introduce the Slicing method, an efficient transfer matrix computation approach compatible with back-propagation. We integrate this method into LuxIA, a unified simulation and training framework. The Slicing method substantially reduces memory usage and execution time, enabling scalable simulation and training of large PNNs. Experimental evaluations across various photonic architectures and standard datasets, including MNIST, Digits, and Olivetti Faces, show that LuxIA consistently surpasses existing tools in speed and scalability. Our results advance the state of the art in PNN simulation, making it feasible to explore and optimize larger, more complex architectures. By addressing key computational bottlenecks, LuxIA facilitates broader adoption and accelerates innovation in AI hardware through photonic technologies. This work paves the way for more efficient and scalable photonic neural network research and development.

</details>


### [72] [Random Controlled Differential Equations](https://arxiv.org/abs/2512.23670)
*Francesco Piatti,Thomas Cass,William F. Turner*

Main category: cs.LG

TL;DR: 提出结合随机特征与控制微分方程的训练高效时间序列学习框架，仅训练线性读出层，实现快速可扩展模型。包含两个变体：随机傅里叶CDE和随机粗糙DE，在无限宽度极限下分别诱导RBF提升签名核和粗糙签名核。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列学习方法计算成本高，特别是显式签名计算效率低下。需要一种既能保留签名理论归纳偏置，又能实现高效训练的方法。

Method: 使用大型随机参数化控制微分方程作为连续时间储层，仅训练线性读出层。提出两个变体：1) RF-CDE：使用随机傅里叶特征提升输入信号；2) R-RDE：通过log-ODE离散化直接处理粗糙路径输入，使用对数签名捕获高阶时间交互。

Result: 在多个时间序列基准测试中表现出竞争性或最先进的性能。证明在无限宽度极限下，模型分别诱导RBF提升签名核和粗糙签名核。

Conclusion: 该方法为显式签名计算提供了实用替代方案，既保留了签名理论的归纳偏置，又受益于随机特征的高效性，统一了随机特征储层、连续时间深度架构和路径签名理论。

Abstract: We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias. Building on this foundation, we propose two variants: (i) Random Fourier CDEs (RF-CDEs): these lift the input signal using random Fourier features prior to the dynamics, providing a kernel-free approximation of RBF-enhanced sequence models; (ii) Random Rough DEs (R-RDEs): these operate directly on rough-path inputs via a log-ODE discretization, using log-signatures to capture higher-order temporal interactions while remaining stable and efficient. We prove that in the infinite-width limit, these model induces the RBF-lifted signature kernel and the rough signature kernel, respectively, offering a unified perspective on random-feature reservoirs, continuous-time deep architectures, and path-signature theory.
  We evaluate both models across a range of time-series benchmarks, demonstrating competitive or state-of-the-art performance. These methods provide a practical alternative to explicit signature computations, retaining their inductive bias while benefiting from the efficiency of random features.

</details>


### [73] [LLMTM: Benchmarking and Optimizing LLMs for Temporal Motif Analysis in Dynamic Graphs](https://arxiv.org/abs/2512.22266)
*Bing Hao,Minglai Shao,Zengyi Wo,Yunlong Chu,Yuhang Liu,Ruijie Wang*

Main category: cs.LG

TL;DR: 该论文系统研究了LLM在动态图时序motif分析中的性能，提出了LLMTM基准测试，开发了工具增强的LLM代理，并设计了结构感知调度器来平衡精度与成本。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，其在动态图处理能力方面的研究日益重要。时序motif作为动态图的基本单元和重要局部属性，能直接反映异常和独特现象，对理解动态图的演化动态和结构特征至关重要。然而，利用LLM进行动态图时序motif分析的研究相对较少。

Method: 1) 提出LLMTM基准测试，包含6个定制任务和9种时序motif类型；2) 通过大量实验分析不同提示技术和LLM模型的影响；3) 开发工具增强的LLM代理，使用精确设计的提示解决任务；4) 提出结构感知调度器，考虑动态图的结构属性和LLM认知负载，智能调度标准LLM提示和更强大的代理之间的查询。

Result: 实验表明，工具增强的LLM代理能以高精度解决任务，但成本较高。结构感知调度器能有效维持高精度同时显著降低成本，在精度与成本之间取得了良好平衡。

Conclusion: 该研究系统探索了LLM在时序motif分析中的性能，提出的LLMTM基准和结构感知调度器为动态图分析提供了有效的解决方案，在保持高精度的同时优化了计算成本。

Abstract: The widespread application of Large Language Models (LLMs) has motivated a growing interest in their capacity for processing dynamic graphs. Temporal motifs, as an elementary unit and important local property of dynamic graphs which can directly reflect anomalies and unique phenomena, are essential for understanding their evolutionary dynamics and structural features. However, leveraging LLMs for temporal motif analysis on dynamic graphs remains relatively unexplored. In this paper, we systematically study LLM performance on temporal motif-related tasks. Specifically, we propose a comprehensive benchmark, LLMTM (Large Language Models in Temporal Motifs), which includes six tailored tasks across nine temporal motif types. We then conduct extensive experiments to analyze the impacts of different prompting techniques and LLMs (including nine models: openPangu-7B, the DeepSeek-R1-Distill-Qwen series, Qwen2.5-32B-Instruct, GPT-4o-mini, DeepSeek-R1, and o3) on model performance. Informed by our benchmark findings, we develop a tool-augmented LLM agent that leverages precisely engineered prompts to solve these tasks with high accuracy. Nevertheless, the high accuracy of the agent incurs a substantial cost. To address this trade-off, we propose a simple yet effective structure-aware dispatcher that considers both the dynamic graph's structural properties and the LLM's cognitive load to intelligently dispatch queries between the standard LLM prompting and the more powerful agent. Our experiments demonstrate that the structure-aware dispatcher effectively maintains high accuracy while reducing cost.

</details>


### [74] [Hierarchical Stacking Optimization Using Dirichlet's Process (SoDip): Towards Accelerated Design for Graft Polymerization](https://arxiv.org/abs/2512.22279)
*Amgad Ahmed Ali Ibrahim,Hein Htet,Ryoji Asahi*

Main category: cs.LG

TL;DR: 开发了一个名为SoDip的层次化堆叠优化框架，用于解决辐射诱导接枝（RIG）中因基膜形态变化导致的重复性问题，通过整合文本和数值数据，实现了比传统方法更好的预测性能和不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 辐射诱导接枝技术虽然能精确功能化聚合物薄膜，但由于基膜形态（结晶度、晶粒取向、自由体积）的未报告变异性，导致单体扩散、自由基分布和Trommsdorff效应不一致，从而产生空间接枝梯度和性能不一致的问题，限制了技术的可重复性。

Method: 提出了一个层次化堆叠优化框架SoDip，包含四个核心组件：1）使用解码器Transformer（DeepSeek-R1）编码文本过程描述符；2）TabNet和XGBoost建模多模态特征交互；3）高斯过程回归与狄利克雷过程混合模型进行不确定性量化和异方差处理；4）贝叶斯优化高效探索高维合成空间。使用ChemDataExtractor 2.0和WebPlotDigitizer构建了包含数百个RIG研究的多样化数据集。

Result: 在交叉验证中，SoDip相比传统高斯过程回归实现了约33%的性能提升，同时提供了校准的置信区间，能够识别低重复性区域。该堆叠架构能够整合稀疏的文本和不同质量的数值输入，超越了先前模型。

Conclusion: SoDip框架为接枝聚合研究建立了可重复、形态感知设计的基础，通过整合多模态数据和先进的不确定性量化方法，显著提高了辐射诱导接枝技术的可重复性和预测性能。

Abstract: Radiation-induced grafting (RIG) enables precise functionalization of polymer films for ion-exchange membranes, CO2-separation membranes, and battery electrolytes by generating radicals on robust substrates to graft desired monomers. However, reproducibility remains limited due to unreported variability in base-film morphology (crystallinity, grain orientation, free volume), which governs monomer diffusion, radical distribution, and the Trommsdorff effect, leading to spatial graft gradients and performance inconsistencies. We present a hierarchical stacking optimization framework with a Dirichlet's Process (SoDip), a hierarchical data-driven framework integrating: (1) a decoder-only Transformer (DeepSeek-R1) to encode textual process descriptors (irradiation source, grafting type, substrate manufacturer); (2) TabNet and XGBoost for modelling multimodal feature interactions; (3) Gaussian Process Regression (GPR) with Dirichlet Process Mixture Models (DPMM) for uncertainty quantification and heteroscedasticity; and (4) Bayesian Optimization for efficient exploration of high-dimensional synthesis space. A diverse dataset was curated using ChemDataExtractor 2.0 and WebPlotDigitizer, incorporating numerical and textual variables across hundreds of RIG studies. In cross-validation, SoDip achieved ~33% improvement over GPR while providing calibrated confidence intervals that identify low-reproducibility regimes. Its stacked architecture integrates sparse textual and numerical inputs of varying quality, outperforming prior models and establishing a foundation for reproducible, morphology-aware design in graft polymerization research.

</details>


### [75] [Valori: A Deterministic Memory Substrate for AI Systems](https://arxiv.org/abs/2512.22280)
*Varshith Gudur*

Main category: cs.LG

TL;DR: Valori提出确定性AI内存基板，用定点算术替代浮点运算，确保跨平台比特一致性，解决AI系统中向量嵌入存储和检索的非确定性问题。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统依赖浮点算术进行向量嵌入存储和搜索，但相同模型、输入和代码在不同硬件架构（如x86 vs ARM）上会产生不同的内存状态和检索结果。这种非确定性破坏了可重现性和安全部署，导致数据静默分歧，影响受监管行业的审计追踪和后验证。

Method: Valori采用确定性AI内存基板设计：1）用定点算术（Q16.16）替代浮点内存操作；2）将内存建模为可重现状态机；3）在内存边界强制执行确定性。

Result: Valori保证跨平台的比特一致内存状态、快照和搜索结果。研究表明非确定性在索引或检索之前就已出现，Valori通过在内存边界强制执行确定性解决了这一问题。

Conclusion: 确定性内存是可信AI系统的必要基础组件。Valori的开源实现为AI系统提供了可重现、可验证的内存基板，特别适用于需要审计追踪的受监管行业。

Abstract: Modern AI systems rely on vector embeddings stored and searched using floating-point arithmetic. While effective for approximate similarity search, this design introduces fundamental non-determinism: identical models, inputs, and code can produce different memory states and retrieval results across hardware architectures (e.g., x86 vs. ARM). This prevents replayability and safe deployment, leading to silent data divergence that prevents post-hoc verification and compromises audit trails in regulated sectors. We present Valori, a deterministic AI memory substrate that replaces floating-point memory operations with fixed-point arithmetic (Q16.16) and models memory as a replayable state machine. Valori guarantees bit-identical memory states, snapshots, and search results across platforms. We demonstrate that non-determinism arises before indexing or retrieval and show how Valori enforces determinism at the memory boundary. Our results suggest that deterministic memory is a necessary primitive for trustworthy AI systems. The reference implementation is open-source and available at https://github.com/varshith-Git/Valori-Kernel (archived at https://zenodo.org/records/18022660).

</details>


### [76] [DBAW-PIKAN: Dynamic Balance Adaptive Weight Kolmogorov-Arnold Neural Network for Solving Partial Differential Equations](https://arxiv.org/abs/2512.22283)
*Guokan Chen,Yao Xiao*

Main category: cs.LG

TL;DR: 提出DBAW-PIKAN方法，结合Kolmogorov-Arnold网络架构与自适应权重策略，解决PINNs在多尺度/高频问题中的梯度流刚度和频谱偏差问题，显著提升收敛速度和精度。


<details>
  <summary>Details</summary>
Motivation: PINNs在科学计算中取得重要进展，但在处理多尺度或高频特征问题时，面临梯度流刚度和频谱偏差的严重挑战，限制了其预测能力。

Method: 提出动态平衡自适应加权物理信息Kolmogorov-Arnold网络（DBAW-PIKAN），结合基于可学习B样条的Kolmogorov-Arnold网络架构与包含动态衰减上界的自适应权重策略。

Result: 相比基线模型，该方法在不增加计算复杂度的情况下，将收敛过程加速至少一个数量级，并提高解精度至少一个数量级。在Klein-Gordon、Burgers和Helmholtz方程等数值基准测试中表现出显著优势。

Conclusion: DBAW-PIKAN能有效缓解PINNs的梯度相关失效模式，克服函数表示瓶颈，在精度和泛化性能方面都有显著提升。

Abstract: Physics-informed neural networks (PINNs) have led to significant advancements in scientific computing by integrating fundamental physical principles with advanced data-driven techniques. However, when dealing with problems characterized by multi-scale or high-frequency features, PINNs encounter persistent and severe challenges related to stiffness in gradient flow and spectral bias, which significantly limit their predictive capabilities. To address these issues, this paper proposes a Dynamic Balancing Adaptive Weighting Physics-Informed Kolmogorov-Arnold Network (DBAW-PIKAN), designed to mitigate such gradient-related failure modes and overcome the bottlenecks in function representation. The core of DBAW-PIKAN combines the Kolmogorov-Arnold network architecture, based on learnable B-splines, with an adaptive weighting strategy that incorporates a dynamic decay upper bound. Compared to baseline models, the proposed method accelerates the convergence process and improves solution accuracy by at least an order of magnitude without introducing additional computational complexity. A series of numerical benchmarks, including the Klein-Gordon, Burgers, and Helmholtz equations, demonstrate the significant advantages of DBAW-PIKAN in enhancing both accuracy and generalization performance.

</details>


### [77] [Cluster Aggregated GAN (CAG): A Cluster-Based Hybrid Model for Appliance Pattern Generation](https://arxiv.org/abs/2512.22287)
*Zikun Guoa,Adeyinka. P. Adedigbaa,Rammohan Mallipeddi*

Main category: cs.LG

TL;DR: 提出Cluster Aggregated GAN框架，通过聚类和分支架构分别处理间歇性和连续性电器，提高合成用电数据的真实性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有GAN方法将所有电器统一处理，忽略了间歇性和连续性电器的行为差异，导致训练不稳定和输出保真度有限。需要解决合成用电数据生成中的这些问题。

Method: 提出Cluster Aggregated GAN混合生成框架：1) 根据电器行为特征将电器路由到专门分支；2) 对间歇性电器使用聚类模块分组相似激活模式，为每个集群分配专用生成器；3) 对连续性电器使用LSTM生成器捕捉时间演化，通过序列压缩保持训练稳定性。

Result: 在UVIC智能插座数据集上的实验表明，该框架在真实性、多样性和训练稳定性指标上持续优于基线方法，聚类作为主动生成组件显著提高了可解释性和可扩展性。

Conclusion: 该框架为非侵入式负载监测研究中的合成负载生成提供了有效方法，通过专门处理不同电器类型的行为特征，解决了现有方法的局限性。

Abstract: Synthetic appliance data are essential for developing non-intrusive load monitoring algorithms and enabling privacy preserving energy research, yet the scarcity of labeled datasets remains a significant barrier. Recent GAN-based methods have demonstrated the feasibility of synthesizing load patterns, but most existing approaches treat all devices uniformly within a single model, neglecting the behavioral differences between intermittent and continuous appliances and resulting in unstable training and limited output fidelity. To address these limitations, we propose the Cluster Aggregated GAN framework, a hybrid generative approach that routes each appliance to a specialized branch based on its behavioral characteristics. For intermittent appliances, a clustering module groups similar activation patterns and allocates dedicated generators for each cluster, ensuring that both common and rare operational modes receive adequate modeling capacity. Continuous appliances follow a separate branch that employs an LSTM-based generator to capture gradual temporal evolution while maintaining training stability through sequence compression. Extensive experiments on the UVIC smart plug dataset demonstrate that the proposed framework consistently outperforms baseline methods across metrics measuring realism, diversity, and training stability, and that integrating clustering as an active generative component substantially improves both interpretability and scalability. These findings establish the proposed framework as an effective approach for synthetic load generation in non-intrusive load monitoring research.

</details>


### [78] [Co-GRPO: Co-Optimized Group Relative Policy Optimization for Masked Diffusion Model](https://arxiv.org/abs/2512.22288)
*Renping Zhou,Zanlin Ni,Tianyi Chen,Zeyu Liu,Yang Yue,Yulin Wang,Yuxuan Wang,Jingshu Liu,Gao Huang*

Main category: cs.LG

TL;DR: Co-GRPO提出一种统一MDP框架，联合优化掩码扩散模型的模型参数和推理调度参数，解决训练与推理之间的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 当前掩码扩散模型存在训练与推理过程的不匹配问题：训练使用单步BERT式目标，而推理是多步迭代过程，依赖未优化的调度策略，导致生成质量受限。

Method: 将MDM生成重构为统一的马尔可夫决策过程，应用轨迹级别的Group Relative Policy Optimization，在共享奖励下协同优化模型参数和调度参数，避免多步生成的反向传播开销。

Result: 在ImageReward、HPS、GenEval和DPG-Bench四个基准测试中验证了方法的有效性，显著提升了生成质量。

Conclusion: Co-GRPO通过联合优化模型和调度参数，实现了训练与推理的更彻底对齐，为掩码扩散模型提供了更有效的优化框架。

Abstract: Recently, Masked Diffusion Models (MDMs) have shown promising potential across vision, language, and cross-modal generation. However, a notable discrepancy exists between their training and inference procedures. In particular, MDM inference is a multi-step, iterative process governed not only by the model itself but also by various schedules that dictate the token-decoding trajectory (e.g., how many tokens to decode at each step). In contrast, MDMs are typically trained using a simplified, single-step BERT-style objective that masks a subset of tokens and predicts all of them simultaneously. This step-level simplification fundamentally disconnects the training paradigm from the trajectory-level nature of inference, leaving the inference schedules never optimized during training. In this paper, we introduce Co-GRPO, which reformulates MDM generation as a unified Markov Decision Process (MDP) that jointly incorporates both the model and the inference schedule. By applying Group Relative Policy Optimization at the trajectory level, Co-GRPO cooperatively optimizes model parameters and schedule parameters under a shared reward, without requiring costly backpropagation through the multi-step generation process. This holistic optimization aligns training with inference more thoroughly and substantially improves generation quality. Empirical results across four benchmarks-ImageReward, HPS, GenEval, and DPG-Bench-demonstrate the effectiveness of our approach. For more details, please refer to our project page: https://co-grpo.github.io/ .

</details>


### [79] [When Algorithms Manage Humans: A Double Machine Learning Approach to Estimating Nonlinear Effects of Algorithmic Control on Gig Worker Performance and Wellbeing](https://arxiv.org/abs/2512.22290)
*Arunkumar V,Nivethitha S,Sharan Srinivas,Gangadharan G. R*

Main category: cs.LG

TL;DR: 使用双重机器学习框架分析算法管理下的人力资源实践对零工工作者福祉与绩效的非线性影响，发现算法监督的透明度是关键调节因素


<details>
  <summary>Details</summary>
Motivation: 研究算法管理时代下，以人为本的管理方式是否还能存在。传统线性模型无法捕捉工作者对算法系统的复杂非线性反应，需要更灵活的方法来理解算法监督如何调节HR实践与工作结果之间的关系。

Method: 采用双重机器学习框架估计调节中介模型，不施加限制性函数形式。使用464名零工工作者的调查数据，分析支持性HR实践、算法监督透明度对工作者福祉和绩效的非单调影响。

Result: 发现明显的非单调模式：支持性HR实践改善工作者福祉，但其与绩效的关联在算法监督存在但难以解释的"模糊中间地带"减弱；当监督透明且可解释时，这种关联再次增强。简单的线性设定会错过这种模式甚至得出相反结论。

Conclusion: 平台设计应避免部分定义的模糊控制，而应提供清晰规则和可信的申诉机制。方法学上，双重机器学习可用于组织研究中的条件间接效应估计，无需强制数据符合线性形状。

Abstract: A central question for the future of work is whether person centered management can survive when algorithms take on managerial roles. Standard tools often miss what is happening because worker responses to algorithmic systems are rarely linear. We use a Double Machine Learning framework to estimate a moderated mediation model without imposing restrictive functional forms. Using survey data from 464 gig workers, we find a clear nonmonotonic pattern. Supportive HR practices improve worker wellbeing, but their link to performance weakens in a murky middle where algorithmic oversight is present yet hard to interpret. The relationship strengthens again when oversight is transparent and explainable. These results show why simple linear specifications can miss the pattern and sometimes suggest the opposite conclusion. For platform design, the message is practical: control that is only partly defined creates confusion, but clear rules and credible recourse can make strong oversight workable. Methodologically, the paper shows how Double Machine Learning can be used to estimate conditional indirect effects in organizational research without forcing the data into a linear shape.

</details>


### [80] [Multi-Head Spectral-Adaptive Graph Anomaly Detection](https://arxiv.org/abs/2512.22291)
*Qingyue Cao,Bo Jin,Changwei Gong,Xin Tong,Wenzheng Li,Xiaodong Zhou*

Main category: cs.LG

TL;DR: 提出MHSA-GNN，通过轻量级超网络根据图实例的"频谱指纹"动态生成Chebyshev滤波器参数，实现自适应滤波，并结合双正则化策略防止多头机制模式崩溃，在异质图异常检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法在处理复杂异常模式时面临挑战，异常节点常伪装成正常节点，导致图中同质性和异质性共存。现有频谱GNN使用固定全局滤波器，容易导致过平滑，丢失异常检测所需的高频信号，且缺乏对不同图实例的自适应能力。

Method: 提出多头频谱自适应图神经网络(MHSA-GNN)：1) 设计轻量级超网络，基于包含结构统计和Rayleigh商特征的"频谱指纹"，为每个实例动态生成Chebyshev滤波器参数；2) 引入双正则化策略：教师-学生对比学习(TSC)保证表示准确性，Barlow Twins多样性损失(BTD)强制多头正交性，防止模式崩溃。

Result: 在四个真实数据集上的实验表明，该方法能有效保留高频异常信号，显著优于现有SOTA方法，在高度异质数据集上表现出优秀的鲁棒性。

Conclusion: MHSA-GNN通过实例自适应的频谱滤波和双正则化多头机制，解决了图异常检测中固定滤波器导致的过平滑问题，在复杂异常模式检测中表现出色，为金融欺诈等应用提供了有效解决方案。

Abstract: Graph anomaly detection technology has broad applications in financial fraud and risk control. However, existing graph anomaly detection methods often face significant challenges when dealing with complex and variable abnormal patterns, as anomalous nodes are often disguised and mixed with normal nodes, leading to the coexistence of homophily and heterophily in the graph domain. Recent spectral graph neural networks have made notable progress in addressing this issue; however, current techniques typically employ fixed, globally shared filters. This 'one-size-fits-all' approach can easily cause over-smoothing, erasing critical high-frequency signals needed for fraud detection, and lacks adaptive capabilities for different graph instances. To solve this problem, we propose a Multi-Head Spectral-Adaptive Graph Neural Network (MHSA-GNN). The core innovation is the design of a lightweight hypernetwork that, conditioned on a 'spectral fingerprint' containing structural statistics and Rayleigh quotient features, dynamically generates Chebyshev filter parameters tailored to each instance. This enables a customized filtering strategy for each node and its local subgraph. Additionally, to prevent mode collapse in the multi-head mechanism, we introduce a novel dual regularization strategy that combines teacher-student contrastive learning (TSC) to ensure representation accuracy and Barlow Twins diversity loss (BTD) to enforce orthogonality among heads. Extensive experiments on four real-world datasets demonstrate that our method effectively preserves high-frequency abnormal signals and significantly outperforms existing state-of-the-art methods, especially showing excellent robustness on highly heterogeneous datasets.

</details>


### [81] [Learning from Negative Examples: Why Warning-Framed Training Data Teaches What It Warns Against](https://arxiv.org/abs/2512.22293)
*Tsogt-Ochir Enkhbayar*

Main category: cs.LG

TL;DR: 警告性内容在训练数据中并不能有效教导语言模型避免被警告的行为，模型在警告和直接内容两种情况下产生被标记内容的概率相近（76.7% vs 83.3%），原因是"描述X"和"执行X"激活了重叠的潜在特征。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解为什么包含警告框架的训练内容（如"不要使用-此代码有漏洞"）无法有效教导语言模型避免被警告的行为，探索当前架构中统计共现与语用解释之间的关系。

Method: 通过实验比较模型在警告框架内容和直接内容下的行为表现，使用稀疏自编码器分析潜在特征激活模式，特别关注特征#8684在警告和利用上下文中的激活情况，并研究"隐形滑移"现象。

Result: 模型在警告框架内容下产生被标记内容的概率为76.7%，与直接内容下的83.3%无统计学显著差异；稀疏自编码器分析显示"描述X"和"执行X"激活了重叠的潜在特征；训练时特征消融能解决问题，而提示和推理时引导无效。

Conclusion: 当前架构中统计共现主导了语用解释，模型学习的是上下文中倾向于出现什么，而不是为什么出现在那里；警告框架无法有效教导模型避免特定行为，需要训练时的特征消融来真正解决问题。

Abstract: Warning-framed content in training data (e.g., "DO NOT USE - this code is vulnerable") does not, it turns out, teach language models to avoid the warned-against behavior. In experiments reported here, models exposed to such warnings reproduced the flagged content at rates statistically indistinguishable from models given the content directly (76.7% vs. 83.3%). Why? Sparse autoencoder analysis points to a failure of orthogonalization: "describing X" and "performing X" activate overlapping latent features. Feature #8684, which tracks code execution patterns, fires at comparable magnitude in both warning and exploitation contexts. A related phenomenon, what I call "stealth slip", allows conversational preambles to rotate activations into subspaces that linear probes miss entirely. Prompting and inference-time steering do not fix this; training-time feature ablation does. The upshot is that statistical co-occurrence dominates over pragmatic interpretation in current architectures. Models learn what tends to follow a context, not why it appeared there.

</details>


### [82] [Hybrid Quantum-Classical Mixture of Experts: Unlocking Topological Advantage via Interference-Based Routing](https://arxiv.org/abs/2512.22296)
*Reda Heddad,Lamiae Bouanane*

Main category: cs.LG

TL;DR: 量子门控网络（路由器）与经典专家结合的混合量子-经典混合专家架构，通过量子干涉实现高维核方法，在非线性可分数据上展现拓扑优势


<details>
  <summary>Details</summary>
Motivation: 传统混合专家架构存在专家不平衡和经典路由机制计算复杂度高等限制，需要探索量子机器学习来解决这些问题

Method: 提出混合量子-经典混合专家架构，使用量子门控网络作为路由器，结合经典专家，通过量子特征映射和波干涉实现高维核方法

Result: 量子路由器在非线性可分数据（如Two Moons数据集）上展现显著拓扑优势，能有效"解开"线性经典路由器无法高效分离的数据分布，且对量子噪声具有鲁棒性

Conclusion: 量子增强路由范式在联邦学习、隐私保护机器学习和自适应系统等应用中具有潜力，验证了量子干涉假说并展示了量子路由器在参数效率方面的优势

Abstract: The Mixture-of-Experts (MoE) architecture has emerged as a powerful paradigm for scaling deep learning models, yet it is fundamentally limited by challenges such as expert imbalance and the computational complexity of classical routing mechanisms. This paper investigates the potential of Quantum Machine Learning (QML) to address these limitations through a novel Hybrid Quantum-Classical Mixture of Experts (QMoE) architecture. Specifically, we conduct an ablation study using a Quantum Gating Network (Router) combined with classical experts to isolate the source of quantum advantage. Our central finding validates the Interference Hypothesis: by leveraging quantum feature maps (Angle Embedding) and wave interference, the Quantum Router acts as a high-dimensional kernel method, enabling the modeling of complex, non-linear decision boundaries with superior parameter efficiency compared to its classical counterparts. Experimental results on non-linearly separable data, such as the Two Moons dataset, demonstrate that the Quantum Router achieves a significant topological advantage, effectively "untangling" data distributions that linear classical routers fail to separate efficiently. Furthermore, we analyze the architecture's robustness against simulated quantum noise, confirming its feasibility for near-term intermediate-scale quantum (NISQ) hardware. We discuss practical applications in federated learning, privacy-preserving machine learning, and adaptive systems that could benefit from this quantum-enhanced routing paradigm.

</details>


### [83] [Statistical and Machine Learning Analysis of Traffic Accidents on US 158 in Currituck County: A Comparison with HSM Predictions](https://arxiv.org/abs/2512.22302)
*Jennifer Sawyer,Julian Allagan*

Main category: cs.LG

TL;DR: 该研究整合了先进的统计、机器学习和空间建模技术，分析美国158号公路5年交通事故数据，识别时空事故模式，预测伤害严重程度，验证空间聚类，为改善交通安全提供可操作见解。


<details>
  <summary>Details</summary>
Motivation: 扩展先前热点和卡方分析，通过整合先进统计、机器学习和空间建模技术，为改善美国158号公路交通安全提供可操作见解，同时为乡村公路安全分析方法论做出贡献。

Method: 整合核密度估计、负二项回归、随机森林分类和公路安全手册安全性能函数比较，分析2019-2023年交通事故数据，使用莫兰指数检验空间聚类。

Result: 随机森林分类器预测伤害严重程度准确率达67%，优于HSM SPF；莫兰指数检验确认空间聚类；核密度分析识别主要交叉口附近的热点区域。

Conclusion: 研究结果支持针对性的干预措施以改善交通安全，为乡村公路安全分析提供了超越基础统计技术的方法论进步。

Abstract: This study extends previous hotspot and Chi-Square analysis by Sawyer \cite{sawyer2025hotspot} by integrating advanced statistical analysis, machine learning, and spatial modeling techniques to analyze five years (2019--2023) of traffic accident data from an 8.4-mile stretch of US 158 in Currituck County, NC. Building upon foundational statistical work, we apply Kernel Density Estimation (KDE), Negative Binomial Regression, Random Forest classification, and Highway Safety Manual (HSM) Safety Performance Function (SPF) comparisons to identify comprehensive temporal and spatial crash patterns. A Random Forest classifier predicts injury severity with 67\% accuracy, outperforming HSM SPF. Spatial clustering is confirmed via Moran's I test ($I = 0.32$, $p < 0.001$), and KDE analysis reveals hotspots near major intersections, validating and extending earlier hotspot identification methods. These results support targeted interventions to improve traffic safety on this vital transportation corridor. Our objective is to provide actionable insights for improving safety on US 158 while contributing to the broader understanding of rural highway safety analysis through methodological advancement beyond basic statistical techniques.

</details>


### [84] [PDx -- Adaptive Credit Risk Forecasting Model in Digital Lending using Machine Learning Operations](https://arxiv.org/abs/2512.22305)
*Sultan Amed,Chan Yu Hang,Sayantan Banerjee*

Main category: cs.LG

TL;DR: PDx是一个基于MLOps的自适应决策系统，用于数字借贷中的信用风险预测，通过动态冠军-挑战者框架和持续模型监控来解决传统PD模型静态化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统PD模型在开发时注重预测准确性，但缺乏对借款人行为变化的持续适应能力，导致模型在生产中性能随时间下降。金融机构也难以将ML模型从开发环境迁移到生产环境并维持其健康状态。

Method: 采用动态、端到端的模型生命周期管理方法，集成持续模型监控、重新训练和验证的稳健MLOps流水线。引入动态冠军-挑战者框架，定期用最新数据重新校准基线模型，并通过超时验证选择最佳性能模型。

Result: 基于决策树的集成模型在违约分类中表现最佳，但需要频繁更新以维持性能。线性模型和神经网络表现出更大的性能退化。PDx能减轻数字贷款人的价值侵蚀，特别是在短期小额贷款中。

Conclusion: PDx通过MLOps驱动的自适应系统有效解决了传统PD模型的局限性，在点对点借贷、商业贷款和汽车贷款数据集上验证了其可扩展性和适应性，为现代信用风险预测提供了实用解决方案。

Abstract: This paper presents PDx, an adaptive, machine learning operations (MLOps) driven decision system for forecasting credit risk using probability of default (PD) modeling in digital lending. While conventional PD models prioritize predictive accuracy during model development with complex machine learning algorithms, they often overlook continuous adaptation to changing borrower behaviour, resulting in static models that degrade over time in production and generate inaccurate default predictions. Many financial institutes also find it difficult transitioning ML models from development environment to production and maintaining their health. With PDx we aimed to addresses these limitations using a dynamic, end-to-end model lifecycle management approach that integrates continuous model monitoring, retraining, and validation through a robust MLOps pipeline. We introduced a dynamic champion-challenger framework for PDx to regularly update baseline models to recalibrate independent parameters with the latest data and select the best-performing model through out-of-time validation, ensuring resilience against data drift and changing credit risk patterns. Our empirical analysis shows that decision tree-based ensemble models consistently outperform others in classifying defaulters but require frequent updates to sustain performance. Linear models (e.g., logistic regression) and neural networks exhibit greater performance degradation. The study demonstrate with PDx we can mitigates value erosion for digital lenders, particularly in short-term, small-ticket loans, where borrower behavior shifts rapidly. We have validated the effectiveness of PDx using datasets from peer-to-peer lending, business loans, and auto loans, demonstrating its scalability and adaptability for modern credit risk forecasting.

</details>


### [85] [LLMBoost: Make Large Language Models Stronger with Boosting](https://arxiv.org/abs/2512.22309)
*Zehao Chen,Tianxiang Ai,Yifei Li,Gongxun Li,Yuyang Wei,Wang Zhou,Guanghui Li,Bin Yu,Zhijun Chen,Hailong Sun,Fuzhen Zhuang,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.LG

TL;DR: LLMBoost：一种新颖的集成微调框架，通过利用LLM的中间状态和跨模型注意力机制，实现层次化错误校正和高效推理


<details>
  <summary>Details</summary>
Motivation: 现有LLM集成方法通常将模型视为黑盒，仅组合输入或最终输出，忽略了丰富的内部表示和跨模型交互。需要打破这一限制，充分利用中间状态来提升性能。

Method: 1. 跨模型注意力机制：使后续模型能够访问和融合前驱模型的隐藏状态；2. 链式训练范式：通过误差抑制目标逐步微调连接模型；3. 近并行推理范式：逐层流水线化隐藏状态，实现接近单模型解码的推理效率。

Result: 在常识推理和算术推理任务上的广泛实验表明，LLMBoost能够持续提升准确率，同时降低推理延迟。

Conclusion: LLMBoost通过利用LLM中间状态和跨模型交互，打破了传统集成方法的黑盒限制，在理论上保证单调改进，在实践中实现性能提升和效率优化。

Abstract: Ensemble learning of LLMs has emerged as a promising alternative to enhance performance, but existing approaches typically treat models as black boxes, combining the inputs or final outputs while overlooking the rich internal representations and interactions across models.In this work, we introduce LLMBoost, a novel ensemble fine-tuning framework that breaks this barrier by explicitly leveraging intermediate states of LLMs. Inspired by the boosting paradigm, LLMBoost incorporates three key innovations. First, a cross-model attention mechanism enables successor models to access and fuse hidden states from predecessors, facilitating hierarchical error correction and knowledge transfer. Second, a chain training paradigm progressively fine-tunes connected models with an error-suppression objective, ensuring that each model rectifies the mispredictions of its predecessor with minimal additional computation. Third, a near-parallel inference paradigm design pipelines hidden states across models layer by layer, achieving inference efficiency approaching single-model decoding. We further establish the theoretical foundations of LLMBoost, proving that sequential integration guarantees monotonic improvements under bounded correction assumptions. Extensive experiments on commonsense reasoning and arithmetic reasoning tasks demonstrate that LLMBoost consistently boosts accuracy while reducing inference latency.

</details>


### [86] [Optimistic Feasible Search for Closed-Loop Fair Threshold Decision-Making](https://arxiv.org/abs/2512.22313)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 提出Optimistic Feasible Search (OFS)方法，用于在公平性约束（人口均等）和服务率约束下，从强盗反馈中在线学习一维阈值策略，解决闭环决策系统中的反馈效应问题。


<details>
  <summary>Details</summary>
Motivation: 闭环决策系统（如贷款、筛选、累犯风险评估）在公平性和服务约束下运行时，会产生反馈效应：决策改变未来出现的人群，导致数据非平稳并可能放大不平等。需要在线学习满足约束的阈值策略。

Method: 提出Optimistic Feasible Search (OFS)：基于网格的方法，为每个候选阈值维护奖励和约束残差的置信边界。每轮选择在置信边界下看似可行的阈值，并在其中最大化乐观奖励；若无可行阈值，则选择最小化乐观约束违反的阈值。

Result: 在三个基准测试中评估：(1) 具有稳定收缩动态的合成闭环基准；(2) 基于德国信贷和COMPAS的半合成闭环基准。OFS在所有环境中比无约束和原始对偶强盗基线获得更高奖励和更小累积约束违反，接近相同扫描程序下最佳可行固定阈值的预言机性能。

Conclusion: OFS方法能有效处理闭环决策系统中的公平性和服务约束问题，特别适用于低维、可解释的策略类别，在保持约束满足的同时实现高奖励性能。

Abstract: Closed-loop decision-making systems (e.g., lending, screening, or recidivism risk assessment) often operate under fairness and service constraints while inducing feedback effects: decisions change who appears in the future, yielding non-stationary data and potentially amplifying disparities. We study online learning of a one-dimensional threshold policy from bandit feedback under demographic parity (DP) and, optionally, service-rate constraints. The learner observes only a scalar score each round and selects a threshold; reward and constraint residuals are revealed only for the chosen threshold.
  We propose Optimistic Feasible Search (OFS), a simple grid-based method that maintains confidence bounds for reward and constraint residuals for each candidate threshold. At each round, OFS selects a threshold that appears feasible under confidence bounds and, among those, maximizes optimistic reward; if no threshold appears feasible, OFS selects the threshold minimizing optimistic constraint violation. This design directly targets feasible high-utility thresholds and is particularly effective for low-dimensional, interpretable policy classes where discretization is natural.
  We evaluate OFS on (i) a synthetic closed-loop benchmark with stable contraction dynamics and (ii) two semi-synthetic closed-loop benchmarks grounded in German Credit and COMPAS, constructed by training a score model and feeding group-dependent acceptance decisions back into population composition. Across all environments, OFS achieves higher reward with smaller cumulative constraint violation than unconstrained and primal-dual bandit baselines, and is near-oracle relative to the best feasible fixed threshold under the same sweep procedure. Experiments are reproducible and organized with double-blind-friendly relative outputs.

</details>


### [87] [LangPrecip: Language-Aware Multimodal Precipitation Nowcasting](https://arxiv.org/abs/2512.22317)
*Xudong Ling,Tianxi Huang,Qian Dong,Tao He,Chaorong Li,Guiduo Duan*

Main category: cs.LG

TL;DR: LangPrecip：一种语言感知的多模态降水临近预报框架，通过将气象文本作为降水演化的语义运动约束，显著提升短期降水预报精度


<details>
  <summary>Details</summary>
Motivation: 短期降水临近预报存在固有的不确定性和约束不足问题，特别是对于快速演变的极端天气事件。现有生成方法主要依赖视觉条件，导致未来运动约束弱且模糊，需要更强的语义约束来改进预报准确性

Method: 提出语言感知多模态临近预报框架(LangPrecip)，将气象文本作为降水演化的语义运动约束。在Rectified Flow范式下将临近预报建模为语义约束的轨迹生成问题，在潜在空间中高效集成文本和雷达信息。同时构建了包含16万对雷达序列和运动描述的大规模多模态数据集LangPrecip-160k

Result: 在瑞典和MRMS数据集上的实验显示，该方法在80分钟预报时效上，对强降水的CSI指标分别获得超过60%和19%的提升，一致优于现有最先进方法

Conclusion: 通过引入语言作为语义运动约束，LangPrecip框架能够有效提升短期降水临近预报的准确性，特别是在极端天气事件中，证明了多模态信息融合在气象预报中的重要性

Abstract: Short-term precipitation nowcasting is an inherently uncertain and under-constrained spatiotemporal forecasting problem, especially for rapidly evolving and extreme weather events. Existing generative approaches rely primarily on visual conditioning, leaving future motion weakly constrained and ambiguous. We propose a language-aware multimodal nowcasting framework(LangPrecip) that treats meteorological text as a semantic motion constraint on precipitation evolution. By formulating nowcasting as a semantically constrained trajectory generation problem under the Rectified Flow paradigm, our method enables efficient and physically consistent integration of textual and radar information in latent space.We further introduce LangPrecip-160k, a large-scale multimodal dataset with 160k paired radar sequences and motion descriptions. Experiments on Swedish and MRMS datasets show consistent improvements over state-of-the-art methods, achieving over 60 \% and 19\% gains in heavy-rainfall CSI at an 80-minute lead time.

</details>


### [88] [Decomposing Uncertainty in Probabilistic Knowledge Graph Embeddings: Why Entity Variance Is Not Enough](https://arxiv.org/abs/2512.22318)
*Chorok Lee*

Main category: cs.LG

TL;DR: 现有概率知识图谱嵌入方法存在关系无关的局限性，无法区分新兴实体和新关系上下文两种不同的分布外现象。本文提出将不确定性分解为语义不确定性和结构不确定性，并通过CAGP方法结合两者，显著提升了时间分布外检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有概率知识图谱嵌入方法使用实体分布的方差来量化认知不确定性，但这些方差是关系无关的，即无论关系上下文如何，实体都具有相同的不确定性。这混淆了两种行为相反的分布外现象：新兴实体（罕见、学习不足）和新关系上下文（熟悉实体在未观察关系中的出现）。

Method: 提出不确定性分解为两个互补成分：1) 语义不确定性：来自实体嵌入方差（检测新兴实体）；2) 结构不确定性：来自实体-关系共现（检测新关系上下文）。理论证明这两个信号是非冗余的，任何凸组合都严格优于单独使用任一信号。提出CAGP方法，通过学习的权重结合语义和结构不确定性。

Result: 在三个数据集（FB15k-237、WN18RR、YAGO3-10）上实证验证了100%的新关系上下文三元组都有频率匹配的分布内对应物。CAGP方法在时间分布外检测上达到0.94-0.99 AUROC，相比关系无关基线相对提升60-80%。在选择性预测中，在85%回答率下减少43%的错误。

Conclusion: 关系无关的不确定性估计无法有效检测新关系上下文，因为实体-关系共现模式是关键信号。通过将不确定性分解为语义和结构成分并适当结合，可以显著提升分布外检测性能，特别是在时间分布偏移场景下。

Abstract: Probabilistic knowledge graph embeddings represent entities as distributions, using learned variances to quantify epistemic uncertainty. We identify a fundamental limitation: these variances are relation-agnostic, meaning an entity receives identical uncertainty regardless of relational context. This conflates two distinct out-of-distribution phenomena that behave oppositely: emerging entities (rare, poorly-learned) and novel relational contexts (familiar entities in unobserved relationships). We prove an impossibility result: any uncertainty estimator using only entity-level statistics independent of relation context achieves near-random OOD detection on novel contexts. We empirically validate this on three datasets, finding 100 percent of novel-context triples have frequency-matched in-distribution counterparts. This explains why existing probabilistic methods achieve 0.99 AUROC on random corruptions but only 0.52-0.64 on temporal distribution shift. We formalize uncertainty decomposition into complementary components: semantic uncertainty from entity embedding variance (detecting emerging entities) and structural uncertainty from entity-relation co-occurrence (detecting novel contexts). Our main theoretical result proves these signals are non-redundant, and that any convex combination strictly dominates either signal alone. Our method (CAGP) combines semantic and structural uncertainty via learned weights, achieving 0.94-0.99 AUROC on temporal OOD detection across multiple benchmarks, a 60-80 percent relative improvement over relation-agnostic baselines. Empirical validation confirms complete frequency overlap on three datasets (FB15k-237, WN18RR, YAGO3-10). On selective prediction, our method reduces errors by 43 percent at 85 percent answer rate.

</details>


### [89] [Expert System for Bitcoin Forecasting: Integrating Global Liquidity via TimeXer Transformers](https://arxiv.org/abs/2512.22326)
*Sravan Karthick T*

Main category: cs.LG

TL;DR: 该论文提出了一种结合全球M2流动性的比特币价格预测模型，通过TimeXer-Exog架构，在70天预测范围内比单变量基准模型误差降低89%。


<details>
  <summary>Details</summary>
Motivation: 比特币价格具有极端波动性和非平稳性，传统单变量时间序列模型在长期预测中表现不佳。论文旨在填补这一空白，通过引入全球宏观经济变量来改善长期预测稳定性。

Method: 整合来自18个主要经济体的全球M2流动性作为领先外生变量，采用12周滞后结构。使用TimeXer架构，构建流动性条件预测模型（TimeXer-Exog），并与LSTM、N-BEATS、PatchTST和标准单变量TimeXer等先进基准进行比较。

Result: 在2020年1月至2025年8月的每日比特币价格数据实验中，宏观经济学条件显著稳定了长期预测。在70天预测范围内，TimeXer-Exog模型的均方误差为1.08e8，比单变量TimeXer基线提高了89%以上。

Conclusion: 在深度学习模型中引入全球流动性条件可以显著改善比特币价格的长期预测性能，证明了宏观经济因素在加密货币价格预测中的重要性。

Abstract: Bitcoin price forecasting is characterized by extreme volatility and non-stationarity, often defying traditional univariate time-series models over long horizons. This paper addresses a critical gap by integrating Global M2 Liquidity, aggregated from 18 major economies, as a leading exogenous variable with a 12-week lag structure. Using the TimeXer architecture, we compare a liquidity-conditioned forecasting model (TimeXer-Exog) against state-of-the-art benchmarks including LSTM, N-BEATS, PatchTST, and a standard univariate TimeXer. Experiments conducted on daily Bitcoin price data from January 2020 to August 2025 demonstrate that explicit macroeconomic conditioning significantly stabilizes long-horizon forecasts. At a 70-day forecast horizon, the proposed TimeXer-Exog model achieves a mean squared error (MSE) 1.08e8, outperforming the univariate TimeXer baseline by over 89 percent. These results highlight that conditioning deep learning models on global liquidity provides substantial improvements in long-horizon Bitcoin price forecasting.

</details>


### [90] [The Effectiveness of Approximate Regularized Replay for Efficient Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2512.22337)
*Matthew Riemer,Erik Miehling,Miao Liu,Djallel Bouneffouf,Murray Campbell*

Main category: cs.LG

TL;DR: LoRA微调可能导致模型能力灾难性退化，但通过正则化近似重放方法（惩罚与初始模型的KL散度，并混合预训练数据）可以几乎消除此问题，在保持通用知识的同时不阻碍新任务学习能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA等参数高效微调方法只修改少量参数，但它们可能对模型产生显著负面影响。研究发现LoRA监督微调即使在小型数据集上训练也会灾难性地降低模型能力，需要找到解决方案来避免这种退化。

Method: 提出正则化近似重放方法：1）惩罚与初始模型的KL散度作为正则化；2）混合来自类似预训练语料库的下一个词预测数据；3）在Qwen指令调优模型中应用此方法，以保持通用知识同时不阻碍新任务学习。

Result: 该方法能够几乎完全消除LoRA微调导致的灾难性退化问题，在保持模型通用知识的同时，允许模型学习新任务，仅增加适度的计算开销。

Conclusion: 虽然简单的LoRA微调方法在实践中可能导致模型能力严重退化，但通过正则化近似重放等小调整可以几乎消除此问题，在保持模型原有知识的同时实现有效的新任务学习。

Abstract: Although parameter-efficient fine-tuning methods, such as LoRA, only modify a small subset of parameters, they can have a significant impact on the model. Our instruction-tuning experiments show that LoRA-based supervised fine-tuning can catastrophically degrade model capabilities, even when trained on very small datasets for relatively few steps. With that said, we demonstrate that while the most straightforward approach (that is likely the most used in practice) fails spectacularly, small tweaks to the training procedure with very little overhead can virtually eliminate the problem. Particularly, in this paper we consider a regularized approximate replay approach which penalizes KL divergence with respect to the initial model and interleaves in data for next token prediction from a different, yet similar, open access corpus to what was used in pre-training. When applied to Qwen instruction-tuned models, we find that this recipe preserves general knowledge in the model without hindering plasticity to new tasks by adding a modest amount of computational overhead.

</details>


### [91] [AFA-LoRA: Enabling Non-Linear Adaptations in LoRA with Activation Function Annealing](https://arxiv.org/abs/2512.22455)
*Jiacheng Li,Jianchao Tan,Zhidong Yang,Feiye Huo,Yerui Sun,Yuchen Xie,Xunliang Cai*

Main category: cs.LG

TL;DR: AFA-LoRA通过引入退火激活函数，在保持LoRA可合并性的同时，为线性适配器增加非线性表达能力，缩小了LoRA与全参数微调之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: LoRA作为广泛使用的参数高效微调方法，其线性适配过程限制了表达能力，导致线性训练与非线性训练之间存在表达能力的差距。

Method: 提出AFA-LoRA训练策略，核心创新是退火激活函数，在训练过程中从非线性变换逐渐过渡到线性变换，使适配器先获得更强的表示能力，最终收敛到可合并的线性形式。

Result: 在监督微调、强化学习和推测解码等任务上实施该方法，结果表明AFA-LoRA显著缩小了LoRA与全参数训练之间的性能差距。

Conclusion: 这项工作为参数高效适配提供了更强大且实用的范式，在保持LoRA可合并性的同时增强了其表达能力。

Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method. However, its linear adaptation process limits its expressive power. This means there is a gap between the expressive power of linear training and non-linear training. To bridge this gap, we propose AFA-LoRA, a novel training strategy that brings non-linear expressivity to LoRA while maintaining its seamless mergeability. Our key innovation is an annealed activation function that transitions from a non-linear to a linear transformation during training, allowing the adapter to initially adopt stronger representational capabilities before converging to a mergeable linear form. We implement our method on supervised fine-tuning, reinforcement learning, and speculative decoding. The results show that AFA-LoRA reduces the performance gap between LoRA and full-parameter training. This work enables a more powerful and practical paradigm of parameter-efficient adaptation.

</details>


### [92] [AMBIT: Augmenting Mobility Baselines with Interpretable Trees](https://arxiv.org/abs/2512.22466)
*Qizhi Wang*

Main category: cs.LG

TL;DR: AMBIT框架结合物理移动模型与可解释树模型，在保持高精度的同时提升OD流量预测的可解释性


<details>
  <summary>Details</summary>
Motivation: 解决OD流量预测中高精度与可解释性之间的冲突，为城市决策提供既准确又透明的预测工具

Method: 首先审计经典空间交互模型，然后基于物理基线构建残差学习器，使用梯度提升树和SHAP分析增强预测能力

Result: 物理基础残差接近强树基预测器精度且保持可解释结构，POI锚定残差在空间泛化中最稳健

Conclusion: AMBIT框架成功平衡精度与可解释性，为城市决策提供可复现的预测管道和丰富的诊断工具

Abstract: Origin-destination (OD) flow prediction remains a core task in GIS and urban analytics, yet practical deployments face two conflicting needs: high accuracy and clear interpretability. This paper develops AMBIT, a gray-box framework that augments physical mobility baselines with interpretable tree models. We begin with a comprehensive audit of classical spatial interaction models on a year-long, hourly NYC taxi OD dataset. The audit shows that most physical models are fragile at this temporal resolution; PPML gravity is the strongest physical baseline, while constrained variants improve when calibrated on full OD margins but remain notably weaker. We then build residual learners on top of physical baselines using gradient-boosted trees and SHAP analysis, demonstrating that (i) physics-grounded residuals approach the accuracy of a strong tree-based predictor while retaining interpretable structure, and (ii) POI-anchored residuals are consistently competitive and most robust under spatial generalization. We provide a reproducible pipeline, rich diagnostics, and spatial error analysis designed for urban decision-making.

</details>


### [93] [GLUE: Gradient-free Learning to Unify Experts](https://arxiv.org/abs/2512.22467)
*Jong-Ik Park,Shreyas Chaudhari,Srinivasa Pranav,Carlee Joe-Wong,José M. F. Moura*

Main category: cs.LG

TL;DR: GLUE提出了一种无需梯度的专家模型融合方法，通过梯度自由的两点更新学习混合系数，只需两次前向传播即可有效初始化目标模型，在多个数据集和架构上优于传统启发式方法。


<details>
  <summary>Details</summary>
Motivation: 在部署系统中，多个预训练专家模型并存，但新目标域需要超越单个专家性能的泛化模型。现有方法通过启发式混合系数（基于数据大小或代理指标）初始化目标模型，但这种方法通常导致目标域测试准确率较低，而基于目标损失的梯度学习需要计算昂贵的完整反向传播。

Method: GLUE（Gradient-free Learning To Unify Experts）将目标模型初始化为固定专家的凸组合，通过梯度自由的两点（SPSA）更新学习混合系数，每个步骤仅需两次前向传播，无需完整反向传播。

Result: 在三个数据集和三种网络架构上的实验表明，GLUE产生的单一先验经过微调后能超越基线方法。GLUE比基于数据大小的加权方法提升测试准确率高达8.5%，比基于代理指标的选择方法提升高达9.1%，与基于反向传播的全梯度混合方法相比，要么表现更好，要么性能差距在1.4%以内。

Conclusion: GLUE提供了一种高效、无需梯度的专家模型融合方法，能够有效初始化目标域模型，在保持计算效率的同时实现优于传统启发式方法的性能，为领域扩展问题提供了实用解决方案。

Abstract: In many deployed systems (multilingual ASR, cross-hospital imaging, region-specific perception), multiple pretrained specialist models coexist. Yet, new target domains often require domain expansion: a generalized model that performs well beyond any single specialist's domain. Given such a new target domain, prior works seek a single strong initialization prior for the model parameters by first blending expert models to initialize a target model. However, heuristic blending -- using coefficients based on data size or proxy metrics -- often yields lower target-domain test accuracy, and learning the coefficients on the target loss typically requires computationally-expensive full backpropagation through the network. We propose GLUE, Gradient-free Learning To Unify Experts, which initializes the target model as a convex combination of fixed experts, learning the mixture coefficients of this combination via a gradient-free two-point (SPSA) update that requires only two forward passes per step. Across experiments on three datasets and three network architectures, GLUE produces a single prior that can be fine-tuned effectively to outperform baselines. GLUE improves test accuracy by up to 8.5% over data-size weighting and by up to 9.1% over proxy-metric selection. GLUE either outperforms backpropagation-based full-gradient mixing or matches its performance within 1.4%.

</details>


### [94] [Collaborative Optimization of Multiclass Imbalanced Learning: Density-Aware and Region-Guided Boosting](https://arxiv.org/abs/2512.22478)
*Chuantao Li,Zhi Li,Jiahao Xu,Jie Li,Sheng Li*

Main category: cs.LG

TL;DR: 提出一种协同优化的Boosting模型，通过集成密度因子和置信因子，设计抗噪声权重更新机制和动态采样策略，实现不平衡学习与模型训练的协同优化。


<details>
  <summary>Details</summary>
Motivation: 现有研究尚未探索不平衡学习与模型训练的协同优化，这一限制阻碍了性能的进一步提升。需要填补这一空白。

Method: 提出协同优化的多类不平衡学习Boosting模型，集成密度因子和置信因子，设计抗噪声权重更新机制和动态采样策略。这些模块紧密集成，协调权重更新、样本区域划分和区域引导采样。

Result: 在20个公共不平衡数据集上的广泛实验表明，该模型显著优于8个最先进的基线方法。

Conclusion: 该研究实现了不平衡学习与模型训练的协同优化，提出的模型简单有效，性能显著优于现有方法。

Abstract: Numerous studies attempt to mitigate classification bias caused by class imbalance. However, existing studies have yet to explore the collaborative optimization of imbalanced learning and model training. This constraint hinders further performance improvements. To bridge this gap, this study proposes a collaborative optimization Boosting model of multiclass imbalanced learning. This model is simple but effective by integrating the density factor and the confidence factor, this study designs a noise-resistant weight update mechanism and a dynamic sampling strategy. Rather than functioning as independent components, these modules are tightly integrated to orchestrate weight updates, sample region partitioning, and region-guided sampling. Thus, this study achieves the collaborative optimization of imbalanced learning and model training. Extensive experiments on 20 public imbalanced datasets demonstrate that the proposed model significantly outperforms eight state-of-the-art baselines. The code for the proposed model is available at: https://github.com/ChuantaoLi/DARG.

</details>


### [95] [Toward Real-World IoT Security: Concept Drift-Resilient IoT Botnet Detection via Latent Space Representation Learning and Alignment](https://arxiv.org/abs/2512.22488)
*Hassan Wasswa,Timothy Lynar*

Main category: cs.LG

TL;DR: 提出一个无需持续重训练的自适应物联网威胁检测框架，通过潜在空间对齐和图神经网络处理概念漂移问题


<details>
  <summary>Details</summary>
Motivation: 现有AI模型依赖静态数据集，无法适应真实物联网NetFlow流量的动态变化和概念漂移问题，而定期重训练方案计算开销大且存在灾难性遗忘风险

Method: 1) 在历史流量潜在空间表示上训练一次分类器；2) 使用对齐模型将新流量映射到已学习的历史潜在空间；3) 将低维潜在表示转换为图结构格式；4) 使用图神经网络进行分类，捕捉攻击样本间的实例关系

Result: 在真实世界异构物联网流量数据集上的实验评估表明，该框架在概念漂移下保持稳健的检测性能

Conclusion: 该框架具有在动态和大规模物联网环境中实际部署的潜力，解决了概念漂移问题同时避免了持续重训练的开销

Abstract: Although AI-based models have achieved high accuracy in IoT threat detection, their deployment in enterprise environments is constrained by reliance on stationary datasets that fail to reflect the dynamic nature of real-world IoT NetFlow traffic, which is frequently affected by concept drift. Existing solutions typically rely on periodic classifier retraining, resulting in high computational overhead and the risk of catastrophic forgetting. To address these challenges, this paper proposes a scalable framework for adaptive IoT threat detection that eliminates the need for continuous classifier retraining. The proposed approach trains a classifier once on latent-space representations of historical traffic, while an alignment model maps incoming traffic to the learned historical latent space prior to classification, thereby preserving knowledge of previously observed attacks. To capture inter-instance relationships among attack samples, the low-dimensional latent representations are further transformed into a graph-structured format and classified using a graph neural network. Experimental evaluations on real-world heterogeneous IoT traffic datasets demonstrate that the proposed framework maintains robust detection performance under concept drift. These results highlight the framework's potential for practical deployment in dynamic and large-scale IoT environments.

</details>


### [96] [The Quest for Winning Tickets in Low-Rank Adapters](https://arxiv.org/abs/2512.22495)
*Hamed Damirchi,Cristian Rodriguez-Opazo,Ehsan Abbasnejad,Zhen Zhang,Javen Shi*

Main category: cs.LG

TL;DR: 研究发现彩票假设在LoRA微调中成立，稀疏子网络能达到密集适配器性能，提出Partial-LoRA方法大幅减少可训练参数


<details>
  <summary>Details</summary>
Motivation: 随着对大型预训练模型微调的依赖增加，研究彩票假设是否适用于参数高效微调方法，特别是LoRA

Method: 提出Partial-LoRA方法，系统识别稀疏子网络，训练与预训练模型任务相关子空间对齐的稀疏低秩适配器

Result: 在8个视觉和12个语言任务上，Partial-LoRA将可训练参数减少高达87%，同时保持或提高准确性

Conclusion: 研究深化了对迁移学习和预训练-微调相互作用的理论理解，为开发更高效的适应策略开辟了新途径

Abstract: The Lottery Ticket Hypothesis (LTH) suggests that over-parameterized neural networks contain sparse subnetworks ("winning tickets") capable of matching full model performance when trained from scratch. With the growing reliance on fine-tuning large pretrained models, we investigate whether LTH extends to parameter-efficient fine-tuning (PEFT), specifically focusing on Low-Rank Adaptation (LoRA) methods. Our key finding is that LTH holds within LoRAs, revealing sparse subnetworks that can match the performance of dense adapters. In particular, we find that the effectiveness of sparse subnetworks depends more on how much sparsity is applied in each layer than on the exact weights included in the subnetwork. Building on this insight, we propose Partial-LoRA, a method that systematically identifies said subnetworks and trains sparse low-rank adapters aligned with task-relevant subspaces of the pre-trained model. Experiments across 8 vision and 12 language tasks in both single-task and multi-task settings show that Partial-LoRA reduces the number of trainable parameters by up to 87\%, while maintaining or improving accuracy. Our results not only deepen our theoretical understanding of transfer learning and the interplay between pretraining and fine-tuning but also open new avenues for developing more efficient adaptation strategies.

</details>


### [97] [Predicting LLM Correctness in Prosthodontics Using Metadata and Hallucination Signals](https://arxiv.org/abs/2512.22508)
*Lucky Susanto,Anasta Pranawijayana,Cortino Sukotjo,Soni Prasad,Derry Wijaya*

Main category: cs.LG

TL;DR: 利用元数据和幻觉信号预测LLM在牙科考试中的回答正确性，准确率最高提升7.14%，但方法尚不足以用于高风险部署


<details>
  <summary>Details</summary>
Motivation: LLM在医疗等高风险领域应用时，生成错误信息（幻觉）的风险很高。虽然已有检测和缓解幻觉的研究，但预测LLM回答是否正确仍是一个关键但未充分探索的问题。

Method: 研究通用模型（GPT-4o）和推理中心模型（OSS-120B）在牙科多选题考试上的表现。使用三种不同提示策略下的元数据和幻觉信号，为每个（模型，提示）组合构建正确性预测器。

Result: 基于元数据的方法可将准确率最高提升+7.14%，达到83.12%的精确度（相比假设所有答案都正确的基线）。实际幻觉是错误回答的强指标，但仅凭元数据信号不能可靠预测幻觉。提示策略虽不影响总体准确率，但显著改变模型内部行为和元数据的预测效用。

Conclusion: 该方法为开发LLM可靠性信号提供了有前景的方向，但本文探索的方法尚不够稳健，无法用于关键的高风险部署。

Abstract: Large language models (LLMs) are increasingly adopted in high-stakes domains such as healthcare and medical education, where the risk of generating factually incorrect (i.e., hallucinated) information is a major concern. While significant efforts have been made to detect and mitigate such hallucinations, predicting whether an LLM's response is correct remains a critical yet underexplored problem. This study investigates the feasibility of predicting correctness by analyzing a general-purpose model (GPT-4o) and a reasoning-centric model (OSS-120B) on a multiple-choice prosthodontics exam. We utilize metadata and hallucination signals across three distinct prompting strategies to build a correctness predictor for each (model, prompting) pair. Our findings demonstrate that this metadata-based approach can improve accuracy by up to +7.14% and achieve a precision of 83.12% over a baseline that assumes all answers are correct. We further show that while actual hallucination is a strong indicator of incorrectness, metadata signals alone are not reliable predictors of hallucination. Finally, we reveal that prompting strategies, despite not affecting overall accuracy, significantly alter the models' internal behaviors and the predictive utility of their metadata. These results present a promising direction for developing reliability signals in LLMs but also highlight that the methods explored in this paper are not yet robust enough for critical, high-stakes deployment.

</details>


### [98] [Decomposing Task Vectors for Refined Model Editing](https://arxiv.org/abs/2512.22511)
*Hamed Damirchi,Ehsan Abbasnejad,Zhen Zhang,Javen Shi*

Main category: cs.LG

TL;DR: 提出一种任务向量分解方法，将每个任务向量分解为共享知识和独特信息两个组件，以解决任务向量算术操作中的概念干扰问题。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型难以精确控制特定概念行为，任务向量算术操作虽然可以组合不同行为，但向量间的重叠概念会相互干扰，导致不可预测的结果。

Method: 通过识别投影中的不变子空间，将每个任务向量分解为两个组件：一个捕获多个任务向量间的共享知识，另一个隔离每个特定任务的独特信息。

Result: 在三个领域验证有效性：图像分类多任务合并提升5%；扩散模型实现干净风格混合而不降低生成质量；语言模型毒性降低47%同时保持通用知识任务性能。

Conclusion: 该方法为理解和控制任务向量算术提供了新框架，解决了模型编辑操作中的基本限制，实现了更精确的概念操控。

Abstract: Large pre-trained models have transformed machine learning, yet adapting these models effectively to exhibit precise, concept-specific behaviors remains a significant challenge. Task vectors, defined as the difference between fine-tuned and pre-trained model parameters, provide a mechanism for steering neural networks toward desired behaviors. This has given rise to large repositories dedicated to task vectors tailored for specific behaviors. The arithmetic operation of these task vectors allows for the seamless combination of desired behaviors without the need for large datasets. However, these vectors often contain overlapping concepts that can interfere with each other during arithmetic operations, leading to unpredictable outcomes. We propose a principled decomposition method that separates each task vector into two components: one capturing shared knowledge across multiple task vectors, and another isolating information unique to each specific task. By identifying invariant subspaces across projections, our approach enables more precise control over concept manipulation without unintended amplification or diminution of other behaviors. We demonstrate the effectiveness of our decomposition method across three domains: improving multi-task merging in image classification by 5% using shared components as additional task vectors, enabling clean style mixing in diffusion models without generation degradation by mixing only the unique components, and achieving 47% toxicity reduction in language models while preserving performance on general knowledge tasks by negating the toxic information isolated to the unique component. Our approach provides a new framework for understanding and controlling task vector arithmetic, addressing fundamental limitations in model editing operations.

</details>


### [99] [Towards Reliable Evaluation of Adversarial Robustness for Spiking Neural Networks](https://arxiv.org/abs/2512.22522)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种更可靠的SNN对抗鲁棒性评估框架，包括自适应锐度替代梯度(ASSG)和稳定自适应投影梯度下降(SA-PGD)攻击方法，显著提高了攻击成功率，揭示了当前SNN鲁棒性被高估的问题。


<details>
  <summary>Details</summary>
Motivation: SNN使用脉冲激活模拟大脑高效信息处理，但其二值不连续特性导致梯度消失，使得基于梯度下降的对抗鲁棒性评估不可靠。现有替代梯度方法在强对抗攻击下的有效性不明确，需要更可靠的评估框架。

Method: 1) 理论分析替代梯度中的梯度消失程度；2) 提出自适应锐度替代梯度(ASSG)，在攻击迭代中根据输入分布自适应调整替代函数形状，提高梯度准确性并缓解梯度消失；3) 设计L∞约束下的稳定自适应投影梯度下降(SA-PGD)攻击，在梯度不精确时实现更快更稳定的收敛。

Result: 实验表明，该方法在各种对抗训练方案、SNN架构和神经元模型中显著提高攻击成功率，提供了更通用可靠的SNN对抗鲁棒性评估。结果进一步揭示当前SNN的鲁棒性被显著高估。

Conclusion: 本文提出的ASSG和SA-PGD框架为SNN对抗鲁棒性提供了更可靠的评估方法，揭示了现有SNN鲁棒性评估的不足，强调了开发更可靠对抗训练方法的必要性。

Abstract: Spiking Neural Networks (SNNs) utilize spike-based activations to mimic the brain's energy-efficient information processing. However, the binary and discontinuous nature of spike activations causes vanishing gradients, making adversarial robustness evaluation via gradient descent unreliable. While improved surrogate gradient methods have been proposed, their effectiveness under strong adversarial attacks remains unclear. We propose a more reliable framework for evaluating SNN adversarial robustness. We theoretically analyze the degree of gradient vanishing in surrogate gradients and introduce the Adaptive Sharpness Surrogate Gradient (ASSG), which adaptively evolves the shape of the surrogate function according to the input distribution during attack iterations, thereby enhancing gradient accuracy while mitigating gradient vanishing. In addition, we design an adversarial attack with adaptive step size under the $L_\infty$ constraint-Stable Adaptive Projected Gradient Descent (SA-PGD), achieving faster and more stable convergence under imprecise gradients. Extensive experiments show that our approach substantially increases attack success rates across diverse adversarial training schemes, SNN architectures and neuron models, providing a more generalized and reliable evaluation of SNN adversarial robustness. The experimental results further reveal that the robustness of current SNNs has been significantly overestimated and highlighting the need for more dependable adversarial training methods.

</details>


### [100] [TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting](https://arxiv.org/abs/2512.22550)
*Jaebin Lee,Hankook Lee*

Main category: cs.LG

TL;DR: TimePerceiver：统一的编码器-解码器时间序列预测框架，通过广义预测任务定义、灵活的编码器-解码器架构和有效的训练策略，在多个基准数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测研究主要关注编码器设计，而将预测和解码视为次要问题。本文旨在提出一个统一的编码器-解码器框架，将预测任务与有效的训练策略紧密结合。

Method: 1. 将预测任务广义化为外推、插值和填补等多种时间预测目标；2. 设计灵活的编码器-解码器架构处理任意位置的时间段；3. 编码器使用潜在瓶颈表示捕获时间和跨通道依赖；4. 解码器使用可学习查询检索相关信息。

Result: 在广泛的基准数据集上，TimePerceiver框架持续且显著地超越了先前的最先进基线方法。

Conclusion: TimePerceiver提供了一个统一且有效的编码器-解码器框架，通过紧密整合预测任务定义、架构设计和训练策略，显著提升了时间序列预测性能。

Abstract: In machine learning, effective modeling requires a holistic consideration of how to encode inputs, make predictions (i.e., decoding), and train the model. However, in time-series forecasting, prior work has predominantly focused on encoder design, often treating prediction and training as separate or secondary concerns. In this paper, we propose TimePerceiver, a unified encoder-decoder forecasting framework that is tightly aligned with an effective training strategy. To be specific, we first generalize the forecasting task to include diverse temporal prediction objectives such as extrapolation, interpolation, and imputation. Since this generalization requires handling input and target segments that are arbitrarily positioned along the temporal axis, we design a novel encoder-decoder architecture that can flexibly perceive and adapt to these varying positions. For encoding, we introduce a set of latent bottleneck representations that can interact with all input segments to jointly capture temporal and cross-channel dependencies. For decoding, we leverage learnable queries corresponding to target timestamps to effectively retrieve relevant information. Extensive experiments demonstrate that our framework consistently and significantly outperforms prior state-of-the-art baselines across a wide range of benchmark datasets. The code is available at https://github.com/efficient-learning-lab/TimePerceiver.

</details>


### [101] [Data-Driven Analysis of Crash Patterns in SAE Level 2 and Level 4 Automated Vehicles Using K-means Clustering and Association Rule Mining](https://arxiv.org/abs/2512.22589)
*Jewel Rana Palit,Vijayalakshmi K Kumarasamy,Osama A. Osman*

Main category: cs.LG

TL;DR: 该研究分析美国NHTSA的2500多起自动驾驶车辆事故数据，使用K-means聚类和关联规则挖掘，揭示不同SAE级别（2级和4级）自动驾驶事故的动态模式和影响因素。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆虽有望提升交通安全，但实际事故数据显示其行为可能偏离预期安全结果。现有研究多基于小规模加州数据，缺乏对不同自动化级别事故趋势的系统分析，需要更全面的数据挖掘来理解事故动态。

Method: 开发两阶段数据挖掘框架：1) 使用K-means聚类基于时空和环境因素将事故记录分为4个行为集群；2) 在每个集群内应用关联规则挖掘，提取事故模式与影响因素（照明条件、路面状况、车辆动态、环境条件）之间的多元关系。

Result: 分析揭示了不同SAE级别（2级和4级）自动驾驶事故的底层动态模式，识别出影响事故的关键因素组合，为理解自动驾驶事故机制提供了数据驱动的见解。

Conclusion: 研究结果为自动驾驶开发者、安全监管机构和政策制定者提供了可操作的指导，有助于制定更有效的自动驾驶部署策略和降低事故风险。

Abstract: Automated Vehicles (AV) hold potential to reduce or eliminate human driving errors, enhance traffic safety, and support sustainable mobility. Recently, crash data has increasingly revealed that AV behavior can deviate from expected safety outcomes, raising concerns about the technology's safety and operational reliability in mixed traffic environments. While past research has investigated AV crash, most studies rely on small-size California-centered datasets, with a limited focus on understanding crash trends across various SAE Levels of automation. This study analyzes over 2,500 AV crash records from the United States National Highway Traffic Safety Administration (NHTSA), covering SAE Levels 2 and 4, to uncover underlying crash dynamics. A two-stage data mining framework is developed. K-means clustering is first applied to segment crash records into 4 distinct behavioral clusters based on temporal, spatial, and environmental factors. Then, Association Rule Mining (ARM) is used to extract interpretable multivariate relationships between crash patterns and crash contributors including lighting conditions, surface condition, vehicle dynamics, and environmental conditions within each cluster. These insights provide actionable guidance for AV developers, safety regulators, and policymakers in formulating AV deployment strategies and minimizing crash risks.

</details>


### [102] [Energy-Guided Flow Matching Enables Few-Step Conformer Generation and Ground-State Identification](https://arxiv.org/abs/2512.22597)
*Guikun Xu,Xiaohan Yi,Peilin Zhao,Yatao Bian*

Main category: cs.LG

TL;DR: EnFlow是一个统一框架，结合流匹配和显式学习的能量模型，通过能量引导采样生成低能构象并识别基态构象


<details>
  <summary>Details</summary>
Motivation: 现有基于物理的计算方法生成低能构象集合和识别基态构象计算成本高。当前基于学习的方法存在碎片化问题：生成模型能捕捉多样性但缺乏可靠的能量校准，而确定性预测器只针对单一结构且无法表示集合变异性。

Method: EnFlow框架将流匹配与显式学习的能量模型耦合，通过沿非高斯流匹配路径定义的能量引导采样方案。在采样过程中加入能量梯度引导，使轨迹向低能区域移动，显著提高构象保真度。学习的能量函数还能对生成的集合进行基于能量的高效排序，实现准确的基态识别。

Result: 在GEOM-QM9和GEOM-Drugs数据集上的广泛实验表明，EnFlow在1-2个ODE步骤内同时改进了生成指标，并相比最先进方法减少了基态预测误差。

Conclusion: EnFlow提供了一个统一框架，通过结合流匹配和能量引导采样，解决了现有方法在构象生成和基态识别中的碎片化问题，在计算效率和准确性方面都取得了显著改进。

Abstract: Generating low-energy conformer ensembles and identifying ground-state conformations from molecular graphs remain computationally demanding with physics-based pipelines. Current learning-based approaches often suffer from a fragmented paradigm: generative models capture diversity but lack reliable energy calibration, whereas deterministic predictors target a single structure and fail to represent ensemble variability. Here we present EnFlow, a unified framework that couples flow matching (FM) with an explicitly learned energy model through an energy-guided sampling scheme defined along a non-Gaussian FM path. By incorporating energy-gradient guidance during sampling, our method steers trajectories toward lower-energy regions, substantially improving conformational fidelity, particularly in the few-step regime. The learned energy function further enables efficient energy-based ranking of generated ensembles for accurate ground-state identification. Extensive experiments on GEOM-QM9 and GEOM-Drugs demonstrate that EnFlow simultaneously improves generation metrics with 1--2 ODE-steps and reduces ground-state prediction errors compared with state-of-the-art methods.

</details>


### [103] [Cryptocurrency Price Prediction Using Parallel Gated Recurrent Units](https://arxiv.org/abs/2512.22599)
*Milad Asadpour,Alireza Rezaee,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出名为PGRU的并行门控循环单元模型，用于加密货币价格预测，通过并行独立处理不同价格特征，在减少输入数据和计算成本的同时获得更高精度


<details>
  <summary>Details</summary>
Motivation: 随着加密货币投资兴起，价格预测成为重要挑战。现有方法需要改进预测精度和效率，特别是减少输入数据和计算成本

Method: 提出并行门控循环单元(PGRU)模型，使用循环神经网络并行独立预测价格，不同并行网络处理不同价格相关特征，最后通过神经网络组合输出进行价格预测

Result: 在窗口长度20和15时分别获得3.243%和2.641%的平均绝对百分比误差(MAPE)，比现有方法精度更高、效率更好，且输入数据更少、计算成本更低

Conclusion: PGRU模型在加密货币价格预测中表现出优越性能，为投资者提供了更准确、高效的价格预测工具

Abstract: According to the advent of cryptocurrencies and Bitcoin, many investments and businesses are now conducted online through cryptocurrencies. Among them, Bitcoin uses blockchain technology to make transactions secure, transparent, traceable, and immutable. It also exhibits significant price fluctuations and performance, which has attracted substantial attention, especially in financial sectors. Consequently, a wide range of investors and individuals have turned to investing in the cryptocurrency market. One of the most important challenges in economics is price forecasting for future trades. Cryptocurrencies are no exception, and investors are looking for methods to predict prices; various theories and methods have been proposed in this field. This paper presents a new deep model, called \emph{Parallel Gated Recurrent Units} (PGRU), for cryptocurrency price prediction. In this model, recurrent neural networks forecast prices in a parallel and independent way. The parallel networks utilize different inputs, each representing distinct price-related features. Finally, the outputs of the parallel networks are combined by a neural network to forecast the future price of cryptocurrencies. The experimental results indicate that the proposed model achieves mean absolute percentage errors (MAPE) of 3.243% and 2.641% for window lengths 20 and 15, respectively. Our method therefore attains higher accuracy and efficiency with fewer input data and lower computational cost compared to existing methods.

</details>


### [104] [Gold Price Prediction Using Long Short-Term Memory and Multi-Layer Perceptron with Gray Wolf Optimizer](https://arxiv.org/abs/2512.22606)
*Hesam Taghipour,Alireza Rezaee,Farshid Hajati*

Main category: cs.LG

TL;DR: 本文提出一种基于LSTM-MLP混合模型的金价预测算法，结合灰狼优化算法优化网络参数，在日度和月度时间框架上预测金价最高、最低和收盘价，并基于预测开发交易策略获得171%的三个月回报率。


<details>
  <summary>Details</summary>
Motivation: 黄金市场因其复杂性，准确预测一直具有挑战性。开发能够准确预测市场走势的模型对金融机构、银行、政府和投资者都具有重要意义。

Method: 使用两个LSTM网络分别进行日度和月度预测，将结果输入MLP网络进行整合，得到最终预测。采用灰狼优化算法优化各网络内部神经元数量，基于最小RMSE误差。数据集包含2010-2021年宏观经济、能源市场、股票和发达国家货币状况数据。

Result: 模型预测日度金价收盘价的MAE为0.21美元，月度预测MAE为22.23美元。基于预测开发的交易策略在三个月内获得171%的回报率。

Conclusion: 提出的LSTM-MLP混合模型能够有效预测金价，为市场参与者提供有价值的预测工具，并可通过优化的交易策略获得显著收益。

Abstract: The global gold market, by its fundamentals, has long been home to many financial institutions, banks, governments, funds, and micro-investors. Due to the inherent complexity and relationship between important economic and political components, accurate forecasting of financial markets has always been challenging. Therefore, providing a model that can accurately predict the future of the markets is very important and will be of great benefit to their developers. In this paper, an artificial intelligence-based algorithm for daily and monthly gold forecasting is presented. Two Long short-term memory (LSTM) networks are responsible for daily and monthly forecasting, the results of which are integrated into a Multilayer perceptrons (MLP) network and provide the final forecast of the next day prices. The algorithm forecasts the highest, lowest, and closing prices on the daily and monthly time frame. Based on these forecasts, a trading strategy for live market trading was developed, according to which the proposed model had a return of 171% in three months. Also, the number of internal neurons in each network is optimized by the Gray Wolf optimization (GWO) algorithm based on the least RMSE error. The dataset was collected between 2010 and 2021 and includes data on macroeconomic, energy markets, stocks, and currency status of developed countries. Our proposed LSTM-MLP model predicted the daily closing price of gold with the Mean absolute error (MAE) of $ 0.21 and the next month's price with $ 22.23.

</details>


### [105] [Scaling Unverifiable Rewards: A Case Study on Visual Insights](https://arxiv.org/abs/2512.22650)
*Shuyu Gan,James Mooney,Pan Hao,Renxiang Wang,Mingyi Hong,Qianwen Wang,Dongyeop Kang*

Main category: cs.LG

TL;DR: 提出Selective TTS框架，在多阶段多智能体管道中分布计算资源，通过过程特定评估器早期剪枝低质量分支，解决无验证奖励任务中的错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界多阶段任务缺乏可验证的最终奖励或足够数据训练鲁棒奖励模型，导致基于评估器的迭代细化容易在阶段间累积错误。

Method: 提出Selective TTS框架，在多智能体管道中跨阶段分布计算而非时间上的重复细化，使用过程特定评估器早期剪枝低质量分支，缓解评估器漂移并稳定细化过程。

Result: 在数据科学管道中构建端到端多智能体系统生成可视化图表和报告，设计出与人类专家对齐的LLM评估器(Kendall's τ=0.55)。Selective TTS在固定计算预算下将平均得分从61.64提升至65.86，同时降低方差。

Conclusion: Selective TTS是扩展具有不可验证奖励的复杂开放式任务（如科学发现和故事生成）的第一步，通过过程基础细化框架改善多阶段管道中的推理质量。

Abstract: Large Language Model (LLM) agents can increasingly automate complex reasoning through Test-Time Scaling (TTS), iterative refinement guided by reward signals. However, many real-world tasks involve multi-stage pipeline whose final outcomes lack verifiable rewards or sufficient data to train robust reward models, making judge-based refinement prone to accumulate error over stages. We propose Selective TTS, a process-based refinement framework that scales inference across different stages in multi-agent pipeline, instead of repeated refinement over time by prior work. By distributing compute across stages and pruning low-quality branches early using process-specific judges, Selective TTS mitigates the judge drift and stabilizes refinement. Grounded in the data science pipeline, we build an end-to-end multi-agent pipeline for generating visually insightful charts and report of given dataset, and design a reliable LLM-based judge model, aligned with human experts (Kendall's τ=0.55). Our proposed selective TTS then improves insight quality under a fixed compute budget, increasing mean scores from 61.64 to 65.86 while reducing variance. We hope our findings serve as the first step toward to scaling complex, open-ended tasks with unverifiable rewards, such as scientific discovery and story generation.

</details>


### [106] [Quantum Generative Models for Computational Fluid Dynamics: A First Exploration of Latent Space Learning in Lattice Boltzmann Simulations](https://arxiv.org/abs/2512.22672)
*Achraf Hsain,Fouad Mohammed Abbou*

Main category: cs.LG

TL;DR: 首次将量子生成模型应用于计算流体动力学数据的潜在空间表示，比较了量子电路玻恩机和量子生成对抗网络与经典LSTM的性能，量子模型在压缩物理模拟数据上表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索量子生成模型在物理模拟数据压缩表示中的应用，填补量子模型与计算流体动力学数据潜在空间生成建模的研究空白。

Method: 使用GPU加速的格子玻尔兹曼方法生成流体涡量场，通过向量量化变分自编码器压缩到7维离散潜在空间，然后比较量子电路玻恩机、量子生成对抗网络和经典长短期记忆网络的生成性能。

Result: 在实验条件下，两种量子模型生成的样本与真实分布的平均最小距离均低于LSTM基线，其中量子电路玻恩机获得了最有利的评估指标。

Conclusion: 该研究建立了连接计算流体动力学模拟与量子机器学习的完整开源流程，首次在物理模拟的压缩潜在表示上进行了量子生成建模的实证研究，为这一交叉领域的未来严格研究奠定了基础。

Abstract: This paper presents the first application of quantum generative models to learned latent space representations of computational fluid dynamics (CFD) data. While recent work has explored quantum models for learning statistical properties of fluid systems, the combination of discrete latent space compression with quantum generative sampling for CFD remains unexplored. We develop a GPU-accelerated Lattice Boltzmann Method (LBM) simulator to generate fluid vorticity fields, which are compressed into a discrete 7-dimensional latent space using a Vector Quantized Variational Autoencoder (VQ-VAE). The central contribution is a comparative analysis of quantum and classical generative approaches for modeling this physics-derived latent distribution: we evaluate a Quantum Circuit Born Machine (QCBM) and Quantum Generative Adversarial Network (QGAN) against a classical Long Short-Term Memory (LSTM) baseline. Under our experimental conditions, both quantum models produced samples with lower average minimum distances to the true distribution compared to the LSTM, with the QCBM achieving the most favorable metrics. This work provides: (1)~a complete open-source pipeline bridging CFD simulation and quantum machine learning, (2)~the first empirical study of quantum generative modeling on compressed latent representations of physics simulations, and (3)~a foundation for future rigorous investigation at this intersection.

</details>


### [107] [Beyond Centralization: Provable Communication Efficient Decentralized Multi-Task Learning](https://arxiv.org/abs/2512.22675)
*Donghwa Kang,Shana Moothedath*

Main category: cs.LG

TL;DR: 提出一种去中心化多任务表示学习算法，通过低秩结构学习共享特征，通信复杂度与目标精度无关，显著降低通信成本


<details>
  <summary>Details</summary>
Motivation: 虽然表示学习在数据稀缺环境中被广泛采用，但现有研究主要集中在中心化方法，去中心化方法仍未被充分探索。在去中心化设置中，任务数据分布在多个节点上，节点间的信息交换受通信网络限制，需要开发高效的去中心化多任务表示学习算法

Method: 提出一种新的交替投影梯度和最小化算法，用于去中心化多任务表示学习。算法假设特征共享低秩结构，每个任务遵循具有任务特定参数的线性模型。算法在通信网络约束下，从分布在多个节点的任务数据中恢复低秩特征矩阵

Result: 算法具有可证明的精度保证，全面表征了时间、通信和样本复杂度。关键创新是通信复杂度与目标精度无关，相比先前方法显著降低通信成本。数值模拟验证了理论分析，并展示了去中心化学习在某些情况下优于中心化联邦方法

Conclusion: 该研究填补了去中心化多任务表示学习的空白，提出的算法在通信效率方面具有显著优势，为分布式环境中的表示学习提供了有效的解决方案，在某些场景下甚至优于中心化方法

Abstract: Representation learning is a widely adopted framework for learning in data-scarce environments, aiming to extract common features from related tasks. While centralized approaches have been extensively studied, decentralized methods remain largely underexplored. We study decentralized multi-task representation learning in which the features share a low-rank structure. We consider multiple tasks, each with a finite number of data samples, where the observations follow a linear model with task-specific parameters. In the decentralized setting, task data are distributed across multiple nodes, and information exchange between nodes is constrained by a communication network. The goal is to recover the underlying feature matrix whose rank is much smaller than both the parameter dimension and the number of tasks. We propose a new alternating projected gradient and minimization algorithm with provable accuracy guarantees. We provide comprehensive characterizations of the time, communication, and sample complexities. Importantly, the communication complexity is independent of the target accuracy, which significantly reduces communication cost compared to prior methods. Numerical simulations validate the theoretical analysis across different dimensions and network topologies, and demonstrate regimes in which decentralized learning outperforms centralized federated approaches.

</details>


### [108] [Learning with the $p$-adics](https://arxiv.org/abs/2512.22692)
*André F. T. Martins*

Main category: cs.LG

TL;DR: 论文探索使用p-adic数（ℚₚ）作为实数（ℝ）的替代，为机器学习提供新的数学框架，特别适用于层次表示学习和编码理论。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习框架基于实数域（ℝ），但作者质疑这是否是唯一选择。p-adic数的超度量结构和层次特性使其在编码理论和层次表示学习中具有独特优势，可能提供更合适的数学基础。

Method: 建立p-adic数在分类、回归和表示学习中的理论基础，提供学习模型和算法。特别展示了如何将简单的Quillian语义网络表示为紧凑的p-adic线性网络，这种构造在实数域中无法实现。

Result: 构建了p-adic机器学习的基本模块，展示了p-adic框架在层次表示方面的独特能力，特别是能够实现实数域中不可能的网络构造。

Conclusion: p-adic数为机器学习提供了有前景的替代数学框架，特别适用于层次结构建模。论文为这一新领域奠定了基础，并指出了未来研究方向。

Abstract: Existing machine learning frameworks operate over the field of real numbers ($\mathbb{R}$) and learn representations in real (Euclidean or Hilbert) vector spaces (e.g., $\mathbb{R}^d$). Their underlying geometric properties align well with intuitive concepts such as linear separability, minimum enclosing balls, and subspace projection; and basic calculus provides a toolbox for learning through gradient-based optimization.
  But is this the only possible choice? In this paper, we study the suitability of a radically different field as an alternative to $\mathbb{R}$ -- the ultrametric and non-archimedean space of $p$-adic numbers, $\mathbb{Q}_p$. The hierarchical structure of the $p$-adics and their interpretation as infinite strings make them an appealing tool for code theory and hierarchical representation learning. Our exploratory theoretical work establishes the building blocks for classification, regression, and representation learning with the $p$-adics, providing learning models and algorithms. We illustrate how simple Quillian semantic networks can be represented as a compact $p$-adic linear network, a construction which is not possible with the field of reals. We finish by discussing open problems and opportunities for future research enabled by this new framework.

</details>


### [109] [What Matters in Deep Learning for Time Series Forecasting?](https://arxiv.org/abs/2512.22702)
*Valentina Moretti,Andrea Cini,Ivan Marisca,Cesare Alippi*

Main category: cs.LG

TL;DR: 该论文分析了时间序列预测深度学习架构的设计空间，指出当前基准测试存在问题，强调基于预测原则（如局部性与全局性）的简单架构能与最先进方法匹敌，并提出辅助预测模型卡来规范架构设计。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列应用中日益流行，但新架构数量激增且实证结果常相互矛盾，难以评估哪些组件真正影响最终性能。需要理清当前时间序列预测深度学习架构的设计空间，理解设计维度和权衡，以解释常令人意外的观察结果。

Method: 论文基于时间序列组的预测原则来讨论模型设计，评估局部性和全局性等概念在近期预测架构中的应用。分析现有架构中被忽视的实现细节如何改变预测方法的类别并显著影响实证结果。提出辅助预测模型卡，通过关键设计选择来表征现有和新预测架构。

Result: 研究表明，考虑局部性和全局性等基础方面比采用特定序列建模层对实现准确结果更为重要，简单但设计良好的预测架构常能与最先进方法匹敌。现有架构中被忽视的实现细节会从根本上改变预测方法的类别并极大影响观察到的实证结果。

Conclusion: 当前有缺陷的基准测试实践需要重新思考，设计架构时应聚焦于预测问题的基础方面。提出的辅助预测模型卡可作为向此方向迈出的一步，帮助系统地表征预测架构的关键设计选择。

Abstract: Deep learning models have grown increasingly popular in time series applications. However, the large quantity of newly proposed architectures, together with often contradictory empirical results, makes it difficult to assess which components contribute significantly to final performance. We aim to make sense of the current design space of deep learning architectures for time series forecasting by discussing the design dimensions and trade-offs that can explain, often unexpected, observed results. This paper discusses the necessity of grounding model design on principles for forecasting groups of time series and how such principles can be applied to current models. In particular, we assess how concepts such as locality and globality apply to recent forecasting architectures. We show that accounting for these aspects can be more relevant for achieving accurate results than adopting specific sequence modeling layers and that simple, well-designed forecasting architectures can often match the state of the art. We discuss how overlooked implementation details in existing architectures (1) fundamentally change the class of the resulting forecasting method and (2) drastically affect the observed empirical results. Our results call for rethinking current faulty benchmarking practices and the need to focus on the foundational aspects of the forecasting problem when designing architectures. As a step in this direction, we propose an auxiliary forecasting model card, whose fields serve to characterize existing and new forecasting architectures based on key design choices.

</details>


### [110] [FoldAct: Efficient and Stable Context Folding for Long-Horizon Search Agents](https://arxiv.org/abs/2512.22733)
*Jiaqi Shao,Yufeng Miao,Wei Zhang,Bing Luo*

Main category: cs.LG

TL;DR: FoldAct框架解决了长时程RL中上下文折叠带来的非平稳观测分布问题，通过分离损失计算、全上下文一致性损失和选择性片段训练，实现稳定训练和5.19倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有上下文折叠方法将摘要动作视为标准动作，忽略了摘要会改变智能体未来的观测空间，导致策略依赖的非平稳观测分布，违反了RL核心假设，带来梯度稀释、自条件和计算成本三大挑战。

Method: FoldAct框架包含三个关键创新：1) 分离损失计算，为摘要和动作token提供独立的梯度信号；2) 全上下文一致性损失，减少分布偏移；3) 选择性片段训练，降低计算成本。

Result: 该方法实现了长时程搜索智能体的稳定训练，解决了非平稳观测问题，同时提高了训练效率，达到5.19倍的加速。

Conclusion: FoldAct成功解决了上下文折叠中的非平稳观测分布问题，为长时程RL提供了稳定高效的训练框架，显著提升了训练速度和稳定性。

Abstract: Long-horizon reinforcement learning (RL) for large language models faces critical scalability challenges from unbounded context growth, leading to context folding methods that compress interaction history during task execution. However, existing approaches treat summary actions as standard actions, overlooking that summaries fundamentally modify the agent's future observation space, creating a policy-dependent, non-stationary observation distribution that violates core RL assumptions. This introduces three fundamental challenges: (1) gradient dilution where summary tokens receive insufficient training signal, (2) self-conditioning where policy updates change summary distributions, creating a vicious cycle of training collapse, and (3) computational cost from processing unique contexts at each turn. We introduce \textbf{FoldAct}\footnote{https://github.com/SHAO-Jiaqi757/FoldAct}, a framework that explicitly addresses these challenges through three key innovations: separated loss computation for independent gradient signals on summary and action tokens, full context consistency loss to reduce distribution shift, and selective segment training to reduce computational cost. Our method enables stable training of long-horizon search agents with context folding, addressing the non-stationary observation problem while improving training efficiency with 5.19$\times$ speedup.

</details>


### [111] [When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction](https://arxiv.org/abs/2512.22740)
*Sungwoo Kang*

Main category: cs.LG

TL;DR: 多任务学习在合金材料预测中呈现矛盾结果：回归任务性能显著下降，但分类任务性能提升


<details>
  <summary>Details</summary>
Motivation: 测试多任务学习在材料科学中的有效性，假设相关材料属性共享底层物理机制可以提升预测性能

Method: 使用54,028个合金样本同时预测电阻率、维氏硬度和非晶形成能力，比较单任务模型与标准/结构化多任务学习

Result: 多任务学习显著降低回归性能（电阻率R²: 0.897→0.844；硬度R²: 0.832→0.694），但提升分类性能（非晶F1: 0.703→0.744；召回率+17%）

Conclusion: 近零的跨任务权重表明属性独立性，回归失败归因于严重数据不平衡导致的负迁移。建议回归任务使用独立模型，分类任务可考虑多任务学习以提高召回率

Abstract: Multi-task learning (MTL) assumes related material properties share underlying physics that can be leveraged for better predictions. We test this by simultaneously predicting electrical resistivity, Vickers hardness, and amorphous-forming ability using 54,028 alloy samples. We compare single-task models against standard and structured MTL. Results reveal a striking dichotomy: MTL significantly degrades regression performance (resistivity $R^2$: 0.897 $\to$ 0.844; hardness $R^2$: 0.832 $\to$ 0.694, $p < 0.01$) but improves classification (amorphous F1: 0.703 $\to$ 0.744, $p < 0.05$; recall +17%). Analysis shows near-zero inter-task weights, indicating property independence. Regression failure is attributed to negative transfer caused by severe data imbalance (52k vs. 800 samples). We recommend independent models for precise regression, while reserving MTL for classification tasks where recall is critical.

</details>


### [112] [Bridging Global Intent with Local Details: A Hierarchical Representation Approach for Semantic Validation in Text-to-SQL](https://arxiv.org/abs/2512.22744)
*Rihong Qiu,Zhibang Yang,Xinke Jiang,Weibin Liao,Xin Gao,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.LG

TL;DR: HEROSQL：一种用于Text-to-SQL语义验证的分层SQL表示方法，通过逻辑计划和抽象语法树整合全局意图和局部细节，使用NMPNN进行信息传播，并采用AST驱动的子SQL增强策略生成高质量负样本，显著提升语义不一致检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL验证方法主要关注语法正确性，缺乏有效的语义验证（检测问题与SQL之间的错位）。语义验证面临两大挑战：1）同时捕捉全局用户意图和SQL结构细节；2）构建高质量的细粒度子SQL标注。

Method: 提出HEROSQL分层SQL表示方法：1）通过逻辑计划（LPs）捕获全局意图，通过抽象语法树（ASTs）捕获局部细节；2）使用嵌套消息传递神经网络（NMPNN）捕捉SQL固有关系信息并在LP和AST间聚合模式引导的语义；3）提出AST驱动的子SQL增强策略生成高质量负样本，优化细粒度语义不一致检测。

Result: 在Text-to-SQL验证基准测试（域内和域外设置）中，方法显著优于现有最先进方法：AUPRC平均提升9.40%，AUROC平均提升12.35%。能够有效检测细粒度语义错误，为大型语言模型提供更细粒度的反馈，增强数据查询平台的可靠性和可解释性。

Conclusion: HEROSQL通过分层SQL表示和NMPNN有效解决了Text-to-SQL语义验证的关键挑战，显著提升了语义不一致检测性能，为数据查询系统提供了更可靠的验证机制。

Abstract: Text-to-SQL translates natural language questions into SQL statements grounded in a target database schema. Ensuring the reliability and executability of such systems requires validating generated SQL, but most existing approaches focus only on syntactic correctness, with few addressing semantic validation (detecting misalignments between questions and SQL). As a consequence, effective semantic validation still faces two key challenges: capturing both global user intent and SQL structural details, and constructing high-quality fine-grained sub-SQL annotations. To tackle these, we introduce HEROSQL, a hierarchical SQL representation approach that integrates global intent (via Logical Plans, LPs) and local details (via Abstract Syntax Trees, ASTs). To enable better information propagation, we employ a Nested Message Passing Neural Network (NMPNN) to capture inherent relational information in SQL and aggregate schema-guided semantics across LPs and ASTs. Additionally, to generate high-quality negative samples, we propose an AST-driven sub-SQL augmentation strategy, supporting robust optimization of fine-grained semantic inconsistencies. Extensive experiments conducted on Text-to-SQL validation benchmarks (both in-domain and out-of-domain settings) demonstrate that our approach outperforms existing state-of-the-art methods, achieving an average 9.40% improvement of AUPRC and 12.35% of AUROC in identifying semantic inconsistencies. It excels at detecting fine-grained semantic errors, provides large language models with more granular feedback, and ultimately enhances the reliability and interpretability of data querying platforms.

</details>


### [113] [From Confounding to Learning: Dynamic Service Fee Pricing on Third-Party Platforms](https://arxiv.org/abs/2512.22749)
*Rui Ai,David Simchi-Levi,Feng Zhu*

Main category: cs.LG

TL;DR: 研究第三方平台面对战略代理时的定价行为，开发了在混淆下学习需求的最优后悔算法，揭示了供应侧噪声对需求可学习性的根本影响。


<details>
  <summary>Details</summary>
Motivation: 平台作为收入最大化者，只能观察到均衡价格和数量，这构成了一个在混淆下的一般需求学习问题。需要解决供应侧噪声如何影响需求学习以及如何设计有效的学习算法。

Method: 开发了具有最优后悔的算法，使用非独立同分布动作作为工具变量来学习需求，提出新颖的同胚构造来建立估计边界，无需假设星形性质，首次为深度神经网络学习需求提供效率保证。

Result: 获得了最优后悔界 $\Tilde{\cO}(\sqrt{T}\wedgeσ_S^{-2})$，揭示了供应侧噪声导致后悔的相变，证明了非i.i.d.动作可以作为学习需求的工具变量，并通过Zomato和Lyft的真实数据验证了方法的实用性。

Conclusion: 供应侧噪声从根本上影响需求的可学习性，非i.i.d.动作可以作为有效的工具变量，提出的同胚构造为深度神经网络学习需求提供了理论保证，方法在实际应用中具有良好效果。

Abstract: We study the pricing behavior of third-party platforms facing strategic agents. Assuming the platform is a revenue maximizer, it observes market features that generally affect demand. Since only the equilibrium price and quantity are observable, this presents a general demand learning problem under confounding. Mathematically, we develop an algorithm with optimal regret of $\Tilde{\cO}(\sqrt{T}\wedgeσ_S^{-2})$. Our results reveal that supply-side noise fundamentally affects the learnability of demand, leading to a phase transition in regret. Technically, we show that non-i.i.d. actions can serve as instrumental variables for learning demand. We also propose a novel homeomorphic construction that allows us to establish estimation bounds without assuming star-shapedness, providing the first efficiency guarantee for learning demand with deep neural networks. Finally, we demonstrate the practical applicability of our approach through simulations and real-world data from Zomato and Lyft.

</details>


### [114] [A Micro-Macro Machine Learning Framework for Predicting Childhood Obesity Risk Using NHANES and Environmental Determinants](https://arxiv.org/abs/2512.22758)
*Eswarasanthosh Kumar Mamillapalli,Nishtha Sharma*

Main category: cs.LG

TL;DR: 提出微-宏观机器学习框架，整合个体与宏观环境数据预测儿童肥胖，XGBoost表现最佳，构建环境脆弱指数，发现环境负担与肥胖风险分布的地理相似性。


<details>
  <summary>Details</summary>
Motivation: 传统流行病学研究通常独立分析个体、家庭和环境层面的风险因素，限制了理解结构性环境条件如何与个体特征相互作用影响健康结果。需要整合多尺度数据来识别环境驱动的肥胖风险差异。

Method: 1) 整合NHANES的个体层面人体测量和社会经济数据；2) 从USDA和EPA数据集中提取宏观层面的结构性环境特征（食品获取、空气质量、社会经济脆弱性）；3) 使用逻辑回归、随机森林、XGBoost和LightGBM四种机器学习模型预测肥胖；4) 构建基于USDA和EPA指标的环境脆弱指数(EnvScore)；5) 进行多层次比较分析。

Result: XGBoost模型表现最佳；构建了州级环境脆弱指数；多层次比较显示高环境负担的州与全国预测的微观层面肥胖风险分布存在强烈地理相似性。

Conclusion: 证明了整合多尺度数据集识别环境驱动肥胖风险差异的可行性；贡献了一个可扩展、数据驱动的多层次建模流程，适用于公共卫生信息学，具有扩展到因果建模、干预规划和实时分析的潜力。

Abstract: Childhood obesity remains a major public health challenge in the United States, strongly influenced by a combination of individual-level, household-level, and environmental-level risk factors. Traditional epidemiological studies typically analyze these levels independently, limiting insights into how structural environmental conditions interact with individual-level characteristics to influence health outcomes. In this study, we introduce a micro-macro machine learning framework that integrates (1) individual-level anthropometric and socioeconomic data from NHANES and (2) macro-level structural environment features, including food access, air quality, and socioeconomic vulnerability extracted from USDA and EPA datasets. Four machine learning models Logistic Regression, Random Forest, XGBoost, and LightGBM were trained to predict obesity using NHANES microdata. XGBoost achieved the strongest performance. A composite environmental vulnerability index (EnvScore) was constructed using normalized indicators from USDA and EPA at the state level. Multi-level comparison revealed strong geographic similarity between states with high environmental burden and the nationally predicted micro-level obesity risk distribution. This demonstrates the feasibility of integrating multi-scale datasets to identify environment-driven disparities in obesity risk. This work contributes a scalable, data-driven, multi-level modeling pipeline suitable for public health informatics, demonstrating strong potential for expansion into causal modeling, intervention planning, and real-time analytics.

</details>


### [115] [GRExplainer: A Universal Explanation Method for Temporal Graph Neural Networks](https://arxiv.org/abs/2512.22772)
*Xuyan Li,Jie Wang,Zheng Yan*

Main category: cs.LG

TL;DR: GRExplainer：首个通用、高效、用户友好的TGNN解释方法，通过节点序列统一特征表示，适用于快照式和事件式TGNN，利用BFS和时间信息提高效率，基于RNN实现自动化解释生成。


<details>
  <summary>Details</summary>
Motivation: 时态图神经网络（TGNN）在处理动态图方面表现出色，但缺乏透明度和可解释性限制了实际应用。现有TGNN解释方法存在三大问题：1）针对特定TGNN类型，缺乏通用性；2）计算成本高，不适用于大规模网络；3）忽视解释的结构连通性且需要先验知识，用户友好性差。

Method: 提出GRExplainer方法：1）提取节点序列作为统一特征表示，使其独立于特定输入格式，适用于快照式和事件式TGNN；2）利用广度优先搜索（BFS）和时间信息构建输入节点序列，减少冗余计算；3）设计基于循环神经网络（RNN）的生成模型，实现自动化连续解释生成。

Result: 在6个真实世界数据集和3个目标TGNN上的实验表明，GRExplainer在通用性、效率和用户友好性方面优于现有基线方法。

Conclusion: GRExplainer解决了TGNN解释性研究中的关键问题，提供了首个通用、高效、用户友好的解释方法，为TGNN的实际应用铺平了道路。

Abstract: Dynamic graphs are widely used to represent evolving real-world networks. Temporal Graph Neural Networks (TGNNs) have emerged as a powerful tool for processing such graphs, but the lack of transparency and explainability limits their practical adoption. Research on TGNN explainability is still in its early stages and faces several key issues: (i) Current methods are tailored to specific TGNN types, restricting generality. (ii) They suffer from high computational costs, making them unsuitable for large-scale networks. (iii) They often overlook the structural connectivity of explanations and require prior knowledge, reducing user-friendliness. To address these issues, we propose GRExplainer, the first universal, efficient, and user-friendly explanation method for TGNNs. GRExplainer extracts node sequences as a unified feature representation, making it independent of specific input formats and thus applicable to both snapshot-based and event-based TGNNs (the major types of TGNNs). By utilizing breadth-first search and temporal information to construct input node sequences, GRExplainer reduces redundant computation and improves efficiency. To enhance user-friendliness, we design a generative model based on Recurrent Neural Networks (RNNs), enabling automated and continuous explanation generation. Experiments on six real-world datasets with three target TGNNs show that GRExplainer outperforms existing baseline methods in generality, efficiency, and user-friendliness.

</details>


### [116] [Schrodinger AI: A Unified Spectral-Dynamical Framework for Classification, Reasoning, and Operator-Based Generalization](https://arxiv.org/abs/2512.22774)
*Truong Son Nguyen*

Main category: cs.LG

TL;DR: Schrödinger AI是一个受量子力学启发的统一机器学习框架，包含三个核心组件：波-能量求解器、动力学求解器和低秩算子演算，提供了一种基于物理学的替代传统交叉熵训练和Transformer注意力的方法。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机是创建一个受量子力学启发的机器学习框架，以替代传统的交叉熵训练和Transformer注意力机制。作者希望建立一个基于物理学的系统，能够提供更好的泛化能力、可解释的语义和涌现的拓扑结构，将学习过程视为发现和导航底层语义能量景观的过程。

Method: Schrödinger AI框架包含三个紧密耦合的组件：1) 时间无关波-能量求解器，将感知和分类视为在学习的哈密顿量下的谱分解；2) 时间相关动力学求解器，控制语义波函数随时间演化，支持上下文感知的决策修订、重新路由和变化环境下的推理；3) 低秩算子演算，通过学习量子类转换算子来学习符号变换（如模运算）。

Result: 实验结果表明：a) 涌现的语义流形能够反映人类概念类别关系而无需显式监督；b) 动态推理能够适应变化环境，包括具有实时势场扰动的迷宫导航；c) 在模运算任务上实现精确的算子泛化，系统学习群作用并在远超训练长度的序列上组合它们。

Conclusion: Schrödinger AI为机器学习提供了一个新的基础方向，将学习过程视为发现和导航底层语义能量景观的过程。该框架展示了基于物理学的机器学习方法的潜力，能够提供鲁棒的泛化、可解释的语义和涌现的拓扑结构。

Abstract: We introduce \textbf{Schrödinger AI}, a unified machine learning framework inspired by quantum mechanics. The system is defined by three tightly coupled components: (1) a {time-independent wave-energy solver} that treats perception and classification as spectral decomposition under a learned Hamiltonian; (2) a {time-dependent dynamical solver} governing the evolution of semantic wavefunctions over time, enabling context-aware decision revision, re-routing, and reasoning under environmental changes; and (3) a {low-rank operator calculus} that learns symbolic transformations such as modular arithmetic through learned quantum-like transition operators. Together, these components form a coherent physics-driven alternative to conventional cross-entropy training and transformer attention, providing robust generalization, interpretable semantics, and emergent topology.
  Empirically, Schrödinger AI demonstrates: (a) emergent semantic manifolds that reflect human-conceived class relations without explicit supervision; (b) dynamic reasoning that adapts to changing environments, including maze navigation with real-time potential-field perturbations; and (c) exact operator generalization on modular arithmetic tasks, where the system learns group actions and composes them across sequences far beyond training length. These results suggest a new foundational direction for machine learning, where learning is cast as discovering and navigating an underlying semantic energy landscape.

</details>


### [117] [Adapting, Fast and Slow: Transportable Circuits for Few-Shot Learning](https://arxiv.org/abs/2512.22777)
*Kasra Jalaldoust,Elias Bareinboim*

Main category: cs.LG

TL;DR: 提出Circuit-TR算法，利用因果图进行零样本组合泛化，通过模块化学习和电路传输实现跨域预测


<details>
  <summary>Details</summary>
Motivation: 跨域泛化需要约束未见目标域与源域关系的结构，现有方法缺乏利用因果结构进行零样本组合泛化的能力

Method: 基于因果可迁移理论，设计Circuit-TR算法：1) 从源数据学习模块集合（局部预测器）；2) 根据因果结构许可，传输/组合模块形成目标域预测电路；3) 提出无需显式因果结构、仅需有限目标数据的监督域适应方案

Result: 理论结果通过图形电路可迁移准则刻画了少样本可学习任务的类别，将少样本泛化能力与电路规模复杂度联系起来；控制模拟验证了理论结果

Conclusion: Circuit-TR通过因果结构指导的模块化学习和电路传输，实现了有效的零样本组合泛化，为跨域学习提供了理论框架和实用算法

Abstract: Generalization across the domains is not possible without asserting a structure that constrains the unseen target domain w.r.t. the source domain. Building on causal transportability theory, we design an algorithm for zero-shot compositional generalization which relies on access to qualitative domain knowledge in form of a causal graph for intra-domain structure and discrepancies oracle for inter-domain mechanism sharing. \textit{Circuit-TR} learns a collection of modules (i.e., local predictors) from the source data, and transport/compose them to obtain a circuit for prediction in the target domain if the causal structure licenses. Furthermore, circuit transportability enables us to design a supervised domain adaptation scheme that operates without access to an explicit causal structure, and instead uses limited target data. Our theoretical results characterize classes of few-shot learnable tasks in terms of graphical circuit transportability criteria, and connects few-shot generalizability with the established notion of circuit size complexity; controlled simulations corroborate our theoretical results.

</details>


### [118] [Discovering Transmission Dynamics of COVID-19 in China](https://arxiv.org/abs/2512.22787)
*Zhou Yang,Edward Dougherty,Chen Zhang,Zhenhe Pan,Fang Jin*

Main category: cs.LG

TL;DR: 该研究通过分析中国COVID-19传播数据，发现大城市的感染更多由社交活动驱动，79%有症状者在症状出现5天内住院，感染源随时间从湖北旅行相关转向社交活动相关。


<details>
  <summary>Details</summary>
Motivation: 通过对公共卫生干预措施（如大规模检测、隔离和接触者追踪）的回顾性分析，识别最有效的疫情防控机制，为未来公共卫生政策提供依据。

Method: 收集中国各地卫健委、中国CDC和官方社交媒体发布的病例报告，运用自然语言处理和人工整理构建传播/追踪链，结合武汉人口流动数据量化分析时空传播动态。

Result: 发现显著地区差异：大城市感染更多，主要由社交活动驱动；79%有症状者在症状出现5天内住院；感染源随时间变化：早期主要与湖北旅行相关，后期更多与社交活动相关。

Conclusion: 公共卫生干预措施的有效性存在地区差异，社交活动是后期传播的主要驱动因素，及时就医和接触者追踪对疫情防控至关重要。

Abstract: A comprehensive retrospective analysis of public health interventions, such as large scale testing, quarantining, and contact tracing, can help identify mechanisms most effective in mitigating COVID-19. We investigate China based SARS-CoV-2 transmission patterns (e.g., infection type and likely transmission source) using publicly released tracking data. We collect case reports from local health commissions, the Chinese CDC, and official local government social media, then apply NLP and manual curation to construct transmission/tracking chains. We further analyze tracking data together with Wuhan population mobility data to quantify and visualize temporal and spatial spread dynamics. Results indicate substantial regional differences, with larger cities showing more infections, likely driven by social activities. Most symptomatic individuals (79\%) were hospitalized within 5 days of symptom onset, and those with confirmed-case contact sought admission in under 5 days. Infection sources also shifted over time: early cases were largely linked to travel to (or contact with travelers from) Hubei Province, while later transmission was increasingly associated with social activities.

</details>


### [119] [ReDiF: Reinforced Distillation for Few Step Diffusion](https://arxiv.org/abs/2512.22802)
*Amirhossein Tighkhorshid,Zahra Dehghanian,Gholamali Aminian,Chengchun Shi,Hamid R. Rabiee*

Main category: cs.LG

TL;DR: 提出基于强化学习的扩散模型蒸馏框架，将蒸馏过程视为策略优化问题，通过奖励信号动态指导学生模型，实现更少推理步骤的高效生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型蒸馏方法依赖固定的重建或一致性损失，限制了学生模型的学习效率和性能。需要一种更灵活、动态的蒸馏方法来提高采样效率。

Method: 将扩散模型蒸馏视为强化学习策略优化问题，使用教师模型输出作为奖励信号，动态指导学生模型探索多种去噪路径，允许采取更长的优化步骤。

Result: 实验结果表明，该方法在显著减少推理步骤和计算资源的情况下，取得了优于现有蒸馏技术的性能表现。

Conclusion: 提出的强化学习蒸馏框架为扩散模型提供了一种通用的优化范式，具有模型无关性，适用于各种扩散模型类型，显著提高了采样效率。

Abstract: Distillation addresses the slow sampling problem in diffusion models by creating models with smaller size or fewer steps that approximate the behavior of high-step teachers. In this work, we propose a reinforcement learning based distillation framework for diffusion models. Instead of relying on fixed reconstruction or consistency losses, we treat the distillation process as a policy optimization problem, where the student is trained using a reward signal derived from alignment with the teacher's outputs. This RL driven approach dynamically guides the student to explore multiple denoising paths, allowing it to take longer, optimized steps toward high-probability regions of the data distribution, rather than relying on incremental refinements. Our framework utilizes the inherent ability of diffusion models to handle larger steps and effectively manage the generative process. Experimental results show that our method achieves superior performance with significantly fewer inference steps and computational resources compared to existing distillation techniques. Additionally, the framework is model agnostic, applicable to any type of diffusion models with suitable reward functions, providing a general optimization paradigm for efficient diffusion learning.

</details>


### [120] [MoR: Mixture Of Representations For Mixed-Precision Training](https://arxiv.org/abs/2512.22804)
*Bor-Yiing Su,Peter Dykas,Mike Chrzanowski,Jatin Chhugani*

Main category: cs.LG

TL;DR: MoR是一个新颖的混合精度训练框架，通过动态分析张量数值特性，在FP8和BF16表示之间进行动态选择，实现了98.38%的张量量化为FP8格式，同时保持模型质量。


<details>
  <summary>Details</summary>
Motivation: 混合精度训练是扩展深度学习模型的关键技术，但成功实施需要找到合适的训练方法组合。现有方法需要精细的分区策略，而本文旨在开发一种更通用、动态的量化框架。

Method: 提出Mixture-of-Representations (MoR)框架，在张量和子张量级别进行动态量化。通过分析张量的数值特性，动态选择FP8或BF16表示。设计了具体算法支持不同粒度的量化分区策略。

Result: 初步结果显示，该方法能实现98.38%的张量量化为FP8格式，达到最先进的结果。在保持模型质量的同时，实现了与现有方法相当的FP8精度，无需精细分区策略。

Conclusion: MoR框架展示了动态、属性感知量化的潜力，能够提高低精度训练的鲁棒性。该方法可以单独使用或与其他训练方法结合，为使用更低精度格式（如NVFP4）提供了可能性。

Abstract: Mixed-precision training is a crucial technique for scaling deep learning models, but successful mixedprecision training requires identifying and applying the right combination of training methods. This paper presents our preliminary study on Mixture-of-Representations (MoR), a novel, per-tensor and sub-tensor level quantization framework that dynamically analyzes a tensor's numerical properties to select between a variety of different representations. Based on the framework, we have proposed and experimented concrete algorithms that choose dynamically between FP8 and BF16 representations for both per-tensor and sub-tensor level granularities. Our universal approach is designed to preserve model quality across various quantization partition strategies and datasets. Our initial findings show that this approach can achieve state-of-the-art results with 98.38% of tensors quantized to the FP8 format. This work highlights the potential of dynamic, property-aware quantization while preserving model quality. We believe this approach can generally improve the robustness of low precision training, as demonstrated by achieving FP8 accuracies that are on par with existing approaches without the need for fine-grain partitioning, or can be used in combination with other training methods to improve the leverage of even lower precision number formats such as NVFP4.

</details>


### [121] [Long-Range Distillation: Distilling 10,000 Years of Simulated Climate into Long Timestep AI Weather Models](https://arxiv.org/abs/2512.22814)
*Scott A. Martin,Noah Brenowitz,Dale Durran,Michael Pritchard*

Main category: cs.LG

TL;DR: 提出长程蒸馏方法，用自回归教师模型生成大量合成数据训练单步长程预测学生模型，显著提升长期天气预报技能


<details>
  <summary>Details</summary>
Motivation: 传统AI天气模型的自回归方法在长期预测中误差累积且不稳定，而单步长程模型需要大量训练数据，但现有再分析数据集样本有限，无法捕捉气候变率的慢模态

Method: 提出长程蒸馏方法：使用短步长自回归教师模型（DLESyM）生成超过10,000年的合成气候数据，训练单步长程概率学生模型直接进行长期预测

Result: 在完美模型实验中，蒸馏模型优于气候学基准，接近自回归教师技能，用单步替代数百个自回归步；在真实世界中，经过ERA5微调后达到与ECMWF集合预报相当的S2S技能；模型技能随合成训练数据增加而提升

Conclusion: 首次证明AI生成的合成训练数据可用于扩展长期预报技能，长程蒸馏方法为解决长期天气预报中的数据稀缺问题提供了有效途径

Abstract: Accurate long-range weather forecasting remains a major challenge for AI models, both because errors accumulate over autoregressive rollouts and because reanalysis datasets used for training offer a limited sample of the slow modes of climate variability underpinning predictability. Most AI weather models are autoregressive, producing short lead forecasts that must be repeatedly applied to reach subseasonal-to-seasonal (S2S) or seasonal lead times, often resulting in instability and calibration issues. Long-timestep probabilistic models that generate long-range forecasts in a single step offer an attractive alternative, but training on the 40-year reanalysis record leads to overfitting, suggesting orders of magnitude more training data are required. We introduce long-range distillation, a method that trains a long-timestep probabilistic "student" model to forecast directly at long-range using a huge synthetic training dataset generated by a short-timestep autoregressive "teacher" model. Using the Deep Learning Earth System Model (DLESyM) as the teacher, we generate over 10,000 years of simulated climate to train distilled student models for forecasting across a range of timescales. In perfect-model experiments, the distilled models outperform climatology and approach the skill of their autoregressive teacher while replacing hundreds of autoregressive steps with a single timestep. In the real world, they achieve S2S forecast skill comparable to the ECMWF ensemble forecast after ERA5 fine-tuning. The skill of our distilled models scales with increasing synthetic training data, even when that data is orders of magnitude larger than ERA5. This represents the first demonstration that AI-generated synthetic training data can be used to scale long-range forecast skill.

</details>


### [122] [TEACH: Temporal Variance-Driven Curriculum for Reinforcement Learning](https://arxiv.org/abs/2512.22824)
*Gaurav Chaudhary,Laxmidhar Behera*

Main category: cs.LG

TL;DR: 提出一种基于时变方差驱动课程的学生-教师学习范式，通过动态选择Q值方差最大的目标来加速目标条件强化学习


<details>
  <summary>Details</summary>
Motivation: 传统多目标强化学习中均匀目标选择导致样本效率低下，受生物系统自适应结构化学习过程启发，需要更高效的目标选择策略

Method: 学生-教师学习范式，教师模块根据策略置信度（Q函数）的时变方差动态优先选择高不确定性目标，提供自适应学习信号

Result: 在11个机器人操作和迷宫导航任务中，相比最先进的课程学习和目标选择方法，取得了持续显著的改进

Conclusion: 提出的时变方差驱动课程方法能有效加速目标条件强化学习，具有算法无关性，可无缝集成到现有RL框架中

Abstract: Reinforcement Learning (RL) has achieved significant success in solving single-goal tasks. However, uniform goal selection often results in sample inefficiency in multi-goal settings where agents must learn a universal goal-conditioned policy. Inspired by the adaptive and structured learning processes observed in biological systems, we propose a novel Student-Teacher learning paradigm with a Temporal Variance-Driven Curriculum to accelerate Goal-Conditioned RL. In this framework, the teacher module dynamically prioritizes goals with the highest temporal variance in the policy's confidence score, parameterized by the state-action value (Q) function. The teacher provides an adaptive and focused learning signal by targeting these high-uncertainty goals, fostering continual and efficient progress. We establish a theoretical connection between the temporal variance of Q-values and the evolution of the policy, providing insights into the method's underlying principles. Our approach is algorithm-agnostic and integrates seamlessly with existing RL frameworks. We demonstrate this through evaluation across 11 diverse robotic manipulation and maze navigation tasks. The results show consistent and notable improvements over state-of-the-art curriculum learning and goal-selection methods.

</details>


### [123] [Federated Multi-Task Clustering](https://arxiv.org/abs/2512.22897)
*S. Dai,G. Sun,F. Li,X. Tang,Q. Wang,Y. Cong*

Main category: cs.LG

TL;DR: 提出联邦多任务聚类框架FMTC，解决异构客户端环境下的隐私保护聚类问题，通过客户端个性化聚类模块和服务端张量相关模块实现高效协同学习。


<details>
  <summary>Details</summary>
Motivation: 传统谱聚类算法主要针对集中式设置，无法适应现代去中心化环境。现有联邦学习方法依赖不可靠的伪标签导致泛化性能差，且未能捕捉异构客户端间的潜在相关性。

Method: FMTC框架包含两个主要组件：1) 客户端个性化聚类模块，学习参数化映射模型支持鲁棒的样本外推理，避免使用不可靠伪标签；2) 服务端张量相关模块，将所有客户端模型组织成统一张量并应用低秩正则化发现其公共子空间。采用基于ADMM的高效隐私保护分布式算法进行优化。

Result: 在多个真实世界数据集上的实验表明，FMTC框架显著优于各种基线和最先进的联邦聚类算法。

Conclusion: FMTC成功解决了异构客户端环境下的联邦聚类问题，通过个性化聚类模型和共享知识捕获的协同设计，实现了隐私保护下的高效聚类性能。

Abstract: Spectral clustering has emerged as one of the most effective clustering algorithms due to its superior performance. However, most existing models are designed for centralized settings, rendering them inapplicable in modern decentralized environments. Moreover, current federated learning approaches often suffer from poor generalization performance due to reliance on unreliable pseudo-labels, and fail to capture the latent correlations amongst heterogeneous clients. To tackle these limitations, this paper proposes a novel framework named Federated Multi-Task Clustering (i.e.,FMTC), which intends to learn personalized clustering models for heterogeneous clients while collaboratively leveraging their shared underlying structure in a privacy-preserving manner. More specifically, the FMTC framework is composed of two main components: client-side personalized clustering module, which learns a parameterized mapping model to support robust out-of-sample inference, bypassing the need for unreliable pseudo-labels; and server-side tensorial correlation module, which explicitly captures the shared knowledge across all clients. This is achieved by organizing all client models into a unified tensor and applying a low-rank regularization to discover their common subspace. To solve this joint optimization problem, we derive an efficient, privacy-preserving distributed algorithm based on the Alternating Direction Method of Multipliers, which decomposes the global problem into parallel local updates on clients and an aggregation step on the server. To the end, several extensive experiments on multiple real-world datasets demonstrate that our proposed FMTC framework significantly outperforms various baseline and state-of-the-art federated clustering algorithms.

</details>


### [124] [Debugging Tabular Log as Dynamic Graphs](https://arxiv.org/abs/2512.22903)
*Chumeng Liang,Zhanyang Jin,Zahaib Akhtar,Mona Pereira,Haofei Yu,Jiaxuan You*

Main category: cs.LG

TL;DR: 提出GraphLogDebugger框架，使用动态图建模表格日志，通过简单的动态GNN在日志调试任务上超越LLMs


<details>
  <summary>Details</summary>
Motivation: 当前处理表格日志的方法过度依赖LLMs等重型模型，导致灵活性和可扩展性受限，需要更高效的解决方案

Method: 构建异构节点表示对象和事件，连接节点间边，将表格日志恢复为演化动态图，使用简单的动态GNN进行调试

Result: 在计算机系统和学术论文的真实日志数据集上验证，动态GNN在调试表格日志任务上表现优于LLMs

Conclusion: GraphLogDebugger框架通过动态图建模有效解决了表格日志调试问题，展示了简单动态GNN相比LLMs的优势

Abstract: Tabular log abstracts objects and events in the real-world system and reports their updates to reflect the change of the system, where one can detect real-world inconsistencies efficiently by debugging corresponding log entries. However, recent advances in processing text-enriched tabular log data overly depend on large language models (LLMs) and other heavy-load models, thus suffering from limited flexibility and scalability. This paper proposes a new framework, GraphLogDebugger, to debug tabular log based on dynamic graphs. By constructing heterogeneous nodes for objects and events and connecting node-wise edges, the framework recovers the system behind the tabular log as an evolving dynamic graph. With the help of our dynamic graph modeling, a simple dynamic Graph Neural Network (GNN) is representative enough to outperform LLMs in debugging tabular log, which is validated by experimental results on real-world log datasets of computer systems and academic papers.

</details>


### [125] [MetaCD: A Meta Learning Framework for Cognitive Diagnosis based on Continual Learning](https://arxiv.org/abs/2512.22904)
*Jin Wu,Chanjin Zheng*

Main category: cs.LG

TL;DR: 提出基于持续学习的元学习认知诊断框架MetaCD，通过元学习优化初始化状态缓解长尾分布问题，利用参数保护机制适应动态变化，在五个真实数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习认知诊断方法受限于数据的长尾分布和动态变化问题，需要一种能够同时处理数据不平衡和适应新技能/任务的解决方案。

Method: 提出MetaCD框架：1）使用元学习学习最优初始化状态，使模型能用少量数据在新任务上取得良好性能；2）采用参数保护机制的持续学习方法，使模型能适应新技能或新任务。

Result: 在五个真实世界数据集上的综合实验表明，MetaCD在准确性和泛化能力方面均优于其他基线方法。

Conclusion: MetaCD框架不仅能提高模型在单个任务上的可塑性，还能确保模型在序列任务上的稳定性和泛化能力，有效解决了认知诊断中的长尾分布和动态变化问题。

Abstract: Cognitive diagnosis is an essential research topic in intelligent education, aimed at assessing the level of mastery of different skills by students. So far, many research works have used deep learning models to explore the complex interactions between students, questions, and skills. However, the performance of existing method is frequently limited by the long-tailed distribution and dynamic changes in the data. To address these challenges, we propose a meta-learning framework for cognitive diagnosis based on continual learning (MetaCD). This framework can alleviate the long-tailed problem by utilizing meta-learning to learn the optimal initialization state, enabling the model to achieve good accuracy on new tasks with only a small amount of data. In addition, we utilize a continual learning method named parameter protection mechanism to give MetaCD the ability to adapt to new skills or new tasks, in order to adapt to dynamic changes in data. MetaCD can not only improve the plasticity of our model on a single task, but also ensure the stability and generalization of the model on sequential tasks. Comprehensive experiments on five real-world datasets show that MetaCD outperforms other baselines in both accuracy and generalization.

</details>


### [126] [Sat-EnQ: Satisficing Ensembles of Weak Q-Learners for Reliable and Compute-Efficient Reinforcement Learning](https://arxiv.org/abs/2512.22910)
*Ünver Çiftçi*

Main category: cs.LG

TL;DR: Sat-EnQ：一个两阶段深度Q学习框架，通过先满足"足够好"再优化的策略，解决了早期训练不稳定问题，显著降低方差并消除灾难性失败。


<details>
  <summary>Details</summary>
Motivation: 深度Q学习算法在早期训练时不稳定，最大化操作会放大估计误差，导致灾难性过估计和训练失败。

Method: 两阶段框架：第一阶段训练轻量级Q网络集成，使用满足性目标限制早期价值增长；第二阶段将集成蒸馏到更大网络中，用标准Double DQN微调。

Result: 实现3.8倍方差降低，消除灾难性失败（0% vs DQN的50%），在环境噪声下保持79%性能，计算需求比自举集成少2.5倍。

Conclusion: 通过先满足后优化的原则性路径，为强化学习提供了更稳健的训练方法，证明了满足性在早期训练中的价值。

Abstract: Deep Q-learning algorithms remain notoriously unstable, especially during early training when the maximization operator amplifies estimation errors. Inspired by bounded rationality theory and developmental learning, we introduce Sat-EnQ, a two-phase framework that first learns to be ``good enough'' before optimizing aggressively. In Phase 1, we train an ensemble of lightweight Q-networks under a satisficing objective that limits early value growth using a dynamic baseline, producing diverse, low-variance estimates while avoiding catastrophic overestimation. In Phase 2, the ensemble is distilled into a larger network and fine-tuned with standard Double DQN. We prove theoretically that satisficing induces bounded updates and cannot increase target variance, with a corollary quantifying conditions for substantial reduction. Empirically, Sat-EnQ achieves 3.8x variance reduction, eliminates catastrophic failures (0% vs 50% for DQN), maintains 79% performance under environmental noise}, and requires 2.5x less compute than bootstrapped ensembles. Our results highlight a principled path toward robust reinforcement learning by embracing satisficing before optimization.

</details>


### [127] [Multiple Token Divergence: Measuring and Steering In-Context Computation Density](https://arxiv.org/abs/2512.22944)
*Vincent Herrmann,Eric Alcaide,Michael Wand,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: 提出MTD（多令牌散度）作为衡量语言模型上下文计算努力的新指标，以及基于此的Divergence Steering解码方法，用于分析和控制文本生成的计算特性。


<details>
  <summary>Details</summary>
Motivation: 现有衡量语言模型计算努力的方法（如下一令牌损失）无法捕捉推理复杂度，而基于潜在状态可压缩性的方法具有侵入性和不稳定性，需要更简单有效的测量工具。

Method: 提出MTD指标，定义为模型完整输出分布与浅层辅助预测头输出分布之间的KL散度。无需额外训练，可直接从具有多个预测头的预训练模型计算。基于MTD提出Divergence Steering解码方法，控制生成文本的计算特性。

Result: MTD比现有方法更有效地区分复杂任务与简单任务；在数学推理基准测试中，MTD与问题难度正相关；较低的MTD与更准确的推理相关；MTD为分析和引导语言模型计算动态提供了实用轻量级工具。

Conclusion: MTD是一种简单有效的语言模型计算努力测量方法，无需额外训练，可直接应用于预训练模型。基于MTD的Divergence Steering方法能够控制生成文本的计算特性，为模型分析和应用提供了新工具。

Abstract: Measuring the in-context computational effort of language models is a key challenge, as metrics like next-token loss fail to capture reasoning complexity. Prior methods based on latent state compressibility can be invasive and unstable. We propose Multiple Token Divergence (MTD), a simple measure of computational effort defined as the KL divergence between a model's full output distribution and that of a shallow, auxiliary prediction head. MTD can be computed directly from pre-trained models with multiple prediction heads, requiring no additional training. Building on this, we introduce Divergence Steering, a novel decoding method to control the computational character of generated text. We empirically show that MTD is more effective than prior methods at distinguishing complex tasks from simple ones. On mathematical reasoning benchmarks, MTD correlates positively with problem difficulty. Lower MTD is associated with more accurate reasoning. MTD provides a practical, lightweight tool for analyzing and steering the computational dynamics of language models.

</details>


### [128] [APO: Alpha-Divergence Preference Optimization](https://arxiv.org/abs/2512.22953)
*Wang Zixian*

Main category: cs.LG

TL;DR: APO提出了一种基于alpha散度的锚定偏好优化框架，可在前向KL和后向KL之间连续插值，通过奖励和置信度引导的alpha调度实现从覆盖到利用的平稳过渡。


<details>
  <summary>Details</summary>
Motivation: 现代对齐实践存在两种主导的散度机制：前向KL（监督微调）提供稳定的模式覆盖但可能未充分利用高奖励模式，后向KL（PPO式RLHF）支持模式寻求但可能导致模式崩溃。现有锚定方法通常只使用单一散度，缺乏灵活调节机制。

Method: 提出Alpha-Divergence Preference Optimization (APO)，使用Csiszar alpha散度在锚定几何中连续插值前向和后向KL行为。推导了参数化alpha的统一梯度动态，分析梯度方差特性，并提出奖励和置信度引导的alpha调度策略，仅在策略改进且置信度校准良好时从覆盖过渡到利用。

Result: 在Qwen3-1.7B模型和math-level3任务上的实验表明，APO在保持训练稳定性的同时，达到了与GRPO和GSPO基线竞争的性能水平。

Conclusion: APO框架通过alpha散度提供了前向和后向KL之间的灵活插值，结合奖励和置信度引导的调度机制，实现了从模式覆盖到模式寻求的平稳过渡，同时保持了训练稳定性。

Abstract: Two divergence regimes dominate modern alignment practice. Supervised fine-tuning and many distillation-style objectives implicitly minimize the forward KL divergence KL(q || pi_theta), yielding stable mode-covering updates but often under-exploiting high-reward modes. In contrast, PPO-style online reinforcement learning from human feedback behaves closer to reverse KL divergence KL(pi_theta || q), enabling mode-seeking improvements but risking mode collapse. Recent anchored methods, such as ADPO, show that performing the projection in anchored coordinates can substantially improve stability, yet they typically commit to a single divergence. We introduce Alpha-Divergence Preference Optimization (APO), an anchored framework that uses Csiszar alpha-divergence to continuously interpolate between forward and reverse KL behavior within the same anchored geometry. We derive unified gradient dynamics parameterized by alpha, analyze gradient variance properties, and propose a practical reward-and-confidence-guarded alpha schedule that transitions from coverage to exploitation only when the policy is both improving and confidently calibrated. Experiments on Qwen3-1.7B with math-level3 demonstrate that APO achieves competitive performance with GRPO and GSPO baselines while maintaining training stability.

</details>


### [129] [FLOW: A Feedback-Driven Synthetic Longitudinal Dataset of Work and Wellbeing](https://arxiv.org/abs/2512.22956)
*Wafaa El Husseini*

Main category: cs.LG

TL;DR: FLOW是一个合成的纵向数据集，模拟工作负荷、生活方式行为和幸福感之间的日常互动，用于支持可重复研究、方法基准测试和教育。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、伦理和后勤限制，获取关于工作生活平衡和幸福感的纵向个体级数据有限，这对压力建模、行为分析和机器学习等领域的可重复研究、方法基准测试和教育构成了挑战。

Method: 使用基于规则的反馈驱动模拟生成合成纵向数据集，模拟1000名个体两年期间的日常数据，包括压力、睡眠、情绪、身体活动和体重等变量。

Result: 创建了FLOW数据集及其可配置的数据生成工具，作为一个公开可用的资源，支持在真实世界数据不可访问时的探索性分析、方法开发和基准测试。

Conclusion: FLOW作为一个受控的实验环境而非真实人群的代理，为无法获取真实数据的研究提供了可重复实验的平台，支持可调整的行为和上下文假设下的方法开发和基准测试。

Abstract: Access to longitudinal, individual-level data on work-life balance and wellbeing is limited by privacy, ethical, and logistical constraints. This poses challenges for reproducible research, methodological benchmarking, and education in domains such as stress modeling, behavioral analysis, and machine learning.
  We introduce FLOW, a synthetic longitudinal dataset designed to model daily interactions between workload, lifestyle behaviors, and wellbeing. FLOW is generated using a rule-based, feedback-driven simulation that produces coherent temporal dynamics across variables such as stress, sleep, mood, physical activity, and body weight. The dataset simulates 1{,}000 individuals over a two-year period with daily resolution and is released as a publicly available resource.
  In addition to the static dataset, we describe a configurable data generation tool that enables reproducible experimentation under adjustable behavioral and contextual assumptions. FLOW is intended as a controlled experimental environment rather than a proxy for observed human populations, supporting exploratory analysis, methodological development, and benchmarking where real-world data are inaccessible.

</details>


### [130] [A Context-Aware Temporal Modeling through Unified Multi-Scale Temporal Encoding and Hierarchical Sequence Learning for Single-Channel EEG Sleep Staging](https://arxiv.org/abs/2512.22976)
*Amirali Vakili,Salar Jahanshiri,Armin Salimi-Badr*

Main category: cs.LG

TL;DR: 提出一个上下文感知且可解释的单通道EEG睡眠分期框架，特别针对N1期检测改进，在SleepEDF数据集上达到89.72%总体准确率和61.7%的N1期F1分数。


<details>
  <summary>Details</summary>
Motivation: 睡眠障碍全球普遍，自动睡眠分期对医疗至关重要。现有单通道EEG方法面临类别不平衡、感受野有限、可解释性不足等问题，特别是N1期检测困难。

Method: 结合紧凑多尺度特征提取与时序建模，捕获局部和长程依赖；使用类别加权损失和数据增强处理不平衡；将EEG信号分段为子时段块，通过平均softmax概率获得最终预测。

Result: 在SleepEDF数据集上达到89.72%总体准确率和85.46%宏平均F1分数，N1期F1分数达61.7%，显著优于先前方法。

Conclusion: 该框架有效提升睡眠分期性能，同时保持可解释性和临床实用性，特别在N1期检测上有显著改进。

Abstract: Automatic sleep staging is a critical task in healthcare due to the global prevalence of sleep disorders. This study focuses on single-channel electroencephalography (EEG), a practical and widely available signal for automatic sleep staging. Existing approaches face challenges such as class imbalance, limited receptive-field modeling, and insufficient interpretability. This work proposes a context-aware and interpretable framework for single-channel EEG sleep staging, with particular emphasis on improving detection of the N1 stage. Many prior models operate as black boxes with stacked layers, lacking clearly defined and interpretable feature extraction roles.The proposed model combines compact multi-scale feature extraction with temporal modeling to capture both local and long-range dependencies. To address data imbalance, especially in the N1 stage, classweighted loss functions and data augmentation are applied. EEG signals are segmented into sub-epoch chunks, and final predictions are obtained by averaging softmax probabilities across chunks, enhancing contextual representation and robustness.The proposed framework achieves an overall accuracy of 89.72% and a macro-average F1-score of 85.46%. Notably, it attains an F1- score of 61.7% for the challenging N1 stage, demonstrating a substantial improvement over previous methods on the SleepEDF datasets. These results indicate that the proposed approach effectively improves sleep staging performance while maintaining interpretability and suitability for real-world clinical applications.

</details>


### [131] [Fusion or Confusion? Multimodal Complexity Is Not All You Need](https://arxiv.org/abs/2512.22991)
*Tillmann Rheude,Roland Eils,Benjamin Wild*

Main category: cs.LG

TL;DR: 复杂多模态学习方法并不比简单的后期融合Transformer基准更有效，在标准化实验条件下，简单方法表现相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 挑战当前多模态学习领域普遍假设：复杂架构必然带来性能提升。通过大规模实证研究检验这一假设，揭示方法学严谨性的重要性。

Method: 提出SimBaMM（简单多模态学习基准），一个简单的后期融合Transformer架构。在标准化条件下重新实现19种高影响力方法，在9个数据集（最多23种模态）上评估，测试其泛化能力和缺失模态情况下的表现。

Result: 复杂架构在标准化实验条件下并不比SimBaMM更可靠地表现出色，统计分析显示复杂方法与简单基准表现相当，且经常无法可靠超越调优良好的单模态基线，特别是在小数据场景下。

Conclusion: 多模态学习研究应从追求架构新颖性转向方法学严谨性，提供可靠性检查清单以促进可比较、稳健和可信的未来评估。

Abstract: Deep learning architectures for multimodal learning have increased in complexity, driven by the assumption that multimodal-specific methods improve performance. We challenge this assumption through a large-scale empirical study reimplementing 19 high-impact methods under standardized conditions, evaluating them across nine diverse datasets with up to 23 modalities, and testing their generalizability to new tasks beyond their original scope, including settings with missing modalities. We propose a Simple Baseline for Multimodal Learning (SimBaMM), a straightforward late-fusion Transformer architecture, and demonstrate that under standardized experimental conditions with rigorous hyperparameter tuning of all methods, more complex architectures do not reliably outperform SimBaMM. Statistical analysis indicates that more complex methods perform comparably to SimBaMM and frequently do not reliably outperform well-tuned unimodal baselines, especially in the small-data regime considered in many original studies. To support our findings, we include a case study of a recent multimodal learning method highlighting the methodological shortcomings in the literature. In addition, we provide a pragmatic reliability checklist to promote comparable, robust, and trustworthy future evaluations. In summary, we argue for a shift in focus: away from the pursuit of architectural novelty and toward methodological rigor.

</details>


### [132] [Merge before Forget: A Single LoRA Continual Learning via Continual Merging](https://arxiv.org/abs/2512.23017)
*Fuli Qiao,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: 提出一种新的持续学习方法，通过正交初始化和顺序合并LoRA更新到单一统一LoRA中，解决现有LoRA持续学习方法的内存增长和任务干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA持续学习方法存在两个主要问题：1) 随着任务增加，计算内存需求不断增长且存储空间有限；2) 缺乏有效的LoRA合并机制导致潜在的任务干扰。

Method: 提出正交初始化并顺序合并LoRA更新的方法：1) 从已学习的LoRA中提取正交基来初始化新任务学习；2) 利用LoRA组件的内在不对称性，使用时感知缩放机制在持续合并过程中平衡新旧知识。

Result: 方法保持相对于任务数量的恒定内存复杂度，通过正交基初始化最小化新旧任务间的干扰，通过自适应缩放提高非对称LoRA合并的性能。在多种Llama模型上的实验证明了方法的有效性和效率。

Conclusion: 该方法解决了LoRA持续学习中的内存增长和任务干扰问题，通过正交初始化和自适应合并机制实现了高效且有效的持续学习。

Abstract: Parameter-efficient continual learning has emerged as a promising approach for large language models (LLMs) to mitigate catastrophic forgetting while enabling adaptation to new tasks. Current Low-Rank Adaptation (LoRA) continual learning techniques often retain and freeze previously learned LoRAs or generate data representations to overcome forgetting, typically utilizing these to support new LoRAs learn new tasks. However, these methods not only ignore growing computational memory with tasks and limited storage space but also suffer from potential task interference due to the lack of effective LoRA merging mechanisms. In this paper, we propose a novel continual learning method that orthogonally initializes and sequentially merges LoRAs updates into a single unified LoRA. Our method leverages orthogonal basis extraction from previously learned LoRA to initialize the learning of new tasks, further exploits the intrinsic asymmetry property of LoRA components by using a time-aware scaling mechanism to balance new and old knowledge during continual merging. Our approach maintains constant memory complexity with respect to the number of tasks, minimizes interference between past and new tasks via orthogonal basis initialization, and improves performance over asymmetric LoRA merging via adaptive scaling. We provide theoretical analysis to justify our design and conduct extensive experiments across diverse continual learning benchmarks using various Llama models, demonstrating the effectiveness and efficiency of our method.

</details>


### [133] [Mechanistic Analysis of Circuit Preservation in Federated Learning](https://arxiv.org/abs/2512.23043)
*Muhammad Haseeb,Salaar Masood,Muhammad Abdullah Sohail*

Main category: cs.LG

TL;DR: 该论文通过机制可解释性分析联邦学习在非IID数据下的性能下降，发现客户端更新冲突导致电路崩溃，即负责特定类别预测的稀疏子网络发生破坏性干扰。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非IID数据条件下性能显著下降，虽然这种准确率损失已被广泛记录，但其内部机制原因仍是一个黑箱。本文旨在通过机制可解释性来诊断这种失败模式。

Method: 使用机制可解释性分析FedAvg算法，训练固有可解释的权重稀疏神经网络，在联邦学习框架中识别和跟踪这些电路，使用IoU指标量化电路保存情况。

Result: 提供了第一个机制证据，表明非IID数据分布导致结构上不同的本地电路发散，进而在全局模型中退化。非IID条件下电路保存率显著降低。

Conclusion: 将联邦学习中的统计漂移问题重新定义为机制保存的具体可观察失败，为更有针对性的解决方案铺平了道路。

Abstract: Federated Learning (FL) enables collaborative training of models on decentralized data, but its performance degrades significantly under Non-IID (non-independent and identically distributed) data conditions. While this accuracy loss is well-documented, the internal mechanistic causes remain a black box. This paper investigates the canonical FedAvg algorithm through the lens of Mechanistic Interpretability (MI) to diagnose this failure mode. We hypothesize that the aggregation of conflicting client updates leads to circuit collapse, the destructive interference of functional, sparse sub-networks responsible for specific class predictions. By training inherently interpretable, weight-sparse neural networks within an FL framework, we identify and track these circuits across clients and communication rounds. Using Intersection-over-Union (IoU) to quantify circuit preservation, we provide the first mechanistic evidence that Non-IID data distributions cause structurally distinct local circuits to diverge, leading to their degradation in the global model. Our findings reframe the problem of statistical drift in FL as a concrete, observable failure of mechanistic preservation, paving the way for more targeted solutions.

</details>


### [134] [PI-MFM: Physics-informed multimodal foundation model for solving partial differential equations](https://arxiv.org/abs/2512.23056)
*Min Zhu,Jingmin Sun,Zecheng Zhang,Hayden Schaeffer,Lu Lu*

Main category: cs.LG

TL;DR: 提出PI-MFM框架，将物理方程约束融入多模态基础模型预训练和微调，提升PDE求解的数据效率和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有多算子学习方法数据需求大且训练时忽略物理约束，需要开发能直接融入物理方程的数据高效PDE求解框架

Method: PI-MFM框架以符号PDE表示作为输入，通过向量化导数计算自动组装PDE残差损失，在预训练和适应阶段统一实施物理约束

Result: 在13个参数化一维时变PDE基准上，PI-MFM优于纯数据驱动方法，尤其在稀疏数据、部分观测域等场景；物理损失提升抗噪性，零样本微调可将测试误差降至约1%

Conclusion: PI-MFM为数据高效、可迁移的PDE求解器提供了实用且可扩展的路径，通过物理约束增强模型性能

Abstract: Partial differential equations (PDEs) govern a wide range of physical systems, and recent multimodal foundation models have shown promise for learning PDE solution operators across diverse equation families. However, existing multi-operator learning approaches are data-hungry and neglect physics during training. Here, we propose a physics-informed multimodal foundation model (PI-MFM) framework that directly enforces governing equations during pretraining and adaptation. PI-MFM takes symbolic representations of PDEs as the input, and automatically assembles PDE residual losses from the input expression via a vectorized derivative computation. These designs enable any PDE-encoding multimodal foundation model to be trained or adapted with unified physics-informed objectives across equation families. On a benchmark of 13 parametric one-dimensional time-dependent PDE families, PI-MFM consistently outperforms purely data-driven counterparts, especially with sparse labeled spatiotemporal points, partially observed time domains, or few labeled function pairs. Physics losses further improve robustness against noise, and simple strategies such as resampling collocation points substantially improve accuracy. We also analyze the accuracy, precision, and computational cost of automatic differentiation and finite differences for derivative computation within PI-MFM. Finally, we demonstrate zero-shot physics-informed fine-tuning to unseen PDE families: starting from a physics-informed pretrained model, adapting using only PDE residuals and initial/boundary conditions, without any labeled solution data, rapidly reduces test errors to around 1% and clearly outperforms physics-only training from scratch. These results show that PI-MFM provides a practical and scalable path toward data-efficient, transferable PDE solvers.

</details>


### [135] [Breaking the Memory Wall: Exact Analytical Differentiation via Tiled Operator-Space Evolution](https://arxiv.org/abs/2512.23068)
*Shuhuan Wang,Yuzhen Xie,Jiayi Li,Yinliang Diao*

Main category: cs.LG

TL;DR: PGF框架通过状态空间流形直接计算精确解析导数，实现O(1)内存复杂度，解决SSM梯度分析中的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 选择性状态空间模型(SSMs)虽然实现线性时间推理，但其基于梯度的敏感性分析在反向传播期间受限于O(L)内存扩展，这阻碍了在消费级硬件上进行基因组规模建模(L > 10^5)。

Method: 引入相位梯度流(PGF)框架，通过在状态空间流形中直接操作计算精确解析导数，避免构建中间计算图。通过将SSM动态重构为平铺算子空间演化(TOSE)，实现相对于序列长度的O(1)内存复杂度。

Result: 相比标准Autograd，PGF实现了94%的峰值VRAM减少和23倍的吞吐量提升。在128,000步序列的脉冲响应基准测试中，PGF能够在传统Autograd遇到内存不足故障的规模上稳定运行，保持接近机器精度。

Conclusion: PGF使单GPU上的染色体规模敏感性分析成为可能，弥合了理论无限上下文模型与实际硬件限制之间的差距，解决了SSM梯度分析中的内存瓶颈问题。

Abstract: Selective State Space Models (SSMs) achieve linear-time inference, yet their gradient-based sensitivity analysis remains bottlenecked by O(L) memory scaling during backpropagation. This memory constraint precludes genomic-scale modeling (L > 10^5) on consumer-grade hardware. We introduce Phase Gradient Flow (PGF), a framework that computes exact analytical derivatives by operating directly in the state-space manifold, bypassing the need to materialize the intermediate computational graph. By reframing SSM dynamics as Tiled Operator-Space Evolution (TOSE), our method delivers O(1) memory complexity relative to sequence length, yielding a 94% reduction in peak VRAM and a 23x increase in throughput compared to standard Autograd. Unlike parallel prefix scans that exhibit numerical divergence in stiff ODE regimes, PGF ensures stability through invariant error scaling, maintaining near-machine precision across extreme sequences. We demonstrate the utility of PGF on an impulse-response benchmark with 128,000-step sequences - a scale where conventional Autograd encounters prohibitive memory overhead, often leading to out-of-memory (OOM) failures in multi-layered models. Our work enables chromosome-scale sensitivity analysis on a single GPU, bridging the gap between theoretical infinite-context models and practical hardware limitations.

</details>


### [136] [FLEX-MoE: Federated Mixture-of-Experts with Load-balanced Expert Assignment](https://arxiv.org/abs/2512.23070)
*Boyang Zhang,Xiaobing Chen,Songyang Zhang,Shuai Zhang,Xiangwei Zhou,Mingxuan Sun*

Main category: cs.LG

TL;DR: FLEX-MoE：一种联合优化专家分配与负载均衡的联邦MoE框架，解决边缘设备存储限制和非IID数据导致的专家负载失衡问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的MoE模型面临两个关键挑战：1) 资源受限的边缘设备无法存储完整的专家集合；2) 非IID数据分布导致严重的专家负载失衡，从而降低模型性能。

Method: 提出FLEX-MoE框架，引入客户端-专家适应度分数量化专家对本地数据集的适用性，并采用基于优化的算法在系统范围内最大化客户端-专家专业化同时强制平衡专家利用率。

Result: 在三个不同数据集上的综合实验表明，FLEX-MoE具有优越性能，并能在多样化的资源受限场景中保持平衡的专家利用率。

Conclusion: FLEX-MoE有效解决了联邦学习中MoE模型的存储限制和负载失衡问题，相比仅关注个性化的贪婪方法，能够处理异构数据环境下特别严重的专家利用率倾斜。

Abstract: Mixture-of-Experts (MoE) models enable scalable neural networks through conditional computation. However, their deployment with federated learning (FL) faces two critical challenges: 1) resource-constrained edge devices cannot store full expert sets, and 2) non-IID data distributions cause severe expert load imbalance that degrades model performance. To this end, we propose \textbf{FLEX-MoE}, a novel federated MoE framework that jointly optimizes expert assignment and load balancing under limited client capacity. Specifically, our approach introduces client-expert fitness scores that quantify the expert suitability for local datasets through training feedback, and employs an optimization-based algorithm to maximize client-expert specialization while enforcing balanced expert utilization system-wide. Unlike existing greedy methods that focus solely on personalization while ignoring load imbalance, our FLEX-MoE is capable of addressing the expert utilization skew, which is particularly severe in FL settings with heterogeneous data. Our comprehensive experiments on three different datasets demonstrate the superior performance of the proposed FLEX-MoE, together with its ability to maintain balanced expert utilization across diverse resource-constrained scenarios.

</details>


### [137] [Rethinking Fine-Tuning: Unlocking Hidden Capabilities in Vision-Language Models](https://arxiv.org/abs/2512.23073)
*Mingyuan Zhang,Yue Bai,Yifan Wang,Yiyang Huang,Yun Fu*

Main category: cs.LG

TL;DR: 该论文提出将掩码微调（MFT）应用于视觉语言模型（VLM），通过为每个权重分配可学习的门控分数来重组内部子网络，而不是更新权重，从而实现高效的下游任务适应。


<details>
  <summary>Details</summary>
Motivation: 当前VLM微调方法主要依赖显式的权重更新，忽略了预训练模型中已编码的丰富表示结构。这些结构未被充分利用，而MFT已被证明是语言模型的高效后训练范式，因此作者希望探索MFT在VLM中的应用潜力。

Method: 从结构重参数化角度重新思考VLM微调，将MFT应用于VLM的语言和投影器组件。该方法不更新权重，而是为每个权重分配可学习的门控分数，让模型能够重组其内部子网络以适应下游任务。

Result: 实验表明，MFT在多种语言骨干网络上一致超越了LoRA变体甚至全微调，在不改变冻结骨干网络的情况下实现了高性能。该方法在多个基准测试中表现出色。

Conclusion: 有效的适应不仅可以通过更新权重实现，还可以通过重新建立模型现有知识之间的连接来实现。MFT为VLM微调提供了一种高效且性能优异的替代方案。

Abstract: Explorations in fine-tuning Vision-Language Models (VLMs), such as Low-Rank Adaptation (LoRA) from Parameter Efficient Fine-Tuning (PEFT), have made impressive progress. However, most approaches rely on explicit weight updates, overlooking the extensive representational structures already encoded in pre-trained models that remain underutilized. Recent works have demonstrated that Mask Fine-Tuning (MFT) can be a powerful and efficient post-training paradigm for language models. Instead of updating weights, MFT assigns learnable gating scores to each weight, allowing the model to reorganize its internal subnetworks for downstream task adaptation. In this paper, we rethink fine-tuning for VLMs from a structural reparameterization perspective grounded in MFT. We apply MFT to the language and projector components of VLMs with different language backbones and compare against strong PEFT baselines. Experiments show that MFT consistently surpasses LoRA variants and even full fine-tuning, achieving high performance without altering the frozen backbone. Our findings reveal that effective adaptation can emerge not only from updating weights but also from reestablishing connections among the model's existing knowledge. Code available at: https://github.com/Ming-K9/MFT-VLM

</details>


### [138] [Multimodal Functional Maximum Correlation for Emotion Recognition](https://arxiv.org/abs/2512.23076)
*Deyang Zheng,Tianyi Zhang,Wenming Zheng,Shujian Yu*

Main category: cs.LG

TL;DR: 提出MFMC框架，通过最大化高阶多模态依赖来学习情感状态的多模态表征，避免成对对比损失的局限性


<details>
  <summary>Details</summary>
Motivation: 情感状态表现为中枢和自主系统的协调但异质的生理反应，现有自监督学习方法主要依赖成对对齐目标，无法捕捉多模态间的高阶交互作用

Method: 提出多模态功能最大相关（MFMC）框架，使用双总相关（DTC）目标最大化高阶多模态依赖，通过功能最大相关分析（FMCA）的迹代理进行优化

Result: 在三个公共情感计算基准测试中，MFMC在主体依赖和主体独立评估协议下均达到最先进或竞争性性能，特别是在CEAP-360VR数据集上显著提升准确率

Conclusion: MFMC能够有效捕捉多模态间的高阶交互，对主体间变异性具有鲁棒性，为情感计算中的多模态表征学习提供了新方法

Abstract: Emotional states manifest as coordinated yet heterogeneous physiological responses across central and autonomic systems, posing a fundamental challenge for multimodal representation learning in affective computing. Learning such joint dynamics is further complicated by the scarcity and subjectivity of affective annotations, which motivates the use of self-supervised learning (SSL). However, most existing SSL approaches rely on pairwise alignment objectives, which are insufficient to characterize dependencies among more than two modalities and fail to capture higher-order interactions arising from coordinated brain and autonomic responses.
  To address this limitation, we propose Multimodal Functional Maximum Correlation (MFMC), a principled SSL framework that maximizes higher-order multimodal dependence through a Dual Total Correlation (DTC) objective. By deriving a tight sandwich bound and optimizing it using a functional maximum correlation analysis (FMCA) based trace surrogate, MFMC captures joint multimodal interactions directly, without relying on pairwise contrastive losses.
  Experiments on three public affective computing benchmarks demonstrate that MFMC consistently achieves state-of-the-art or competitive performance under both subject-dependent and subject-independent evaluation protocols, highlighting its robustness to inter-subject variability. In particular, MFMC improves subject-dependent accuracy on CEAP-360VR from 78.9% to 86.8%, and subject-independent accuracy from 27.5% to 33.1% using the EDA signal alone. Moreover, MFMC remains within 0.8 percentage points of the best-performing method on the most challenging EEG subject-independent split of MAHNOB-HCI. Our code is available at https://github.com/DY9910/MFMC.

</details>


### [139] [Osmotic Learning: A Self-Supervised Paradigm for Decentralized Contextual Data Representation](https://arxiv.org/abs/2512.23096)
*Mario Colosi,Reza Farahani,Maria Fazio,Radu Prodan,Massimo Villari*

Main category: cs.LG

TL;DR: OSM-L是一种自监督分布式学习范式，通过渗透过程从分布式数据中提取高层次潜在知识，无需原始数据交换，实现本地表示对齐和信息扩散。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中相互依赖的数据源包含隐藏关系和潜在结构，这些信息对许多应用具有价值。传统方法需要原始数据交换，存在隐私和效率问题。

Method: 提出渗透学习(OSM-L)范式，核心是渗透过程：通过提取上下文信息合成密集紧凑表示，无需原始数据交换。迭代对齐本地数据表示，实现信息扩散和收敛到动态平衡，同时识别相关数据组作为去中心化聚类机制。

Result: 在结构化数据集上验证了OSM-L的收敛性和表示能力，本地信息对齐准确率超过0.99，同时保持了上下文完整性。

Conclusion: OSM-L能够有效从分布式数据中提取高层次潜在知识，通过渗透过程实现高效的信息扩散和表示学习，为分布式学习提供了新的自监督范式。

Abstract: Data within a specific context gains deeper significance beyond its isolated interpretation. In distributed systems, interdependent data sources reveal hidden relationships and latent structures, representing valuable information for many applications. This paper introduces Osmotic Learning (OSM-L), a self-supervised distributed learning paradigm designed to uncover higher-level latent knowledge from distributed data. The core of OSM-L is osmosis, a process that synthesizes dense and compact representation by extracting contextual information, eliminating the need for raw data exchange between distributed entities. OSM-L iteratively aligns local data representations, enabling information diffusion and convergence into a dynamic equilibrium that captures contextual patterns. During training, it also identifies correlated data groups, functioning as a decentralized clustering mechanism. Experimental results confirm OSM-L's convergence and representation capabilities on structured datasets, achieving over 0.99 accuracy in local information alignment while preserving contextual integrity.

</details>


### [140] [A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms](https://arxiv.org/abs/2512.23097)
*Yingru Li,Ziniu Li,Jiacai Liu*

Main category: cs.LG

TL;DR: 提出统一的LLM微调框架，结合模仿学习和强化学习，通过梯度分解实现高效优化


<details>
  <summary>Details</summary>
Motivation: 现有LLM微调方法通常单独使用模仿学习或强化学习，缺乏统一的框架来同时利用两者的优势。模仿学习能提供密集的token级监督，而强化学习能优化长时程任务奖励，需要一种方法将两者有效结合。

Method: 通过分析结合轨迹级KL散度和任务奖励的复合目标函数的梯度，推导出自然分解：1）可解析计算的密集梯度（用于token级模仿），2）蒙特卡洛估计的稀疏梯度（用于长时程奖励优化）。密集梯度具有闭式logit级公式，支持高效的GPU实现。

Result: 提出了统一的LLM微调框架，将模仿学习和强化学习梯度分解为可高效计算的密集梯度和稀疏梯度，实现了两者的有效结合。

Conclusion: 该框架为LLM微调提供了统一的理论基础，通过梯度分解实现了模仿学习和强化学习的有效结合，既保持了token级模仿的密集监督，又实现了长时程奖励优化，且支持高效计算。

Abstract: We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.

</details>


### [141] [SE-MLP Model for Predicting Prior Acceleration Features in Penetration Signals](https://arxiv.org/abs/2512.23131)
*Yankang Li,Changsheng Li*

Main category: cs.LG

TL;DR: 本文提出了一种集成通道注意力机制和残差连接的SE-MLP模型，用于快速预测侵彻加速度特征值，替代传统耗时计算，在精度、泛化性和稳定性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 侵彻过程的准确识别依赖于侵彻加速度的先验特征值，但这些特征值通常需要通过长时间仿真和昂贵计算获得，限制了实际应用效率。

Method: 提出SE-MLP多层感知器架构，集成通道注意力机制和残差连接，以不同工况下的物理参数作为输入，输出层间加速度特征，建立物理参数与侵彻特性之间的非线性映射。

Result: 与常规MLP、XGBoost和Transformer模型相比，SE-MLP在预测精度、泛化性和稳定性方面表现更优；消融实验证实通道注意力模块和残差结构对性能提升有显著贡献；数值仿真和射程恢复测试显示预测与实测加速度峰值和脉冲宽度的差异在工程容差范围内。

Conclusion: 该方法验证了快速预测侵彻加速度特征值的可行性和工程适用性，为侵彻引信快速生成先验特征值提供了实用基础。

Abstract: Accurate identification of the penetration process relies heavily on prior feature values of penetration acceleration. However, these feature values are typically obtained through long simulation cycles and expensive computations. To overcome this limitation, this paper proposes a multi-layer Perceptron architecture, termed squeeze and excitation multi-layer perceptron (SE-MLP), which integrates a channel attention mechanism with residual connections to enable rapid prediction of acceleration feature values. Using physical parameters under different working conditions as inputs, the model outputs layer-wise acceleration features, thereby establishing a nonlinear mapping between physical parameters and penetration characteristics. Comparative experiments against conventional MLP, XGBoost, and Transformer models demonstrate that SE-MLP achieves superior prediction accuracy, generalization, and stability. Ablation studies further confirm that both the channel attention module and residual structure contribute significantly to performance gains. Numerical simulations and range recovery tests show that the discrepancies between predicted and measured acceleration peaks and pulse widths remain within acceptable engineering tolerances. These results validate the feasibility and engineering applicability of the proposed method and provide a practical basis for rapidly generating prior feature values for penetration fuzes.

</details>


### [142] [Graph Neural Networks with Transformer Fusion of Brain Connectivity Dynamics and Tabular Data for Forecasting Future Tobacco Use](https://arxiv.org/abs/2512.23137)
*Runzhi Zhou,Xi Luo*

Main category: cs.LG

TL;DR: 提出GNN-TF模型，整合非欧几里得脑成像数据和欧几里得表格数据，用于预测未来烟草使用，在纵向fMRI研究中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 整合非欧几里得脑成像数据（如动态脑连接）与欧几里得表格数据（临床和人口统计信息）在医学影像分析中具有挑战性，特别是在预测未来结果的纵向研究中。

Method: 提出时间感知图神经网络与Transformer融合模型（GNN-TF），灵活整合表格数据和动态脑连接数据，利用变量时间顺序，在统一框架中处理多模态数据。

Result: 在NCANDA纵向静息态fMRI数据集上，GNN-TF在预测未来烟草使用方面优于多种现有机器学习和深度学习方法，表现出更优的预测准确性。

Conclusion: GNN-TF的端到端时间感知Transformer融合结构成功整合多模态数据并利用时间动态，成为功能脑成像研究中临床结果预测的有价值分析工具。

Abstract: Integrating non-Euclidean brain imaging data with Euclidean tabular data, such as clinical and demographic information, poses a substantial challenge for medical imaging analysis, particularly in forecasting future outcomes. While machine learning and deep learning techniques have been applied successfully to cross-sectional classification and prediction tasks, effectively forecasting outcomes in longitudinal imaging studies remains challenging. To address this challenge, we introduce a time-aware graph neural network model with transformer fusion (GNN-TF). This model flexibly integrates both tabular data and dynamic brain connectivity data, leveraging the temporal order of these variables within a coherent framework. By incorporating non-Euclidean and Euclidean sources of information from a longitudinal resting-state fMRI dataset from the National Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA), the GNN-TF enables a comprehensive analysis that captures critical aspects of longitudinal imaging data. Comparative analyses against a variety of established machine learning and deep learning models demonstrate that GNN-TF outperforms these state-of-the-art methods, delivering superior predictive accuracy for predicting future tobacco usage. The end-to-end, time-aware transformer fusion structure of the proposed GNN-TF model successfully integrates multiple data modalities and leverages temporal dynamics, making it a valuable analytic tool for functional brain imaging studies focused on clinical outcome prediction.

</details>


### [143] [A Weak Signal Learning Dataset and Its Baseline Method](https://arxiv.org/abs/2512.23160)
*Xianqi Liu,Xiangru Li,Lefeng He,Ziyu Fang*

Main category: cs.LG

TL;DR: 该研究构建了首个弱信号特征学习专用数据集，并提出PDVFN模型来处理低信噪比、分布偏斜和双重不平衡问题，为弱信号学习提供了新解决方案。


<details>
  <summary>Details</summary>
Motivation: 弱信号学习在许多领域（如故障诊断、医学成像、自动驾驶）都存在挑战，关键信息常被噪声和干扰掩盖，特征识别困难。即使有强信号的任务中，提升模型性能的关键也在于有效提取弱信号。然而，缺乏专用数据集长期制约了相关研究。

Method: 构建了首个弱信号特征学习专用数据集（13,158个光谱样本），具有低信噪比主导（超过55%样本SNR低于50）和极端类别不平衡（类别比例达29:1）的特点。提出PDVFN模型，采用双视图表示（向量+时频图），并行提取局部序列特征和全局频域结构，遵循局部增强、序列建模、噪声抑制、多尺度捕获、频率提取和全局感知原则。

Result: 实验表明该方法在处理弱信号、高噪声和极端类别不平衡方面实现了更高的准确性和鲁棒性，特别是在低信噪比和不平衡场景下。PDVFN模型通过多源互补增强了低信噪比和不平衡数据的表示能力。

Conclusion: 该研究为弱信号学习提供了专用数据集、基线模型，并建立了未来研究的基础。提出的PDVFN模型为天文光谱学等弱信号学习任务提供了新颖解决方案，通过多源互补增强了低信噪比和不平衡数据的表示能力。

Abstract: Weak signal learning (WSL) is a common challenge in many fields like fault diagnosis, medical imaging, and autonomous driving, where critical information is often masked by noise and interference, making feature identification difficult. Even in tasks with abundant strong signals, the key to improving model performance often lies in effectively extracting weak signals. However, the lack of dedicated datasets has long constrained research. To address this, we construct the first specialized dataset for weak signal feature learning, containing 13,158 spectral samples. It features low SNR dominance (over 55% samples with SNR below 50) and extreme class imbalance (class ratio up to 29:1), providing a challenging benchmark for classification and regression in weak signal scenarios. We also propose a dual-view representation (vector + time-frequency map) and a PDVFN model tailored to low SNR, distribution skew, and dual imbalance. PDVFN extracts local sequential features and global frequency-domain structures in parallel, following principles of local enhancement, sequential modeling, noise suppression, multi-scale capture, frequency extraction, and global perception. This multi-source complementarity enhances representation for low-SNR and imbalanced data, offering a novel solution for WSL tasks like astronomical spectroscopy. Experiments show our method achieves higher accuracy and robustness in handling weak signals, high noise, and extreme class imbalance, especially in low SNR and imbalanced scenarios. This study provides a dedicated dataset, a baseline model, and establishes a foundation for future WSL research.

</details>


### [144] [Diffusion-based Decentralized Federated Multi-Task Representation Learning](https://arxiv.org/abs/2512.23161)
*Donghwa Kang,Shana Moothedath*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的去中心化多任务表示学习算法，用于解决数据稀缺环境下多个线性回归模型共享低维线性表示的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管表示学习在数据稀缺环境中被广泛采用，但去中心化方法仍相对未被充分探索。本研究旨在开发去中心化的多任务表示学习方法，以解决多个相关任务共享共同低维表示的问题。

Method: 提出了一种基于扩散的去中心化和联邦式交替投影梯度下降最小化算法，用于恢复低秩特征矩阵。算法采用投影梯度下降方法，在去中心化网络中通过扩散机制进行协作学习。

Result: 获得了构造性的理论保证，包括所需样本复杂度的下界和迭代复杂度的上界。分析表明算法具有快速的时间复杂度和高效的通信复杂度。数值模拟验证了算法性能优于基准算法。

Conclusion: 成功开发了一种高效的去中心化多任务表示学习算法，在理论保证和实际性能方面均表现出色，填补了去中心化表示学习领域的空白。

Abstract: Representation learning is a widely adopted framework for learning in data-scarce environments to obtain a feature extractor or representation from various different yet related tasks. Despite extensive research on representation learning, decentralized approaches remain relatively underexplored. This work develops a decentralized projected gradient descent-based algorithm for multi-task representation learning. We focus on the problem of multi-task linear regression in which multiple linear regression models share a common, low-dimensional linear representation. We present an alternating projected gradient descent and minimization algorithm for recovering a low-rank feature matrix in a diffusion-based decentralized and federated fashion. We obtain constructive, provable guarantees that provide a lower bound on the required sample complexity and an upper bound on the iteration complexity of our proposed algorithm. We analyze the time and communication complexity of our algorithm and show that it is fast and communication-efficient. We performed numerical simulations to validate the performance of our algorithm and compared it with benchmark algorithms.

</details>


### [145] [Evaluating Parameter Efficient Methods for RLVR](https://arxiv.org/abs/2512.23165)
*Qingyu Yin,Yulun Wu,Zhennan Shen,Sunbowen Li,Zhilin Wang,Yanshu Li,Chak Tou Leong,Jiale Kang,Jinjin Gu*

Main category: cs.LG

TL;DR: 本文首次系统评估了12种参数高效微调方法在可验证奖励强化学习范式下的表现，发现结构变体优于标准LoRA，SVD初始化策略存在谱崩溃问题，极端参数减少会严重限制推理能力。


<details>
  <summary>Details</summary>
Motivation: 在可验证奖励强化学习范式下，虽然LoRA等方法被广泛使用，但最优的参数高效微调架构仍未确定。本文旨在通过系统评估为参数高效的强化学习方法提供指导。

Method: 在DeepSeek-R1-Distill系列模型上，对超过12种参数高效微调方法在数学推理基准上进行全面评估，包括消融实验和扩展实验验证发现。

Result: 1. 结构变体如DoRA、AdaLoRA和MiSS持续优于标准LoRA；2. 发现SVD初始化策略存在谱崩溃现象，归因于主成分更新与RL优化的根本错配；3. 极端参数减少会严重瓶颈推理能力。

Conclusion: 本文挑战了默认采用标准LoRA的做法，为参数高效强化学习方法提供了明确的指导，并呼吁更多探索。

Abstract: We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.

</details>


### [146] [HELM-BERT: A Transformer for Medium-sized Peptide Property Prediction](https://arxiv.org/abs/2512.23175)
*Seungeon Lee,Takuto Koyama,Itsuki Maeda,Shigeyuki Matsumoto,Yasushi Okuno*

Main category: cs.LG

TL;DR: HELM-BERT：首个基于HELM表示法的肽语言模型，在肽类药物性质预测任务上显著优于现有SMILES模型


<details>
  <summary>Details</summary>
Motivation: 现有分子语言模型（SMILES或氨基酸级表示）无法充分捕捉治疗性肽的化学复杂性和拓扑结构，需要更好的表示方法来加速肽类药物开发

Method: 提出HELM-BERT模型，基于DeBERTa架构，使用HELM（Hierarchical Editing Language for Macromolecules）表示法训练，能精确描述单体组成和连接性，捕获层次依赖关系

Result: 在39,079个化学多样性肽的数据集上预训练，在环肽膜渗透性和肽-蛋白相互作用预测等下游任务上显著优于最先进的SMILES模型

Conclusion: HELM的明确单体和拓扑感知表示为治疗性肽建模提供了显著的数据效率优势，填补了小分子和蛋白质语言模型之间的长期空白

Abstract: Therapeutic peptides have emerged as a pivotal modality in modern drug discovery, occupying a chemically and topologically rich space. While accurate prediction of their physicochemical properties is essential for accelerating peptide development, existing molecular language models rely on representations that fail to capture this complexity. Atom-level SMILES notation generates long token sequences and obscures cyclic topology, whereas amino-acid-level representations cannot encode the diverse chemical modifications central to modern peptide design. To bridge this representational gap, the Hierarchical Editing Language for Macromolecules (HELM) offers a unified framework enabling precise description of both monomer composition and connectivity, making it a promising foundation for peptide language modeling. Here, we propose HELM-BERT, the first encoder-based peptide language model trained on HELM notation. Based on DeBERTa, HELM-BERT is specifically designed to capture hierarchical dependencies within HELM sequences. The model is pre-trained on a curated corpus of 39,079 chemically diverse peptides spanning linear and cyclic structures. HELM-BERT significantly outperforms state-of-the-art SMILES-based language models in downstream tasks, including cyclic peptide membrane permeability prediction and peptide-protein interaction prediction. These results demonstrate that HELM's explicit monomer- and topology-aware representations offer substantial data-efficiency advantages for modeling therapeutic peptides, bridging a long-standing gap between small-molecule and protein language models.

</details>


### [147] [Machine Learning-Assisted Vocal Cord Ultrasound Examination: Project VIPR](https://arxiv.org/abs/2512.23177)
*Will Sebelik-Lassiter,Evan Schubert,Muhammad Alliyu,Quentin Robbins,Excel Olatunji,Mustafa Barry*

Main category: cs.LG

TL;DR: 利用机器学习算法自动识别声带并区分正常声带与声带麻痹的超声图像，验证准确率达99%


<details>
  <summary>Details</summary>
Motivation: 声带超声检查虽然创伤小、耐受性好，但其准确性高度依赖操作者经验。本研究旨在开发机器学习辅助算法，实现声带的自动识别和声带麻痹的自动诊断，减少人为误差。

Method: 从30名志愿者采集声带超声视频，分割为静态帧并统一裁剪尺寸。使用健康声带和模拟声带麻痹图像作为训练数据，分别训练声带分割模型和声带麻痹分类模型。

Result: 声带分割模型验证准确率达96%，最佳分类模型(VIPRnet)验证准确率达99%，显示机器学习在声带超声分析中的高准确性。

Conclusion: 机器学习辅助的声带超声分析在提高诊断准确性方面展现出巨大潜力，有望超越依赖操作者经验的人工判读。

Abstract: Intro: Vocal cord ultrasound (VCUS) has emerged as a less invasive and better tolerated examination technique, but its accuracy is operator dependent. This research aims to apply a machine learning-assisted algorithm to automatically identify the vocal cords and distinguish normal vocal cord images from vocal cord paralysis (VCP). Methods: VCUS videos were acquired from 30 volunteers, which were split into still frames and cropped to a uniform size. Healthy and simulated VCP images were used as training data for vocal cord segmentation and VCP classification models. Results: The vocal cord segmentation model achieved a validation accuracy of 96%, while the best classification model (VIPRnet) achieved a validation accuracy of 99%. Conclusion: Machine learning-assisted analysis of VCUS shows great promise in improving diagnostic accuracy over operator-dependent human interpretation.

</details>


### [148] [PGOT: A Physics-Geometry Operator Transformer for Complex PDEs](https://arxiv.org/abs/2512.23192)
*Zhuo Zhang,Xi Yang,Yuan Zhao,Canqun Yang*

Main category: cs.LG

TL;DR: PGOT提出了一种物理-几何算子Transformer，通过谱保持几何注意力机制解决大规模非结构化网格建模中的几何混叠问题，实现空间自适应的高精度物理场建模。


<details>
  <summary>Details</summary>
Motivation: Transformer在PDE建模中表现出色，但处理大规模非结构化网格和复杂几何时面临挑战。现有高效架构采用特征降维策略会导致几何混叠，丢失关键的物理边界信息。

Method: 提出PGOT框架，包含谱保持几何注意力模块，采用"物理切片-几何注入"机制，融入多尺度几何编码。同时基于空间坐标动态路由计算，平滑区域使用低阶线性路径，激波和不连续区域使用高阶非线性路径。

Result: PGOT在四个标准基准测试中取得一致的SOTA性能，并在翼型和汽车设计等大规模工业任务中表现出色。

Conclusion: PGOT通过显式几何感知重建物理特征学习，有效解决几何混叠问题，实现空间自适应的高精度物理场建模，在大规模工业应用中具有重要价值。

Abstract: While Transformers have demonstrated remarkable potential in modeling Partial Differential Equations (PDEs), modeling large-scale unstructured meshes with complex geometries remains a significant challenge. Existing efficient architectures often employ feature dimensionality reduction strategies, which inadvertently induces Geometric Aliasing, resulting in the loss of critical physical boundary information. To address this, we propose the Physics-Geometry Operator Transformer (PGOT), designed to reconstruct physical feature learning through explicit geometry awareness. Specifically, we propose Spectrum-Preserving Geometric Attention (SpecGeo-Attention). Utilizing a ``physics slicing-geometry injection" mechanism, this module incorporates multi-scale geometric encodings to explicitly preserve multi-scale geometric features while maintaining linear computational complexity $O(N)$. Furthermore, PGOT dynamically routes computations to low-order linear paths for smooth regions and high-order non-linear paths for shock waves and discontinuities based on spatial coordinates, enabling spatially adaptive and high-precision physical field modeling. PGOT achieves consistent state-of-the-art performance across four standard benchmarks and excels in large-scale industrial tasks including airfoil and car designs.

</details>


### [149] [Energy and Memory-Efficient Federated Learning With Ordered Layer Freezing](https://arxiv.org/abs/2512.23200)
*Ziru Niu,Hai Dong,A. K. Qin,Tao Gu,Pengcheng Zhang*

Main category: cs.LG

TL;DR: FedOLF：一种通过有序层冻结和Tensor操作近似来降低联邦学习中计算、内存和通信开销的新方法，在非独立同分布数据上实现了更高的准确率和能效。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在物联网边缘设备上训练模型时面临计算能力、内存和带宽限制的挑战。现有方法如dropout或层冻结往往牺牲准确性或忽略内存约束，需要更高效的解决方案。

Method: 提出FedOLF方法：1）在训练前按预定义顺序持续冻结层，减少计算和内存需求；2）引入Tensor操作近似作为轻量级替代方案，比传统量化更好地保持模型准确性，进一步降低通信和能耗。

Result: 在非独立同分布数据上，FedOLF在多个数据集和模型上均优于现有方法：EMNIST(CNN)准确率提高0.3%，CIFAR-10(AlexNet)提高6.4%，CIFAR-100(ResNet20/44)分别提高5.81%和4.4%，CINIC-10(ResNet20/44)分别提高6.27%和1.29%，同时具有更高的能效和更低的内存占用。

Conclusion: FedOLF通过有序层冻结和Tensor操作近似，有效解决了联邦学习在资源受限边缘设备上的效率问题，在保持甚至提高准确性的同时显著降低了计算、内存和通信开销。

Abstract: Federated Learning (FL) has emerged as a privacy-preserving paradigm for training machine learning models across distributed edge devices in the Internet of Things (IoT). By keeping data local and coordinating model training through a central server, FL effectively addresses privacy concerns and reduces communication overhead. However, the limited computational power, memory, and bandwidth of IoT edge devices pose significant challenges to the efficiency and scalability of FL, especially when training deep neural networks. Various FL frameworks have been proposed to reduce computation and communication overheads through dropout or layer freezing. However, these approaches often sacrifice accuracy or neglect memory constraints. To this end, in this work, we introduce Federated Learning with Ordered Layer Freezing (FedOLF). FedOLF consistently freezes layers in a predefined order before training, significantly mitigating computation and memory requirements. To further reduce communication and energy costs, we incorporate Tensor Operation Approximation (TOA), a lightweight alternative to conventional quantization that better preserves model accuracy. Experimental results demonstrate that over non-iid data, FedOLF achieves at least 0.3%, 6.4%, 5.81%, 4.4%, 6.27% and 1.29% higher accuracy than existing works respectively on EMNIST (with CNN), CIFAR-10 (with AlexNet), CIFAR-100 (with ResNet20 and ResNet44), and CINIC-10 (with ResNet20 and ResNet44), along with higher energy efficiency and lower memory footprint.

</details>


### [150] [FairGFL: Privacy-Preserving Fairness-Aware Federated Learning with Overlapping Subgraphs](https://arxiv.org/abs/2512.23235)
*Zihao Zhou,Shusen Yang,Fangyuan Zhao,Xuebin Ren*

Main category: cs.LG

TL;DR: FairGFL：一种解决图联邦学习中重叠子图不平衡导致不公平问题的新算法，通过隐私保护的重叠率估计和加权聚合来提升跨客户端公平性。


<details>
  <summary>Details</summary>
Motivation: 图联邦学习中的重叠子图通常存在不平衡分布，现有研究只关注了重叠数据的正面作用，但未探索其负面影响，特别是当重叠在不同客户端间不平衡时会导致不公平问题。

Method: 提出FairGFL算法：1) 使用隐私保护方法估计客户端重叠率；2) 采用可解释的加权聚合方法增强跨客户端公平性；3) 在联邦复合损失函数中集成精心设计的正则化器，平衡模型效用与公平性。

Result: 在四个基准图数据集上的实验表明，FairGFL在模型效用和公平性方面均优于四种代表性基线算法。

Conclusion: FairGFL成功解决了图联邦学习中由不平衡重叠子图引起的不公平问题，在保护隐私的同时实现了模型效用与公平性的更好权衡。

Abstract: Graph federated learning enables the collaborative extraction of high-order information from distributed subgraphs while preserving the privacy of raw data. However, graph data often exhibits overlap among different clients. Previous research has demonstrated certain benefits of overlapping data in mitigating data heterogeneity. However, the negative effects have not been explored, particularly in cases where the overlaps are imbalanced across clients. In this paper, we uncover the unfairness issue arising from imbalanced overlapping subgraphs through both empirical observations and theoretical reasoning. To address this issue, we propose FairGFL (FAIRness-aware subGraph Federated Learning), a novel algorithm that enhances cross-client fairness while maintaining model utility in a privacy-preserving manner. Specifically, FairGFL incorporates an interpretable weighted aggregation approach to enhance fairness across clients, leveraging privacy-preserving estimation of their overlapping ratios. Furthermore, FairGFL improves the tradeoff between model utility and fairness by integrating a carefully crafted regularizer into the federated composite loss function. Through extensive experiments on four benchmark graph datasets, we demonstrate that FairGFL outperforms four representative baseline algorithms in terms of both model utility and fairness.

</details>


### [151] [KernelEvolve: Scaling Agentic Kernel Coding for Heterogeneous AI Accelerators at Meta](https://arxiv.org/abs/2512.23236)
*Gang Liao,Hongsen Qin,Ying Wang,Alicia Golden,Michael Kuchnik,Yavuz Yetim,Jia Jiunn Ang,Chunli Fu,Yihan He,Samuel Hsia,Zewei Jiang,Dianshi Li,Uladzimir Pashkevich,Varna Puvvada,Feng Shi,Matt Steiner,Ruichao Xiao,Nathan Yan,Xiayu Yu,Zhou Fang,Abdul Zainul-Abedin,Ketan Singh,Hongtao Yu,Wenyuan Chi,Barney Huang,Sean Zhang,Noah Weller,Zach Marine,Wyatt Cook,Carole-Jean Wu,Gaoxiang Liu*

Main category: cs.LG

TL;DR: KernelEvolve是一个面向DLRM的自动化内核生成与优化框架，通过图搜索和检索增强提示合成技术，解决推荐模型训练推理中的硬件异构性问题，显著提升性能并降低开发时间。


<details>
  <summary>Details</summary>
Motivation: 深度学习推荐模型（DLRM）的训练和推理需要高效快速，但面临三大系统挑战：模型架构多样性、内核原语多样性、硬件世代和架构异构性。现有方法难以在不同硬件平台上实现高效的内核优化。

Method: KernelEvolve采用多级编程抽象（从Triton/CuTe DSL到低级硬件无关语言），通过图搜索算法（包含选择策略、通用算子、适应度函数和终止规则）进行内核优化，并利用检索增强提示合成动态适应运行时执行上下文。

Result: 在KernelBench测试套件上实现100%通过率（250个问题，三个难度级别），在三个异构硬件平台上支持160个PyTorch ATen算子并保持100%正确性。将开发时间从数周缩短到数小时，并在多种生产用例中显著超越PyTorch基线性能。

Conclusion: KernelEvolve有效解决了DLRM在异构硬件上的内核优化挑战，不仅大幅提升性能效率，还通过自动化内核生成为新型AI硬件降低了编程门槛，实现了大规模异构AI系统的可扩展优化。

Abstract: Making deep learning recommendation model (DLRM) training and inference fast and efficient is important. However, this presents three key system challenges - model architecture diversity, kernel primitive diversity, and hardware generation and architecture heterogeneity. This paper presents KernelEvolve-an agentic kernel coding framework-to tackle heterogeneity at-scale for DLRM. KernelEvolve is designed to take kernel specifications as input and automate the process of kernel generation and optimization for recommendation model across heterogeneous hardware architectures. KernelEvolve does so by operating at multiple programming abstractions, from Triton and CuTe DSL to low-level hardware agnostic languages, spanning the full hardware-software optimization stack. The kernel optimization process is described as graph-based search with selection policy, universal operator, fitness function, and termination rule, dynamically adapts to runtime execution context through retrieval-augmented prompt synthesis. We designed, implemented, and deployed KernelEvolve to optimize a wide variety of production recommendation models across generations of NVIDIA and AMD GPUs, as well as Meta's AI accelerators. We validate KernelEvolve on the publicly-available KernelBench suite, achieving 100% pass rate on all 250 problems across three difficulty levels, and 160 PyTorch ATen operators across three heterogeneous hardware platforms, demonstrating 100% correctness. KernelEvolve reduces development time from weeks to hours and achieves substantial performance improvements over PyTorch baselines across diverse production use cases and for heterogeneous AI systems at-scale. Beyond performance efficiency improvements, KernelEvolve significantly mitigates the programmability barrier for new AI hardware by enabling automated kernel generation for in-house developed AI hardware.

</details>


### [152] [PFed-Signal: An ADR Prediction Model based on Federated Learning](https://arxiv.org/abs/2512.23262)
*Tao Li,Peilin Li,Kui Lu,Yilei Wang,Junliang Shang,Guangshun Li,Huiyu Zhou*

Main category: cs.LG

TL;DR: PFed-signal：基于联邦学习的药物不良反应信号预测模型，通过欧氏距离消除FAERS数据中的偏差，提高预测准确性


<details>
  <summary>Details</summary>
Motivation: FAERS系统中存在偏差记录，基于这些数据预测的药物不良反应可能误导在线诊断。传统方法如ROR和PRR依赖统计方法，无法消除数据偏差，导致信号预测不准确。

Method: 提出PFed-signal模型，包含两个部分：1) Pfed-Split方法将原始数据集按ADR分割；2) ADR-signal模型，包括基于联邦学习的偏差数据识别方法（使用欧氏距离）和基于Transformer的ADR预测模型。先识别并删除偏差数据生成干净数据集，再在干净数据上训练Transformer预测模型。

Result: 在干净数据集上的ROR和PRR优于传统方法。PFed-Signal的准确率、F1分数、召回率和AUC分别为0.887、0.890、0.913和0.957，均高于基线方法。

Conclusion: PFed-signal通过联邦学习和欧氏距离有效消除FAERS数据偏差，显著提高了药物不良反应信号预测的准确性。

Abstract: The adverse drug reactions (ADRs) predicted based on the biased records in FAERS (U.S. Food and Drug Administration Adverse Event Reporting System) may mislead diagnosis online. Generally, such problems are solved by optimizing reporting odds ratio (ROR) or proportional reporting ratio (PRR). However, these methods that rely on statistical methods cannot eliminate the biased data, leading to inaccurate signal prediction. In this paper, we propose PFed-signal, a federated learning-based signal prediction model of ADR, which utilizes the Euclidean distance to eliminate the biased data from FAERS, thereby improving the accuracy of ADR prediction. Specifically, we first propose Pfed-Split, a method to split the original dataset into a split dataset based on ADR. Then we propose ADR-signal, an ADR prediction model, including a biased data identification method based on federated learning and an ADR prediction model based on Transformer. The former identifies the biased data according to the Euclidean distance and generates a clean dataset by deleting the biased data. The latter is an ADR prediction model based on Transformer trained on the clean data set. The results show that the ROR and PRR on the clean dataset are better than those of the traditional methods. Furthermore, the accuracy rate, F1 score, recall rate and AUC of PFed-Signal are 0.887, 0.890, 0.913 and 0.957 respectively, which are higher than the baselines.

</details>


### [153] [On the Inverse Flow Matching Problem in the One-Dimensional and Gaussian Cases](https://arxiv.org/abs/2512.23265)
*Alexander Korotin,Gudmund Pammer*

Main category: cs.LG

TL;DR: 该论文研究了具有有限指数矩的分布之间的流匹配逆问题，证明了在一维和高斯情况下的解唯一性，但多维一般情况仍为开放问题。


<details>
  <summary>Details</summary>
Motivation: 研究流匹配逆问题的动机源于现代生成式AI应用，特别是流匹配模型的蒸馏问题，需要理解在什么条件下流匹配的解是唯一的。

Method: 论文研究了具有有限指数矩的分布之间的流匹配逆问题，通过理论分析建立了两种特殊情况下的唯一性：一维设置和高斯分布情况。

Result: 证明了在一维情况和高斯情况下流匹配逆问题解的唯一性，但发现一般的多维问题仍然未解决，需要进一步研究。

Conclusion: 流匹配逆问题在一维和高斯情况下具有唯一解，但一般多维情况仍为开放问题，为未来研究指明了方向。

Abstract: This paper studies the inverse problem of flow matching (FM) between distributions with finite exponential moment, a problem motivated by modern generative AI applications such as the distillation of flow matching models. Uniqueness of the solution is established in two cases - the one-dimensional setting and the Gaussian case. The general multidimensional problem remains open for future studies.

</details>


### [154] [Spectral Analysis of Hard-Constraint PINNs: The Spatial Modulation Mechanism of Boundary Functions](https://arxiv.org/abs/2512.23295)
*Yuchen Xie,Honghang Chi,Haopeng Quan,Yahui Wang,Wei Wang,Yu Ma*

Main category: cs.LG

TL;DR: 该论文建立了硬约束物理信息神经网络的理论框架，揭示了边界函数作为谱滤波器的作用，并提出了基于有效秩的收敛性预测指标。


<details>
  <summary>Details</summary>
Motivation: 硬约束PINNs通过试函数严格强制执行边界条件，但其训练动力学理论机制尚未被探索。与软约束方法不同，硬约束中的边界函数引入了乘法空间调制，需要理论分析其影响。

Method: 建立了HC-PINNs的神经正切核框架，推导了显式核组合定律。通过谱分析识别残差核的有效秩作为训练收敛的确定性预测指标。验证了边界函数可能引发谱塌缩的问题。

Result: 边界函数B(x)作为谱滤波器，重塑神经网络原生核的特征谱。有效秩比经典条件数更能预测训练收敛。常用边界函数可能无意中导致谱塌缩，造成优化停滞。

Conclusion: 该框架将边界函数设计从启发式选择转变为原则性谱优化问题，为科学机器学习中的几何硬约束提供了坚实的理论基础。

Abstract: Physics-Informed Neural Networks with hard constraints (HC-PINNs) are increasingly favored for their ability to strictly enforce boundary conditions via a trial function ansatz $\tilde{u} = A + B \cdot N$, yet the theoretical mechanisms governing their training dynamics have remained unexplored.
  Unlike soft-constrained formulations where boundary terms act as additive penalties, this work reveals that the boundary function $B$ introduces a multiplicative spatial modulation that fundamentally alters the learning landscape.
  A rigorous Neural Tangent Kernel (NTK) framework for HC-PINNs is established, deriving the explicit kernel composition law.
  This relationship demonstrates that the boundary function $B(\vec{x})$ functions as a spectral filter, reshaping the eigenspectrum of the neural network's native kernel.
  Through spectral analysis, the effective rank of the residual kernel is identified as a deterministic predictor of training convergence, superior to classical condition numbers.
  It is shown that widely used boundary functions can inadvertently induce spectral collapse, leading to optimization stagnation despite exact boundary satisfaction.
  Validated across multi-dimensional benchmarks, this framework transforms the design of boundary functions from a heuristic choice into a principled spectral optimization problem, providing a solid theoretical foundation for geometric hard constraints in scientific machine learning.

</details>


### [155] [Splitwise: Collaborative Edge-Cloud Inference for LLMs via Lyapunov-Assisted DRL](https://arxiv.org/abs/2512.23310)
*Abolfazl Younesi,Abbas Shabrang Maryan,Elyas Oustad,Zahra Najafabadi Samani,Mohsen Ansari,Thomas Fahringer*

Main category: cs.LG

TL;DR: Splitwise是一个基于Lyapunov辅助深度强化学习的框架，用于在边缘和云环境中对大型语言模型进行细粒度自适应分区，以降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署大型语言模型面临内存和功耗限制，纯云推理延迟高成本大，静态分区无法适应带宽波动，需要自适应解决方案。

Method: 将Transformer层分解为注意力头和前馈子块，采用Lyapunov优化的分层深度强化学习策略，联合优化延迟、能耗和精度损失，保证队列稳定性，并包含分区检查点和指数退避恢复机制。

Result: 在Jetson Orin NX、Galaxy S23和Raspberry Pi 5上测试GPT-2、LLaMA-7B和LLaMA-13B，相比现有分区器降低端到端延迟1.4-2.8倍，能耗降低达41%，相比纯云执行降低95百分位延迟53-61%。

Conclusion: Splitwise通过细粒度自适应分区有效解决了边缘设备部署LLM的挑战，在延迟、能耗和精度间取得良好平衡，具有实际部署价值。

Abstract: Deploying large language models (LLMs) on edge devices is challenging due to their limited memory and power resources. Cloud-only inference reduces device burden but introduces high latency and cost. Static edge-cloud partitions optimize a single metric and struggle when bandwidth fluctuates. We propose Splitwise, a novel Lyapunov-assisted deep reinforcement learning (DRL) framework for fine-grained, adaptive partitioning of LLMs across edge and cloud environments. Splitwise decomposes transformer layers into attention heads and feed-forward sub-blocks, exposing more partition choices than layer-wise schemes. A hierarchical DRL policy, guided by Lyapunov optimization, jointly minimizes latency, energy consumption, and accuracy degradation while guaranteeing queue stability under stochastic workloads and variable network bandwidth. Splitwise also guarantees robustness via partition checkpoints with exponential backoff recovery in case of communication failures. Experiments on Jetson Orin NX, Galaxy S23, and Raspberry Pi 5 with GPT-2 (1.5B), LLaMA-7B, and LLaMA-13B show that Splitwise reduces end-to-end latency by 1.4x-2.8x and cuts energy consumption by up to 41% compared with existing partitioners. It lowers the 95th-percentile latency by 53-61% relative to cloud-only execution, while maintaining accuracy and modest memory requirements.

</details>


### [156] [Deep learning for pedestrians: backpropagation in Transformers](https://arxiv.org/abs/2512.23329)
*Laurent Boué*

Main category: cs.LG

TL;DR: 本文延续前作，将向量化无索引方法应用于Transformer架构，手动推导了嵌入层、多头自注意力、层归一化等组件的反向传播梯度，并提供了完整的最小化GPT网络PyTorch实现。


<details>
  <summary>Details</summary>
Motivation: 尽管已有自动微分工具，但手动推导反向传播能加深对前向传播值如何影响最终输出的理解，揭示操作间的依赖关系，建立对Transformer架构梯度流的直观认识。

Method: 采用轻量级无索引方法，将向量化推导技术应用于Transformer架构的关键组件：嵌入层、多头自注意力机制、层归一化，并推导了LoRA层的梯度表达式以说明参数高效微调。

Result: 提供了Transformer架构中所有关键组件的完整梯度表达式，包括嵌入层、多头自注意力、层归一化以及LoRA层的梯度更新公式，并给出了最小化GPT网络的完整PyTorch实现。

Conclusion: 手动推导Transformer架构的反向传播不仅具有教学价值，还能帮助研究者深入理解梯度流动机制，为优化模型架构和训练过程提供理论基础，同时展示了参数高效微调技术的数学基础。

Abstract: This document is a follow-up to our previous paper dedicated to a vectorized derivation of backpropagation in CNNs. Following the same principles and notations already put in place there, we now focus on transformer-based next-token-prediction architectures. To this end, we apply our lightweight index-free methodology to new types of layers such as embedding, multi-headed self-attention and layer normalization. In addition, we also provide gradient expressions for LoRA layers to illustrate parameter-efficient fine-tuning. Why bother doing manual backpropagation when there are so many tools that do this automatically? Any gap in understanding of how values propagate forward will become evident when attempting to differentiate the loss function. By working through the backward pass manually, we gain a deeper intuition for how each operation influences the final output. A complete PyTorch implementation of a minimalistic GPT-like network is also provided along with analytical expressions for of all of its gradient updates.

</details>


### [157] [The Law of Multi-Model Collaboration: Scaling Limits of Model Ensembling for Large Language Models](https://arxiv.org/abs/2512.23340)
*Dakuan Lu,Jiaqi Zhang,Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了多模型协作定律，预测LLM集成性能随总参数量呈幂律缩放，多模型协作比单模型缩放具有更显著的性能提升趋势和更低的理论损失下限。


<details>
  <summary>Details</summary>
Motivation: 单个大语言模型的能力存在固有边界，而多模型集成技术（如模型路由和后集成）虽然快速发展，但缺乏统一的理论框架来预测多模型协作的性能扩展规律。

Method: 提出多模型协作定律，采用方法无关的公式化方法，假设理想化的集成预言机，其中每个样本的总交叉熵损失由模型池中任何模型的最小损失决定，通过实验验证多模型系统的幂律缩放特性。

Result: 多模型系统相对于总参数量遵循幂律缩放，相比单模型缩放表现出更显著的改进趋势和更低的理论损失下限；异构模型家族的集成比同构模型家族获得更好的性能缩放，表明模型多样性是协作增益的主要驱动力。

Conclusion: 模型协作代表了扩展LLM智能前沿的关键维度，多模型协作定律为理解和预测集成系统性能提供了理论框架。

Abstract: Recent advances in large language models (LLMs) have been largely driven by scaling laws for individual models, which predict performance improvements as model parameters and data volume increase. However, the capabilities of any single LLM are inherently bounded. One solution originates from intricate interactions among multiple LLMs, rendering their collective performance surpasses that of any constituent model. Despite the rapid proliferation of multi-model integration techniques such as model routing and post-hoc ensembling, a unifying theoretical framework of performance scaling for multi-model collaboration remains absent. In this work, we propose the Law of Multi-model Collaboration, a scaling law that predicts the performance limits of LLM ensembles based on their aggregated parameter budget. To quantify the intrinsic upper bound of multi-model collaboration, we adopt a method-agnostic formulation and assume an idealized integration oracle where the total cross-entropy loss of each sample is determined by the minimum loss of any model in the model pool. Experimental results reveal that multi-model systems follow a power-law scaling with respect to the total parameter count, exhibiting a more significant improvement trend and a lower theoretical loss floor compared to single model scaling. Moreover, ensembles of heterogeneous model families achieve better performance scaling than those formed within a single model family, indicating that model diversity is a primary driver of collaboration gains. These findings suggest that model collaboration represents a critical axis for extending the intelligence frontier of LLMs.

</details>


### [158] [ECG-RAMBA: Zero-Shot ECG Generalization by Morphology-Rhythm Disentanglement and Long-Range Modeling](https://arxiv.org/abs/2512.23347)
*Hai Duong Nguyen,Xuan-The Tran*

Main category: cs.LG

TL;DR: ECG-RAMBA：通过分离心电图的形态和节律特征，再通过上下文感知融合重新整合，提升心电图分类的跨域泛化能力


<details>
  <summary>Details</summary>
Motivation: 深度学习在心电图分类中表现良好，但跨异构采集设置的泛化能力不足，阻碍临床部署和纵向监测。现有模型常将形态波形模式和节律动态隐式纠缠，导致捷径学习和分布偏移敏感性增强

Method: 1) 使用MiniRocket提取确定性形态特征；2) 从心率变异性计算全局节律描述符；3) 通过双向Mamba骨干进行长程上下文建模；4) 引入数值稳定的Power Mean池化算子(Q=3)增强对短暂异常的敏感性

Result: 在Chapman-Shaoxing数据集上达到约0.85的宏观ROC-AUC；在零样本迁移中，在CPSC-2021数据集上获得0.708的PR-AUC用于房颤检测，显著优于原始信号Mamba基线；在PTB-XL上显示一致的跨数据集性能

Conclusion: 确定性形态特征提供坚实基础，显式节律建模和长程上下文是跨域鲁棒性的关键驱动因素。ECG-RAMBA框架通过分离和重新整合形态与节律特征，有效提升了心电图分类的泛化能力

Abstract: Deep learning has achieved strong performance for electrocardiogram (ECG) classification within individual datasets, yet dependable generalization across heterogeneous acquisition settings remains a major obstacle to clinical deployment and longitudinal monitoring. A key limitation of many model architectures is the implicit entanglement of morphological waveform patterns and rhythm dynamics, which can promote shortcut learning and amplify sensitivity to distribution shifts. We propose ECG-RAMBA, a framework that separates morphology and rhythm and then re-integrates them through context-aware fusion. ECG-RAMBA combines: (i) deterministic morphological features extracted by MiniRocket, (ii) global rhythm descriptors computed from heart-rate variability (HRV), and (iii) long-range contextual modeling via a bi-directional Mamba backbone. To improve sensitivity to transient abnormalities under windowed inference, we introduce a numerically stable Power Mean pooling operator ($Q=3$) that emphasizes high-evidence segments while avoiding the brittleness of max pooling and the dilution of averaging. We evaluate under a protocol-faithful setting with subject-level cross-validation, a fixed decision threshold, and no test-time adaptation. On the Chapman--Shaoxing dataset, ECG-RAMBA achieves a macro ROC-AUC $\approx 0.85$. In zero-shot transfer, it attains PR-AUC $=0.708$ for atrial fibrillation detection on the external CPSC-2021 dataset, substantially outperforming a comparable raw-signal Mamba baseline, and shows consistent cross-dataset performance on PTB-XL. Ablation studies indicate that deterministic morphology provides a strong foundation, while explicit rhythm modeling and long-range context are critical drivers of cross-domain robustness.

</details>


### [159] [ISOPO: Proximal policy gradients without pi-old](https://arxiv.org/abs/2512.23353)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: ISOPO是一种高效的单步梯度方法，用于近似自然策略梯度，相比现有方法需要多步梯度和重要性采样裁剪，ISOPO通过归一化对数概率梯度或基于神经正切核变换优势函数来实现。


<details>
  <summary>Details</summary>
Motivation: 现有近端策略优化方法（如GRPO或CISPO）需要多步梯度计算和重要性采样裁剪来近似自然梯度步长，计算效率较低。ISOPO旨在开发一种更高效的单一梯度步长方法来近似自然策略梯度。

Method: ISOPO有两种变体：1）在Fisher度量下归一化每个序列的对数概率梯度，然后与优势函数进行收缩；2）基于每层的神经正切核变换微批次优势函数。ISOPO在单次反向传播中逐层应用这些变换，计算开销几乎与标准REINFORCE相同。

Result: ISOPO能够以单步梯度计算有效近似自然策略梯度，相比现有方法显著提高了计算效率，同时保持了与REINFORCE相当的计算开销。

Conclusion: ISOPO提供了一种高效的单步梯度方法来近似自然策略梯度，通过归一化梯度或基于神经正切核的优势变换，在保持计算效率的同时改进了策略优化过程。

Abstract: This note introduces Isometric Policy Optimization (ISOPO), an efficient method to approximate the natural policy gradient in a single gradient step. In comparison, existing proximal policy methods such as GRPO or CISPO use multiple gradient steps with variants of importance ratio clipping to approximate a natural gradient step relative to a reference policy. In its simplest form, ISOPO normalizes the log-probability gradient of each sequence in the Fisher metric before contracting with the advantages. Another variant of ISOPO transforms the microbatch advantages based on the neural tangent kernel in each layer. ISOPO applies this transformation layer-wise in a single backward pass and can be implemented with negligible computational overhead compared to vanilla REINFORCE.

</details>


### [160] [Post-Training Quantization of OpenPangu Models for Efficient Deployment on Atlas A2](https://arxiv.org/abs/2512.23367)
*Yilun Luo,HuaQing Zheng,Haoqian Meng,Wenyuan Liu,Peng Zhang*

Main category: cs.LG

TL;DR: 华为openPangu-Embedded模型通过低比特量化技术（INT8和W4A8）优化推理效率，在Ascend NPU上实现高效CoT推理，保持90%以上FP16精度，获得1.5倍预填充加速。


<details>
  <summary>Details</summary>
Motivation: openPangu-Embedded模型的三种CoT推理模式（slow_think、auto_think、no_think）虽然增强了推理能力，但生成长推理轨迹导致显著的内存和延迟开销，在Ascend NPU上实际部署面临挑战。

Method: 采用低比特量化技术，将FP16计算转换为更高效的整数运算。提出统一低比特推理框架，支持INT8（W8A8）和W4A8量化，专门针对Atlas A2上的openPangu-Embedded模型进行优化。

Result: 在代码生成基准测试（HumanEval和MBPP）上，INT8量化保持90%以上FP16基线精度，在Atlas A2上实现1.5倍预填充加速。W4A8量化显著降低内存消耗，但精度有所折衷。

Conclusion: 低比特量化能有效促进Ascend NPU上的高效CoT推理，同时保持高模型保真度，为实际部署提供了可行的解决方案。

Abstract: Huawei's openPangu-Embedded-1B and openPangu-Embedded-7B, variants of the openPangu large language model, integrate three distinct Chain-of-Thought (CoT) reasoning paradigms, namely slow_think, auto_think, and no_think. While these CoT modes enhance reasoning capabilities, their generation of extended reasoning traces introduces substantial memory and latency overheads, posing challenges for practical deployment on Ascend NPUs. This paper addresses these computational constraints by leveraging low-bit quantization, which transforms FP16 computations into more efficient integer arithmetic. We introduce a unified low-bit inference framework, supporting INT8 (W8A8) and W4A8 quantization, specifically optimized for openPangu-Embedded models on the Atlas A2. Our comprehensive evaluation, conducted across all three CoT modes on code generation benchmarks (HumanEval and MBPP), demonstrates the efficacy of this approach. INT8 quantization consistently preserves over 90\% of the FP16 baseline accuracy and achieves a 1.5x prefill speedup on the Atlas A2. Furthermore, W4A8 quantization significantly reduces memory consumption, albeit with a moderate trade-off in accuracy. These findings collectively indicate that low-bit quantization effectively facilitates efficient CoT reasoning on Ascend NPUs, maintaining high model fidelity.

</details>


### [161] [Diffusion priors enhanced velocity model building from time-lag images using a neural operator](https://arxiv.org/abs/2512.23375)
*Xiao Ma,Mohammad Hasyim Taufik,Tariq Alkhalifah*

Main category: cs.LG

TL;DR: 提出结合生成模型与神经算子的新框架，用于高效构建高分辨率速度模型，通过神经算子作为前向映射算子生成RTM图像，并利用生成模型作为正则化器提升分辨率。


<details>
  <summary>Details</summary>
Motivation: 传统速度模型构建方法计算成本高、耗时，而深度学习特别是生成模型和神经算子的发展为解决这些限制提供了新途径。

Method: 提出结合生成模型与神经算子的框架：1) 神经算子作为前向映射算子，从真实速度和偏移速度快速生成RTM扩展图像；2) 通过自动微分逐步更新偏移速度；3) 嵌入在真实速度模型分布上训练的生成模型作为正则化器，提升分辨率。

Result: 合成数据和实际数据实验都证明了所提出的基于生成神经算子的速度模型构建方法的有效性，能够获得更清晰、更高分辨率的速度模型。

Conclusion: 该框架成功结合了生成模型和神经算子的优势，能够高效构建高分辨率速度模型，为解决传统方法计算成本高的问题提供了有前景的解决方案。

Abstract: Velocity model building serves as a crucial component for achieving high precision subsurface imaging. However, conventional velocity model building methods are often computationally expensive and time consuming. In recent years, with the rapid advancement of deep learning, particularly the success of generative models and neural operators, deep learning based approaches that integrate data and their statistics have attracted increasing attention in addressing the limitations of traditional methods. In this study, we propose a novel framework that combines generative models with neural operators to obtain high resolution velocity models efficiently. Within this workflow, the neural operator functions as a forward mapping operator to rapidly generate time lag reverse time migration (RTM) extended images from the true and migration velocity models. In this framework, the neural operator is acting as a surrogate for modeling followed by migration, which uses the true and migration velocities, respectively. The trained neural operator is then employed, through automatic differentiation, to gradually update the migration velocity placed in the true velocity input channel with high resolution components so that the output of the network matches the time lag images of observed data obtained using the migration velocity. By embedding a generative model, trained on a high-resolution velocity model distribution, which corresponds to the true velocity model distribution used to train the neural operator, as a regularizer, the resulting predictions are cleaner with higher resolution information. Both synthetic and field data experiments demonstrate the effectiveness of the proposed generative neural operator based velocity model building approach.

</details>


### [162] [A unified framework for detecting point and collective anomalies in operating system logs via collaborative transformers](https://arxiv.org/abs/2512.23380)
*Mohammad Nasirzadeh,Jafar Tahmoresnezhad,Parviz Rashidi-Khazaee*

Main category: cs.LG

TL;DR: CoLog是一个用于日志异常检测的多模态协作框架，通过协同Transformer和多头注意力机制处理日志的不同模态，在多个基准数据集上达到99.6%以上的F1分数。


<details>
  <summary>Details</summary>
Motivation: 日志数据包含多种模态信息，但现有单模态方法忽略了这种多模态特性，而多模态方法又未能有效处理模态间的交互关系，导致日志异常检测效果受限。

Method: 提出CoLog框架，采用协同Transformer和多头注意力机制学习不同日志模态间的交互；引入模态适配层处理模态异构性；能够同时检测点异常和集体异常。

Result: 在7个日志异常检测基准数据集上，CoLog达到平均精确率99.63%、召回率99.59%、F1分数99.61%，优于现有最先进方法。

Conclusion: CoLog通过统一框架有效解决日志多模态交互问题，显著提升了异常检测性能，为网络安全、系统监控和运维效率提供了先进解决方案。

Abstract: Log anomaly detection is crucial for preserving the security of operating systems. Depending on the source of log data collection, various information is recorded in logs that can be considered log modalities. In light of this intuition, unimodal methods often struggle by ignoring the different modalities of log data. Meanwhile, multimodal methods fail to handle the interactions between these modalities. Applying multimodal sentiment analysis to log anomaly detection, we propose CoLog, a framework that collaboratively encodes logs utilizing various modalities. CoLog utilizes collaborative transformers and multi-head impressed attention to learn interactions among several modalities, ensuring comprehensive anomaly detection. To handle the heterogeneity caused by these interactions, CoLog incorporates a modality adaptation layer, which adapts the representations from different log modalities. This methodology enables CoLog to learn nuanced patterns and dependencies within the data, enhancing its anomaly detection capabilities. Extensive experiments demonstrate CoLog's superiority over existing state-of-the-art methods. Furthermore, in detecting both point and collective anomalies, CoLog achieves a mean precision of 99.63%, a mean recall of 99.59%, and a mean F1 score of 99.61% across seven benchmark datasets for log-based anomaly detection. The comprehensive detection capabilities of CoLog make it highly suitable for cybersecurity, system monitoring, and operational efficiency. CoLog represents a significant advancement in log anomaly detection, providing a sophisticated and effective solution to point and collective anomaly detection through a unified framework and a solution to the complex challenges automatic log data analysis poses. We also provide the implementation of CoLog at https://github.com/NasirzadehMoh/CoLog.

</details>


### [163] [Task-driven Heterophilic Graph Structure Learning](https://arxiv.org/abs/2512.23406)
*Ayushman Raghuvanshi,Gonzalo Mateos,Sundeep Prabhakar Chepuri*

Main category: cs.LG

TL;DR: 提出FgGSL框架，通过频率引导的图结构学习解决异配图上的GNN性能问题，联合学习同配和异配图结构，在六个异配基准上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统GNN在异配图上表现不佳，因为连接节点往往具有不同标签，特征相似性提供的结构线索较弱。需要开发能够同时处理同配和异配关系的图学习框架。

Method: 提出频率引导图结构学习(FgGSL)框架：1) 使用可学习的对称特征驱动掩码函数推断互补图结构；2) 使用预设计的低通和高通图滤波器组处理；3) 引入基于标签的结构损失显式促进同配和异配边的恢复；4) 推导结构损失的稳定性界限和滤波器组在图扰动下的鲁棒性保证。

Result: 在六个异配基准测试中，FgGSL一致优于最先进的GNN和图重连方法，证明了将频率信息与监督拓扑推断相结合的优势。

Conclusion: FgGSL通过联合学习同配和异配图结构，结合频率信息和监督拓扑推断，有效解决了异配图上的表示学习问题，为图神经网络在复杂图结构上的应用提供了新思路。

Abstract: Graph neural networks (GNNs) often struggle to learn discriminative node representations for heterophilic graphs, where connected nodes tend to have dissimilar labels and feature similarity provides weak structural cues. We propose frequency-guided graph structure learning (FgGSL), an end-to-end graph inference framework that jointly learns homophilic and heterophilic graph structures along with a spectral encoder. FgGSL employs a learnable, symmetric, feature-driven masking function to infer said complementary graphs, which are processed using pre-designed low- and high-pass graph filter banks. A label-based structural loss explicitly promotes the recovery of homophilic and heterophilic edges, enabling task-driven graph structure learning. We derive stability bounds for the structural loss and establish robustness guarantees for the filter banks under graph perturbations. Experiments on six heterophilic benchmarks demonstrate that FgGSL consistently outperforms state-of-the-art GNNs and graph rewiring methods, highlighting the benefits of combining frequency information with supervised topology inference.

</details>


### [164] [Theoretical Foundations of Scaling Law in Familial Models](https://arxiv.org/abs/2512.23407)
*Huan Song,Qingfei Zhao,Ting Long,Shuyu Tian,Hongjun An,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 该论文将神经缩放定律扩展到家族模型范式，引入粒度(G)作为新的缩放变量，建立了统一的L(N, D, G)函数形式，证明粒度惩罚遵循指数极小的乘性幂律，验证了"一次训练，多次部署"的可行性。


<details>
  <summary>Details</summary>
Motivation: 传统神经缩放定律仅考虑单一密集模型输出，忽略了家族模型这种在异构设备-边缘-云层次结构中实现普适智能的关键范式。家族模型通过早期退出和接力式推理，从单一共享主干生成多个可部署子模型，需要理论扩展来支持这种"一次运行，多个模型"的范式。

Method: 提出将粒度(G)作为模型大小(N)和训练标记(D)之外的基本缩放变量，建立统一的函数形式L(N, D, G)。采用严格的IsoFLOP实验设计，在固定计算预算下系统扫描模型大小和粒度，动态调整标记数量，从而解耦粒度边际成本与规模效益，实现高保真参数化。

Result: 研究发现粒度惩罚遵循乘性幂律，且指数极小。这理论上连接了固定计算训练与动态架构，实践上验证了"训练一次，部署多次"的范式，表明在不损害密集基线计算最优性的前提下实现部署灵活性是可行的。

Conclusion: 成功将神经缩放定律扩展到家族模型范式，建立了包含粒度变量的统一缩放定律，证明了部署灵活性可以与计算最优性共存，为异构设备环境中的高效模型部署提供了理论基础。

Abstract: Neural scaling laws have become foundational for optimizing large language model (LLM) training, yet they typically assume a single dense model output. This limitation effectively overlooks "Familial models, a transformative paradigm essential for realizing ubiquitous intelligence across heterogeneous device-edge-cloud hierarchies. Transcending static architectures, familial models integrate early exits with relay-style inference to spawn G deployable sub-models from a single shared backbone. In this work, we theoretically and empirically extend the scaling law to capture this "one-run, many-models" paradigm by introducing Granularity (G) as a fundamental scaling variable alongside model size (N) and training tokens (D). To rigorously quantify this relationship, we propose a unified functional form L(N, D, G) and parameterize it using large-scale empirical runs. Specifically, we employ a rigorous IsoFLOP experimental design to strictly isolate architectural impact from computational scale. Across fixed budgets, we systematically sweep model sizes (N) and granularities (G) while dynamically adjusting tokens (D). This approach effectively decouples the marginal cost of granularity from the benefits of scale, ensuring high-fidelity parameterization of our unified scaling law. Our results reveal that the granularity penalty follows a multiplicative power law with an extremely small exponent. Theoretically, this bridges fixed-compute training with dynamic architectures. Practically, it validates the "train once, deploy many" paradigm, demonstrating that deployment flexibility is achievable without compromising the compute-optimality of dense baselines.

</details>


### [165] [Directly Constructing Low-Dimensional Solution Subspaces in Deep Neural Networks](https://arxiv.org/abs/2512.23410)
*Yusuf Kalyoncuoglu*

Main category: cs.LG

TL;DR: 该论文提出了一种构造性子空间方法，通过解耦解几何与搜索空间，实现神经网络分类头的巨大压缩（16倍），并引入子空间原生蒸馏新范式。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的深度神经网络虽然权重矩阵和特征流形具有低内在维度，但仍依赖大规模高维宽度。这种冗余不是表示所必需的，而是为了解决非凸优化搜索问题（寻找全局最小值），这对于紧凑网络来说仍然难以处理。

Method: 提出构造性方法绕过优化瓶颈：通过解耦解几何与环境搜索空间，在ResNet-50、ViT和BERT上实证展示分类头可压缩16倍且性能几乎不降。引入子空间原生蒸馏新范式，直接在构造的子空间中定义目标，为学生模型提供稳定的几何坐标系。

Result: 实验证明分类头可压缩高达16倍而性能几乎不下降，为"训练大模型、部署小模型"愿景提供了实现路径。

Conclusion: 通过构造性子空间方法，可以绕过传统高维优化搜索问题，实现神经网络的高效压缩，子空间原生蒸馏为学生模型提供了稳定的几何坐标系，有望实现"训练大、部署小"的愿景。

Abstract: While it is well-established that the weight matrices and feature manifolds of deep neural networks exhibit a low Intrinsic Dimension (ID), current state-of-the-art models still rely on massive high-dimensional widths. This redundancy is not required for representation, but is strictly necessary to solve the non-convex optimization search problem-finding a global minimum, which remains intractable for compact networks. In this work, we propose a constructive approach to bypass this optimization bottleneck. By decoupling the solution geometry from the ambient search space, we empirically demonstrate across ResNet-50, ViT, and BERT that the classification head can be compressed by even huge factors of 16 with negligible performance degradation. This motivates Subspace-Native Distillation as a novel paradigm: by defining the target directly in this constructed subspace, we provide a stable geometric coordinate system for student models, potentially allowing them to circumvent the high-dimensional search problem entirely and realize the vision of Train Big, Deploy Small.

</details>


### [166] [Stochastic Siamese MAE Pretraining for Longitudinal Medical Images](https://arxiv.org/abs/2512.23441)
*Taha Emre,Arunava Chakravarty,Thomas Pinetz,Dmitrii Lachinov,Martin J. Menten,Hendrik Scholl,Sobha Sivaprasad,Daniel Rueckert,Andrew Lotery,Stefan Sacu,Ursula Schmidt-Erfurth,Hrvoje Bogunović*

Main category: cs.LG

TL;DR: STAMP是一个基于Siamese MAE框架的随机时间自编码器，通过条件变分推理学习医学影像中的非确定性时间动态，在AMD和AD疾病进展预测中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法（如MAE）虽然具有强大的表示学习能力，但缺乏时间感知能力，无法捕捉医学影像纵向数据中的疾病进展动态。确定性方法无法处理疾病演变中的固有不确定性。

Method: 提出STAMP框架：基于Siamese MAE架构，通过随机过程编码时间信息，以两个输入体积之间的时间差为条件。将MAE重建损失重构为条件变分推理目标，从而随机学习时间动态。

Result: 在两个OCT数据集和一个MRI数据集上评估，STAMP预训练的ViT模型在晚期年龄相关性黄斑变性和阿尔茨海默病进展预测任务上，优于现有时间MAE方法和基础模型。

Conclusion: STAMP能够有效学习医学影像中的非确定性时间动态，为捕捉疾病进展提供了更好的时间感知表示学习方法。

Abstract: Temporally aware image representations are crucial for capturing disease progression in 3D volumes of longitudinal medical datasets. However, recent state-of-the-art self-supervised learning approaches like Masked Autoencoding (MAE), despite their strong representation learning capabilities, lack temporal awareness. In this paper, we propose STAMP (Stochastic Temporal Autoencoder with Masked Pretraining), a Siamese MAE framework that encodes temporal information through a stochastic process by conditioning on the time difference between the 2 input volumes. Unlike deterministic Siamese approaches, which compare scans from different time points but fail to account for the inherent uncertainty in disease evolution, STAMP learns temporal dynamics stochastically by reframing the MAE reconstruction loss as a conditional variational inference objective. We evaluated STAMP on two OCT and one MRI datasets with multiple visits per patient. STAMP pretrained ViT models outperformed both existing temporal MAE methods and foundation models on different late stage Age-Related Macular Degeneration and Alzheimer's Disease progression prediction which require models to learn the underlying non-deterministic temporal dynamics of the diseases.

</details>


### [167] [Dynamic Subspace Composition: Efficient Adaptation via Contractive Basis Expansion](https://arxiv.org/abs/2512.23448)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: 提出Dynamic Subspace Composition (DSC)框架，通过状态依赖的稀疏基向量组合近似上下文相关权重，解决MoE模型中的表示坍塌和梯度不稳定问题，显著降低参数复杂度和内存开销。


<details>
  <summary>Details</summary>
Motivation: 混合专家(MoE)模型虽然能扩展容量，但常面临表示坍塌和梯度不稳定问题。现有方法如Mixture-of-LoRAs参数复杂度高，需要检索独立的低秩矩阵，效率较低。

Method: DSC框架将权重更新建模为星形域内的残差轨迹，使用幅度门控单纯形插值确保在单位矩阵处的连续性。通过解耦的单位范数基向量构建组合式秩K近似，而非检索独立的秩r矩阵。

Result: 将参数复杂度从O(M rd)降低到O(M d)，内存流量减少到O(Kd)。通过框架理论正则化和谱约束提供动态更新的严格最坏情况边界。

Conclusion: DSC提供了一种高效、稳定的动态权重更新方法，解决了MoE模型的表示坍塌和梯度不稳定问题，同时显著降低了计算和内存开销。

Abstract: Mixture of Experts (MoE) models scale capacity but often suffer from representation collapse and gradient instability. We propose Dynamic Subspace Composition (DSC), a framework that approximates context-dependent weights via a state-dependent, sparse expansion of a shared basis bank. Formally, DSC models the weight update as a residual trajectory within a Star- Shaped Domain, employing a Magnitude-Gated Simplex Interpolation to ensure continuity at the identity. Unlike standard Mixture-of-LoRAs, which incurs O(M rd) parameter complexity by retrieving independent rank-r matrices, DSC constructs a compositional rank-K approximation from decoupled unit-norm basis vectors. This reduces parameter complexity to O(M d) and memory traffic to O(Kd), while Frame-Theoretic regularization and spectral constraints provide rigorous worst-case bounds on the dynamic update. The code is available at https://github. com/VladimerKhasia/DSC

</details>


### [168] [Eliminating Inductive Bias in Reward Models with Information-Theoretic Guidance](https://arxiv.org/abs/2512.23461)
*Zhuo Li,Pengyu Cheng,Zhechao Yu,Feifei Tong,Anningzhe Gao,Tsung-Hui Chang,Xiang Wan,Erchao Zhao,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.LG

TL;DR: 提出DIR方法，通过信息优化来消除奖励模型中的归纳偏差，能处理非线性相关偏差并提升RLHF性能


<details>
  <summary>Details</summary>
Motivation: 奖励模型训练数据质量低，包含各种归纳偏差（如响应长度、迎合性、格式等），现有方法只能处理单一偏差或简单线性相关，无法应对复杂多样的偏差问题

Method: 提出基于信息瓶颈的DIR方法：最大化奖励模型分数与人类偏好对之间的互信息，同时最小化奖励模型输出与偏好输入中偏差属性之间的互信息

Result: DIR能有效消除响应长度、迎合性和格式三种偏差，提升RLHF在各种基准测试中的性能，增强泛化能力

Conclusion: DIR通过信息理论方法有效处理复杂非线性相关偏差，扩展了奖励模型去偏方法在实际应用中的适用性

Abstract: Reward models (RMs) are essential in reinforcement learning from human feedback (RLHF) to align large language models (LLMs) with human values. However, RM training data is commonly recognized as low-quality, containing inductive biases that can easily lead to overfitting and reward hacking. For example, more detailed and comprehensive responses are usually human-preferred but with more words, leading response length to become one of the inevitable inductive biases. A limited number of prior RM debiasing approaches either target a single specific type of bias or model the problem with only simple linear correlations, \textit{e.g.}, Pearson coefficients. To mitigate more complex and diverse inductive biases in reward modeling, we introduce a novel information-theoretic debiasing method called \textbf{D}ebiasing via \textbf{I}nformation optimization for \textbf{R}M (DIR). Inspired by the information bottleneck (IB), we maximize the mutual information (MI) between RM scores and human preference pairs, while minimizing the MI between RM outputs and biased attributes of preference inputs. With theoretical justification from information theory, DIR can handle more sophisticated types of biases with non-linear correlations, broadly extending the real-world application scenarios for RM debiasing methods. In experiments, we verify the effectiveness of DIR with three types of inductive biases: \textit{response length}, \textit{sycophancy}, and \textit{format}. We discover that DIR not only effectively mitigates target inductive biases but also enhances RLHF performance across diverse benchmarks, yielding better generalization abilities. The code and training recipes are available at https://github.com/Qwen-Applications/DIR.

</details>


### [169] [FRoD: Full-Rank Efficient Fine-Tuning with Rotational Degrees for Fast Convergence](https://arxiv.org/abs/2512.23485)
*Guoan Wan,Tianyu Chen,Fangzheng Feng,Haoyi Zhou,Runhua Xu*

Main category: cs.LG

TL;DR: FRoD是一种新颖的参数高效微调方法，通过层次联合分解和旋转自由度，在仅使用1.72%可训练参数的情况下，在20个基准测试中达到全模型微调的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调方法（如LoRA）在效率和表达能力之间存在权衡，由于低秩约束导致收敛速度慢和适应能力有限，难以捕捉复杂任务模式。

Method: FRoD结合层次联合分解与旋转自由度，提取跨层的全局共享基础，并通过稀疏可学习的缩放因子扰动实现灵活的全秩更新。

Result: 在涵盖视觉、推理和语言理解的20个基准测试中，FRoD在相同训练预算下，仅使用1.72%的可训练参数就达到了全模型微调的准确率。

Conclusion: FRoD通过增强表达能力和效率，实现了更快、更稳健的收敛，为参数高效微调提供了有效的解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have emerged as a practical solution for adapting large foundation models to downstream tasks, reducing computational and memory costs by updating only a small subset of parameters. Among them, approaches like LoRA aim to strike a balance between efficiency and expressiveness, but often suffer from slow convergence and limited adaptation capacity due to their inherent low-rank constraints. This trade-off hampers the ability of PEFT methods to capture complex patterns needed for diverse tasks. To address these challenges, we propose FRoD, a novel fine-tuning method that combines hierarchical joint decomposition with rotational degrees of freedom. By extracting a globally shared basis across layers and injecting sparse, learnable perturbations into scaling factors for flexible full-rank updates, FRoD enhances expressiveness and efficiency, leading to faster and more robust convergence. On 20 benchmarks spanning vision, reasoning, and language understanding, FRoD matches full model fine-tuning in accuracy, while using only 1.72% of trainable parameters under identical training budgets.

</details>


### [170] [Joint Link Adaptation and Device Scheduling Approach for URLLC Industrial IoT Network: A DRL-based Method with Bayesian Optimization](https://arxiv.org/abs/2512.23493)
*Wei Gao,Paul Zheng,Peng Wu,Yulin Hu,Anke Schmeink*

Main category: cs.LG

TL;DR: 提出一种基于贝叶斯优化的TD3方法，用于工业物联网中多设备动态URLLC场景下的联合链路自适应和设备调度，以最大化传输速率并满足严格的误块率约束。


<details>
  <summary>Details</summary>
Motivation: 工业物联网中多设备动态超可靠低延迟通信面临信道状态信息不完美、URLLC网络中的错误样本不平衡以及TD3算法参数敏感等问题，这些问题会降低算法收敛速度和可靠性。

Method: 提出贝叶斯优化驱动的TD3方法，基于不完美的CSI自适应确定设备服务顺序和相应的调制编码方案；同时提出基于贝叶斯优化的训练机制，通过提供更可靠的学习方向和样本选择方法来改善收敛速度。

Result: 通过大量仿真验证，所提算法相比现有解决方案实现了更快的收敛速度和更高的总速率性能。

Conclusion: 提出的贝叶斯优化驱动的TD3方法有效解决了工业物联网中多设备URLLC场景下的联合链路自适应和设备调度问题，在收敛速度和总速率性能方面优于现有方法。

Abstract: In this article, we consider an industrial internet of things (IIoT) network supporting multi-device dynamic ultra-reliable low-latency communication (URLLC) while the channel state information (CSI) is imperfect. A joint link adaptation (LA) and device scheduling (including the order) design is provided, aiming at maximizing the total transmission rate under strict block error rate (BLER) constraints. In particular, a Bayesian optimization (BO) driven Twin Delayed Deep Deterministic Policy Gradient (TD3) method is proposed, which determines the device served order sequence and the corresponding modulation and coding scheme (MCS) adaptively based on the imperfect CSI. Note that the imperfection of CSI, error sample imbalance in URLLC networks, as well as the parameter sensitivity nature of the TD3 algorithm likely diminish the algorithm's convergence speed and reliability. To address such an issue, we proposed a BO based training mechanism for the convergence speed improvement, which provides a more reliable learning direction and sample selection method to track the imbalance sample problem. Via extensive simulations, we show that the proposed algorithm achieves faster convergence and higher sum-rate performance compared to existing solutions.

</details>


### [171] [EEG-based Graph-guided Domain Adaptation for Robust Cross-Session Emotion Recognition](https://arxiv.org/abs/2512.23526)
*Maryam Mirzaei,Farzaneh Shayegh,Hamed Narimani*

Main category: cs.LG

TL;DR: EGDA框架通过联合对齐全局和类别特定分布，减少跨会话差异，提升EEG情绪识别的泛化能力


<details>
  <summary>Details</summary>
Motivation: EEG情绪识别中，不同记录会话间的差异严重影响了模型的泛化能力，需要解决跨会话分布不一致的问题

Method: 提出EGDA框架，联合对齐全局（边际）和类别特定（条件）分布，同时通过图正则化保持EEG数据的固有结构

Result: 在SEED-IV数据集上，EGDA在三个迁移任务中分别达到81.22%、80.15%和83.27%的准确率，优于多个基线方法

Conclusion: EGDA能有效减少跨会话差异，Gamma频段最具判别性，中央-顶叶和前额叶脑区对情绪识别至关重要

Abstract: Accurate recognition of human emotional states is critical for effective human-machine interaction. Electroencephalography (EEG) offers a reliable source for emotion recognition due to its high temporal resolution and its direct reflection of neural activity. Nevertheless, variations across recording sessions present a major challenge for model generalization. To address this issue, we propose EGDA, a framework that reduces cross-session discrepancies by jointly aligning the global (marginal) and class-specific (conditional) distributions, while preserving the intrinsic structure of EEG data through graph regularization. Experimental results on the SEED-IV dataset demonstrate that EGDA achieves robust cross-session performance, obtaining accuracies of 81.22%, 80.15%, and 83.27% across three transfer tasks, and surpassing several baseline methods. Furthermore, the analysis highlights the Gamma frequency band as the most discriminative and identifies the central-parietal and prefrontal brain regions as critical for reliable emotion recognition.

</details>


### [172] [VL-RouterBench: A Benchmark for Vision-Language Model Routing](https://arxiv.org/abs/2512.23562)
*Zhehao Huang,Baijiong Lin,Jingyuan Zhang,Jingying Wang,Yuhang Liu,Ning Lu,Tao Li,Xiaolin Huang*

Main category: cs.LG

TL;DR: VL-RouterBench：首个系统性、可复现的视觉语言模型路由基准，覆盖14个数据集、17个模型、30,540个样本，评估10种路由方法，发现现有路由器与理想Oracle仍有明显差距。


<details>
  <summary>Details</summary>
Motivation: 多模型路由已从工程技术发展为关键基础设施，但现有研究缺乏系统性、可复现的基准来评估视觉语言模型路由系统。

Method: 基于VLMs原始推理和评分日志构建质量与成本矩阵；覆盖14个数据集、3个任务组、30,540个样本；包含15个开源模型和2个API模型；建立联合评估协议，测量平均准确率、平均成本和吞吐量，构建归一化成本与准确率的调和平均数作为排名分数。

Result: 观察到显著的路由能力增益，但当前最佳路由器与理想Oracle仍有明显差距；表明通过更精细的视觉线索和文本结构建模，路由器架构仍有很大改进空间。

Conclusion: VL-RouterBench为多模态路由研究提供了系统性评估基准，将开源完整数据构建和评估工具链，以促进可比性、可复现性和实际部署。

Abstract: Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs. In scale, VL-RouterBench covers 14 datasets across 3 task groups, totaling 30,540 samples, and includes 15 open-source models and 2 API models, yielding 519,180 sample-model pairs and a total input-output token volume of 34,494,977. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and builds a ranking score from the harmonic mean of normalized cost and accuracy to enable comparison across router configurations and cost budgets. On this benchmark, we evaluate 10 routing methods and baselines and observe a significant routability gain, while the best current routers still show a clear gap to the ideal Oracle, indicating considerable room for improvement in router architecture through finer visual cues and modeling of textual structure. We will open-source the complete data construction and evaluation toolchain to promote comparability, reproducibility, and practical deployment in multimodal routing research.

</details>


### [173] [Distribution-Free Process Monitoring with Conformal Prediction](https://arxiv.org/abs/2512.23602)
*Christopher Burger*

Main category: cs.LG

TL;DR: 提出结合传统统计过程控制与保形预测的混合框架，增强质量监控的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统SPC依赖统计假设且常被违反，在现代复杂制造环境中监控不可靠，需要更稳健的方法

Method: 集成保形预测的分布无关、模型不可知保证，提出两种应用：保形增强控制图（可视化过程不确定性）和保形增强过程监控（将多元控制重构为异常检测问题）

Result: 框架提供更鲁棒、统计严谨的质量控制方法，同时保持经典方法的可解释性和易用性

Conclusion: 通过结合保形预测，显著提升了传统SPC在现代复杂制造环境中的可靠性和实用性

Abstract: Traditional Statistical Process Control (SPC) is essential for quality management but is limited by its reliance on often violated statistical assumptions, leading to unreliable monitoring in modern, complex manufacturing environments. This paper introduces a hybrid framework that enhances SPC by integrating the distribution free, model agnostic guarantees of Conformal Prediction. We propose two novel applications: Conformal-Enhanced Control Charts, which visualize process uncertainty and enable proactive signals like 'uncertainty spikes', and Conformal-Enhanced Process Monitoring, which reframes multivariate control as a formal anomaly detection problem using an intuitive p-value chart. Our framework provides a more robust and statistically rigorous approach to quality control while maintaining the interpretability and ease of use of classic methods.

</details>


### [174] [BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization](https://arxiv.org/abs/2512.23631)
*Iris Xu,Guangtao Zeng,Zexue He,Charles Jin,Aldo Pareja,Dan Gutfreund,Chuang Gan,Zhang-Wei Hong*

Main category: cs.LG

TL;DR: BOAD框架通过多臂老虎机优化自动发现分层多智能体系统，显著提升大语言模型在真实世界软件工程任务上的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有单一智能体系统在处理长视野、分布外软件工程问题时存在局限性，需要像人类工程师那样分解复杂问题，但手动设计分层多智能体系统面临组合爆炸和信用分配挑战

Method: 提出BOAD框架，将分层发现建模为多臂老虎机问题，每个臂代表候选子智能体，奖励衡量其在团队协作中的有效性，在有限评估预算下高效探索子智能体设计

Result: 在SWE-bench-Verified上优于单一智能体和手动设计多智能体系统；在SWE-bench-Live上，36B系统在评估时排名第二，超越GPT-4和Claude等更大模型

Conclusion: 自动发现的分层多智能体系统能显著提升大语言模型在挑战性长视野软件工程任务上的泛化能力

Abstract: Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.

</details>


### [175] [End-to-End Test-Time Training for Long Context](https://arxiv.org/abs/2512.23675)
*Arnuv Tandon,Karan Dalal,Xinhao Li,Daniel Koceja,Marcel Rød,Sam Buchanan,Xiaolong Wang,Jure Leskovec,Sanmi Koyejo,Tatsunori Hashimoto,Carlos Guestrin,Jed McCaleb,Yejin Choi,Yu Sun*

Main category: cs.LG

TL;DR: 将长上下文语言建模重新定义为持续学习问题，通过测试时训练和元学习，使用标准滑动窗口注意力Transformer实现恒定推理延迟


<details>
  <summary>Details</summary>
Motivation: 传统方法通过架构设计解决长上下文建模问题，本文提出将其视为持续学习问题，让模型在测试时继续学习，从而压缩上下文信息到权重中

Method: 使用标准滑动窗口注意力Transformer架构，通过测试时训练在推理时进行下一个token预测来学习上下文，同时在训练时通过元学习优化模型初始化

Result: TTT-E2E方法在3B模型上展现出与全注意力Transformer相似的上下文长度扩展能力，同时具有恒定推理延迟（比全注意力快2.7倍），优于Mamba 2和Gated DeltaNet

Conclusion: 将长上下文建模视为持续学习问题，通过测试时训练和元学习的端到端方法，可以在保持标准架构的同时实现高效的长上下文处理

Abstract: We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model's initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.

</details>


### [176] [Training AI Co-Scientists Using Rubric Rewards](https://arxiv.org/abs/2512.23707)
*Shashwat Goel,Rishi Hazra,Dulhan Jayalath,Timon Willi,Parag Jain,William F. Shen,Ilias Leontiadis,Francesco Barbieri,Yoram Bachrach,Jonas Geiping,Chenxi Whitehouse*

Main category: cs.LG

TL;DR: 利用强化学习训练语言模型生成更好的研究计划，通过从现有论文自动提取研究目标和评分标准，实现无需人工监督的改进


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在生成符合约束和隐含要求的研究计划方面存在困难，需要开发能够更好协助人类研究者的AI合作科学家

Method: 从多领域论文自动提取研究目标和特定目标评分标准，构建训练语料库，通过强化学习结合自我评分进行模型微调

Result: 在机器学习领域，专家更偏好微调后模型生成的研究计划（70%胜率），84%的自动提取评分标准获得批准；在医学和新arXiv预印本上也显示12-22%的相对改进和跨领域泛化能力

Conclusion: 该方法展示了可扩展、自动化的训练方案潜力，是改进通用AI合作科学家的重要一步，即使在医学研究等执行反馈不可行的问题设置中也有效

Abstract: AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [177] [A review of NMF, PLSA, LBA, EMA, and LCA with a focus on the identifiability issue](https://arxiv.org/abs/2512.22282)
*Qianqian Qi,Peter G. M. van der Heijden*

Main category: stat.ML

TL;DR: 本文系统分析了五种非负矩阵分解模型（LBA、LCA、EMA、PLSA、NMF）的相似性和可识别性，证明了它们的解唯一性等价，并提供了算法综述和应用示例。


<details>
  <summary>Details</summary>
Motivation: 机器学习、社会科学、地理学等领域存在多种非负矩阵分解模型，它们虽然本质上相似或等价，但以不同名称出现且相似性未被充分认识。本文旨在揭示这些模型之间的内在联系，特别是关注可识别性这一核心问题。

Method: 通过理论分析比较五种流行模型：潜在预算分析（LBA）、潜在类别分析（LCA）、端元分析（EMA）、概率潜在语义分析（PLSA）和非负矩阵分解（NMF）。重点研究这些模型的可识别性问题，证明LBA、EMA、LCA、PLSA解唯一的充要条件是NMF解唯一。同时提供这些算法的简要综述，并以社会科学中的时间预算数据集进行实例说明。

Result: 证明了五种模型在可识别性上的等价性：LBA、EMA、LCA、PLSA的解唯一当且仅当NMF的解唯一。这一理论结果统一了不同领域中的相似模型，为跨学科研究提供了理论基础。

Conclusion: 五种非负矩阵分解模型本质上是相似或等价的，它们的可识别性条件相同。这一发现有助于消除不同领域间的术语障碍，促进模型间的知识迁移。论文还讨论了与原型分析等密切相关模型的联系，为未来研究提供了方向。

Abstract: Across fields such as machine learning, social science, geography, considerable attention has been given to models that factorize a nonnegative matrix into the product of two or three matrices, subject to nonnegative or row-sum-to-1 constraints. Although these models are to a large extend similar or even equivalent, they are presented under different names, and their similarity is not well known. This paper highlights similarities among five popular models, latent budget analysis (LBA), latent class analysis (LCA), end-member analysis (EMA), probabilistic latent semantic analysis (PLSA), and nonnegative matrix factorization (NMF). We focus on an essential issue-identifiability-of these models and prove that the solution of LBA, EMA, LCA, PLSA is unique if and only if the solution of NMF is unique. We also provide a brief review for algorithms of these models. We illustrate the models with a time budget dataset from social science, and end the paper with a discussion of closely related models such as archetypal analysis.

</details>


### [178] [On Fibonacci Ensembles: An Alternative Approach to Ensemble Learning Inspired by the Timeless Architecture of the Golden Ratio](https://arxiv.org/abs/2512.22284)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 提出基于斐波那契序列的集成学习框架，通过斐波那契权重和二阶递归动态来改进经典集成方法


<details>
  <summary>Details</summary>
Motivation: 受自然界斐波那契序列在生长、和谐与递归稳定性中普遍存在的启发，希望将这种数学原理应用于集成学习，超越传统的bagging、boosting和随机森林方法

Method: 提出斐波那契集成框架：1) 使用正交化和Rao-Blackwell优化的归一化斐波那契权重实现基学习器的系统方差减少；2) 模仿斐波那契流的二阶递归集成动态，增强表示深度

Result: 在一维回归实验中，使用随机傅里叶特征集成和多项式集成，斐波那契加权在某些情况下匹配或优于均匀平均，并能与正交Rao-Blackwell化原则性地交互

Conclusion: 斐波那契集成在集成学习理论中形成了一个自然且可解释的设计点，表明当学习系统遵循自然界内在和谐规律时能够蓬勃发展

Abstract: Nature rarely reveals her secrets bluntly, yet in the Fibonacci sequence she grants us a glimpse of her quiet architecture of growth, harmony, and recursive stability \citep{Koshy2001Fibonacci, Livio2002GoldenRatio}. From spiral galaxies to the unfolding of leaves, this humble sequence reflects a universal grammar of balance. In this work, we introduce \emph{Fibonacci Ensembles}, a mathematically principled yet philosophically inspired framework for ensemble learning that complements and extends classical aggregation schemes such as bagging, boosting, and random forests \citep{Breiman1996Bagging, Breiman2001RandomForests, Friedman2001GBM, Zhou2012Ensemble, HastieTibshiraniFriedman2009ESL}. Two intertwined formulations unfold: (1) the use of normalized Fibonacci weights -- tempered through orthogonalization and Rao--Blackwell optimization -- to achieve systematic variance reduction among base learners, and (2) a second-order recursive ensemble dynamic that mirrors the Fibonacci flow itself, enriching representational depth beyond classical boosting. The resulting methodology is at once rigorous and poetic: a reminder that learning systems flourish when guided by the same intrinsic harmonies that shape the natural world. Through controlled one-dimensional regression experiments using both random Fourier feature ensembles \citep{RahimiRecht2007RFF} and polynomial ensembles, we exhibit regimes in which Fibonacci weighting matches or improves upon uniform averaging and interacts in a principled way with orthogonal Rao--Blackwellization. These findings suggest that Fibonacci ensembles form a natural and interpretable design point within the broader theory of ensemble learning.

</details>


### [179] [A General Weighting Theory for Ensemble Learning: Beyond Variance Reduction via Spectral and Geometric Structure](https://arxiv.org/abs/2512.22286)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 该论文提出了一个超越传统方差减少解释的集成学习加权理论，解释了为什么即使对于方差已经很低的基础学习器，结构化加权也能通过重塑近似几何和重新分配谱复杂度来提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统集成学习理论主要基于方差减少来解释其有效性，但这无法解释为什么对于方差已经通过正则化控制得很好的稳定学习器（如平滑样条、核岭回归等），集成学习仍然有效。需要一个新的理论框架来解释这种现象。

Method: 将集成学习形式化为作用于假设空间的线性算子，赋予加权序列空间几何和谱约束。在此框架下推导了改进的偏差-方差近似分解，展示了非均匀结构化权重如何通过重塑近似几何和重新分配谱复杂度来超越均匀平均。主要结果提供了结构化加权优于均匀集成的条件，并表明最优权重是约束二次规划的解。

Result: 建立了结构驱动集成学习的理论基础，解释了为什么集成学习对平滑、低方差的基础学习器仍然有效。经典平均、堆叠和最近提出的基于斐波那契的集成都作为该统一理论的特例出现，该理论还容纳了几何、次指数和重尾加权律。

Conclusion: 该工作为结构驱动集成学习建立了原则性基础，解释了为什么集成学习对稳定基础学习器仍然有效，并为后续工作中开发分布自适应和动态演化的加权方案奠定了基础。

Abstract: Ensemble learning is traditionally justified as a variance-reduction strategy, explaining its strong performance for unstable predictors such as decision trees. This explanation, however, does not account for ensembles constructed from intrinsically stable estimators-including smoothing splines, kernel ridge regression, Gaussian process regression, and other regularized reproducing kernel Hilbert space (RKHS) methods whose variance is already tightly controlled by regularization and spectral shrinkage. This paper develops a general weighting theory for ensemble learning that moves beyond classical variance-reduction arguments. We formalize ensembles as linear operators acting on a hypothesis space and endow the space of weighting sequences with geometric and spectral constraints. Within this framework, we derive a refined bias-variance approximation decomposition showing how non-uniform, structured weights can outperform uniform averaging by reshaping approximation geometry and redistributing spectral complexity, even when variance reduction is negligible. Our main results provide conditions under which structured weighting provably dominates uniform ensembles, and show that optimal weights arise as solutions to constrained quadratic programs. Classical averaging, stacking, and recently proposed Fibonacci-based ensembles appear as special cases of this unified theory, which further accommodates geometric, sub-exponential, and heavy-tailed weighting laws. Overall, the work establishes a principled foundation for structure-driven ensemble learning, explaining why ensembles remain effective for smooth, low-variance base learners and setting the stage for distribution-adaptive and dynamically evolving weighting schemes developed in subsequent work.

</details>


### [180] [Gradient Dynamics of Attention: How Cross-Entropy Sculpts Bayesian Manifolds](https://arxiv.org/abs/2512.22473)
*Naman Aggarwal,Siddhartha R. Dalal,Vishal Misra*

Main category: stat.ML

TL;DR: 论文分析了Transformer注意力头中交叉熵训练如何通过梯度流塑造内部几何结构，揭示了注意力分数和值向量的更新机制，以及这种机制如何实现类似EM算法的贝叶斯推理。


<details>
  <summary>Details</summary>
Motivation: Transformer在精心设计的"贝叶斯风洞"和大规模语言模型中表现出精确的概率推理能力，但梯度学习如何创建所需内部几何结构的机制仍不明确。需要理解交叉熵训练如何重塑注意力分数和值向量。

Method: 提供注意力头中交叉熵训练的一阶分析，推导出注意力分数的"基于优势的路由定律"和值向量的"责任加权更新"方程。通过控制模拟（包括粘性马尔可夫链任务），比较闭式EM风格更新与标准SGD。

Result: 揭示了注意力权重实现E步（软责任分配），值向量实现M步（责任加权原型更新），查询和键调整假设框架。这种耦合专业化行为类似于两时间尺度EM过程，梯度动态塑造了低维流形，实现了贝叶斯推理。

Conclusion: 优化（梯度流）产生几何结构（贝叶斯流形），进而支持功能（上下文概率推理），为Transformer如何通过梯度学习实现概率推理提供了统一的理论框架。

Abstract: Transformers empirically perform precise probabilistic reasoning in carefully constructed ``Bayesian wind tunnels'' and in large-scale language models, yet the mechanisms by which gradient-based learning creates the required internal geometry remain opaque. We provide a complete first-order analysis of how cross-entropy training reshapes attention scores and value vectors in a transformer attention head. Our core result is an \emph{advantage-based routing law} for attention scores, \[ \frac{\partial L}{\partial s_{ij}} = α_{ij}\bigl(b_{ij}-\mathbb{E}_{α_i}[b]\bigr), \qquad b_{ij} := u_i^\top v_j, \] coupled with a \emph{responsibility-weighted update} for values, \[ Δv_j = -η\sum_i α_{ij} u_i, \] where $u_i$ is the upstream gradient at position $i$ and $α_{ij}$ are attention weights. These equations induce a positive feedback loop in which routing and content specialize together: queries route more strongly to values that are above-average for their error signal, and those values are pulled toward the queries that use them. We show that this coupled specialization behaves like a two-timescale EM procedure: attention weights implement an E-step (soft responsibilities), while values implement an M-step (responsibility-weighted prototype updates), with queries and keys adjusting the hypothesis frame. Through controlled simulations, including a sticky Markov-chain task where we compare a closed-form EM-style update to standard SGD, we demonstrate that the same gradient dynamics that minimize cross-entropy also sculpt the low-dimensional manifolds identified in our companion work as implementing Bayesian inference. This yields a unified picture in which optimization (gradient flow) gives rise to geometry (Bayesian manifolds), which in turn supports function (in-context probabilistic reasoning).

</details>


### [181] [Likelihood-Preserving Embeddings for Statistical Inference](https://arxiv.org/abs/2512.22638)
*Deniz Akdemir*

Main category: stat.ML

TL;DR: 该论文提出了"似然保持嵌入"理论，通过控制似然比失真度Δ_n来确保嵌入表示不改变统计推断结论，建立了Hinge定理证明Δ_n控制是保持推断的充要条件。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习嵌入虽然能有效压缩高维数据，但通常破坏了经典似然统计推断所需的几何结构。需要开发既能压缩数据又能保持推断结论的嵌入方法。

Method: 提出似然比失真度Δ_n度量嵌入引起的对数似然比最大误差；建立Hinge定理证明控制Δ_n是保持推断的充要条件；使用神经网络作为近似充分统计量，通过训练损失与推断保证的显式边界连接提供构造性框架。

Result: 证明了如果Δ_n = o_p(1)，则所有基于似然比的检验和贝叶斯因子都渐近保持，替代最大似然估计量与全数据MLE渐近等价；实验验证了指数族理论的尖锐相变预测，分布式临床推断应用展示了实际效用。

Conclusion: 提出了严格的似然保持嵌入理论，建立了控制似然比失真度是保持统计推断的关键，为在压缩表示上进行可靠推断提供了理论基础和实用框架。

Abstract: Modern machine learning embeddings provide powerful compression of high-dimensional data, yet they typically destroy the geometric structure required for classical likelihood-based statistical inference. This paper develops a rigorous theory of likelihood-preserving embeddings: learned representations that can replace raw data in likelihood-based workflows -- hypothesis testing, confidence interval construction, model selection -- without altering inferential conclusions. We introduce the Likelihood-Ratio Distortion metric $Δ_n$, which measures the maximum error in log-likelihood ratios induced by an embedding. Our main theoretical contribution is the Hinge Theorem, which establishes that controlling $Δ_n$ is necessary and sufficient for preserving inference. Specifically, if the distortion satisfies $Δ_n = o_p(1)$, then (i) all likelihood-ratio based tests and Bayes factors are asymptotically preserved, and (ii) surrogate maximum likelihood estimators are asymptotically equivalent to full-data MLEs. We prove an impossibility result showing that universal likelihood preservation requires essentially invertible embeddings, motivating the need for model-class-specific guarantees. We then provide a constructive framework using neural networks as approximate sufficient statistics, deriving explicit bounds connecting training loss to inferential guarantees. Experiments on Gaussian and Cauchy distributions validate the sharp phase transition predicted by exponential family theory, and applications to distributed clinical inference demonstrate practical utility.

</details>


### [182] [JADAI: Jointly Amortizing Adaptive Design and Bayesian Inference](https://arxiv.org/abs/2512.22999)
*Niels Bracher,Lars Kühmichel,Desi R. Ivanova,Xavier Intes,Paul-Christian Bürkner,Stefan T. Radev*

Main category: stat.ML

TL;DR: JADAI是一个联合摊销贝叶斯自适应设计与推理的框架，通过端到端训练策略网络、历史网络和推理网络来主动优化实验设计以最大化信息增益。


<details>
  <summary>Details</summary>
Motivation: 在参数估计问题中，需要主动优化设计变量以最大化信息增益，但传统方法在高维、多模态后验分布场景下面临挑战。

Method: 提出JADAI框架，联合训练策略网络（决定实验设计）、历史网络（处理实验历史）和推理网络（估计后验分布），使用扩散模型作为后验估计器，通过最小化实验序列中后验误差的增量减少来优化网络。

Result: 在标准自适应设计基准测试中，JADAI实现了优越或具有竞争力的性能。

Conclusion: JADAI提供了一个有效的端到端框架，能够处理高维、多模态后验分布的自适应实验设计问题，在多个基准测试中表现出色。

Abstract: We consider problems of parameter estimation where design variables can be actively optimized to maximize information gain. To this end, we introduce JADAI, a framework that jointly amortizes Bayesian adaptive design and inference by training a policy, a history network, and an inference network end-to-end. The networks minimize a generic loss that aggregates incremental reductions in posterior error along experimental sequences. Inference networks are instantiated with diffusion-based posterior estimators that can approximate high-dimensional and multimodal posteriors at every experimental step. Across standard adaptive design benchmarks, JADAI achieves superior or competitive performance.

</details>


### [183] [Federated Learning With L0 Constraint Via Probabilistic Gates For Sparsity](https://arxiv.org/abs/2512.23071)
*Krishna Harsha Kovelakuntla Huthasana,Alireza Olama,Andreas Lundell*

Main category: stat.ML

TL;DR: 提出一种联邦学习中基于L0约束的稀疏化方法，通过概率门和连续松弛实现参数稀疏，在数据异构和客户端参与异构情况下保持统计性能，比基于幅度的剪枝方法更优。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据和模型的固有稀疏性未得到充分利用，导致模型过于稠密，在数据和客户端参与异构情况下泛化能力差。需要一种能在分布式隐私保护环境下实现模型稀疏化的方法。

Method: 提出联邦学习中基于L0约束的稀疏化方法，通过概率门和连续松弛技术实现参数稀疏控制。该方法源于随机门的熵最大化问题，采用联邦随机梯度下降算法进行分布式学习。

Result: 实验表明，在数据和客户端参与异构情况下，该方法能在目标密度低至ρ=0.05（合成数据）和ρ=0.005（公开数据集）时实现参数稀疏，通信效率高，统计性能优于基于幅度剪枝的阈值方法。

Conclusion: 提出的L0约束联邦学习方法能有效实现模型稀疏化，在保持统计性能的同时提高通信效率，适用于线性回归、逻辑回归、多类分类、多标签分类和卷积神经网络等多种模型。

Abstract: Federated Learning (FL) is a distributed machine learning setting that requires multiple clients to collaborate on training a model while maintaining data privacy. The unaddressed inherent sparsity in data and models often results in overly dense models and poor generalizability under data and client participation heterogeneity. We propose FL with an L0 constraint on the density of non-zero parameters, achieved through a reparameterization using probabilistic gates and their continuous relaxation: originally proposed for sparsity in centralized machine learning. We show that the objective for L0 constrained stochastic minimization naturally arises from an entropy maximization problem of the stochastic gates and propose an algorithm based on federated stochastic gradient descent for distributed learning. We demonstrate that the target density (rho) of parameters can be achieved in FL, under data and client participation heterogeneity, with minimal loss in statistical performance for linear and non-linear models: Linear regression (LR), Logistic regression (LG), Softmax multi-class classification (MC), Multi-label classification with logistic units (MLC), Convolution Neural Network (CNN) for multi-class classification (MC). We compare the results with a magnitude pruning-based thresholding algorithm for sparsity in FL. Experiments on synthetic data with target density down to rho = 0.05 and publicly available RCV1, MNIST, and EMNIST datasets with target density down to rho = 0.005 demonstrate that our approach is communication-efficient and consistently better in statistical performance.

</details>


### [184] [Probabilistic Modelling is Sufficient for Causal Inference](https://arxiv.org/abs/2512.23408)
*Bruno Mlodozeniec,David Krueger,Richard E. Turner*

Main category: stat.ML

TL;DR: 论文主张因果推断问题完全可以在概率建模框架内解决，无需专门的因果工具或符号


<details>
  <summary>Details</summary>
Motivation: 机器学习领域对因果推断存在混淆，普遍认为需要专门的因果框架或符号才能回答因果问题。作者希望澄清这种误解，证明标准概率工具足以处理因果推断。

Method: 通过具体示例展示如何通过"写下所有事物的概率"来回答因果问题，将因果工具重新解释为标准概率建模和推断的产物。

Result: 论证了任何因果推断问题都可以在概率建模和推断的范畴内解决，无需专门的因果特定工具或符号。

Conclusion: 因果工具本质上是标准概率方法的特例，理解这一点有助于澄清因果推断的本质并阐明这些工具的必要性和实用性。

Abstract: Causal inference is a key research area in machine learning, yet confusion reigns over the tools needed to tackle it. There are prevalent claims in the machine learning literature that you need a bespoke causal framework or notation to answer causal questions. In this paper, we want to make it clear that you \emph{can} answer any causal inference question within the realm of probabilistic modelling and inference, without causal-specific tools or notation. Through concrete examples, we demonstrate how causal questions can be tackled by writing down the probability of everything. Lastly, we reinterpret causal tools as emerging from standard probabilistic modelling and inference, elucidating their necessity and utility.

</details>


### [185] [The Nonstationarity-Complexity Tradeoff in Return Prediction](https://arxiv.org/abs/2512.23596)
*Agostino Capponi,Chengpiao Huang,J. Antonio Sidaoui,Kaizheng Wang,Jiacheng Zou*

Main category: stat.ML

TL;DR: 论文提出了一种在非平稳环境中进行股票收益预测的模型选择方法，通过联合优化模型类别和训练窗口大小来平衡模型复杂性与非平稳性，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 在非平稳环境中进行股票收益预测面临一个基本困境：复杂模型可以减少设定误差，但需要更长的训练窗口，而这会引入更强的非平稳性。现有方法无法有效平衡这一矛盾。

Method: 提出了一种新颖的模型选择方法，通过锦标赛程序联合优化模型类别和训练窗口大小。该方法在非平稳验证数据上自适应评估候选模型，平衡设定误差、估计方差和非平稳性。

Result: 在17个行业投资组合收益预测中，该方法持续优于标准滚动窗口基准，样本外R²平均提高14-23%。在经济衰退期间表现尤为突出：在海湾战争衰退期间获得正R²（基准为负），在2001年衰退期间R²绝对提升至少80个基点，在2008年金融危机期间也表现优异。

Conclusion: 该方法有效解决了非平稳环境中的模型选择问题，理论分析表明其性能接近事后最优模型。基于所选模型的交易策略在行业间平均产生31%更高的累计收益，具有重要的实际应用价值。

Abstract: We investigate machine learning models for stock return prediction in non-stationary environments, revealing a fundamental nonstationarity-complexity tradeoff: complex models reduce misspecification error but require longer training windows that introduce stronger non-stationarity. We resolve this tension with a novel model selection method that jointly optimizes model class and training window size using a tournament procedure that adaptively evaluates candidates on non-stationary validation data. Our theoretical analysis demonstrates that this approach balances misspecification error, estimation variance, and non-stationarity, performing close to the best model in hindsight. Applying our method to 17 industry portfolio returns, we consistently outperform standard rolling-window benchmarks, improving out-of-sample $R^2$ by 14-23% on average. During NBER-designated recessions, improvements are substantial: our method achieves positive $R^2$ during the Gulf War recession while benchmarks are negative, and improves $R^2$ in absolute terms by at least 80bps during the 2001 recession as well as superior performance during the 2008 Financial Crisis. Economically, a trading strategy based on our selected model generates 31% higher cumulative returns averaged across the industries.

</details>


### [186] [Calibrated Multi-Level Quantile Forecasting](https://arxiv.org/abs/2512.23671)
*Tiffany Ding,Isaac Gibbs,Ryan J. Tibshirani*

Main category: stat.ML

TL;DR: 提出MultiQT方法，在線保證多個分位數預測同時校準，即使面對對抗性分佈變化也能確保預測有序且不降低原有預測器性能


<details>
  <summary>Details</summary>
Motivation: 現有的分位數預測方法在面對分佈變化時可能失去校準性，且多個分位數預測之間可能出現順序不一致的問題（如0.5分位數預測大於0.6分位數預測）。需要一種輕量級方法來保證多分位數預測的同時校準和有序性。

Method: 提出Multi-Level Quantile Tracker (MultiQT)方法，作為一個包裝器可以應用於任何現有的點預測或分位數預測器。該方法通過在線調整來保證校準性，即使面對對抗性分佈變化也能確保預測有序（分位數預測保持單調性）。

Result: 實驗表明MultiQT在流行病預測和能源預測問題中顯著提高了真實預測器的校準性。方法具有無悔保證，不會漸進地降低原有預測器的分位數損失性能。

Conclusion: MultiQT是一種有效的輕量級方法，能夠保證多分位數預測的同時校準和有序性，對抗分佈變化，且不降低原有預測器性能，在實際應用中表現良好。

Abstract: We present an online method for guaranteeing calibration of quantile forecasts at multiple quantile levels simultaneously. A sequence of $α$-level quantile forecasts is calibrated if the forecasts are larger than the target value at an $α$-fraction of time steps. We introduce a lightweight method called Multi-Level Quantile Tracker (MultiQT) that wraps around any existing point or quantile forecaster to produce corrected forecasts guaranteed to achieve calibration, even against adversarial distribution shifts, while ensuring that the forecasts are ordered -- e.g., the 0.5-level quantile forecast is never larger than the 0.6-level forecast. Furthermore, the method comes with a no-regret guarantee that implies it will not worsen the performance of an existing forecaster, asymptotically, with respect to the quantile loss. In experiments, we find that MultiQT significantly improves the calibration of real forecasters in epidemic and energy forecasting problems.

</details>


### [187] [Bellman Calibration for V-Learning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.23694)
*Lars van der Laan,Nathan Kallus*

Main category: stat.ML

TL;DR: 提出Iterated Bellman Calibration，一种模型无关的后处理校准方法，用于无限时域MDP中的离策略价值预测校准


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，价值函数的准确估计对于策略评估和优化至关重要。现有方法通常需要Bellman完备性或可实现性等强假设，限制了实际应用。需要一种简单、模型无关的校准方法，能够在弱假设下保证校准质量

Method: 提出Iterated Bellman Calibration方法：1) 将经典直方图和等渗校准扩展到动态、反事实设置；2) 通过重复将拟合的Bellman目标回归到模型预测上；3) 使用双重稳健伪结果处理离策略数据；4) 形成一维拟合价值迭代方案，可应用于任何价值估计器

Result: 在弱假设下（无需Bellman完备性或可实现性）为校准和预测提供了有限样本保证，方法简单且模型无关，可应用于任何价值估计器

Conclusion: Iterated Bellman Calibration是一种有效的离策略价值预测校准方法，具有理论保证和实际应用价值，为强化学习中的价值函数校准提供了新思路

Abstract: We introduce Iterated Bellman Calibration, a simple, model-agnostic, post-hoc procedure for calibrating off-policy value predictions in infinite-horizon Markov decision processes. Bellman calibration requires that states with similar predicted long-term returns exhibit one-step returns consistent with the Bellman equation under the target policy. We adapt classical histogram and isotonic calibration to the dynamic, counterfactual setting by repeatedly regressing fitted Bellman targets onto a model's predictions, using a doubly robust pseudo-outcome to handle off-policy data. This yields a one-dimensional fitted value iteration scheme that can be applied to any value estimator. Our analysis provides finite-sample guarantees for both calibration and prediction under weak assumptions, and critically, without requiring Bellman completeness or realizability.

</details>
