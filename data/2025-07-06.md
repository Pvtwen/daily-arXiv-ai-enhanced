<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 12]
- [cs.LG](#cs.LG) [Total: 64]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Joint Radiation Power, Antenna Position, and Beamforming Optimization for Pinching-Antenna Systems with Motion Power Consumption](https://arxiv.org/abs/2507.02348)
*Yiming Xu,Dongfang Xu,Xianghao Yu,Shenghui Song,Zhiguo Ding,Robert Schober*

Main category: eess.SP

TL;DR: 本文研究了考虑天线运动功耗和可调辐射功率的Pinching-antenna系统（PASS）设计，通过联合优化天线位置、辐射功率比和波束成形，最小化平均功耗以满足服务质量要求。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽略了天线放置的物理约束和固定辐射功率的假设，本文填补了这一空白。

Method: 提出基于ADMM的框架解决连续天线移动问题，离散问题则通过BCD方法求解MINLP。

Result: 仿真结果表明，考虑天线运动和可调辐射功率能显著提升PASS性能，且优于传统MIMO系统。

Conclusion: PASS设计通过优化天线位置和辐射功率，有效减少了路径损耗和用户间干扰。

Abstract: Pinching-antenna systems (PASS) have been recently proposed to improve the
performance of wireless networks by reconfiguring both the large-scale and
small-scale channel conditions. However, existing studies ignore the physical
constraints of antenna placement and assume fixed antenna radiation power. To
fill this research gap, this paper investigates the design of PASS taking into
account the motion power consumption of pinching-antennas (PAs) and the impact
of adjustable antenna radiation power. To that end, we minimize the average
power consumption for a given quality-of-service (QoS) requirement, by jointly
optimizing the antenna positions, antenna radiation power ratios, and transmit
beamforming. To the best of the authors' knowledge, this is the first work to
consider radiation power optimization in PASS, which provides an additional
degree of freedom (DoF) for system design. The cases with both continuous and
discrete antenna placement are considered, where the main challenge lies in the
fact that the antenna positions affect both the magnitude and phase of the
channel coefficients of PASS, making system optimization very challenging. To
tackle the resulting unique obstacles, an alternating direction method of
multipliers (ADMM)-based framework is proposed to solve the problem for
continuous antenna movement, while its discrete counterpart is formulated as a
mixed integer nonlinear programming (MINLP) problem and solved by the block
coordinate descent (BCD) method. Simulation results validate the performance
enhancement achieved by incorporating PA movement power assumption and
adjustable radiation power into PASS design, while also demonstrating the
efficiency of the proposed optimization framework. The benefits of PASS over
conventional multiple-input multiple-output (MIMO) systems in mitigating the
large-scale path loss and inter-user interference is also revealed.

</details>


### [2] [Derivative-Free Optimization-Empowered Wireless Channel Reconfiguration for 6G](https://arxiv.org/abs/2507.02243)
*Peilan Wang,Jun Fang,Xianlong Zeng,Bin Wang,Zhi Chen,Yonina C. Eldar*

Main category: eess.SP

TL;DR: 论文提出了一种无导数优化技术（ZO优化），用于直接优化可重构天线系数以塑造无线端到端信道，无需信道建模或隐式环境传播参数估计。


<details>
  <summary>Details</summary>
Motivation: 在6G及未来无线通信中，如何实时优化可重构设备的系数以形成有利的无线信道是一个重大挑战，传统方法需要复杂的信道建模和参数估计。

Method: 采用无导数优化技术（ZO优化），绕过信道建模和隐式参数估计，直接优化可重构系数。

Result: 通过RIS和可移动天线SISO系统的案例研究，验证了ZO优化方法优于现有技术。

Conclusion: 无导数优化为可重构天线技术提供了新的研究方向，具有显著潜力。

Abstract: Reconfigurable antennas, including reconfigurable intelligent surface (RIS),
movable antenna (MA), fluid antenna (FA), and other advanced antenna
techniques, have been studied extensively in the context of reshaping wireless
propagation environments for 6G and beyond wireless communications.
Nevertheless, how to reconfigure/optimize the real-time controllable
coefficients to achieve a favorable end-to-end wireless channel remains a
substantial challenge, as it usually requires accurate modeling of the complex
interaction between the reconfigurable devices and the electromagnetic waves,
as well as knowledge of implicit channel propagation parameters. In this paper,
we introduce a derivative-free optimization (a.k.a., zeroth-order (ZO)
optimization) technique to directly optimize reconfigurable coefficients to
shape the wireless end-to-end channel, without the need of channel modeling and
estimation of the implicit environmental propagation parameters. We present the
fundamental principles of ZO optimization and discuss its potential advantages
in wireless channel reconfiguration. Two case studies for RIS and movable
antenna-enabled single-input single-output (SISO) systems are provided to show
the superiority of ZO-based methods as compared to state-of-the-art techniques.
Finally, we outline promising future research directions and offer concluding
insights on derivative-free optimization for reconfigurable antenna
technologies.

</details>


### [3] [Localized kernel method for separation of linear chirps](https://arxiv.org/abs/2507.02262)
*Eric Mason,Sippanon Kitimoon,Hrushikesh Mhaskar*

Main category: eess.SP

TL;DR: 本文提出了一种改进的信号分离方法，用于处理交叉、低信噪比和不连续性的线性调频信号。


<details>
  <summary>Details</summary>
Motivation: 解决信号处理中信号叠加分离的挑战，特别是在交叉、低信噪比和不连续性条件下的线性调频信号分离问题。

Method: 对Chui和Mhaskar提出的信号分离算子（SSO）进行改进和扩展，分析其在噪声条件下的行为。

Result: 通过理论分析和数值实验验证了方法的有效性，并在包含7个模拟信号的数据集上展示了结果。

Conclusion: 改进后的SSO方法能够有效处理复杂条件下的信号分离问题。

Abstract: The task of separating a superposition of signals into its individual
components is a common challenge encountered in various signal processing
applications, especially in domains such as audio and radar signals. A previous
paper by Chui and Mhaskar proposes a method called Signal Separation Operator
(SSO) to find the instantaneous frequencies and amplitudes of such
superpositions where both of these change continuously and slowly over time. In
this paper, we amplify and modify this method in order to separate chirp
signals in the presence of crossovers, a very low SNR, and discontinuities. We
give a theoretical analysis of the behavior of SSO in the presence of noise to
examine the relationship between the minimal separation, minimal amplitude,
SNR, and sampling frequency. Our method is illustrated with a few examples, and
numerical results are reported on a simulated dataset comprising 7 simulated
signals.

</details>


### [4] [STAR-RIS Transceivers: Integrated Sensing and Communication with Pulsed Signals](https://arxiv.org/abs/2507.02346)
*Hedieh Taremizadeh,Emanuele Grossi,Luca Venturino*

Main category: eess.SP

TL;DR: 该研究提出了一种集成感知与通信（ISAC）的收发器，利用STAR-RIS和PESA接收器，通过空间和时间调制实现多目标检测和通信。


<details>
  <summary>Details</summary>
Motivation: 探索如何在雷达和通信系统中高效集成感知与通信功能，同时保持雷达性能。

Method: 使用STAR-RIS进行空间调制，结合时间调制和正交二进制码本，实现目标检测和通信消息嵌入。

Result: 时间调制对雷达性能影响最小，同时实现了通信功能。

Conclusion: 该方法为ISAC系统提供了一种高效且性能优越的解决方案。

Abstract: This study examines an integrated sensing and communication (ISAC)
transceiver featuring a simultaneous transmitting and reflecting reconfigurable
intelligent surface (STAR-RIS) and a receiver equipped with a passive
electronically scanned array (PESA) and a single digital channel. By utilizing
a periodic pulsed signal emitted by a feeder, we introduce at the STAR-RIS a
space modulation to illuminate two angular directions observed by the radar
receiver, one in each half-space, and a time modulation to distinguish the
corresponding echoes from prospective moving targets and embed communication
messages. The proposed time modulation employs orthogonal binary codebooks with
different trade-offs in transmission and error rates, while having minimal
impact on the radar performance, evaluated by probability of detection and root
mean square error in the radial velocity estimation.

</details>


### [5] [Track-before-detect in RIS-aided Integrated Sensing and Communication](https://arxiv.org/abs/2507.02352)
*Georgios Mylonopoulos,Luca Venturino,Emanuele Grossi,Stefano Buzzi,Ciro D'Elia*

Main category: eess.SP

TL;DR: 研究提出了一种结合感知与通信功能的基站系统，通过多帧雷达检测器提升用户频谱效率，同时保持感知性能。


<details>
  <summary>Details</summary>
Motivation: 探索如何在基站系统中平衡感知与通信性能，以实现更优的系统权衡。

Method: 利用多帧雷达检测器（包括检测器、绘图提取器和检测前跟踪处理器）联合处理多次扫描数据。

Result: 数值分析验证了该方法的有效性，并评估了可实现的系统权衡。

Conclusion: 通过联合处理多次扫描数据，可以在保持感知性能的同时提升用户频谱效率。

Abstract: This study considers a base station equipped with sensing and communication
capabilities, which serves a ground user and scans a portion of the sky via a
passive reconfigurable intelligent surface. To achieve more favorable system
tradeoffs, we utilize a multi-frame radar detector, comprising a detector, a
plot-extractor, and a track-before-detect processor. The main idea proposed
here is that user spectral efficiency can be enhanced by increasing the number
of scans jointly processed by the multi-frame radar detector while maintaining
the same sensing performance. A numerical analysis is conducted to verify the
effectiveness of the proposed solution and to evaluate the achievable system
tradeoffs.

</details>


### [6] [Predictive Control over LAWN: Joint Trajectory Design and Resource Allocation](https://arxiv.org/abs/2507.02374)
*Haijia Jin,Jun Wu,Weijie Yuan,Ruizhi Ruan,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour*

Main category: eess.SP

TL;DR: 该论文研究了低空无线网络（LAWN）中无人机通过有限块长度（FBL）传输为多移动自动导引车（AGV）提供实时无线控制的问题，提出了一种基于模型预测控制（MPC）和交替优化（AO）框架的联合优化方法。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络（LAWN）在物联网（IoT）系统中具有潜力，但实时无线控制面临通信可靠性和轨迹跟踪的挑战，需要一种高效解决方案。

Method: 采用模型预测控制（MPC）确保轨迹跟踪，分析通信可靠性；通过交替优化（AO）框架解决非凸优化问题，结合投影梯度下降（PGD）和逐次凸近似（SCA）技术。

Result: 仿真和实验验证了所提方法在控制性能上的优越性，优于基线方案。

Conclusion: 提出的方法有效解决了LAWN系统中的实时控制问题，为未来研究提供了高效优化框架。

Abstract: Low-altitude wireless networks (LAWNs) have been envisioned as flexible and
transformative platforms for enabling delay-sensitive control applications in
Internet of Things (IoT) systems. In this work, we investigate the real-time
wireless control over a LAWN system, where an aerial drone is employed to serve
multiple mobile automated guided vehicles (AGVs) via finite blocklength (FBL)
transmission. Toward this end, we adopt the model predictive control (MPC) to
ensure accurate trajectory tracking, while we analyze the communication
reliability using the outage probability. Subsequently, we formulate an
optimization problem to jointly determine control policy, transmit power
allocation, and drone trajectory by accounting for the maximum travel distance
and control input constraints. To address the resultant non-convex optimization
problem, we first derive the closed-form expression of the outage probability
under FBL transmission. Based on this, we reformulate the original problem as a
quadratic programming (QP) problem, followed by developing an alternating
optimization (AO) framework. Specifically, we employ the projected gradient
descent (PGD) method and the successive convex approximation (SCA) technique to
achieve computationally efficient sub-optimal solutions. Furthermore, we
thoroughly analyze the convergence and computational complexity of the proposed
algorithm. Extensive simulations and AirSim-based experiments are conducted to
validate the superiority of our proposed approach compared to the baseline
schemes in terms of control performance.

</details>


### [7] [Parameter estimation of range-migrating targets using OTFS signals from LEO satellites](https://arxiv.org/abs/2507.02385)
*Tong Ding,Luca Venturino,Emanuele Grossi*

Main category: eess.SP

TL;DR: 研究提出了一种基于OTFS调制的LEO卫星通信与感知一体化系统，用于估计高速目标的参数，揭示了目标响应在延迟-多普勒域的稀疏结构，并提出了一种最大似然估计器的近似实现方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用LEO卫星的OTFS调制信号估计高速目标的参数，解决传统方法中因目标高速运动导致的信号处理难题。

Method: 通过OTFS收发器的信号处理，推导了高速目标回波的输入输出模型，利用延迟-多普勒域的稀疏结构，提出了一种基于块正交匹配追踪和匹配滤波器的最大似然估计器近似实现方法。

Result: 研究发现目标响应在延迟-多普勒域具有稀疏性，且范围迁移会导致响应扩散。数值实验验证了所提估计方法的性能。

Conclusion: 研究为高速目标参数估计提供了一种有效方法，揭示了OTFS调制在通信与感知一体化系统中的潜力。

Abstract: This study investigates a communication-centric integrated sensing and
communication (ISAC) system that utilizes orthogonal time frequency space
(OTFS) modulated signals emitted by low Earth orbit (LEO) satellites to
estimate the parameters of space targets experiencing range migration,
henceforth referred to as high-speed targets. Leveraging the specific signal
processing performed by OTFS transceivers, we derive a novel input-output model
for the echo generated by a high-speed target in scenarios where ideal and
rectangular shaping filters are employed. Our findings reveal that the target
response exhibits a sparse structure in the delay-Doppler domain, dependent
solely upon the initial range and range-rate; notably, range migration causes a
spread in the target response, marking a significant departure from previous
studies. Utilizing this signal structure, we propose an approximate
implementation of the maximum likelihood estimator for the target's initial
range, range-rate, and amplitude. The estimation process involves obtaining
coarse information on the target response using a block orthogonal matching
pursuit algorithm, followed by a refinement step using a bank of matched
filters focused on a smaller range and range-rate region. Finally, numerical
examples are provided to evaluate the estimation performance.

</details>


### [8] [When Attention is Beneficial for Learning Wireless Resource Allocation Efficiently?](https://arxiv.org/abs/2507.02427)
*Jia Guo,Chenyang Yang*

Main category: eess.SP

TL;DR: 论文探讨了注意力机制在图神经网络（GNNs）中是否必要，通过分析集合上的函数结构和数值算法，发现注意力机制在无干扰时可省略，但在有干扰时需使用。


<details>
  <summary>Details</summary>
Motivation: 研究注意力机制在无线资源分配中的必要性，以优化GNNs的设计。

Method: 分析集合上的置换等变函数结构，重新表达数值算法，并提出与结构对齐的GNN设计框架。

Result: 发现注意力机制在无干扰时可省略，但在有干扰时需使用；提出的GNN框架在模拟中验证了学习效率。

Conclusion: 注意力机制在资源分配中的必要性取决于干扰情况，提出的GNN设计框架能有效提升学习效率。

Abstract: Owing to the use of attention mechanism to leverage the dependency across
tokens, Transformers are efficient for natural language processing. By
harnessing permutation properties broadly exist in resource allocation
policies, each mapping measurable environmental parameters (e.g., channel
matrix) to optimized variables (e.g., precoding matrix), graph neural networks
(GNNs) are promising for learning these policies efficiently in terms of
scalability and generalizability. To reap the benefits of both architectures,
there is a recent trend of incorporating attention mechanism with GNNs for
learning wireless policies. Nevertheless, is the attention mechanism really
needed for resource allocation? In this paper, we strive to answer this
question by analyzing the structures of functions defined on sets and numerical
algorithms, given that the permutation properties of wireless policies are
induced by the involved sets (say user set). In particular, we prove that the
permutation equivariant functions on a single set can be recursively expressed
by two types of functions: one involves attention, and the other does not. We
proceed to re-express the numerical algorithms for optimizing several
representative resource allocation problems in recursive forms. We find that
when interference (say multi-user or inter-data stream interference) is not
reflected in the measurable parameters of a policy, attention needs to be used
to model the interference. With the insight, we establish a framework of
designing GNNs by aligning with the structures. By taking reconfigurable
intelligent surface-aided hybrid precoding as an example, the learning
efficiency of the proposed GNN is validated via simulations.

</details>


### [9] [Corrections to Published Values of Frequency Sampling Filter Transition Coefficients](https://arxiv.org/abs/2507.02556)
*C. S. Ramalingam*

Main category: eess.SP

TL;DR: 本文纠正了Rabiner等人和Lyons关于频率采样滤波器（FSF）设计中过渡系数和峰值旁瓣水平（PSL）的错误数据，并提供了优化后的正确值。


<details>
  <summary>Details</summary>
Motivation: 发现Rabiner等人和Lyons在FSF设计中提供的过渡系数和PSL值存在差异且不准确，需修正并提供优化值。

Method: 通过程序计算优化低通和带通滤波器的过渡系数及PSL值。

Result: 得出了过渡系数和PSL的优化值，例如N=16和BW=4时的最优值为0.40474097。

Conclusion: 提供了准确的过渡系数和PSL值，纠正了先前文献中的错误。

Abstract: Tables of optimal transition coefficients and peak sidelobe level (PSL, in
dB) associated with frequency sampling filter (FSF) design were published by
Rabiner et al. (Jun 1970), and reproduced, for example, in the book Digital
Signal Processing by Proakis and Manolakis (4/e, 2007). A set of values are
also given in Appendix H of Understanding Digital Signal Processing} by Lyons
(3/e, 2011), but there are significant differences between these two sets. For
example, for $N=16$ and BW$=4$, two different transition coefficient values
have been reported, viz., $0.38916626$ (Rabiner, et al.) and $0.34918551$
(Lyons). Neither is optimal, for we find the optimum value to be $0.40474097$.
The published values of the corresponding PSLs were also found to be incorrect.
In this paper we give the optimal values of the transition coefficients and PSL
values as estimated by our program for the lowpass and bandpass filters listed
in Rabiner et al. and Lyons.

</details>


### [10] [Pinching-Antenna-Assisted Index Modulation: Channel Modeling, Transceiver Design, and Performance Analysis](https://arxiv.org/abs/2507.02641)
*Shuaixin Yang,Yijia Li,Yue Xiao,Yong Liang Guan,Xianfu Lei,Zhiguo Ding*

Main category: eess.SP

TL;DR: 提出了一种新型的捏合天线辅助索引调制（PA-IM）方案，通过结合天线位置模式索引和QAM符号提高频谱效率，同时保持硬件复杂度不变。


<details>
  <summary>Details</summary>
Motivation: 在无线通信中，提高频谱效率而不增加硬件复杂度是一个关键挑战。PA-IM方案通过利用天线位置模式索引传递额外信息，解决了这一问题。

Method: 设计了全面的收发器方案，包括信道建模、低复杂度BOSD检测算法、BER上界分析以及基于流形优化的预编码方法。

Result: 仿真结果表明，PA-IM方案在性能上显著优于传统方法，且预编码设计显著降低了系统的BER。

Conclusion: PA-IM方案为高效频谱利用提供了可行方案，同时通过优化设计解决了信号检测和性能优化的关键问题。

Abstract: In this paper, a novel pinching-antenna assisted index modulation (PA-IM)
scheme is proposed for improving the spectral efficiency without increasing the
hardware complexity, where the information bits are conveyed not only by the
conventional M-ary quadrature amplitude modulation (QAM) symbols but also by
the indices of pinching antenna (PA) position patterns. To realize the full
potential of this scheme, this paper focuses on the comprehensive transceiver
design, addressing key challenges in signal detection at the receiver and
performance optimization at thetransmitter. First, a comprehensive channel
model is formulated for this architecture, which sophisticatedly integrates the
deterministic in-waveguide propagation effects with the stochastic nature of
wireless channels, including both largescale path loss and small-scale fading.
Next, to overcome the prohibitive complexity of optimal maximum likelihood (ML)
detection, a low-complexity box-optimized sphere decoding (BOSD) algorithm is
designed, which adaptively prunes the search space whilst preserving optimal ML
performance. Furthermore, an analytical upper bound on the bit error rate (BER)
is derived and validated by the simulations. Moreover, a new transmit precoding
method is designed using manifold optimization, which minimizes the BER by
jointly optimizing the complex-valued precoding coefficients across the
waveguides for the sake of maximizing the minimum Euclidean distance of all
received signal points. Finally, the simulation results demonstrate that the
proposed PA-IM scheme attains a significant performance gain over its
conventional counterparts and that the overall BER of the pinching-antenna
system is substantially improved by the proposed precoding design.

</details>


### [11] [AREE-Based Decoupled Design of Hybrid Beamformers in mmWave XL-MIMO Systems](https://arxiv.org/abs/2507.02802)
*Jiazhe Li,Nicolò Decarli,Francesco Guidi,Heng Dong,Anna Guerra,Alessandro Bazzi,Zhuoming Li*

Main category: eess.SP

TL;DR: 提出了一种交替残差消除（AREE）算法，用于毫米波通信中的混合波束成形问题，通过分解为低维子问题实现模拟和数字预编码器的解耦，提高计算和频谱效率。


<details>
  <summary>Details</summary>
Motivation: 混合波束成形在毫米波通信中硬件复杂性和频谱效率之间存在矛盾，现有算法因模拟和数字预编码器的耦合而受限。

Method: AREE算法将混合波束成形问题分解为两个低维子问题，通过迭代消除残差实现解耦，并采用低复杂度几何信道SVD算法简化问题。

Result: 仿真结果表明，AREE算法能以低复杂度实现模拟和数字预编码器的解耦，快速收敛且频谱效率优于现有方法。

Conclusion: AREE算法为解决混合波束成形问题提供了一种高效且性能优越的解决方案。

Abstract: Hybrid beamforming has been widely employed in mmWave communications such as
vehicular-to-everything (V2X) scenarios, as a compromise between hardware
complexity and spectral efficiency. However, the inherent coupling between
analog and digital precoders in hybrid array architecture significantly limits
the computational and spectral efficiency of existing algorithms. To address
this issue, we propose an alternating residual error elimination (AREE)
algorithm, which decomposes the hybrid beamforming problem into two
low-dimensional subproblems, each exhibiting a favorable matrix structure that
enables effective decoupling of analog and digital precoders from the matrix
product formulation. These subproblems iteratively eliminate each other's
residual errors, driving the original problem toward the optimal hybrid
beamforming performance. The proposed initialization ensures rapid convergence,
while a low-complexity geometric channel SVD algorithm is developed by
transforming the high-dimensional sparse channel into a low-dimensional
equivalent, thereby simplifying the derivation of subproblems. Simulation
results demonstrate that the AREE algorithm effectively decouples analog and
digital precoders with low complexity, achieves fast convergence, and offers
higher spectral efficiency than existing beamforming methods.

</details>


### [12] [DNN-Based Precoding in RIS-Aided mmWave MIMO Systems With Practical Phase Shift](https://arxiv.org/abs/2507.02824)
*Po-Heng Chou,Ching-Wen Chen,Wan-Jen Huang,Walid Saad,Yu Tsao,Ronald Y. Chang*

Main category: eess.SP

TL;DR: 论文研究了毫米波MIMO系统中通过RIS增强传输的预编码设计，提出使用DNN加速码字选择，以减少传统方法的计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 毫米波MIMO系统中直接通信路径受阻时，需要高效预编码设计以最大化吞吐量，传统方法计算复杂度高。

Method: 采用RIS增强传输，结合DFT向量设计码本，并开发DNN模型加速码字选择。

Result: 仿真表明DNN在测试阶段保持次优频谱效率，适应距离变化。

Conclusion: DNN在RIS辅助系统中具有潜力，可显著降低计算复杂度。

Abstract: In this paper, the precoding design is investigated for maximizing the
throughput of millimeter wave (mmWave) multiple-input multiple-output (MIMO)
systems with obstructed direct communication paths. In particular, a
reconfigurable intelligent surface (RIS) is employed to enhance MIMO
transmissions, considering mmWave characteristics related to line-of-sight
(LoS) and multipath effects. The traditional exhaustive search (ES) for optimal
codewords in the continuous phase shift is computationally intensive and
time-consuming. To reduce computational complexity, permuted discrete Fourier
transform (DFT) vectors are used for finding codebook design, incorporating
amplitude responses for practical or ideal RIS systems. However, even if the
discrete phase shift is adopted in the ES, it results in significant
computation and is time-consuming. Instead, the trained deep neural network
(DNN) is developed to facilitate faster codeword selection. Simulation results
show that the DNN maintains sub-optimal spectral efficiency even as the
distance between the end-user and the RIS has variations in the testing phase.
These results highlight the potential of DNN in advancing RIS-aided systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [13] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: 提出了一种可学习和可微分的有限体积求解器LDSolver，用于在粗网格上高效准确地模拟流体流动，解决了传统数值求解器和机器学习方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统数值求解器计算成本高，而机器学习方法存在可解释性、泛化性和数据依赖性等问题。

Method: LDSolver包含可微分有限体积求解器和可学习模块，用于粗网格上的通量等效近似和时间误差校正。

Result: 实验表明，LDSolver在多种流动系统中表现优异，超越基线模型。

Conclusion: LDSolver在有限训练数据下仍能保持高精度和泛化能力，显著加速模拟。

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [14] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: 提出了一种名为DKGCM的图卷积网络结构，通过动态时间规整和K均值聚类改进空间依赖性捕捉，结合FFT和双向Mamba框架优化时间依赖性，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 交通需求预测的准确性对资源分配至关重要，但复杂的时空关系限制了现有模型的性能。

Method: 提出DK-GCN方法，利用DTW和K-means聚类分组交通节点；结合FFT和双向Mamba框架捕捉时间依赖性；引入GRPO强化学习策略优化训练。

Result: 在三个公开数据集上表现优于多种先进方法。

Conclusion: DKGCM显著提升了交通需求预测的准确性，为交通管理提供了有效工具。

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [15] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 研究探讨了多模态特征组合（文本、图像和社交特征）在社交媒体虚假信息检测中的有效性，结合无监督和监督机器学习模型，性能提升15%。


<details>
  <summary>Details</summary>
Motivation: 社交媒体在选举和危机期间充斥虚假信息，现有研究多关注单模态（文本或图像），多模态特征组合研究较少。

Method: 采用早期融合方法，结合文本、图像和社交特征，分析1,529条推文，并通过数据增强提取视觉和社交特征。

Result: 多模态模型性能比单模态提升15%，比双模态提升5%，并分析了虚假信息的传播模式。

Conclusion: 多模态特征组合显著提升虚假信息检测性能，为未来研究提供新方向。

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [16] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: 提出了一种基于采样技术和粗糙集理论的特征选择方法，用于处理大规模数据，能够在个人电脑上高效找到具有高判别能力的特征子集。


<details>
  <summary>Details</summary>
Motivation: 智能机器在处理大规模数据时计算资源有限，需要高效的特征选择方法。

Method: 利用可区分对象对比例衡量特征集的判别能力，构建正区域保留样本以选择特征子集。

Result: 在11个数据集上验证，方法能在短时间内找到近似约简，判别能力高于预估下限。

Conclusion: 该方法高效且适用于大规模数据，能在个人电脑上快速获得高判别能力的特征子集。

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [17] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 论文提出了一种基于机器学习的多变量时间序列异常检测方法，通过连续小波变换将数据转换为图像，并利用预训练的VGG-16架构和Siamese网络进行高效分类。


<details>
  <summary>Details</summary>
Motivation: 半导体制造过程中数据复杂且噪声多，传统方法难以有效检测异常，需要一种更鲁棒的方法。

Method: 1. 使用连续小波变换将时间序列数据转换为图像；2. 微调预训练的VGG-16架构；3. 构建Siamese网络进行图像对分类。

Result: 在真实半导体制造数据集上表现出高准确性，适用于离线和半监督场景。

Conclusion: 该方法为复杂工业环境中的异常检测提供了灵活且高效的解决方案。

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [18] [Padé Approximant Neural Networks for Enhanced Electric Motor Fault Diagnosis Using Vibration and Acoustic Data](https://arxiv.org/abs/2507.02599)
*Sertac Kilickaya,Levent Eren*

Main category: cs.LG

TL;DR: 本研究旨在通过Padé Approximant Neuron (PAON)模型提升感应电机的故障诊断性能，比较PadéNets与传统CNN和Self-ONNs在振动和声学数据上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有感应电机故障诊断方法主要依赖加速度计和麦克风，但非线性神经元架构的深度学习模型有望提升诊断性能。

Method: 比较一维CNN、Self-ONNs和PadéNets三种深度学习架构在公开数据集上的表现，PadéNet设计为增强非线性并兼容无界激活函数。

Result: PadéNets在多个传感器数据上表现优于基线模型，诊断准确率高达99.96%。

Conclusion: PadéNets的非线性增强和无界激活函数兼容性显著提升了感应电机故障诊断性能。

Abstract: Purpose: The primary aim of this study is to enhance fault diagnosis in
induction machines by leveraging the Pad\'e Approximant Neuron (PAON) model.
While accelerometers and microphones are standard in motor condition
monitoring, deep learning models with nonlinear neuron architectures offer
promising improvements in diagnostic performance. This research addresses the
question: Can Pad\'e Approximant Neural Networks (Pad\'eNets) outperform
conventional Convolutional Neural Networks (CNNs) and Self-Organized
Operational Neural Networks (Self-ONNs) in diagnosing electrical and mechanical
faults using vibration and acoustic data?
  Methods: We evaluate and compare the diagnostic capabilities of three deep
learning architectures: one-dimensional CNNs, Self-ONNs, and Pad\'eNets. These
models are tested on the University of Ottawa's publicly available
constant-speed induction motor datasets, which include both vibration and
acoustic sensor data. The Pad\'eNet model is designed to introduce enhanced
nonlinearity and is compatible with unbounded activation functions such as
Leaky ReLU.
  Results and Conclusion: Pad\'eNets consistently outperformed the baseline
models, achieving diagnostic accuracies of 99.96%, 98.26%, 97.61%, and 98.33%
for accelerometers 1, 2, 3, and the acoustic sensor, respectively. The enhanced
nonlinearity of Pad\'eNets, together with their compatibility with unbounded
activation functions, significantly improves fault diagnosis performance in
induction motor condition monitoring.

</details>


### [19] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: 提出了一种名为Temporal Chain of Thought的推理策略，通过迭代选择最相关视频帧来提升长视频问答的准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有视觉语言模型（VLMs）能处理约1000帧输入，但在长视频理解中仍难以有效利用上下文，易受无关帧干扰。

Method: 利用VLM自身迭代识别并提取最相关帧，用于问答任务。

Result: 在4个视频问答数据集上取得最佳成绩，尤其在超过1小时的长视频上表现突出。

Conclusion: 通过推理时计算优化上下文选择，显著提升了长视频问答的准确性。

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [20] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: 论文提出AIRES，一种算法-系统协同设计解决方案，用于加速GCN中的外存SpGEMM计算，解决了现有系统的高I/O延迟和GPU利用率不足问题。


<details>
  <summary>Details</summary>
Motivation: 随着图数据规模扩大，外存SpGEMM因GPU内存限制成为性能瓶颈，现有系统存在高I/O延迟和GPU利用率不足问题。

Method: AIRES通过块级稀疏格式数据对齐和分块算法优化数据对齐，并采用三阶段动态调度和分层内存系统（GPU内存、GDS、主机内存）减少I/O延迟。

Result: AIRES在真实图处理基准测试中表现优异，延迟降低达1.8倍。

Conclusion: AIRES通过算法和系统协同设计，显著提升了外存SpGEMM的性能，为大规模图计算提供了高效解决方案。

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [21] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda是一种SE(3)-等变适配器框架，用于高效微调几何扩散模型，保持几何一致性并避免过拟合和灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 当前几何扩散模型在下游任务中的高效微调方法尚未充分探索，尤其是针对不同几何控制的任务。

Method: 提出SE(3)-等变适配器框架（GeoAda），通过轻量级适配模块实现参数高效微调，保持模型几何一致性。

Result: GeoAda在多种几何控制任务中表现优异，保持原始任务准确性，优于其他基线方法。

Conclusion: GeoAda为几何扩散模型的灵活微调提供了高效解决方案，适用于广泛的应用领域。

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [22] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 论文比较了通用大语言模型（LLM）与专有招聘模型（Match Score）在候选人匹配中的表现，发现Match Score在准确性和公平性上均优于LLM，并讨论了预训练偏见的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于评估LLM在招聘中的准确性和公平性，揭示其潜在偏见，并验证专有模型是否能同时实现高准确性和公平性。

Method: 方法包括对多个LLM和专有模型进行基准测试，评估其预测准确性（如ROC AUC）和公平性（如影响比率）。

Result: 结果显示Match Score在准确性（ROC AUC 0.85 vs 0.77）和公平性（种族影响比率0.957 vs 0.809）上均优于LLM。

Conclusion: 结论强调在高风险领域（如招聘）中，应优先使用领域专用模型并进行偏见审计，而非依赖未经充分公平性保障的通用LLM。

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [23] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [24] [Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo](https://arxiv.org/abs/2507.02315)
*Sooyeon Kim,Giung Nam,Juho Lee*

Main category: cs.LG

TL;DR: 论文提出通过自蒸馏迭代优化基础模型，以解决约束文本生成中奖励信号稀疏的问题，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 在约束文本生成中，目标分布集中在基础模型不太可能的输出上，导致学习困难。

Method: 采用自蒸馏方法迭代优化基础模型，逐步对齐目标分布。

Result: 实验表明，该方法显著提高了生成质量。

Conclusion: 自蒸馏迭代优化是解决约束生成中奖励稀疏问题的有效方法。

Abstract: Recent work has framed constrained text generation with autoregressive
language models as a probabilistic inference problem. Among these, Zhao et al.
(2024) introduced a promising approach based on twisted Sequential Monte Carlo,
which incorporates learned twist functions and twist-induced proposals to guide
the generation process. However, in constrained generation settings where the
target distribution concentrates on outputs that are unlikely under the base
model, learning becomes challenging due to sparse and uninformative reward
signals. We show that iteratively refining the base model through
self-distillation alleviates this issue by making the model progressively more
aligned with the target, leading to substantial gains in generation quality.

</details>


### [25] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 论文提出了一种新型的Energy-Based Transformers（EBTs），通过无监督学习实现类似人类System 2 Thinking的推理能力，并在多种任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有推理计算方法多为模态或问题特定，或需要额外监督训练。本文旨在探索是否可以通过无监督学习实现通用的System 2 Thinking能力。

Method: 训练EBTs（基于能量的模型），通过能量最小化实现预测，支持离散和连续模态任务。

Result: EBTs在训练和推理中表现优于Transformer++和Diffusion Transformers，数据扩展率提高35%，语言任务性能提升29%，图像去噪效果更优。

Conclusion: EBTs是一种有前景的新范式，可扩展模型的学习和推理能力。

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [26] [Online Conformal Prediction with Efficiency Guarantees](https://arxiv.org/abs/2507.02496)
*Vaidehi Srinivas*

Main category: cs.LG

TL;DR: 研究在线框架下的共形预测问题，直接优化效率，针对任意和可交换输入序列提出算法，展示了两者之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 探索在线共形预测问题，目标是实现覆盖率和效率的平衡，特别是在任意和可交换输入序列下的表现差异。

Method: 提出确定性算法，针对可交换序列实现接近最优覆盖率和长度，对任意序列则展示性能限制并给出匹配算法。

Result: 可交换序列下算法接近最优，任意序列下存在性能限制，算法能恢复所有Pareto最优设置。

Conclusion: 在线共形预测在可交换和任意序列下存在性能差距，提出算法能有效平衡两者需求。

Abstract: We study the problem of conformal prediction in a novel online framework that
directly optimizes efficiency. In our problem, we are given a target
miscoverage rate $\alpha > 0$, and a time horizon $T$. On each day $t \le T$ an
algorithm must output an interval $I_t \subseteq [0, 1]$, then a point $y_t \in
[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,
$y_t \in I_t$ on (close to) a $(1 - \alpha)$-fraction of days, while
maintaining efficiency, that is, minimizing the average volume (length) of the
intervals played. This problem is an online analogue to the problem of
constructing efficient confidence intervals.
  We study this problem over arbitrary and exchangeable (random order) input
sequences. For exchangeable sequences, we show that it is possible to construct
intervals that achieve coverage $(1 - \alpha) - o(1)$, while having length
upper bounded by the best fixed interval that achieves coverage in hindsight.
For arbitrary sequences however, we show that any algorithm that achieves a
$\mu$-approximation in average length compared to the best fixed interval
achieving coverage in hindsight, must make a multiplicative factor more
mistakes than $\alpha T$, where the multiplicative factor depends on $\mu$ and
the aspect ratio of the problem. Our main algorithmic result is a matching
algorithm that can recover all Pareto-optimal settings of $\mu$ and number of
mistakes. Furthermore, our algorithm is deterministic and therefore robust to
an adaptive adversary.
  This gap between the exchangeable and arbitrary settings is in contrast to
the classical online learning problem. In fact, we show that no single
algorithm can simultaneously be Pareto-optimal for arbitrary sequences and
optimal for exchangeable sequences. On the algorithmic side, we give an
algorithm that achieves the near-optimal tradeoff between the two cases.

</details>


### [27] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: PANAMA是一个基于WaveNet架构的主动学习框架，用于训练端到端的参数化吉他放大器模型，通过优化采样策略减少数据点需求。


<details>
  <summary>Details</summary>
Motivation: 旨在通过主动学习策略最小化数据点（如放大器旋钮设置）的使用，高效训练吉他放大器模型。

Method: 采用WaveNet-like架构，结合梯度优化算法确定最优采样点。

Result: 在有限样本下，该方法有效优化了采样策略，提升了模型训练效率。

Conclusion: PANAMA框架通过主动学习策略，显著减少了训练吉他放大器模型所需的数据量。

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [28] [Classification by Separating Hypersurfaces: An Entropic Approach](https://arxiv.org/abs/2507.02732)
*Argimiro Arratia,Mahmoud El Daou,Henryk Gzyl*

Main category: cs.LG

TL;DR: 提出了一种基于熵最小化的新分类方法，通过搜索有界超立方体中的参数向量和正向量，扩展至多项式曲面，优于传统线性或二次优化技术。


<details>
  <summary>Details</summary>
Motivation: 解决高维空间中两类点的分离问题，传统方法如支持向量机和梯度下降存在局限性。

Method: 在未知变量空间上定义熵基函数，通过最小化该函数搜索参数向量和正向量，支持多项式曲面分离。

Result: 数值实验表明该方法高效且适用于线性和非线性分类任务。

Conclusion: 该方法为分类问题提供了鲁棒的替代方案，尤其适用于复杂决策边界。

Abstract: We consider the following classification problem: Given a population of
individuals characterized by a set of attributes represented as a vector in
${\mathbb R}^N$, the goal is to find a hyperplane in ${\mathbb R}^N$ that
separates two sets of points corresponding to two distinct classes. This
problem, with a history dating back to the perceptron model, remains central to
machine learning. In this paper we propose a novel approach by searching for a
vector of parameters in a bounded $N$-dimensional hypercube centered at the
origin and a positive vector in ${\mathbb R}^M$, obtained through the
minimization of an entropy-based function defined over the space of unknown
variables. The method extends to polynomial surfaces, allowing the separation
of data points by more complex decision boundaries. This provides a robust
alternative to traditional linear or quadratic optimization techniques, such as
support vector machines and gradient descent. Numerical experiments demonstrate
the efficiency and versatility of the method in handling diverse classification
tasks, including linear and non-linear separability.

</details>


### [29] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: 研究发现，计算最优训练的神经网络模型在归一化后，损失曲线会收敛到一条通用曲线上，称为“超级收敛”，并揭示了其与典型神经缩放律的幂律结构相关。


<details>
  <summary>Details</summary>
Motivation: 探讨模型规模和训练时间共同增长时，神经网络训练动态的缩放规律。

Method: 通过分析不同模型规模、学习率计划和数据集的训练动态，研究损失曲线的归一化行为。

Result: 发现损失曲线在归一化后表现出超级收敛现象，且与学习率计划和超参数优化密切相关。

Conclusion: 超级收敛现象为模型缩放提供了精确的实用指标，并通过简单模型解释了其动力学机制。

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [30] [Contextual Online Pricing with (Biased) Offline Data](https://arxiv.org/abs/2507.02762)
*Yixuan Zhang,Ruihao Zhu,Qiaomin Xie*

Main category: cs.LG

TL;DR: 论文研究了基于有偏离线数据的上下文在线定价问题，提出了实例依赖的统计复杂度度量，并设计了最优策略以实现最小化后悔。


<details>
  <summary>Details</summary>
Motivation: 解决在有偏离线数据下进行上下文在线定价时的统计复杂度和后悔边界问题。

Method: 提出Optimism-in-the-Face-of-Uncertainty (OFU)策略，并针对不同价格弹性情况设计了算法。

Result: 在标量价格弹性情况下，实现了实例依赖的最优后悔边界；在一般价格弹性情况下，达到了最坏情况下的最优后悔率。

Conclusion: 论文首次为有偏离线数据下的上下文定价提供了紧致的后悔保证，并推广到随机线性bandits问题。

Abstract: We study contextual online pricing with biased offline data. For the scalar
price elasticity case, we identify the instance-dependent quantity $\delta^2$
that measures how far the offline data lies from the (unknown) online optimum.
We show that the time length $T$, bias bound $V$, size $N$ and dispersion
$\lambda_{\min}(\hat{\Sigma})$ of the offline data, and $\delta^2$ jointly
determine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty
(OFU) policy achieves a minimax-optimal, instance-dependent regret bound
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T +
\frac{dT}{\lambda_{\min}(\hat{\Sigma}) + (N \wedge T) \delta^2})\big)$. For
general price elasticity, we establish a worst-case, minimax-optimal rate
$\tilde{\mathcal{O}}\big(d\sqrt{T} \wedge (V^2T + \frac{dT
}{\lambda_{\min}(\hat{\Sigma})})\big)$ and provide a generalized OFU algorithm
that attains it. When the bias bound $V$ is unknown, we design a robust variant
that always guarantees sub-linear regret and strictly improves on purely online
methods whenever the exact bias is small. These results deliver the first tight
regret guarantees for contextual pricing in the presence of biased offline
data. Our techniques also transfer verbatim to stochastic linear bandits with
biased offline data, yielding analogous bounds.

</details>


### [31] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP是一个基于大型语言模型（LLM）的自动VLSI设计流程调优框架，通过向量化RTL代码、相似电路匹配和检索增强生成（RAG）技术，显著提升设计效率并降低功耗。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI设计复杂，EDA工具的参数空间庞大，手动调优效率低且依赖专家经验，亟需自动化解决方案。

Method: CROP包括RTL代码向量化、相似电路匹配系统和RAG增强的LLM参数搜索系统。

Result: 实验显示CROP在工业设计中能以更少迭代实现更优结果，功耗降低9.9%。

Conclusion: CROP为VLSI设计提供了一种高效、自动化的参数调优方法，显著提升设计质量。

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [32] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: 提出了一种高效的潜在扩散框架，结合变分自编码器和条件扩散模型，显著提升了生成模型在数据压缩中的性能。


<details>
  <summary>Details</summary>
Motivation: 生成模型在条件设置下表现优异，但可控性和重建精度限制了其实际应用。

Method: 通过变分自编码器和条件扩散模型，仅压缩少量关键帧并生成其余帧，减少存储需求。

Result: 实验显示，压缩比提升10倍，性能比领先学习方法高63%。

Conclusion: 该方法在减少存储成本的同时实现了高精度的时空重建。

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [33] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: NCPNET是一种针对时序图的端到端共形预测框架，解决了现有方法在动态图中因时间依赖性导致的统计覆盖问题。


<details>
  <summary>Details</summary>
Motivation: 现有共形预测方法主要针对静态图，忽略了真实图中时间依赖性对交换性假设的破坏。

Method: 提出扩散型非共形分数捕捉拓扑和时间不确定性，并开发效率感知优化算法。

Result: 在多个真实时序图数据集上验证，NCPNET显著减少预测集大小（如WIKI数据集减少31%）。

Conclusion: NCPNET在动态图中提供统计覆盖保证，提升计算效率。

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>


### [34] [Statistical Inference for Responsiveness Verification](https://arxiv.org/abs/2507.02169)
*Seung Hyun Cheon,Meredith Stewart,Bogdan Kulynych,Tsui-Wei Weng,Berk Ustun*

Main category: cs.LG

TL;DR: 论文提出了一种验证机器学习模型预测响应性的方法，通过干预特征来评估模型的敏感性，支持黑盒测试和实际应用。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在分配预测时（如贷款、招聘或内容审核）常因未考虑个体输入变化而导致安全失败，需一种验证方法。

Method: 提出基于敏感性分析的响应性验证框架，通过指定干预约束和下游效应分布，估计模型预测的响应性。

Result: 开发了算法生成可达点样本，支持模型预测的响应性估计，应用于再犯预测、器官移植优先级和内容审核等场景。

Conclusion: 该方法能有效提升机器学习模型在实际应用中的安全性。

Abstract: Many safety failures in machine learning arise when models are used to assign
predictions to people (often in settings like lending, hiring, or content
moderation) without accounting for how individuals can change their inputs. In
this work, we introduce a formal validation procedure for the responsiveness of
predictions with respect to interventions on their features. Our procedure
frames responsiveness as a type of sensitivity analysis in which practitioners
control a set of changes by specifying constraints over interventions and
distributions over downstream effects. We describe how to estimate
responsiveness for the predictions of any model and any dataset using only
black-box access, and how to use these estimates to support tasks such as
falsification and failure probability estimation. We develop algorithms that
construct these estimates by generating a uniform sample of reachable points,
and demonstrate how they can promote safety in real-world applications such as
recidivism prediction, organ transplant prioritization, and content moderation.

</details>


### [35] [Metric Design != Metric Behavior: Improving Metric Selection for the Unbiased Evaluation of Dimensionality Reduction](https://arxiv.org/abs/2507.02225)
*Jiyeon Bae,Hyeon Jeon,Jinwook Seo*

Main category: cs.LG

TL;DR: 提出了一种通过聚类评估指标来减少降维投影评估偏差的新工作流程。


<details>
  <summary>Details</summary>
Motivation: 降维投影评估中，若选择高度相关的指标可能导致偏差，需解决这一问题。

Method: 基于经验相关性聚类评估指标，减少重叠，并从每个簇中选择代表性指标。

Result: 实验表明该方法提高了评估的稳定性，减少了偏差。

Conclusion: 该工作流程有效减少了降维投影评估中的偏差。

Abstract: Evaluating the accuracy of dimensionality reduction (DR) projections in
preserving the structure of high-dimensional data is crucial for reliable
visual analytics. Diverse evaluation metrics targeting different structural
characteristics have thus been developed. However, evaluations of DR
projections can become biased if highly correlated metrics--those measuring
similar structural characteristics--are inadvertently selected, favoring DR
techniques that emphasize those characteristics. To address this issue, we
propose a novel workflow that reduces bias in the selection of evaluation
metrics by clustering metrics based on their empirical correlations rather than
on their intended design characteristics alone. Our workflow works by computing
metric similarity using pairwise correlations, clustering metrics to minimize
overlap, and selecting a representative metric from each cluster. Quantitative
experiments demonstrate that our approach improves the stability of DR
evaluation, which indicates that our workflow contributes to mitigating
evaluation bias.

</details>


### [36] [PhysicsCorrect: A Training-Free Approach for Stable Neural PDE Simulations](https://arxiv.org/abs/2507.02227)
*Xinquan Huang,Paris Perdikaris*

Main category: cs.LG

TL;DR: PhysicsCorrect是一种无需训练的校正框架，通过基于PDE残差的线性化逆问题解决神经网络在长期预测中的误差累积问题，显著提升物理模拟的准确性。


<details>
  <summary>Details</summary>
Motivation: 神经网络作为PDE求解的替代方法存在长期预测误差累积的问题，导致物理解失效。

Method: 提出PhysicsCorrect框架，利用预计算的Jacobian和伪逆矩阵，以线性化逆问题形式校正PDE残差。

Result: 在三种PDE系统中，预测误差降低100倍，推理时间增加不足5%。

Conclusion: PhysicsCorrect将不稳定的神经代理转化为可靠的模拟工具，平衡了深度学习效率与物理保真度。

Abstract: Neural networks have emerged as powerful surrogates for solving partial
differential equations (PDEs), offering significant computational speedups over
traditional methods. However, these models suffer from a critical limitation:
error accumulation during long-term rollouts, where small inaccuracies compound
exponentially, eventually causing complete divergence from physically valid
solutions. We present PhysicsCorrect, a training-free correction framework that
enforces PDE consistency at each prediction step by formulating correction as a
linearized inverse problem based on PDE residuals. Our key innovation is an
efficient caching strategy that precomputes the Jacobian and its pseudoinverse
during an offline warm-up phase, reducing computational overhead by two orders
of magnitude compared to standard correction approaches. Across three
representative PDE systems -- Navier-Stokes fluid dynamics, wave equations, and
the chaotic Kuramoto-Sivashinsky equation -- PhysicsCorrect reduces prediction
errors by up to 100x while adding negligible inference time (under 5\%). The
framework integrates seamlessly with diverse architectures including Fourier
Neural Operators, UNets, and Vision Transformers, effectively transforming
unstable neural surrogates into reliable simulation tools that bridge the gap
between deep learning's computational efficiency and the physical fidelity
demanded by practical scientific applications.

</details>


### [37] [VERBA: Verbalizing Model Differences Using Large Language Models](https://arxiv.org/abs/2507.02241)
*Shravan Doda,Shashidhar Reddy Javaji,Zining Zhu*

Main category: cs.LG

TL;DR: 论文提出VERBA方法，利用大语言模型（LLM）生成模型差异的文本描述，以解决模型选择中大量成对比较的问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习中存在'模型湖'现象，即大量性能相似但行为不同的模型难以比较和选择，手动成对比较成本过高。

Method: 通过LLM采样生成模型差异的文本描述，并设计协议评估其信息量。

Result: VERBA在决策树模型上能准确描述差异（80%准确率），结合结构信息后提升至90%。

Conclusion: VERBA为提升模型透明度和可比性提供了新思路。

Abstract: In the current machine learning landscape, we face a "model lake" phenomenon:
Given a task, there is a proliferation of trained models with similar
performances despite different behavior. For model users attempting to navigate
and select from the models, documentation comparing model pairs is helpful.
However, for every $N$ models there could be $O(N^2)$ pairwise comparisons, a
number prohibitive for the model developers to manually perform pairwise
comparisons and prepare documentations. To facilitate fine-grained pairwise
comparisons among models, we introduced $\textbf{VERBA}$. Our approach
leverages a large language model (LLM) to generate verbalizations of model
differences by sampling from the two models. We established a protocol that
evaluates the informativeness of the verbalizations via simulation. We also
assembled a suite with a diverse set of commonly used machine learning models
as a benchmark. For a pair of decision tree models with up to 5% performance
difference but 20-25% behavioral differences, $\textbf{VERBA}$ effectively
verbalizes their variations with up to 80% overall accuracy. When we included
the models' structural information, the verbalization's accuracy further
improved to 90%. $\textbf{VERBA}$ opens up new research avenues for improving
the transparency and comparability of machine learning models in a post-hoc
manner.

</details>


### [38] [Order Acquisition Under Competitive Pressure: A Rapidly Adaptive Reinforcement Learning Approach for Ride-Hailing Subsidy Strategies](https://arxiv.org/abs/2507.02244)
*Fangzhou Shi,Xiaopeng Ke,Xinye Xiong,Kexin Meng,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的补贴策略框架FCA-RL，用于动态适应竞争对手的价格调整，并通过实验验证其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 网约车聚合平台的竞争排名机制促使服务提供商通过优惠券策略降低价格以获取更多订单，但现有研究较少，设计动态适应市场波动且优化订单获取的策略是重要挑战。

Method: 提出FCA-RL框架，结合快速竞争适应（FCA）和强化拉格朗日调整（RLA）技术，并开发了专用模拟环境RideGym。

Result: 实验结果表明，FCA-RL在多样化市场条件下均优于基线方法。

Conclusion: FCA-RL框架在优化网约车服务提供商的补贴策略方面具有显著效果。

Abstract: The proliferation of ride-hailing aggregator platforms presents significant
growth opportunities for ride-service providers by increasing order volume and
gross merchandise value (GMV). On most ride-hailing aggregator platforms,
service providers that offer lower fares are ranked higher in listings and,
consequently, are more likely to be selected by passengers. This competitive
ranking mechanism creates a strong incentive for service providers to adopt
coupon strategies that lower prices to secure a greater number of orders, as
order volume directly influences their long-term viability and sustainability.
Thus, designing an effective coupon strategy that can dynamically adapt to
market fluctuations while optimizing order acquisition under budget constraints
is a critical research challenge. However, existing studies in this area remain
scarce.
  To bridge this gap, we propose FCA-RL, a novel reinforcement learning-based
subsidy strategy framework designed to rapidly adapt to competitors' pricing
adjustments. Our approach integrates two key techniques: Fast Competition
Adaptation (FCA), which enables swift responses to dynamic price changes, and
Reinforced Lagrangian Adjustment (RLA), which ensures adherence to budget
constraints while optimizing coupon decisions on new price landscape.
Furthermore, we introduce RideGym, the first dedicated simulation environment
tailored for ride-hailing aggregators, facilitating comprehensive evaluation
and benchmarking of different pricing strategies without compromising
real-world operational efficiency. Experimental results demonstrate that our
proposed method consistently outperforms baseline approaches across diverse
market conditions, highlighting its effectiveness in subsidy optimization for
ride-hailing service providers.

</details>


### [39] [Uncertainty-aware Reward Design Process](https://arxiv.org/abs/2507.02256)
*Yang Yang,Xiaolu Zhou,Bosong Ding,Miao Xin*

Main category: cs.LG

TL;DR: 论文提出了一种名为URDP的新框架，结合大型语言模型和贝叶斯优化，以高效设计强化学习中的奖励函数。


<details>
  <summary>Details</summary>
Motivation: 传统奖励函数设计方法效率低且不一致，现有方法如大型语言模型在数值优化上表现不佳，进化搜索范式则资源利用效率低。

Method: URDP通过自一致性分析量化奖励函数的不确定性，引入不确定性感知的贝叶斯优化（UABO），并采用双层优化架构。

Result: 在35个任务上的实验表明，URDP能生成更高质量的奖励函数，并显著提升设计效率。

Conclusion: URDP通过结合语言模型的逻辑推理和贝叶斯优化的数值优化能力，解决了奖励函数设计的效率和质量问题。

Abstract: Designing effective reward functions is a cornerstone of reinforcement
learning (RL), yet it remains a challenging process due to the inefficiencies
and inconsistencies inherent in conventional reward engineering methodologies.
Recent advances have explored leveraging large language models (LLMs) to
automate reward function design. However, their suboptimal performance in
numerical optimization often yields unsatisfactory reward quality, while the
evolutionary search paradigm demonstrates inefficient utilization of simulation
resources, resulting in prohibitively lengthy design cycles with
disproportionate computational overhead. To address these challenges, we
propose the Uncertainty-aware Reward Design Process (URDP), a novel framework
that integrates large language models to streamline reward function design and
evaluation in RL environments. URDP quantifies candidate reward function
uncertainty based on self-consistency analysis, enabling simulation-free
identification of ineffective reward components while discovering novel reward
components. Furthermore, we introduce uncertainty-aware Bayesian optimization
(UABO), which incorporates uncertainty estimation to significantly enhance
hyperparameter configuration efficiency. Finally, we construct a bi-level
optimization architecture by decoupling the reward component optimization and
the hyperparameter tuning. URDP orchestrates synergistic collaboration between
the reward logic reasoning of the LLMs and the numerical optimization strengths
of the Bayesian Optimization. We conduct a comprehensive evaluation of URDP
across 35 diverse tasks spanning three benchmark environments. Our experimental
results demonstrate that URDP not only generates higher-quality reward
functions but also achieves significant improvements in the efficiency of
automated reward design compared to existing approaches.

</details>


### [40] [Knowledge Graph-Based Explainable and Generalized Zero-Shot Semantic Communications](https://arxiv.org/abs/2507.02291)
*Zhaoyu Zhang,Lingyi Wang,Wei Wu,Fuhui Zhou,Qihui Wu*

Main category: cs.LG

TL;DR: 提出了一种基于知识图的零样本语义通信网络（KGZS-SC），通过结构化语义信息提升泛化能力和推理能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的语义通信缺乏可解释性和泛化能力，尤其在面对未见数据时表现不佳。

Method: 利用知识图语义知识库（KG-SKB）对齐语义特征，选择性传输紧凑视觉语义；接收端采用零样本学习（ZSL）直接分类未见数据。

Result: 在APY数据集上，KGZS-SC网络展现出强大泛化能力，在不同信噪比下对未见类别的分类表现显著优于现有框架。

Conclusion: KGZS-SC网络通过知识图增强和零样本学习，有效解决了语义通信的泛化和效率问题。

Abstract: Data-driven semantic communication is based on superficial statistical
patterns, thereby lacking interpretability and generalization, especially for
applications with the presence of unseen data. To address these challenges, we
propose a novel knowledge graph-enhanced zero-shot semantic communication
(KGZS-SC) network. Guided by the structured semantic information from a
knowledge graph-based semantic knowledge base (KG-SKB), our scheme provides
generalized semantic representations and enables reasoning for unseen cases.
Specifically, the KG-SKB aligns the semantic features in a shared category
semantics embedding space and enhances the generalization ability of the
transmitter through aligned semantic features, thus reducing communication
overhead by selectively transmitting compact visual semantics. At the receiver,
zero-shot learning (ZSL) is leveraged to enable direct classification for
unseen cases without the demand for retraining or additional computational
overhead, thereby enhancing the adaptability and efficiency of the
classification process in dynamic or resource-constrained environments. The
simulation results conducted on the APY datasets show that the proposed KGZS-SC
network exhibits robust generalization and significantly outperforms existing
SC frameworks in classifying unseen categories across a range of SNR levels.

</details>


### [41] [Holistic Continual Learning under Concept Drift with Adaptive Memory Realignment](https://arxiv.org/abs/2507.02310)
*Alif Ashrafee,Jedrzej Kozal,Michal Wozniak,Bartosz Krawczyk*

Main category: cs.LG

TL;DR: 论文提出了一种名为AMR的轻量级方法，用于动态数据流中的持续学习，通过选择性更新记忆缓冲区来应对概念漂移，性能接近完全重新学习但成本更低。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法假设数据分布静态，忽视了现实数据流的动态性（如概念漂移），需要兼顾稳定性和快速适应性。

Method: 提出AMR方法，通过选择性移除过时样本并补充新样本，动态调整记忆缓冲区，以低成本适应概念漂移。

Result: 在多个概念漂移数据集上，AMR表现接近完全重新学习，但显著减少了标注和计算开销。

Conclusion: AMR是一种可扩展的解决方案，能在非静态环境中平衡稳定性和可塑性。

Abstract: Traditional continual learning methods prioritize knowledge retention and
focus primarily on mitigating catastrophic forgetting, implicitly assuming that
the data distribution of previously learned tasks remains static. This
overlooks the dynamic nature of real-world data streams, where concept drift
permanently alters previously seen data and demands both stability and rapid
adaptation.
  We introduce a holistic framework for continual learning under concept drift
that simulates realistic scenarios by evolving task distributions. As a
baseline, we consider Full Relearning (FR), in which the model is retrained
from scratch on newly labeled samples from the drifted distribution. While
effective, this approach incurs substantial annotation and computational
overhead. To address these limitations, we propose Adaptive Memory Realignment
(AMR), a lightweight alternative that equips rehearsal-based learners with a
drift-aware adaptation mechanism. AMR selectively removes outdated samples of
drifted classes from the replay buffer and repopulates it with a small number
of up-to-date instances, effectively realigning memory with the new
distribution. This targeted resampling matches the performance of FR while
reducing the need for labeled data and computation by orders of magnitude.
  To enable reproducible evaluation, we introduce four concept-drift variants
of standard vision benchmarks: Fashion-MNIST-CD, CIFAR10-CD, CIFAR100-CD, and
Tiny-ImageNet-CD, where previously seen classes reappear with shifted
representations. Comprehensive experiments on these datasets using several
rehearsal-based baselines show that AMR consistently counters concept drift,
maintaining high accuracy with minimal overhead. These results position AMR as
a scalable solution that reconciles stability and plasticity in non-stationary
continual learning environments.

</details>


### [42] [Transformer-based EEG Decoding: A Survey](https://arxiv.org/abs/2507.02320)
*Haodong Zhang,Hongqi Li*

Main category: cs.LG

TL;DR: 本文综述了Transformer模型在EEG解码中的最新应用，探讨了其架构演变、混合模型设计及未来挑战。


<details>
  <summary>Details</summary>
Motivation: EEG解码是脑机接口研究的前沿，传统机器学习方法逐渐被深度学习方法取代，尤其是Transformer因其强大的序列数据处理能力受到关注。

Method: 文章梳理了Transformer在EEG解码中的基础应用、混合架构（如与卷积/循环/图神经网络结合）以及定制化Transformer结构的改进。

Result: 总结了Transformer在EEG解码中的优势及当前研究进展，展示了其潜力。

Conclusion: 讨论了该领域的挑战与未来发展方向，旨在为读者提供清晰的研究现状和未来研究启示。

Abstract: Electroencephalography (EEG) is one of the most common signals used to
capture the electrical activity of the brain, and the decoding of EEG, to
acquire the user intents, has been at the forefront of brain-computer/machine
interfaces (BCIs/BMIs) research. Compared to traditional EEG analysis methods
with machine learning, the advent of deep learning approaches have gradually
revolutionized the field by providing an end-to-end long-cascaded architecture,
which can learn more discriminative features automatically. Among these,
Transformer is renowned for its strong handling capability of sequential data
by the attention mechanism, and the application of Transformers in various EEG
processing tasks is increasingly prevalent. This article delves into a relevant
survey, summarizing the latest application of Transformer models in EEG
decoding since it appeared. The evolution of the model architecture is followed
to sort and organize the related advances, in which we first elucidate the
fundamentals of the Transformer that benefits EEG decoding and its direct
application. Then, the common hybrid architectures by integrating basic
Transformer with other deep learning techniques
(convolutional/recurrent/graph/spiking neural netwo-rks, generative adversarial
networks, diffusion models, etc.) is overviewed in detail. The research
advances of applying the modified intrinsic structures of customized
Transformer have also been introduced. Finally, the current challenges and
future development prospects in this rapidly evolving field are discussed. This
paper aims to help readers gain a clear understanding of the current state of
Transformer applications in EEG decoding and to provide valuable insights for
future research endeavors.

</details>


### [43] [DeltaSHAP: Explaining Prediction Evolutions in Online Patient Monitoring with Shapley Values](https://arxiv.org/abs/2507.02342)
*Changhun Kim,Yechan Mun,Sangchul Hahn,Eunho Yang*

Main category: cs.LG

TL;DR: DeltaSHAP是一种新型可解释人工智能算法，专为在线患者监测系统设计，通过改进Shapley值适应临床时间序列解释需求，提供实时、高效的解释。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法无法满足临床时间序列解释的独特需求，如连续预测变化的原因分析、特征归因的大小和方向，以及实时性。

Method: DeltaSHAP通过改进Shapley值，捕捉特征组合效应，并仅使用实际观察到的特征组合进行预测变化归因。

Result: 在MIMIC-III基准测试中，DeltaSHAP在解释质量上优于现有方法62%，计算效率提升33%。

Conclusion: DeltaSHAP为临床时间序列解释提供了一种高效、实用的解决方案，适用于时间敏感的医疗应用。

Abstract: This study proposes DeltaSHAP, a novel explainable artificial intelligence
(XAI) algorithm specifically designed for online patient monitoring systems. In
clinical environments, discovering the causes driving patient risk evolution is
critical for timely intervention, yet existing XAI methods fail to address the
unique requirements of clinical time series explanation tasks. To this end,
DeltaSHAP addresses three key clinical needs: explaining the changes in the
consecutive predictions rather than isolated prediction scores, providing both
magnitude and direction of feature attributions, and delivering these insights
in real time. By adapting Shapley values to temporal settings, our approach
accurately captures feature coalition effects. It further attributes prediction
changes using only the actually observed feature combinations, making it
efficient and practical for time-sensitive clinical applications. We also
introduce new evaluation metrics to evaluate the faithfulness of the
attributions for online time series, and demonstrate through experiments on
online patient monitoring tasks that DeltaSHAP outperforms state-of-the-art XAI
methods in both explanation quality as 62% and computational efficiency as 33%
time reduction on the MIMIC-III decompensation benchmark. We release our code
at https://github.com/AITRICS/DeltaSHAP.

</details>


### [44] [Offline Reinforcement Learning with Penalized Action Noise Injection](https://arxiv.org/abs/2507.02356)
*JunHyeok Oh,Byung-Jun Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为PANI的简单方法，通过注入噪声动作来增强离线强化学习性能，同时惩罚噪声量，无需复杂扩散模型。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习在环境交互成本高时具有实用性，但现有扩散模型计算开销大，是否必要存疑。

Method: 提出PANI方法，通过噪声注入动作覆盖动作空间，并根据噪声量进行惩罚。

Result: PANI在多种基准测试中显著提升性能，且兼容现有算法。

Conclusion: PANI是一种简单有效的方法，无需复杂模型即可提升离线强化学习性能。

Abstract: Offline reinforcement learning (RL) optimizes a policy using only a fixed
dataset, making it a practical approach in scenarios where interaction with the
environment is costly. Due to this limitation, generalization ability is key to
improving the performance of offline RL algorithms, as demonstrated by recent
successes of offline RL with diffusion models. However, it remains questionable
whether such diffusion models are necessary for highly performing offline RL
algorithms, given their significant computational requirements during
inference. In this paper, we propose Penalized Action Noise Injection (PANI), a
method that simply enhances offline learning by utilizing noise-injected
actions to cover the entire action space, while penalizing according to the
amount of noise injected. This approach is inspired by how diffusion models
have worked in offline RL algorithms. We provide a theoretical foundation for
this method, showing that offline RL algorithms with such noise-injected
actions solve a modified Markov Decision Process (MDP), which we call the noisy
action MDP. PANI is compatible with a wide range of existing off-policy and
offline RL algorithms, and despite its simplicity, it demonstrates significant
performance improvements across various benchmarks.

</details>


### [45] [Deep Reinforcement Learning-Based DRAM Equalizer Parameter Optimization Using Latent Representations](https://arxiv.org/abs/2507.02365)
*Muhammad Usama,Dong Eui Chang*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动和强化学习的均衡器参数优化方法，显著提升了高速DRAM系统的信号完整性。


<details>
  <summary>Details</summary>
Motivation: 高速DRAM系统中均衡器参数优化对信号完整性至关重要，但传统方法计算量大或依赖模型。

Method: 结合学习的潜在信号表示和模型无关的Advantage Actor-Critic强化学习代理，实现高效优化。

Result: 在行业标准DRAM波形上，眼图开窗面积显著提升（42.7%和36.8%），优于现有技术。

Conclusion: 该方法在性能、计算效率和泛化能力上均表现出色，为复杂均衡器架构提供了有效解决方案。

Abstract: Equalizer parameter optimization for signal integrity in high-speed Dynamic
Random Access Memory systems is crucial but often computationally demanding or
model-reliant. This paper introduces a data-driven framework employing learned
latent signal representations for efficient signal integrity evaluation,
coupled with a model-free Advantage Actor-Critic reinforcement learning agent
for parameter optimization. The latent representation captures vital signal
integrity features, offering a fast alternative to direct eye diagram analysis
during optimization, while the reinforcement learning agent derives optimal
equalizer settings without explicit system models. Applied to industry-standard
Dynamic Random Access Memory waveforms, the method achieved significant
eye-opening window area improvements: 42.7\% for cascaded Continuous-Time
Linear Equalizer and Decision Feedback Equalizer structures, and 36.8\% for
Decision Feedback Equalizer-only configurations. These results demonstrate
superior performance, computational efficiency, and robust generalization
across diverse Dynamic Random Access Memory units compared to existing
techniques. Core contributions include an efficient latent signal integrity
metric for optimization, a robust model-free reinforcement learning strategy,
and validated superior performance for complex equalizer architectures.

</details>


### [46] [Improving Consistency in Vehicle Trajectory Prediction Through Preference Optimization](https://arxiv.org/abs/2507.02406)
*Caio Azevedo,Lina Achaji,Stefano Sabatini,Nicola Poerio,Grzegorz Bartyzel,Sascha Hornauer,Fabien Moutarde*

Main category: cs.LG

TL;DR: 通过偏好优化微调多智能体轨迹预测模型，显著提升场景一致性，同时几乎不影响预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在复杂交互场景中难以捕捉智能体间的依赖关系，导致预测不一致。

Method: 利用自动计算的偏好排名微调轨迹预测模型，优化多智能体场景中的预测一致性。

Result: 在三个数据集上显著提升场景一致性，预测精度几乎不受影响，且推理时无额外计算负担。

Conclusion: 偏好优化是提升多智能体轨迹预测一致性的有效方法。

Abstract: Trajectory prediction is an essential step in the pipeline of an autonomous
vehicle. Inaccurate or inconsistent predictions regarding the movement of
agents in its surroundings lead to poorly planned maneuvers and potentially
dangerous situations for the end-user. Current state-of-the-art
deep-learning-based trajectory prediction models can achieve excellent accuracy
on public datasets. However, when used in more complex, interactive scenarios,
they often fail to capture important interdependencies between agents, leading
to inconsistent predictions among agents in the traffic scene. Inspired by the
efficacy of incorporating human preference into large language models, this
work fine-tunes trajectory prediction models in multi-agent settings using
preference optimization. By taking as input automatically calculated preference
rankings among predicted futures in the fine-tuning process, our
experiments--using state-of-the-art models on three separate datasets--show
that we are able to significantly improve scene consistency while minimally
sacrificing trajectory prediction accuracy and without adding any excess
computational requirements at inference time.

</details>


### [47] [S2FGL: Spatial Spectral Federated Graph Learning](https://arxiv.org/abs/2507.02409)
*Zihan Tan,Suyuan Huang,Guancheng Wan,Wenke Huang,He Li,Mang Ye*

Main category: cs.LG

TL;DR: 论文提出了一种结合空间和频谱策略的联邦图学习框架S2FGL，解决了子图联邦学习中标签信号中断和频谱客户端漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前子图联邦学习仅从结构角度研究，忽略了图信号在空间和频谱域的传播问题，导致标签信号中断和频谱客户端漂移。

Method: 提出全局知识库缓解标签信号中断，并通过频率对齐解决频谱客户端漂移，形成框架S2FGL。

Result: 在多个数据集上的实验证明了S2FGL的优越性。

Conclusion: S2FGL有效解决了子图联邦学习中的空间和频谱问题，提升了全局泛化能力。

Abstract: Federated Graph Learning (FGL) combines the privacy-preserving capabilities
of federated learning (FL) with the strong graph modeling capability of Graph
Neural Networks (GNNs). Current research addresses subgraph-FL only from the
structural perspective, neglecting the propagation of graph signals on spatial
and spectral domains of the structure. From a spatial perspective, subgraph-FL
introduces edge disconnections between clients, leading to disruptions in label
signals and a degradation in the class knowledge of the global GNN. From a
spectral perspective, spectral heterogeneity causes inconsistencies in signal
frequencies across subgraphs, which makes local GNNs overfit the local signal
propagation schemes. As a result, spectral client drifts occur, undermining
global generalizability. To tackle the challenges, we propose a global
knowledge repository to mitigate label signal disruption and a frequency
alignment to address spectral client drifts. The combination of spatial and
spectral strategies forms our framework S2FGL. Extensive experiments on
multiple datasets demonstrate the superiority of S2FGL. The code is available
at https://github.com/Wonder7racer/S2FGL.git.

</details>


### [48] [Variational Kolmogorov-Arnold Network](https://arxiv.org/abs/2507.02466)
*Francesco Alesiani,Henrik Christiansen,Federico Errica*

Main category: cs.LG

TL;DR: InfinityKAN通过变分推理优化自适应学习无限基函数，扩展了Kolmogorov Arnold Networks（KANs）的适用性。


<details>
  <summary>Details</summary>
Motivation: KANs基于Kolmogorov-Arnold定理，但作为多层感知机（MLP）的替代方案时，其基函数数量选择是临时的，限制了其应用。

Method: 将问题建模为变分推理优化问题，通过反向传播自适应学习无限基函数。

Result: 提出的InfinityKAN能够动态学习基函数数量，扩展了KANs的潜力。

Conclusion: InfinityKAN通过优化基函数选择，提升了KANs的实用性和灵活性。

Abstract: Kolmogorov Arnold Networks (KANs) are an emerging architecture for building
machine learning models. KANs are based on the theoretical foundation of the
Kolmogorov-Arnold Theorem and its expansions, which provide an exact
representation of a multi-variate continuous bounded function as the
composition of a limited number of univariate continuous functions. While such
theoretical results are powerful, their use as a representation learning
alternative to a multi-layer perceptron (MLP) hinges on the ad-hoc choice of
the number of bases modeling each of the univariate functions. In this work, we
show how to address this problem by adaptively learning a potentially infinite
number of bases for each univariate function during training. We therefore
model the problem as a variational inference optimization problem. Our
proposal, called InfinityKAN, which uses backpropagation, extends the potential
applicability of KANs by treating an important hyperparameter as part of the
learning process.

</details>


### [49] [Continual Gradient Low-Rank Projection Fine-Tuning for LLMs](https://arxiv.org/abs/2507.02503)
*Chenxu Wang,Yilin Lyu,Zicheng Sun,Liping Jing*

Main category: cs.LG

TL;DR: GORP是一种新的训练策略，通过结合全参数和低秩参数，在统一低秩梯度子空间中联合更新，解决了LLMs持续微调中效率与表达能力的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决低秩适应（LoRA）在持续学习中因低秩特性和显式参数约束导致的学习新任务和知识迁移能力受限的问题。

Method: 提出GORP，结合全参数和低秩参数，在统一低秩梯度子空间中联合更新，扩展优化空间并保持效率。

Result: 在持续学习基准测试中表现优于现有方法，有效缓解灾难性遗忘。

Conclusion: GORP是一种高效且表达能力强的持续学习策略，优于现有方法。

Abstract: Continual fine-tuning of Large Language Models (LLMs) is hampered by the
trade-off between efficiency and expressiveness. Low-Rank Adaptation (LoRA)
offers efficiency but constrains the model's ability to learn new tasks and
transfer knowledge due to its low-rank nature and reliance on explicit
parameter constraints. We propose GORP (Gradient LOw Rank Projection) for
Continual Learning, a novel training strategy that overcomes these limitations
by synergistically combining full and low-rank parameters and jointly updating
within a unified low-rank gradient subspace. GORP expands the optimization
space while preserving efficiency and mitigating catastrophic forgetting.
Extensive experiments on continual learning benchmarks demonstrate GORP's
superior performance compared to existing state-of-the-art approaches. Code is
available at https://github.com/Wcxwcxw/GORP.

</details>


### [50] [TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification](https://arxiv.org/abs/2507.02510)
*Ahmed G. Habashi,Ahmed M. Azab,Seif Eldawlatly,Gamal M. Aly*

Main category: cs.LG

TL;DR: 论文提出了一种通过优化预处理和深度学习技术显著提升跨被试运动想象分类性能的新方法，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 跨被试运动想象分类因脑电图模式差异大而准确率低，限制了无校准脑机接口的实际应用。

Method: 采用短时傅里叶变换处理脑电图数据，优化参数，结合平衡批处理策略训练卷积神经网络。

Result: 在多个数据集上表现优异，准确率分别为67.60%、65.96%和80.22%，优于现有技术。

Conclusion: 该方法为无校准运动想象分类设定了新基准，并提供了开放数据集推动研究。

Abstract: Cross-subject motor imagery (CS-MI) classification in brain-computer
interfaces (BCIs) is a challenging task due to the significant variability in
Electroencephalography (EEG) patterns across different individuals. This
variability often results in lower classification accuracy compared to
subject-specific models, presenting a major barrier to developing
calibration-free BCIs suitable for real-world applications. In this paper, we
introduce a novel approach that significantly enhances cross-subject MI
classification performance through optimized preprocessing and deep learning
techniques. Our approach involves direct classification of Short-Time Fourier
Transform (STFT)-transformed EEG data, optimized STFT parameters, and a
balanced batching strategy during training of a Convolutional Neural Network
(CNN). This approach is uniquely validated across four different datasets,
including three widely-used benchmark datasets leading to substantial
improvements in cross-subject classification, achieving 67.60% on the BCI
Competition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on
Dataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we
systematically investigate the classification performance using MI windows
ranging from the full 4-second window to 1-second windows. These results
establish a new benchmark for generalizable, calibration-free MI classification
in addition to contributing a robust open-access dataset to advance research in
this domain.

</details>


### [51] [RetrySQL: text-to-SQL training with retry data for self-correcting query generation](https://arxiv.org/abs/2507.02529)
*Alicja Rączkowska,Riccardo Belluzzo,Piotr Zieliński,Joanna Baran,Paweł Olszewski*

Main category: cs.LG

TL;DR: RetrySQL是一种新的文本到SQL生成模型训练方法，通过引入自我纠正机制，提高了生成准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL任务的研究缺乏针对SQL特定生成模型的探索，同时自我纠正生成策略的应用尚未被研究。

Method: 通过准备参考SQL查询的推理步骤并故意引入错误，生成包含错误和纠正步骤的数据，用于持续预训练开源编码模型。

Result: RetrySQL在整体和挑战性执行准确性指标上提高了4个百分点，并验证了全参数预训练的必要性。

Conclusion: RetrySQL展示了自我纠正机制在文本到SQL任务中的有效性，为SQL导向语言模型提供了一种新的改进生成准确性的方法。

Abstract: The text-to-SQL task is an active challenge in Natural Language Processing.
Many existing solutions focus on using black-box language models extended with
specialized components within customized end-to-end text-to-SQL pipelines.
While these solutions use both closed-source proprietary language models and
coding-oriented open-source models, there is a lack of research regarding
SQL-specific generative models. At the same time, recent advancements in
self-correcting generation strategies show promise for improving the
capabilities of existing architectures. The application of these concepts to
the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL,
a new approach to training text-to-SQL generation models. We prepare reasoning
steps for reference SQL queries and then corrupt them to create retry data that
contains both incorrect and corrected steps, divided with a special token. We
continuously pre-train an open-source coding model with this data and
demonstrate that retry steps yield an improvement of up to 4 percentage points
in both overall and challenging execution accuracy metrics, compared to
pre-training without retry data. Additionally, we confirm that supervised
fine-tuning with LoRA is ineffective for learning from retry data and that
full-parameter pre-training is a necessary requirement for that task. We
showcase that the self-correcting behavior is learned by the model and the
increase in downstream accuracy metrics is a result of this additional skill.
Finally, we incorporate RetrySQL-trained models into the full text-to-SQL
pipeline and showcase that they are competitive in terms of execution accuracy
with proprietary models that contain orders of magnitude more parameters.
RetrySQL demonstrates that self-correction can be learned in the text-to-SQL
task and provides a novel way of improving generation accuracy for SQL-oriented
language models.

</details>


### [52] [Position: A Theory of Deep Learning Must Include Compositional Sparsity](https://arxiv.org/abs/2507.02550)
*David A. Danhofer,Davide D'Ascenzo,Rafael Dubach,Tomaso Poggio*

Main category: cs.LG

TL;DR: 论文认为深度神经网络（DNNs）的成功源于其能够利用目标函数的组合稀疏结构，这种结构普遍存在于高效可计算函数中。


<details>
  <summary>Details</summary>
Motivation: 探讨DNNs成功背后的基本原理，尤其是其如何利用组合稀疏性来克服高维问题。

Method: 通过理论分析，论证组合稀疏性是DNNs成功的关键，并指出其在高效可计算函数中的普遍性。

Result: 组合稀疏性是DNNs成功的重要原则，但其在学习和优化中的具体作用仍需进一步研究。

Conclusion: 理解组合稀疏性在深度学习中的作用对构建全面的人工智能理论至关重要。

Abstract: Overparametrized Deep Neural Networks (DNNs) have demonstrated remarkable
success in a wide variety of domains too high-dimensional for classical shallow
networks subject to the curse of dimensionality. However, open questions about
fundamental principles, that govern the learning dynamics of DNNs, remain. In
this position paper we argue that it is the ability of DNNs to exploit the
compositionally sparse structure of the target function driving their success.
As such, DNNs can leverage the property that most practically relevant
functions can be composed from a small set of constituent functions, each of
which relies only on a low-dimensional subset of all inputs. We show that this
property is shared by all efficiently Turing-computable functions and is
therefore highly likely present in all current learning problems. While some
promising theoretical insights on questions concerned with approximation and
generalization exist in the setting of compositionally sparse functions,
several important questions on the learnability and optimization of DNNs
remain. Completing the picture of the role of compositional sparsity in deep
learning is essential to a comprehensive theory of artificial, and even
general, intelligence.

</details>


### [53] [Transformers Don't Need LayerNorm at Inference Time: Scaling LayerNorm Removal to GPT-2 XL and the Implications for Mechanistic Interpretability](https://arxiv.org/abs/2507.02559)
*Luca Baroni,Galvin Khara,Joachim Schaeffer,Marat Subkhankulov,Stefan Heimersheim*

Main category: cs.LG

TL;DR: 研究表明，层归一化（LN）在GPT-2模型中并非必需，移除后仅轻微增加验证损失，且不影响语言建模能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LN在推理阶段的作用及其对模型可解释性的影响。

Method: 移除GPT-2模型中的所有LN层，并评估其性能变化。

Result: 移除LN层仅导致验证损失小幅增加（如GPT-2 XL增加0.03交叉熵损失），且不影响语言建模能力。

Conclusion: LN层在语言建模中作用有限，移除后模型仍能有效运行，为可解释性研究提供了新工具。

Abstract: Layer-wise normalization (LN) is an essential component of virtually all
transformer-based large language models. While its effects on training
stability are well documented, its role at inference time is poorly understood.
Additionally, LN layers hinder mechanistic interpretability by introducing
additional nonlinearities and increasing the interconnectedness of individual
model components. Here, we show that all LN layers can be removed from every
GPT-2 model with only a small increase in validation loss (e.g. +0.03
cross-entropy loss for GPT-2 XL). Thus, LN cannot play a substantial role in
language modeling. We find that the amount of fine-tuning data needed for LN
removal grows sublinearly with model parameters, suggesting scaling to larger
models is feasible. We release a suite of LN-free GPT-2 models on Hugging Face.
Furthermore, we test interpretability techniques on LN-free models. Direct
logit attribution now gives the exact direct effect of individual components,
while the accuracy of attribution patching does not significantly improve. We
also confirm that GPT-2's "confidence neurons" are inactive in the LN-free
models. Our work clarifies the role of LN layers in language modeling, showing
that GPT-2-class models can function without LN layers. We hope that our
LN-free analogs of the GPT-2 family of models will enable more precise
interpretability research and improve our understanding of language models.

</details>


### [54] [Scalable Interconnect Learning in Boolean Networks](https://arxiv.org/abs/2507.02585)
*Fabian Kresse,Emily Yu,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 论文扩展了可微分布尔逻辑网络（DBNs），通过可训练、参数固定的互联结构使其适用于更宽的输入层，同时提出两种剪枝方法以减小模型规模。


<details>
  <summary>Details</summary>
Motivation: 提升DBNs在资源受限硬件上的扩展性和效率，同时保持其高精度。

Method: 引入可训练、参数固定的互联结构；提出基于SAT的逻辑等价剪枝和数据驱动的相似性剪枝。

Result: DBNs能够扩展到更宽的输入层，剪枝方法在压缩和精度之间取得更好平衡。

Conclusion: 扩展的DBNs和剪枝方法显著提升了模型的效率和可扩展性。

Abstract: Learned Differentiable Boolean Logic Networks (DBNs) already deliver
efficient inference on resource-constrained hardware. We extend them with a
trainable, differentiable interconnect whose parameter count remains constant
as input width grows, allowing DBNs to scale to far wider layers than earlier
learnable-interconnect designs while preserving their advantageous accuracy. To
further reduce model size, we propose two complementary pruning stages: an
SAT-based logic equivalence pass that removes redundant gates without affecting
performance, and a similarity-based, data-driven pass that outperforms a
magnitude-style greedy baseline and offers a superior compression-accuracy
trade-off.

</details>


### [55] [Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation](https://arxiv.org/abs/2507.02608)
*François Rozet,Ruben Ohana,Michael McCabe,Gilles Louppe,François Lanusse,Shirley Ho*

Main category: cs.LG

TL;DR: 论文探讨了在动力学系统仿真中应用潜在空间扩散模型的可行性及其效果，发现其在高压缩率下仍保持准确性，且优于非生成式模型。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在推理时的高计算成本限制了其作为快速物理仿真器的应用，研究旨在探索潜在空间仿真是否能解决这一问题。

Method: 通过潜在空间扩散模型仿真动力学系统，测试不同压缩率下的效果，并与非生成式模型对比。

Result: 潜在空间仿真在高压缩率（高达1000倍）下仍保持准确性，扩散模型比非生成式模型更准确且预测多样性更强。

Conclusion: 潜在空间扩散模型在动力学系统仿真中具有高效性和准确性，设计选择（如架构和优化器）对训练效果至关重要。

Abstract: The steep computational cost of diffusion models at inference hinders their
use as fast physics emulators. In the context of image and video generation,
this computational drawback has been addressed by generating in the latent
space of an autoencoder instead of the pixel space. In this work, we
investigate whether a similar strategy can be effectively applied to the
emulation of dynamical systems and at what cost. We find that the accuracy of
latent-space emulation is surprisingly robust to a wide range of compression
rates (up to 1000x). We also show that diffusion-based emulators are
consistently more accurate than non-generative counterparts and compensate for
uncertainty in their predictions with greater diversity. Finally, we cover
practical design choices, spanning from architectures to optimizers, that we
found critical to train latent-space emulators.

</details>


### [56] [L-VAE: Variational Auto-Encoder with Learnable Beta for Disentangled Representation](https://arxiv.org/abs/2507.02619)
*Hazal Mogultay Ozcan,Sinan Kalkan,Fatos T. Yarman-Vural*

Main category: cs.LG

TL;DR: L-VAE是一种新型模型，通过学习损失函数的超参数和表示解耦，改进了β-VAE的动态权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决β-VAE中超参数η需经验调整的局限性，实现损失函数权重与模型参数的联合学习。

Method: 提出L-VAE模型，通过添加正则化项动态平衡解耦和重构损失，同时学习模型架构参数。

Result: 在多个数据集上表现优于或接近现有模型，成功解耦面部属性。

Conclusion: L-VAE在解耦和重构之间找到了有效平衡，性能优于或接近现有方法。

Abstract: In this paper, we propose a novel model called Learnable VAE (L-VAE), which
learns a disentangled representation together with the hyperparameters of the
cost function. L-VAE can be considered as an extension of \b{eta}-VAE, wherein
the hyperparameter, \b{eta}, is empirically adjusted. L-VAE mitigates the
limitations of \b{eta}-VAE by learning the relative weights of the terms in the
loss function to control the dynamic trade-off between disentanglement and
reconstruction losses. In the proposed model, the weight of the loss terms and
the parameters of the model architecture are learned concurrently. An
additional regularization term is added to the loss function to prevent bias
towards either reconstruction or disentanglement losses. Experimental analyses
show that the proposed L-VAE finds an effective balance between reconstruction
fidelity and disentangling the latent dimensions. Comparisons of the proposed
L-VAE against \b{eta}-VAE, VAE, ControlVAE, DynamicVAE, and {\sigma}-VAE on
datasets, such as dSprites, MPI3D-complex, Falcor3D, and Isaac3D reveals that
L-VAE consistently provides the best or the second best performances measured
by a set of disentanglement metrics. Moreover, qualitative experiments on
CelebA dataset, confirm the success of the L-VAE model for disentangling the
facial attributes.

</details>


### [57] [A Matrix Variational Auto-Encoder for Variant Effect Prediction in Pharmacogenes](https://arxiv.org/abs/2507.02624)
*Antoine Honoré,Borja Rodríguez Gálvez,Yoomi Park,Yitian Zhou,Volker M. Lauschke,Ming Xiao*

Main category: cs.LG

TL;DR: 提出一种基于Transformer的矩阵变分自编码器（matVAE），用于评估蛋白质变体的功能影响，并在DMS数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖多序列比对（MSA），但某些药物基因进化压力低，假设不成立。DMS数据集提供了变体的定量适应性评分，可作为替代。

Method: 使用matVAE模型，结合结构化先验，并在33个DMS数据集上评估性能。还比较了基于MSA和DMS训练的模型，并引入AlphaFold结构信息。

Result: matVAE-MSA在零样本预测中优于DeepSequence，参数更少且计算效率更高。结合AlphaFold结构后，性能进一步提升。

Conclusion: DMS数据集有望替代MSA，且性能损失小，值得进一步开发和探索。

Abstract: Variant effect predictors (VEPs) aim to assess the functional impact of
protein variants, traditionally relying on multiple sequence alignments (MSAs).
This approach assumes that naturally occurring variants are fit, an assumption
challenged by pharmacogenomics, where some pharmacogenes experience low
evolutionary pressure. Deep mutational scanning (DMS) datasets provide an
alternative by offering quantitative fitness scores for variants. In this work,
we propose a transformer-based matrix variational auto-encoder (matVAE) with a
structured prior and evaluate its performance on 33 DMS datasets corresponding
to 26 drug target and ADME proteins from the ProteinGym benchmark. Our model
trained on MSAs (matVAE-MSA) outperforms the state-of-the-art DeepSequence
model in zero-shot prediction on DMS datasets, despite using an order of
magnitude fewer parameters and requiring less computation at inference time. We
also compare matVAE-MSA to matENC-DMS, a model of similar capacity trained on
DMS data, and find that the latter performs better on supervised prediction
tasks. Additionally, incorporating AlphaFold-generated structures into our
transformer model further improves performance, achieving results comparable to
DeepSequence trained on MSAs and finetuned on DMS. These findings highlight the
potential of DMS datasets to replace MSAs without significant loss in
predictive performance, motivating further development of DMS datasets and
exploration of their relationships to enhance variant effect prediction.

</details>


### [58] [Medical Data Pecking: A Context-Aware Approach for Automated Quality Evaluation of Structured Medical Data](https://arxiv.org/abs/2507.02628)
*Irena Girshovitz,Atai Ambus,Moni Shahar,Ran Gilad-Bachrach*

Main category: cs.LG

TL;DR: 论文提出了一种基于软件工程概念的数据质量评估方法（Medical Data Pecking），通过自动化测试工具（MDPT）识别EHR数据中的问题，提高了研究结果的可靠性。


<details>
  <summary>Details</summary>
Motivation: EHR数据在流行病学研究和AI训练中应用广泛，但其数据质量问题（如偏差、错误）影响了研究结果的可靠性，现有评估方法不足。

Method: 采用软件工程中的单元测试和覆盖率概念，开发了MDPT工具，包括自动化测试生成器和数据测试框架，用于识别数据问题。

Result: 在三个数据集（AoU、MIMIC-III、SyntheticMass）上测试，成功识别了20-43个数据问题，并分析了LLM生成测试的准确性。

Conclusion: 该方法通过结合外部医学知识，提升了数据质量测试的上下文敏感性，为未来改进（如多模态数据支持）奠定了基础。

Abstract: Background: The use of Electronic Health Records (EHRs) for epidemiological
studies and artificial intelligence (AI) training is increasing rapidly. The
reliability of the results depends on the accuracy and completeness of EHR
data. However, EHR data often contain significant quality issues, including
misrepresentations of subpopulations, biases, and systematic errors, as they
are primarily collected for clinical and billing purposes. Existing quality
assessment methods remain insufficient, lacking systematic procedures to assess
data fitness for research.
  Methods: We present the Medical Data Pecking approach, which adapts unit
testing and coverage concepts from software engineering to identify data
quality concerns. We demonstrate our approach using the Medical Data Pecking
Tool (MDPT), which consists of two main components: (1) an automated test
generator that uses large language models and grounding techniques to create a
test suite from data and study descriptions, and (2) a data testing framework
that executes these tests, reporting potential errors and coverage.
  Results: We evaluated MDPT on three datasets: All of Us (AoU), MIMIC-III, and
SyntheticMass, generating 55-73 tests per cohort across four conditions. These
tests correctly identified 20-43 non-aligned or non-conforming data issues. We
present a detailed analysis of the LLM-generated test suites in terms of
reference grounding and value accuracy.
  Conclusion: Our approach incorporates external medical knowledge to enable
context-sensitive data quality testing as part of the data analysis workflow to
improve the validity of its outcomes. Our approach tackles these challenges
from a quality assurance perspective, laying the foundation for further
development such as additional data modalities and improved grounding methods.

</details>


### [59] [High-Order Deep Meta-Learning with Category-Theoretic Interpretation](https://arxiv.org/abs/2507.02634)
*David H. Mguni*

Main category: cs.LG

TL;DR: 提出了一种新的分层深度学习框架，用于递归高阶元学习，使神经网络能够构建、解决和泛化任务层次结构。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习依赖人类生成数据的局限性，通过虚拟任务生成和软约束学习，实现任务间的泛化和规则发现。

Method: 采用生成机制创建虚拟任务，通过元学习器迭代优化约束区域，结合范畴论中的函子概念构建分层学习结构。

Result: 框架能够自主生成任务和数据，增强归纳偏置，支持抽象和知识迁移，推动机器学习向通用人工智能发展。

Conclusion: 该架构为下一代神经网络提供了设计原则，有望实现自主生成任务和解决方案，推动通用人工智能的进步。

Abstract: We introduce a new hierarchical deep learning framework for recursive
higher-order meta-learning that enables neural networks (NNs) to construct,
solve, and generalise across hierarchies of tasks. Central to this approach is
a generative mechanism that creates \emph{virtual tasks} -- synthetic problem
instances designed to enable the meta-learner to learn \emph{soft constraints}
and unknown generalisable rules across related tasks. Crucially, this enables
the framework to generate its own informative, task-grounded datasets thereby
freeing machine learning (ML) training from the limitations of relying entirely
on human-generated data. By actively exploring the virtual point landscape and
seeking out tasks lower-level learners find difficult, the meta-learner
iteratively refines constraint regions. This enhances inductive biases,
regularises the adaptation process, and produces novel, unanticipated tasks and
constraints required for generalisation. Each meta-level of the hierarchy
corresponds to a progressively abstracted generalisation of problems solved at
lower levels, enabling a structured and interpretable learning progression. By
interpreting meta-learners as category-theoretic \emph{functors} that generate
and condition a hierarchy of subordinate learners, we establish a compositional
structure that supports abstraction and knowledge transfer across progressively
generalised tasks. The category-theoretic perspective unifies existing
meta-learning models and reveals how learning processes can be transformed and
compared through functorial relationships, while offering practical design
principles for structuring meta-learning. We speculate this architecture may
underpin the next generation of NNs capable of autonomously generating novel,
instructive tasks and their solutions, thereby advancing ML towards general
artificial intelligence.

</details>


### [60] [On Efficient Bayesian Exploration in Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.02639)
*Alberto Caron,Chris Hicks,Vasilios Mavroudis*

Main category: cs.LG

TL;DR: 该论文提出了一种基于信息论的内在动机方法，通过针对认知不确定性设计探索奖励，实现数据高效的强化学习探索。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中数据高效探索的挑战，特别是针对认知不确定性而非环境固有噪声。

Method: 提出了一种基于信息论的探索奖励方法，并通过稀疏变分高斯过程、深度核和深度集成模型实现可操作性。进一步提出了PTS-BE框架，结合模型规划与信息论奖励。

Result: PTS-BE在稀疏奖励或纯探索任务中显著优于其他基线方法。

Conclusion: 该方法为基于信息增益的探索提供了理论保证，并通过PTS-BE框架实现了高效的深度探索。

Abstract: In this work, we address the challenge of data-efficient exploration in
reinforcement learning by examining existing principled, information-theoretic
approaches to intrinsic motivation. Specifically, we focus on a class of
exploration bonuses that targets epistemic uncertainty rather than the
aleatoric noise inherent in the environment. We prove that these bonuses
naturally signal epistemic information gains and converge to zero once the
agent becomes sufficiently certain about the environment's dynamics and
rewards, thereby aligning exploration with genuine knowledge gaps. Our analysis
provides formal guarantees for IG-based approaches, which previously lacked
theoretical grounding. To enable practical use, we also discuss tractable
approximations via sparse variational Gaussian Processes, Deep Kernels and Deep
Ensemble models. We then outline a general framework - Predictive Trajectory
Sampling with Bayesian Exploration (PTS-BE) - which integrates model-based
planning with information-theoretic bonuses to achieve sample-efficient deep
exploration. We empirically demonstrate that PTS-BE substantially outperforms
other baselines across a variety of environments characterized by sparse
rewards and/or purely exploratory tasks.

</details>


### [61] [Fair Deepfake Detectors Can Generalize](https://arxiv.org/abs/2507.02645)
*Harry Cheng,Ming-Hui Liu,Yangyang Guo,Tianyi Wang,Liqiang Nie,Mohan Kankanhalli*

Main category: cs.LG

TL;DR: 论文提出DAID框架，通过因果分析解决深度伪造检测中公平性与泛化性的冲突，并在实验中验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法在公平性与泛化性之间存在冲突，论文首次揭示并定义了两者的因果关系，旨在通过控制混杂变量实现两者平衡。

Method: 提出DAID框架，包括人口统计属性感知的数据再平衡（逆倾向加权和子群特征归一化）和人口统计无关的特征聚合（对齐损失抑制敏感信号）。

Result: 在三个跨域基准测试中，DAID在公平性和泛化性上均优于现有先进检测器。

Conclusion: DAID通过因果分析解决了公平性与泛化性的冲突，实验验证了其理论基础和实际效果。

Abstract: Deepfake detection models face two critical challenges: generalization to
unseen manipulations and demographic fairness among population groups. However,
existing approaches often demonstrate that these two objectives are inherently
conflicting, revealing a trade-off between them. In this paper, we, for the
first time, uncover and formally define a causal relationship between fairness
and generalization. Building on the back-door adjustment, we show that
controlling for confounders (data distribution and model capacity) enables
improved generalization via fairness interventions. Motivated by this insight,
we propose Demographic Attribute-insensitive Intervention Detection (DAID), a
plug-and-play framework composed of: i) Demographic-aware data rebalancing,
which employs inverse-propensity weighting and subgroup-wise feature
normalization to neutralize distributional biases; and ii) Demographic-agnostic
feature aggregation, which uses a novel alignment loss to suppress
sensitive-attribute signals. Across three cross-domain benchmarks, DAID
consistently achieves superior performance in both fairness and generalization
compared to several state-of-the-art detectors, validating both its theoretical
foundation and practical effectiveness.

</details>


### [62] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: OmniDraft是一个统一框架，使单个草稿模型能与任何目标模型配合使用，并通过动态适应用户数据提升解码速度。


<details>
  <summary>Details</summary>
Motivation: 解决在线部署中草稿模型与目标模型不兼容及延迟改进的问题。

Method: 引入在线n-gram缓存和混合蒸馏微调，结合自适应草拟技术。

Result: 单个Llama-68M模型可与多种目标模型配对，解码速度提升1.5-2倍。

Conclusion: OmniDraft适用于设备端LLM应用，实现“一草稿模型适配所有”的范式。

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [63] [Guided Generation for Developable Antibodies](https://arxiv.org/abs/2507.02670)
*Siqi Zhao,Joshua Moller,Porfi Quintero-Cadena,Lood van Niekerk*

Main category: cs.LG

TL;DR: 论文提出了一种基于离散扩散模型的计算框架，用于优化抗体序列的可开发性，结合SVDD模块提升生成候选物的生物物理可行性。


<details>
  <summary>Details</summary>
Motivation: 临床有效的治疗性抗体需要高亲和力、良好的可制造性、稳定性和安全性，这些统称为可开发性。论文旨在通过计算框架优化抗体序列以满足这些要求。

Method: 引入了一种基于自然配对重链和轻链序列的离散扩散模型，结合SVDD模块引导生成生物物理可行的候选物。

Result: 模型在无约束采样中重现了自然抗体库和已批准治疗剂的全局特征，SVDD引导显著提升了可开发性评分。

Conclusion: 该框架结合高通量可开发性检测，为同时满足结合和生物物理标准的抗体设计提供了迭代的机器学习驱动流程。

Abstract: Therapeutic antibodies require not only high-affinity target engagement, but
also favorable manufacturability, stability, and safety profiles for clinical
effectiveness. These properties are collectively called `developability'. To
enable a computational framework for optimizing antibody sequences for
favorable developability, we introduce a guided discrete diffusion model
trained on natural paired heavy- and light-chain sequences from the Observed
Antibody Space (OAS) and quantitative developability measurements for 246
clinical-stage antibodies. To steer generation toward biophysically viable
candidates, we integrate a Soft Value-based Decoding in Diffusion (SVDD) Module
that biases sampling without compromising naturalness. In unconstrained
sampling, our model reproduces global features of both the natural repertoire
and approved therapeutics, and under SVDD guidance we achieve significant
enrichment in predicted developability scores over unguided baselines. When
combined with high-throughput developability assays, this framework enables an
iterative, ML-driven pipeline for designing antibodies that satisfy binding and
biophysical criteria in tandem.

</details>


### [64] [Embedding-Based Federated Data Sharing via Differentially Private Conditional VAEs](https://arxiv.org/abs/2507.02671)
*Francesco Di Salvo,Hanh Huyen My Nguyen,Christian Ledig*

Main category: cs.LG

TL;DR: 提出了一种基于差分隐私生成模型的数据共享方法，通过基础模型提取紧凑嵌入，降低计算开销，支持多样化下游任务。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像中的应用受限于数据稀缺和隐私法规，联邦学习存在通信成本高和任务单一的问题。

Method: 采用差分隐私条件变分自编码器（DP-CVAE）建模全局隐私感知数据分布，提取紧凑嵌入。

Result: 方法在隐私性、可扩展性和效率上优于传统联邦学习分类器，且嵌入质量高于DP-CGAN。

Conclusion: DP-CVAE是一种高效、隐私保护的数据共享解决方案，适用于多样化下游任务。

Abstract: Deep Learning (DL) has revolutionized medical imaging, yet its adoption is
constrained by data scarcity and privacy regulations, limiting access to
diverse datasets. Federated Learning (FL) enables decentralized training but
suffers from high communication costs and is often restricted to a single
downstream task, reducing flexibility. We propose a data-sharing method via
Differentially Private (DP) generative models. By adopting foundation models,
we extract compact, informative embeddings, reducing redundancy and lowering
computational overhead. Clients collaboratively train a Differentially Private
Conditional Variational Autoencoder (DP-CVAE) to model a global, privacy-aware
data distribution, supporting diverse downstream tasks. Our approach, validated
across multiple feature extractors, enhances privacy, scalability, and
efficiency, outperforming traditional FL classifiers while ensuring
differential privacy. Additionally, DP-CVAE produces higher-fidelity embeddings
than DP-CGAN while requiring $5{\times}$ fewer parameters.

</details>


### [65] [Multi-Agent Reinforcement Learning for Dynamic Pricing in Supply Chains: Benchmarking Strategic Agent Behaviours under Realistically Simulated Market Conditions](https://arxiv.org/abs/2507.02698)
*Thomas Hazenberg,Yao Ma,Seyed Sahand Mohammadi Ziabari,Marijn van Rijswijk*

Main category: cs.LG

TL;DR: 研究探讨多智能体强化学习（MARL）如何改进供应链动态定价策略，对比静态规则方法，发现MARL能引入竞争动态。


<details>
  <summary>Details</summary>
Motivation: 传统ERP系统的静态定价策略忽略市场参与者间的战略互动，而单智能体强化学习无法模拟真实供应链的相互依赖性。

Method: 在模拟环境中评估三种MARL算法（MADDPG、MADQN、QMIX）与静态规则基线的表现，结合真实电商数据和LightGBM需求预测模型。

Result: 静态规则方法公平性和价格稳定性最高，但缺乏竞争；MADQN定价最激进，MADDPG平衡竞争与公平。

Conclusion: MARL能捕捉静态规则无法模拟的战略行为，为动态定价未来发展提供参考。

Abstract: This study investigates how Multi-Agent Reinforcement Learning (MARL) can
improve dynamic pricing strategies in supply chains, particularly in contexts
where traditional ERP systems rely on static, rule-based approaches that
overlook strategic interactions among market actors. While recent research has
applied reinforcement learning to pricing, most implementations remain
single-agent and fail to model the interdependent nature of real-world supply
chains. This study addresses that gap by evaluating the performance of three
MARL algorithms: MADDPG, MADQN, and QMIX against static rule-based baselines,
within a simulated environment informed by real e-commerce transaction data and
a LightGBM demand prediction model. Results show that rule-based agents achieve
near-perfect fairness (Jain's Index: 0.9896) and the highest price stability
(volatility: 0.024), but they fully lack competitive dynamics. Among MARL
agents, MADQN exhibits the most aggressive pricing behaviour, with the highest
volatility and the lowest fairness (0.5844). MADDPG provides a more balanced
approach, supporting market competition (share volatility: 9.5 pp) while
maintaining relatively high fairness (0.8819) and stable pricing. These
findings suggest that MARL introduces emergent strategic behaviour not captured
by static pricing rules and may inform future developments in dynamic pricing.

</details>


### [66] [Fluid Democracy in Federated Data Aggregation](https://arxiv.org/abs/2507.02710)
*Aditya Vema Reddy Kesari,Krishna Reddy Kesari*

Main category: cs.LG

TL;DR: 论文提出了一种基于共识的联邦学习协议（FedVRD），通过动态限制恶意节点的影响并优化数据传输成本，优于传统的1p1v方法。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习中客户端无论权重是否有用均需传输数据，导致资源浪费。

Method: 提出新的流体民主协议（viscous-retained democracy）和算法FedVRD，动态限制恶意节点影响。

Result: FedVRD在相同假设下优于1p1v，且能防止影响力积累和对抗攻击。

Conclusion: FedVRD是一种高效且安全的联邦学习协议，适用于动态和对抗性环境。

Abstract: Federated learning (FL) mechanisms typically require each client to transfer
their weights to a central server, irrespective of how useful they are. In
order to avoid wasteful data transfer costs from clients to the central server,
we propose the use of consensus based protocols to identify a subset of clients
with most useful model weights at each data transfer step. First, we explore
the application of existing fluid democracy protocols to FL from a performance
standpoint, comparing them with traditional one-person-one-vote (also known as
1p1v or FedAvg). We propose a new fluid democracy protocol named
viscous-retained democracy that always does better than 1p1v under the same
assumptions as existing fluid democracy protocols while also not allowing for
influence accumulation. Secondly, we identify weaknesses of fluid democracy
protocols from an adversarial lens in terms of their dependence on topology
and/ or number of adversaries required to negatively impact the global model
weights. To this effect, we propose an algorithm (FedVRD) that dynamically
limits the effect of adversaries while minimizing cost by leveraging the
delegation topology.

</details>


### [67] [A Forget-and-Grow Strategy for Deep Reinforcement Learning Scaling in Continuous Control](https://arxiv.org/abs/2507.02712)
*Zilin Kang,Chenyuan Hu,Yu Luo,Zhecheng Yuan,Ruijie Zheng,Huazhe Xu*

Main category: cs.LG

TL;DR: 论文提出FoG算法，通过遗忘早期经验和动态扩展网络容量，解决了深度强化学习中的primacy bias问题，显著提升了样本效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的深度强化学习方法存在primacy bias问题，即过度拟合早期经验，影响样本效率和泛化能力。受人类婴儿遗忘现象的启发，论文提出新算法以解决这一问题。

Method: FoG算法包含两个机制：1) Experience Replay Decay（ER Decay），逐步减少早期经验的影响；2) Network Expansion，动态增加网络参数以增强学习能力。

Result: 在40多个任务的连续控制基准测试中，FoG算法表现优于现有SoTA方法（如BRO、SimBa和TD-MPC2）。

Conclusion: FoG算法通过模仿人类的遗忘与生长机制，有效解决了primacy bias问题，提升了深度强化学习的性能。

Abstract: Deep reinforcement learning for continuous control has recently achieved
impressive progress. However, existing methods often suffer from primacy bias,
a tendency to overfit early experiences stored in the replay buffer, which
limits an RL agent's sample efficiency and generalizability. In contrast,
humans are less susceptible to such bias, partly due to infantile amnesia,
where the formation of new neurons disrupts early memory traces, leading to the
forgetting of initial experiences. Inspired by this dual processes of
forgetting and growing in neuroscience, in this paper, we propose Forget and
Grow (FoG), a new deep RL algorithm with two mechanisms introduced. First,
Experience Replay Decay (ER Decay) "forgetting early experience", which
balances memory by gradually reducing the influence of early experiences.
Second, Network Expansion, "growing neural capacity", which enhances agents'
capability to exploit the patterns of existing data by dynamically adding new
parameters during training. Empirical results on four major continuous control
benchmarks with more than 40 tasks demonstrate the superior performance of FoG
against SoTA existing deep RL algorithms, including BRO, SimBa, and TD-MPC2.

</details>


### [68] [A Comprehensive Machine Learning Framework for Micromobility Demand Prediction](https://arxiv.org/abs/2507.02715)
*Omri Porat,Michael Fire,Eran Ben-Elia*

Main category: cs.LG

TL;DR: 本文提出了一种整合空间、时间和网络依赖性的框架，用于提升无桩电动滑板车的需求预测准确性，效果优于基线模型27%至49%。


<details>
  <summary>Details</summary>
Motivation: 无桩电动滑板车作为环保灵活的交通工具，对城市短途出行有重要意义，但需准确的需求预测以优化管理。

Method: 提出一个整合空间、时间和网络依赖性的框架，用于需求预测。

Result: 框架将需求预测准确性提升27%至49%，优于基线模型。

Conclusion: 该框架支持数据驱动的管理，优化车辆分布、降低成本，并促进可持续城市规划。

Abstract: Dockless e-scooters, a key micromobility service, have emerged as
eco-friendly and flexible urban transport alternatives. These services improve
first and last-mile connectivity, reduce congestion and emissions, and
complement public transport for short-distance travel. However, effective
management of these services depends on accurate demand prediction, which is
crucial for optimal fleet distribution and infrastructure planning. While
previous studies have focused on analyzing spatial or temporal factors in
isolation, this study introduces a framework that integrates spatial, temporal,
and network dependencies for improved micromobility demand forecasting. This
integration enhances accuracy while providing deeper insights into urban
micromobility usage patterns. Our framework improves demand prediction accuracy
by 27 to 49% over baseline models, demonstrating its effectiveness in capturing
micromobility demand patterns. These findings support data-driven micromobility
management, enabling optimized fleet distribution, cost reduction, and
sustainable urban planning.

</details>


### [69] [Hierarchical Multi-Label Contrastive Learning for Protein-Protein Interaction Prediction Across Organisms](https://arxiv.org/abs/2507.02724)
*Shiyi Liu,Buwen Liang,Yuetong Fang,Zixuan Jiang,Renjing Xu*

Main category: cs.LG

TL;DR: HIPPO是一种基于层次对比学习的蛋白质-蛋白质相互作用预测框架，通过多层级生物表征匹配和自适应领域知识整合，实现了跨物种的高性能预测和零样本迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决蛋白质-蛋白质相互作用（PPI）预测中数据异质性和跨物种泛化能力不足的问题。

Method: 提出层次对比学习框架HIPPO，结合多层级生物表征匹配和自适应数据驱动惩罚机制。

Result: 在基准数据集上表现优异，尤其在低数据量和零样本迁移场景下具有鲁棒性。

Conclusion: HIPPO为稀疏或多物种不平衡数据下的PPI预测提供了统一框架，并揭示了层次特征融合的重要性。

Abstract: Recent advances in AI for science have highlighted the power of contrastive
learning in bridging heterogeneous biological data modalities. Building on this
paradigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction
across Organisms), a hierarchical contrastive framework for protein-protein
interaction(PPI) prediction, where protein sequences and their hierarchical
attributes are aligned through multi-tiered biological representation matching.
The proposed approach incorporates hierarchical contrastive loss functions that
emulate the structured relationship among functional classes of proteins. The
framework adaptively incorporates domain and family knowledge through a
data-driven penalty mechanism, enforcing consistency between the learned
embedding space and the intrinsic hierarchy of protein functions. Experiments
on benchmark datasets demonstrate that HIPPO achieves state-of-the-art
performance, outperforming existing methods and showing robustness in low-data
regimes. Notably, the model demonstrates strong zero-shot transferability to
other species without retraining, enabling reliable PPI prediction and
functional inference even in less characterized or rare organisms where
experimental data are limited. Further analysis reveals that hierarchical
feature fusion is critical for capturing conserved interaction determinants,
such as binding motifs and functional annotations. This work advances
cross-species PPI prediction and provides a unified framework for interaction
prediction in scenarios with sparse or imbalanced multi-species data.

</details>


### [70] [Fast and Simplex: 2-Simplicial Attention in Triton](https://arxiv.org/abs/2507.02754)
*Aurko Roy,Timothy Chou,Sai Surya Duvvuri,Sijia Chen,Jiecao Yu,Xiaodong Wang,Manzil Zaheer,Rohan Anil*

Main category: cs.LG

TL;DR: 论文探讨了2-simplicial Transformer架构，通过三线性函数改进标准点积注意力，提升token效率，并在数学、编码等任务中表现优于标准Transformer。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型依赖海量数据，但现有缩放定律假设数据无限且适用于计算受限场景，实际中token效率的重要性凸显。

Method: 提出2-simplicial Transformer，通过Triton内核实现三线性函数，替代标准点积注意力。

Result: 在相同token预算下，2-simplicial Transformer在数学、编码等任务中表现更优，并改变了知识推理任务的缩放定律指数。

Conclusion: 2-simplicial Transformer通过提升token效率，为计算非受限场景下的模型优化提供了新方向。

Abstract: Recent work has shown that training loss scales as a power law with both
model size and the number of tokens, and that achieving compute-optimal models
requires scaling model size and token count together. However, these scaling
laws assume an infinite supply of data and apply primarily in compute-bound
settings. As modern large language models increasingly rely on massive
internet-scale datasets, the assumption that they are compute-bound is becoming
less valid. This shift highlights the need for architectures that prioritize
token efficiency.
  In this work, we investigate the use of the 2-simplicial Transformer, an
architecture that generalizes standard dot-product attention to trilinear
functions through an efficient Triton kernel implementation. We demonstrate
that the 2-simplicial Transformer achieves better token efficiency than
standard Transformers: for a fixed token budget, similarly sized models
outperform their dot-product counterparts on tasks involving mathematics,
coding, reasoning, and logic. We quantify these gains by demonstrating that
$2$-simplicial attention changes the exponent in the scaling laws for knowledge
and reasoning tasks compared to dot product attention.

</details>


### [71] [Understanding and Improving Length Generalization in Recurrent Models](https://arxiv.org/abs/2507.02782)
*Ricardo Buitrago Ruiz,Albert Gu*

Main category: cs.LG

TL;DR: 论文探讨了循环模型在序列长度超出训练范围时性能下降的问题，提出了一种简单训练干预方法以提升长度泛化能力。


<details>
  <summary>Details</summary>
Motivation: 循环模型理论上能处理任意长序列，但实际性能在超出训练长度时会显著下降，研究旨在解决这一问题。

Method: 通过增加训练时模型状态的覆盖范围（如引入高斯噪声或不同序列的最终状态）来提升泛化能力。

Result: 仅需少量额外训练步骤，模型即可处理远超训练长度的序列（如从2k到128k），并在长上下文任务中表现更优。

Conclusion: 提出的训练干预方法简单高效，能显著提升循环模型的长度泛化能力。

Abstract: Recently, recurrent models such as state space models and linear attention
have become popular due to their linear complexity in the sequence length.
Thanks to their recurrent nature, in principle they can process arbitrarily
long sequences, but their performance sometimes drops considerably beyond their
training context lengths-i.e. they fail to length generalize. In this work, we
provide comprehensive empirical and theoretical analysis to support the
unexplored states hypothesis, which posits that models fail to length
generalize when during training they are only exposed to a limited subset of
the distribution of all attainable states (i.e. states that would be attained
if the recurrence was applied to long sequences). Furthermore, we investigate
simple training interventions that aim to increase the coverage of the states
that the model is trained on, e.g. by initializing the state with Gaussian
noise or with the final state of a different input sequence. With only 500
post-training steps ($\sim 0.1\%$ of the pre-training budget), these
interventions enable length generalization for sequences that are orders of
magnitude longer than the training context (e.g. $2k\longrightarrow 128k$) and
show improved performance in long context tasks, thus presenting a simple and
efficient way to enable robust length generalization in general recurrent
models.

</details>


### [72] [In-Training Multicalibrated Survival Analysis for Healthcare via Constrained Optimization](https://arxiv.org/abs/2507.02807)
*Thiti Suttaket,Stanley Kok*

Main category: cs.LG

TL;DR: GRADUATE模型通过多校准优化解决生存分析中少数群体校准不足的问题，平衡校准与区分度。


<details>
  <summary>Details</summary>
Motivation: 现有生存模型仅在全群体层面校准，可能导致少数群体校准不佳，影响临床决策。

Method: GRADUATE将多校准建模为约束优化问题，同时优化校准与区分度。

Result: 理论证明优化方法高效可行，实验验证在真实临床数据上优于基线模型。

Conclusion: GRADUATE解决了少数群体校准问题，为生存分析提供了更可靠的模型。

Abstract: Survival analysis is an important problem in healthcare because it models the
relationship between an individual's covariates and the onset time of an event
of interest (e.g., death). It is important for survival models to be
well-calibrated (i.e., for their predicted probabilities to be close to
ground-truth probabilities) because badly calibrated systems can result in
erroneous clinical decisions. Existing survival models are typically calibrated
at the population level only, and thus run the risk of being poorly calibrated
for one or more minority subpopulations. We propose a model called GRADUATE
that achieves multicalibration by ensuring that all subpopulations are
well-calibrated too. GRADUATE frames multicalibration as a constrained
optimization problem, and optimizes both calibration and discrimination
in-training to achieve a good balance between them. We mathematically prove
that the optimization method used yields a solution that is both near-optimal
and feasible with high probability. Empirical comparisons against
state-of-the-art baselines on real-world clinical datasets demonstrate
GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of
the baselines vis-a-vis GRADUATE's strengths.

</details>


### [73] [Replicable Distribution Testing](https://arxiv.org/abs/2507.02814)
*Ilias Diakonikolas,Jingyi Gao,Daniel Kane,Sihan Liu,Christopher Ye*

Main category: cs.LG

TL;DR: 本文系统研究了算法可复制性框架下的分布测试问题，提出了新的可复制算法用于测试离散分布的接近性和独立性，并开发了新的下界证明方法。


<details>
  <summary>Details</summary>
Motivation: 研究在算法可复制性框架下，如何高效测试概率分布的自然性质，填补了现有研究的空白。

Method: 开发了新的可复制算法用于测试分布的接近性和独立性，并提出了一种新的下界证明方法。

Result: 建立了近乎最优的样本复杂度下界，解决了先前工作中的开放性问题。

Conclusion: 本文为可复制分布测试提供了理论和算法基础，具有广泛的应用潜力。

Abstract: We initiate a systematic investigation of distribution testing in the
framework of algorithmic replicability. Specifically, given independent samples
from a collection of probability distributions, the goal is to characterize the
sample complexity of replicably testing natural properties of the underlying
distributions. On the algorithmic front, we develop new replicable algorithms
for testing closeness and independence of discrete distributions. On the lower
bound front, we develop a new methodology for proving sample complexity lower
bounds for replicable testing that may be of broader interest. As an
application of our technique, we establish near-optimal sample complexity lower
bounds for replicable uniformity testing -- answering an open question from
prior work -- and closeness testing.

</details>


### [74] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 论文提出了一种名为ExPO的新方法，用于解决强化学习后训练中模型难以生成有效正样本的问题，通过结合当前策略和正确答案生成高质量样本，提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的后训练方法依赖模型初始能力生成正样本，限制了其在早期训练和复杂推理任务中的应用。需要探索新方法以生成更有效的正样本。

Method: 提出Self-Explanation Policy Optimization (ExPO)，通过结合当前策略和正确答案生成高质量样本，优化模型推理轨迹。

Result: ExPO在推理基准测试中表现优于基于专家示范的方法，尤其在MATH level-5等复杂任务中显著提升模型性能。

Conclusion: ExPO通过生成更符合策略的高质量样本，有效提升了模型在复杂推理任务中的表现，为强化学习后训练提供了新思路。

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


### [75] [LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding](https://arxiv.org/abs/2507.02843)
*Yuchen Ma,Dennis Frauen,Jonas Schweisthal,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 论文提出了一种新框架，解决训练与推理数据不一致导致的治疗效应估计偏差问题，利用大语言模型和双重稳健学习器。


<details>
  <summary>Details</summary>
Motivation: 临床实践中，训练数据与推理数据的差异可能导致治疗效应估计偏差，影响个性化医疗决策。

Method: 提出新框架，结合大语言模型和双重稳健学习器，解决推理时文本混杂问题。

Result: 实验证明框架在真实场景中有效。

Conclusion: 该框架能显著减少因数据不一致导致的偏差，提升治疗效应估计的准确性。

Abstract: Estimating treatment effects is crucial for personalized decision-making in
medicine, but this task faces unique challenges in clinical practice. At
training time, models for estimating treatment effects are typically trained on
well-structured medical datasets that contain detailed patient information.
However, at inference time, predictions are often made using textual
descriptions (e.g., descriptions with self-reported symptoms), which are
incomplete representations of the original patient information. In this work,
we make three contributions. (1) We show that the discrepancy between the data
available during training time and inference time can lead to biased estimates
of treatment effects. We formalize this issue as an inference time text
confounding problem, where confounders are fully observed during training time
but only partially available through text at inference time. (2) To address
this problem, we propose a novel framework for estimating treatment effects
that explicitly accounts for inference time text confounding. Our framework
leverages large language models together with a custom doubly robust learner to
mitigate biases caused by the inference time text confounding. (3) Through a
series of experiments, we demonstrate the effectiveness of our framework in
real-world applications.

</details>


### [76] [MvHo-IB: Multi-View Higher-Order Information Bottleneck for Brain Disorder Diagnosis](https://arxiv.org/abs/2507.02847)
*Kunyu Zhang,Qiang Li,Shujian Yu*

Main category: cs.LG

TL;DR: MvHo-IB是一种多视图学习框架，通过整合高阶互动（HOIs）和成对互动，提升fMRI数据的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效提取和利用高阶互动（HOIs），限制了诊断系统的性能。

Method: 结合信息论中的O-information和矩阵熵估计器量化HOIs，设计Brain3DCNN编码器，并引入多视图信息瓶颈目标。

Result: 在三个基准fMRI数据集上表现优于现有方法，包括最新的超图技术。

Conclusion: MvHo-IB通过创新方法显著提升了fMRI数据的诊断性能。

Abstract: Recent evidence suggests that modeling higher-order interactions (HOIs) in
functional magnetic resonance imaging (fMRI) data can enhance the diagnostic
accuracy of machine learning systems. However, effectively extracting and
utilizing HOIs remains a significant challenge. In this work, we propose
MvHo-IB, a novel multi-view learning framework that integrates both pairwise
interactions and HOIs for diagnostic decision-making, while automatically
compressing task-irrelevant redundant information. MvHo-IB introduces several
key innovations: (1) a principled method that combines O-information from
information theory with a matrix-based Renyi alpha-order entropy estimator to
quantify and extract HOIs, (2) a purpose-built Brain3DCNN encoder to
effectively utilize these interactions, and (3) a new multi-view learning
information bottleneck objective to enhance representation learning.
Experiments on three benchmark fMRI datasets demonstrate that MvHo-IB achieves
state-of-the-art performance, significantly outperforming previous methods,
including recent hypergraph-based techniques. The implementation of MvHo-IB is
available at https://github.com/zky04/MvHo-IB.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [77] [Adaptive Iterative Soft-Thresholding Algorithm with the Median Absolute Deviation](https://arxiv.org/abs/2507.02084)
*Yining Feng,Ivan Selesnick*

Main category: stat.ML

TL;DR: 本文对自适应迭代软阈值算法（ISTA）进行了理论分析，重点研究了基于中位数绝对偏差估计噪声水平的阈值策略，证明了算法的固定点性质、局部线性收敛保证及全局收敛行为。


<details>
  <summary>Details</summary>
Motivation: 尽管自适应ISTA在实践中表现良好，但缺乏理论支持。本文旨在填补这一空白，为其提供理论分析。

Method: 采用基于中位数绝对偏差的阈值策略，分析自适应ISTA的固定点性质、局部稳定性和收敛性。

Result: 证明了算法的固定点具有尺度等变性、非唯一性和局部稳定性，并提供了局部线性收敛保证和全局收敛行为。

Conclusion: 自适应ISTA在理论上具有可靠的收敛性和稳定性，为实际应用提供了理论依据。

Abstract: The adaptive Iterative Soft-Thresholding Algorithm (ISTA) has been a popular
algorithm for finding a desirable solution to the LASSO problem without
explicitly tuning the regularization parameter $\lambda$. Despite that the
adaptive ISTA is a successful practical algorithm, few theoretical results
exist. In this paper, we present the theoretical analysis on the adaptive ISTA
with the thresholding strategy of estimating noise level by median absolute
deviation. We show properties of the fixed points of the algorithm, including
scale equivariance, non-uniqueness, and local stability, prove the local linear
convergence guarantee, and show its global convergence behavior.

</details>


### [78] [Hybrid least squares for learning functions from highly noisy data](https://arxiv.org/abs/2507.02215)
*Ben Adcock,Bernhard Hientzsch,Akil Narayan,Yiming Xu*

Main category: stat.ML

TL;DR: 提出了一种结合Christoffel采样和最优实验设计的混合方法，用于高效估计条件期望，尤其在噪声较大的数据中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在小噪声环境下表现良好，但在大噪声情况下效果不佳，需要一种更高效的方法来估计条件期望。

Method: 结合Christoffel采样与最优实验设计，提出混合算法，并扩展到凸约束和随机场期望的场景。

Result: 算法在样本点生成和噪声抑制方面具有最优性，计算效率和样本复杂度优于现有方法。

Conclusion: 理论分析和数值实验验证了方法的有效性，适用于合成数据和计算金融中的随机模拟问题。

Abstract: Motivated by the need for efficient estimation of conditional expectations,
we consider a least-squares function approximation problem with heavily
polluted data. Existing methods that are powerful in the small noise regime are
suboptimal when large noise is present. We propose a hybrid approach that
combines Christoffel sampling with certain types of optimal experimental design
to address this issue. We show that the proposed algorithm enjoys appropriate
optimality properties for both sample point generation and noise mollification,
leading to improved computational efficiency and sample complexity compared to
existing methods. We also extend the algorithm to convex-constrained settings
with similar theoretical guarantees. When the target function is defined as the
expectation of a random field, we extend our approach to leverage adaptive
random subspaces and establish results on the approximation capacity of the
adaptive procedure. Our theoretical findings are supported by numerical studies
on both synthetic data and on a more challenging stochastic simulation problem
in computational finance.

</details>


### [79] [Transfer Learning for Matrix Completion](https://arxiv.org/abs/2507.02248)
*Dali Liu,Haolei Weng*

Main category: stat.ML

TL;DR: 本文研究了在矩阵补全设置下的知识迁移，提出了一种利用辅助数据提升低秩目标矩阵估计的迁移学习方法，并证明了其收敛速度和极小极大最优性。


<details>
  <summary>Details</summary>
Motivation: 通过利用源数据集的先验信息，提升目标矩阵的估计精度，特别是在源矩阵与目标矩阵接近时优于传统方法。

Method: 提出了一种迁移学习流程，利用源数据集信息，并结合先进的浓度不等式消除收敛速率中的对数因子。

Result: 分析表明，当源矩阵与目标矩阵接近时，方法优于传统单目标数据方法，并实现了极小极大最优性。

Conclusion: 通过模拟和真实数据验证了方法的有效性，并开发了高效检测程序以识别相关源数据集。

Abstract: In this paper, we explore the knowledge transfer under the setting of matrix
completion, which aims to enhance the estimation of a low-rank target matrix
with auxiliary data available. We propose a transfer learning procedure given
prior information on which source datasets are favorable. We study its
convergence rates and prove its minimax optimality. Our analysis reveals that
with the source matrices close enough to the target matrix, out method
outperforms the traditional method using the single target data. In particular,
we leverage the advanced sharp concentration inequalities introduced in
\cite{brailovskaya2024universality} to eliminate a logarithmic factor in the
convergence rate, which is crucial for proving the minimax optimality. When the
relevance of source datasets is unknown, we develop an efficient detection
procedure to identify informative sources and establish its selection
consistency. Simulations and real data analysis are conducted to support the
validity of our methodology.

</details>


### [80] [It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation](https://arxiv.org/abs/2507.02275)
*Jikai Jin,Lester Mackey,Vasilis Syrgkanis*

Main category: stat.ML

TL;DR: 研究发现，处理噪声的分布对结构无关因果推断的效果有显著影响。DML估计器在高斯噪声下最优，但在非高斯噪声下表现不佳。新提出的ACE方法通过高阶稳健性提升了估计效果。


<details>
  <summary>Details</summary>
Motivation: 探讨结构无关因果推断中处理噪声分布对估计效果的影响，解决DML估计器在非高斯噪声下的局限性。

Method: 基于部分线性模型，分析DML估计器在高斯和非高斯噪声下的表现，并提出新的ACE方法，利用高阶累积量实现稳健性。

Result: DML在高斯噪声下最优，但在非高斯噪声下表现不佳；ACE方法通过高阶稳健性显著提升估计效果。

Conclusion: 处理噪声分布对因果推断至关重要，ACE方法为结构无关推断提供了更优的解决方案。

Abstract: Structure-agnostic causal inference studies how well one can estimate a
treatment effect given black-box machine learning estimates of nuisance
functions (like the impact of confounders on treatment and outcomes). Here, we
find that the answer depends in a surprising way on the distribution of the
treatment noise. Focusing on the partially linear model of
\citet{robinson1988root}, we first show that the widely adopted double machine
learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise,
resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for
independent non-Gaussian treatment noise, we show that DML is always suboptimal
by constructing new practical procedures with higher-order robustness to
nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant
estimators to achieve $r$-th order insensitivity to nuisance errors whenever
the $(r+1)$-st treatment cumulant is non-zero. We complement these core results
with novel minimax guarantees for binary treatments in the partially linear
model. Finally, using synthetic demand estimation experiments, we demonstrate
the practical benefits of our higher-order robust estimators.

</details>


### [81] [Sparse Gaussian Processes: Structured Approximations and Power-EP Revisited](https://arxiv.org/abs/2507.02377)
*Thang D. Bui,Michalis K. Titsias*

Main category: stat.ML

TL;DR: 论文提出了一种基于块对角结构的稀疏变分高斯过程改进方法，通过引入块对角缩放矩阵，提升了变分下界，并在回归实验中表现优于现有对角近似方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏变分高斯过程在扩展GP模型时存在改进空间，尤其是通过引入更灵活的缩放矩阵结构可以提升性能。

Method: 提出块对角缩放矩阵的变分近似方法，并基于Power Expectation Propagation（PEP）框架，利用结构化后验提升性能。

Result: 块对角近似在回归实验中表现优于对角近似，且计算成本相当；PEP框架在不同超参数设置下均具竞争力。

Conclusion: 块对角结构和PEP框架为稀疏高斯过程提供了更灵活且高效的替代方案。

Abstract: Inducing-point-based sparse variational Gaussian processes have become the
standard workhorse for scaling up GP models. Recent advances show that these
methods can be improved by introducing a diagonal scaling matrix to the
conditional posterior density given the inducing points. This paper first
considers an extension that employs a block-diagonal structure for the scaling
matrix, provably tightening the variational lower bound. We then revisit the
unifying framework of sparse GPs based on Power Expectation Propagation (PEP)
and show that it can leverage and benefit from the new structured approximate
posteriors. Through extensive regression experiments, we show that the proposed
block-diagonal approximation consistently performs similarly to or better than
existing diagonal approximations while maintaining comparable computational
costs. Furthermore, the new PEP framework with structured posteriors provides
competitive performance across various power hyperparameter settings, offering
practitioners flexible alternatives to standard variational approaches.

</details>
