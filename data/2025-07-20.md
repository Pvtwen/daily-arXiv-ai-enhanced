<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [cs.LG](#cs.LG) [Total: 63]
- [stat.ML](#stat.ML) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Differential Communication in Channels with Mobility and Delay Spread using Zak-OTFS](https://arxiv.org/abs/2507.12593)
*Sandesh Rao Mattu,Nishant Mehrotra,Robert Calderbank*

Main category: eess.SP

TL;DR: 提出了一种基于Zak-OTFS的差分通信方案，减少周期性导频传输需求，利用检测数据作为导频，提升能效和频谱效率。


<details>
  <summary>Details</summary>
Motivation: Zak-OTFS在延迟-多普勒域中信道可预测，但仍需周期性导频传输。本文旨在减少导频需求，提升系统效率。

Method: 利用检测数据作为导频，通过DD域信道预测能力，将前一时隙的信道估计用于下一时隙的数据检测，实现差分通信。

Result: 方案提升了数据符号能量分配和频谱效率，相比现有方案，在更低复杂度下实现更好的误码率性能。

Conclusion: 提出的差分通信方案有效减少导频需求，提升Zak-OTFS系统的能效和频谱效率。

Abstract: Zak-transform based orthogonal time frequency space (Zak-OTFS) is a
delay-Doppler (DD) domain modulation scheme in which the signal processing is
carried out in the DD domain. The channel when viewed in the DD domain is
predictable. However, even with Zak-OTFS, pilots need to be sent periodically,
albeit at a lower rate. In this paper, we propose a differential communication
scheme for Zak-OTFS systems that alleviates the need for periodic pilot
transmission. Towards this, we analytically show that the detected data can be
used as a pilot and that the channel estimate obtained from the detected data
can enable further detection enabling the "differential" aspect of the
communication. Specifically, we leverage the prediction capability of the DD
channel in Zak-OTFS to use the channel estimate (obtained from detected data
symbols treated as pilots) in the previous instant to detect data in the next
instant and propagate this forward. The advantages are two fold. First, it
allows the data symbols to enjoy higher energy since the energy that would
otherwise be required for pilot symbols can also be allocated to data symbols.
Second, it allows for full spectral efficiency compared to point or embedded
pilots. Comparison with the full spectral efficiency achieving spread pilot
scheme shows that the proposed method achieves better bit-error rate at lower
complexity.

</details>


### [2] [Achieving Robust Channel Estimation Neural Networks by Designed Training Data](https://arxiv.org/abs/2507.12630)
*Dianxin Luan,John Thompson*

Main category: eess.SP

TL;DR: 论文提出了一种离线训练的神经网络设计方法，通过生成合成训练数据集，确保网络在新信道上达到一定的均方误差（MSE），无需实际信道信息或参数更新。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动方法在新数据上表现不佳，且无法满足低延迟和有限计算资源的要求，因此需要设计离线训练的神经网络以实现鲁棒性能。

Method: 提出设计准则生成合成训练数据集，并进一步提出基准设计，确保神经网络在不同信道配置下智能运行。

Result: 仿真显示，神经网络在固定和可变延迟扩展的信道上均实现了鲁棒泛化，且泛化性能与网络架构无关。

Conclusion: 所提方法无需实际信道信息或在线训练，适用于不同复杂度的神经网络，具有广泛适用性。

Abstract: Channel estimation is crucial in cognitive communications, as it enables
intelligent spectrum sensing and adaptive transmission by providing accurate
information about the current channel state. However, in many papers neural
networks are frequently tested by training and testing on one example channel
or similar channels. This is because data-driven methods often degrade on new
data which they are not trained on, as they cannot extrapolate their training
knowledge. This is despite the fact physical channels are often assumed to be
time-variant. However, due to the low latency requirements and limited
computing resources, neural networks may not have enough time and computing
resources to execute online training to fine-tune the parameters. This
motivates us to design offline-trained neural networks that can perform
robustly over wireless channels, but without any actual channel information
being known at design time. In this paper, we propose design criteria to
generate synthetic training datasets for neural networks, which guarantee that
after training the resulting networks achieve a certain mean squared error
(MSE) on new and previously unseen channels. Therefore, neural network
solutions require no prior channel information or parameters update for
real-world implementations. Based on the proposed design criteria, we further
propose a benchmark design which ensures intelligent operation for different
channel profiles. To demonstrate general applicability, we use neural networks
with different levels of complexity to show that the generalization achieved
appears to be independent of neural network architecture. From simulations,
neural networks achieve robust generalization to wireless channels with both
fixed channel profiles and variable delay spreads.

</details>


### [3] [A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis](https://arxiv.org/abs/2507.12645)
*Mohammed Guhdar,Ramadhan J. Mstafa,Abdulhakeem O. Mohammed*

Main category: eess.SP

TL;DR: 提出了一种统一的深度学习框架，结合ResNet-CNN和注意力机制，通过时间域数据增强和Focal Loss解决生物信号分析和类别不平衡问题，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号（如ECG和EEG）的多样性和类别不平衡问题限制了传统方法的性能，需要统一的架构进行高效处理。

Method: 采用ResNet-CNN结合注意力机制，引入时间域数据增强和Focal Loss，预处理包括小波去噪和标准化。

Result: 在UCI Seizure EEG、MIT-BIH Arrhythmia和PTB Diagnostic ECG数据集上分别达到99.96%、99.78%和100%的准确率，内存占用130MB，处理速度10ms/样本。

Conclusion: 该框架在性能和效率上均表现出色，适用于低端或可穿戴设备部署。

Abstract: The increasing need for accurate and unified analysis of diverse biological
signals, such as ECG and EEG, is paramount for comprehensive patient
assessment, especially in synchronous monitoring. Despite advances in
multi-sensor fusion, a critical gap remains in developing unified architectures
that effectively process and extract features from fundamentally different
physiological signals. Another challenge is the inherent class imbalance in
many biomedical datasets, often causing biased performance in traditional
methods. This study addresses these issues by proposing a novel and unified
deep learning framework that achieves state-of-the-art performance across
different signal types. Our method integrates a ResNet-based CNN with an
attention mechanism, enhanced by a novel data augmentation strategy:
time-domain concatenation of multiple augmented variants of each signal to
generate richer representations. Unlike prior work, we scientifically increase
signal complexity to achieve future-reaching capabilities, which resulted in
the best predictions compared to the state of the art. Preprocessing steps
included wavelet denoising, baseline removal, and standardization. Class
imbalance was effectively managed through the combined use of this advanced
data augmentation and the Focal Loss function. Regularization techniques were
applied during training to ensure generalization. We rigorously evaluated the
proposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH
Arrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%,
and 100%, respectively, demonstrating robustness across diverse signal types
and clinical contexts. Finally, the architecture requires ~130 MB of memory and
processes each sample in ~10 ms, suggesting suitability for deployment on
low-end or wearable devices.

</details>


### [4] [Enhancing Urban GNSS Positioning Reliability via Conservative Satellite Selection Using Unanimous Voting Across Multiple Machine Learning Classifiers](https://arxiv.org/abs/2507.12706)
*Sanghyun Kim,Jiwon Seo*

Main category: eess.SP

TL;DR: 论文提出了一种基于一致投票的多分类器卫星选择策略，增强了城市环境中GNSS定位的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中GNSS信号易受建筑物遮挡和多径效应影响，导致定位误差较大。

Method: 采用随机森林、梯度提升决策树和支持向量机三种分类器进行LOS/NLOS分类，通过一致投票策略选择卫星。

Result: 实验表明，该方法显著提高了定位成功率和接收器包含率，尽管卫星数量减少导致定位边界略有增加。

Conclusion: 该方法有效提升了城市GNSS环境的定位可靠性。

Abstract: In urban environments, global navigation satellite system (GNSS) positioning
is often compromised by signal blockages and multipath effects caused by
buildings, leading to significant positioning errors. To address this issue,
this study proposes a robust enhancement of zonotope shadow matching
(ZSM)-based positioning by employing a conservative satellite selection
strategy using unanimous voting across multiple machine learning classifiers.
Three distinct models - random forest (RF), gradient boosting decision tree
(GBDT), and support vector machine (SVM) - were trained to perform
line-of-sight (LOS) and non-line-of-sight (NLOS) classification based on global
positioning system (GPS) signal features. A satellite is selected for
positioning only when all classifiers unanimously agree on its classification
and their associated confidence scores exceed a threshold. Experiments with
real-world GPS data collected in dense urban areas demonstrate that the
proposed method significantly improves the positioning success rate and the
receiver containment rate, even with imperfect LOS/NLOS classification.
Although a slight increase in the position bound was observed due to the
reduced number of satellites used, overall positioning reliability was
substantially enhanced, indicating the effectiveness of the proposed approach
in urban GNSS environments.

</details>


### [5] [Beamforming Tradeoff for Sensing and Communication in Cell-Free MIMO](https://arxiv.org/abs/2507.12917)
*Xi Ding,Luca Kunz,E. Jorswieck*

Main category: eess.SP

TL;DR: 本文提出了一种基于SDR的全局最优波束成形框架，用于小型无小区MIMO系统中的联合感知与通信优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在联合感知与通信优化中缺乏全局最优性或需要额外处理步骤，本文旨在解决这些问题。

Method: 采用SDR优化框架，无需后处理即可保证全局最优解，并引入独立波束成形策略作为基准。

Result: 提出的框架在计算效率和全局最优性上优于现有方法。

Conclusion: 该框架为下一代无线网络的发展提供了有价值的见解。

Abstract: This paper studies optimal joint beamforming (BF) for joint sensing and
communication (JSAC) in small-scale cell-free MIMO (CF-MIMO) systems. While
prior works have explored JSAC optimization using methods such as successive
convex approximation (SCA) and semidefinite relaxation (SDR), many of these
approaches either lack global optimality or require additional rank-reduction
steps. In contrast, we propose an SDR-based optimization framework that
guarantees globally optimal solutions without post-processing. To benchmark its
performance, we introduce a standalone BF strategy that dedicates each access
point (AP) exclusively to either communication or sensing. The proposed
formulation builds upon a general multi-user system model, enabling future
extensions beyond the single-user setting. Overall, our framework offers a
globally optimal and computationally efficient BF design, providing valuable
insights for the development of next-generation wireless networks.

</details>


### [6] [Multiple-Mode Affine Frequency Division Multiplexing with Index Modulation](https://arxiv.org/abs/2507.13037)
*Guangyao Liu,Tianqi Mao,Yanqun Tang,Jingjing Zhao,Zhenyu Xiao*

Main category: eess.SP

TL;DR: 本文提出了一种基于仿射频率分复用（AFDM）的多模式索引调制方案（MM-AFDM-IM），旨在提高AFDM的频谱和能量效率。通过动态选择星座模式和激活子载波，额外信息比特得以传输，且无需额外能耗。仿真结果表明其性能优于传统方案。


<details>
  <summary>Details</summary>
Motivation: AFDM是一种适用于高移动性通信场景的多载波技术，但其频谱和能量效率仍有提升空间。本文旨在通过多模式索引调制进一步优化AFDM的性能。

Method: 开发了MM-AFDM-IM方案，通过动态选择星座模式和激活子载波传输额外信息比特。讨论了模式选择策略，并推导了最大似然检测下的误码率上限。

Result: 仿真结果表明，MM-AFDM-IM在频谱和能量效率上优于传统方案。

Conclusion: MM-AFDM-IM是一种有效的AFDM优化方案，适用于高移动性通信场景，具有显著的性能优势。

Abstract: Affine frequency division multiplexing (AFDM), a promising multicarrier
technique utilizing chirp signals, has been envisioned as an effective solution
for high-mobility communication scenarios. In this paper, we develop a
multiple-mode index modulation scheme tailored for AFDM, termed as MM-AFDM-IM,
which aims to further improve the spectral and energy efficiencies of AFDM.
Specifically, multiple constellation alphabets are selected for different
chirp-based subcarriers (chirps). Aside from classical amplitude/phase
modulation, additional information bits can be conveyed by the dynamic patterns
of both constellation mode selection and chirp activation, without extra energy
consumption. Furthermore, we discuss the mode selection strategy and derive an
asymptotically tight upper bound on the bit error rate (BER) of the proposed
scheme under maximum-likelihood detection. Simulation results are provided to
demonstrate the superior performance of MM-AFDM-IM compared to conventional
benchmark schemes.

</details>


### [7] [Unmodulated Visible Light Positioning: A Deep Dive into Techniques, Studies, and Future Prospects](https://arxiv.org/abs/2507.13080)
*Morteza Alijani,Wout Joseph,David Plets*

Main category: eess.SP

TL;DR: 本文介绍了无调制可见光定位（uVLP）技术，分析了其与传统VLP的优劣，并分类了现有uVLP技术，提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统VLP技术因调制LED带来的高成本和低照明效率限制了其广泛应用，uVLP通过利用未调制光源提供了一种低成本、高效的替代方案。

Method: 文章分析了uVLP的基本原理，比较了uVLP与传统VLP，并根据接收器技术将uVLP方法分为强度型和成像型，提出了分类法。

Result: uVLP技术为室内定位提供了低成本、高效的解决方案，但仍面临挑战。

Conclusion: uVLP技术具有潜力，但需进一步研究以解决现有问题并推动其广泛应用。

Abstract: Visible Light Positioning (VLP) has emerged as a promising technology for
next-generation indoor positioning systems (IPS), particularly within the scope
of sixth-generation (6G) wireless networks. Its attractiveness stems from
leveraging existing lighting infrastructures equipped with light-emitting
diodes (LEDs), enabling cost-efficient deployments and achieving high-precision
positioning accuracy in the centimeter-todecimeter range. However, widespread
adoption of traditional VLP solutions faces significant barriers due to the
increased costs and operational complexity associated with modulating LEDs,
which consequently reduces illumination efficiency by lowering their radiant
flux. To address these limitations, recent research has introduced the concept
of unmodulated Visible Light Positioning (uVLP), which exploits Light Signals
of Opportunity (LSOOP) emitted by unmodulated illumination sources such as
conventional LEDs. This paradigm offers a cost-effective, lowinfrastructure
alternative for indoor positioning by eliminating the need for modulation
hardware and maintaining lighting efficiency. This paper delineates the
fundamental principles of uVLP, provides a comparative analysis of uVLP versus
conventional VLP methods, and classifies existing uVLP techniques according to
receiver technologies into intensity-based methods (e.g., photodiodes, solar
cells, etc.) and imaging-based methods. Additionally, we propose a
comprehensive taxonomy categorizing techniques into demultiplexed and
undemultiplexed approaches. Within this structured framework, we critically
review current advancements in uVLP, discuss prevailing challenges, and outline
promising research directions essential for developing robust, scalable, and
widely deployable uVLP solutions.

</details>


### [8] [Angle Estimation of a Single Source with Massive Uniform Circular Arrays](https://arxiv.org/abs/2507.13086)
*Mingyan Gong*

Main category: eess.SP

TL;DR: 提出了一种基于均匀圆形阵列（UCA）的简单二维DOA估计方法，适用于实时信号处理。


<details>
  <summary>Details</summary>
Motivation: 均匀线性阵列只能估计源方位角，而UCA能提供360°方位角和仰角信息，因此需要一种简单高效的DOA估计方法。

Method: 通过量化方位角并计算协方差，结合显式公式估计仰角，计算简单。

Result: 数值结果表明，该方法能有效估计方位角和仰角，且适用于非均匀噪声环境。

Conclusion: 该方法计算简单，可作为高精度多维搜索的初始点，适用于实时处理。

Abstract: Estimating the directions of arrival (DOAs) of incoming plane waves is an
essential topic in array signal processing. Widely adopted uniform linear
arrays can only provide estimates of source azimuth. Thus, uniform circular
arrays (UCAs) are attractive in that they can provide $360^{\circ}$ azimuthal
coverage and additional elevation angle information. Considering that with a
massive UCA, its polar angles of array sensors can approximately represent
azimuth angles over $360^{\circ}$ using angle quantization, a simple
two-dimensional DOA estimation method for a single source is proposed. In this
method, the quantized azimuth angle estimate is obtained by only calculating
and comparing a number of covariances, based on which the elevation angle
estimate is then obtained by an explicit formula. Thus, the proposed method is
computationally simple and suitable for real-time signal processing. Numerical
results verify that the proposed method can obtain azimuth as well as elevation
angle estimates and the estimates can be used as starting points of
multidimensional searches for methods with higher accuracy. Additionally, the
proposed method can still work in the presence of nonuniform noise.

</details>


### [9] [Multifrequency system model for multiport time-modulated scatterers](https://arxiv.org/abs/2507.13130)
*Aleksandr D. Kuznetsov,Jari Holopainen,Ville Viikari*

Main category: eess.SP

TL;DR: 提出了一种基于多端口S参数的多频散射模型，适用于非周期性、时空调制的散射结构，扩展了传统S矩阵的适用范围。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以准确预测多频操作和时空调制散射结构的性能，需要一种更全面的建模方法。

Method: 扩展多端口S参数模型，纳入结构散射、互耦合、非数字调制和非周期性配置。

Result: 模型验证了时空调制散射结构的准确性，具有实际应用价值。

Conclusion: 该模型为通信和传感系统的精确分析和优化提供了有效工具。

Abstract: Utilizing scatterers in communication engineering, such as reconfigurable
intelligent surfaces (RISs) and backscatter systems, requires physically
consistent models for accurate performance prediction. A multiport model, which
also accounts for structural scattering, has been developed for non-periodic
scatterers. However, many emerging systems operate at multiple frequencies or
generate intermodulation harmonics, particularly when incorporating space-time
modulation (STM) or dynamic load control. These functionalities demand advanced
modeling approaches capable of capturing scattering behavior across several
frequencies and directions simultaneously. This article extends a multiport
S-parameters-based model for predicting the scattering properties of
multifrequency operating structures. The model extends the applicability of
convenient S-matrix models to time-modulated multiport structures. Unlike known
approaches, this model incorporates structural scattering, mutual coupling, the
possibility of non-digital modulation, and non-periodic configurations,
enabling precise analysis and optimization for a broad range of communication
and sensing systems. Validation against experimental results for a space-time
modulated scattering structure demonstrates the accuracy and practical
applicability of the proposed model.

</details>


### [10] [Disentangling coincident cell events using deep transfer learning and compressive sensing](https://arxiv.org/abs/2507.13176)
*Moritz Leuthner,Rafael Vorländer,Oliver Hayden*

Main category: eess.SP

TL;DR: 提出了一种结合全卷积神经网络（FCN）和压缩感知（CS）的混合框架，用于解决单细胞分析中的信号重叠问题，显著提高了事件恢复和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 单细胞分析在诊断和免疫监测中至关重要，但信号重叠会严重影响数据准确性，需要一种高效的方法来解决这一问题。

Method: 使用FCN估计重叠事件数量，并结合CS模块重构信号，通过磁流式细胞术（MFC）验证。

Result: 相比传统方法，该框架恢复了21%更多事件，分类准确率超过97%。

Conclusion: 该框架为下一代非光学单细胞传感平台奠定了基础，扩展了细胞术在医学中的应用。

Abstract: Accurate single-cell analysis is critical for diagnostics, immunomonitoring,
and cell therapy, but coincident events - where multiple cells overlap in a
sensing zone - can severely compromise signal fidelity. We present a hybrid
framework combining a fully convolutional neural network (FCN) with compressive
sensing (CS) to disentangle such overlapping events in one-dimensional sensor
data. The FCN, trained on bead-derived datasets, accurately estimates
coincident event counts and generalizes to immunomagnetically labeled CD4+ and
CD14+ cells in whole blood without retraining. Using this count, the CS module
reconstructs individual signal components with high fidelity, enabling precise
recovery of single-cell features, including velocity, amplitude, and
hydrodynamic diameter. Benchmarking against conventional state-machine
algorithms shows superior performance - recovering up to 21% more events and
improving classification accuracy beyond 97%. Explinability via class
activation maps and parameterized Gaussian template fitting ensures
transparency and clinical interpretability. Demonstrated with magnetic flow
cytometry (MFC), the framework is compatible with other waveform-generating
modalities, including impedance cytometry, nanopore, and resistive pulse
sensing. This work lays the foundation for next-generation non-optical
single-cell sensing platforms that are automated, generalizable, and capable of
resolving overlapping events, broadening the utility of cytometry in
translational medicine and precision diagnostics, e.g. cell-interaction
studies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training](https://arxiv.org/abs/2507.12507)
*Mingjie Liu,Shizhe Diao,Jian Hu,Ximing Lu,Xin Dong,Hao Zhang,Alexander Bukharin,Shaokun Zhang,Jiaqi Zeng,Makesh Narsimhan Sreedhar,Gerald Shen,David Mosallanezhad,Di Zhang,Jonas Yang,June Yang,Oleksii Kuchaiev,Guilin Liu,Zhiding Yu,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.LG

TL;DR: 研究探讨了长时间强化学习对小语言模型在多样化推理任务中的影响，提出了关键训练要素，并显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索长时间强化学习对小语言模型在复杂推理任务中的效果，以验证其可扩展性和实用性。

Method: 结合可验证奖励任务、改进的GRPO算法，以及控制KL正则化、裁剪比率和周期性参考策略重置等技术。

Result: 模型在数学、编程和逻辑谜题任务上分别提升了14.7%、13.9%和54.8%。

Conclusion: 通过优化训练策略，小语言模型也能在复杂推理任务中取得显著进步，为后续研究提供了公开模型。

Abstract: Recent advancements in reasoning-focused language models such as OpenAI's O1
and DeepSeek-R1 have shown that scaling test-time computation-through
chain-of-thought reasoning and iterative exploration-can yield substantial
improvements on complex tasks like mathematics and code generation. These
breakthroughs have been driven by large-scale reinforcement learning (RL),
particularly when combined with verifiable reward signals that provide
objective and grounded supervision. In this report, we investigate the effects
of prolonged reinforcement learning on a small language model across a diverse
set of reasoning domains. Our work identifies several key ingredients for
effective training, including the use of verifiable reward tasks, enhancements
to Group Relative Policy Optimization (GRPO), and practical techniques to
improve training stability and generalization. We introduce controlled KL
regularization, clipping ratio, and periodic reference policy resets as
critical components for unlocking long-term performance gains. Our model
achieves significant improvements over strong baselines, including +14.7% on
math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate
continued research, we release our model publicly.

</details>


### [12] [Leveraging Asynchronous Cross-border Market Data for Improved Day-Ahead Electricity Price Forecasting in European Markets](https://arxiv.org/abs/2507.13250)
*Maria Margarida Mascarenhas,Jilles De Blauwe,Mikael Amelin,Hussain Kazmi*

Main category: cs.LG

TL;DR: 研究探讨了利用不同市场关闸时间（GCT）的异步电价数据提升其他市场电价预测准确性的方法，结果显示在比利时和瑞典市场分别提升了22%和9%的预测精度。


<details>
  <summary>Details</summary>
Motivation: 短期电价精准预测对电力市场投标策略至关重要，但现有数据驱动方法依赖输入数据的质量。

Method: 采用先进模型集成，利用关闸时间较早的互联市场（如德国-卢森堡、奥地利和瑞士）的电价数据，预测关闸时间较晚的市场（如比利时和瑞典）的电价。

Result: 在比利时和瑞典市场分别实现了22%和9%的预测精度提升，且在极端市场条件下同样有效。

Conclusion: 研究为欧洲互联电力市场的参与者提供了优化投标策略的指导，同时指出频繁模型校准和过多市场数据可能带来的问题。

Abstract: Accurate short-term electricity price forecasting is crucial for
strategically scheduling demand and generation bids in day-ahead markets. While
data-driven techniques have shown considerable prowess in achieving high
forecast accuracy in recent years, they rely heavily on the quality of input
covariates. In this paper, we investigate whether asynchronously published
prices as a result of differing gate closure times (GCTs) in some bidding zones
can improve forecasting accuracy in other markets with later GCTs. Using a
state-of-the-art ensemble of models, we show significant improvements of 22%
and 9% in forecast accuracy in the Belgian (BE) and Swedish bidding zones (SE3)
respectively, when including price data from interconnected markets with
earlier GCT (Germany-Luxembourg, Austria, and Switzerland). This improvement
holds for both general as well as extreme market conditions. Our analysis also
yields further important insights: frequent model recalibration is necessary
for maximum accuracy but comes at substantial additional computational costs,
and using data from more markets does not always lead to better performance - a
fact we delve deeper into with interpretability analysis of the forecast
models. Overall, these findings provide valuable guidance for market
participants and decision-makers aiming to optimize bidding strategies within
increasingly interconnected and volatile European energy markets.

</details>


### [13] [The Serial Scaling Hypothesis](https://arxiv.org/abs/2507.12549)
*Yuxi Liu,Konpat Preechakul,Kananart Kuwaranancharoen,Yutong Bai*

Main category: cs.LG

TL;DR: 论文指出机器学习中的并行化存在盲点，某些问题本质上是顺序性的，无法并行化，需重视顺序计算。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习依赖大规模并行化，但忽视了某些问题（如数学推理、物理模拟、顺序决策）本质上是顺序性的，需依赖计算步骤。

Method: 通过复杂性理论形式化并行与顺序计算的区分，分析现有并行架构的局限性。

Result: 证明并行架构在顺序性任务上存在根本限制，需重新设计模型和硬件。

Conclusion: 未来AI需同时扩展并行和顺序计算能力，以应对复杂推理任务。

Abstract: While machine learning has advanced through massive parallelization, we
identify a critical blind spot: some problems are fundamentally sequential.
These "inherently serial" problems-from mathematical reasoning to physical
simulations to sequential decision-making-require dependent computational steps
that cannot be parallelized. Drawing from complexity theory, we formalize this
distinction and demonstrate that current parallel-centric architectures face
fundamental limitations on such tasks. We argue that recognizing the serial
nature of computation holds profound implications on machine learning, model
design, hardware development. As AI tackles increasingly complex reasoning,
deliberately scaling serial computation-not just parallel computation-is
essential for continued progress.

</details>


### [14] [Can Mental Imagery Improve the Thinking Capabilities of AI Systems?](https://arxiv.org/abs/2507.12555)
*Slimane Larabi*

Main category: cs.LG

TL;DR: 论文提出了一种结合心理意象的机器思维框架，以解决现有模型在自主推理和多领域知识整合上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有模型缺乏自主行动和独立推理能力，且输入数据通常为显式查询，而AI代理在多领域知识整合上表现不佳。心理意象在人类思维中起关键作用，因此研究如何将其融入机器思维框架。

Method: 提出一个机器思维框架，包含认知思维单元及三个辅助单元（输入数据单元、需求单元和心理意象单元），数据以自然语言或草图形式表示。

Result: 验证测试展示了该框架的有效性，结果进行了详细讨论。

Conclusion: 结合心理意象的机器思维框架有望提升机器的自主推理和多领域知识整合能力。

Abstract: Although existing models can interact with humans and provide satisfactory
responses, they lack the ability to act autonomously or engage in independent
reasoning. Furthermore, input data in these models is typically provided as
explicit queries, even when some sensory data is already acquired.
  In addition, AI agents, which are computational entities designed to perform
tasks and make decisions autonomously based on their programming, data inputs,
and learned knowledge, have shown significant progress. However, they struggle
with integrating knowledge across multiple domains, unlike humans.
  Mental imagery plays a fundamental role in the brain's thinking process,
which involves performing tasks based on internal multisensory data, planned
actions, needs, and reasoning capabilities. In this paper, we investigate how
to integrate mental imagery into a machine thinking framework and how this
could be beneficial in initiating the thinking process. Our proposed machine
thinking framework integrates a Cognitive thinking unit supported by three
auxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery
Unit. Within this framework, data is represented as natural language sentences
or drawn sketches, serving both informative and decision-making purposes. We
conducted validation tests for this framework, and the results are presented
and discussed.

</details>


### [15] [IncA-DES: An incremental and adaptive dynamic ensemble selection approach using online K-d tree neighborhood search for data streams with concept drift](https://arxiv.org/abs/2507.12573)
*Eduardo V. L. Barboza,Paulo R. Lisboa de Almeida,Alceu de Souza Britto Jr.,Robert Sabourin,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 本文提出了一种名为IncA-DES的新方法，用于处理数据流中的概念漂移问题，通过融合分类器和在线K-d树算法，提高了准确性和处理效率。


<details>
  <summary>Details</summary>
Motivation: 数据流中的概念漂移问题对机器学习提出了挑战，现有方法在处理连续到达的数据时效率不足。

Method: 提出IncA-DES方法，结合局部专家生成、概念漂移检测器和重叠分类过滤器，并引入在线K-d树算法优化处理时间。

Result: 实验表明，该方法在准确性和处理时间上优于七种先进方法，且在线K-d树的融合显著提升了效率。

Conclusion: IncA-DES框架有效解决了数据流中的概念漂移问题，并在性能和效率上取得了显著提升。

Abstract: Data streams pose challenges not usually encountered in batch-based ML. One
of them is concept drift, which is characterized by the change in data
distribution over time. Among many approaches explored in literature, the
fusion of classifiers has been showing good results and is getting growing
attention. DS methods, due to the ensemble being instance-based, seem to be an
efficient choice under drifting scenarios. However, some attention must be paid
to adapting such methods for concept drift. The training must be done in order
to create local experts, and the commonly used neighborhood-search DS may
become prohibitive with the continuous arrival of data. In this work, we
propose IncA-DES, which employs a training strategy that promotes the
generation of local experts with the assumption that different regions of the
feature space become available with time. Additionally, the fusion of a concept
drift detector supports the maintenance of information and adaptation to a new
concept. An overlap-based classification filter is also employed in order to
avoid using the DS method when there is a consensus in the neighborhood, a
strategy that we argue every DS method should employ, as it was shown to make
them more applicable and quicker. Moreover, aiming to reduce the processing
time of the kNN, we propose an Online K-d tree algorithm, which can quickly
remove instances without becoming inconsistent and deals with unbalancing
concerns that may occur in data streams. Experimental results showed that the
proposed framework got the best average accuracy compared to seven
state-of-the-art methods considering different levels of label availability and
presented the smaller processing time between the most accurate methods.
Additionally, the fusion with the Online K-d tree has improved processing time
with a negligible loss in accuracy. We have made our framework available in an
online repository.

</details>


### [16] [Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions](https://arxiv.org/abs/2507.12659)
*Athanasios Papastathopoulos-Katsaros,Alexandra Stavrianidi,Zhandong Liu*

Main category: cs.LG

TL;DR: 本文提出了一种结合迁移学习和自适应激活函数的PINNs方法，显著提升了模型在训练域外的外推性能。


<details>
  <summary>Details</summary>
Motivation: 尽管PINNs在结合物理定律和数据驱动建模方面表现出色，但其外推性能较差且对激活函数选择敏感，因此需要改进。

Method: 采用迁移学习扩展训练域，并引入自适应激活函数（标准激活函数的线性组合）。

Result: 实验表明，该方法在外推域中平均减少40%的相对L2误差和50%的平均绝对误差，且计算成本未显著增加。

Conclusion: 该方法有效提升了PINNs的外推能力和鲁棒性，代码已开源。

Abstract: Physics-Informed Neural Networks (PINNs) are deep learning models that
incorporate the governing physical laws of a system into the learning process,
making them well-suited for solving complex scientific and engineering
problems. Recently, PINNs have gained widespread attention as a powerful
framework for combining physical principles with data-driven modeling to
improve prediction accuracy. Despite their successes, however, PINNs often
exhibit poor extrapolation performance outside the training domain and are
highly sensitive to the choice of activation functions (AFs). In this paper, we
introduce a transfer learning (TL) method to improve the extrapolation
capability of PINNs. Our approach applies transfer learning (TL) within an
extended training domain, using only a small number of carefully selected
collocation points. Additionally, we propose an adaptive AF that takes the form
of a linear combination of standard AFs, which improves both the robustness and
accuracy of the model. Through a series of experiments, we demonstrate that our
method achieves an average of 40% reduction in relative L2 error and an average
of 50% reduction in mean absolute error in the extrapolation domain, all
without a significant increase in computational cost. The code is available at
https://github.com/LiuzLab/PINN-extrapolation .

</details>


### [17] [Assay2Mol: large language model-based drug design using BioAssay context](https://arxiv.org/abs/2507.12574)
*Yifan Deng,Spencer S. Ericksen,Anthony Gitter*

Main category: cs.LG

TL;DR: Assay2Mol是一种基于大型语言模型的工作流，利用现有生化筛选数据生成候选分子，优于其他机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 生化筛选数据中的非结构化文本包含丰富信息，但未被充分利用。

Method: Assay2Mol通过检索类似目标的现有筛选记录，利用上下文学习生成候选分子。

Result: Assay2Mol在生成候选配体分子方面优于其他方法，且更易合成。

Conclusion: Assay2Mol为早期药物发现提供了高效工具。

Abstract: Scientific databases aggregate vast amounts of quantitative data alongside
descriptive text. In biochemistry, molecule screening assays evaluate the
functional responses of candidate molecules against disease targets.
Unstructured text that describes the biological mechanisms through which these
targets operate, experimental screening protocols, and other attributes of
assays offer rich information for new drug discovery campaigns but has been
untapped because of that unstructured format. We present Assay2Mol, a large
language model-based workflow that can capitalize on the vast existing
biochemical screening assays for early-stage drug discovery. Assay2Mol
retrieves existing assay records involving targets similar to the new target
and generates candidate molecules using in-context learning with the retrieved
assay screening data. Assay2Mol outperforms recent machine learning approaches
that generate candidate ligand molecules for target protein structures, while
also promoting more synthesizable molecule generation.

</details>


### [18] [A Kernel Distribution Closeness Testing](https://arxiv.org/abs/2507.12843)
*Zhijian Zhou,Liuhua Peng,Xunye Tian,Feng Liu*

Main category: cs.LG

TL;DR: 论文提出了一种新的分布接近性测试方法NAMMD，通过结合RKHS范数改进MMD，提高了测试能力。


<details>
  <summary>Details</summary>
Motivation: 现有DCT方法主要针对离散一维空间，难以处理复杂数据（如图像），且MMD在多分布对评估中信息不足。

Method: 设计NAMMD，通过RKHS范数缩放MMD值，并基于其渐近分布提出NAMMD-based DCT。

Result: 理论证明NAMMD-based DCT比MMD-based DCT具有更高的测试能力，实验验证了其在多种数据上的有效性。

Conclusion: NAMMD不仅适用于DCT，还在两样本测试中表现优于MMD，具有理论和实验支持。

Abstract: The distribution closeness testing (DCT) assesses whether the distance
between a distribution pair is at least $\epsilon$-far. Existing DCT methods
mainly measure discrepancies between a distribution pair defined on discrete
one-dimensional spaces (e.g., using total variation), which limits their
applications to complex data (e.g., images). To extend DCT to more types of
data, a natural idea is to introduce maximum mean discrepancy (MMD), a powerful
measurement of the distributional discrepancy between two complex
distributions, into DCT scenarios. However, we find that MMD's value can be the
same for many pairs of distributions that have different norms in the same
reproducing kernel Hilbert space (RKHS), making MMD less informative when
assessing the closeness levels for multiple distribution pairs. To mitigate the
issue, we design a new measurement of distributional discrepancy, norm-adaptive
MMD (NAMMD), which scales MMD's value using the RKHS norms of distributions.
Based on the asymptotic distribution of NAMMD, we finally propose the
NAMMD-based DCT to assess the closeness levels of a distribution pair.
Theoretically, we prove that NAMMD-based DCT has higher test power compared to
MMD-based DCT, with bounded type-I error, which is also validated by extensive
experiments on many types of data (e.g., synthetic noise, real images).
Furthermore, we also apply the proposed NAMMD for addressing the two-sample
testing problem and find NAMMD-based two-sample test has higher test power than
the MMD-based two-sample test in both theory and experiments.

</details>


### [19] [Ranking Vectors Clustering: Theory and Applications](https://arxiv.org/abs/2507.12583)
*Ali Fattahi,Ali Eshragh,Babak Aslani,Meysam Rabiee*

Main category: cs.LG

TL;DR: 论文研究了基于排序向量的聚类问题（KRC），提出了一种高效的近似算法KRCA，并通过理论和实验验证其优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究KRC问题的动机在于其在实际应用（如个性化推荐和大规模决策）中的重要性，同时解决了传统k-means聚类无法直接处理排序向量的局限性。

Method: 论文提出KRCA算法，通过迭代优化初始解，并结合分支定界（BnB）算法提升效率。此外，单簇情况下的最优质心有闭式解。

Result: 实验表明KRCA在合成和真实数据集上均优于基线方法，且计算速度快。理论分析提供了误差界限。

Conclusion: KRC在个性化和大规模决策中具有实际意义，KRCA为未来研究提供了方法论基础和启示。

Abstract: We study the problem of clustering ranking vectors, where each vector
represents preferences as an ordered list of distinct integers. Specifically,
we focus on the k-centroids ranking vectors clustering problem (KRC), which
aims to partition a set of ranking vectors into k clusters and identify the
centroid of each cluster. Unlike classical k-means clustering (KMC), KRC
constrains both the observations and centroids to be ranking vectors. We
establish the NP-hardness of KRC and characterize its feasible set. For the
single-cluster case, we derive a closed-form analytical solution for the
optimal centroid, which can be computed in linear time. To address the
computational challenges of KRC, we develop an efficient approximation
algorithm, KRCA, which iteratively refines initial solutions from KMC, referred
to as the baseline solution. Additionally, we introduce a branch-and-bound
(BnB) algorithm for efficient cluster reconstruction within KRCA, leveraging a
decision tree framework to reduce computational time while incorporating a
controlling parameter to balance solution quality and efficiency. We establish
theoretical error bounds for KRCA and BnB. Through extensive numerical
experiments on synthetic and real-world datasets, we demonstrate that KRCA
consistently outperforms baseline solutions, delivering significant
improvements in solution quality with fast computational times. This work
highlights the practical significance of KRC for personalization and
large-scale decision making, offering methodological advancements and insights
that can be built upon in future studies.

</details>


### [20] [Second-Order Bounds for [0,1]-Valued Regression via Betting Loss](https://arxiv.org/abs/2507.12584)
*Yinan Li,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 论文研究了[0,1]值回归问题，提出了一种新的损失函数（betting loss），实现了无需方差知识的方差自适应界限。


<details>
  <summary>Details</summary>
Motivation: 探索在[0,1]值回归问题中是否存在一种损失函数，能够实现比一阶界限更优的方差依赖界限（二阶界限）。

Method: 提出了一种名为betting loss的新损失函数，并分析了其在回归问题中的表现。

Result: 证明了betting loss能够实现方差自适应界限，且无需事先知道方差信息。

Conclusion: betting loss在[0,1]值回归问题中表现优异，能够自适应地实现二阶界限。

Abstract: We consider the $[0,1]$-valued regression problem in the i.i.d. setting. In a
related problem called cost-sensitive classification, \citet{foster21efficient}
have shown that the log loss minimizer achieves an improved generalization
bound compared to that of the squared loss minimizer in the sense that the
bound scales with the cost of the best classifier, which can be arbitrarily
small depending on the problem at hand. Such a result is often called a
first-order bound. For $[0,1]$-valued regression, we first show that the log
loss minimizer leads to a similar first-order bound. We then ask if there
exists a loss function that achieves a variance-dependent bound (also known as
a second order bound), which is a strict improvement upon first-order bounds.
We answer this question in the affirmative by proposing a novel loss function
called the betting loss. Our result is ``variance-adaptive'' in the sense that
the bound is attained \textit{without any knowledge about the variance}, which
is in contrast to modeling label (or reward) variance or the label distribution
itself explicitly as part of the function class such as distributional
reinforcement learning.

</details>


### [21] [Are encoders able to learn landmarkers for warm-starting of Hyperparameter Optimization?](https://arxiv.org/abs/2507.12604)
*Antoni Zajko,Katarzyna Woźnica*

Main category: cs.LG

TL;DR: 论文提出两种针对元任务（贝叶斯超参数优化的热启动）的表格表示学习方法，分别基于深度度量学习和标志重建，但实验表明其性能提升有限。


<details>
  <summary>Details</summary>
Motivation: 异构表格数据集在元学习中的表示仍是一个开放问题，现有方法依赖通用表示，本文针对特定元任务提出定制化方法。

Method: 提出两种方法：1) 深度度量学习；2) 标志重建，旨在捕捉标志属性。

Result: 实验表明，提出的编码器能有效学习与标志对齐的表示，但在元任务中性能提升不显著。

Conclusion: 定制化表示方法虽能捕捉特定属性，但未直接转化为元任务的显著性能提升。

Abstract: Effectively representing heterogeneous tabular datasets for meta-learning
purposes is still an open problem. Previous approaches rely on representations
that are intended to be universal. This paper proposes two novel methods for
tabular representation learning tailored to a specific meta-task -
warm-starting Bayesian Hyperparameter Optimization. Both follow the specific
requirement formulated by ourselves that enforces representations to capture
the properties of landmarkers. The first approach involves deep metric
learning, while the second one is based on landmarkers reconstruction. We
evaluate the proposed encoders in two ways. Next to the gain in the target
meta-task, we also use the degree of fulfillment of the proposed requirement as
the evaluation metric. Experiments demonstrate that while the proposed encoders
can effectively learn representations aligned with landmarkers, they may not
directly translate to significant performance gains in the meta-task of HPO
warm-starting.

</details>


### [22] [Learning What Matters: Probabilistic Task Selection via Mutual Information for Model Finetuning](https://arxiv.org/abs/2507.12612)
*Prateek Chanda,Saral Sureka,Parth Pratim Chatterjee,Krishnateja Killamsetty,Nikhil Shivakumar Nayak,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: TASKPGM是一个用于优化大型语言模型（LLM）微调任务混合比例的框架，通过最小化马尔可夫随机场的能量函数，自动选择任务比例，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前任务混合比例的选择依赖手动和启发式方法，缺乏系统性优化。

Method: 使用行为差异（如Jensen Shannon Divergence和Pointwise Mutual Information）建模任务关系，通过MRF框架优化任务比例。

Result: 在Llama 2和Mistral上，TASKPGM在MMLU和BIGBench等评估中表现优于传统方法。

Conclusion: TASKPGM为LLM微调提供了高效、可解释的任务混合优化工具。

Abstract: The performance of finetuned large language models (LLMs) hinges critically
on the composition of the training mixture. However, selecting an optimal blend
of task datasets remains a largely manual, heuristic driven process, with
practitioners often relying on uniform or size based sampling strategies. We
introduce TASKPGM, a principled and scalable framework for mixture optimization
that selects continuous task proportions by minimizing an energy function over
a Markov Random Field (MRF). Task relationships are modeled using behavioral
divergences such as Jensen Shannon Divergence and Pointwise Mutual Information
computed from the predictive distributions of single task finetuned models. Our
method yields a closed form solution under simplex constraints and provably
balances representativeness and diversity among tasks. We provide theoretical
guarantees, including weak submodularity for budgeted variants, and demonstrate
consistent empirical improvements on Llama 2 and Mistral across evaluation
suites such as MMLU and BIGBench. Beyond performance, TASKPGM offers
interpretable insights into task influence and mixture composition, making it a
powerful tool for efficient and robust LLM finetuning.

</details>


### [23] [BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training](https://arxiv.org/abs/2507.12619)
*Rui Li,Xiaoyun Zhi,Jinxin Chi,Menghan Yu,Lixin Huang,Jia Zhu,Weilun Zhang,Xing Ma,Wenjia Liu,Zhicheng Zhu,Daowen Luo,Zuquan Song,Xin Yin,Chao Xiang,Shuguang Wang,Wencong Xiao,Gene Cooperman*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLM）训练中的启动开销问题，提出了优化框架Bootseer，减少了50%的启动时间。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在工业规模中的应用，启动开销成为关键问题，尤其是在多团队迭代调试的场景中。研究表明，启动开销浪费了大量GPU时间。

Method: 通过分析真实生产数据，识别启动开销的主要瓶颈（容器镜像加载、运行时依赖安装、模型检查点恢复），并设计Bootseer框架优化这些瓶颈。

Result: Bootseer在实际部署中减少了50%的启动开销。

Conclusion: 启动开销是LLM训练中的重要问题，Bootseer通过系统级优化显著提升了效率。

Abstract: Large Language Models (LLMs) have become a cornerstone of modern AI, driving
breakthroughs in natural language processing and expanding into multimodal jobs
involving images, audio, and video. As with most computational software, it is
important to distinguish between ordinary runtime performance and startup
overhead. Prior research has focused on runtime performance: improving training
efficiency and stability. This work focuses instead on the increasingly
critical issue of startup overhead in training: the delay before training jobs
begin execution. Startup overhead is particularly important in large,
industrial-scale LLMs, where failures occur more frequently and multiple teams
operate in iterative update-debug cycles. In one of our training clusters, more
than 3.5% of GPU time is wasted due to startup overhead alone.
  In this work, we present the first in-depth characterization of LLM training
startup overhead based on real production data. We analyze the components of
startup cost, quantify its direct impact, and examine how it scales with job
size. These insights motivate the design of Bootseer, a system-level
optimization framework that addresses three primary startup bottlenecks: (a)
container image loading, (b) runtime dependency installation, and (c) model
checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three
techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and
(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment
and evaluated on real LLM training workloads, demonstrating a 50% reduction in
startup overhead.

</details>


### [24] [Reasoning-Finetuning Repurposes Latent Representations in Base Models](https://arxiv.org/abs/2507.12638)
*Jake Ward,Chuqiao Lin,Constantin Venhoff,Neel Nanda*

Main category: cs.LG

TL;DR: 研究发现，推理微调模型中的回溯行为部分由基础模型中已存在的方向驱动，而非全新学习。


<details>
  <summary>Details</summary>
Motivation: 探索推理微调模型中回溯行为的驱动机制，揭示其与基础模型的关系。

Method: 通过识别基础模型中的特定方向，并验证其在蒸馏推理模型中对回溯行为的影响。

Result: 发现基础模型中的特定方向能诱导蒸馏模型回溯，且该行为无法通过词级属性简单解释。

Conclusion: 推理微调模型通过重新利用基础模型的表征形成新行为回路，而非从头学习。

Abstract: Backtracking, an emergent behavior elicited by reasoning fine-tuning, has
been shown to be a key mechanism in reasoning models' enhanced capabilities.
Prior work has succeeded in manipulating this behavior via steering vectors,
but the underlying mechanism remains poorly understood. In this work, we show
that the emergence of backtracking in DeepSeek-R1-Distill-Llama-8B is in part
driven by a repurposed direction already present in base model activations.
Specifically, we identify a direction in base Llama-3.1-8B's residual stream
which systematically induces backtracking when used to steer the distilled
reasoning model, and find that the effects of steering with this direction
cannot be trivially explained by token-level attributes. We further find that
this direction does not induce backtracking in the base model, suggesting that
the reasoning finetuning process repurposes pre-existing representations to
form new behavioral circuits. Additionally, we hypothesize that this direction
is one of several which may work together to mediate backtracking. Our findings
offer a compelling picture that reasoning-finetuned models repurpose
pre-existing base model representations, rather than learn new capabilities
from scratch.

</details>


### [25] [Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective](https://arxiv.org/abs/2507.12652)
*Kai Malcolm,César Uribe,Momona Yamagami*

Main category: cs.LG

TL;DR: 论文探讨了联邦学习在神经解码中的应用，分析了其在开环和闭环场景下的性能与隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 神经信号包含敏感信息，数据共享存在隐私问题，联邦学习提供了一种隐私保护的解决方案。

Method: 引入基于联邦学习的神经解码方法，并在开环和闭环场景下进行系统性评估。

Result: 开环中联邦学习表现优于本地学习，闭环中需调整方法，本地学习性能更好但隐私风险更高。

Conclusion: 需设计专门针对单用户实时应用的联邦学习方法，以平衡性能与隐私。

Abstract: Invasive and non-invasive neural interfaces hold promise as high-bandwidth
input devices for next-generation technologies. However, neural signals
inherently encode sensitive information about an individual's identity and
health, making data sharing for decoder training a critical privacy challenge.
Federated learning (FL), a distributed, privacy-preserving learning framework,
presents a promising solution, but it remains unexplored in closed-loop
adaptive neural interfaces. Here, we introduce FL-based neural decoding and
systematically evaluate its performance and privacy using high-dimensional
electromyography signals in both open- and closed-loop scenarios. In open-loop
simulations, FL significantly outperformed local learning baselines,
demonstrating its potential for high-performance, privacy-conscious neural
decoding. In contrast, closed-loop user studies required adapting FL methods to
accommodate single-user, real-time interactions, a scenario not supported by
standard FL. This modification resulted in local learning decoders surpassing
the adapted FL approach in closed-loop performance, yet local learning still
carried higher privacy risks. Our findings highlight a critical
performance-privacy tradeoff in real-time adaptive applications and indicate
the need for FL methods specifically designed for co-adaptive, single-user
applications.

</details>


### [26] [Data Transformation Strategies to Remove Heterogeneity](https://arxiv.org/abs/2507.12677)
*Sangbong Yoo,Jaeyoung Lee,Chanyoung Yoon,Geonyeong Son,Hyein Hong,Seongbum Seo,Soobin Yim,Chanyoung Jung,Jungsoo Park,Misuk Kim,Yun Jang*

Main category: cs.LG

TL;DR: 论文探讨了数据异构性问题及其来源，系统分类并提出了解决数据格式差异的策略，强调了数据转换在AI应用中的重要性。


<details>
  <summary>Details</summary>
Motivation: 数据异构性及其复杂性导致需要专家介入解决，而现有方法多关注数据结构和模式，忽略了数据转换的关键作用。随着AI应用的扩展，对高效数据准备的需求增加。

Method: 系统分类并分析数据异构性来源，提出针对数据格式差异的策略，并探讨每种策略的挑战。

Result: 揭示了数据转换在AI学习效率和模型适应性中的重要性，填补了现有研究中数据转换方法综述的空白。

Conclusion: 数据转换是解决数据异构性的关键，需进一步研究以优化其在AI中的应用。

Abstract: Data heterogeneity is a prevalent issue, stemming from various conflicting
factors, making its utilization complex. This uncertainty, particularly
resulting from disparities in data formats, frequently necessitates the
involvement of experts to find resolutions. Current methodologies primarily
address conflicts related to data structures and schemas, often overlooking the
pivotal role played by data transformation. As the utilization of artificial
intelligence (AI) continues to expand, there is a growing demand for a more
streamlined data preparation process, and data transformation becomes
paramount. It customizes training data to enhance AI learning efficiency and
adapts input formats to suit diverse AI models. Selecting an appropriate
transformation technique is paramount in preserving crucial data details.
Despite the widespread integration of AI across various industries,
comprehensive reviews concerning contemporary data transformation approaches
are scarce. This survey explores the intricacies of data heterogeneity and its
underlying sources. It systematically categorizes and presents strategies to
address heterogeneity stemming from differences in data formats, shedding light
on the inherent challenges associated with each strategy.

</details>


### [27] [PinFM: Foundation Model for User Activity Sequences at a Billion-scale Visual Discovery Platform](https://arxiv.org/abs/2507.12704)
*Xiangyi Chen,Kousik Rajesh,Matthew Lawhon,Zelun Wang,Hanyu Li,Haomiao Li,Saurabh Vishwas Joshi,Pong Eksombatchai,Jaewon Yang,Yi-Ping Hsu,Jiajing Xu,Charles Rosenberg*

Main category: cs.LG

TL;DR: PinFM是一个基于Transformer的基础模型，用于处理大规模用户活动序列，通过预训练和微调提升推荐系统性能，解决了工业级推荐系统中的挑战。


<details>
  <summary>Details</summary>
Motivation: 用户活动序列是推荐系统中的重要信号，但现有方法在工业级应用中面临扩展性、成本和延迟等挑战。

Method: 预训练一个20B+参数的Transformer模型，结合创新技术（如DCAT）进行优化，并微调以适应特定应用。

Result: PinFM提升了600%的吞吐量，并通过调整输入序列使新物品的互动率提高了20%。

Conclusion: PinFM成功部署，改善了超过5亿用户的体验，展示了基础模型在推荐系统中的潜力。

Abstract: User activity sequences have emerged as one of the most important signals in
recommender systems. We present a foundational model, PinFM, for understanding
user activity sequences across multiple applications at a billion-scale visual
discovery platform. We pretrain a transformer model with 20B+ parameters using
extensive user activity data, then fine-tune it for specific applications,
efficiently coupling it with existing models. While this
pretraining-and-fine-tuning approach has been popular in other domains, such as
Vision and NLP, its application in industrial recommender systems presents
numerous challenges. The foundational model must be scalable enough to score
millions of items every second while meeting tight cost and latency constraints
imposed by these systems. Additionally, it should capture the interactions
between user activities and other features and handle new items that were not
present during the pretraining stage.
  We developed innovative techniques to address these challenges. Our
infrastructure and algorithmic optimizations, such as the Deduplicated
Cross-Attention Transformer (DCAT), improved our throughput by 600% on
Pinterest internal data. We demonstrate that PinFM can learn interactions
between user sequences and candidate items by altering input sequences, leading
to a 20% increase in engagement with new items. PinFM is now deployed to help
improve the experience of more than a half billion users across various
applications.

</details>


### [28] [From SGD to Spectra: A Theory of Neural Network Weight Dynamics](https://arxiv.org/abs/2507.12709)
*Brian Richard Olsen,Sam Fatehmanesh,Frank Xiao,Adarsh Kumarappan,Anirudh Gajula*

Main category: cs.LG

TL;DR: 论文提出了一个连续时间矩阵值随机微分方程（SDE）框架，将SGD的微观动力学与权重矩阵奇异值谱的宏观演化联系起来，解释了深度学习中观察到的谱结构。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的训练动力学在理论上尚不清晰，需要一种严格的框架来解释其行为。

Method: 开发了一个矩阵值SDE框架，推导了奇异值平方遵循Dyson布朗运动的精确SDE，并描述了稳态分布为伽马型密度。

Result: 理论预测与实验验证一致，解释了深度网络中观察到的'bulk+tail'谱结构。

Conclusion: 该研究为理解深度学习为何有效提供了严格的理论基础。

Abstract: Deep neural networks have revolutionized machine learning, yet their training
dynamics remain theoretically unclear-we develop a continuous-time,
matrix-valued stochastic differential equation (SDE) framework that rigorously
connects the microscopic dynamics of SGD to the macroscopic evolution of
singular-value spectra in weight matrices. We derive exact SDEs showing that
squared singular values follow Dyson Brownian motion with eigenvalue repulsion,
and characterize stationary distributions as gamma-type densities with
power-law tails, providing the first theoretical explanation for the
empirically observed 'bulk+tail' spectral structure in trained networks.
Through controlled experiments on transformer and MLP architectures, we
validate our theoretical predictions and demonstrate quantitative agreement
between SDE-based forecasts and observed spectral evolution, providing a
rigorous foundation for understanding why deep learning works.

</details>


### [29] [Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient Data-Centric Learning](https://arxiv.org/abs/2507.12750)
*Suorong Yang,Peijia Li,Yujie Liu,Zhiming Xu,Peng Ye,Wanli Ouyang,Furao Shen,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 提出一种动态数据集剪枝框架，结合任务驱动难度和跨模态语义一致性，提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据集剪枝方法依赖静态启发式或任务特定指标，缺乏跨领域鲁棒性和通用性。

Method: 利用预训练多模态基础模型的监督，动态选择训练样本，过滤无用数据。

Result: 框架能有效捕捉训练动态，提升样本选择效率。

Conclusion: 跨模态对齐在数据为中心的学习中具有潜力，可推动更高效和鲁棒的实践。

Abstract: Modern deep models are trained on large real-world datasets, where data
quality varies and redundancy is common. Data-centric approaches such as
dataset pruning have shown promise in improving training efficiency and model
performance. However, most existing methods rely on static heuristics or
task-specific metrics, limiting their robustness and generalizability across
domains. In this work, we introduce a dynamic dataset pruning framework that
adaptively selects training samples based on both task-driven difficulty and
cross-modality semantic consistency. By incorporating supervision from
pretrained multimodal foundation models, our approach captures training
dynamics while effectively filtering out uninformative samples. Our work
highlights the potential of integrating cross-modality alignment for robust
sample selection, advancing data-centric learning toward more efficient and
robust practices across application domains.

</details>


### [30] [Layer Separation Deep Learning Model with Auxiliary Variables for Partial Differential Equations](https://arxiv.org/abs/2507.12766)
*Yaru Liu,Yiqi Gu*

Main category: cs.LG

TL;DR: 提出了一种名为LySep的优化框架，通过引入辅助变量分离深度神经网络的层，解决了深度学习在求解偏微分方程时因高度非凸损失函数导致的局部极小值和梯度问题。


<details>
  <summary>Details</summary>
Motivation: 现有优化算法在深度学习求解偏微分方程时，常因损失函数高度非凸而陷入局部极小值或梯度问题，导致性能不佳。

Method: 通过引入辅助变量表示每层的输出及其导数，将深度架构分解为一系列浅层架构，并设计新的损失函数和交替方向算法。

Result: 理论分析表明LySep模型与原深度模型一致，高维数值结果验证了其在损失最小化和减少解误差方面的优势。

Conclusion: LySep模型有效解决了深度学习优化问题，提升了求解偏微分方程的性能。

Abstract: In this paper, we propose a new optimization framework, the layer separation
(LySep) model, to improve the deep learning-based methods in solving partial
differential equations. Due to the highly non-convex nature of the loss
function in deep learning, existing optimization algorithms often converge to
suboptimal local minima or suffer from gradient explosion or vanishing,
resulting in poor performance. To address these issues, we introduce auxiliary
variables to separate the layers of deep neural networks. Specifically, the
output and its derivatives of each layer are represented by auxiliary
variables, effectively decomposing the deep architecture into a series of
shallow architectures. New loss functions with auxiliary variables are
established, in which only variables from two neighboring layers are coupled.
Corresponding algorithms based on alternating directions are developed, where
many variables can be updated optimally in closed forms. Moreover, we provide
theoretical analyses demonstrating the consistency between the LySep model and
the original deep model. High-dimensional numerical results validate our theory
and demonstrate the advantages of LySep in minimizing loss and reducing
solution error.

</details>


### [31] [A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models](https://arxiv.org/abs/2507.12774)
*Weijieying Ren,Jingxi Zhu,Zehao Liu,Tianxiang Zhao,Vasant Honavar*

Main category: cs.LG

TL;DR: 该论文综述了深度学习、大语言模型与电子健康记录（EHR）建模的最新进展，提出了一个统一的分类法，涵盖五个关键设计维度，并讨论了未来挑战。


<details>
  <summary>Details</summary>
Motivation: EHR数据的异质性、时间不规则性和领域特异性带来了独特挑战，需要新的方法来解决。

Method: 通过分类法分析五个设计维度：数据为中心的方法、神经网络设计、学习策略、多模态学习和基于LLM的建模系统。

Result: 总结了代表性方法，如数据质量增强、自监督学习和临床知识整合，并指出新兴趋势（如基础模型和LLM驱动的临床代理）。

Conclusion: 论文为AI驱动的EHR建模和临床决策支持提供了结构化路线图，并提出了开放挑战（如基准测试和临床对齐）。

Abstract: Artificial intelligence (AI) has demonstrated significant potential in
transforming healthcare through the analysis and modeling of electronic health
records (EHRs). However, the inherent heterogeneity, temporal irregularity, and
domain-specific nature of EHR data present unique challenges that differ
fundamentally from those in vision and natural language tasks. This survey
offers a comprehensive overview of recent advancements at the intersection of
deep learning, large language models (LLMs), and EHR modeling. We introduce a
unified taxonomy that spans five key design dimensions: data-centric
approaches, neural architecture design, learning-focused strategies, multimodal
learning, and LLM-based modeling systems. Within each dimension, we review
representative methods addressing data quality enhancement, structural and
temporal representation, self-supervised learning, and integration with
clinical knowledge. We further highlight emerging trends such as foundation
models, LLM-driven clinical agents, and EHR-to-text translation for downstream
reasoning. Finally, we discuss open challenges in benchmarking, explainability,
clinical alignment, and generalization across diverse clinical settings. This
survey aims to provide a structured roadmap for advancing AI-driven EHR
modeling and clinical decision support. For a comprehensive list of EHR-related
methods, kindly refer to https://survey-on-tabular-data.github.io/.

</details>


### [32] [Multi-Channel Graph Neural Network for Financial Risk Prediction of NEEQ Enterprises](https://arxiv.org/abs/2507.12787)
*Jianyu Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种多通道深度学习框架，用于预测中国新三板上市公司的财务风险，结合结构化财务指标、文本披露和企业关系数据，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 新三板作为中小企业重要融资平台，其上市公司因规模小、财务韧性不足而面临较高财务风险，需更精准的风险预测方法。

Method: 设计了三通道图同构网络（GIN），分别处理数值、文本和图数据，通过注意力机制和门控单元融合多模态表示。

Result: 在7,731家新三板公司数据上，模型在AUC、精确率、召回率和F1分数上显著优于传统方法和单模态基线。

Conclusion: 该研究为中小企业风险建模提供了理论和实践支持，为监管者和投资者提供了数据驱动工具。

Abstract: With the continuous evolution of China's multi-level capital market, the
National Equities Exchange and Quotations (NEEQ), also known as the "New Third
Board," has become a critical financing platform for small and medium-sized
enterprises (SMEs). However, due to their limited scale and financial
resilience, many NEEQ-listed companies face elevated risks of financial
distress. To address this issue, we propose a multi-channel deep learning
framework that integrates structured financial indicators, textual disclosures,
and enterprise relationship data for comprehensive financial risk prediction.
Specifically, we design a Triple-Channel Graph Isomorphism Network (GIN) that
processes numeric, textual, and graph-based inputs separately. These
modality-specific representations are fused using an attention-based mechanism
followed by a gating unit to enhance robustness and prediction accuracy.
Experimental results on data from 7,731 real-world NEEQ companies demonstrate
that our model significantly outperforms traditional machine learning methods
and single-modality baselines in terms of AUC, Precision, Recall, and F1 Score.
This work provides theoretical and practical insights into risk modeling for
SMEs and offers a data-driven tool to support financial regulators and
investors.

</details>


### [33] [FLDmamba: Integrating Fourier and Laplace Transform Decomposition with Mamba for Enhanced Time Series Prediction](https://arxiv.org/abs/2507.12803)
*Qianru Zhang,Chenglei Yu,Haixin Wang,Yudong Yan,Yuansheng Cao,Siu-Ming Yiu,Tailin Wu,Hongzhi Yin*

Main category: cs.LG

TL;DR: FLDmamba结合傅里叶和拉普拉斯变换，解决了时间序列预测中多尺度周期性和瞬态动态的捕捉问题，同时提升了模型对数据噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据具有非平稳性、多尺度周期性和瞬态动态等复杂性，现有方法（如Transformer和Mamba）在长期预测中效率不足或无法有效捕捉这些特性。

Method: 提出FLDmamba框架，利用傅里叶和拉普拉斯变换的优势，捕捉多尺度周期性和瞬态动态，并增强对数据噪声的鲁棒性。

Result: FLDmamba在时间序列预测基准测试中表现优异，优于基于Transformer和其他Mamba的架构。

Conclusion: FLDmamba为时间序列预测提供了一种高效且鲁棒的解决方案，代码和数据已开源以促进可复现性。

Abstract: Time series prediction, a crucial task across various domains, faces
significant challenges due to the inherent complexities of time series data,
including non-stationarity, multi-scale periodicity, and transient dynamics,
particularly when tackling long-term predictions. While Transformer-based
architectures have shown promise, their quadratic complexity with sequence
length hinders their efficiency for long-term predictions. Recent advancements
in State-Space Models, such as Mamba, offer a more efficient alternative for
long-term modeling, but they cannot capture multi-scale periodicity and
transient dynamics effectively. Meanwhile, they are susceptible to data noise
issues in time series. This paper proposes a novel framework, FLDmamba (Fourier
and Laplace Transform Decomposition Mamba), addressing these limitations.
FLDmamba leverages the strengths of both Fourier and Laplace transforms to
effectively capture both multi-scale periodicity, transient dynamics within
time series data, and improve the robustness of the model to the data noise
issue. Our extensive experiments demonstrate that FLDmamba achieves superior
performance on time series prediction benchmarks, outperforming both
Transformer-based and other Mamba-based architectures. To promote the
reproducibility of our method, we have made both the code and data accessible
via the following
URL:{\href{https://github.com/AI4Science-WestlakeU/FLDmamba}{https://github.com/AI4Science-WestlakeU/\model}.

</details>


### [34] [PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database](https://arxiv.org/abs/2507.12805)
*Hui Sun,Yanfeng Ding,Liping Yi,Huidong Ma,Gang Wang,Xiaoguang Liu,Cheng Zhong,Wentong Cai*

Main category: cs.LG

TL;DR: 论文提出了一种基于并行多知识学习的无损压缩器（PMKLC），解决了现有学习型压缩器在压缩比、吞吐量和鲁棒性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有学习型无损压缩器在压缩比、吞吐量和鲁棒性方面表现不足，限制了其在基因组数据库备份、存储和管理中的广泛应用。

Method: 1) 提出自动化多知识学习框架；2) 设计GPU加速的(s,k)-mer编码器；3) 引入数据块分区和逐步模型传递机制；4) 提供单GPU和多GPU两种压缩模式。

Result: PMKLC在15个真实数据集上测试，压缩比提升最高达73.609%，吞吐量提升最高达10.710倍，且鲁棒性和内存效率表现优异。

Conclusion: PMKLC在压缩性能、吞吐量和鲁棒性上显著优于现有方法，适用于不同场景和资源受限设备。

Abstract: Learning-based lossless compressors play a crucial role in large-scale
genomic database backup, storage, transmission, and management. However, their
1) inadequate compression ratio, 2) low compression \& decompression
throughput, and 3) poor compression robustness limit their widespread adoption
and application in both industry and academia. To solve those challenges, we
propose a novel \underline{P}arallel \underline{M}ulti-\underline{K}nowledge
\underline{L}earning-based \underline{C}ompressor (PMKLC) with four crucial
designs: 1) We propose an automated multi-knowledge learning-based compression
framework as compressors' backbone to enhance compression ratio and robustness;
2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression
throughput and computing resource usage; 3) we introduce data block
partitioning and Step-wise Model Passing (SMP) mechanisms for parallel
acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet
the complex application scenarios, where the former runs on a
resource-constrained single GPU and the latter is multi-GPU accelerated. We
benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15
real-world datasets with different species and data sizes. Compared to
baselines on the testing datasets, PMKLC-S/M achieve the average compression
ratio improvement up to 73.609\% and 73.480\%, the average throughput
improvement up to 3.036$\times$ and 10.710$\times$, respectively. Besides,
PMKLC-S/M also achieve the best robustness and competitive memory cost,
indicating its greater stability against datasets with different probability
distribution perturbations, and its strong ability to run on memory-constrained
devices.

</details>


### [35] [RONOM: Reduced-Order Neural Operator Modeling](https://arxiv.org/abs/2507.12814)
*Sven Dummer,Dongwei Ye,Christoph Brune*

Main category: cs.LG

TL;DR: 论文提出了一种结合降阶模型（ROM）和神经算子学习的新框架RONOM，用于解决偏微分方程的多分辨率问题，并在性能和鲁棒性上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的降阶模型（ROM）依赖于固定离散化，限制了灵活性；神经算子学习虽能适应不同分辨率，但缺乏对无限维与离散化算子间误差的量化。RONOM旨在结合两者的优势。

Method: 提出RONOM框架，结合ROM和神经算子学习，建立离散化误差界限，并验证其在输入泛化、空间超分辨率和离散化鲁棒性上的表现。

Result: RONOM在输入泛化、空间超分辨率和离散化鲁棒性上表现优于现有神经算子，同时在时间超分辨率场景中提供了新见解。

Conclusion: RONOM成功结合了ROM和神经算子学习的优势，为偏微分方程的多分辨率建模提供了更灵活且性能优越的解决方案。

Abstract: Time-dependent partial differential equations are ubiquitous in physics-based
modeling, but they remain computationally intensive in many-query scenarios,
such as real-time forecasting, optimal control, and uncertainty quantification.
Reduced-order modeling (ROM) addresses these challenges by constructing a
low-dimensional surrogate model but relies on a fixed discretization, which
limits flexibility across varying meshes during evaluation. Operator learning
approaches, such as neural operators, offer an alternative by parameterizing
mappings between infinite-dimensional function spaces, enabling adaptation to
data across different resolutions. Whereas ROM provides rigorous numerical
error estimates, neural operator learning largely focuses on discretization
convergence and invariance without quantifying the error between the
infinite-dimensional and the discretized operators. This work introduces the
reduced-order neural operator modeling (RONOM) framework, which bridges
concepts from ROM and operator learning. We establish a discretization error
bound analogous to those in ROM, and get insights into RONOM's discretization
convergence and discretization robustness. Moreover, two numerical examples are
presented that compare RONOM to existing neural operators for solving partial
differential equations. The results demonstrate that RONOM using standard
vector-to-vector neural networks achieves comparable performance in input
generalization and superior performance in both spatial super-resolution and
discretization robustness, while also offering novel insights into temporal
super-resolution scenarios.

</details>


### [36] [From Novelty to Imitation: Self-Distilled Rewards for Offline Reinforcement Learning](https://arxiv.org/abs/2507.12815)
*Gaurav Chaudhary,Laxmidhar Behera*

Main category: cs.LG

TL;DR: ReLOAD提出了一种基于随机网络蒸馏的离线强化学习奖励标注框架，无需人工标注奖励信号。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习需要显式奖励标注，成本高且难以获取。

Method: 利用随机网络蒸馏（RND）从专家演示中生成内在奖励，通过嵌入差异度量提供结构化奖励信号。

Result: 在D4RL基准测试中表现优异，与传统奖励标注方法竞争。

Conclusion: ReLOAD为离线强化学习提供了一种高效且无需人工标注的奖励生成方法。

Abstract: Offline Reinforcement Learning (RL) aims to learn effective policies from a
static dataset without requiring further agent-environment interactions.
However, its practical adoption is often hindered by the need for explicit
reward annotations, which can be costly to engineer or difficult to obtain
retrospectively. To address this, we propose ReLOAD (Reinforcement Learning
with Offline Reward Annotation via Distillation), a novel reward annotation
framework for offline RL. Unlike existing methods that depend on complex
alignment procedures, our approach adapts Random Network Distillation (RND) to
generate intrinsic rewards from expert demonstrations using a simple yet
effective embedding discrepancy measure. First, we train a predictor network to
mimic a fixed target network's embeddings based on expert state transitions.
Later, the prediction error between these networks serves as a reward signal
for each transition in the static dataset. This mechanism provides a structured
reward signal without requiring handcrafted reward annotations. We provide a
formal theoretical construct that offers insights into how RND prediction
errors effectively serve as intrinsic rewards by distinguishing expert-like
transitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables
robust offline policy learning and achieves performance competitive with
traditional reward-annotated methods.

</details>


### [37] [Understanding the Evolution of the Neural Tangent Kernel at the Edge of Stability](https://arxiv.org/abs/2507.12837)
*Kaiqi Jiang,Jeremy Cohen,Yuanzhi Li*

Main category: cs.LG

TL;DR: 本文研究了在Edge of Stability（EoS）现象中NTK特征向量的动态行为，发现较大学习率会增强最终NTK特征向量与训练目标的匹配度，并提供了理论分析。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究深入探讨了NTK特征值在EoS中的行为机制，但对特征向量行为的理解仍不足。本文旨在填补这一空白。

Method: 通过实验观察不同架构下NTK特征向量的动态，并对两层线性网络进行理论分析。

Result: 发现较大学习率会提高最终NTK特征向量与训练目标的匹配度。

Conclusion: 研究增进了对GD训练动态的理解，特别是在EoS现象中NTK特征向量的行为。

Abstract: The study of Neural Tangent Kernels (NTKs) in deep learning has drawn
increasing attention in recent years. NTKs typically actively change during
training and are related to feature learning. In parallel, recent work on
Gradient Descent (GD) has found a phenomenon called Edge of Stability (EoS), in
which the largest eigenvalue of the NTK oscillates around a value inversely
proportional to the step size. However, although follow-up works have explored
the underlying mechanism of such eigenvalue behavior in depth, the
understanding of the behavior of the NTK eigenvectors during EoS is still
missing. This paper examines the dynamics of NTK eigenvectors during EoS in
detail. Across different architectures, we observe that larger learning rates
cause the leading eigenvectors of the final NTK, as well as the full NTK
matrix, to have greater alignment with the training target. We then study the
underlying mechanism of this phenomenon and provide a theoretical analysis for
a two-layer linear network. Our study enhances the understanding of GD training
dynamics in deep learning.

</details>


### [38] [Transformer-Based Person Identification via Wi-Fi CSI Amplitude and Phase Perturbations](https://arxiv.org/abs/2507.12854)
*Danilo Avola,Andrea Bernardini,Francesco Danese,Mario Lezoche,Maurizio Mancini,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于Transformer的方法，利用Wi-Fi信号中的CSI（信道状态信息）在用户静止状态下进行身份识别，准确率达99.82%。


<details>
  <summary>Details</summary>
Motivation: 探索无线信号在用户静止状态下进行身份识别的潜力，填补现有研究依赖运动模式的空白。

Method: 采用双分支Transformer架构，分别处理CSI的幅度和相位信息，并通过预处理（异常值去除、平滑、相位校准）提升信号质量。

Result: 在控制环境中，使用ESP32设备采集的数据集上达到99.82%的分类准确率，优于卷积和多层感知机基线。

Conclusion: 研究表明CSI扰动具有区分性，能够编码生物特征，验证了低成本Wi-Fi硬件在无源、无设备身份识别中的可行性。

Abstract: Wi-Fi sensing is gaining momentum as a non-intrusive and privacy-preserving
alternative to vision-based systems for human identification. However, person
identification through wireless signals, particularly without user motion,
remains largely unexplored. Most prior wireless-based approaches rely on
movement patterns, such as walking gait, to extract biometric cues. In
contrast, we propose a transformer-based method that identifies individuals
from Channel State Information (CSI) recorded while the subject remains
stationary. CSI captures fine-grained amplitude and phase distortions induced
by the unique interaction between the human body and the radio signal. To
support evaluation, we introduce a dataset acquired with ESP32 devices in a
controlled indoor environment, featuring six participants observed across
multiple orientations. A tailored preprocessing pipeline, including outlier
removal, smoothing, and phase calibration, enhances signal quality. Our
dual-branch transformer architecture processes amplitude and phase modalities
separately and achieves 99.82\% classification accuracy, outperforming
convolutional and multilayer perceptron baselines. These results demonstrate
the discriminative potential of CSI perturbations, highlighting their capacity
to encode biometric traits in a consistent manner. They further confirm the
viability of passive, device-free person identification using low-cost
commodity Wi-Fi hardware in real-world settings.

</details>


### [39] [Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)](https://arxiv.org/abs/2507.12856)
*Chongli Qin,Jost Tobias Springenberg*

Main category: cs.LG

TL;DR: 论文提出了一种改进的行为克隆方法（iw-SFT），通过重要性加权优化RL目标，性能优于传统SFT，适用于语言模型和连续控制任务。


<details>
  <summary>Details</summary>
Motivation: 传统的行为克隆（BC）和SFT在稀疏奖励设置中表现良好，但可以进一步优化以更接近RL目标。

Method: 提出重要性加权SFT（iw-SFT），优化更紧的RL目标下界，并支持质量评分数据训练。

Result: iw-SFT在语言模型和连续控制任务中表现优异，例如在AIME 2024数据集上达到66.7%。

Conclusion: iw-SFT是一种简单有效的改进方法，性能接近高级RL算法。

Abstract: Behavior Cloning (BC) on curated (or filtered) data is the predominant
paradigm for supervised fine-tuning (SFT) of large language models; as well as
for imitation learning of control policies. Here, we draw on a connection
between this successful strategy and the theory and practice of finding optimal
policies via Reinforcement Learning (RL). Building on existing literature, we
clarify that SFT can be understood as maximizing a lower bound on the RL
objective in a sparse reward setting. Giving support to its often observed good
performance. From this viewpoint, we realize that a small modification to SFT
leads to an importance weighted variant that behaves closer to training with RL
as it: i) optimizes a tighter bound to the RL objective and, ii) can improve
performance compared to SFT on curated data. We refer to this variant as
importance weighted supervised fine-tuning (iw-SFT). We show that it is easy to
implement and can be further generalized to training with quality scored data.
The resulting SFT variants are competitive with more advanced RL algorithms for
large language models and for training policies in continuous control tasks.
For example achieving 66.7% on the AIME 2024 dataset.

</details>


### [40] [An Investigation of Ear-EEG Signals for a Novel Biometric Authentication System](https://arxiv.org/abs/2507.12873)
*Danilo Avola,Giancarlo Crocetti,Gian Luca Foresti,Daniele Pannone,Claudio Piciarelli,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 研究探讨了利用耳内设备采集的EEG信号进行生物识别的可行性，提出了一种基于耳-EEG的用户友好型生物认证框架，实验结果显示平均准确率达82%。


<details>
  <summary>Details</summary>
Motivation: 传统基于头皮EEG的生物识别系统因设备笨重而用户体验差，耳-EEG提供了一种更实用的替代方案。

Method: 从耳-EEG信号中提取时间和频谱特征，并通过全连接深度神经网络进行主体识别。

Result: 实验结果表明，耳-EEG在主体识别场景中平均准确率为82%。

Conclusion: 耳-EEG具有成为下一代现实世界生物识别系统的潜力。

Abstract: This work explores the feasibility of biometric authentication using EEG
signals acquired through in-ear devices, commonly referred to as ear-EEG.
Traditional EEG-based biometric systems, while secure, often suffer from low
usability due to cumbersome scalp-based electrode setups. In this study, we
propose a novel and practical framework leveraging ear-EEG signals as a
user-friendly alternative for everyday biometric authentication. The system
extracts an original combination of temporal and spectral features from ear-EEG
signals and feeds them into a fully connected deep neural network for subject
identification. Experimental results on the only currently available ear-EEG
dataset suitable for different purposes, including biometric authentication,
demonstrate promising performance, with an average accuracy of 82\% in a
subject identification scenario. These findings confirm the potential of
ear-EEG as a viable and deployable direction for next-generation real-world
biometric systems.

</details>


### [41] [Topology-Aware Activation Functions in Neural Networks](https://arxiv.org/abs/2507.12874)
*Pavel Snopov,Oleg R. Musin*

Main category: cs.LG

TL;DR: 论文提出两种新型激活函数SmoothSplit和ParametricSplit，通过引入拓扑“切割”能力提升神经网络处理数据拓扑的能力。


<details>
  <summary>Details</summary>
Motivation: 传统激活函数（如ReLU）在数据拓扑处理上存在局限性，无法有效处理复杂数据流形。

Method: 提出SmoothSplit和ParametricSplit两种激活函数，具有拓扑切割能力，适用于低维数据场景。

Result: 实验表明，ParametricSplit在低维数据中表现优于传统激活函数，同时在高维数据中保持竞争力。

Conclusion: 拓扑感知激活函数在神经网络架构中具有潜在优势，代码已开源。

Abstract: This study explores novel activation functions that enhance the ability of
neural networks to manipulate data topology during training. Building on the
limitations of traditional activation functions like $\mathrm{ReLU}$, we
propose $\mathrm{SmoothSplit}$ and $\mathrm{ParametricSplit}$, which introduce
topology "cutting" capabilities. These functions enable networks to transform
complex data manifolds effectively, improving performance in scenarios with
low-dimensional layers. Through experiments on synthetic and real-world
datasets, we demonstrate that $\mathrm{ParametricSplit}$ outperforms
traditional activations in low-dimensional settings while maintaining
competitive performance in higher-dimensional ones. Our findings highlight the
potential of topology-aware activation functions in advancing neural network
architectures. The code is available via
https://github.com/Snopoff/Topology-Aware-Activations.

</details>


### [42] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng,Hengkai Tan,Xinyi Mao,Guodong Liu,Shuhe Huang,Chendong Xiang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: VIDAR是一个两阶段框架，通过扩散模型预训练和掩码逆向动力学模型提升双手机器人操作的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作面临数据稀缺和异构性问题，需要更高效的泛化方法。

Method: 结合大规模视频扩散预训练和掩码逆向动力学模型，提取动作相关信息。

Result: 仅需20分钟人类演示，VIDAR在未见过的任务和背景中表现优异。

Conclusion: 视频基础模型与掩码动作预测结合，有望实现多样化机器人操作的规模化应用。

Abstract: Bimanual robotic manipulation, which involves the coordinated control of two
robotic arms, is foundational for solving challenging tasks. Despite recent
progress in general-purpose manipulation, data scarcity and embodiment
heterogeneity remain serious obstacles to further scaling up in bimanual
settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning
(VIDAR), a two-stage framework that leverages large-scale, diffusion-based
video pre-training and a novel masked inverse dynamics model for action
prediction. We pre-train the video diffusion model on 750K multi-view videos
from three real-world bimanual robot platforms, utilizing a unified observation
space that encodes robot, camera, task, and scene contexts. Our masked inverse
dynamics model learns masks to extract action-relevant information from
generated trajectories without requiring pixel-level labels, and the masks can
effectively generalize to unseen backgrounds. Our experiments demonstrate that
with only 20 minutes of human demonstrations on an unseen robot platform (only
1% of typical data requirements), VIDAR generalizes to unseen tasks and
backgrounds with strong semantic understanding, surpassing state-of-the-art
methods. Our findings highlight the potential of video foundation models,
coupled with masked action prediction, to enable scalable and generalizable
robotic manipulation in diverse real-world settings.

</details>


### [43] [Learning to Reject Low-Quality Explanations via User Feedback](https://arxiv.org/abs/2507.12900)
*Luca Stradiotti,Dario Pesenti,Stefano Teso,Jesse Davis*

Main category: cs.LG

TL;DR: 论文提出了一种框架（LtX），让分类器可以拒绝解释质量低的输入，并介绍了ULER方法，通过学习人类评分来评估解释质量。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，机器学习预测的解释质量影响用户信任和决策，但现有解释可能难以理解或不可信。

Method: 引入ULER（用户为中心的低质量解释拒绝器），通过学习人类评分和特征相关性判断来评估解释质量。

Result: ULER在八个分类和回归基准及新标注数据集上优于现有方法。

Conclusion: ULER能有效识别低质量解释，提升用户信任和决策质量。

Abstract: Machine Learning predictors are increasingly being employed in high-stakes
applications such as credit scoring. Explanations help users unpack the reasons
behind their predictions, but are not always "high quality''. That is,
end-users may have difficulty interpreting or believing them, which can
complicate trust assessment and downstream decision-making. We argue that
classifiers should have the option to refuse handling inputs whose predictions
cannot be explained properly and introduce a framework for learning to reject
low-quality explanations (LtX) in which predictors are equipped with a rejector
that evaluates the quality of explanations. In this problem setting, the key
challenges are how to properly define and assess explanation quality and how to
design a suitable rejector. Focusing on popular attribution techniques, we
introduce ULER (User-centric Low-quality Explanation Rejector), which learns a
simple rejector from human ratings and per-feature relevance judgments to
mirror human judgments of explanation quality. Our experiments show that ULER
outperforms both state-of-the-art and explanation-aware learning to reject
strategies at LtX on eight classification and regression benchmarks and on a
new human-annotated dataset, which we will publicly release to support future
research.

</details>


### [44] [Fremer: Lightweight and Effective Frequency Transformer for Workload Forecasting in Cloud Services](https://arxiv.org/abs/2507.12908)
*Jiadong Chen,Hengyu Ye,Fuxin Jiang,Xiao He,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: Fremer是一种高效深度学习模型，用于云服务中的工作负载预测，优于现有Transformer模型，并在准确性和效率上表现卓越。


<details>
  <summary>Details</summary>
Motivation: 云服务中的工作负载预测对自动扩展和调度至关重要，但现有Transformer模型计算效率不足，需在频域解决复杂周期性模式。

Method: 提出Fremer模型，专注于频域处理，满足高效、高精度和鲁棒性需求。

Result: Fremer在多个指标上优于SOTA模型（MSE提升5.5%，MAE提升4.7%，SMAPE提升8.6%），并降低计算成本。

Conclusion: Fremer在实际应用中显著提升性能（延迟降低18.78%，资源消耗减少2.35%），验证了其有效性。

Abstract: Workload forecasting is pivotal in cloud service applications, such as
auto-scaling and scheduling, with profound implications for operational
efficiency. Although Transformer-based forecasting models have demonstrated
remarkable success in general tasks, their computational efficiency often falls
short of the stringent requirements in large-scale cloud environments. Given
that most workload series exhibit complicated periodic patterns, addressing
these challenges in the frequency domain offers substantial advantages. To this
end, we propose Fremer, an efficient and effective deep forecasting model.
Fremer fulfills three critical requirements: it demonstrates superior
efficiency, outperforming most Transformer-based forecasting models; it
achieves exceptional accuracy, surpassing all state-of-the-art (SOTA) models in
workload forecasting; and it exhibits robust performance for multi-period
series. Furthermore, we collect and open-source four high-quality, open-source
workload datasets derived from ByteDance's cloud services, encompassing
workload data from thousands of computing instances. Extensive experiments on
both our proprietary datasets and public benchmarks demonstrate that Fremer
consistently outperforms baseline models, achieving average improvements of
5.5% in MSE, 4.7% in MAE, and 8.6% in SMAPE over SOTA models, while
simultaneously reducing parameter scale and computational costs. Additionally,
in a proactive auto-scaling test based on Kubernetes, Fremer improves average
latency by 18.78% and reduces resource consumption by 2.35%, underscoring its
practical efficacy in real-world applications.

</details>


### [45] [Robust Explanations Through Uncertainty Decomposition: A Path to Trustworthier AI](https://arxiv.org/abs/2507.12913)
*Chenrui Zhu,Louenas Bounia,Vu Linh Nguyen,Sébastien Destercke,Arthur Hoarau*

Main category: cs.LG

TL;DR: 论文提出利用预测不确定性作为传统可解释性方法的补充，通过区分数据相关和模型相关的不确定性来指导解释选择。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型复杂度的增加，可解释性降低，需要透明化模型预测。

Method: 区分数据相关（aleatoric）和模型相关（epistemic）不确定性，指导解释选择，如拒绝不可靠解释或选择特征重要性解释。

Result: 实验表明，这种不确定性感知方法提高了传统机器学习和深度学习场景中解释的鲁棒性和可达性。

Conclusion: 不确定性量化与解耦驱动的框架为模型解释提供了新的视角和实用性。

Abstract: Recent advancements in machine learning have emphasized the need for
transparency in model predictions, particularly as interpretability diminishes
when using increasingly complex architectures. In this paper, we propose
leveraging prediction uncertainty as a complementary approach to classical
explainability methods. Specifically, we distinguish between aleatoric
(data-related) and epistemic (model-related) uncertainty to guide the selection
of appropriate explanations. Epistemic uncertainty serves as a rejection
criterion for unreliable explanations and, in itself, provides insight into
insufficient training (a new form of explanation). Aleatoric uncertainty
informs the choice between feature-importance explanations and counterfactual
explanations. This leverages a framework of explainability methods driven by
uncertainty quantification and disentanglement. Our experiments demonstrate the
impact of this uncertainty-aware approach on the robustness and attainability
of explanations in both traditional machine learning and deep learning
scenarios.

</details>


### [46] [Trace Reconstruction with Language Models](https://arxiv.org/abs/2507.12927)
*Franziska Weindel,Michael Girsch,Reinhard Heckel*

Main category: cs.LG

TL;DR: TReconLM利用语言模型改进痕迹重建，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储中的错误需要高效算法校正，痕迹重建是关键步骤。

Method: 预训练语言模型于合成数据，并在真实数据上微调以适应技术特定错误模式。

Result: TReconLM显著优于现有方法，恢复更高比例的无错误序列。

Conclusion: TReconLM为痕迹重建提供了高效解决方案，适用于DNA数据存储等应用。

Abstract: The general trace reconstruction problem seeks to recover an original
sequence from its noisy copies independently corrupted by deletions,
insertions, and substitutions. This problem arises in applications such as DNA
data storage, a promising storage medium due to its high information density
and longevity. However, errors introduced during DNA synthesis, storage, and
sequencing require correction through algorithms and codes, with trace
reconstruction often used as part of the data retrieval process. In this work,
we propose TReconLM, which leverages language models trained on next-token
prediction for trace reconstruction. We pretrain language models on synthetic
data and fine-tune on real-world data to adapt to technology-specific error
patterns. TReconLM outperforms state-of-the-art trace reconstruction
algorithms, including prior deep learning approaches, recovering a
substantially higher fraction of sequences without error.

</details>


### [47] [From a Mixed-Policy Perspective: Improving Differentiable Automatic Post-editing Optimization](https://arxiv.org/abs/2507.12931)
*Hongze Tan*

Main category: cs.LG

TL;DR: 本文提出了两种改进DAPO算法的方法，通过混合策略视角提升训练稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 标准策略梯度方法在稀疏奖励环境下存在不稳定和样本效率低的问题。

Method: 1. 引入预训练的稳定引导策略提供离策略经验；2. 重新利用零奖励样本，作为专家策略引导的独立批次。

Result: 理论分析表明，目标函数在强化学习框架内收敛到最优解。

Conclusion: 混合策略框架有效平衡探索与利用，提升了策略优化的稳定性和效率。

Abstract: This paper introduces two novel modifications to the Differentiable Automatic
Post-editing Optimization (DAPO) algorithm, approached from a mixed-policy
perspective. Standard policy gradient methods can suffer from instability and
sample inefficiency, particularly in sparse reward settings. To address this,
we first propose a method that incorporates a pre-trained, stable guiding
policy ($\piphi$) to provide off-policy experience, thereby regularizing the
training of the target policy ($\pion$). This approach improves training
stability and convergence speed by adaptively adjusting the learning step size.
Secondly, we extend this idea to re-utilize zero-reward samples, which are
often discarded by dynamic sampling strategies like DAPO's. By treating these
samples as a distinct batch guided by the expert policy, we further enhance
sample efficiency. We provide a theoretical analysis for both methods,
demonstrating that their objective functions converge to the optimal solution
within the established theoretical framework of reinforcement learning. The
proposed mixed-policy framework effectively balances exploration and
exploitation, promising more stable and efficient policy optimization.

</details>


### [48] [MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration](https://arxiv.org/abs/2507.12935)
*Shirui Zhao,Jun Yin,Lingyun Yao,Martin Andraud,Wannes Meert,Marian Verhelst*

Main category: cs.LG

TL;DR: MC²A是一种算法-硬件协同设计框架，用于高效且灵活地加速MCMC算法，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: MCMC算法计算成本高，现有加速方案在硬件灵活性或系统效率上不足，限制了其在大规模问题中的应用。

Method: MC²A通过扩展处理器性能模型、设计参数化硬件加速器架构和新型Gumbel采样器，优化MCMC加速。

Result: 在端到端案例中，MC²A相比CPU、GPU、TPU和现有MCMC加速器，分别实现了307.6倍、1.4倍、2.0倍和84.2倍的加速。

Conclusion: MC²A展示了通用硬件加速的可行性，为MCMC解决方案在多样化应用中的普及提供了支持。

Abstract: An increasing number of applications are exploiting sampling-based algorithms
for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)
algorithms form the computational backbone of this emerging branch of machine
learning. Unfortunately, the high computational cost limits their feasibility
for large-scale problems and real-world applications, and the existing MCMC
acceleration solutions are either limited in hardware flexibility or fail to
maintain efficiency at the system level across a variety of end-to-end
applications. This paper introduces \textbf{MC$^2$A}, an algorithm-hardware
co-design framework, enabling efficient and flexible optimization for MCMC
acceleration. Firstly, \textbf{MC$^2$A} analyzes the MCMC workload diversity
through an extension of the processor performance roofline model with a 3rd
dimension to derive the optimal balance between the compute, sampling and
memory parameters. Secondly, \textbf{MC$^2$A} proposes a parametrized hardware
accelerator architecture with flexible and efficient support of MCMC kernels
with a pipeline of ISA-programmable tree-structured processing units,
reconfigurable samplers and a crossbar interconnect to support irregular
access. Thirdly, the core of \textbf{MC$^2$A} is powered by a novel Gumbel
sampler that eliminates exponential and normalization operations. In the
end-to-end case study, \textbf{MC$^2$A} achieves an overall {$307.6\times$,
$1.4\times$, $2.0\times$, $84.2\times$} speedup compared to the CPU, GPU, TPU
and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC
workloads, this work demonstrates and exploits the feasibility of general
hardware acceleration to popularize MCMC-based solutions in diverse application
domains.

</details>


### [49] [Probabilistic Soundness Guarantees in LLM Reasoning Chains](https://arxiv.org/abs/2507.12948)
*Weiqiu You,Anton Xue,Shreya Havaldar,Delip Rao,Helen Jin,Chris Callison-Burch,Eric Wong*

Main category: cs.LG

TL;DR: 论文提出ARES框架，通过概率方法检测推理链中的传播错误，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM错误检测方法未能有效处理早期错误对下游推理的负面影响。

Method: 引入ARES框架，基于已评估的可靠前提逐步判断每个主张。

Result: 在四个基准测试中表现最佳（72.1% Macro-F1，提升8.2分），在长推理链中尤其稳健（90.3% F1，提升27.6分）。

Conclusion: ARES能有效检测传播错误并提供统计保证，优于现有方法。

Abstract: In reasoning chains generated by large language models (LLMs), initial errors
often propagate and undermine the reliability of the final conclusion. Current
LLM-based error detection methods often fail to detect propagated errors
because they do not properly account for how earlier errors might corrupt
judgments of downstream reasoning. To better detect such propagated errors, we
introduce Autoregressive Reasoning Entailment Stability (ARES), a novel
probabilistic framework that prevents error propagation by judging each claim
based only on previously-assessed sound premises. This inductive method yields
a nuanced score for each step and provides certified statistical guarantees of
its soundness, rather than a brittle binary label. ARES achieves
state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2
points) and demonstrates superior robustness on very long synthetic reasoning
chains, where it excels at detecting propagated errors (90.3% F1, +27.6
points).

</details>


### [50] [Insights into a radiology-specialised multimodal large language model with sparse autoencoders](https://arxiv.org/abs/2507.12950)
*Kenza Bouzid,Shruthi Bannur,Daniel Coelho de Castro,Anton Schwaighofer,Javier Alvarez-Valle,Stephanie L. Hyland*

Main category: cs.LG

TL;DR: 研究通过稀疏自编码器（SAE）解释多模态大语言模型MAIRA-2的内部表示，识别出临床相关概念，并探索其对模型行为的影响。


<details>
  <summary>Details</summary>
Motivation: 提高AI模型在医疗应用中的安全性、透明度和信任度，尤其是在决策具有重大后果的场景。

Method: 使用Matryoshka-SAE分析MAIRA-2的内部表示，并通过大规模自动解释识别临床相关特征。

Result: 识别出医疗设备、病理特征等临床相关概念，并通过“steering”方法部分实现对模型行为的控制。

Conclusion: 研究揭示了MAIRA-2内部学习的概念，为提升模型透明度和可解释性提供了初步见解。

Abstract: Interpretability can improve the safety, transparency and trust of AI models,
which is especially important in healthcare applications where decisions often
carry significant consequences. Mechanistic interpretability, particularly
through the use of sparse autoencoders (SAEs), offers a promising approach for
uncovering human-interpretable features within large transformer-based models.
In this study, we apply Matryoshka-SAE to the radiology-specialised multimodal
large language model, MAIRA-2, to interpret its internal representations. Using
large-scale automated interpretability of the SAE features, we identify a range
of clinically relevant concepts - including medical devices (e.g., line and
tube placements, pacemaker presence), pathologies such as pleural effusion and
cardiomegaly, longitudinal changes and textual features. We further examine the
influence of these features on model behaviour through steering, demonstrating
directional control over generations with mixed success. Our results reveal
practical and methodological challenges, yet they offer initial insights into
the internal concepts learned by MAIRA-2 - marking a step toward deeper
mechanistic understanding and interpretability of a radiology-adapted
multimodal large language model, and paving the way for improved model
transparency. We release the trained SAEs and interpretations:
https://huggingface.co/microsoft/maira-2-sae.

</details>


### [51] [A Spectral Interpretation of Redundancy in a Graph Reservoir](https://arxiv.org/abs/2507.12963)
*Anna Bison,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 本文提出了一种基于Fairing算法的MRGNN变体，用于解决图神经网络中的过平滑问题，并通过理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络（GNNs）在多层操作中常出现过平滑问题，即图信号收敛于图拉普拉斯算子的低频分量。本文旨在通过改进MRGNN中的储层定义，提出一种新的方法来解决这一问题。

Method: 提出了一种基于Fairing算法的MRGNN变体，该算法提供了一种带通谱滤波器，能够在图拉普拉斯算子下实现无收缩的平滑。

Result: 理论分析表明，通过调整谱系数可以调节冗余随机游走的贡献。实验验证了该方法在图分类任务中的潜力。

Conclusion: 该方法为图神经网络中的平滑问题提供了新的解决方案，并展示了未来研究的可能方向。

Abstract: Reservoir computing has been successfully applied to graphs as a
preprocessing method to improve the training efficiency of Graph Neural
Networks (GNNs). However, a common issue that arises when repeatedly applying
layer operators on graphs is over-smoothing, which consists in the convergence
of graph signals toward low-frequency components of the graph Laplacian. This
work revisits the definition of the reservoir in the Multiresolution Reservoir
Graph Neural Network (MRGNN), a spectral reservoir model, and proposes a
variant based on a Fairing algorithm originally introduced in the field of
surface design in computer graphics. This algorithm provides a pass-band
spectral filter that allows smoothing without shrinkage, and it can be adapted
to the graph setting through the Laplacian operator. Given its spectral
formulation, this method naturally connects to GNN architectures for tasks
where smoothing, when properly controlled, can be beneficial,such as graph
classification. The core contribution of the paper lies in the theoretical
analysis of the algorithm from a random walks perspective. In particular, it
shows how tuning the spectral coefficients can be interpreted as modulating the
contribution of redundant random walks. Exploratory experiments based on the
MRGNN architecture illustrate the potential of this approach and suggest
promising directions for future research.

</details>


### [52] [WaveletInception Networks for Drive-by Vibration-Based Infrastructure Health Monitoring](https://arxiv.org/abs/2507.12969)
*Reza Riahi Samani,Alfredo Nunez,Bart De Schutter*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的框架WaveletInception-BiLSTM，用于通过振动信号监测基础设施健康状态，结合小波变换和LSTM提取多尺度特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基础设施健康监测需要高效、自动化的方法，传统方法依赖预处理且性能有限。

Method: 使用WaveletInception-BiLSTM网络，结合LWPT提取频谱特征和Inception网络提取多尺度特征，再用BiLSTM建模时间依赖关系。

Result: 在铁路轨道刚度估计案例中，模型显著优于现有方法，实现了高分辨率、自动化的健康评估。

Conclusion: 该方法为基础设施健康监测提供了准确、局部化和自动化的解决方案。

Abstract: This paper presents a novel deep learning-based framework for infrastructure
health monitoring using drive-by vibration response signals. Recognizing the
importance of spectral and temporal information, we introduce the
WaveletInception-BiLSTM network. The WaveletInception feature extractor
utilizes a Learnable Wavelet Packet Transform (LWPT) as the stem for extracting
vibration signal features, incorporating spectral information in the early
network layers. This is followed by 1D Inception networks that extract
multi-scale, high-level features at deeper layers. The extracted vibration
signal features are then integrated with operational conditions via a Long
Short-term Memory (LSTM) layer. The resulting feature extraction network
effectively analyzes drive-by vibration signals across various measurement
speeds without preprocessing and uses LSTM to capture interrelated temporal
dependencies among different modes of information and to create feature vectors
for health condition estimation. The estimator head is designed with a
sequential modeling architecture using bidirectional LSTM (BiLSTM) networks,
capturing bi-directional temporal relationships from drive-by measurements.
This architecture allows for a high-resolution, beam-level assessment of
infrastructure health conditions. A case study focusing on railway track
stiffness estimation with simulated drive-by vibration signals shows that the
model significantly outperforms state-of-the-art methods in estimating railway
ballast and railpad stiffness parameters. Results underscore the potential of
this approach for accurate, localized, and fully automated drive-by
infrastructure health monitoring.

</details>


### [53] [A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints](https://arxiv.org/abs/2507.12979)
*Youssef Tawfilis,Hossam Amer,Minar El-Aasser,Tallal Elshabrawy*

Main category: cs.LG

TL;DR: 提出了一种去中心化的GAN训练方法，利用分布式数据和低能力设备，同时不共享原始数据。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型训练中数据隐私和资源不足的问题。

Method: 结合KLD加权的聚类联邦学习和异构U型分割学习，处理数据异构和设备异构问题。

Result: 在图像生成和分类指标上表现优异，生成分数提升1.1-2.2倍，分类指标平均提升10%。

Conclusion: 该方法在去中心化环境中高效且隐私友好。

Abstract: Federated Learning has gained increasing attention for its ability to enable
multiple nodes to collaboratively train machine learning models without sharing
their raw data. At the same time, Generative AI -- particularly Generative
Adversarial Networks (GANs) -- have achieved remarkable success across a wide
range of domains, such as healthcare, security, and Image Generation. However,
training generative models typically requires large datasets and significant
computational resources, which are often unavailable in real-world settings.
Acquiring such resources can be costly and inefficient, especially when many
underutilized devices -- such as IoT devices and edge devices -- with varying
capabilities remain idle. Moreover, obtaining large datasets is challenging due
to privacy concerns and copyright restrictions, as most devices are unwilling
to share their data. To address these challenges, we propose a novel approach
for decentralized GAN training that enables the utilization of distributed data
and underutilized, low-capability devices while not sharing data in its raw
form. Our approach is designed to tackle key challenges in decentralized
environments, combining KLD-weighted Clustered Federated Learning to address
the issues of data heterogeneity and multi-domain datasets, with Heterogeneous
U-Shaped split learning to tackle the challenge of device heterogeneity under
strict data sharing constraints -- ensuring that no labels or raw data, whether
real or synthetic, are ever shared between nodes. Experimental results shows
that our approach demonstrates consistent and significant improvements across
key performance metrics, where it achieves 1.1x -- 2.2x higher image generation
scores, an average 10% boost in classification metrics (up to 50% in
multi-domain non-IID settings), in much lower latency compared to several
benchmarks. Find our code at https://github.com/youssefga28/HuSCF-GAN.

</details>


### [54] [FedGA: A Fair Federated Learning Framework Based on the Gini Coefficient](https://arxiv.org/abs/2507.12983)
*ShanBin Liu*

Main category: cs.LG

TL;DR: FedGA是一种公平感知的联邦学习算法，通过动态调整聚合权重和公平干预时机，减少客户端间的性能差异。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异构性导致客户端性能差异显著，引发公平性问题。

Method: 使用基尼系数衡量性能差异，建立其与全局模型更新规模的关系，动态调整聚合权重。

Result: 在多个数据集上验证，FedGA显著提升了公平性指标（如方差和基尼系数），同时保持整体性能。

Conclusion: FedGA有效解决了联邦学习中的公平性问题，具有实际应用价值。

Abstract: Fairness has emerged as one of the key challenges in federated learning. In
horizontal federated settings, data heterogeneity often leads to substantial
performance disparities across clients, raising concerns about equitable model
behavior. To address this issue, we propose FedGA, a fairness-aware federated
learning algorithm. We first employ the Gini coefficient to measure the
performance disparity among clients. Based on this, we establish a relationship
between the Gini coefficient $G$ and the update scale of the global model
${U_s}$, and use this relationship to adaptively determine the timing of
fairness intervention. Subsequently, we dynamically adjust the aggregation
weights according to the system's real-time fairness status, enabling the
global model to better incorporate information from clients with relatively
poor performance.We conduct extensive experiments on the Office-Caltech-10,
CIFAR-10, and Synthetic datasets. The results show that FedGA effectively
improves fairness metrics such as variance and the Gini coefficient, while
maintaining strong overall performance, demonstrating the effectiveness of our
approach.

</details>


### [55] [Teach Old SAEs New Domain Tricks with Boosting](https://arxiv.org/abs/2507.12990)
*Nikita Koriagin,Yaroslav Aksenov,Daniil Laptev,Gleb Gerasimov,Nikita Balagansky,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 提出一种残差学习方法，通过训练次级稀疏自编码器（SAE）捕捉主SAE在特定领域文本中的重构误差，从而提升模型在专业领域的表现。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏自编码器在捕捉特定领域特征时的不足，避免完全重新训练。

Method: 训练次级SAE建模主SAE在特定领域文本上的重构误差，推理时结合两者输出。

Result: 在多个专业领域中显著提升LLM的交叉熵和解释方差指标。

Conclusion: 该方法高效地将新领域知识融入现有SAE，同时保持通用任务性能，为LLM的针对性机制解释提供了新可能。

Abstract: Sparse Autoencoders have emerged as powerful tools for interpreting the
internal representations of Large Language Models, yet they often fail to
capture domain-specific features not prevalent in their training corpora. This
paper introduces a residual learning approach that addresses this feature
blindness without requiring complete retraining. We propose training a
secondary SAE specifically to model the reconstruction error of a pretrained
SAE on domain-specific texts, effectively capturing features missed by the
primary model. By summing the outputs of both models during inference, we
demonstrate significant improvements in both LLM cross-entropy and explained
variance metrics across multiple specialized domains. Our experiments show that
this method efficiently incorporates new domain knowledge into existing SAEs
while maintaining their performance on general tasks. This approach enables
researchers to selectively enhance SAE interpretability for specific domains of
interest, opening new possibilities for targeted mechanistic interpretability
of LLMs.

</details>


### [56] [SMART: Relation-Aware Learning of Geometric Representations for Knowledge Graphs](https://arxiv.org/abs/2507.13001)
*Kossi Amouzouvi,Bowen Song,Andrea Coletta,Luigi Bellomarini,Jens Lehmann,Sahar Vahdati*

Main category: cs.LG

TL;DR: 该论文提出了一种框架，通过评估每种关系与不同几何变换的匹配程度，为知识图谱中的关系分配最佳变换或通过多数投票选择一种变换类型。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入模型在几何变换的使用上不足，未能充分考虑关系特定的变换需求。

Method: 提出一个框架，通过排名评估关系与几何变换的匹配度，并利用注意力机制学习低维空间中的关系特定变换。

Result: 在三个基准知识图谱和一个真实金融知识图谱上验证了模型的有效性，性能与领先模型相当。

Conclusion: 该框架能够更灵活地为知识图谱中的关系分配几何变换，提升嵌入模型的性能。

Abstract: Knowledge graph representation learning approaches provide a mapping between
symbolic knowledge in the form of triples in a knowledge graph (KG) and their
feature vectors. Knowledge graph embedding (KGE) models often represent
relations in a KG as geometric transformations. Most state-of-the-art (SOTA)
KGE models are derived from elementary geometric transformations (EGTs), such
as translation, scaling, rotation, and reflection, or their combinations. These
geometric transformations enable the models to effectively preserve specific
structural and relational patterns of the KG. However, the current use of EGTs
by KGEs remains insufficient without considering relation-specific
transformations. Although recent models attempted to address this problem by
ensembling SOTA baseline models in different ways, only a single or composite
version of geometric transformations are used by such baselines to represent
all the relations. In this paper, we propose a framework that evaluates how
well each relation fits with different geometric transformations. Based on this
ranking, the model can: (1) assign the best-matching transformation to each
relation, or (2) use majority voting to choose one transformation type to apply
across all relations. That is, the model learns a single relation-specific EGT
in low dimensional vector space through an attention mechanism. Furthermore, we
use the correlation between relations and EGTs, which are learned in a low
dimension, for relation embeddings in a high dimensional vector space. The
effectiveness of our models is demonstrated through comprehensive evaluations
on three benchmark KGs as well as a real-world financial KG, witnessing a
performance comparable to leading models

</details>


### [57] [Fault detection and diagnosis for the engine electrical system of a space launcher based on a temporal convolutional autoencoder and calibrated classifiers](https://arxiv.org/abs/2507.13022)
*Luis Basora,Louison Bocquet-Nouaille,Elinirina Robinson,Serge Le Gonidec*

Main category: cs.LG

TL;DR: 提出了一种基于时序卷积自编码器的故障检测与诊断框架，用于下一代可重复使用太空发射器的电气系统健康监测。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能满足广泛需求，如预测置信度估计、异常数据检测和误报控制。

Method: 使用时序卷积自编码器提取特征，结合二元和多类分类器进行故障检测与诊断，并采用简单技术处理异常数据和误报。

Result: 在模拟数据上表现良好，但需真实数据验证。

Conclusion: 该框架是迈向成熟解决方案的第一步，但需进一步测试。

Abstract: In the context of the health monitoring for the next generation of reusable
space launchers, we outline a first step toward developing an onboard fault
detection and diagnostic capability for the electrical system that controls the
engine valves. Unlike existing approaches in the literature, our solution is
designed to meet a broader range of key requirements. This includes estimating
confidence levels for predictions, detecting out-of-distribution (OOD) cases,
and controlling false alarms. The proposed solution is based on a temporal
convolutional autoencoder to automatically extract low-dimensional features
from raw sensor data. Fault detection and diagnosis are respectively carried
out using a binary and a multiclass classifier trained on the autoencoder
latent and residual spaces. The classifiers are histogram-based gradient
boosting models calibrated to output probabilities that can be interpreted as
confidence levels. A relatively simple technique, based on inductive conformal
anomaly detection, is used to identify OOD data. We leverage other simple yet
effective techniques, such as cumulative sum control chart (CUSUM) to limit the
false alarms, and threshold moving to address class imbalance in fault
detection. The proposed framework is highly configurable and has been evaluated
on simulated data, covering both nominal and anomalous operational scenarios.
The results indicate that our solution is a promising first step, though
testing with real data will be necessary to ensure that it achieves the
required maturity level for operational use.

</details>


### [58] [Confidence-Filtered Relevance (CFR): An Interpretable and Uncertainty-Aware Machine Learning Framework for Naturalness Assessment in Satellite Imagery](https://arxiv.org/abs/2507.13034)
*Ahmed Emam,Ribana Roscher*

Main category: cs.LG

TL;DR: 提出了一种名为CFR的数据中心框架，结合LRP Attention Rollout和DDU估计，分析模型不确定性对卫星图像自然性评估解释性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法在卫星图像监测中缺乏可解释性和不确定性意识，且未探讨不确定性如何影响自然性评估。

Method: CFR框架通过不确定性阈值划分数据集，系统分析不确定性对自然性解释的影响。

Result: CFR在AnthroProtect数据集上对灌木丛、森林和湿地赋予更高相关性，且不确定性增加时解释性下降、熵增。

Conclusion: CFR提供了一种基于确定性的数据中心方法，评估卫星图像中模式对自然性的相关性。

Abstract: Protected natural areas play a vital role in ecological balance and ecosystem
services. Monitoring these regions at scale using satellite imagery and machine
learning is promising, but current methods often lack interpretability and
uncertainty-awareness, and do not address how uncertainty affects naturalness
assessment. In contrast, we propose Confidence-Filtered Relevance (CFR), a
data-centric framework that combines LRP Attention Rollout with Deep
Deterministic Uncertainty (DDU) estimation to analyze how model uncertainty
influences the interpretability of relevance heatmaps. CFR partitions the
dataset into subsets based on uncertainty thresholds, enabling systematic
analysis of how uncertainty shapes the explanations of naturalness in satellite
imagery. Applied to the AnthroProtect dataset, CFR assigned higher relevance to
shrublands, forests, and wetlands, aligning with other research on naturalness
assessment. Moreover, our analysis shows that as uncertainty increases, the
interpretability of these relevance heatmaps declines and their entropy grows,
indicating less selective and more ambiguous attributions. CFR provides a
data-centric approach to assess the relevance of patterns to naturalness in
satellite imagery based on their associated certainty.

</details>


### [59] [The Power of Architecture: Deep Dive into Transformer Architectures for Long-Term Time Series Forecasting](https://arxiv.org/abs/2507.13043)
*Lefei Shen,Mouxiang Chen,Han Fu,Xiaoxue Ren,Xiaoyun Joy Wang,Jianling Sun,Zhuo Li,Chenghao Liu*

Main category: cs.LG

TL;DR: 论文探讨了Transformer架构在长期时间序列预测（LTSF）中的最佳设计，提出了一种新的分类法以分离不同设计元素，并通过实验验证了双向注意力、完整预测聚合和直接映射范式的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在LTSF任务中架构多样，但设计元素耦合紧密，难以单独评估架构影响，因此需要一种分类法来清晰比较不同架构。

Method: 提出一种新的分类法，分离注意力机制、预测聚合、预测范式和归一化层等关键设计元素，并通过实验验证不同架构的效果。

Result: 实验表明，双向注意力结合联合注意力最有效，完整预测聚合提升性能，直接映射范式优于自回归方法。综合最优架构的模型表现优于现有模型。

Conclusion: 研究为LTSF中Transformer架构设计提供了有价值的指导，验证了最优架构选择的合理性。

Abstract: Transformer-based models have recently become dominant in Long-term Time
Series Forecasting (LTSF), yet the variations in their architecture, such as
encoder-only, encoder-decoder, and decoder-only designs, raise a crucial
question: What Transformer architecture works best for LTSF tasks? However,
existing models are often tightly coupled with various time-series-specific
designs, making it difficult to isolate the impact of the architecture itself.
To address this, we propose a novel taxonomy that disentangles these designs,
enabling clearer and more unified comparisons of Transformer architectures. Our
taxonomy considers key aspects such as attention mechanisms, forecasting
aggregations, forecasting paradigms, and normalization layers. Through
extensive experiments, we uncover several key insights: bi-directional
attention with joint-attention is most effective; more complete forecasting
aggregation improves performance; and the direct-mapping paradigm outperforms
autoregressive approaches. Furthermore, our combined model, utilizing optimal
architectural choices, consistently outperforms several existing models,
reinforcing the validity of our conclusions. We hope these findings offer
valuable guidance for future research on Transformer architectural designs in
LTSF. Our code is available at https://github.com/HALF111/TSF_architecture.

</details>


### [60] [On statistical learning of graphs](https://arxiv.org/abs/2507.13054)
*Vittorio Cipriani,Valentino Delle Rose,Luca San Mauro,Giovanni Solda*

Main category: cs.LG

TL;DR: 研究了由无限可数图G的副本形成的假设类的PAC和在线可学习性，其中每个副本通过置换G的顶点生成。主要结果表明，有限支持副本的PAC可学习性暗示了G的完全同构类型的在线可学习性，并等价于自同构平凡性条件。


<details>
  <summary>Details</summary>
Motivation: 探索图标记学习问题，已知图结构和标签集，研究其在不同置换条件下的可学习性。

Method: 分析由有限支持置换生成的图副本的PAC可学习性，并扩展到在线学习场景。

Result: 证明了有限支持副本的PAC可学习性等价于G的在线可学习性和自同构平凡性。此外，对两顶点置换不可学习的图进行了刻画。

Conclusion: 研究揭示了无限图的可学习性分类，并展示了描述集理论和可计算性理论工具的应用。

Abstract: We study PAC and online learnability of hypothesis classes formed by copies
of a countably infinite graph G, where each copy is induced by permuting G's
vertices. This corresponds to learning a graph's labeling, knowing its
structure and label set. We consider classes where permutations move only
finitely many vertices. Our main result shows that PAC learnability of all such
finite-support copies implies online learnability of the full isomorphism type
of G, and is equivalent to the condition of automorphic triviality. We also
characterize graphs where copies induced by swapping two vertices are not
learnable, using a relaxation of the extension property of the infinite random
graph. Finally, we show that, for all G and k>2, learnability for k-vertex
permutations is equivalent to that for 2-vertex permutations, yielding a
four-class partition of infinite graphs, whose complexity we also determine
using tools coming from both descriptive set theory and computability theory.

</details>


### [61] [DASViT: Differentiable Architecture Search for Vision Transformer](https://arxiv.org/abs/2507.13079)
*Pengjin Wu,Ferrante Neri,Zhenhua Feng*

Main category: cs.LG

TL;DR: DASViT是一种可微分架构搜索方法，专为Vision Transformers设计，解决了现有方法的创新性不足和计算资源消耗大的问题，并在性能和效率上超越传统ViT架构。


<details>
  <summary>Details</summary>
Motivation: 现有NAS方法在ViT架构搜索中面临创新性不足、计算资源消耗大和时间长的问题，需要一种更高效的方法。

Method: 提出DASViT，一种可微分架构搜索方法，专注于ViT的架构搜索，突破传统Transformer编码器设计。

Result: DASViT生成的架构在多个数据集上超越ViT-B/16，参数和计算量更少，效率更高。

Conclusion: DASViT为ViT架构搜索提供了一种高效且创新的解决方案，展示了其在性能和效率上的优势。

Abstract: Designing effective neural networks is a cornerstone of deep learning, and
Neural Architecture Search (NAS) has emerged as a powerful tool for automating
this process. Among the existing NAS approaches, Differentiable Architecture
Search (DARTS) has gained prominence for its efficiency and ease of use,
inspiring numerous advancements. Since the rise of Vision Transformers (ViT),
researchers have applied NAS to explore ViT architectures, often focusing on
macro-level search spaces and relying on discrete methods like evolutionary
algorithms. While these methods ensure reliability, they face challenges in
discovering innovative architectural designs, demand extensive computational
resources, and are time-intensive. To address these limitations, we introduce
Differentiable Architecture Search for Vision Transformer (DASViT), which
bridges the gap in differentiable search for ViTs and uncovers novel designs.
Experiments show that DASViT delivers architectures that break traditional
Transformer encoder designs, outperform ViT-B/16 on multiple datasets, and
achieve superior efficiency with fewer parameters and FLOPs.

</details>


### [62] [MUPAX: Multidimensional Problem Agnostic eXplainable AI](https://arxiv.org/abs/2507.13090)
*Vincenzo Dentamaro,Felice Franchini,Giuseppe Pirlo,Irina Voiculescu*

Main category: cs.LG

TL;DR: MUPAX是一种确定性、模型无关且保证收敛的XAI技术，通过结构化扰动分析提供特征重要性归因，适用于多种数据模态和任务，并能提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有XAI技术通常缺乏确定性、模型无关性或收敛保证，MUPAX旨在填补这一空白，提供更可靠和通用的解释方法。

Method: MUPAX基于测度理论，通过结构化扰动分析发现输入的内在模式并消除虚假关系，适用于任意维度和损失函数。

Result: 在音频分类、图像分类、医学图像分析和解剖标志检测等任务中，MUPAX表现出维度无关的有效性，并能提升模型准确性。

Conclusion: MUPAX在生成精确、一致且可理解的解释方面优于现有XAI方法，是迈向可信AI的重要一步。

Abstract: Robust XAI techniques should ideally be simultaneously deterministic, model
agnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM
AGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability
technique, with guaranteed convergency. MUPAX measure theoretic formulation
gives principled feature importance attribution through structured perturbation
analysis that discovers inherent input patterns and eliminates spurious
relationships. We evaluate MUPAX on an extensive range of data modalities and
tasks: audio classification (1D), image classification (2D), volumetric medical
image analysis (3D), and anatomical landmark detection, demonstrating dimension
agnostic effectiveness. The rigorous convergence guarantees extend to any loss
function and arbitrary dimensions, making MUPAX applicable to virtually any
problem context for AI. By contrast with other XAI methods that typically
decrease performance when masking, MUPAX not only preserves but actually
enhances model accuracy by capturing only the most important patterns of the
original data. Extensive benchmarking against the state of the XAI art
demonstrates MUPAX ability to generate precise, consistent and understandable
explanations, a crucial step towards explainable and trustworthy AI systems.
The source code will be released upon publication.

</details>


### [63] [Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces](https://arxiv.org/abs/2507.13092)
*Hyo-Jeong Jang,Hye-Bin Shin,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 提出了一种跨模态知识蒸馏框架，解决EEG学习中的模态差异和标签不一致问题，提升情感回归和分类性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受噪声和标签错误影响，传统知识蒸馏方法面临模态差异和软标签不对齐的挑战。

Method: 通过原型相似性模块对齐特征语义，并引入任务特定的蒸馏头解决标签不一致问题。

Result: 在公共多模态数据集上，该方法优于单模态和多模态基线。

Conclusion: 该框架在BCI应用中具有潜力。

Abstract: Electroencephalography (EEG) is a fundamental modality for cognitive state
monitoring in brain-computer interfaces (BCIs). However, it is highly
susceptible to intrinsic signal errors and human-induced labeling errors, which
lead to label noise and ultimately degrade model performance. To enhance EEG
learning, multimodal knowledge distillation (KD) has been explored to transfer
knowledge from visual models with rich representations to EEG-based models.
Nevertheless, KD faces two key challenges: modality gap and soft label
misalignment. The former arises from the heterogeneous nature of EEG and visual
feature spaces, while the latter stems from label inconsistencies that create
discrepancies between ground truth labels and distillation targets. This paper
addresses semantic uncertainty caused by ambiguous features and weakly defined
labels. We propose a novel cross-modal knowledge distillation framework that
mitigates both modality and label inconsistencies. It aligns feature semantics
through a prototype-based similarity module and introduces a task-specific
distillation head to resolve label-induced inconsistency in supervision.
Experimental results demonstrate that our approach improves EEG-based emotion
regression and classification performance, outperforming both unimodal and
multimodal baselines on a public multimodal dataset. These findings highlight
the potential of our framework for BCI applications.

</details>


### [64] [NGTM: Substructure-based Neural Graph Topic Model for Interpretable Graph Generation](https://arxiv.org/abs/2507.13133)
*Yuanxin Zhuang,Dazhong Shen,Ying Sun*

Main category: cs.LG

TL;DR: NGTM是一种新型图生成框架，通过潜在主题建模提升图结构的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法缺乏可解释性，NGTM旨在解决这一问题。

Method: NGTM将图表示为潜在主题的混合，每个主题定义语义子结构的分布。

Result: NGTM在生成质量上具有竞争力，同时支持细粒度控制和语义追踪。

Conclusion: NGTM为图生成提供了可解释性和可控性，适用于分子设计等领域。

Abstract: Graph generation plays a pivotal role across numerous domains, including
molecular design and knowledge graph construction. Although existing methods
achieve considerable success in generating realistic graphs, their
interpretability remains limited, often obscuring the rationale behind
structural decisions. To address this challenge, we propose the Neural Graph
Topic Model (NGTM), a novel generative framework inspired by topic modeling in
natural language processing. NGTM represents graphs as mixtures of latent
topics, each defining a distribution over semantically meaningful
substructures, which facilitates explicit interpretability at both local and
global scales. The generation process transparently integrates these topic
distributions with a global structural variable, enabling clear semantic
tracing of each generated graph. Experiments demonstrate that NGTM achieves
competitive generation quality while uniquely enabling fine-grained control and
interpretability, allowing users to tune structural features or induce
biological properties through topic-level adjustments.

</details>


### [65] [NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal Vocalizations with Emotion Annotations for Text-to-Speech](https://arxiv.org/abs/2507.13155)
*Maksim Borisov,Egor Spirin,Daria Diatlova*

Main category: cs.LG

TL;DR: 论文介绍了NonverbalTTS（NVTTS）数据集，包含17小时的非语言声音（NVs）标注，并提出了一套标注流程和TTS模型微调方法，性能媲美闭源系统。


<details>
  <summary>Details</summary>
Motivation: 当前表达性语音合成模型受限于开源数据集中非语言声音（NVs）的多样性不足，因此需要构建一个开放且多样化的NVs数据集。

Method: 通过自动检测和人工验证，从VoxCeleb和Expresso数据集中提取NVs，并整合ASR、NV标记、情感分类和多标注者转录融合算法。

Result: 在NVTTS数据集上微调的开源TTS模型性能与闭源系统（如CosyVoice2）相当，通过人工评估和自动指标验证。

Conclusion: NVTTS数据集及其标注指南解决了表达性TTS研究的关键瓶颈，数据集已开源。

Abstract: Current expressive speech synthesis models are constrained by the limited
availability of open-source datasets containing diverse nonverbal vocalizations
(NVs). In this work, we introduce NonverbalTTS (NVTTS), a 17-hour open-access
dataset annotated with 10 types of NVs (e.g., laughter, coughs) and 8 emotional
categories. The dataset is derived from popular sources, VoxCeleb and Expresso,
using automated detection followed by human validation. We propose a
comprehensive pipeline that integrates automatic speech recognition (ASR), NV
tagging, emotion classification, and a fusion algorithm to merge transcriptions
from multiple annotators. Fine-tuning open-source text-to-speech (TTS) models
on the NVTTS dataset achieves parity with closed-source systems such as
CosyVoice2, as measured by both human evaluation and automatic metrics,
including speaker similarity and NV fidelity. By releasing NVTTS and its
accompanying annotation guidelines, we address a key bottleneck in expressive
TTS research. The dataset is available at
https://huggingface.co/datasets/deepvk/NonverbalTTS.

</details>


### [66] [Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities](https://arxiv.org/abs/2507.13158)
*Hao Sun,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文综述了通过逆向强化学习（IRL）改进大语言模型（LLM）对齐的最新进展，强调了与传统强化学习（RL）的区别，并探讨了构建神经奖励模型的实际意义。


<details>
  <summary>Details</summary>
Motivation: 在LLM时代，对齐问题成为提升模型可靠性、可控性和能力的关键挑战，而RL在其中的作用日益凸显。

Method: 通过IRL构建神经奖励模型，结合人类数据，探讨其在LLM对齐中的应用。

Result: 总结了当前研究的进展、挑战和机遇，并提出了未来研究方向。

Conclusion: RL和IRL技术为LLM对齐提供了新思路，但仍需解决开放性问题，未来研究应关注实际应用和效率提升。

Abstract: In the era of Large Language Models (LLMs), alignment has emerged as a
fundamental yet challenging problem in the pursuit of more reliable,
controllable, and capable machine intelligence. The recent success of reasoning
models and conversational AI systems has underscored the critical role of
reinforcement learning (RL) in enhancing these systems, driving increased
research interest at the intersection of RL and LLM alignment. This paper
provides a comprehensive review of recent advances in LLM alignment through the
lens of inverse reinforcement learning (IRL), emphasizing the distinctions
between RL techniques employed in LLM alignment and those in conventional RL
tasks. In particular, we highlight the necessity of constructing neural reward
models from human data and discuss the formal and practical implications of
this paradigm shift. We begin by introducing fundamental concepts in RL to
provide a foundation for readers unfamiliar with the field. We then examine
recent advances in this research agenda, discussing key challenges and
opportunities in conducting IRL for LLM alignment. Beyond methodological
considerations, we explore practical aspects, including datasets, benchmarks,
evaluation metrics, infrastructure, and computationally efficient training and
inference techniques. Finally, we draw insights from the literature on
sparse-reward RL to identify open questions and potential research directions.
By synthesizing findings from diverse studies, we aim to provide a structured
and critical overview of the field, highlight unresolved challenges, and
outline promising future directions for improving LLM alignment through RL and
IRL techniques.

</details>


### [67] [Spectral Bellman Method: Unifying Representation and Exploration in RL](https://arxiv.org/abs/2507.13181)
*Ofir Nabati,Bo Dai,Shie Mannor,Guy Tennenholtz*

Main category: cs.LG

TL;DR: 提出了一种基于Inherent Bellman Error（IBE）条件的Spectral Bellman Representation框架，用于强化学习中的表示学习，直接对齐Bellman更新结构，提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有表示学习方法主要基于模型学习，与强化学习任务不匹配，需要一种直接面向值函数空间的表示学习方法。

Method: 通过发现零IBE条件下的谱关系，提出一种新的理论目标，学习与Bellman动态对齐的状态-动作特征。

Result: 实验表明，该方法能实现结构化探索，提升性能，尤其在困难探索和长期信用分配任务中表现优异。

Conclusion: Spectral Bellman Representation为值基强化学习提供了一种理论扎实且有效的表示学习方法。

Abstract: The effect of representation has been demonstrated in reinforcement learning,
from both theoretical and empirical successes. However, the existing
representation learning mainly induced from model learning aspects, misaligning
with our RL tasks. This work introduces Spectral Bellman Representation, a
novel framework derived from the Inherent Bellman Error (IBE) condition, which
aligns with the fundamental structure of Bellman updates across a space of
possible value functions, therefore, directly towards value-based RL. Our key
insight is the discovery of a fundamental spectral relationship: under the
zero-IBE condition, the transformation of a distribution of value functions by
the Bellman operator is intrinsically linked to the feature covariance
structure. This spectral connection yields a new, theoretically-grounded
objective for learning state-action features that inherently capture this
Bellman-aligned covariance. Our method requires a simple modification to
existing algorithms. We demonstrate that our learned representations enable
structured exploration, by aligning feature covariance with Bellman dynamics,
and improve overall performance, particularly in challenging hard-exploration
and long-horizon credit assignment tasks. Our framework naturally extends to
powerful multi-step Bellman operators, further broadening its impact. Spectral
Bellman Representation offers a principled and effective path toward learning
more powerful and structurally sound representations for value-based
reinforcement learning.

</details>


### [68] [GradNetOT: Learning Optimal Transport Maps with GradNets](https://arxiv.org/abs/2507.13191)
*Shreyas Chaudhari,Srinivasa Pranav,José M. F. Moura*

Main category: cs.LG

TL;DR: 论文提出了一种名为Monotone Gradient Networks (mGradNets)的神经网络，用于直接参数化单调梯度映射空间，并通过最小化基于Monge-Ampère方程的损失函数来学习最优传输映射。


<details>
  <summary>Details</summary>
Motivation: 单调梯度函数在解决Monge形式的最优传输问题中起核心作用，该问题在流体动力学和机器人群体控制等现代应用中有重要意义。

Method: 利用mGradNets直接参数化单调梯度映射空间，并通过最小化基于Monge-Ampère方程的损失函数来学习最优传输映射。

Result: 实验表明，mGradNets的结构偏置有助于学习最优传输映射，并成功应用于机器人群体控制问题。

Conclusion: mGradNets为学习最优传输映射提供了一种有效方法，并在实际应用中展现了潜力。

Abstract: Monotone gradient functions play a central role in solving the Monge
formulation of the optimal transport problem, which arises in modern
applications ranging from fluid dynamics to robot swarm control. When the
transport cost is the squared Euclidean distance, Brenier's theorem guarantees
that the unique optimal map is the gradient of a convex function, namely a
monotone gradient map, and it satisfies a Monge-Amp\`ere equation. In
[arXiv:2301.10862] [arXiv:2404.07361], we proposed Monotone Gradient Networks
(mGradNets), neural networks that directly parameterize the space of monotone
gradient maps. In this work, we leverage mGradNets to directly learn the
optimal transport mapping by minimizing a training loss function defined using
the Monge-Amp\`ere equation. We empirically show that the structural bias of
mGradNets facilitates the learning of optimal transport maps and employ our
method for a robot swarm control problem.

</details>


### [69] [MoTM: Towards a Foundation Model for Time Series Imputation based on Continuous Modeling](https://arxiv.org/abs/2507.13207)
*Etienne Le Naour,Tahar Nabil,Ghislain Agoua*

Main category: cs.LG

TL;DR: 提出了一种基于隐式神经表示（INRs）的时间序列填补方法MoTM，用于解决分布偏移下的缺失值填补问题。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型主要关注预测任务，而跨域缺失值填补任务研究不足。

Method: MoTM结合了多个独立训练的INRs和一个岭回归器，通过混合已见过的模式适应新时间序列。

Result: MoTM在多种缺失场景（如块缺失、点缺失、可变采样率）下表现出鲁棒的域内和跨域泛化能力。

Conclusion: MoTM为适应性强的填补基础模型提供了初步解决方案。

Abstract: Recent years have witnessed a growing interest for time series foundation
models, with a strong emphasis on the forecasting task. Yet, the crucial task
of out-of-domain imputation of missing values remains largely underexplored. We
propose a first step to fill this gap by leveraging implicit neural
representations (INRs). INRs model time series as continuous functions and
naturally handle various missing data scenarios and sampling rates. While they
have shown strong performance within specific distributions, they struggle
under distribution shifts. To address this, we introduce MoTM (Mixture of
Timeflow Models), a step toward a foundation model for time series imputation.
Building on the idea that a new time series is a mixture of previously seen
patterns, MoTM combines a basis of INRs, each trained independently on a
distinct family of time series, with a ridge regressor that adapts to the
observed context at inference. We demonstrate robust in-domain and
out-of-domain generalization across diverse imputation scenarios (e.g., block
and pointwise missingness, variable sampling rates), paving the way for
adaptable foundation imputation models.

</details>


### [70] [Merge Kernel for Bayesian Optimization on Permutation Space](https://arxiv.org/abs/2507.13263)
*Zikai Xie,Linjiang Chen*

Main category: cs.LG

TL;DR: 提出了一种基于排序算法的新框架，用于生成排列空间上的核函数，其中Mallows核被视为由冒泡排序衍生的特例。进一步提出了复杂度更低的Merge Kernel，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Mallows核在排列空间上的计算复杂度较高（Ω(n²)），限制了其在大规模问题中的应用。

Method: 基于排序算法框架生成核函数，提出Merge Kernel（复杂度Θ(n log n)），并引入三种轻量级描述符提升鲁棒性。

Result: 实验表明，Merge Kernel在多个排列优化基准测试中优于Mallows核，提供更紧凑且高效的解决方案。

Conclusion: Merge Kernel为排列空间上的贝叶斯优化提供了更高效且有效的核函数选择。

Abstract: Bayesian Optimization (BO) algorithm is a standard tool for black-box
optimization problems. The current state-of-the-art BO approach for permutation
spaces relies on the Mallows kernel-an $\Omega(n^2)$ representation that
explicitly enumerates every pairwise comparison. Inspired by the close
relationship between the Mallows kernel and pairwise comparison, we propose a
novel framework for generating kernel functions on permutation space based on
sorting algorithms. Within this framework, the Mallows kernel can be viewed as
a special instance derived from bubble sort. Further, we introduce the
\textbf{Merge Kernel} constructed from merge sort, which replaces the quadratic
complexity with $\Theta(n\log n)$ to achieve the lowest possible complexity.
The resulting feature vector is significantly shorter, can be computed in
linearithmic time, yet still efficiently captures meaningful permutation
distances. To boost robustness and right-invariance without sacrificing
compactness, we further incorporate three lightweight, task-agnostic
descriptors: (1) a shift histogram, which aggregates absolute element
displacements and supplies a global misplacement signal; (2) a split-pair line,
which encodes selected long-range comparisons by aligning elements across the
two halves of the whole permutation; and (3) sliding-window motifs, which
summarize local order patterns that influence near-neighbor objectives. Our
empirical evaluation demonstrates that the proposed kernel consistently
outperforms the state-of-the-art Mallows kernel across various permutation
optimization benchmarks. Results confirm that the Merge Kernel provides a more
compact yet more effective solution for Bayesian optimization in permutation
space.

</details>


### [71] [Boosting Team Modeling through Tempo-Relational Representation Learning](https://arxiv.org/abs/2507.13305)
*Vincenzo Marco De Luca,Giovanna Varni,Andrea Passerini*

Main category: cs.LG

TL;DR: 论文提出TRENN和MT-TRENN模型，结合时间与关系动态，同时预测多个团队构念，显著优于仅依赖时间或关系信息的方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型无法同时满足动态与关系建模需求，且缺乏统一模型和可解释性。

Method: TRENN整合时间图提取器、编码器、解码器和解释模块；MT-TRENN扩展为多任务头，预测多个团队构念。

Result: 实验表明模型性能显著优于基线，解释模块提供可操作建议。

Conclusion: 模型适用于高协作环境的人本AI应用。

Abstract: Team modeling remains a fundamental challenge at the intersection of
Artificial Intelligence and the Social Sciences. Social Science research
emphasizes the need to jointly model dynamics and relations, while practical
applications demand unified models capable of inferring multiple team
constructs simultaneously, providing interpretable insights and actionable
recommendations to enhance team performance. However, existing works do not
meet these practical demands. To bridge this gap, we present TRENN, a novel
tempo-relational architecture that integrates: (i) an automatic temporal graph
extractor, (ii) a tempo-relational encoder, (iii) a decoder for team construct
prediction, and (iv) two complementary explainability modules. TRENN jointly
captures relational and temporal team dynamics, providing a solid foundation
for MT-TRENN, which extends TReNN by replacing the decoder with a multi-task
head, enabling the model to learn shared Social Embeddings and simultaneously
predict multiple team constructs, including Emergent Leadership, Leadership
Style, and Teamwork components. Experimental results demonstrate that our
approach significantly outperforms approaches that rely exclusively on temporal
or relational information. Additionally, experimental evaluation has shown that
the explainability modules integrated in MT-TRENN yield interpretable insights
and actionable suggestions to support team improvement. These capabilities make
our approach particularly well-suited for Human-Centered AI applications, such
as intelligent decision-support systems in high-stakes collaborative
environments.

</details>


### [72] [GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic Estimation using LLM](https://arxiv.org/abs/2507.13323)
*Kyeongjin Ahn,Sungwon Han,Seungeon Lee,Donghyun Ahn,Hyoshin Kim,Jungwon Kim,Jihee Kim,Sangyoon Park,Meeyoung Cha*

Main category: cs.LG

TL;DR: GeoReg模型结合卫星图像和地理空间数据，利用大语言模型解决数据稀缺问题，有效估计社会经济指标。


<details>
  <summary>Details</summary>
Motivation: 社会经济指标对政策制定和可持续发展至关重要，但数据稀缺地区（如发展中国家）难以获取准确数据。

Method: 整合卫星图像和地理空间数据，利用大语言模型提取特征，分类特征相关性，并引入非线性变换和特征交互。

Result: 在三个不同发展阶段国家的实验中，模型优于基线方法，尤其适用于数据稀缺的低收入国家。

Conclusion: GeoReg模型为数据稀缺地区的社会经济指标估计提供了有效解决方案。

Abstract: Socio-economic indicators like regional GDP, population, and education
levels, are crucial to shaping policy decisions and fostering sustainable
development. This research introduces GeoReg a regression model that integrates
diverse data sources, including satellite imagery and web-based geospatial
information, to estimate these indicators even for data-scarce regions such as
developing countries. Our approach leverages the prior knowledge of large
language model (LLM) to address the scarcity of labeled data, with the LLM
functioning as a data engineer by extracting informative features to enable
effective estimation in few-shot settings. Specifically, our model obtains
contextual relationships between data features and the target indicator,
categorizing their correlations as positive, negative, mixed, or irrelevant.
These features are then fed into the linear estimator with tailored weight
constraints for each category. To capture nonlinear patterns, the model also
identifies meaningful feature interactions and integrates them, along with
nonlinear transformations. Experiments across three countries at different
stages of development demonstrate that our model outperforms baselines in
estimating socio-economic indicators, even for low-income countries with
limited data availability.

</details>


### [73] [Training Transformers with Enforced Lipschitz Constants](https://arxiv.org/abs/2507.13338)
*Laker Newhouse,R. Preston Hess,Franz Cesista,Andrii Zahorodnii,Jeremy Bernstein,Phillip Isola*

Main category: cs.LG

TL;DR: 论文研究了如何通过Lipschitz约束训练现代神经网络（如Transformer），提出高效工具并优化训练方法，发现优化器选择对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 神经网络的输入和权重扰动敏感性导致对抗样本脆弱性、训练发散和过拟合等问题，现有Lipschitz约束方法尚未成熟。

Method: 开发高效工具约束权重矩阵范数，结合优化器Muon改进训练，设计新权重约束方法提升Lipschitz与性能的平衡。

Result: 在Shakespeare文本上，2-Lipschitz Transformer验证准确率达60%；145M参数模型在互联网文本上达21%准确率，但需放宽约束以匹配基线。

Conclusion: Lipschitz约束Transformer无需额外稳定性措施即可训练，但高准确率需放宽约束，优化器选择对性能至关重要。

Abstract: Neural networks are often highly sensitive to input and weight perturbations.
This sensitivity has been linked to pathologies such as vulnerability to
adversarial examples, divergent training, and overfitting. To combat these
problems, past research has looked at building neural networks entirely from
Lipschitz components. However, these techniques have not matured to the point
where researchers have trained a modern architecture such as a transformer with
a Lipschitz certificate enforced beyond initialization. To explore this gap, we
begin by developing and benchmarking novel, computationally-efficient tools for
maintaining norm-constrained weight matrices. Applying these tools, we are able
to train transformer models with Lipschitz bounds enforced throughout training.
We find that optimizer dynamics matter: switching from AdamW to Muon improves
standard methods -- weight decay and spectral normalization -- allowing models
to reach equal performance with a lower Lipschitz bound. Inspired by Muon's
update having a fixed spectral norm, we co-design a weight constraint method
that improves the Lipschitz vs. performance tradeoff on MLPs and 2M parameter
transformers. Our 2-Lipschitz transformer on Shakespeare text reaches
validation accuracy 60%. Scaling to 145M parameters, our 10-Lipschitz
transformer reaches 21% accuracy on internet text. However, to match the
NanoGPT baseline validation accuracy of 39.4%, our Lipschitz upper bound
increases to 10^264. Nonetheless, our Lipschitz transformers train without
stability measures such as layer norm, QK norm, and logit tanh softcapping.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [74] [Physics constrained learning of stochastic characteristics](https://arxiv.org/abs/2507.12661)
*Pardha Sai Krishna Ala,Ameya Salvi,Venkat Krovi,Matthias Schmid*

Main category: stat.ML

TL;DR: 本文提出了一种基于学习的方法来识别噪声特性，用于实时车辆状态估计中的协方差矩阵选择问题


<details>
  <summary>Details</summary>
Motivation: 准确的状态估计需要仔细考虑过程模型和测量模型的不确定性，但这些特性通常不为人知，需要有经验的设计师来选择协方差矩阵。协方差矩阵选择错误可能影响估计算法的精度，有时甚至导致滤波器发散。由于噪声源的不确定性和系统性噪声建模的困难，识别噪声特性一直是一个挑战性问题

Method: 提出了一种基于学习的方法，使用不同的损失函数来识别噪声特性。该方法区别于现有的通过涉及新息序列的优化算法来识别未知协方差矩阵的方法，采用学习方法来确定过程模型和测量模型的随机特性

Result: 测试了这些方法在实时车辆状态估计中的性能表现

Conclusion: 基于学习的方法能够有效识别噪声特性，为实时车辆状态估计提供了新的解决方案

Abstract: Accurate state estimation requires careful consideration of uncertainty
surrounding the process and measurement models; these characteristics are
usually not well-known and need an experienced designer to select the
covariance matrices. An error in the selection of covariance matrices could
impact the accuracy of the estimation algorithm and may sometimes cause the
filter to diverge. Identifying noise characteristics has long been a
challenging problem due to uncertainty surrounding noise sources and
difficulties in systematic noise modeling. Most existing approaches try
identifying unknown covariance matrices through an optimization algorithm
involving innovation sequences. In recent years, learning approaches have been
utilized to determine the stochastic characteristics of process and measurement
models. We present a learning-based methodology with different loss functions
to identify noise characteristics and test these approaches' performance for
real-time vehicle state estimation

</details>


### [75] [Finite-Dimensional Gaussian Approximation for Deep Neural Networks: Universality in Random Weights](https://arxiv.org/abs/2507.12686)
*Krishnakumar Balasubramanian,Nathan Ross*

Main category: stat.ML

TL;DR: 研究了具有有限阶矩的随机初始化权重深度神经网络的有限维分布（FDDs），并在Wasserstein-1范数下建立了FDDs与其高斯极限之间的近似界限。


<details>
  <summary>Details</summary>
Motivation: 探索深度神经网络在随机初始化权重下的有限维分布行为，特别是在宽度趋于无穷时的收敛性质。

Method: 假设激活函数为Lipschitz连续，允许各层宽度以任意相对速率增长，推导高斯近似界限。

Result: 在宽度与共同尺度参数n成比例且存在L-1隐藏层时，收敛速率为n^{-(1/6)^{L-1} + ε}（ε>0）。

Conclusion: 证明了深度神经网络FDDs在高斯极限下的收敛性，并给出了具体的收敛速率。

Abstract: We study the Finite-Dimensional Distributions (FDDs) of deep neural networks
with randomly initialized weights that have finite-order moments. Specifically,
we establish Gaussian approximation bounds in the Wasserstein-$1$ norm between
the FDDs and their Gaussian limit assuming a Lipschitz activation function and
allowing the layer widths to grow to infinity at arbitrary relative rates. In
the special case where all widths are proportional to a common scale parameter
$n$ and there are $L-1$ hidden layers, we obtain convergence rates of order
$n^{-({1}/{6})^{L-1} + \epsilon}$, for any $\epsilon > 0$.

</details>


### [76] [Self Balancing Neural Network: A Novel Method to Estimate Average Treatment Effect](https://arxiv.org/abs/2507.12818)
*Atomsa Gemechu Abdisa,Yingchun Zhou,Yuqi Qiu*

Main category: stat.ML

TL;DR: 提出了一种名为“自平衡神经网络”（Sbnet）的新方法，用于解决观测性研究中因混杂变量和工具变量导致的平均处理效应估计偏差问题。该方法通过平衡网络自动获取伪倾向得分，并在一步中完成估计。


<details>
  <summary>Details</summary>
Motivation: 观测性研究中，混杂变量和工具变量导致平均处理效应估计偏差，传统倾向得分方法存在模型误设风险。

Method: 提出自平衡神经网络（Sbnet），利用平衡网络生成伪倾向得分，并通过多伪倾向得分框架估计平均处理效应。

Result: 在模拟实验和真实数据集上，Sbnet表现优于现有方法。

Conclusion: Sbnet有效解决了倾向得分模型误设问题，提高了平均处理效应估计的准确性。

Abstract: In observational studies, confounding variables affect both treatment and
outcome. Moreover, instrumental variables also influence the treatment
assignment mechanism. This situation sets the study apart from a standard
randomized controlled trial, where the treatment assignment is random. Due to
this situation, the estimated average treatment effect becomes biased. To
address this issue, a standard approach is to incorporate the estimated
propensity score when estimating the average treatment effect. However, these
methods incur the risk of misspecification in propensity score models. To solve
this issue, a novel method called the "Self balancing neural network" (Sbnet),
which lets the model itself obtain its pseudo propensity score from the
balancing net, is proposed in this study. The proposed method estimates the
average treatment effect by using the balancing net as a key part of the
feedforward neural network. This formulation resolves the estimation of the
average treatment effect in one step. Moreover, the multi-pseudo propensity
score framework, which is estimated from the diversified balancing net and used
for the estimation of the average treatment effect, is presented. Finally, the
proposed methods are compared with state-of-the-art methods on three simulation
setups and real-world datasets. It has been shown that the proposed
self-balancing neural network shows better performance than state-of-the-art
methods.

</details>


### [77] [Bayesian Modeling and Estimation of Linear Time-Variant Systems using Neural Networks and Gaussian Processes](https://arxiv.org/abs/2507.12878)
*Yaniv Shulman*

Main category: stat.ML

TL;DR: 提出了一种统一的贝叶斯框架，用于从输入-输出数据中识别线性时变系统，通过建模系统的脉冲响应为随机过程，并利用现代机器学习技术进行推断。


<details>
  <summary>Details</summary>
Motivation: 线性时变系统的识别是一个基础但具有挑战性的不适定逆问题，需要一种能够量化不确定性的方法。

Method: 将系统的脉冲响应分解为后验均值和随机波动项，利用贝叶斯神经网络和高斯过程进行可扩展的变分推断。

Result: 实验表明，该框架能够从单次噪声观测中稳健推断线性时不变系统的特性，在模拟环境噪声层析问题中表现出优于经典方法的数据效率，并能成功跟踪连续变化的线性时变脉冲响应。

Conclusion: 该研究为动态环境中的不确定性感知系统识别提供了一种灵活且稳健的方法论。

Abstract: The identification of Linear Time-Variant (LTV) systems from input-output
data is a fundamental yet challenging ill-posed inverse problem. This work
introduces a unified Bayesian framework that models the system's impulse
response, $h(t, \tau)$, as a stochastic process. We decompose the response into
a posterior mean and a random fluctuation term, a formulation that provides a
principled approach for quantifying uncertainty and naturally defines a new,
useful system class we term Linear Time-Invariant in Expectation (LTIE). To
perform inference, we leverage modern machine learning techniques, including
Bayesian neural networks and Gaussian Processes, using scalable variational
inference. We demonstrate through a series of experiments that our framework
can robustly infer the properties of an LTI system from a single noisy
observation, show superior data efficiency compared to classical methods in a
simulated ambient noise tomography problem, and successfully track a
continuously varying LTV impulse response by using a structured Gaussian
Process prior. This work provides a flexible and robust methodology for
uncertainty-aware system identification in dynamic environments.

</details>


### [78] [When Pattern-by-Pattern Works: Theoretical and Empirical Insights for Logistic Models with Missing Values](https://arxiv.org/abs/2507.13024)
*Christophe Muller,Erwan Scornet,Julie Josse*

Main category: stat.ML

TL;DR: 论文研究了逻辑回归模型在部分缺失输入下的预测问题，提出了一种基于缺失模式的策略（PbP），并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决部分缺失输入下的预测问题，尤其是在逻辑回归模型中，填补了现有研究的空白。

Method: 采用Pattern-by-Pattern（PbP）策略，为每种缺失模式训练一个逻辑回归模型，并与多种方法（如均值填补、MICE.RF.Y等）进行比较。

Result: 理论证明PbP能准确逼近贝叶斯概率；实验表明，小样本下均值填补可作为基线，大样本下PbP或MICE.RF.Y表现最佳。

Conclusion: PbP在特定场景下表现优异，MICE.RF.Y适用于非线性特征，为逻辑回归缺失值处理提供了全面指导。

Abstract: Predicting a response with partially missing inputs remains a challenging
task even in parametric models, since parameter estimation in itself is not
sufficient to predict on partially observed inputs. Several works study
prediction in linear models. In this paper, we focus on logistic models, which
present their own difficulties. From a theoretical perspective, we prove that a
Pattern-by-Pattern strategy (PbP), which learns one logistic model per
missingness pattern, accurately approximates Bayes probabilities in various
missing data scenarios (MCAR, MAR and MNAR). Empirically, we thoroughly compare
various methods (constant and iterative imputations, complete case analysis,
PbP, and an EM algorithm) across classification, probability estimation,
calibration, and parameter inference. Our analysis provides a comprehensive
view on the logistic regression with missing values. It reveals that mean
imputation can be used as baseline for low sample sizes, and improved
performance is obtained via nonlinear multiple iterative imputation techniques
with the labels (MICE.RF.Y). For large sample sizes, PbP is the best method for
Gaussian mixtures, and we recommend MICE.RF.Y in presence of nonlinear
features.

</details>


### [79] [Relation-Aware Slicing in Cross-Domain Alignment](https://arxiv.org/abs/2507.13194)
*Dhruv Sarkar,Aprameyo Chakrabartty,Anish Chakrabarty,Swagatam Das*

Main category: stat.ML

TL;DR: 提出了一种优化自由的切片分布方法（RASD），通过关系感知投影方向（RAPD）改进SGW距离的计算效率和代表性。


<details>
  <summary>Details</summary>
Motivation: SGW距离的计算成本高且方向选择不理想，影响了其代表性和效率。

Method: 引入RAPD捕捉随机向量对的关联性，并基于此提出RASD分布，进一步提出RASGW距离及其变种（如IWRASGW）。

Result: 理论分析和实验验证表明，RASGW及其变种在多种对齐任务中表现优于SGW。

Conclusion: RASD和RASGW提供了一种高效且具有代表性的替代方案，解决了SGW的局限性。

Abstract: The Sliced Gromov-Wasserstein (SGW) distance, aiming to relieve the
computational cost of solving a non-convex quadratic program that is the
Gromov-Wasserstein distance, utilizes projecting directions sampled uniformly
from unit hyperspheres. This slicing mechanism incurs unnecessary computational
costs due to uninformative directions, which also affects the representative
power of the distance. However, finding a more appropriate distribution over
the projecting directions (slicing distribution) is often an optimization
problem in itself that comes with its own computational cost. In addition, with
more intricate distributions, the sampling itself may be expensive. As a
remedy, we propose an optimization-free slicing distribution that provides fast
sampling for the Monte Carlo approximation. We do so by introducing the
Relation-Aware Projecting Direction (RAPD), effectively capturing the pairwise
association of each of two pairs of random vectors, each following their
ambient law. This enables us to derive the Relation-Aware Slicing Distribution
(RASD), a location-scale law corresponding to sampled RAPDs. Finally, we
introduce the RASGW distance and its variants, e.g., IWRASGW (Importance
Weighted RASGW), which overcome the shortcomings experienced by SGW. We
theoretically analyze its properties and substantiate its empirical prowess
using extensive experiments on various alignment tasks.

</details>
