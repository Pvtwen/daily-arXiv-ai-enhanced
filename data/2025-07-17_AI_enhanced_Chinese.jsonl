{"id": "2507.11768", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11768", "abs": "https://arxiv.org/abs/2507.11768", "authors": ["Leon Chlon", "Sarah Rashidi", "Zein Khamis", "MarcAntonio M. Awada"], "title": "LLMs are Bayesian, in Expectation, not in Realization", "comment": null, "summary": "Large language models demonstrate remarkable in-context learning\ncapabilities, adapting to new tasks without parameter updates. While this\nphenomenon has been successfully modeled as implicit Bayesian inference, recent\nempirical findings reveal a fundamental contradiction: transformers\nsystematically violate the martingale property, a cornerstone requirement of\nBayesian updating on exchangeable data. This violation challenges the\ntheoretical foundations underlying uncertainty quantification in critical\napplications.\n  Our theoretical analysis establishes four key results: (1) positional\nencodings induce martingale violations of order $\\Theta(\\log n / n)$; (2)\ntransformers achieve information-theoretic optimality with excess risk\n$O(n^{-1/2})$ in expectation over orderings; (3) the implicit posterior\nrepresentation converges to the true Bayesian posterior in the space of\nsufficient statistics; and (4) we derive the optimal chain-of-thought length as\n$k^* = \\Theta(\\sqrt{n}\\log(1/\\varepsilon))$ with explicit constants, providing\na principled approach to reduce inference costs while maintaining performance.\nEmpirical validation on GPT-3 confirms predictions (1)-(3), with transformers\nreaching 99\\% of theoretical entropy limits within 20 examples. Our framework\nprovides practical methods for extracting calibrated uncertainty estimates from\nposition-aware architectures and optimizing computational efficiency in\ndeployment.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0Transformer\u8fdd\u53cd\u8d1d\u53f6\u65af\u66f4\u65b0\u7684\u57fa\u672c\u6027\u8d28\uff0c\u5e76\u63d0\u51fa\u4e86\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5176\u4e0e\u8d1d\u53f6\u65af\u63a8\u65ad\u7406\u8bba\u7684\u77db\u76fe\uff0c\u4ee5\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a8\u5bfc\u51fa\u56db\u4e2a\u5173\u952e\u7ed3\u679c\uff0c\u5e76\u5728GPT-3\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u4f4d\u7f6e\u7f16\u7801\u5bfc\u81f4\u8fdd\u53cd\u8d1d\u53f6\u65af\u6027\u8d28\uff0cTransformer\u5728\u4fe1\u606f\u8bba\u4e0a\u8fbe\u5230\u6700\u4f18\uff0c\u5e76\u63d0\u4f9b\u4e86\u4f18\u5316\u63a8\u7406\u6210\u672c\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8bba\u6587\u4e3a\u4f4d\u7f6e\u611f\u77e5\u67b6\u6784\u63d0\u4f9b\u4e86\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u548c\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u7684\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2507.11891", "categories": ["stat.ML", "cs.LG", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.11891", "abs": "https://arxiv.org/abs/2507.11891", "authors": ["Shuangning Li", "Chonghuan Wang", "Jingyan Wang"], "title": "Choosing the Better Bandit Algorithm under Data Sharing: When Do A/B Experiments Work?", "comment": null, "summary": "We study A/B experiments that are designed to compare the performance of two\nrecommendation algorithms. Prior work has shown that the standard\ndifference-in-means estimator is biased in estimating the global treatment\neffect (GTE) due to a particular form of interference between experimental\nunits. Specifically, units under the treatment and control algorithms\ncontribute to a shared pool of data that subsequently train both algorithms,\nresulting in interference between the two groups. The bias arising from this\ntype of data sharing is known as \"symbiosis bias\". In this paper, we highlight\nthat, for decision-making purposes, the sign of the GTE often matters more than\nits precise magnitude when selecting the better algorithm. We formalize this\ninsight under a multi-armed bandit framework and theoretically characterize\nwhen the sign of the expected GTE estimate under data sharing aligns with or\ncontradicts the sign of the true GTE. Our analysis identifies the level of\nexploration versus exploitation as a key determinant of how symbiosis bias\nimpacts algorithm selection.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86A/B\u5b9e\u9a8c\u4e2d\u63a8\u8350\u7b97\u6cd5\u6027\u80fd\u6bd4\u8f83\u7684\u504f\u5dee\u95ee\u9898\uff0c\u91cd\u70b9\u5173\u6ce8\u6570\u636e\u5171\u4eab\u5bfc\u81f4\u7684\u201c\u5171\u751f\u504f\u5dee\u201d\uff0c\u5e76\u63a2\u8ba8\u4e86\u5728\u51b3\u7b56\u4e2dGTE\u7b26\u53f7\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u6807\u51c6\u5747\u503c\u5dee\u4f30\u8ba1\u5668\u5728\u4f30\u8ba1\u5168\u5c40\u5904\u7406\u6548\u5e94\uff08GTE\uff09\u65f6\u5b58\u5728\u504f\u5dee\uff0c\u4e3b\u8981\u7531\u4e8e\u5b9e\u9a8c\u5355\u5143\u95f4\u7684\u5e72\u6270\uff08\u5373\u6570\u636e\u5171\u4eab\u5bfc\u81f4\u7684\u201c\u5171\u751f\u504f\u5dee\u201d\uff09\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7406\u89e3\u8fd9\u79cd\u504f\u5dee\u5bf9\u7b97\u6cd5\u9009\u62e9\u7684\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\uff0c\u7406\u8bba\u5206\u6790\u4e86\u6570\u636e\u5171\u4eab\u4e0bGTE\u4f30\u8ba1\u7b26\u53f7\u4e0e\u771f\u5b9eGTE\u7b26\u53f7\u7684\u5173\u7cfb\uff0c\u5e76\u63a2\u8ba8\u4e86\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u5bf9\u504f\u5dee\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u662f\u5171\u751f\u504f\u5dee\u5f71\u54cd\u7b97\u6cd5\u9009\u62e9\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u5728\u51b3\u7b56\u4e2d\uff0cGTE\u7684\u7b26\u53f7\u6bd4\u5176\u7cbe\u786e\u503c\u66f4\u91cd\u8981\uff0c\u4e14\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u5bf9\u504f\u5dee\u7684\u5f71\u54cd\u663e\u8457\u3002"}}
{"id": "2507.11895", "categories": ["stat.ML", "cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.11895", "abs": "https://arxiv.org/abs/2507.11895", "authors": ["Haolin Zou", "Arnab Auddy", "Yongchan Kwon", "Kamiar Rahnama Rad", "Arian Maleki"], "title": "Newfluence: Boosting Model interpretability and Understanding in High Dimensions", "comment": null, "summary": "The increasing complexity of machine learning (ML) and artificial\nintelligence (AI) models has created a pressing need for tools that help\nscientists, engineers, and policymakers interpret and refine model decisions\nand predictions. Influence functions, originating from robust statistics, have\nemerged as a popular approach for this purpose.\n  However, the heuristic foundations of influence functions rely on\nlow-dimensional assumptions where the number of parameters $p$ is much smaller\nthan the number of observations $n$. In contrast, modern AI models often\noperate in high-dimensional regimes with large $p$, challenging these\nassumptions.\n  In this paper, we examine the accuracy of influence functions in\nhigh-dimensional settings. Our theoretical and empirical analyses reveal that\ninfluence functions cannot reliably fulfill their intended purpose. We then\nintroduce an alternative approximation, called Newfluence, that maintains\nsimilar computational efficiency while offering significantly improved\naccuracy.\n  Newfluence is expected to provide more accurate insights than many existing\nmethods for interpreting complex AI models and diagnosing their issues.\nMoreover, the high-dimensional framework we develop in this paper can also be\napplied to analyze other popular techniques, such as Shapley values.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u9ad8\u7ef4\u73af\u5883\u4e0b\u5f71\u54cd\u51fd\u6570\u7684\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u66ff\u4ee3\u65b9\u6cd5Newfluence\u3002", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u548c\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u9700\u8981\u5de5\u5177\u6765\u89e3\u91ca\u548c\u4f18\u5316\u6a21\u578b\u51b3\u7b56\u3002\u5f71\u54cd\u51fd\u6570\u867d\u6d41\u884c\uff0c\u4f46\u5176\u4f4e\u7ef4\u5047\u8bbe\u5728\u9ad8\u7ef4\u573a\u666f\u4e0b\u4e0d\u9002\u7528\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u8bc4\u4f30\u5f71\u54cd\u51fd\u6570\u5728\u9ad8\u7ef4\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u65b0\u65b9\u6cd5Newfluence\u3002", "result": "\u5f71\u54cd\u51fd\u6570\u5728\u9ad8\u7ef4\u73af\u5883\u4e2d\u4e0d\u53ef\u9760\uff0cNewfluence\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "conclusion": "Newfluence\u4e3a\u89e3\u91ca\u590d\u6742AI\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u5de5\u5177\uff0c\u5176\u9ad8\u7ef4\u6846\u67b6\u8fd8\u53ef\u7528\u4e8e\u5206\u6790\u5176\u4ed6\u6d41\u884c\u65b9\u6cd5\u3002"}}
{"id": "2507.12021", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12021", "abs": "https://arxiv.org/abs/2507.12021", "authors": ["Aleix Alcacer", "Irene Epifanio"], "title": "Incorporating Fairness Constraints into Archetypal Analysis", "comment": null, "summary": "Archetypal Analysis (AA) is an unsupervised learning method that represents\ndata as convex combinations of extreme patterns called archetypes. While AA\nprovides interpretable and low-dimensional representations, it can\ninadvertently encode sensitive attributes, leading to fairness concerns. In\nthis work, we propose Fair Archetypal Analysis (FairAA), a modified formulation\nthat explicitly reduces the influence of sensitive group information in the\nlearned projections. We also introduce FairKernelAA, a nonlinear extension that\naddresses fairness in more complex data distributions. Our approach\nincorporates a fairness regularization term while preserving the structure and\ninterpretability of the archetypes. We evaluate FairAA and FairKernelAA on\nsynthetic datasets, including linear, nonlinear, and multi-group scenarios,\ndemonstrating their ability to reduce group separability -- as measured by mean\nmaximum discrepancy and linear separability -- without substantially\ncompromising explained variance. We further validate our methods on the\nreal-world ANSUR I dataset, confirming their robustness and practical utility.\nThe results show that FairAA achieves a favorable trade-off between utility and\nfairness, making it a promising tool for responsible representation learning in\nsensitive applications.", "AI": {"tldr": "Fair Archetypal Analysis (FairAA) \u548c FairKernelAA \u662f\u6539\u8fdb\u7684 AA \u65b9\u6cd5\uff0c\u901a\u8fc7\u516c\u5e73\u6027\u6b63\u5219\u5316\u51cf\u5c11\u654f\u611f\u5c5e\u6027\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u91ca\u6027\u548c\u7ed3\u6784\u3002", "motivation": "\u4f20\u7edf AA \u53ef\u80fd\u65e0\u610f\u4e2d\u7f16\u7801\u654f\u611f\u5c5e\u6027\uff0c\u5f15\u53d1\u516c\u5e73\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa FairAA \u548c FairKernelAA\uff0c\u52a0\u5165\u516c\u5e73\u6027\u6b63\u5219\u5316\uff0c\u9002\u7528\u4e8e\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u6570\u636e\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u51cf\u5c11\u7ec4\u53ef\u5206\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u91ca\u65b9\u5dee\u3002", "conclusion": "FairAA \u5728\u6548\u7528\u548c\u516c\u5e73\u6027\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u9002\u5408\u654f\u611f\u573a\u666f\u7684\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2507.12146", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12146", "abs": "https://arxiv.org/abs/2507.12146", "authors": ["Carl Collmann", "Bitan Banerjee", "Ahmad Nimr", "Gerhard Fettweis"], "title": "A Practical Analysis: Understanding Phase Noise Modelling in Time and Frequency Domain for Phase-Locked Loops", "comment": "14 Pages", "summary": "In MIMO systems, the presence of phase noise is a significant factor that can\ndegrade performance. For MIMO testbeds build from SDR devices, phase noise\ncannot be ignored, particular in applications that require phase\nsynchronization. This is especially relevant in MIMO systems that employ\ndigital beamforming, where precise phase alignment is crucial. Accordingly,\naccurate phase noise modelling of SDR devices is essential. However, the\ninformation provided in data sheets for different SDR models varies widely and\nis often insufficient for comprehensive characterization of their phase noise\nperformance. While numerical simulations of PLL phase noise behavior are\ndocumented in the literature, there is a lack of extensive measurements\nsupported by appropriate system modelling. In this work, we present a practical\nphase noise modeling methodology applied to an SDR from the USRP X310 series.\nBased on measurement data, we derive estimates of key PLL performance\nindicators such as cycle-to-cycle jitter, oscillator constants, and PLL\nbandwidth. Furthermore, we propose a parametric model for the phase noise PSD\nof the PLL circuit and provide corresponding parameter estimates. This model\ncan be used for further investigation into the impact of phase noise on MIMO\nsystem performance implemented by similar SDR devices.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9USRP X310\u7cfb\u5217SDR\u8bbe\u5907\u7684\u76f8\u4f4d\u566a\u58f0\u5efa\u6a21\u65b9\u6cd5\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u4e2d\u7f3a\u4e4f\u5b9e\u6d4b\u6570\u636e\u652f\u6301\u7684\u7a7a\u767d\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u952ePLL\u6027\u80fd\u6307\u6807\u548c\u76f8\u4f4d\u566a\u58f0PSD\u7684\u53c2\u6570\u5316\u6a21\u578b\u3002", "motivation": "MIMO\u7cfb\u7edf\u4e2d\uff0c\u76f8\u4f4d\u566a\u58f0\u4f1a\u663e\u8457\u5f71\u54cd\u6027\u80fd\uff0c\u5c24\u5176\u662f\u9700\u8981\u76f8\u4f4d\u540c\u6b65\u7684\u6570\u5b57\u6ce2\u675f\u6210\u5f62\u5e94\u7528\u3002\u73b0\u6709SDR\u8bbe\u5907\u7684\u76f8\u4f4d\u566a\u58f0\u6570\u636e\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5b9e\u6d4b\u652f\u6301\u7684\u7cfb\u7edf\u5efa\u6a21\u3002", "method": "\u57fa\u4e8eUSRP X310\u7cfb\u5217SDR\u7684\u5b9e\u6d4b\u6570\u636e\uff0c\u63d0\u53d6\u5173\u952ePLL\u6027\u80fd\u6307\u6807\uff08\u5982\u5468\u671f\u6296\u52a8\u3001\u632f\u8361\u5668\u5e38\u6570\u548cPLL\u5e26\u5bbd\uff09\uff0c\u5e76\u5efa\u7acb\u76f8\u4f4d\u566a\u58f0PSD\u7684\u53c2\u6570\u5316\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u76f8\u4f4d\u566a\u58f0\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u578b\u53c2\u6570\u4f30\u8ba1\uff0c\u53ef\u7528\u4e8e\u5206\u6790\u7c7b\u4f3cSDR\u8bbe\u5907\u5728MIMO\u7cfb\u7edf\u4e2d\u7684\u76f8\u4f4d\u566a\u58f0\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aMIMO\u7cfb\u7edf\u4e2dSDR\u8bbe\u5907\u7684\u76f8\u4f4d\u566a\u58f0\u5efa\u6a21\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u7814\u7a76\u76f8\u4f4d\u566a\u58f0\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.11547", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11547", "abs": "https://arxiv.org/abs/2507.11547", "authors": ["Yingxue Zhao", "Qianyi Chen", "Haoran Li", "Haosu Zhou", "Hamid Reza Attar", "Tobias Pfaff", "Tailin Wu", "Nan Li"], "title": "Recurrent U-Net-Based Graph Neural Network (RUGNN) for Accurate Deformation Predictions in Sheet Material Forming", "comment": null, "summary": "In recent years, various artificial intelligence-based surrogate models have\nbeen proposed to provide rapid manufacturability predictions of material\nforming processes. However, traditional AI-based surrogate models, typically\nbuilt with scalar or image-based neural networks, are limited in their ability\nto capture complex 3D spatial relationships and to operate in a\npermutation-invariant manner. To overcome these issues, emerging graph-based\nsurrogate models are developed using graph neural networks. This study\ndeveloped a new graph neural network surrogate model named Recurrent U\nNet-based Graph Neural Network (RUGNN). The RUGNN model can achieve accurate\npredictions of sheet material deformation fields across multiple forming\ntimesteps. The RUGNN model incorporates Gated Recurrent Units (GRUs) to model\ntemporal dynamics and a U-Net inspired graph-based downsample/upsample\nmechanism to handle spatial long-range dependencies. A novel 'node-to-surface'\ncontact representation method was proposed, offering significant improvements\nin computational efficiency for large-scale contact interactions. The RUGNN\nmodel was validated using a cold forming case study and a more complex hot\nforming case study using aluminium alloys. Results demonstrate that the RUGNN\nmodel provides accurate deformation predictions closely matching ground truth\nFE simulations and outperforming several baseline GNN architectures. Model\ntuning was also performed to identify suitable hyperparameters, training\nstrategies, and input feature representations. These results demonstrate that\nRUGNN is a reliable approach to support sheet material forming design by\nenabling accurate manufacturability predictions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684RUGNN\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u6750\u6599\u6210\u5f62\u8fc7\u7a0b\u4e2d\u7684\u53d8\u5f62\u573a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfAI\u6a21\u578b\u57283D\u7a7a\u95f4\u5173\u7cfb\u548c\u7f6e\u6362\u4e0d\u53d8\u6027\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfAI\u6a21\u578b\u96be\u4ee5\u6355\u6349\u590d\u6742\u76843D\u7a7a\u95f4\u5173\u7cfb\u548c\u7f6e\u6362\u4e0d\u53d8\u6027\uff0c\u56e0\u6b64\u5f00\u53d1\u4e86\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684RUGNN\u6a21\u578b\u3002", "method": "RUGNN\u7ed3\u5408\u4e86\u95e8\u63a7\u5faa\u73af\u5355\u5143\uff08GRU\uff09\u548cU-Net\u7ed3\u6784\u7684\u56fe\u4e0a\u4e0b\u91c7\u6837\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684'\u8282\u70b9\u5230\u8868\u9762'\u63a5\u89e6\u8868\u793a\u65b9\u6cd5\u3002", "result": "RUGNN\u5728\u51b7\u6210\u5f62\u548c\u70ed\u6210\u5f62\u6848\u4f8b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9884\u6d4b\u7ed3\u679c\u4e0e\u6709\u9650\u5143\u4eff\u771f\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebfGNN\u67b6\u6784\u3002", "conclusion": "RUGNN\u662f\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u677f\u6750\u6210\u5f62\u8bbe\u8ba1\u63d0\u4f9b\u51c6\u786e\u7684\u5236\u9020\u6027\u9884\u6d4b\u3002"}}
{"id": "2507.11783", "categories": ["eess.SP", "cs.AI", "cs.LG", "q-bio.NC", "A.1; I.2; I.5; J.3"], "pdf": "https://arxiv.org/pdf/2507.11783", "abs": "https://arxiv.org/abs/2507.11783", "authors": ["Gayal Kuruppu", "Neeraj Wagh", "Yogatheesan Varatharajah"], "title": "Foundation Models for Brain Signals: A Critical Review of Current Progress and Future Directions", "comment": "20 pages, 5 figures, 2 tables", "summary": "Patterns of electrical brain activity recorded via electroencephalography\n(EEG) offer immense value for scientific and clinical investigations. The\ninability of supervised EEG encoders to learn robust EEG patterns and their\nover-reliance on expensive signal annotations have sparked a transition towards\ngeneral-purpose self-supervised EEG encoders, i.e., EEG foundation models\n(EEG-FMs), for robust and scalable EEG feature extraction. However, the\nreal-world readiness of early EEG-FMs and the rubric for long-term research\nprogress remain unclear. A systematic and comprehensive review of\nfirst-generation EEG-FMs is therefore necessary to understand the current\nstate-of-the-art and identify key directions for future EEG-FMs. To that end,\nthis study reviews 10 early EEG-FMs and presents a critical synthesis of their\nmethodology, empirical findings, and outstanding research gaps. We find that\nmost EEG-FMs adopt a sequence-based modeling scheme that relies on\ntransformer-based backbones and the reconstruction of masked sequences for\nself-supervision. However, model evaluations remain heterogeneous and largely\nlimited, making it challenging to assess their practical off-the-shelf utility.\nIn addition to adopting standardized and realistic evaluations, future work\nshould demonstrate more substantial scaling effects and make principled and\ntrustworthy choices throughout the EEG representation learning pipeline. We\nbelieve that developing benchmarks, software tools, technical methodologies,\nand applications in collaboration with domain experts may further advance the\ntranslational utility and real-world adoption of EEG-FMs.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e8610\u79cd\u65e9\u671f\u81ea\u76d1\u7763\u8111\u7535\u56fe\u57fa\u7840\u6a21\u578b\uff08EEG-FMs\uff09\uff0c\u5206\u6790\u4e86\u5176\u65b9\u6cd5\u3001\u5b9e\u8bc1\u7ed3\u679c\u53ca\u7814\u7a76\u7a7a\u767d\uff0c\u6307\u51fa\u672a\u6765\u9700\u6807\u51c6\u5316\u8bc4\u4f30\u3001\u6269\u5927\u89c4\u6a21\u6548\u5e94\u5e76\u4f18\u5316\u5b66\u4e60\u6d41\u7a0b\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763EEG\u7f16\u7801\u5668\u4f9d\u8d56\u6602\u8d35\u6807\u6ce8\u4e14\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u63a8\u52a8\u81ea\u76d1\u7763EEG-FMs\u7684\u53d1\u5c55\uff0c\u4f46\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u5206\u679010\u79cdEEG-FMs\uff0c\u805a\u7126\u5e8f\u5217\u5efa\u6a21\uff08\u5982Transformer\uff09\u548c\u63a9\u7801\u5e8f\u5217\u91cd\u5efa\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "result": "\u5f53\u524dEEG-FMs\u8bc4\u4f30\u4e0d\u7edf\u4e00\u4e14\u5c40\u9650\uff0c\u5b9e\u7528\u6027\u96be\u4ee5\u8bc4\u4f30\u3002", "conclusion": "\u672a\u6765\u9700\u6807\u51c6\u5316\u8bc4\u4f30\u3001\u6269\u5927\u89c4\u6a21\u6548\u5e94\u3001\u4f18\u5316\u5b66\u4e60\u6d41\u7a0b\uff0c\u5e76\u4e0e\u9886\u57df\u4e13\u5bb6\u5408\u4f5c\u63a8\u52a8EEG-FMs\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.11574", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11574", "abs": "https://arxiv.org/abs/2507.11574", "authors": ["Kazuma Kobayashi", "Shailesh Garg", "Farid Ahmed", "Souvik Chakraborty", "Syed Bahauddin Alam"], "title": "Distribution-Free Uncertainty-Aware Virtual Sensing via Conformalized Neural Operators", "comment": null, "summary": "Robust uncertainty quantification (UQ) remains a critical barrier to the safe\ndeployment of deep learning in real-time virtual sensing, particularly in\nhigh-stakes domains where sparse, noisy, or non-collocated sensor data are the\nnorm. We introduce the Conformalized Monte Carlo Operator (CMCO), a framework\nthat transforms neural operator-based virtual sensing with calibrated,\ndistribution-free prediction intervals. By unifying Monte Carlo dropout with\nsplit conformal prediction in a single DeepONet architecture, CMCO achieves\nspatially resolved uncertainty estimates without retraining, ensembling, or\ncustom loss design. Our method addresses a longstanding challenge: how to endow\noperator learning with efficient and reliable UQ across heterogeneous domains.\nThrough rigorous evaluation on three distinct applications: turbulent flow,\nelastoplastic deformation, and global cosmic radiation dose estimation-CMCO\nconsistently attains near-nominal empirical coverage, even in settings with\nstrong spatial gradients and proxy-based sensing. This breakthrough offers a\ngeneral-purpose, plug-and-play UQ solution for neural operators, unlocking\nreal-time, trustworthy inference in digital twins, sensor fusion, and\nsafety-critical monitoring. By bridging theory and deployment with minimal\ncomputational overhead, CMCO establishes a new foundation for scalable,\ngeneralizable, and uncertainty-aware scientific machine learning.", "AI": {"tldr": "CMCO\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8499\u7279\u5361\u6d1bdropout\u548c\u5206\u5f62\u9884\u6d4b\uff0c\u4e3a\u795e\u7ecf\u7b97\u5b50\u63d0\u4f9b\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5b9a\u5236\u635f\u5931\u8bbe\u8ba1\u7684\u5206\u5e03\u65e0\u5173\u9884\u6d4b\u533a\u95f4\uff0c\u89e3\u51b3\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u5b9e\u65f6\u865a\u62df\u4f20\u611f\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5b9e\u65f6\u865a\u62df\u4f20\u611f\u4e2d\u7a00\u758f\u3001\u566a\u58f0\u6216\u975e\u5171\u4f4d\u4f20\u611f\u5668\u6570\u636e\u5e26\u6765\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6311\u6218\u3002", "method": "\u63d0\u51faConformalized Monte Carlo Operator (CMCO)\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1bdropout\u548c\u5206\u5f62\u9884\u6d4b\uff0c\u5728DeepONet\u67b6\u6784\u4e2d\u5b9e\u73b0\u7a7a\u95f4\u5206\u8fa8\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "result": "\u5728\u6e4d\u6d41\u3001\u5f39\u5851\u6027\u53d8\u5f62\u548c\u5168\u7403\u5b87\u5b99\u8f90\u5c04\u5242\u91cf\u4f30\u8ba1\u4e09\u4e2a\u5e94\u7528\u4e2d\uff0cCMCO\u5b9e\u73b0\u4e86\u63a5\u8fd1\u540d\u4e49\u8986\u76d6\u7387\u7684\u6027\u80fd\u3002", "conclusion": "CMCO\u4e3a\u795e\u7ecf\u7b97\u5b50\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u3001\u5373\u63d2\u5373\u7528\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u53ef\u6269\u5c55\u3001\u6cdb\u5316\u6027\u5f3a\u4e14\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u3002"}}
{"id": "2507.12166", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12166", "abs": "https://arxiv.org/abs/2507.12166", "authors": ["Xiucheng Wang", "Qiming Zhang", "Nan Cheng", "Junting Chen", "Zezhong Zhang", "Zan Li", "Shuguang Cui", "Xuemin Shen"], "title": "RadioDiff-3D: A 3D$\\times$3D Radio Map Dataset and Generative Diffusion Based Benchmark for 6G Environment-Aware Communication", "comment": null, "summary": "Radio maps (RMs) serve as a critical foundation for enabling\nenvironment-aware wireless communication, as they provide the spatial\ndistribution of wireless channel characteristics. Despite recent progress in RM\nconstruction using data-driven approaches, most existing methods focus solely\non pathloss prediction in a fixed 2D plane, neglecting key parameters such as\ndirection of arrival (DoA), time of arrival (ToA), and vertical spatial\nvariations. Such a limitation is primarily due to the reliance on static\nlearning paradigms, which hinder generalization beyond the training data\ndistribution. To address these challenges, we propose UrbanRadio3D, a\nlarge-scale, high-resolution 3D RM dataset constructed via ray tracing in\nrealistic urban environments. UrbanRadio3D is over 37$\\times$3 larger than\nprevious datasets across a 3D space with 3 metrics as pathloss, DoA, and ToA,\nforming a novel 3D$\\times$33D dataset with 7$\\times$3 more height layers than\nprior state-of-the-art (SOTA) dataset. To benchmark 3D RM construction, a UNet\nwith 3D convolutional operators is proposed. Moreover, we further introduce\nRadioDiff-3D, a diffusion-model-based generative framework utilizing the 3D\nconvolutional architecture. RadioDiff-3D supports both radiation-aware\nscenarios with known transmitter locations and radiation-unaware settings based\non sparse spatial observations. Extensive evaluations on UrbanRadio3D validate\nthat RadioDiff-3D achieves superior performance in constructing rich,\nhigh-dimensional radio maps under diverse environmental dynamics. This work\nprovides a foundational dataset and benchmark for future research in 3D\nenvironment-aware communication. The dataset is available at\nhttps://github.com/UNIC-Lab/UrbanRadio3D.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86UrbanRadio3D\u6570\u636e\u96c6\u548cRadioDiff-3D\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u9ad8\u5206\u8fa8\u73873D\u65e0\u7ebf\u7535\u5730\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5782\u76f4\u7a7a\u95f4\u53d8\u5316\u548c\u591a\u53c2\u6570\u9884\u6d4b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u65e0\u7ebf\u7535\u5730\u56fe\u6784\u5efa\u65b9\u6cd5\u5c40\u9650\u4e8e2D\u5e73\u9762\u548c\u9759\u6001\u5b66\u4e60\u8303\u5f0f\uff0c\u5ffd\u7565\u4e86\u65b9\u5411\u5230\u8fbe\uff08DoA\uff09\u3001\u65f6\u95f4\u5230\u8fbe\uff08ToA\uff09\u548c\u5782\u76f4\u7a7a\u95f4\u53d8\u5316\u7b49\u5173\u952e\u53c2\u6570\u3002", "method": "\u901a\u8fc7\u5c04\u7ebf\u8ffd\u8e2a\u6784\u5efaUrbanRadio3D\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8e3D\u5377\u79ef\u7684UNet\u548c\u6269\u6563\u6a21\u578bRadioDiff-3D\uff0c\u652f\u6301\u8f90\u5c04\u611f\u77e5\u548c\u975e\u611f\u77e5\u573a\u666f\u3002", "result": "RadioDiff-3D\u5728UrbanRadio3D\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6784\u5efa\u9ad8\u7ef4\u52a8\u6001\u65e0\u7ebf\u7535\u5730\u56fe\u3002", "conclusion": "UrbanRadio3D\u548cRadioDiff-3D\u4e3a3D\u73af\u5883\u611f\u77e5\u901a\u4fe1\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002"}}
{"id": "2507.11570", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11570", "abs": "https://arxiv.org/abs/2507.11570", "authors": ["Ha Na Cho", "Sairam Sutari", "Alexander Lopez", "Hansen Bow", "Kai Zheng"], "title": "SurgeryLSTM: A Time-Aware Neural Model for Accurate and Explainable Length of Stay Prediction After Spine Surgery", "comment": null, "summary": "Objective: To develop and evaluate machine learning (ML) models for\npredicting length of stay (LOS) in elective spine surgery, with a focus on the\nbenefits of temporal modeling and model interpretability. Materials and\nMethods: We compared traditional ML models (e.g., linear regression, random\nforest, support vector machine (SVM), and XGBoost) with our developed model,\nSurgeryLSTM, a masked bidirectional long short-term memory (BiLSTM) with an\nattention, using structured perioperative electronic health records (EHR) data.\nPerformance was evaluated using the coefficient of determination (R2), and key\npredictors were identified using explainable AI. Results: SurgeryLSTM achieved\nthe highest predictive accuracy (R2=0.86), outperforming XGBoost (R2 = 0.85)\nand baseline models. The attention mechanism improved interpretability by\ndynamically identifying influential temporal segments within preoperative\nclinical sequences, allowing clinicians to trace which events or features most\ncontributed to each LOS prediction. Key predictors of LOS included bone\ndisorder, chronic kidney disease, and lumbar fusion identified as the most\nimpactful predictors of LOS. Discussion: Temporal modeling with attention\nmechanisms significantly improves LOS prediction by capturing the sequential\nnature of patient data. Unlike static models, SurgeryLSTM provides both higher\naccuracy and greater interpretability, which are critical for clinical\nadoption. These results highlight the potential of integrating attention-based\ntemporal models into hospital planning workflows. Conclusion: SurgeryLSTM\npresents an effective and interpretable AI solution for LOS prediction in\nelective spine surgery. Our findings support the integration of temporal,\nexplainable ML approaches into clinical decision support systems to enhance\ndischarge readiness and individualized patient care.", "AI": {"tldr": "\u8bba\u6587\u5f00\u53d1\u4e86SurgeryLSTM\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u810a\u67f1\u624b\u672f\u4f4f\u9662\u65f6\u957f\uff0c\u7ed3\u5408\u65f6\u95f4\u5efa\u6a21\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u63d0\u5347\u810a\u67f1\u624b\u672f\u4f4f\u9662\u65f6\u957f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4ee5\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u6bd4\u8f83\u4f20\u7edf\u6a21\u578b\uff08\u5982\u7ebf\u6027\u56de\u5f52\u3001\u968f\u673a\u68ee\u6797\uff09\u4e0eSurgeryLSTM\uff08\u5e26\u6ce8\u610f\u529b\u7684BiLSTM\uff09\uff0c\u4f7f\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u3002", "result": "SurgeryLSTM\u9884\u6d4b\u51c6\u786e\u7387\u6700\u9ad8\uff08R2=0.86\uff09\uff0c\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002\u5173\u952e\u9884\u6d4b\u56e0\u7d20\u5305\u62ec\u9aa8\u75c5\u548c\u6162\u6027\u80be\u75c5\u3002", "conclusion": "SurgeryLSTM\u4e3a\u810a\u67f1\u624b\u672f\u4f4f\u9662\u65f6\u957f\u9884\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u7cfb\u7edf\u96c6\u6210\u3002"}}
{"id": "2507.11846", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11846", "abs": "https://arxiv.org/abs/2507.11846", "authors": ["Yulu Guo", "Tongjia Zhang", "Xiangwen Gu", "Shu Sun", "Meixia Tao", "Ruifeng Gao"], "title": "Directional Measurements and Analysis for FR3 Low-Altitude Channels in a Campus Environment", "comment": null, "summary": "In this paper, we present detailed low-altitude channel measurements at the\nFR3 band in an outdoor campus environment. Using a time-domain channel sounder\nsystem, we conduct two types of measurements: path loss measurements by moving\nthe transmitter (Tx) at one-meter intervals along a 26-point rooftop path, and\ndirectional power angular spectrum measurements through antenna scanning at\nhalf-power beam width intervals. The path loss analysis across different Rx\nshows that the close-in model outperforms conventional 3GPP models and\nheight-corrected variants, with path loss exponents close to free space values\nindicating line-of-sight dominance. The power angular spectrum measurements\nshow that propagation behavior varies significantly with environmental\nconditions. Closer Rx exhibit stronger sensitivity to ground reflections during\ndownward Tx tilting, while obstructed links display uniform angular\ncharacteristics due to dominant scattering effects, and corridor environments\nproduce asymmetric power distributions. These results indicate that\nlow-altitude propagation is characterized by complex interactions between Tx\nheight and ground scattering mechanisms, providing fundamental insights for\nchannel modeling in emerging mid-band communication systems.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u4f4e\u7a7aFR3\u9891\u6bb5\u4fe1\u9053\u6d4b\u91cf\uff0c\u5206\u6790\u4e86\u8def\u5f84\u635f\u8017\u548c\u529f\u7387\u89d2\u8c31\uff0c\u53d1\u73b0\u8fd1\u8ddd\u79bb\u6a21\u578b\u4f18\u4e8e3GPP\u6a21\u578b\uff0c\u4e14\u4f20\u64ad\u884c\u4e3a\u53d7\u73af\u5883\u6761\u4ef6\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u4f4e\u7a7aFR3\u9891\u6bb5\u5728\u5ba4\u5916\u6821\u56ed\u73af\u5883\u4e2d\u7684\u4fe1\u9053\u7279\u6027\uff0c\u4e3a\u65b0\u5174\u4e2d\u9891\u901a\u4fe1\u7cfb\u7edf\u7684\u4fe1\u9053\u5efa\u6a21\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u65f6\u57df\u4fe1\u9053\u63a2\u6d4b\u7cfb\u7edf\u8fdb\u884c\u8def\u5f84\u635f\u8017\u6d4b\u91cf\u548c\u5b9a\u5411\u529f\u7387\u89d2\u8c31\u6d4b\u91cf\u3002", "result": "\u8fd1\u8ddd\u79bb\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u4f20\u64ad\u884c\u4e3a\u56e0\u73af\u5883\u6761\u4ef6\u800c\u5f02\uff0c\u5730\u9762\u53cd\u5c04\u548c\u6563\u5c04\u6548\u5e94\u663e\u8457\u3002", "conclusion": "\u4f4e\u7a7a\u4f20\u64ad\u53d7\u53d1\u5c04\u9ad8\u5ea6\u548c\u5730\u9762\u6563\u5c04\u673a\u5236\u590d\u6742\u4ea4\u4e92\u5f71\u54cd\uff0c\u4e3a\u4e2d\u9891\u901a\u4fe1\u7cfb\u7edf\u4fe1\u9053\u5efa\u6a21\u63d0\u4f9b\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2507.11706", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11706", "abs": "https://arxiv.org/abs/2507.11706", "authors": ["Taira Tsuchiya", "Shinji Ito", "Haipeng Luo"], "title": "Reinforcement Learning from Adversarial Preferences in Tabular MDPs", "comment": "40 pages", "summary": "We introduce a new framework of episodic tabular Markov decision processes\n(MDPs) with adversarial preferences, which we refer to as preference-based MDPs\n(PbMDPs). Unlike standard episodic MDPs with adversarial losses, where the\nnumerical value of the loss is directly observed, in PbMDPs the learner instead\nobserves preferences between two candidate arms, which represent the choices\nbeing compared. In this work, we focus specifically on the setting where the\nreward functions are determined by Borda scores. We begin by establishing a\nregret lower bound for PbMDPs with Borda scores. As a preliminary step, we\npresent a simple instance to prove a lower bound of $\\Omega(\\sqrt{HSAT})$ for\nepisodic MDPs with adversarial losses, where $H$ is the number of steps per\nepisode, $S$ is the number of states, $A$ is the number of actions, and $T$ is\nthe number of episodes. Leveraging this construction, we then derive a regret\nlower bound of $\\Omega( (H^2 S K)^{1/3} T^{2/3} )$ for PbMDPs with Borda\nscores, where $K$ is the number of arms. Next, we develop algorithms that\nachieve a regret bound of order $T^{2/3}$. We first propose a global\noptimization approach based on online linear optimization over the set of all\noccupancy measures, achieving a regret bound of $\\tilde{O}((H^2 S^2 K)^{1/3}\nT^{2/3} )$ under known transitions. However, this approach suffers from\nsuboptimal dependence on the potentially large number of states $S$ and\ncomputational inefficiency. To address this, we propose a policy optimization\nalgorithm whose regret is roughly bounded by $\\tilde{O}( (H^6 S K^5)^{1/3}\nT^{2/3} )$ under known transitions, and further extend the result to the\nunknown-transition setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u504f\u597d\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08PbMDPs\uff09\u6846\u67b6\uff0c\u7814\u7a76\u4e86Borda\u5206\u6570\u4e0b\u7684\u9057\u61be\u4e0b\u754c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\u4ee5\u5b9e\u73b0\u6b21\u7ebf\u6027\u9057\u61be\u3002", "motivation": "\u4f20\u7edfMDP\u4e2d\u635f\u5931\u51fd\u6570\u76f4\u63a5\u89c2\u6d4b\uff0c\u800cPbMDPs\u901a\u8fc7\u504f\u597d\u6bd4\u8f83\u9009\u62e9\uff0c\u66f4\u8d34\u8fd1\u5b9e\u9645\u573a\u666f\u3002\u7814\u7a76Borda\u5206\u6570\u4e0b\u7684PbMDPs\u586b\u8865\u4e86\u7406\u8bba\u7a7a\u767d\u3002", "method": "1. \u5efa\u7acbBorda\u5206\u6570\u4e0bPbMDPs\u7684\u9057\u61be\u4e0b\u754c\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u5168\u5c40\u4f18\u5316\u7684\u7b97\u6cd5\u548c\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u8bc1\u660e\u4e86\u9057\u61be\u4e0b\u754c\u4e3a\u03a9((H\u00b2SK)^(1/3)T^(2/3))\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5\u5206\u522b\u8fbe\u5230O\u0303((H\u00b2S\u00b2K)^(1/3)T^(2/3))\u548cO\u0303((H\u2076SK\u2075)^(1/3)T^(2/3))\u7684\u9057\u61be\u4e0a\u754c\u3002", "conclusion": "PbMDPs\u5728Borda\u5206\u6570\u4e0b\u5177\u6709\u7406\u8bba\u53ef\u884c\u6027\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5df2\u77e5\u548c\u672a\u77e5\u8f6c\u79fb\u60c5\u51b5\u4e0b\u5747\u6709\u6548\u3002"}}
{"id": "2507.12196", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12196", "abs": "https://arxiv.org/abs/2507.12196", "authors": ["Nikolaos Louloudakis", "Ajitha Rajan"], "title": "Selective Quantization Tuning for ONNX Models", "comment": "5 pages, 3 figures, 2 tables", "summary": "Quantization is a process that reduces the precision of deep neural network\nmodels to lower model size and computational demands, often at the cost of\naccuracy. However, fully quantized models may exhibit sub-optimal performance\nbelow acceptable levels and face deployment challenges on low-end hardware\naccelerators due to practical constraints. To address these issues,\nquantization can be selectively applied to only a subset of layers, but\nselecting which layers to exclude is non-trivial. To this direction, we propose\nTuneQn, a suite enabling selective quantization, deployment and execution of\nONNX models across various CPU and GPU devices, combined with profiling and\nmulti-objective optimization. TuneQn generates selectively quantized ONNX\nmodels, deploys them on different hardware, measures performance on metrics\nlike accuracy and size, performs Pareto Front minimization to identify the best\nmodel candidate and visualizes the results. To demonstrate the effectiveness of\nTuneQn, we evaluated TuneQn on four ONNX models with two quantization settings\nacross CPU and GPU devices. As a result, we demonstrated that our utility\neffectively performs selective quantization and tuning, selecting ONNX model\ncandidates with up to a $54.14$% reduction in accuracy loss compared to the\nfully quantized model, and up to a $72.9$% model size reduction compared to the\noriginal model.", "AI": {"tldr": "TuneQn\u662f\u4e00\u4e2a\u9009\u62e9\u6027\u91cf\u5316\u548c\u4f18\u5316\u5de5\u5177\uff0c\u7528\u4e8e\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u6a21\u578b\u5927\u5c0f\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u5b8c\u5168\u91cf\u5316\u7684\u6a21\u578b\u53ef\u80fd\u5728\u7cbe\u5ea6\u548c\u90e8\u7f72\u4e0a\u5b58\u5728\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u9009\u62e9\u6027\u91cf\u5316\u3002", "method": "\u63d0\u51faTuneQn\u5de5\u5177\uff0c\u7ed3\u5408\u591a\u76ee\u6807\u4f18\u5316\u548c\u6027\u80fd\u5206\u6790\uff0c\u9009\u62e9\u6027\u91cf\u5316ONNX\u6a21\u578b\u5e76\u5728\u4e0d\u540c\u786c\u4ef6\u4e0a\u90e8\u7f72\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cTuneQn\u80fd\u51cf\u5c1154.14%\u7684\u7cbe\u5ea6\u635f\u5931\u548c72.9%\u7684\u6a21\u578b\u5927\u5c0f\u3002", "conclusion": "TuneQn\u6709\u6548\u89e3\u51b3\u4e86\u9009\u62e9\u6027\u91cf\u5316\u548c\u6a21\u578b\u4f18\u5316\u7684\u95ee\u9898\u3002"}}
{"id": "2507.11912", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11912", "abs": "https://arxiv.org/abs/2507.11912", "authors": ["Tzu-Hsuan Chou", "Nicolo Michelusi", "David J. Love", "James V. Krogmeier"], "title": "Joint UAV Placement and Transceiver Design in Multi-User Wireless Relay Networks", "comment": "This paper is accepted for publication in IEEE Transactions on\n  Communications. 17 pages", "summary": "In this paper, a novel approach is proposed to improve the minimum\nsignal-to-interference-plus-noise-ratio (SINR) among users in non-orthogonal\nmulti-user wireless relay networks, by optimizing the placement of unmanned\naerial vehicle (UAV) relays, relay beamforming, and receive combining. The\ndesign is separated into two problems: beamforming-aware UAV placement\noptimization and transceiver design for minimum SINR maximization. A\nsignificant challenge in beamforming-aware UAV placement optimization is the\nlack of instantaneous channel state information (CSI) prior to deploying UAV\nrelays, making it difficult to derive the beamforming SINR in non-orthogonal\nmulti-user transmission. To address this issue, an approximation of the\nexpected beamforming SINR is derived using the narrow beam property of a\nmassive MIMO base station. Based on this, a UAV placement algorithm is proposed\nto provide UAV positions that improve the minimum expected beamforming SINR\namong users, using a difference-of-convex framework. Subsequently, after\ndeploying the UAV relays to the optimized positions, and with estimated CSI\navailable, a joint relay beamforming and receive combining (JRBC) algorithm is\nproposed to optimize the transceiver to improve the minimum beamforming SINR\namong users, using a block-coordinate descent approach. Numerical results show\nthat the UAV placement algorithm combined with the JRBC algorithm provides a\n4.6 dB SINR improvement over state-of-the-art schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u65e0\u4eba\u673a\u4e2d\u7ee7\u653e\u7f6e\u3001\u6ce2\u675f\u6210\u5f62\u548c\u63a5\u6536\u7ec4\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u975e\u6b63\u4ea4\u591a\u7528\u6237\u65e0\u7ebf\u4e2d\u7ee7\u7f51\u7edc\u4e2d\u7684\u6700\u5c0fSINR\u3002", "motivation": "\u89e3\u51b3\u5728\u7f3a\u4e4f\u77ac\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u65e0\u4eba\u673a\u4e2d\u7ee7\u653e\u7f6e\u548c\u6ce2\u675f\u6210\u5f62\u7684\u6311\u6218\u3002", "method": "\u5c06\u8bbe\u8ba1\u5206\u4e3a\u6ce2\u675f\u6210\u5f62\u611f\u77e5\u7684\u65e0\u4eba\u673a\u653e\u7f6e\u4f18\u5316\u548c\u6536\u53d1\u5668\u8bbe\u8ba1\u4e24\u90e8\u5206\uff0c\u5206\u522b\u4f7f\u7528\u5dee\u5206\u51f8\u6846\u67b6\u548c\u5757\u5750\u6807\u4e0b\u964d\u6cd5\u3002", "result": "\u6570\u503c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u6280\u672f\u65b9\u6848\u63d0\u5347\u4e864.6 dB\u7684SINR\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u5408\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u975e\u6b63\u4ea4\u591a\u7528\u6237\u7f51\u7edc\u4e2d\u7684\u6700\u5c0fSINR\u6027\u80fd\u3002"}}
{"id": "2507.11732", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11732", "abs": "https://arxiv.org/abs/2507.11732", "authors": ["Shiyu Chen", "Cencheng Shen", "Youngser Park", "Carey E. Priebe"], "title": "Graph Neural Networks Powered by Encoder Embedding for Improved Node Learning", "comment": null, "summary": "Graph neural networks (GNNs) have emerged as a powerful framework for a wide\nrange of node-level graph learning tasks. However, their performance is often\nconstrained by reliance on random or minimally informed initial feature\nrepresentations, which can lead to slow convergence and suboptimal solutions.\nIn this paper, we leverage a statistically grounded method, one-hot graph\nencoder embedding (GEE), to generate high-quality initial node features that\nenhance the end-to-end training of GNNs. We refer to this integrated framework\nas the GEE-powered GNN (GG), and demonstrate its effectiveness through\nextensive simulations and real-world experiments across both unsupervised and\nsupervised settings. In node clustering, GG consistently achieves\nstate-of-the-art performance, ranking first across all evaluated real-world\ndatasets, while exhibiting faster convergence compared to the standard GNN. For\nnode classification, we further propose an enhanced variant, GG-C, which\nconcatenates the outputs of GG and GEE and outperforms competing baselines.\nThese results confirm the importance of principled, structure-aware feature\ninitialization in realizing the full potential of GNNs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7edf\u8ba1\u65b9\u6cd5\u7684\u56fe\u7f16\u7801\u5d4c\u5165\uff08GEE\uff09\u6765\u521d\u59cb\u5316\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u8282\u70b9\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86GNN\u7684\u6027\u80fd\u548c\u6536\u655b\u901f\u5ea6\u3002", "motivation": "GNN\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u968f\u673a\u6216\u4f4e\u8d28\u91cf\u521d\u59cb\u7279\u5f81\u8868\u793a\uff0c\u5bfc\u81f4\u6536\u655b\u6162\u548c\u6b21\u4f18\u89e3\u3002", "method": "\u5229\u7528GEE\u751f\u6210\u9ad8\u8d28\u91cf\u521d\u59cb\u8282\u70b9\u7279\u5f81\uff0c\u7ed3\u5408GNN\u5f62\u6210GG\u6846\u67b6\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51faGG-C\u53d8\u4f53\u7528\u4e8e\u8282\u70b9\u5206\u7c7b\u3002", "result": "\u5728\u8282\u70b9\u805a\u7c7b\u4efb\u52a1\u4e2d\uff0cGG\u5728\u6240\u6709\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\u4e14\u6536\u655b\u66f4\u5feb\uff1bGG-C\u5728\u8282\u70b9\u5206\u7c7b\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u6784\u611f\u77e5\u7684\u7279\u5f81\u521d\u59cb\u5316\u5bf9\u53d1\u6325GNN\u6f5c\u529b\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.11589", "categories": ["cs.LG", "gr-qc"], "pdf": "https://arxiv.org/pdf/2507.11589", "abs": "https://arxiv.org/abs/2507.11589", "authors": ["Sandeep Suresh Cranganore", "Andrei Bodnar", "Arturs Berzins", "Johannes Brandstetter"], "title": "Einstein Fields: A Neural Perspective To Computational General Relativity", "comment": "63 pages, 22 figures, 10 Tables, Github:\n  https://github.com/AndreiB137/EinFields", "summary": "We introduce Einstein Fields, a neural representation that is designed to\ncompress computationally intensive four-dimensional numerical relativity\nsimulations into compact implicit neural network weights. By modeling the\n\\emph{metric}, which is the core tensor field of general relativity, Einstein\nFields enable the derivation of physical quantities via automatic\ndifferentiation. However, unlike conventional neural fields (e.g., signed\ndistance, occupancy, or radiance fields), Einstein Fields are \\emph{Neural\nTensor Fields} with the key difference that when encoding the spacetime\ngeometry of general relativity into neural field representations, dynamics\nemerge naturally as a byproduct. Einstein Fields show remarkable potential,\nincluding continuum modeling of 4D spacetime, mesh-agnosticity, storage\nefficiency, derivative accuracy, and ease of use. We address these challenges\nacross several canonical test beds of general relativity and release an open\nsource JAX-based library, paving the way for more scalable and expressive\napproaches to numerical relativity. Code is made available at\nhttps://github.com/AndreiB137/EinFields", "AI": {"tldr": "Einstein Fields\u662f\u4e00\u79cd\u795e\u7ecf\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u538b\u7f29\u8ba1\u7b97\u5bc6\u96c6\u578b\u56db\u7ef4\u6570\u503c\u76f8\u5bf9\u8bba\u6a21\u62df\u4e3a\u7d27\u51d1\u7684\u9690\u5f0f\u795e\u7ecf\u7f51\u7edc\u6743\u91cd\u3002\u901a\u8fc7\u5efa\u6a21\u5e7f\u4e49\u76f8\u5bf9\u8bba\u7684\u6838\u5fc3\u5f20\u91cf\u573a\uff08\u5ea6\u91cf\uff09\uff0c\u5b83\u652f\u6301\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u63a8\u5bfc\u7269\u7406\u91cf\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u76f8\u5bf9\u8bba\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cEinstein Fields\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faNeural Tensor Fields\uff08\u795e\u7ecf\u5f20\u91cf\u573a\uff09\uff0c\u5c06\u65f6\u7a7a\u51e0\u4f55\u7f16\u7801\u4e3a\u795e\u7ecf\u7f51\u7edc\u8868\u793a\uff0c\u52a8\u6001\u81ea\u7136\u6d8c\u73b0\u3002", "result": "Einstein Fields\u57284D\u65f6\u7a7a\u8fde\u7eed\u5efa\u6a21\u3001\u5b58\u50a8\u6548\u7387\u3001\u5bfc\u6570\u7cbe\u5ea6\u7b49\u65b9\u9762\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u5e76\u5728\u591a\u4e2a\u5e7f\u4e49\u76f8\u5bf9\u8bba\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u3002", "conclusion": "Einstein Fields\u4e3a\u6570\u503c\u76f8\u5bf9\u8bba\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u8868\u8fbe\u6027\u5f3a\u7684\u5de5\u5177\uff0c\u5f00\u6e90\u5e93\u8fdb\u4e00\u6b65\u63a8\u52a8\u5176\u5e94\u7528\u3002"}}
{"id": "2507.11913", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11913", "abs": "https://arxiv.org/abs/2507.11913", "authors": ["Chen Zhu", "Siyun Liang", "Zhouxiang Zhao", "Jianrong Bao", "Zhaohui Yang", "Zhaoyang Zhang", "Dusit Niyato"], "title": "Scene Graph-Aided Probabilistic Semantic Communication for Image Transmission", "comment": null, "summary": "Semantic communication emphasizes the transmission of meaning rather than raw\nsymbols. It offers a promising solution to alleviate network congestion and\nimprove transmission efficiency. In this paper, we propose a wireless image\ncommunication framework that employs probability graphs as shared semantic\nknowledge base among distributed users. High-level image semantics are\nrepresented via scene graphs, and a two-stage compression algorithm is devised\nto remove predictable components based on learned conditional and co-occurrence\nprobabilities. At the transmitter, the algorithm filters redundant relations\nand entity pairs, while at the receiver, semantic recovery leverages the same\nprobability graphs to reconstruct omitted information. For further research, we\nalso put forward a multi-round semantic compression algorithm with its\ntheoretical performance analysis. Simulation results demonstrate that our\nsemantic-aware scheme achieves superior transmission throughput and satiable\nsemantic alignment, validating the efficacy of leveraging high-level semantics\nfor image communication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u56fe\u7684\u65e0\u7ebf\u56fe\u50cf\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u538b\u7f29\u7b97\u6cd5\u63d0\u5347\u4f20\u8f93\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7f51\u7edc\u62e5\u585e\u548c\u4f20\u8f93\u6548\u7387\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49\u901a\u4fe1\u4f20\u8f93\u610f\u4e49\u800c\u975e\u539f\u59cb\u7b26\u53f7\u3002", "method": "\u4f7f\u7528\u573a\u666f\u56fe\u8868\u793a\u9ad8\u7ea7\u8bed\u4e49\uff0c\u8bbe\u8ba1\u4e24\u9636\u6bb5\u538b\u7f29\u7b97\u6cd5\u53bb\u9664\u53ef\u9884\u6d4b\u90e8\u5206\uff0c\u5229\u7528\u6982\u7387\u56fe\u5171\u4eab\u8bed\u4e49\u77e5\u8bc6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\u65b9\u6848\u5728\u4f20\u8f93\u541e\u5410\u91cf\u548c\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u5229\u7528\u9ad8\u7ea7\u8bed\u4e49\u8fdb\u884c\u56fe\u50cf\u901a\u4fe1\u662f\u6709\u6548\u7684\uff0c\u672a\u6765\u53ef\u7814\u7a76\u591a\u8f6e\u8bed\u4e49\u538b\u7f29\u7b97\u6cd5\u3002"}}
{"id": "2507.11847", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11847", "abs": "https://arxiv.org/abs/2507.11847", "authors": ["Yu-Jie Zhang", "Sheng-An Xu", "Peng Zhao", "Masashi Sugiyama"], "title": "Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update", "comment": null, "summary": "We study the generalized linear bandit (GLB) problem, a contextual\nmulti-armed bandit framework that extends the classical linear model by\nincorporating a non-linear link function, thereby modeling a broad class of\nreward distributions such as Bernoulli and Poisson. While GLBs are widely\napplicable to real-world scenarios, their non-linear nature introduces\nsignificant challenges in achieving both computational and statistical\nefficiency. Existing methods typically trade off between two objectives, either\nincurring high per-round costs for optimal regret guarantees or compromising\nstatistical efficiency to enable constant-time updates. In this paper, we\npropose a jointly efficient algorithm that attains a nearly optimal regret\nbound with $\\mathcal{O}(1)$ time and space complexities per round. The core of\nour method is a tight confidence set for the online mirror descent (OMD)\nestimator, which is derived through a novel analysis that leverages the notion\nof mix loss from online prediction. The analysis shows that our OMD estimator,\neven with its one-pass updates, achieves statistical efficiency comparable to\nmaximum likelihood estimation, thereby leading to a jointly efficient\noptimistic method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u9ad8\u6548\u7684\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\uff08GLB\uff09\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u9057\u61be\u8fb9\u754c\uff0c\u4e14\u6bcf\u8f6e\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3a\u5e38\u6570\u3002", "motivation": "\u5e7f\u4e49\u7ebf\u6027\u8d4c\u535a\u673a\uff08GLB\uff09\u56e0\u5176\u975e\u7ebf\u6027\u7279\u6027\u5728\u8ba1\u7b97\u548c\u7edf\u8ba1\u6548\u7387\u4e0a\u5b58\u5728\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u5728\u4e24\u8005\u95f4\u6743\u8861\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u955c\u50cf\u4e0b\u964d\uff08OMD\uff09\u4f30\u8ba1\u5668\u6784\u5efa\u7d27\u7f6e\u4fe1\u96c6\uff0c\u5229\u7528\u6df7\u5408\u635f\u5931\u7684\u65b0\u9896\u5206\u6790\u5b9e\u73b0\u7edf\u8ba1\u6548\u7387\u3002", "result": "\u7b97\u6cd5\u5728\u6bcf\u8f6e\u65f6\u95f4\u548c\u7a7a\u95f4\u590d\u6742\u5ea6\u4e3a\u5e38\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u9057\u61be\u8fb9\u754c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u548c\u7edf\u8ba1\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2507.11590", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11590", "abs": "https://arxiv.org/abs/2507.11590", "authors": ["Raju Challagundla", "Mohsen Dorodchi", "Pu Wang", "Minwoo Lee"], "title": "Synthetic Tabular Data Generation: A Comparative Survey for Modern Techniques", "comment": null, "summary": "As privacy regulations become more stringent and access to real-world data\nbecomes increasingly constrained, synthetic data generation has emerged as a\nvital solution, especially for tabular datasets, which are central to domains\nlike finance, healthcare and the social sciences. This survey presents a\ncomprehensive and focused review of recent advances in synthetic tabular data\ngeneration, emphasizing methods that preserve complex feature relationships,\nmaintain statistical fidelity, and satisfy privacy requirements. A key\ncontribution of this work is the introduction of a novel taxonomy based on\npractical generation objectives, including intended downstream applications,\nprivacy guarantees, and data utility, directly informing methodological design\nand evaluation strategies. Therefore, this review prioritizes the actionable\ngoals that drive synthetic data creation, including conditional generation and\nrisk-sensitive modeling. Additionally, the survey proposes a benchmark\nframework to align technical innovation with real-world demands. By bridging\ntheoretical foundations with practical deployment, this work serves as both a\nroadmap for future research and a guide for implementing synthetic tabular data\nin privacy-critical environments.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u5408\u6210\u8868\u683c\u6570\u636e\u751f\u6210\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u63d0\u51fa\u57fa\u4e8e\u5b9e\u7528\u76ee\u6807\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u51c6\u6846\u67b6\u4ee5\u8fde\u63a5\u7406\u8bba\u4e0e\u5b9e\u9645\u9700\u6c42\u3002", "motivation": "\u9690\u79c1\u6cd5\u89c4\u8d8b\u4e25\uff0c\u771f\u5b9e\u6570\u636e\u83b7\u53d6\u53d7\u9650\uff0c\u5408\u6210\u6570\u636e\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u8868\u683c\u6570\u636e\u9886\u57df\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u751f\u6210\u76ee\u6807\u7684\u5206\u7c7b\u6cd5\uff0c\u5f3a\u8c03\u65b9\u6cd5\u9700\u4fdd\u6301\u7279\u5f81\u5173\u7cfb\u3001\u7edf\u8ba1\u4fdd\u771f\u548c\u9690\u79c1\u8981\u6c42\u3002", "result": "\u63d0\u51fa\u4e86\u57fa\u51c6\u6846\u67b6\uff0c\u8fde\u63a5\u6280\u672f\u521b\u65b0\u4e0e\u5b9e\u9645\u9700\u6c42\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u9690\u79c1\u5173\u952e\u73af\u5883\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e3a\u5408\u6210\u8868\u683c\u6570\u636e\u7684\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u8def\u7ebf\u56fe\u548c\u6307\u5357\u3002"}}
{"id": "2507.11919", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11919", "abs": "https://arxiv.org/abs/2507.11919", "authors": ["Wei Zhou", "Wei-Jian Li", "Wei-Xin Ren"], "title": "STFT-based Time-Frequency Mode Decomposition: A Fast and Robust Method for Multicomponent Signal Analysis", "comment": null, "summary": "The decomposition of complex, multicomponent, and non-stationary signals into\ntheir constituent modes is a fundamental yet significant challenge in science\nand engineering. Existing methods often struggle with a trade-off among\naccuracy, computational cost, and the need for prior information such as the\nnumber of modes. This paper introduces time-frequency mode decomposition\n(TFMD), a novel framework for the fast, robust, and adaptive decomposition of\nsuch signals. TFMD operates on the principle that modes form contiguous\nhigh-energy regions in the time-frequency domain. Its non-iterative pipeline\nreframes signal decomposition as an image segmentation task: a signal is\ntransformed into a spectrogram, which is then smoothed to enhance the\ncontinuity of these high-energy regions. A sequence of adaptive thresholding\nand connected-component labeling with size-based filtering is then employed to\nautomatically segment the spectrogram and generate a mask for each mode. The\nmodes are finally reconstructed via the inverse short-time Fourier transform.\nValidation on diverse synthetic signals demonstrates that TFMD accurately\ndetermines the number of modes and reconstructs them with high fidelity. Its\nperformance is particularly strong in high-noise conditions. A comparative\nanalysis confirms that TFMD provides robust, competitive performance across a\nwider variety of signal types, while a theoretical complexity analysis reveals\nits superior computational efficiency stemming from its non-iterative design.\nThe method's practical utility is further demonstrated by successfully\nextracting modal responses from a real-world footbridge vibration signal. TFMD\nprovides a computationally efficient and powerful paradigm for multicomponent\nsignal analysis, offering a compelling balance of accuracy, versatility, and\nefficiency for large-scale or time-sensitive applications.", "AI": {"tldr": "TFMD\u662f\u4e00\u79cd\u65b0\u578b\u65f6\u9891\u6a21\u6001\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u5206\u5272\u6280\u672f\u9ad8\u6548\u5206\u89e3\u590d\u6742\u4fe1\u53f7\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u7cbe\u5ea6\u3001\u8ba1\u7b97\u6210\u672c\u548c\u5148\u9a8c\u4fe1\u606f\u9700\u6c42\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u5c06\u4fe1\u53f7\u8f6c\u6362\u4e3a\u65f6\u9891\u8c31\uff0c\u901a\u8fc7\u5e73\u6ed1\u3001\u81ea\u9002\u5e94\u9608\u503c\u548c\u8fde\u901a\u533a\u57df\u6807\u8bb0\u5206\u5272\u6a21\u6001\uff0c\u6700\u540e\u9006\u53d8\u6362\u91cd\u6784\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4fe1\u53f7\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u9ad8\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "conclusion": "TFMD\u4e3a\u591a\u5206\u91cf\u4fe1\u53f7\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7cbe\u786e\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12257", "categories": ["cs.LG", "physics.data-an", "stat.ML", "stat.OT"], "pdf": "https://arxiv.org/pdf/2507.12257", "abs": "https://arxiv.org/abs/2507.12257", "authors": ["Matteo Tusoni", "Giuseppe Masi", "Andrea Coletta", "Aldo Glielmo", "Viviana Arrigoni", "Novella Bartolini"], "title": "Robust Causal Discovery in Real-World Time Series with Power-Laws", "comment": null, "summary": "Exploring causal relationships in stochastic time series is a challenging yet\ncrucial task with a vast range of applications, including finance, economics,\nneuroscience, and climate science. Many algorithms for Causal Discovery (CD)\nhave been proposed, but they often exhibit a high sensitivity to noise,\nresulting in misleading causal inferences when applied to real data. In this\npaper, we observe that the frequency spectra of typical real-world time series\nfollow a power-law distribution, notably due to an inherent self-organizing\nbehavior. Leveraging this insight, we build a robust CD method based on the\nextraction of power -law spectral features that amplify genuine causal signals.\nOur method consistently outperforms state-of-the-art alternatives on both\nsynthetic benchmarks and real-world datasets with known causal structures,\ndemonstrating its robustness and practical relevance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e42\u5f8b\u8c31\u7279\u5f81\u7684\u9c81\u68d2\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u968f\u673a\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u6613\u5bfc\u81f4\u8bef\u5bfc\u6027\u63a8\u65ad\u3002", "method": "\u5229\u7528\u771f\u5b9e\u65f6\u95f4\u5e8f\u5217\u7684\u5e42\u5f8b\u8c31\u5206\u5e03\u7279\u5f81\uff0c\u63d0\u53d6\u653e\u5927\u771f\u5b9e\u56e0\u679c\u4fe1\u53f7\u7684\u8c31\u7279\u5f81\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u5df2\u77e5\u56e0\u679c\u7ed3\u6784\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.11620", "categories": ["cs.LG", "astro-ph.HE", "astro-ph.IM", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11620", "abs": "https://arxiv.org/abs/2507.11620", "authors": ["Steven Dillmann", "Juan Rafael Mart\u00ednez-Galarza"], "title": "Learning Representations of Event Time Series with Sparse Autoencoders for Anomaly Detection, Similarity Search, and Unsupervised Classification", "comment": "Accepted at the 2025 ICML Workshop on Machine Learning for\n  Astrophysics, Code available at:\n  https://github.com/StevenDillmann/ml-xraytransients-mnras", "summary": "Event time series are sequences of discrete events occurring at irregular\ntime intervals, each associated with a domain-specific observational modality.\nThey are common in domains such as high-energy astrophysics, computational\nsocial science, cybersecurity, finance, healthcare, neuroscience, and\nseismology. Their unstructured and irregular structure poses significant\nchallenges for extracting meaningful patterns and identifying salient phenomena\nusing conventional techniques. We propose novel two- and three-dimensional\ntensor representations for event time series, coupled with sparse autoencoders\nthat learn physically meaningful latent representations. These embeddings\nsupport a variety of downstream tasks, including anomaly detection,\nsimilarity-based retrieval, semantic clustering, and unsupervised\nclassification. We demonstrate our approach on a real-world dataset from X-ray\nastronomy, showing that these representations successfully capture temporal and\nspectral signatures and isolate diverse classes of X-ray transients. Our\nframework offers a flexible, scalable, and generalizable solution for analyzing\ncomplex, irregular event time series across scientific and industrial domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f20\u91cf\u8868\u793a\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u89c4\u5219\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\uff0c\u652f\u6301\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "motivation": "\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u5e38\u89c1\uff0c\u4f46\u5176\u4e0d\u89c4\u5219\u6027\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u63d0\u53d6\u6709\u610f\u4e49\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u4e8c\u7ef4\u548c\u4e09\u7ef4\u5f20\u91cf\u8868\u793a\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\uff0c\u7ed3\u5408\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7269\u7406\u610f\u4e49\u660e\u786e\u7684\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5728X\u5c04\u7ebf\u5929\u6587\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u6210\u529f\u6355\u83b7\u4e86\u65f6\u95f4\u548c\u5149\u8c31\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5206\u6790\u548c\u5904\u7406\u590d\u6742\u4e8b\u4ef6\u65f6\u95f4\u5e8f\u5217\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12000", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12000", "abs": "https://arxiv.org/abs/2507.12000", "authors": ["Jiahong Ning", "Ce Zheng", "Tingting Yang"], "title": "DSSD: Efficient Edge-Device Deployment and Collaborative Inference via Distributed Split Speculative Decoding", "comment": "ICML 2025", "summary": "Large language models (LLMs) have transformed natural language processing but\nface critical deployment challenges in device-edge systems due to resource\nlimitations and communication overhead. To address these issues, collaborative\nframeworks have emerged that combine small language models (SLMs) on devices\nwith LLMs at the edge, using speculative decoding (SD) to improve efficiency.\nHowever, existing solutions often trade inference accuracy for latency or\nsuffer from high uplink transmission costs when verifying candidate tokens. In\nthis paper, we propose Distributed Split Speculative Decoding (DSSD), a novel\narchitecture that not only preserves the SLM-LLM split but also partitions the\nverification phase between the device and edge. In this way, DSSD replaces the\nuplink transmission of multiple vocabulary distributions with a single downlink\ntransmission, significantly reducing communication latency while maintaining\ninference quality. Experiments show that our solution outperforms current\nmethods, and codes are at:\nhttps://github.com/JasonNing96/DSSD-Efficient-Edge-Computing", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u5206\u5272\u63a8\u6d4b\u89e3\u7801\uff08DSSD\uff09\u67b6\u6784\uff0c\u901a\u8fc7\u8bbe\u5907\u4e0e\u8fb9\u7f18\u534f\u4f5c\u51cf\u5c11\u901a\u4fe1\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bbe\u5907-\u8fb9\u7f18\u7cfb\u7edf\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u8d44\u6e90\u9650\u5236\u548c\u901a\u4fe1\u5f00\u9500\u95ee\u9898\u3002", "method": "\u63d0\u51faDSSD\u67b6\u6784\uff0c\u5c06\u9a8c\u8bc1\u9636\u6bb5\u5206\u5272\u5230\u8bbe\u5907\u548c\u8fb9\u7f18\uff0c\u51cf\u5c11\u4e0a\u884c\u4f20\u8f93\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDSSD\u5728\u51cf\u5c11\u901a\u4fe1\u5ef6\u8fdf\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u8d28\u91cf\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DSSD\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u8bbe\u5907-\u8fb9\u7f18\u534f\u4f5c\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u90e8\u7f72\u6548\u7387\u3002"}}
{"id": "2507.12262", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.12262", "abs": "https://arxiv.org/abs/2507.12262", "authors": ["Zachary James", "Joseph Guinness"], "title": "A Framework for Nonstationary Gaussian Processes with Neural Network Parameters", "comment": null, "summary": "Gaussian processes have become a popular tool for nonparametric regression\nbecause of their flexibility and uncertainty quantification. However, they\noften use stationary kernels, which limit the expressiveness of the model and\nmay be unsuitable for many datasets. We propose a framework that uses\nnonstationary kernels whose parameters vary across the feature space, modeling\nthese parameters as the output of a neural network that takes the features as\ninput. The neural network and Gaussian process are trained jointly using the\nchain rule to calculate derivatives. Our method clearly describes the behavior\nof the nonstationary parameters and is compatible with approximation methods\nfor scaling to large datasets. It is flexible and easily adapts to different\nnonstationary kernels without needing to redesign the optimization procedure.\nOur methods are implemented with the GPyTorch library and can be readily\nmodified. We test a nonstationary variance and noise variant of our method on\nseveral machine learning datasets and find that it achieves better accuracy and\nlog-score than both a stationary model and a hierarchical model approximated\nwith variational inference. Similar results are observed for a model with only\nnonstationary variance. We also demonstrate our approach's ability to recover\nthe nonstationary parameters of a spatial dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u975e\u5e73\u7a33\u6838\u7684\u9ad8\u65af\u8fc7\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u6838\u53c2\u6570\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u9ad8\u65af\u8fc7\u7a0b\u4f7f\u7528\u5e73\u7a33\u6838\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u96be\u4ee5\u9002\u5e94\u590d\u6742\u6570\u636e\u96c6\u3002", "method": "\u5c06\u975e\u5e73\u7a33\u6838\u53c2\u6570\u5efa\u6a21\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\uff0c\u8054\u5408\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u548c\u9ad8\u65af\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5e73\u7a33\u6a21\u578b\u548c\u53d8\u5206\u63a8\u65ad\u5c42\u6b21\u6a21\u578b\uff0c\u4e14\u80fd\u6709\u6548\u6062\u590d\u975e\u5e73\u7a33\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7075\u6d3b\u3001\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u975e\u5e73\u7a33\u6838\uff0c\u4e14\u6613\u4e8e\u6269\u5c55\u5230\u5927\u89c4\ufffd\ufffd\u6570\u636e\u96c6\u3002"}}
{"id": "2507.11639", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11639", "abs": "https://arxiv.org/abs/2507.11639", "authors": ["Fouad Oubari", "Raphael Meunier", "Rodrigue D\u00e9catoire", "Mathilde Mougeot"], "title": "Deep Generative Methods and Tire Architecture Design", "comment": null, "summary": "As deep generative models proliferate across the AI landscape, industrial\npractitioners still face critical yet unanswered questions about which deep\ngenerative models best suit complex manufacturing design tasks. This work\naddresses this question through a complete study of five representative models\n(Variational Autoencoder, Generative Adversarial Network, multimodal\nVariational Autoencoder, Denoising Diffusion Probabilistic Model, and\nMultinomial Diffusion Model) on industrial tire architecture generation. Our\nevaluation spans three key industrial scenarios: (i) unconditional generation\nof complete multi-component designs, (ii) component-conditioned generation\n(reconstructing architectures from partial observations), and (iii)\ndimension-constrained generation (creating designs that satisfy specific\ndimensional requirements). To enable discrete diffusion models to handle\nconditional scenarios, we introduce categorical inpainting, a mask-aware\nreverse diffusion process that preserves known labels without requiring\nadditional training. Our evaluation employs geometry-aware metrics specifically\ncalibrated for industrial requirements, quantifying spatial coherence,\ncomponent interaction, structural connectivity, and perceptual fidelity. Our\nfindings reveal that diffusion models achieve the strongest overall\nperformance; a masking-trained VAE nonetheless outperforms the multimodal\nvariant MMVAE\\textsuperscript{+} on nearly all component-conditioned metrics,\nand within the diffusion family MDM leads in-distribution whereas DDPM\ngeneralises better to out-of-distribution dimensional constraints.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e94\u79cd\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u5728\u5de5\u4e1a\u8f6e\u80ce\u67b6\u6784\u751f\u6210\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6269\u6563\u6a21\u578b\u6574\u4f53\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u7c7b\u522b\u4fee\u590d\u65b9\u6cd5\u3002", "motivation": "\u5de5\u4e1a\u5b9e\u8df5\u4e2d\u7f3a\u4e4f\u5173\u4e8e\u54ea\u79cd\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u6700\u9002\u5408\u590d\u6742\u5236\u9020\u8bbe\u8ba1\u4efb\u52a1\u7684\u660e\u786e\u6307\u5bfc\u3002", "method": "\u7814\u7a76\u4e86\u4e94\u79cd\u6a21\u578b\uff08VAE\u3001GAN\u3001MMVAE\u3001DDPM\u3001MDM\uff09\uff0c\u5e76\u5f15\u5165\u7c7b\u522b\u4fee\u590d\u65b9\u6cd5\u5904\u7406\u6761\u4ef6\u751f\u6210\u3002", "result": "\u6269\u6563\u6a21\u578b\u6574\u4f53\u8868\u73b0\u6700\u597d\uff0cMDM\u5728\u5206\u5e03\u5185\u8868\u73b0\u4f18\u5f02\uff0cDDPM\u5728\u5206\u5e03\u5916\u7ea6\u675f\u4e0b\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u5728\u5de5\u4e1a\u8f6e\u80ce\u67b6\u6784\u751f\u6210\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u7c7b\u522b\u4fee\u590d\u65b9\u6cd5\u6709\u6548\u652f\u6301\u6761\u4ef6\u751f\u6210\u3002"}}
{"id": "2507.12010", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12010", "abs": "https://arxiv.org/abs/2507.12010", "authors": ["Julia Beuster", "Carsten Andrich", "Sebastian Giehl", "Marc Miranda", "Lorenz Mohr", "Dieter Novotny", "Tom Kaufmann", "Christian Schneider", "Reiner Thom\u00e4"], "title": "Enhancing Situational Awareness in ISAC Networks via Drone Swarms: A Real-World Channel Sounding Data Set", "comment": null, "summary": "With the upcoming capabilities of integrated sensing and communication (ISAC)\nand the incorporation of user equipment (UE) like unmanned aerial vehicles\n(UAVs) in 6G mobile networks, there is a significant opportunity to enhance\nsituational awareness through multi-static radar sensing in meshed ISAC\nnetworks. This paper presents a real-world channel sounding data set acquired\nusing a testbed with synchronized, distributed ground-based sensor nodes and\nflying sensor nodes within a swarm of up to four drones. The conducted\nmeasurement campaign is designed to sense the bi-static reflectivity of objects\nsuch as parking cars, vertical take-off and landing (VTOL) aircraft, and small\ndrones in multi-path environments. We detail the rationale behind the selection\nof the included scenarios and the configuration of the participating nodesand\npresent exemplary results to demonstrate the potential of using collaborating\ndrone swarms for multi-static radar tracking and localization in air-to-air\n(A2A) and air-to-ground (A2G) scenarios. The data sets are publicly available\nto support the development and validation of future ISAC algorithms in\nreal-world environments rather than relying solely on simulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e6G\u7f51\u7edc\u4e2d\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u548c\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u591a\u9759\u6001\u96f7\u8fbe\u611f\u77e5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9e\u9645\u6d4b\u91cf\u6570\u636e\u9a8c\u8bc1\u4e86\u5176\u5728\u7a7a\u5bf9\u7a7a\uff08A2A\uff09\u548c\u7a7a\u5bf9\u5730\uff08A2G\uff09\u573a\u666f\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u5229\u75286G\u7f51\u7edc\u4e2dISAC\u548c\u65e0\u4eba\u673a\u7684\u534f\u540c\u80fd\u529b\uff0c\u63d0\u5347\u591a\u9759\u6001\u96f7\u8fbe\u611f\u77e5\u7684\u60c5\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5206\u5e03\u5f0f\u5730\u9762\u548c\u98de\u884c\u4f20\u611f\u5668\u8282\u70b9\u7ec4\u6210\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u91c7\u96c6\u591a\u8def\u5f84\u73af\u5883\u4e2d\u7684\u53cc\u9759\u6001\u53cd\u5c04\u7387\u6570\u636e\u3002", "result": "\u5c55\u793a\u4e86\u65e0\u4eba\u673a\u7fa4\u5728\u591a\u9759\u6001\u96f7\u8fbe\u8ddf\u8e2a\u548c\u5b9a\u4f4d\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765ISAC\u7b97\u6cd5\u7684\u5f00\u53d1\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9645\u6d4b\u91cf\u9a8c\u8bc1\u4e86\u65e0\u4eba\u673a\u7fa4\u5728ISAC\u7f51\u7edc\u4e2d\u7684\u591a\u9759\u6001\u96f7\u8fbe\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u672a\u6765\u7b97\u6cd5\u5f00\u53d1\u63d0\u4f9b\u4e86\u771f\u5b9e\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2507.12399", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.12399", "abs": "https://arxiv.org/abs/2507.12399", "authors": ["Florian E. Dorner", "Yatong Chen", "Andr\u00e9 F. Cruz", "Fanny Yang"], "title": "ROC-n-reroll: How verifier imperfection affects test-time scaling", "comment": "35 pages, 9 Figures", "summary": "Test-time scaling aims to improve language model performance by leveraging\nadditional compute during inference. While many works have empirically studied\ntechniques like Best-of-N (BoN) and rejection sampling that make use of a\nverifier to enable test-time scaling, there is little theoretical understanding\nof how verifier imperfection affects performance. In this work, we address this\ngap. Specifically, we prove how instance-level accuracy of these methods is\nprecisely characterized by the geometry of the verifier's ROC curve.\nInterestingly, while scaling is determined by the local geometry of the ROC\ncurve for rejection sampling, it depends on global properties of the ROC curve\nfor BoN. As a consequence when the ROC curve is unknown, it is impossible to\nextrapolate the performance of rejection sampling based on the low-compute\nregime. Furthermore, while rejection sampling outperforms BoN for fixed\ncompute, in the infinite-compute limit both methods converge to the same level\nof accuracy, determined by the slope of the ROC curve near the origin. Our\ntheoretical results are confirmed by experiments on GSM8K using different\nversions of Llama and Qwen to generate and verify solutions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6d4b\u8bd5\u65f6\u6269\u5c55\u6280\u672f\uff08\u5982Best-of-N\u548c\u62d2\u7edd\u91c7\u6837\uff09\u5728\u9a8c\u8bc1\u5668\u4e0d\u5b8c\u7f8e\u65f6\u7684\u6027\u80fd\u5f71\u54cd\uff0c\u53d1\u73b0\u5176\u51c6\u786e\u6027\u4e0e\u9a8c\u8bc1\u5668ROC\u66f2\u7ebf\u7684\u51e0\u4f55\u7279\u6027\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u9a8c\u8bc1\u5668\u4e0d\u5b8c\u7f8e\u5982\u4f55\u5f71\u54cd\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\u80fd\u7684\u7406\u8bba\u7406\u89e3\uff0c\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u9a8c\u8bc1\u5668\u7684ROC\u66f2\u7ebf\u51e0\u4f55\u7279\u6027\uff0c\u7814\u7a76\u5176\u5bf9Best-of-N\u548c\u62d2\u7edd\u91c7\u6837\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u62d2\u7edd\u91c7\u6837\u5728\u56fa\u5b9a\u8ba1\u7b97\u91cf\u4e0b\u4f18\u4e8eBest-of-N\uff0c\u4f46\u5728\u65e0\u9650\u8ba1\u7b97\u91cf\u4e0b\u4e24\u8005\u6027\u80fd\u8d8b\u540c\uff0c\u53d6\u51b3\u4e8eROC\u66f2\u7ebf\u539f\u70b9\u9644\u8fd1\u7684\u659c\u7387\u3002", "conclusion": "\u9a8c\u8bc1\u5668\u7684ROC\u66f2\u7ebf\u51e0\u4f55\u7279\u6027\u662f\u51b3\u5b9a\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u62d2\u7edd\u91c7\u6837\u548cBest-of-N\u5728\u4e0d\u540c\u8ba1\u7b97\u91cf\u4e0b\u8868\u73b0\u4e0d\u540c\u3002"}}
{"id": "2507.11645", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11645", "abs": "https://arxiv.org/abs/2507.11645", "authors": ["Ahmed Salah", "David Yevick"], "title": "Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation", "comment": "15 pages, 11 figures", "summary": "Grokking refers to delayed generalization in which the increase in test\naccuracy of a neural network occurs appreciably after the improvement in\ntraining accuracy This paper introduces several practical metrics including\nvariance under dropout, robustness, embedding similarity, and sparsity\nmeasures, that can forecast grokking behavior. Specifically, the resilience of\nneural networks to noise during inference is estimated from a Dropout\nRobustness Curve (DRC) obtained from the variation of the accuracy with the\ndropout rate as the model transitions from memorization to generalization. The\nvariance of the test accuracy under stochastic dropout across training\ncheckpoints further exhibits a local maximum during the grokking. Additionally,\nthe percentage of inactive neurons decreases during generalization, while the\nembeddings tend to a bimodal distribution independent of initialization that\ncorrelates with the observed cosine similarity patterns and dataset symmetries.\nThese metrics additionally provide valuable insight into the origin and\nbehaviour of grokking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u51e0\u79cd\u5b9e\u7528\u6307\u6807\uff08\u5982Dropout\u9c81\u68d2\u6027\u3001\u5d4c\u5165\u76f8\u4f3c\u6027\u548c\u7a00\u758f\u6027\uff09\u6765\u9884\u6d4b\u795e\u7ecf\u7f51\u7edc\u7684\u201c\u987f\u609f\u201d\u884c\u4e3a\uff0c\u5e76\u63ed\u793a\u5176\u8d77\u6e90\u548c\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u540e\u671f\u7a81\u7136\u8868\u73b0\u63d0\u5347\uff08\u987f\u609f\uff09\u7684\u73b0\u8c61\uff0c\u5e76\u5bfb\u627e\u53ef\u9884\u6d4b\u548c\u89e3\u91ca\u8be5\u884c\u4e3a\u7684\u6307\u6807\u3002", "method": "\u901a\u8fc7Dropout\u9c81\u68d2\u6027\u66f2\u7ebf\uff08DRC\uff09\u3001\u6d4b\u8bd5\u7cbe\u5ea6\u65b9\u5dee\u3001\u795e\u7ecf\u5143\u6fc0\u6d3b\u7387\u548c\u5d4c\u5165\u5206\u5e03\u7b49\u6307\u6807\u5206\u6790\u987f\u609f\u884c\u4e3a\u3002", "result": "\u53d1\u73b0\u987f\u609f\u671f\u95f4\u6d4b\u8bd5\u7cbe\u5ea6\u65b9\u5dee\u51fa\u73b0\u5c40\u90e8\u5cf0\u503c\uff0c\u795e\u7ecf\u5143\u6fc0\u6d3b\u7387\u589e\u52a0\uff0c\u5d4c\u5165\u5206\u5e03\u8d8b\u4e8e\u53cc\u5cf0\u4e14\u4e0e\u6570\u636e\u96c6\u5bf9\u79f0\u6027\u76f8\u5173\u3002", "conclusion": "\u63d0\u51fa\u7684\u6307\u6807\u80fd\u6709\u6548\u9884\u6d4b\u987f\u609f\u884c\u4e3a\uff0c\u5e76\u63ed\u793a\u5176\u4e0e\u7f51\u7edc\u9c81\u68d2\u6027\u548c\u6570\u636e\u7ed3\u6784\u7684\u5173\u8054\u3002"}}
{"id": "2507.12132", "categories": ["eess.SP", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12132", "abs": "https://arxiv.org/abs/2507.12132", "authors": ["Navid Hasanzadeh", "Shahrokh Valaee"], "title": "DoRF: Doppler Radiance Fields for Robust Human Activity Recognition Using Wi-Fi", "comment": null, "summary": "Wi-Fi Channel State Information (CSI) has gained increasing interest for\nremote sensing applications. Recent studies show that Doppler velocity\nprojections extracted from CSI can enable human activity recognition (HAR) that\nis robust to environmental changes and generalizes to new users. However,\ndespite these advances, generalizability still remains insufficient for\npractical deployment. Inspired by neural radiance fields (NeRF), which learn a\nvolumetric representation of a 3D scene from 2D images, this work proposes a\nnovel approach to reconstruct an informative 3D latent motion representation\nfrom one-dimensional Doppler velocity projections extracted from Wi-Fi CSI. The\nresulting latent representation is then used to construct a uniform Doppler\nradiance field (DoRF) of the motion, providing a comprehensive view of the\nperformed activity and improving the robustness to environmental variability.\nThe results show that the proposed approach noticeably enhances the\ngeneralization accuracy of Wi-Fi-based HAR, highlighting the strong potential\nof DoRFs for practical sensing applications.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWi-Fi CSI\u7684\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u65b9\u6cd5\uff0c\u901a\u8fc73D\u6f5c\u5728\u8fd0\u52a8\u8868\u793a\u548c\u5747\u5300\u591a\u666e\u52d2\u8f90\u5c04\u573a\uff08DoRF\uff09\u63d0\u5347\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1Wi-Fi CSI\u5728\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u6cdb\u5316\u80fd\u529b\u4ecd\u4e0d\u8db3\u4ee5\u5b9e\u9645\u5e94\u7528\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u53d7NeRF\u542f\u53d1\uff0c\u4ece\u4e00\u7ef4\u591a\u666e\u52d2\u901f\u5ea6\u6295\u5f71\u91cd\u5efa3D\u6f5c\u5728\u8fd0\u52a8\u8868\u793a\uff0c\u6784\u5efa\u5747\u5300\u591a\u666e\u52d2\u8f90\u5c04\u573a\uff08DoRF\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86Wi-Fi HAR\u7684\u6cdb\u5316\u51c6\u786e\u6027\u3002", "conclusion": "DoRF\u5728\u5b9e\u7528\u4f20\u611f\u5e94\u7528\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.11649", "categories": ["cs.LG", "cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11649", "abs": "https://arxiv.org/abs/2507.11649", "authors": ["Daniel Commey", "Benjamin Appiah", "Griffith S. Klogo", "Garth V. Crosby"], "title": "ZKP-FedEval: Verifiable and Privacy-Preserving Federated Evaluation using Zero-Knowledge Proofs", "comment": null, "summary": "Federated Learning (FL) enables collaborative model training on decentralized\ndata without exposing raw data. However, the evaluation phase in FL may leak\nsensitive information through shared performance metrics. In this paper, we\npropose a novel protocol that incorporates Zero-Knowledge Proofs (ZKPs) to\nenable privacy-preserving and verifiable evaluation for FL. Instead of\nrevealing raw loss values, clients generate a succinct proof asserting that\ntheir local loss is below a predefined threshold. Our approach is implemented\nwithout reliance on external APIs, using self-contained modules for federated\nlearning simulation, ZKP circuit design, and experimental evaluation on both\nthe MNIST and Human Activity Recognition (HAR) datasets. We focus on a\nthreshold-based proof for a simple Convolutional Neural Network (CNN) model\n(for MNIST) and a multi-layer perceptron (MLP) model (for HAR), and evaluate\nthe approach in terms of computational overhead, communication cost, and\nverifiability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96f6\u77e5\u8bc6\u8bc1\u660e\u7684\u8054\u90a6\u5b66\u4e60\u9690\u79c1\u4fdd\u62a4\u8bc4\u4f30\u534f\u8bae\uff0c\u907f\u514d\u901a\u8fc7\u6027\u80fd\u6307\u6807\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u8bc4\u4f30\u9636\u6bb5\u53ef\u80fd\u901a\u8fc7\u5171\u4eab\u7684\u6027\u80fd\u6307\u6807\u6cc4\u9732\u654f\u611f\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u4e14\u53ef\u9a8c\u8bc1\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\uff08ZKPs\uff09\uff0c\u5ba2\u6237\u7aef\u751f\u6210\u7b80\u6d01\u8bc1\u660e\uff0c\u58f0\u660e\u5176\u672c\u5730\u635f\u5931\u4f4e\u4e8e\u9884\u5b9a\u4e49\u9608\u503c\uff0c\u65e0\u9700\u4f9d\u8d56\u5916\u90e8API\u3002", "result": "\u5728MNIST\u548cHAR\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc4\u4f30\u4e86\u8ba1\u7b97\u5f00\u9500\u3001\u901a\u4fe1\u6210\u672c\u548c\u53ef\u9a8c\u8bc1\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u8bae\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u53ef\u9a8c\u8bc1\u7684\u8054\u90a6\u5b66\u4e60\u8bc4\u4f30\u3002"}}
{"id": "2507.11660", "categories": ["cs.LG", "cs.MA", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11660", "abs": "https://arxiv.org/abs/2507.11660", "authors": ["Joao F. Rocha", "Ke Xu", "Xingzhi Sun", "Ananya Krishna", "Dhananjay Bhaskar", "Blanche Mongeon", "Morgan Craig", "Mark Gerstein", "Smita Krishnaswamy"], "title": "STAGED: A Multi-Agent Neural Network for Learning Cellular Interaction Dynamics", "comment": null, "summary": "The advent of single-cell technology has significantly improved our\nunderstanding of cellular states and subpopulations in various tissues under\nnormal and diseased conditions by employing data-driven approaches such as\nclustering and trajectory inference. However, these methods consider cells as\nindependent data points of population distributions. With spatial\ntranscriptomics, we can represent cellular organization, along with dynamic\ncell-cell interactions that lead to changes in cell state. Still, key\ncomputational advances are necessary to enable the data-driven learning of such\ncomplex interactive cellular dynamics. While agent-based modeling (ABM)\nprovides a powerful framework, traditional approaches rely on handcrafted rules\nderived from domain knowledge rather than data-driven approaches. To address\nthis, we introduce Spatio Temporal Agent-Based Graph Evolution Dynamics(STAGED)\nintegrating ABM with deep learning to model intercellular communication, and\nits effect on the intracellular gene regulatory network. Using graph ODE\nnetworks (GDEs) with shared weights per cell type, our approach represents\ngenes as vertices and interactions as directed edges, dynamically learning\ntheir strengths through a designed attention mechanism. Trained to match\ncontinuous trajectories of simulated as well as inferred trajectories from\nspatial transcriptomics data, the model captures both intercellular and\nintracellular interactions, enabling a more adaptive and accurate\nrepresentation of cellular dynamics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u57fa\u4e8e\u4ee3\u7406\u7684\u5efa\u6a21\uff08ABM\uff09\u7684\u65b9\u6cd5STAGED\uff0c\u7528\u4e8e\u6a21\u62df\u7ec6\u80de\u95f4\u901a\u4fe1\u53ca\u5176\u5bf9\u7ec6\u80de\u5185\u57fa\u56e0\u8c03\u63a7\u7f51\u7edc\u7684\u5f71\u54cd\u3002", "motivation": "\u5355\u7ec6\u80de\u6280\u672f\u867d\u80fd\u63ed\u793a\u7ec6\u80de\u72b6\u6001\u548c\u4e9a\u7fa4\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5c06\u7ec6\u80de\u89c6\u4e3a\u72ec\u7acb\u6570\u636e\u70b9\uff0c\u5ffd\u89c6\u4e86\u7ec6\u80de\u95f4\u7684\u52a8\u6001\u76f8\u4e92\u4f5c\u7528\u3002\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u63d0\u4f9b\u4e86\u7ec6\u80de\u7ec4\u7ec7\u548c\u4e92\u52a8\u7684\u6570\u636e\uff0c\u4f46\u9700\u8981\u65b0\u7684\u8ba1\u7b97\u65b9\u6cd5\u6765\u5b66\u4e60\u8fd9\u79cd\u590d\u6742\u7684\u52a8\u6001\u3002", "method": "STAGED\u6574\u5408ABM\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff0c\u4f7f\u7528\u56feODE\u7f51\u7edc\uff08GDEs\uff09\u52a8\u6001\u5b66\u4e60\u57fa\u56e0\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5339\u914d\u6a21\u62df\u548c\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66\u6570\u636e\u7684\u8fde\u7eed\u8f68\u8ff9\uff0c\u6355\u6349\u7ec6\u80de\u95f4\u548c\u7ec6\u80de\u5185\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u81ea\u9002\u5e94\u52a8\u6001\u8868\u793a\u3002", "conclusion": "STAGED\u4e3a\u7814\u7a76\u7ec6\u80de\u52a8\u6001\u63d0\u4f9b\u4e86\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u5f25\u8865\u4e86\u4f20\u7edfABM\u4f9d\u8d56\u624b\u5de5\u89c4\u5219\u7684\u4e0d\u8db3\u3002"}}
{"id": "2507.12210", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12210", "abs": "https://arxiv.org/abs/2507.12210", "authors": ["Jialiang Zhu", "Sanoopkumar P. S.", "Arman Farhang"], "title": "PAPR of DFT-s-OTFS with Pulse Shaping", "comment": null, "summary": "Orthogonal Time Frequency Space (OTFS) suffers from high peak-to-average\npower ratio (PAPR) when the number of Doppler bins is large. To address this\nissue, a discrete Fourier transform spread OTFS (DFT-s-OTFS) scheme is employed\nby applying DFT spreading across the Doppler dimension. This paper presents a\nthorough PAPR analysis of DFT-s-OTFS in the uplink scenario using different\npulse shaping filters and resource allocation strategies. Specifically, we\nderive a PAPR upper bound of DFT-s-OTFS with interleaved and block Doppler\nresource allocation schemes. Our analysis reveals that DFT-s-OTFS with\ninterleaved allocation yields a lower PAPR than that of block allocation.\nFurthermore, we show that interleaved allocation produces a periodic\ntime-domain signal composed of repeated quadrature amplitude modulated (QAM)\nsymbols which simplifies the transmitter design. Based on our analytical\nresults, the root raised cosine (RRC) pulse generally results in a higher\nmaximum PAPR compared to the rectangular pulse. Simulation results confirm the\nvalidity of the derived PAPR upper bounds. Furthermore, we also demonstrate\nthrough BER simulation analysis that the DFT-s-OTFS gives the same performance\nas OTFS without DFT spreading.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86DFT-s-OTFS\u65b9\u6848\u5728\u4e0a\u884c\u94fe\u8def\u4e2d\u7684PAPR\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u8109\u51b2\u6574\u5f62\u6ee4\u6ce2\u5668\u548c\u8d44\u6e90\u5206\u914d\u7b56\u7565\uff0c\u53d1\u73b0\u4ea4\u9519\u5206\u914d\u80fd\u964d\u4f4ePAPR\u5e76\u7b80\u5316\u53d1\u5c04\u673a\u8bbe\u8ba1\u3002", "motivation": "\u89e3\u51b3OTFS\u5728\u591a\u666e\u52d2\u9891\u6bb5\u8f83\u591a\u65f6\u7684\u9ad8PAPR\u95ee\u9898\u3002", "method": "\u91c7\u7528DFT\u6269\u5c55OTFS\uff08DFT-s-OTFS\uff09\uff0c\u5206\u6790\u5176PAPR\u4e0a\u754c\uff0c\u6bd4\u8f83\u4ea4\u9519\u548c\u5757\u72b6\u591a\u666e\u52d2\u8d44\u6e90\u5206\u914d\u7b56\u7565\u3002", "result": "\u4ea4\u9519\u5206\u914d\u6bd4\u5757\u72b6\u5206\u914dPAPR\u66f4\u4f4e\uff0c\u4e14RRC\u8109\u51b2\u7684PAPR\u9ad8\u4e8e\u77e9\u5f62\u8109\u51b2\u3002BER\u6027\u80fd\u4e0e\u672a\u6269\u5c55OTFS\u76f8\u540c\u3002", "conclusion": "DFT-s-OTFS\u662f\u964d\u4f4ePAPR\u7684\u6709\u6548\u65b9\u6848\uff0c\u5c24\u5176\u9002\u5408\u4e0a\u884c\u94fe\u8def\u3002"}}
{"id": "2507.11688", "categories": ["cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2507.11688", "abs": "https://arxiv.org/abs/2507.11688", "authors": ["Travis Pence", "Daisuke Yamada", "Vikas Singh"], "title": "Composing Linear Layers from Irreducibles", "comment": "27 Pages, 13 Tables, 8 Figures", "summary": "Contemporary large models often exhibit behaviors suggesting the presence of\nlow-level primitives that compose into modules with richer functionality, but\nthese fundamental building blocks remain poorly understood. We investigate this\ncompositional structure in linear layers by asking: can we identify/synthesize\nlinear transformations from a minimal set of geometric primitives? Using\nClifford algebra, we show that linear layers can be expressed as compositions\nof bivectors -- geometric objects encoding oriented planes -- and introduce a\ndifferentiable algorithm that decomposes them into products of rotors. This\nconstruction uses only O(log^2 d) parameters, versus O(d^2) required by dense\nmatrices. Applied to the key, query, and value projections in LLM attention\nlayers, our rotor-based layers match the performance of strong baselines such\nas block-Hadamard and low-rank approximations. Our findings provide an\nalgebraic perspective on how these geometric primitives can compose into\nhigher-level functions within deep models.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7ebf\u6027\u5c42\u4e2d\u7684\u51e0\u4f55\u57fa\u5143\uff08\u5982\u53cc\u5411\u91cf\uff09\u5982\u4f55\u7ec4\u5408\u6210\u66f4\u9ad8\u7ea7\u7684\u529f\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eClifford\u4ee3\u6570\u7684\u53ef\u5fae\u5206\u7b97\u6cd5\uff0c\u5c06\u7ebf\u6027\u53d8\u6362\u5206\u89e3\u4e3a\u8f6c\u5b50\u4e58\u79ef\uff0c\u53c2\u6570\u6548\u7387\u9ad8\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u6a21\u578b\u4e2d\u7684\u4f4e\u5c42\u7ea7\u57fa\u5143\u5982\u4f55\u7ec4\u5408\u6210\u6a21\u5757\u5316\u529f\u80fd\uff0c\u63a2\u7d22\u7ebf\u6027\u5c42\u7684\u51e0\u4f55\u7ed3\u6784\u3002", "method": "\u4f7f\u7528Clifford\u4ee3\u6570\u5c06\u7ebf\u6027\u5c42\u8868\u793a\u4e3a\u53cc\u5411\u91cf\u7ec4\u5408\uff0c\u5e76\u5f00\u53d1\u53ef\u5fae\u5206\u7b97\u6cd5\u5206\u89e3\u4e3a\u8f6c\u5b50\u4e58\u79ef\uff0c\u53c2\u6570\u590d\u6742\u5ea6\u4e3aO(log\u00b2d)\u3002", "result": "\u5728LLM\u6ce8\u610f\u529b\u5c42\u4e2d\uff0c\u57fa\u4e8e\u8f6c\u5b50\u7684\u5c42\u6027\u80fd\u4e0e\u5f3a\u57fa\u7ebf\uff08\u5982\u5757Hadamard\u548c\u4f4e\u79e9\u8fd1\u4f3c\uff09\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6df1\u5ea6\u6a21\u578b\u4e2d\u51e0\u4f55\u57fa\u5143\u7684\u7ec4\u5408\u63d0\u4f9b\u4e86\u4ee3\u6570\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u53c2\u6570\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.12211", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12211", "abs": "https://arxiv.org/abs/2507.12211", "authors": ["Sa\u00fal Fenollosa"], "title": "Cell Sensing: Traffic detection", "comment": "39 pages, 16 figures. Student project report at the\n  Telecommunications Circuits Laboratory (TCL), EPFL. Supervised by Prof.\n  Andreas Burg and Sitian Li", "summary": "This work presents a passive sensing system for traffic monitoring using\nambient Long Term Evolution (LTE) signals as a non-intrusive and scalable\nalternative to traditional surveillance methods. The approach employs a\ndual-receiver architecture analyzing Channel State Information (CSI) to isolate\ndifferential Doppler shifts induced by moving targets, effectively mitigating\nhardware-induced phase impairments. Implemented with a Software Defined Radio\n(SDR) platform and srsRAN software, the system demonstrated over 90% detection\naccuracy for speeds above 6000 mm/min in controlled indoor tests, and provided\nreliable speed estimations for pedestrians and vehicles in outdoor evaluations.\nDespite challenges at low speeds, directional ambiguity, and multipath fading\nin urban settings, the results validate LTE-based passive sensing as a feasible\ntraffic monitoring method, identifying critical areas for future research such\nas angle-of-arrival (AoA) integration, machine learning, and real-time embedded\nsystem development.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eLTE\u4fe1\u53f7\u7684\u88ab\u52a8\u4ea4\u901a\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u63a5\u6536\u5668\u67b6\u6784\u5206\u6790CSI\uff0c\u6709\u6548\u68c0\u6d4b\u79fb\u52a8\u76ee\u6807\uff0c\u5ba4\u5185\u6d4b\u8bd5\u51c6\u786e\u7387\u8d8590%\uff0c\u6237\u5916\u6d4b\u8bd5\u8868\u73b0\u53ef\u9760\u3002", "motivation": "\u4f20\u7edf\u4ea4\u901a\u76d1\u6d4b\u65b9\u6cd5\u4fb5\u5165\u6027\u5f3a\u4e14\u6269\u5c55\u6027\u5dee\uff0c\u9700\u4e00\u79cd\u975e\u4fb5\u5165\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u53cc\u63a5\u6536\u5668\u67b6\u6784\u5206\u6790CSI\uff0c\u5229\u7528SDR\u5e73\u53f0\u548csrsRAN\u8f6f\u4ef6\u5b9e\u73b0\u7cfb\u7edf\uff0c\u901a\u8fc7\u5dee\u5206\u591a\u666e\u52d2\u9891\u79fb\u68c0\u6d4b\u79fb\u52a8\u76ee\u6807\u3002", "result": "\u5ba4\u5185\u6d4b\u8bd5\u4e2d\u901f\u5ea6\u9ad8\u4e8e6000 mm/min\u65f6\u68c0\u6d4b\u51c6\u786e\u7387\u8d8590%\uff0c\u6237\u5916\u6d4b\u8bd5\u4e2d\u5bf9\u884c\u4eba\u548c\u8f66\u8f86\u901f\u5ea6\u4f30\u8ba1\u53ef\u9760\u3002", "conclusion": "LTE\u88ab\u52a8\u4f20\u611f\u5728\u4ea4\u901a\u76d1\u6d4b\u4e2d\u53ef\u884c\uff0c\u672a\u6765\u9700\u7814\u7a76AoA\u96c6\u6210\u3001\u673a\u5668\u5b66\u4e60\u548c\u5b9e\u65f6\u5d4c\u5165\u5f0f\u7cfb\u7edf\u5f00\u53d1\u3002"}}
{"id": "2507.11690", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11690", "abs": "https://arxiv.org/abs/2507.11690", "authors": ["Amaya Dharmasiri", "William Yang", "Polina Kirichenko", "Lydia Liu", "Olga Russakovsky"], "title": "The Impact of Coreset Selection on Spurious Correlations and Group Robustness", "comment": "10 pages, 9 additional pages for Appendix", "summary": "Coreset selection methods have shown promise in reducing the training data\nsize while maintaining model performance for data-efficient machine learning.\nHowever, as many datasets suffer from biases that cause models to learn\nspurious correlations instead of causal features, it is important to understand\nwhether and how dataset reduction methods may perpetuate, amplify, or mitigate\nthese biases. In this work, we conduct the first comprehensive analysis of the\nimplications of data selection on the spurious bias levels of the selected\ncoresets and the robustness of downstream models trained on them. We use an\nextensive experimental setting spanning ten different spurious correlations\nbenchmarks, five score metrics to characterize sample importance/ difficulty,\nand five data selection policies across a broad range of coreset sizes.\nThereby, we unravel a series of nontrivial nuances in interactions between\nsample difficulty and bias alignment, as well as dataset bias and resultant\nmodel robustness. For example, we find that selecting coresets using\nembedding-based sample characterization scores runs a comparatively lower risk\nof inadvertently exacerbating bias than selecting using characterizations based\non learning dynamics. Most importantly, our analysis reveals that although some\ncoreset selection methods could achieve lower bias levels by prioritizing\ndifficult samples, they do not reliably guarantee downstream robustness.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff08\u5982\u6838\u5fc3\u96c6\u9009\u62e9\uff09\u5bf9\u6570\u636e\u96c6\u504f\u89c1\u548c\u4e0b\u6e38\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u6837\u672c\u96be\u5ea6\u4e0e\u504f\u89c1\u5bf9\u9f50\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\u3002", "motivation": "\u7814\u7a76\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\u662f\u5426\u4f1a\u52a0\u5267\u6216\u7f13\u89e3\u6570\u636e\u96c6\u4e2d\u7684\u504f\u89c1\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u4e0b\u6e38\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\u3002", "method": "\u5728\u5341\u4e2a\u4e0d\u540c\u7684\u4f2a\u76f8\u5173\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528\u4e94\u79cd\u8bc4\u5206\u6307\u6807\u548c\u4e94\u79cd\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u5206\u6790\u3002", "result": "\u53d1\u73b0\u57fa\u4e8e\u5d4c\u5165\u7684\u6837\u672c\u8bc4\u5206\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u5b66\u4e60\u52a8\u6001\u7684\u65b9\u6cd5\u66f4\u4e0d\u6613\u52a0\u5267\u504f\u89c1\uff0c\u4f46\u4f18\u5148\u9009\u62e9\u56f0\u96be\u6837\u672c\u7684\u65b9\u6cd5\u5e76\u4e0d\u80fd\u53ef\u9760\u4fdd\u8bc1\u6a21\u578b\u9c81\u68d2\u6027\u3002", "conclusion": "\u6838\u5fc3\u96c6\u9009\u62e9\u65b9\u6cd5\u5728\u51cf\u5c11\u504f\u89c1\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8c28\u614e\u8bbe\u8ba1\u4ee5\u907f\u514d\u610f\u5916\u52a0\u5267\u504f\u89c1\u3002"}}
{"id": "2507.12221", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12221", "abs": "https://arxiv.org/abs/2507.12221", "authors": ["Alejandro Castilla", "Sa\u00fal Fenollosa", "Monika Drozdowska", "Alejandro Lopez-Escudero", "Sergio Mic\u00f2-Rosa", "Narcis Cardona"], "title": "Novel Approach to Dual-Channel Estimation in Integrated Sensing and Communications for 6G", "comment": "6 pages, 13 figures. Accepted for publication at the 2024 IEEE 35th\n  International Symposium on Personal, Indoor and Mobile Radio Communications\n  (PIMRC)", "summary": "Integrated Sensing and Communication (ISAC) design is crucial for 6G and\nharmonizes environmental data sensing with communication, emphasizing the need\nto understand and model these elements. This paper delves into dual-channel\nmodels for ISAC, employing channel extraction techniques to validate and\nenhance accuracy. Focusing on millimeter wave (mmWave) radars, it explores the\nextraction of the bistatic sensing channel from monostatic measurements and\nsubsequent communication channel estimation. The proposed methods involve\ninterference extraction, module and phase correlation analyses, chirp\nclustering, and auto-clutter reduction. A comprehensive set-up in an anechoic\nchamber with controlled scenarios evaluates the proposed techniques,\ndemonstrating successful channel extraction and validation through Root Mean\nSquare Delay Spread (RMS DS), Power Delay Profile (PDP), and Angle of Arrival\n(AoA) analysis. Comparison with Ray-Tracing (RT) simulations confirms the\neffectiveness of the proposed approach, presenting an innovative stride towards\nfully integrated sensing and communication in future networks.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e866G\u4e2d\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u8bbe\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u901a\u9053\u6a21\u578b\uff0c\u901a\u8fc7\u6beb\u7c73\u6ce2\u96f7\u8fbe\u63d0\u53d6\u53cc\u57fa\u5730\u611f\u77e5\u901a\u9053\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "motivation": "6G\u9700\u8981\u5c06\u73af\u5883\u611f\u77e5\u4e0e\u901a\u4fe1\u7ed3\u5408\uff0c\u56e0\u6b64\u9700\u8981\u7406\u89e3\u548c\u5efa\u6a21ISAC\u7684\u53cc\u901a\u9053\u7279\u6027\u3002", "method": "\u91c7\u7528\u5e72\u6270\u63d0\u53d6\u3001\u6a21\u5757\u548c\u76f8\u4f4d\u76f8\u5173\u6027\u5206\u6790\u3001\u5541\u557e\u805a\u7c7b\u548c\u81ea\u52a8\u6742\u6ce2\u51cf\u5c11\u7b49\u6280\u672f\uff0c\u4ece\u5355\u57fa\u5730\u6d4b\u91cf\u4e2d\u63d0\u53d6\u53cc\u57fa\u5730\u611f\u77e5\u901a\u9053\u3002", "result": "\u5728\u6d88\u58f0\u5ba4\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7RMS DS\u3001PDP\u548cAoA\u5206\u6790\u5c55\u793a\u4e86\u6210\u529f\u901a\u9053\u63d0\u53d6\uff0c\u5e76\u4e0e\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\u7ed3\u679c\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u672a\u6765\u7f51\u7edc\u7684\u5b8c\u5168\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\u63d0\u4f9b\u4e86\u521b\u65b0\u6027\u8fdb\u5c55\u3002"}}
{"id": "2507.11702", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11702", "abs": "https://arxiv.org/abs/2507.11702", "authors": ["Hein de Wilde", "Ali Mohammed Mansoor Alsahag", "Pierre Blanchet"], "title": "Time series classification of satellite data using LSTM networks: an approach for predicting leaf-fall to minimize railroad traffic disruption", "comment": null, "summary": "Railroad traffic disruption as a result of leaf-fall cost the UK rail\nindustry over 300 million per year and measures to mitigate such disruptions\nare employed on a large scale, with 1.67 million kilometers of track being\ntreated in the UK in 2021 alone. Therefore, the ability to anticipate the\ntiming of leaf-fall would offer substantial benefits for rail network\noperators, enabling the efficient scheduling of such mitigation measures.\nHowever, current methodologies for predicting leaf-fall exhibit considerable\nlimitations in terms of scalability and reliability. This study endeavors to\ndevise a prediction system that leverages specialized prediction methods and\nthe latest satellite data sources to generate both scalable and reliable\ninsights into leaf-fall timings. An LSTM network trained on ground-truth\nleaf-falling data combined with multispectral and meteorological satellite data\ndemonstrated a root-mean-square error of 6.32 days for predicting the start of\nleaf-fall and 9.31 days for predicting the end of leaf-fall. The model, which\nimproves upon previous work on the topic, offers promising opportunities for\nthe optimization of leaf mitigation measures in the railway industry and the\nimprovement of our understanding of complex ecological systems.", "AI": {"tldr": "\u82f1\u56fd\u94c1\u8def\u56e0\u843d\u53f6\u5bfc\u81f4\u6bcf\u5e74\u635f\u59313\u4ebf\u82f1\u9551\uff0c\u73b0\u6709\u9884\u6d4b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u7814\u7a76\u5229\u7528LSTM\u7f51\u7edc\u7ed3\u5408\u536b\u661f\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u843d\u53f6\u65f6\u95f4\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u843d\u53f6\u5bfc\u81f4\u94c1\u8def\u4ea4\u901a\u4e2d\u65ad\uff0c\u6bcf\u5e74\u9020\u6210\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\uff0c\u4e9f\u9700\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u65b9\u6cd5\u6765\u4f18\u5316\u5e94\u5bf9\u63aa\u65bd\u3002", "method": "\u91c7\u7528LSTM\u7f51\u7edc\uff0c\u7ed3\u5408\u5730\u9762\u843d\u53f6\u6570\u636e\u548c\u591a\u5149\u8c31\u6c14\u8c61\u536b\u661f\u6570\u636e\uff0c\u9884\u6d4b\u843d\u53f6\u65f6\u95f4\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u843d\u53f6\u5f00\u59cb\u548c\u7ed3\u675f\u65f6\u95f4\u7684\u5747\u65b9\u6839\u8bef\u5dee\u5206\u522b\u4e3a6.32\u5929\u548c9.31\u5929\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u94c1\u8def\u884c\u4e1a\u4f18\u5316\u843d\u53f6\u5e94\u5bf9\u63aa\u65bd\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5e76\u6709\u52a9\u4e8e\u7406\u89e3\u590d\u6742\u751f\u6001\u7cfb\u7edf\u3002"}}
{"id": "2507.12235", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12235", "abs": "https://arxiv.org/abs/2507.12235", "authors": ["Sa\u00fal Fenollosa", "Monika Drozdowska", "Wenfei Yang", "Sergio Mic\u00f3-Rosa", "Alejandro Castilla", "Alejandro Lopez-Escudero", "Jian Li", "Narcis Cardona"], "title": "Frequency-responsive RCS characteristics and scaling implications for ISAC development", "comment": "6 pages, 12 figures, 3 tables. Accepted for publication at the 2024\n  IEEE Global Communications Conference (GLOBECOM), WS-02: Workshop on\n  Propagation Channel Models and Evaluation Methodologies for 6G", "summary": "This paper presents an investigation on the Radar Cross-Section (RCS) of\nvarious targets, with the objective of analysing how RCS properties vary with\nfrequency. Targets such as an Automated Guided Vehicle (AGV), a pedestrian, and\na full-scale car were measured in the frequency bands referred to in industry\nstandards as FR2 and FR3. Measurements were taken in diverse environments,\nindoors and outdoors, to ensure comprehensive scenario coverage. The\nmethodology employed in RCS extraction performs background subtraction,\nfollowed by time-domain gating to isolate the influence of the target. This\nanalysis compares the RCS values and how the points of greatest contribution\nare distributed across different bands based on the range response of the RCS.\nAnalysis of the results demonstrated how RCS values change with frequency and\ntarget shape, providing insights into the electromagnetic behaviour of these\ntargets. Key findings highlight how much scaling RCS values based on frequency\nand geometry is complex and varies among different types of materials and\nshapes. These insights are instrumental for advancing sensing systems and\nenhancing 3GPP channel models, particularly for Integrated Sensing and\nCommunications (ISAC) techniques proposed for 6G standards.", "AI": {"tldr": "\u7814\u7a76\u4e0d\u540c\u76ee\u6807\u7684\u96f7\u8fbe\u6563\u5c04\u622a\u9762\uff08RCS\uff09\u968f\u9891\u7387\u53d8\u5316\u7684\u7279\u6027\uff0c\u5206\u6790\u4e86AGV\u3001\u884c\u4eba\u548c\u5168\u5c3a\u5bf8\u6c7d\u8f66\u7684RCS\u503c\u53ca\u5176\u5206\u5e03\u3002", "motivation": "\u63a2\u7d22RCS\u968f\u9891\u7387\u548c\u76ee\u6807\u5f62\u72b6\u7684\u53d8\u5316\u89c4\u5f8b\uff0c\u4e3a6G\u6807\u51c6\u4e2d\u7684\u96c6\u6210\u611f\u77e5\u4e0e\u901a\u4fe1\uff08ISAC\uff09\u6280\u672f\u63d0\u4f9b\u652f\u6301\u3002", "method": "\u91c7\u7528\u80cc\u666f\u51cf\u9664\u548c\u65f6\u57df\u95e8\u63a7\u6280\u672f\u63d0\u53d6RCS\uff0c\u5e76\u5728\u5ba4\u5185\u5916\u591a\u79cd\u73af\u5883\u4e2d\u6d4b\u91cf\u3002", "result": "RCS\u503c\u968f\u9891\u7387\u548c\u5f62\u72b6\u53d8\u5316\u590d\u6742\uff0c\u4e0d\u540c\u6750\u6599\u548c\u5f62\u72b6\u7684\u76ee\u6807\u8868\u73b0\u5404\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u63d0\u5347\u611f\u77e5\u7cfb\u7edf\u548c\u4f18\u53163GPP\u4fe1\u9053\u6a21\u578b\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2507.12301", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.12301", "abs": "https://arxiv.org/abs/2507.12301", "authors": ["Zhenyu Liu", "Yi Ma", "Rahim Tafazolli", "Zhi Ding"], "title": "Leveraging Bi-Directional Channel Reciprocity for Robust Ultra-Low-Rate Implicit CSI Feedback with Deep Learning", "comment": null, "summary": "Deep learning-based implicit channel state information (CSI) feedback has\nbeen introduced to enhance spectral efficiency in massive MIMO systems.\nExisting methods often show performance degradation in ultra-low-rate scenarios\nand inadaptability across diverse environments. In this paper, we propose\nDual-ImRUNet, an efficient uplink-assisted deep implicit CSI feedback framework\nincorporating two novel plug-in preprocessing modules to achieve ultra-low\nfeedback rates while maintaining high environmental robustness. First, a novel\nbi-directional correlation enhancement module is proposed to strengthen the\ncorrelation between uplink and downlink CSI eigenvector matrices. This module\nprojects highly correlated uplink and downlink channel matrices into their\nrespective eigenspaces, effectively reducing redundancy for ultra-low-rate\nfeedback. Second, an innovative input format alignment module is designed to\nmaintain consistent data distributions at both encoder and decoder sides\nwithout extra transmission overhead, thereby enhancing robustness against\nenvironmental variations. Finally, we develop an efficient transformer-based\nimplicit CSI feedback network to exploit angular-delay domain sparsity and\nbi-directional correlation for ultra-low-rate CSI compression. Simulation\nresults demonstrate successful reduction of the feedback overhead by 85%\ncompared with the state-of-the-art method and robustness against unseen\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDual-ImRUNet\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u9884\u5904\u7406\u6a21\u5757\u5b9e\u73b0\u8d85\u4f4e\u53cd\u9988\u901f\u7387\u548c\u9ad8\u73af\u5883\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8d85\u4f4e\u901f\u7387\u573a\u666f\u548c\u591a\u6837\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u63d0\u5347\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u76f8\u5173\u6027\u589e\u5f3a\u6a21\u5757\u548c\u8f93\u5165\u683c\u5f0f\u5bf9\u9f50\u6a21\u5757\uff0c\u7ed3\u5408\u57fa\u4e8eTransformer\u7684\u9690\u5f0fCSI\u53cd\u9988\u7f51\u7edc\u3002", "result": "\u53cd\u9988\u5f00\u9500\u51cf\u5c1185%\uff0c\u5e76\u5728\u672a\u77e5\u73af\u5883\u4e2d\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "Dual-ImRUNet\u5728\u8d85\u4f4e\u901f\u7387\u548c\u591a\u6837\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.11710", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11710", "abs": "https://arxiv.org/abs/2507.11710", "authors": ["Jay Revolinsky", "Harry Shomer", "Jiliang Tang"], "title": "Subgraph Generation for Generalizing on Out-of-Distribution Links", "comment": "18 pages, 7 figures, preprint", "summary": "Graphs Neural Networks (GNNs) demonstrate high-performance on the link\nprediction (LP) task. However, these models often rely on all dataset samples\nbeing drawn from the same distribution. In addition, graph generative models\n(GGMs) show a pronounced ability to generate novel output graphs. Despite this,\nGGM applications remain largely limited to domain-specific tasks. To bridge\nthis gap, we propose FLEX as a GGM framework which leverages two mechanism: (1)\nstructurally-conditioned graph generation, and (2) adversarial co-training\nbetween an auto-encoder and GNN. As such, FLEX ensures structural-alignment\nbetween sample distributions to enhance link-prediction performance in\nout-of-distribution (OOD) scenarios. Notably, FLEX does not require expert\nknowledge to function in different OOD scenarios. Numerous experiments are\nconducted in synthetic and real-world OOD settings to demonstrate FLEX's\nperformance-enhancing ability, with further analysis for understanding the\neffects of graph data augmentation on link structures. The source code is\navailable here: https://github.com/revolins/FlexOOD.", "AI": {"tldr": "FLEX\u662f\u4e00\u4e2a\u56fe\u751f\u6210\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u6761\u4ef6\u751f\u6210\u548c\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u5916\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u4f9d\u8d56\u540c\u5206\u5e03\u6570\u636e\uff1b\u56fe\u751f\u6210\u6a21\u578b\u867d\u80fd\u751f\u6210\u65b0\u56fe\uff0c\u4f46\u5e94\u7528\u53d7\u9650\u3002FLEX\u65e8\u5728\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u63d0\u5347\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u6027\u80fd\u3002", "method": "FLEX\u91c7\u7528\u4e24\u79cd\u673a\u5236\uff1a\u7ed3\u6784\u6761\u4ef6\u56fe\u751f\u6210\u548c\u81ea\u52a8\u7f16\u7801\u5668\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5bf9\u6297\u534f\u540c\u8bad\u7ec3\uff0c\u786e\u4fdd\u6837\u672c\u5206\u5e03\u7684\u7ed3\u6784\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFLEX\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5206\u5e03\u5916\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u56fe\u6570\u636e\u589e\u5f3a\u5bf9\u94fe\u63a5\u7ed3\u6784\u7684\u5f71\u54cd\u3002", "conclusion": "FLEX\u65e0\u9700\u4e13\u5bb6\u77e5\u8bc6\u5373\u53ef\u9002\u7528\u4e8e\u4e0d\u540c\u5206\u5e03\u5916\u573a\u666f\uff0c\u4e3a\u56fe\u751f\u6210\u6a21\u578b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.12317", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12317", "abs": "https://arxiv.org/abs/2507.12317", "authors": ["Martin Agebj\u00e4r", "Gustav Zetterqvist", "Fredrik Gustafsson", "Johan Wahlstr\u00f6m", "Gustaf Hendeby"], "title": "Road Roughness Estimation via Fusion of Standard Onboard Automotive Sensors", "comment": "Accepted for publication in FUSION 2025 - 28th International\n  Conference on Information Fusion (FUSION), IEEE (2025)", "summary": "Road roughness significantly affects vehicle vibrations and ride quality. We\nintroduce a Kalman filter (KF)-based method for estimating road roughness in\nterms of the international roughness index (IRI) by fusing inertial and speed\nmeasurements, offering a cost-effective solution for pavement monitoring. The\nmethod involves system identification on a physical vehicle to estimate\nrealistic model parameters, followed by KF-based reconstruction of the\nlongitudinal road profile to compute IRI values. It explores IRI estimation\nusing vertical and lateral vibrations, the latter more common in modern\nvehicles. Validation on 230 km of real-world data shows promising results, with\nIRI estimation errors ranging from 1% to 10% of the reference values. However,\naccuracy deteriorates significantly when using only lateral vibrations,\nhighlighting their limitations. These findings demonstrate the potential of\nKF-based estimation for efficient road roughness monitoring.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u60ef\u6027\u548c\u901f\u5ea6\u6d4b\u91cf\u6765\u4f30\u8ba1\u9053\u8def\u7c97\u7cd9\u5ea6\uff08IRI\uff09\uff0c\u9a8c\u8bc1\u7ed3\u679c\u663e\u793a\u8bef\u5dee\u57281%\u523010%\u4e4b\u95f4\uff0c\u4f46\u4ec5\u4f7f\u7528\u6a2a\u5411\u632f\u52a8\u65f6\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u9053\u8def\u7c97\u7cd9\u5ea6\u5bf9\u8f66\u8f86\u632f\u52a8\u548c\u4e58\u5750\u8d28\u91cf\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8fa8\u8bc6\u4f30\u8ba1\u8f66\u8f86\u6a21\u578b\u53c2\u6570\uff0c\u5229\u7528\u5361\u5c14\u66fc\u6ee4\u6ce2\u91cd\u5efa\u7eb5\u5411\u9053\u8def\u8f6e\u5ed3\u4ee5\u8ba1\u7b97IRI\u503c\uff0c\u5e76\u63a2\u7d22\u4e86\u5782\u76f4\u548c\u6a2a\u5411\u632f\u52a8\u7684\u4f7f\u7528\u3002", "result": "\u5728230\u516c\u91cc\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0cIRI\u4f30\u8ba1\u8bef\u5dee\u4e3a1%\u81f310%\uff0c\u4f46\u4ec5\u4f7f\u7528\u6a2a\u5411\u632f\u52a8\u65f6\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u5361\u5c14\u66fc\u6ee4\u6ce2\u65b9\u6cd5\u5728\u9053\u8def\u7c97\u7cd9\u5ea6\u76d1\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u6a2a\u5411\u632f\u52a8\u7684\u4f7f\u7528\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2507.11729", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11729", "abs": "https://arxiv.org/abs/2507.11729", "authors": ["Amirhossein Ahmadi", "Hamidreza Zareipour", "Henry Leung"], "title": "Globalization for Scalable Short-term Load Forecasting", "comment": "63 pages with 22 figures", "summary": "Forecasting load in power transmission networks is essential across various\nhierarchical levels, from the system level down to individual points of\ndelivery (PoD). While intuitive and locally accurate, traditional local\nforecasting models (LFMs) face significant limitations, particularly in\nhandling generalizability, overfitting, data drift, and the cold start problem.\nThese methods also struggle with scalability, becoming computationally\nexpensive and less efficient as the network's size and data volume grow. In\ncontrast, global forecasting models (GFMs) offer a new approach to enhance\nprediction generalizability, scalability, accuracy, and robustness through\nglobalization and cross-learning. This paper investigates global load\nforecasting in the presence of data drifts, highlighting the impact of\ndifferent modeling techniques and data heterogeneity. We explore\nfeature-transforming and target-transforming models, demonstrating how\nglobalization, data heterogeneity, and data drift affect each differently. In\naddition, we examine the role of globalization in peak load forecasting and its\npotential for hierarchical forecasting. To address data heterogeneity and the\nbalance between globality and locality, we propose separate time series\nclustering (TSC) methods, introducing model-based TSC for feature-transforming\nmodels and new weighted instance-based TSC for target-transforming models.\nThrough extensive experiments on a real-world dataset of Alberta's electricity\nload, we demonstrate that global target-transforming models consistently\noutperform their local counterparts, especially when enriched with global\nfeatures and clustering techniques. In contrast, global feature-transforming\nmodels face challenges in balancing local and global dynamics, often requiring\nTSC to manage data heterogeneity effectively.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u7535\u529b\u4f20\u8f93\u7f51\u7edc\u4e2d\u7684\u5168\u5c40\u8d1f\u8377\u9884\u6d4b\u65b9\u6cd5\uff0c\u63a2\u8ba8\u4e86\u6570\u636e\u6f02\u79fb\u3001\u5efa\u6a21\u6280\u672f\u548c\u6570\u636e\u5f02\u8d28\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u65b9\u6cd5\u4ee5\u4f18\u5316\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5c40\u90e8\u9884\u6d4b\u6a21\u578b\u5728\u6cdb\u5316\u6027\u3001\u8fc7\u62df\u5408\u3001\u6570\u636e\u6f02\u79fb\u548c\u51b7\u542f\u52a8\u95ee\u9898\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u5168\u5c40\u9884\u6d4b\u6a21\u578b\u901a\u8fc7\u5168\u5c40\u5316\u548c\u4ea4\u53c9\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7814\u7a76\u4e86\u7279\u5f81\u8f6c\u6362\u548c\u76ee\u6807\u8f6c\u6362\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u548c\u52a0\u6743\u5b9e\u4f8b\u805a\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5168\u5c40\u76ee\u6807\u8f6c\u6362\u6a21\u578b\u5728\u5168\u5c40\u7279\u5f81\u548c\u805a\u7c7b\u6280\u672f\u652f\u6301\u4e0b\u8868\u73b0\u4f18\u4e8e\u5c40\u90e8\u6a21\u578b\uff0c\u800c\u5168\u5c40\u7279\u5f81\u8f6c\u6362\u6a21\u578b\u9700\u8981\u65f6\u95f4\u5e8f\u5217\u805a\u7c7b\u6765\u5e73\u8861\u5c40\u90e8\u4e0e\u5168\u5c40\u52a8\u6001\u3002", "conclusion": "\u5168\u5c40\u76ee\u6807\u8f6c\u6362\u6a21\u578b\u5728\u8d1f\u8377\u9884\u6d4b\u4e2d\u66f4\u5177\u4f18\u52bf\uff0c\u800c\u7279\u5f81\u8f6c\u6362\u6a21\u578b\u9700\u7ed3\u5408\u805a\u7c7b\u6280\u672f\u4ee5\u5e94\u5bf9\u6570\u636e\u5f02\u8d28\u6027\u3002"}}
{"id": "2507.12133", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.12133", "abs": "https://arxiv.org/abs/2507.12133", "authors": ["Hanwen Liu", "Yuhe Huang", "Yifeng Gong", "Yanjie Zhai", "Jiaxuan Lu"], "title": "HyDRA: A Hybrid Dual-Mode Network for Closed- and Open-Set RFFI with Optimized VMD", "comment": null, "summary": "Device recognition is vital for security in wireless communication systems,\nparticularly for applications like access control. Radio Frequency Fingerprint\nIdentification (RFFI) offers a non-cryptographic solution by exploiting\nhardware-induced signal distortions. This paper proposes HyDRA, a Hybrid\nDual-mode RF Architecture that integrates an optimized Variational Mode\nDecomposition (VMD) with a novel architecture based on the fusion of\nConvolutional Neural Networks (CNNs), Transformers, and Mamba components,\ndesigned to support both closed-set and open-set classification tasks. The\noptimized VMD enhances preprocessing efficiency and classification accuracy by\nfixing center frequencies and using closed-form solutions. HyDRA employs the\nTransformer Dynamic Sequence Encoder (TDSE) for global dependency modeling and\nthe Mamba Linear Flow Encoder (MLFE) for linear-complexity processing, adapting\nto varying conditions. Evaluation on public datasets demonstrates\nstate-of-the-art (SOTA) accuracy in closed-set scenarios and robust performance\nin our proposed open-set classification method, effectively identifying\nunauthorized devices. Deployed on NVIDIA Jetson Xavier NX, HyDRA achieves\nmillisecond-level inference speed with low power consumption, providing a\npractical solution for real-time wireless authentication in real-world\nenvironments.", "AI": {"tldr": "HyDRA\u662f\u4e00\u79cd\u6df7\u5408\u53cc\u6a21RF\u67b6\u6784\uff0c\u7ed3\u5408\u4f18\u5316\u7684VMD\u548c\u65b0\u578bCNN\u3001Transformer\u4e0eMamba\u878d\u5408\u67b6\u6784\uff0c\u652f\u6301\u95ed\u96c6\u548c\u5f00\u96c6\u5206\u7c7b\u4efb\u52a1\uff0c\u5728\u65e0\u7ebf\u8bbe\u5907\u8bc6\u522b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5b9e\u65f6\u8ba4\u8bc1\u3002", "motivation": "\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u8bbe\u5907\u8bc6\u522b\u5bf9\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u8bbf\u95ee\u63a7\u5236\u7b49\u5e94\u7528\u3002RFFI\u63d0\u4f9b\u4e86\u4e00\u79cd\u975e\u52a0\u5bc6\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u786c\u4ef6\u5f15\u8d77\u7684\u4fe1\u53f7\u5931\u771f\u5b9e\u73b0\u8bc6\u522b\u3002", "method": "HyDRA\u7ed3\u5408\u4f18\u5316\u7684VMD\u9884\u5904\u7406\u548cCNN\u3001Transformer\u3001Mamba\u878d\u5408\u67b6\u6784\uff0c\u4f7f\u7528TDSE\u548cMLFE\u5206\u522b\u5efa\u6a21\u5168\u5c40\u4f9d\u8d56\u548c\u7ebf\u6027\u590d\u6742\u5ea6\u5904\u7406\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cHyDRA\u5728\u95ed\u96c6\u573a\u666f\u4e2d\u8fbe\u5230SOTA\u7cbe\u5ea6\uff0c\u5f00\u96c6\u5206\u7c7b\u65b9\u6cd5\u8868\u73b0\u7a33\u5065\uff0c\u5e76\u5728NVIDIA Jetson Xavier NX\u4e0a\u5b9e\u73b0\u6beb\u79d2\u7ea7\u63a8\u7406\u901f\u5ea6\u548c\u4f4e\u529f\u8017\u3002", "conclusion": "HyDRA\u4e3a\u5b9e\u65f6\u65e0\u7ebf\u8ba4\u8bc1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u73af\u5883\u3002"}}
{"id": "2507.11739", "categories": ["cs.LG", "cs.CE", "math.DS"], "pdf": "https://arxiv.org/pdf/2507.11739", "abs": "https://arxiv.org/abs/2507.11739", "authors": ["Urban Fasel"], "title": "Sparse Identification of Nonlinear Dynamics with Conformal Prediction", "comment": null, "summary": "The Sparse Identification of Nonlinear Dynamics (SINDy) is a method for\ndiscovering nonlinear dynamical system models from data. Quantifying\nuncertainty in SINDy models is essential for assessing their reliability,\nparticularly in safety-critical applications. While various uncertainty\nquantification methods exist for SINDy, including Bayesian and ensemble\napproaches, this work explores the integration of Conformal Prediction, a\nframework that can provide valid prediction intervals with coverage guarantees\nbased on minimal assumptions like data exchangeability. We introduce three\napplications of conformal prediction with Ensemble-SINDy (E-SINDy): (1)\nquantifying uncertainty in time series prediction, (2) model selection based on\nlibrary feature importance, and (3) quantifying the uncertainty of identified\nmodel coefficients using feature conformal prediction. We demonstrate the three\napplications on stochastic predator-prey dynamics and several chaotic dynamical\nsystems. We show that conformal prediction methods integrated with E-SINDy can\nreliably achieve desired target coverage for time series forecasting,\neffectively quantify feature importance, and produce more robust uncertainty\nintervals for model coefficients, even under non-Gaussian noise, compared to\nstandard E-SINDy coefficient estimates.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06Conformal Prediction\u6846\u67b6\u4e0eEnsemble-SINDy\u7ed3\u5408\uff0c\u7528\u4e8e\u91cf\u5316\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3001\u7279\u5f81\u91cd\u8981\u6027\u6a21\u578b\u9009\u62e9\u548c\u6a21\u578b\u7cfb\u6570\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e2d\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u91cf\u5316SINDy\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u5bf9\u5176\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002", "method": "\u96c6\u6210Conformal Prediction\u4e0eEnsemble-SINDy\uff08E-SINDy\uff09\uff0c\u5e94\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3001\u7279\u5f81\u91cd\u8981\u6027\u6a21\u578b\u9009\u62e9\u548c\u6a21\u578b\u7cfb\u6570\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u968f\u673a\u6355\u98df\u8005-\u730e\u7269\u52a8\u529b\u5b66\u548c\u6df7\u6c8c\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\uff0cConformal Prediction\u65b9\u6cd5\u80fd\u53ef\u9760\u5b9e\u73b0\u76ee\u6807\u8986\u76d6\u8303\u56f4\uff0c\u6709\u6548\u91cf\u5316\u7279\u5f81\u91cd\u8981\u6027\uff0c\u5e76\u5728\u975e\u9ad8\u65af\u566a\u58f0\u4e0b\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u7cfb\u6570\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\u3002", "conclusion": "Conformal Prediction\u4e0eE-SINDy\u7ed3\u5408\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u91cf\u5316\u6548\u679c\uff0c\u9002\u7528\u4e8e\u590d\u6742\u975e\u7ebf\u6027\u7cfb\u7edf\u3002"}}
{"id": "2507.11757", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11757", "abs": "https://arxiv.org/abs/2507.11757", "authors": ["Yuehua Song", "Yong Gao"], "title": "A Graph-in-Graph Learning Framework for Drug-Target Interaction Prediction", "comment": null, "summary": "Accurately predicting drug-target interactions (DTIs) is pivotal for\nadvancing drug discovery and target validation techniques. While machine\nlearning approaches including those that are based on Graph Neural Networks\n(GNN) have achieved notable success in DTI prediction, many of them have\ndifficulties in effectively integrating the diverse features of drugs, targets\nand their interactions. To address this limitation, we introduce a novel\nframework to take advantage of the power of both transductive learning and\ninductive learning so that features at molecular level and drug-target\ninteraction network level can be exploited. Within this framework is a\nGNN-based model called Graph-in-Graph (GiG) that represents graphs of drug and\ntarget molecular structures as meta-nodes in a drug-target interaction graph,\nenabling a detailed exploration of their intricate relationships. To evaluate\nthe proposed model, we have compiled a special benchmark comprising drug\nSMILES, protein sequences, and their interaction data, which is interesting in\nits own right. Our experimental results demonstrate that the GiG model\nsignificantly outperforms existing approaches across all evaluation metrics,\nhighlighting the benefits of integrating different learning paradigms and\ninteraction data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGraph-in-Graph\uff08GiG\uff09\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u8f6c\u5bfc\u5b66\u4e60\u548c\u5f52\u7eb3\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u836f\u7269-\u9776\u6807\u76f8\u4e92\u4f5c\u7528\uff08DTI\uff09\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684DTI\u9884\u6d4b\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6574\u5408\u836f\u7269\u3001\u9776\u6807\u53ca\u5176\u76f8\u4e92\u4f5c\u7528\u7684\u591a\u6837\u5316\u7279\u5f81\u3002", "method": "GiG\u6846\u67b6\u5c06\u836f\u7269\u548c\u9776\u6807\u5206\u5b50\u7ed3\u6784\u56fe\u8868\u793a\u4e3a\u836f\u7269-\u9776\u6807\u76f8\u4e92\u4f5c\u7528\u56fe\u4e2d\u7684\u5143\u8282\u70b9\uff0c\u7ed3\u5408\u8f6c\u5bfc\u5b66\u4e60\u548c\u5f52\u7eb3\u5b66\u4e60\uff0c\u63a2\u7d22\u5176\u590d\u6742\u5173\u7cfb\u3002", "result": "GiG\u6a21\u578b\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "GiG\u6846\u67b6\u901a\u8fc7\u6574\u5408\u4e0d\u540c\u5b66\u4e60\u8303\u5f0f\u548c\u76f8\u4e92\u4f5c\u7528\u6570\u636e\uff0c\u4e3aDTI\u9884\u6d4b\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11759", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11759", "abs": "https://arxiv.org/abs/2507.11759", "authors": ["Alexandra Volokhova", "L\u00e9na N\u00e9hale Ezzine", "Piotr Gai\u0144ski", "Luca Scimeca", "Emmanuel Bengio", "Prudencio Tossou", "Yoshua Bengio", "Alex Hernandez-Garcia"], "title": "Torsional-GFN: a conditional conformation generator for small molecules", "comment": "The two first authors are Alexandra Volokhova and L\\'ena N\\'ehale\n  Ezzine, with equal contribution", "summary": "Generating stable molecular conformations is crucial in several drug\ndiscovery applications, such as estimating the binding affinity of a molecule\nto a target. Recently, generative machine learning methods have emerged as a\npromising, more efficient method than molecular dynamics for sampling of\nconformations from the Boltzmann distribution. In this paper, we introduce\nTorsional-GFN, a conditional GFlowNet specifically designed to sample\nconformations of molecules proportionally to their Boltzmann distribution,\nusing only a reward function as training signal. Conditioned on a molecular\ngraph and its local structure (bond lengths and angles), Torsional-GFN samples\nrotations of its torsion angles. Our results demonstrate that Torsional-GFN is\nable to sample conformations approximately proportional to the Boltzmann\ndistribution for multiple molecules with a single model, and allows for\nzero-shot generalization to unseen bond lengths and angles coming from the MD\nsimulations for such molecules. Our work presents a promising avenue for\nscaling the proposed approach to larger molecular systems, achieving zero-shot\ngeneralization to unseen molecules, and including the generation of the local\nstructure into the GFlowNet model.", "AI": {"tldr": "Torsional-GFN\u662f\u4e00\u79cd\u57fa\u4e8eGFlowNet\u7684\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u4e2d\u91c7\u6837\u5206\u5b50\u6784\u8c61\uff0c\u4ec5\u9700\u5956\u52b1\u51fd\u6570\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\u3002", "motivation": "\u5728\u836f\u7269\u53d1\u73b0\u4e2d\uff0c\u751f\u6210\u7a33\u5b9a\u7684\u5206\u5b50\u6784\u8c61\u5bf9\u4f30\u8ba1\u5206\u5b50\u4e0e\u9776\u6807\u7684\u7ed3\u5408\u4eb2\u548c\u529b\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u65b9\u6cd5\u5982\u5206\u5b50\u52a8\u529b\u5b66\u6548\u7387\u8f83\u4f4e\uff0c\u800c\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u66f4\u5177\u6f5c\u529b\u3002", "method": "Torsional-GFN\u662f\u4e00\u79cd\u6761\u4ef6GFlowNet\uff0c\u57fa\u4e8e\u5206\u5b50\u56fe\u548c\u5c40\u90e8\u7ed3\u6784\uff08\u952e\u957f\u548c\u952e\u89d2\uff09\u91c7\u6837\u626d\u8f6c\u89d2\u7684\u65cb\u8f6c\u3002", "result": "Torsional-GFN\u80fd\u591f\u8fd1\u4f3c\u73bb\u5c14\u5179\u66fc\u5206\u5e03\u91c7\u6837\u5206\u5b50\u6784\u8c61\uff0c\u5e76\u5b9e\u73b0\u5bf9\u65b0\u952e\u957f\u548c\u952e\u89d2\u7684\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6269\u5c55\u5230\u66f4\u5927\u5206\u5b50\u7cfb\u7edf\u3001\u5b9e\u73b0\u5bf9\u65b0\u5206\u5b50\u7684\u96f6\u6837\u672c\u6cdb\u5316\u4ee5\u53ca\u751f\u6210\u5c40\u90e8\u7ed3\u6784\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2507.11771", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11771", "abs": "https://arxiv.org/abs/2507.11771", "authors": ["Sheikh Abdur Raheem Ali", "Justin Xu", "Ivory Yang", "Jasmine Xinze Li", "Ayse Arslan", "Clark Benham"], "title": "Scaling laws for activation steering with Llama 2 models and refusal mechanisms", "comment": null, "summary": "As large language models (LLMs) evolve in complexity and capability, the\nefficacy of less widely deployed alignment techniques are uncertain. Building\non previous work on activation steering and contrastive activation addition\n(CAA), this paper explores the effectiveness of CAA with model scale using the\nfamily of Llama 2 models (7B, 13B, and 70B). CAA works by finding desirable\n'directions' in the model's residual stream vector space using contrastive\npairs (for example, hate to love) and adding this direction to the residual\nstream during the forward pass. It directly manipulates the residual stream and\naims to extract features from language models to better control their outputs.\nUsing answer matching questions centered around the refusal behavior, we found\nthat 1) CAA is most effective when applied at early-mid layers. 2) The\neffectiveness of CAA diminishes with model size. 3) Negative steering has more\npronounced effects than positive steering across all model sizes.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5bf9\u6bd4\u6fc0\u6d3b\u52a0\u6cd5\uff08CAA\uff09\u5728Llama 2\u6a21\u578b\uff087B\u300113B\u300170B\uff09\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5176\u5728\u65e9\u671f\u5230\u4e2d\u5c42\u6700\u6709\u6548\uff0c\u4f46\u968f\u7740\u6a21\u578b\u89c4\u6a21\u589e\u5927\u6548\u679c\u51cf\u5f31\uff0c\u4e14\u8d1f\u5411\u5f15\u5bfc\u6bd4\u6b63\u5411\u5f15\u5bfc\u66f4\u663e\u8457\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u529b\u589e\u5f3a\uff0c\u8f83\u5c11\u5e7f\u6cdb\u90e8\u7f72\u7684\u5bf9\u9f50\u6280\u672f\u6548\u679c\u5c1a\u4e0d\u660e\u786e\uff0c\u56e0\u6b64\u63a2\u7d22CAA\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u5bf9\uff08\u5982\u4ec7\u6068\u5230\u7231\uff09\u5728\u6b8b\u5dee\u6d41\u5411\u91cf\u7a7a\u95f4\u4e2d\u627e\u5230\u65b9\u5411\uff0c\u5e76\u5728\u524d\u5411\u4f20\u64ad\u65f6\u6dfb\u52a0\u8be5\u65b9\u5411\uff0c\u76f4\u63a5\u64cd\u63a7\u6b8b\u5dee\u6d41\u4ee5\u63d0\u53d6\u7279\u5f81\u63a7\u5236\u8f93\u51fa\u3002", "result": "1\uff09CAA\u5728\u65e9\u671f\u5230\u4e2d\u5c42\u6700\u6709\u6548\uff1b2\uff09CAA\u6548\u679c\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u51cf\u5f31\uff1b3\uff09\u8d1f\u5411\u5f15\u5bfc\u5728\u6240\u6709\u6a21\u578b\u89c4\u6a21\u4e2d\u6548\u679c\u66f4\u663e\u8457\u3002", "conclusion": "CAA\u7684\u6548\u679c\u53d7\u6a21\u578b\u89c4\u6a21\u548c\u5f15\u5bfc\u65b9\u5411\u5f71\u54cd\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u4f18\u5316\u5bf9\u9f50\u6280\u672f\u3002"}}
{"id": "2507.11776", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11776", "abs": "https://arxiv.org/abs/2507.11776", "authors": ["Merel Kampere", "Ali Mohammed Mansoor Alsahag"], "title": "Predicting Delayed Trajectories Using Network Features: A Study on the Dutch Railway Network", "comment": null, "summary": "The Dutch railway network is one of the busiest in the world, with delays\nbeing a prominent concern for the principal passenger railway operator NS. This\nresearch addresses a gap in delay prediction studies within the Dutch railway\nnetwork by employing an XGBoost Classifier with a focus on topological\nfeatures. Current research predominantly emphasizes short-term predictions and\nneglects the broader network-wide patterns essential for mitigating ripple\neffects. This research implements and improves an existing methodology,\noriginally designed to forecast the evolution of the fast-changing US air\nnetwork, to predict delays in the Dutch Railways. By integrating Node\nCentrality Measures and comparing multiple classifiers like RandomForest,\nDecisionTree, GradientBoosting, AdaBoost, and LogisticRegression, the goal is\nto predict delayed trajectories. However, the results reveal limited\nperformance, especially in non-simultaneous testing scenarios, suggesting the\nnecessity for more context-specific adaptations. Regardless, this research\ncontributes to the understanding of transportation network evaluation and\nproposes future directions for developing more robust predictive models for\ndelays.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528XGBoost\u5206\u7c7b\u5668\u7ed3\u5408\u62d3\u6251\u7279\u5f81\u9884\u6d4b\u8377\u5170\u94c1\u8def\u7f51\u7edc\u5ef6\u8bef\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4f46\u7ed3\u679c\u8868\u73b0\u6709\u9650\u3002", "motivation": "\u8377\u5170\u94c1\u8def\u7f51\u7edc\u662f\u5168\u7403\u6700\u7e41\u5fd9\u7684\u94c1\u8def\u4e4b\u4e00\uff0c\u5ef6\u8bef\u95ee\u9898\u7a81\u51fa\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u77ed\u671f\u9884\u6d4b\uff0c\u5ffd\u7565\u4e86\u7f51\u7edc\u8303\u56f4\u7684\u6a21\u5f0f\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6539\u8fdb\u73b0\u6709\u65b9\u6cd5\uff08\u539f\u7528\u4e8e\u7f8e\u56fd\u822a\u7a7a\u7f51\u7edc\uff09\uff0c\u7ed3\u5408\u8282\u70b9\u4e2d\u5fc3\u6027\u5ea6\u91cf\uff0c\u6bd4\u8f83\u591a\u79cd\u5206\u7c7b\u5668\uff08\u5982\u968f\u673a\u68ee\u6797\u3001\u51b3\u7b56\u6811\u7b49\uff09\u9884\u6d4b\u5ef6\u8bef\u8f68\u8ff9\u3002", "result": "\u7ed3\u679c\u8868\u73b0\u6709\u9650\uff0c\u5c24\u5176\u5728\u975e\u540c\u6b65\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0c\u8868\u660e\u9700\u8981\u66f4\u591a\u4e0a\u4e0b\u6587\u7279\u5b9a\u8c03\u6574\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4ea4\u901a\u7f51\u7edc\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u5f00\u53d1\u66f4\u9c81\u68d2\u9884\u6d4b\u6a21\u578b\u7684\u65b9\u5411\u3002"}}
{"id": "2507.11789", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11789", "abs": "https://arxiv.org/abs/2507.11789", "authors": ["Alessandro Palma", "Sergei Rybakov", "Leon Hetzel", "Stephan G\u00fcnnemann", "Fabian J. Theis"], "title": "Enforcing Latent Euclidean Geometry in Single-Cell VAEs for Manifold Interpolation", "comment": "31 pages, 14 figures", "summary": "Latent space interpolations are a powerful tool for navigating deep\ngenerative models in applied settings. An example is single-cell RNA\nsequencing, where existing methods model cellular state transitions as latent\nspace interpolations with variational autoencoders, often assuming linear\nshifts and Euclidean geometry. However, unless explicitly enforced, linear\ninterpolations in the latent space may not correspond to geodesic paths on the\ndata manifold, limiting methods that assume Euclidean geometry in the data\nrepresentations. We introduce FlatVI, a novel training framework that\nregularises the latent manifold of discrete-likelihood variational autoencoders\ntowards Euclidean geometry, specifically tailored for modelling single-cell\ncount data. By encouraging straight lines in the latent space to approximate\ngeodesic interpolations on the decoded single-cell manifold, FlatVI enhances\ncompatibility with downstream approaches that assume Euclidean latent geometry.\nExperiments on synthetic data support the theoretical soundness of our\napproach, while applications to time-resolved single-cell RNA sequencing data\ndemonstrate improved trajectory reconstruction and manifold interpolation.", "AI": {"tldr": "FlatVI\u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u79bb\u6563\u4f3c\u7136\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u6f5c\u5728\u6d41\u5f62\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff0c\u4ece\u800c\u4f18\u5316\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u7684\u5efa\u6a21\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u4e2d\u5047\u8bbe\u7ebf\u6027\u8f6c\u79fb\u548c\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff0c\u4f46\u7ebf\u6027\u63d2\u503c\u53ef\u80fd\u4e0d\u7b26\u5408\u6570\u636e\u6d41\u5f62\u4e0a\u7684\u6d4b\u5730\u7ebf\u8def\u5f84\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u6548\u679c\u3002", "method": "FlatVI\u901a\u8fc7\u6b63\u5219\u5316\u6f5c\u5728\u6d41\u5f62\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u6b27\u51e0\u91cc\u5f97\u51e0\u4f55\uff0c\u4ece\u800c\u4f18\u5316\u5355\u7ec6\u80de\u6570\u636e\u7684\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eFlatVI\u5728\u5408\u6210\u6570\u636e\u548c\u5355\u7ec6\u80deRNA\u6d4b\u5e8f\u6570\u636e\u4e2d\u63d0\u9ad8\u4e86\u8f68\u8ff9\u91cd\u5efa\u548c\u6d41\u5f62\u63d2\u503c\u7684\u51c6\u786e\u6027\u3002", "conclusion": "FlatVI\u901a\u8fc7\u4f18\u5316\u6f5c\u5728\u6d41\u5f62\u7684\u51e0\u4f55\u7279\u6027\uff0c\u63d0\u5347\u4e86\u5355\u7ec6\u80de\u6570\u636e\u5efa\u6a21\u7684\u6548\u679c\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38\u5206\u6790\u3002"}}
{"id": "2507.11807", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11807", "abs": "https://arxiv.org/abs/2507.11807", "authors": ["Ruofan Hu", "Dongyu Zhang", "Huayi Zhang", "Elke Rundensteiner"], "title": "CLID-MU: Cross-Layer Information Divergence Based Meta Update Strategy for Learning with Noisy Labels", "comment": "KDD 2025, 12 pages, 7 figures", "summary": "Learning with noisy labels (LNL) is essential for training deep neural\nnetworks with imperfect data. Meta-learning approaches have achieved success by\nusing a clean unbiased labeled set to train a robust model. However, this\napproach heavily depends on the availability of a clean labeled meta-dataset,\nwhich is difficult to obtain in practice. In this work, we thus tackle the\nchallenge of meta-learning for noisy label scenarios without relying on a clean\nlabeled dataset. Our approach leverages the data itself while bypassing the\nneed for labels. Building on the insight that clean samples effectively\npreserve the consistency of related data structures across the last hidden and\nthe final layer, whereas noisy samples disrupt this consistency, we design the\nCross-layer Information Divergence-based Meta Update Strategy (CLID-MU).\nCLID-MU leverages the alignment of data structures across these diverse feature\nspaces to evaluate model performance and use this alignment to guide training.\nExperiments on benchmark datasets with varying amounts of labels under both\nsynthetic and real-world noise demonstrate that CLID-MU outperforms\nstate-of-the-art methods. The code is released at\nhttps://github.com/ruofanhu/CLID-MU.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f9d\u8d56\u5e72\u51c0\u6807\u6ce8\u6570\u636e\u7684\u5143\u5b66\u4e60\u65b9\u6cd5CLID-MU\uff0c\u7528\u4e8e\u5904\u7406\u566a\u58f0\u6807\u7b7e\u573a\u666f\uff0c\u901a\u8fc7\u8de8\u5c42\u4fe1\u606f\u5dee\u5f02\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u5e76\u6307\u5bfc\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5143\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5e72\u51c0\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u96be\u4ee5\u83b7\u53d6\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u65e0\u5e72\u51c0\u6807\u6ce8\u6570\u636e\u65f6\u7684\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u63d0\u51faCLID-MU\u7b56\u7565\uff0c\u5229\u7528\u5e72\u51c0\u6837\u672c\u5728\u9690\u85cf\u5c42\u548c\u8f93\u51fa\u5c42\u6570\u636e\u7ed3\u6784\u7684\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u8de8\u5c42\u4fe1\u606f\u5dee\u5f02\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u5e76\u6307\u5bfc\u8bad\u7ec3\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u566a\u58f0\u573a\u666f\u4e0b\uff0cCLID-MU\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CLID-MU\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u5e72\u51c0\u6807\u6ce8\u6570\u636e\u65f6\u7684\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.11818", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11818", "abs": "https://arxiv.org/abs/2507.11818", "authors": ["Andrei Rekesh", "Miruna Cretu", "Dmytro Shevchuk", "Vignesh Ram Somnath", "Pietro Li\u00f2", "Robert A. Batey", "Mike Tyers", "Micha\u0142 Koziarski", "Cheng-Hao Liu"], "title": "SynCoGen: Synthesizable 3D Molecule Generation via Joint Reaction and Coordinate Modeling", "comment": null, "summary": "Ensuring synthesizability in generative small molecule design remains a major\nchallenge. While recent developments in synthesizable molecule generation have\ndemonstrated promising results, these efforts have been largely confined to 2D\nmolecular graph representations, limiting the ability to perform geometry-based\nconditional generation. In this work, we present SynCoGen (Synthesizable\nCo-Generation), a single framework that combines simultaneous masked graph\ndiffusion and flow matching for synthesizable 3D molecule generation. SynCoGen\nsamples from the joint distribution of molecular building blocks, chemical\nreactions, and atomic coordinates. To train the model, we curated SynSpace, a\ndataset containing over 600K synthesis-aware building block graphs and 3.3M\nconformers. SynCoGen achieves state-of-the-art performance in unconditional\nsmall molecule graph and conformer generation, and the model delivers\ncompetitive performance in zero-shot molecular linker design for protein ligand\ngeneration in drug discovery. Overall, this multimodal formulation represents a\nfoundation for future applications enabled by non-autoregressive molecular\ngeneration, including analog expansion, lead optimization, and direct structure\nconditioning.", "AI": {"tldr": "SynCoGen\u662f\u4e00\u4e2a\u7ed3\u5408\u63a9\u7801\u56fe\u6269\u6563\u548c\u6d41\u5339\u914d\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u5408\u6210\u76843D\u5206\u5b50\uff0c\u5728\u65e0\u6761\u4ef6\u5c0f\u5206\u5b50\u56fe\u548c\u6784\u8c61\u751f\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u751f\u6210\u53ef\u5408\u6210\u7684\u5c0f\u5206\u5b50\u8bbe\u8ba1\u4ecd\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5c40\u9650\u4e8e2D\u5206\u5b50\u56fe\u8868\u793a\uff0c\u9650\u5236\u4e86\u57fa\u4e8e\u51e0\u4f55\u7684\u6761\u4ef6\u751f\u6210\u80fd\u529b\u3002", "method": "\u63d0\u51faSynCoGen\u6846\u67b6\uff0c\u7ed3\u5408\u63a9\u7801\u56fe\u6269\u6563\u548c\u6d41\u5339\u914d\uff0c\u4ece\u5206\u5b50\u6784\u5efa\u5757\u3001\u5316\u5b66\u53cd\u5e94\u548c\u539f\u5b50\u5750\u6807\u7684\u8054\u5408\u5206\u5e03\u4e2d\u91c7\u6837\u3002", "result": "\u5728\u65e0\u6761\u4ef6\u5c0f\u5206\u5b50\u56fe\u548c\u6784\u8c61\u751f\u6210\u4e2d\u8fbe\u5230SOTA\uff0c\u5e76\u5728\u96f6\u5c04\u51fb\u5206\u5b50\u8fde\u63a5\u8bbe\u8ba1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u591a\u6a21\u6001\u6846\u67b6\u4e3a\u672a\u6765\u975e\u81ea\u56de\u5f52\u5206\u5b50\u751f\u6210\u5e94\u7528\uff08\u5982\u7c7b\u4f3c\u7269\u6269\u5c55\u3001\u5148\u5bfc\u4f18\u5316\uff09\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.11821", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11821", "abs": "https://arxiv.org/abs/2507.11821", "authors": ["Pouya Shaeri", "Arash Karimi", "Ariane Middel"], "title": "MNIST-Gen: A Modular MNIST-Style Dataset Generation Using Hierarchical Semantics, Reinforcement Learning, and Category Theory", "comment": "Submitted to a computer science conference", "summary": "Neural networks are often benchmarked using standard datasets such as MNIST,\nFashionMNIST, or other variants of MNIST, which, while accessible, are limited\nto generic classes such as digits or clothing items. For researchers working on\ndomain-specific tasks, such as classifying trees, food items, or other\nreal-world objects, these data sets are insufficient and irrelevant.\nAdditionally, creating and publishing a custom dataset can be time consuming,\nlegally constrained, or beyond the scope of individual projects. We present\nMNIST-Gen, an automated, modular, and adaptive framework for generating\nMNIST-style image datasets tailored to user-specified categories using\nhierarchical semantic categorization. The system combines CLIP-based semantic\nunderstanding with reinforcement learning and human feedback to achieve\nintelligent categorization with minimal manual intervention. Our hierarchical\napproach supports complex category structures with semantic characteristics,\nenabling fine-grained subcategorization and multiple processing modes:\nindividual review for maximum control, smart batch processing for large\ndatasets, and fast batch processing for rapid creation. Inspired by category\ntheory, MNIST-Gen models each data transformation stage as a composable\nmorphism, enhancing clarity, modularity, and extensibility. As proof of\nconcept, we generate and benchmark two novel datasets-\\textit{Tree-MNIST} and\n\\textit{Food-MNIST}-demonstrating MNIST-Gen's utility for producing\ntask-specific evaluation data while achieving 85\\% automatic categorization\naccuracy and 80\\% time savings compared to manual approaches.", "AI": {"tldr": "MNIST-Gen\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7279\u5b9a\u9886\u57df\u7684MNIST\u98ce\u683c\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u6807\u51c6\u6570\u636e\u96c6\u5728\u7279\u5b9a\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u6807\u51c6\u6570\u636e\u96c6\uff08\u5982MNIST\uff09\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u4e0d\u9002\u7528\uff0c\u4e14\u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\u8017\u65f6\u4e14\u590d\u6742\u3002", "method": "\u7ed3\u5408CLIP\u8bed\u4e49\u7406\u89e3\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u4eba\u7c7b\u53cd\u9988\uff0c\u901a\u8fc7\u5206\u5c42\u8bed\u4e49\u5206\u7c7b\u751f\u6210\u6570\u636e\u96c6\u3002", "result": "\u751f\u6210\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff08Tree-MNIST\u548cFood-MNIST\uff09\uff0c\u81ea\u52a8\u5206\u7c7b\u51c6\u786e\u7387\u8fbe85%\uff0c\u8282\u770180%\u65f6\u95f4\u3002", "conclusion": "MNIST-Gen\u4e3a\u7279\u5b9a\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u7684\u6570\u636e\u96c6\u751f\u6210\u5de5\u5177\uff0c\u5177\u6709\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.11836", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11836", "abs": "https://arxiv.org/abs/2507.11836", "authors": ["Jian Gao", "Jianshe Wu", "JingYi Ding"], "title": "HyperEvent:Learning Cohesive Events for Large-scale Dynamic Link Prediction", "comment": null, "summary": "Dynamic link prediction in continuous-time dynamic graphs is a fundamental\ntask for modeling evolving complex systems. Existing node-centric and\nevent-centric methods focus on individual interactions or atomic states,\nfailing to capture the structural cohesion of composite hyper-events, groups of\ncausally related events. To address this, we propose HyperEvent, a framework\nreframing dynamic link prediction as hyper-event recognition. Central to\nHyperEvent is the dynamic construction of an association sequence using event\ncorrelation vectors. These vectors quantify pairwise dependencies between the\nquery event and relevant historical events, thereby characterizing the\nstructural cohesion of a potential hyper-event. The framework predicts the\noccurrence of the query event by evaluating whether it collectively forms a\nvalid hyper-event with these historical events. Notably, HyperEvent outperforms\nstate-of-the-art methods on 4 out of 5 datasets in the official leaderboard.\nFor scalability, we further introduce an efficient parallel training algorithm\nthat segments large event streams to enable concurrent training. Experiments\nvalidate HyperEvent's superior accuracy and efficiency on large-scale graphs.\nAmong which HyperEvent achieves a 6.95% improvement in Mean Reciprocal Rank\nover state-of-the-art baseline on the large-scale Flight dataset while\nutilizing only 10.17% of the training time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faHyperEvent\u6846\u67b6\uff0c\u5c06\u52a8\u6001\u94fe\u63a5\u9884\u6d4b\u91cd\u6784\u4e3a\u8d85\u4e8b\u4ef6\u8bc6\u522b\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u5173\u6027\u5411\u91cf\u52a8\u6001\u6784\u5efa\u5173\u8054\u5e8f\u5217\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u590d\u5408\u8d85\u4e8b\u4ef6\u7684\u7ed3\u6784\u51dd\u805a\u529b\uff0c\u9700\u89e3\u51b3\u52a8\u6001\u56fe\u4e2d\u94fe\u63a5\u9884\u6d4b\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faHyperEvent\u6846\u67b6\uff0c\u5229\u7528\u4e8b\u4ef6\u76f8\u5173\u6027\u5411\u91cf\u52a8\u6001\u6784\u5efa\u5173\u8054\u5e8f\u5217\uff0c\u8bc4\u4f30\u67e5\u8be2\u4e8b\u4ef6\u4e0e\u5386\u53f2\u4e8b\u4ef6\u662f\u5426\u5f62\u6210\u6709\u6548\u8d85\u4e8b\u4ef6\u3002", "result": "\u57285\u4e2a\u6570\u636e\u96c6\u4e2d4\u4e2a\u8868\u73b0\u6700\u4f18\uff0c\u5927\u89c4\u6a21Flight\u6570\u636e\u96c6\u4e0aMRR\u63d0\u53476.95%\uff0c\u8bad\u7ec3\u65f6\u95f4\u4ec5\u970010.17%\u3002", "conclusion": "HyperEvent\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u52a8\u6001\u56fe\u3002"}}
{"id": "2507.11839", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.11839", "abs": "https://arxiv.org/abs/2507.11839", "authors": ["Chengyue Gong", "Xinshi Chen", "Yuxuan Zhang", "Yuxuan Song", "Hao Zhou", "Wenzhi Xiao"], "title": "Protenix-Mini: Efficient Structure Predictor via Compact Architecture, Few-Step Diffusion and Switchable pLM", "comment": null, "summary": "Lightweight inference is critical for biomolecular structure prediction and\nother downstream tasks, enabling efficient real-world deployment and\ninference-time scaling for large-scale applications. In this work, we address\nthe challenge of balancing model efficiency and prediction accuracy by making\nseveral key modifications, 1) Multi-step AF3 sampler is replaced by a few-step\nODE sampler, significantly reducing computational overhead for the diffusion\nmodule part during inference; 2) In the open-source Protenix framework, a\nsubset of pairformer or diffusion transformer blocks doesn't make contributions\nto the final structure prediction, presenting opportunities for architectural\npruning and lightweight redesign; 3) A model incorporating an ESM module is\ntrained to substitute the conventional MSA module, reducing MSA preprocessing\ntime. Building on these key insights, we present Protenix-Mini, a compact and\noptimized model designed for efficient protein structure prediction. This\nstreamlined version incorporates a more efficient architectural design with a\ntwo-step Ordinary Differential Equation (ODE) sampling strategy. By eliminating\nredundant Transformer components and refining the sampling process,\nProtenix-Mini significantly reduces model complexity with slight accuracy drop.\nEvaluations on benchmark datasets demonstrate that it achieves high-fidelity\npredictions, with only a negligible 1 to 5 percent decrease in performance on\nbenchmark datasets compared to its full-scale counterpart. This makes\nProtenix-Mini an ideal choice for applications where computational resources\nare limited but accurate structure prediction remains crucial.", "AI": {"tldr": "Protenix-Mini\u662f\u4e00\u4e2a\u8f7b\u91cf\u5316\u7684\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u4f18\u5316\u67b6\u6784\u548c\u91c7\u6837\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u5206\u5b50\u7ed3\u6784\u9884\u6d4b\u4e2d\u6a21\u578b\u6548\u7387\u4e0e\u9884\u6d4b\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u5e94\u7528\u7684\u9ad8\u6548\u90e8\u7f72\u3002", "method": "1) \u7528\u5c11\u6b65ODE\u91c7\u6837\u5668\u66ff\u4ee3\u591a\u6b65AF3\u91c7\u6837\u5668\uff1b2) \u526a\u679d\u5197\u4f59\u7684Transformer\u6a21\u5757\uff1b3) \u7528ESM\u6a21\u5757\u66ff\u4ee3\u4f20\u7edfMSA\u6a21\u5757\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cProtenix-Mini\u4ec5\u6bd4\u5b8c\u6574\u6a21\u578b\u6027\u80fd\u4e0b\u964d1-5%\uff0c\u4f46\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "Protenix-Mini\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u4f46\u9700\u8981\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u7684\u573a\u666f\u3002"}}
{"id": "2507.11855", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11855", "abs": "https://arxiv.org/abs/2507.11855", "authors": ["Davin Hill", "Brian L. Hill", "Aria Masoomi", "Vijay S. Nori", "Robert E. Tillman", "Jennifer Dy"], "title": "OrdShap: Feature Position Importance for Sequential Black-Box Models", "comment": null, "summary": "Sequential deep learning models excel in domains with temporal or sequential\ndependencies, but their complexity necessitates post-hoc feature attribution\nmethods for understanding their predictions. While existing techniques quantify\nfeature importance, they inherently assume fixed feature ordering - conflating\nthe effects of (1) feature values and (2) their positions within input\nsequences. To address this gap, we introduce OrdShap, a novel attribution\nmethod that disentangles these effects by quantifying how a model's predictions\nchange in response to permuting feature position. We establish a game-theoretic\nconnection between OrdShap and Sanchez-Berganti\\~nos values, providing a\ntheoretically grounded approach to position-sensitive attribution. Empirical\nresults from health, natural language, and synthetic datasets highlight\nOrdShap's effectiveness in capturing feature value and feature position\nattributions, and provide deeper insight into model behavior.", "AI": {"tldr": "OrdShap\u662f\u4e00\u79cd\u65b0\u7684\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316\u7279\u5f81\u4f4d\u7f6e\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7279\u5f81\u503c\u4e0e\u4f4d\u7f6e\u6df7\u6dc6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u5047\u8bbe\u7279\u5f81\u987a\u5e8f\u56fa\u5b9a\uff0c\u65e0\u6cd5\u533a\u5206\u7279\u5f81\u503c\u4e0e\u5176\u4f4d\u7f6e\u5bf9\u6a21\u578b\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51faOrdShap\u65b9\u6cd5\uff0c\u901a\u8fc7\u6392\u5217\u7279\u5f81\u4f4d\u7f6e\u6765\u91cf\u5316\u5176\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u4e0eSanchez-Berganti\u00f1os\u503c\u5efa\u7acb\u7406\u8bba\u8054\u7cfb\u3002", "result": "\u5728\u5065\u5eb7\u3001\u81ea\u7136\u8bed\u8a00\u548c\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOrdShap\u80fd\u6709\u6548\u6355\u6349\u7279\u5f81\u503c\u548c\u4f4d\u7f6e\u7684\u5f71\u54cd\uff0c\u63d0\u4f9b\u66f4\u6df1\u5165\u7684\u6a21\u578b\u884c\u4e3a\u7406\u89e3\u3002", "conclusion": "OrdShap\u4e3a\u7406\u89e3\u5e8f\u5217\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7ec6\u7c92\u5ea6\u7684\u5f52\u56e0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7279\u5f81\u503c\u4e0e\u4f4d\u7f6e\u6df7\u6dc6\u7684\u95ee\u9898\u3002"}}
{"id": "2507.11865", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11865", "abs": "https://arxiv.org/abs/2507.11865", "authors": ["Hanwen Dai", "Chang Gao", "Fang He", "Congyuan Ji", "Yanni Yang"], "title": "A Policy-Improved Deep Deterministic Policy Gradient Framework for the Discount Order Acceptance Strategy of Ride-hailing Drivers", "comment": null, "summary": "The rapid expansion of platform integration has emerged as an effective\nsolution to mitigate market fragmentation by consolidating multiple\nride-hailing platforms into a single application. To address heterogeneous\npassenger preferences, third-party integrators provide Discount Express service\ndelivered by express drivers at lower trip fares. For the individual platform,\nencouraging broader participation of drivers in Discount Express services has\nthe potential to expand the accessible demand pool and improve matching\nefficiency, but often at the cost of reduced profit margins. This study aims to\ndynamically manage drivers' acceptance of Discount Express from the perspective\nof individual platforms. The lack of historical data under the new business\nmodel necessitates online learning. However, early-stage exploration through\ntrial and error can be costly in practice, highlighting the need for reliable\nearly-stage performance in real-world deployment. To address these challenges,\nthis study formulates the decision regarding the proportion of drivers'\nacceptance behavior as a continuous control task. In response to the high\nstochasticity, the opaque matching mechanisms employed by third-party\nintegrator, and the limited availability of historical data, we propose a\npolicy-improved deep deterministic policy gradient (pi-DDPG) framework. The\nproposed framework incorporates a refiner module to boost policy performance\nduring the early training phase, leverages a convolutional long short-term\nmemory network to effectively capture complex spatiotemporal patterns, and\nadopts a prioritized experience replay mechanism to enhance learning\nefficiency. A simulator based on a real-world dataset is developed to validate\nthe effectiveness of the proposed pi-DDPG. Numerical experiments demonstrate\nthat pi-DDPG achieves superior learning efficiency and significantly reduces\nearly-stage training losses.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u7ba1\u7406\u53f8\u673a\u63a5\u53d7\u6298\u6263\u5feb\u8f66\u670d\u52a1\u7684\u65b9\u6cd5\uff0c\u91c7\u7528pi-DDPG\u6846\u67b6\u89e3\u51b3\u9ad8\u968f\u673a\u6027\u548c\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5e73\u53f0\u6574\u5408\u867d\u80fd\u51cf\u5c11\u5e02\u573a\u788e\u7247\u5316\uff0c\u4f46\u6298\u6263\u5feb\u8f66\u670d\u52a1\u53ef\u80fd\u964d\u4f4e\u5229\u6da6\u3002\u7814\u7a76\u65e8\u5728\u52a8\u6001\u7ba1\u7406\u53f8\u673a\u53c2\u4e0e\u884c\u4e3a\uff0c\u5e73\u8861\u9700\u6c42\u6269\u5c55\u4e0e\u5229\u6da6\u635f\u5931\u3002", "method": "\u63d0\u51fapi-DDPG\u6846\u67b6\uff0c\u5305\u542b\u7cbe\u70bc\u6a21\u5757\u3001\u5377\u79efLSTM\u7f51\u7edc\u548c\u4f18\u5148\u7ecf\u9a8c\u56de\u653e\u673a\u5236\uff0c\u4ee5\u5e94\u5bf9\u9ad8\u968f\u673a\u6027\u548c\u6570\u636e\u4e0d\u8db3\u3002", "result": "pi-DDPG\u5728\u4eff\u771f\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u5b66\u4e60\u80fd\u529b\uff0c\u663e\u8457\u51cf\u5c11\u65e9\u671f\u8bad\u7ec3\u635f\u5931\u3002", "conclusion": "pi-DDPG\u6846\u67b6\u80fd\u6709\u6548\u7ba1\u7406\u53f8\u673a\u53c2\u4e0e\u884c\u4e3a\uff0c\u4e3a\u5e73\u53f0\u63d0\u4f9b\u53ef\u9760\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2507.11901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11901", "abs": "https://arxiv.org/abs/2507.11901", "authors": ["Juscimara G. Avelino", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "Imbalanced Regression Pipeline Recommendation", "comment": null, "summary": "Imbalanced problems are prevalent in various real-world scenarios and are\nextensively explored in classification tasks. However, they also present\nchallenges for regression tasks due to the rarity of certain target values. A\ncommon alternative is to employ balancing algorithms in preprocessing to\naddress dataset imbalance. However, due to the variety of resampling methods\nand learning models, determining the optimal solution requires testing many\ncombinations. Furthermore, the learning model, dataset, and evaluation metric\naffect the best strategies. This work proposes the Meta-learning for Imbalanced\nRegression (Meta-IR) framework, which diverges from existing literature by\ntraining meta-classifiers to recommend the best pipeline composed of the\nresampling strategy and learning model per task in a zero-shot fashion. The\nmeta-classifiers are trained using a set of meta-features to learn how to map\nthe meta-features to the classes indicating the best pipeline. We propose two\nformulations: Independent and Chained. Independent trains the meta-classifiers\nto separately indicate the best learning algorithm and resampling strategy.\nChained involves a sequential procedure where the output of one meta-classifier\nis used as input for another to model intrinsic relationship factors. The\nChained scenario showed superior performance, suggesting a relationship between\nthe learning algorithm and the resampling strategy per task. Compared with\nAutoML frameworks, Meta-IR obtained better results. Moreover, compared with\nbaselines of six learning algorithms and six resampling algorithms plus no\nresampling, totaling 42 (6 X 7) configurations, Meta-IR outperformed all of\nthem. The code, data, and further information of the experiments can be found\non GitHub: https://github.com/JusciAvelino/Meta-IR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMeta-IR\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u8bad\u7ec3\u5143\u5206\u7c7b\u5668\u63a8\u8350\u6700\u4f73\u7684\u6570\u636e\u9884\u5904\u7406\u548c\u5b66\u4e60\u6a21\u578b\u7ec4\u5408\u3002", "motivation": "\u56de\u5f52\u4efb\u52a1\u4e2d\u76ee\u6807\u503c\u7a00\u5c11\u5bfc\u81f4\u7684\u4e0d\u5e73\u8861\u95ee\u9898\u9700\u8981\u591a\u79cd\u9884\u5904\u7406\u548c\u5b66\u4e60\u6a21\u578b\u7684\u7ec4\u5408\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6d4b\u8bd5\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faMeta-IR\u6846\u67b6\uff0c\u8bad\u7ec3\u5143\u5206\u7c7b\u5668\u57fa\u4e8e\u5143\u7279\u5f81\u63a8\u8350\u6700\u4f73\u7ec4\u5408\uff0c\u5305\u62ec\u72ec\u7acb\u548c\u94fe\u5f0f\u4e24\u79cd\u5f62\u5f0f\u3002", "result": "\u94fe\u5f0f\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0cMeta-IR\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8eAutoML\u6846\u67b6\u548c42\u79cd\u57fa\u7ebf\u914d\u7f6e\u3002", "conclusion": "Meta-IR\u901a\u8fc7\u5143\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u56de\u5f52\u4efb\u52a1\u4e2d\u7684\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u94fe\u5f0f\u65b9\u6cd5\u5c55\u793a\u4e86\u5b66\u4e60\u7b97\u6cd5\u4e0e\u91cd\u91c7\u6837\u7b56\u7565\u4e4b\u95f4\u7684\u5173\u7cfb\u3002"}}
{"id": "2507.11902", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11902", "abs": "https://arxiv.org/abs/2507.11902", "authors": ["Juscimara G. Avelino", "George D. C. Cavalcanti", "Rafael M. O. Cruz"], "title": "Resampling strategies for imbalanced regression: a survey and empirical analysis", "comment": null, "summary": "Imbalanced problems can arise in different real-world situations, and to\naddress this, certain strategies in the form of resampling or balancing\nalgorithms are proposed. This issue has largely been studied in the context of\nclassification, and yet, the same problem features in regression tasks, where\ntarget values are continuous. This work presents an extensive experimental\nstudy comprising various balancing and predictive models, and wich uses metrics\nto capture important elements for the user and to evaluate the predictive model\nin an imbalanced regression data context. It also proposes a taxonomy for\nimbalanced regression approaches based on three crucial criteria: regression\nmodel, learning process, and evaluation metrics. The study offers new insights\ninto the use of such strategies, highlighting the advantages they bring to each\nmodel's learning process, and indicating directions for further studies. The\ncode, data and further information related to the experiments performed herein\ncan be found on GitHub: https://github.com/JusciAvelino/imbalancedRegression.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e0d\u5e73\u8861\u56de\u5f52\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u56de\u5f52\u6a21\u578b\u3001\u5b66\u4e60\u8fc7\u7a0b\u548c\u8bc4\u4f30\u6307\u6807\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e0d\u540c\u5e73\u8861\u7b56\u7565\u7684\u6548\u679c\u3002", "motivation": "\u4e0d\u5e73\u8861\u95ee\u9898\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u540c\u6837\u5b58\u5728\uff0c\u4f46\u76ee\u524d\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u4e0d\u5e73\u8861\u56de\u5f52\u7684\u89e3\u51b3\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u7814\u7a76\u591a\u79cd\u5e73\u8861\u548c\u9884\u6d4b\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u7279\u5b9a\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u56de\u5f52\u6570\u636e\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e0d\u5e73\u8861\u56de\u5f52\u65b9\u6cd5\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u7b56\u7565\u5bf9\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4e0d\u5e73\u8861\u56de\u5f52\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u65b9\u5411\u3002"}}
{"id": "2507.11926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11926", "abs": "https://arxiv.org/abs/2507.11926", "authors": ["Max Hopkins", "Sihan Liu", "Christopher Ye", "Yuichi Yoshida"], "title": "From Generative to Episodic: Sample-Efficient Replicable Reinforcement Learning", "comment": "67 pages", "summary": "The epidemic failure of replicability across empirical science and machine\nlearning has recently motivated the formal study of replicable learning\nalgorithms [Impagliazzo et al. (2022)]. In batch settings where data comes from\na fixed i.i.d. source (e.g., hypothesis testing, supervised learning), the\ndesign of data-efficient replicable algorithms is now more or less understood.\nIn contrast, there remain significant gaps in our knowledge for control\nsettings like reinforcement learning where an agent must interact directly with\na shifting environment. Karbasi et. al show that with access to a generative\nmodel of an environment with $S$ states and $A$ actions (the RL 'batch\nsetting'), replicably learning a near-optimal policy costs only\n$\\tilde{O}(S^2A^2)$ samples. On the other hand, the best upper bound without a\ngenerative model jumps to $\\tilde{O}(S^7 A^7)$ [Eaton et al. (2024)] due to the\nsubstantial difficulty of environment exploration. This gap raises a key\nquestion in the broader theory of replicability: Is replicable exploration\ninherently more expensive than batch learning? Is sample-efficient replicable\nRL even possible?\n  In this work, we (nearly) resolve this problem (for low-horizon tabular\nMDPs): exploration is not a significant barrier to replicable learning! Our\nmain result is a replicable RL algorithm on $\\tilde{O}(S^2A)$ samples, bridging\nthe gap between the generative and episodic settings. We complement this with a\nmatching $\\tilde{\\Omega}(S^2A)$ lower bound in the generative setting (under\nthe common parallel sampling assumption) and an unconditional lower bound in\nthe episodic setting of $\\tilde{\\Omega}(S^2)$ showcasing the near-optimality of\nour algorithm with respect to the state space $S$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u53ef\u590d\u73b0\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7b97\u6cd5\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\uff0c\u586b\u8865\u4e86\u751f\u6210\u6a21\u578b\u4e0e\u975e\u751f\u6210\u6a21\u578b\u4e4b\u95f4\u7684\u6837\u672c\u590d\u6742\u5ea6\u5dee\u8ddd\u3002", "motivation": "\u7531\u4e8e\u5b9e\u8bc1\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u53ef\u590d\u73b0\u6027\u7684\u666e\u904d\u5931\u8d25\uff0c\u7814\u7a76\u53ef\u590d\u73b0\u5b66\u4e60\u7b97\u6cd5\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u63a2\u7d22\u73af\u5883\u7684\u6837\u672c\u6548\u7387\u95ee\u9898\u5c1a\u672a\u89e3\u51b3\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u590d\u73b0\u7684RL\u7b97\u6cd5\uff0c\u5728\u4f4e\u6c34\u5e73\u8868\u683cMDP\u4e2d\u5b9e\u73b0\u4e86\u6837\u672c\u590d\u6742\u5ea6\u4e3a$\\tilde{O}(S^2A)$\uff0c\u5e76\u901a\u8fc7\u5339\u914d\u7684\u4e0b\u754c\u8bc1\u660e\u4e86\u5176\u8fd1\u6700\u4f18\u6027\u3002", "result": "\u7b97\u6cd5\u5728\u751f\u6210\u6a21\u578b\u548c\u975e\u751f\u6210\u6a21\u578b\uff08episodic setting\uff09\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\uff0c\u6837\u672c\u590d\u6742\u5ea6\u5206\u522b\u4e3a$\\tilde{O}(S^2A)$\u548c$\\tilde{\\Omega}(S^2A)$\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u53ef\u590d\u73b0\u63a2\u7d22\u5e76\u975e\u6bd4\u6279\u91cf\u5b66\u4e60\u66f4\u6602\u8d35\uff0c\u4e14\u6837\u672c\u9ad8\u6548\u7684RL\u7b97\u6cd5\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2507.11928", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11928", "abs": "https://arxiv.org/abs/2507.11928", "authors": ["Abhishek Sriram", "Neal Tuffy"], "title": "Accelerating RF Power Amplifier Design via Intelligent Sampling and ML-Based Parameter Tuning", "comment": "This paper is a pre-print version and has been submitted to the IEEE\n  International Conference on Future Machine Learning and Data Science (FMLDS\n  2025)", "summary": "This paper presents a machine learning-accelerated optimization framework for\nRF power amplifier design that reduces simulation requirements by 65% while\nmaintaining $\\pm0.3$ to $\\pm0.4$ dBm accuracy. The proposed method combines\nMaxMin Latin Hypercube Sampling with CatBoost gradient boosting to\nintelligently explore multidimensional parameter spaces. Instead of\nexhaustively simulating all parameter combinations to achieve target P2dB\ncompression specifications, our approach strategically selects approximately\n35% of critical simulation points. The framework processes ADS netlists,\nexecutes harmonic balance simulations on the reduced dataset, and trains a\nCatBoost model to predict P2dB performance across the entire design space.\nValidation across 15 PA operating modes yields an average $R^2$ of 0.901, with\nthe system ranking parameter combinations by their likelihood of meeting target\nspecifications. The integrated solution delivers 58.24% to 77.78% reduction in\nsimulation time through automated GUI-based workflows, enabling rapid design\niterations without compromising accuracy standards required for production RF\ncircuits.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u5b66\u4e60\u52a0\u901f\u7684\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8eRF\u529f\u7387\u653e\u5927\u5668\u8bbe\u8ba1\uff0c\u51cf\u5c1165%\u7684\u4eff\u771f\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u00b10.3\u81f3\u00b10.4 dBm\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u7a77\u4e3e\u6240\u6709\u53c2\u6570\u7ec4\u5408\u4ee5\u5b9e\u73b0\u76ee\u6807P2dB\u538b\u7f29\u89c4\u683c\uff0c\u8017\u65f6\u4e14\u4f4e\u6548\u3002", "method": "\u7ed3\u5408MaxMin\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837\u4e0eCatBoost\u68af\u5ea6\u63d0\u5347\uff0c\u667a\u80fd\u63a2\u7d22\u591a\u7ef4\u53c2\u6570\u7a7a\u95f4\uff0c\u4ec5\u9009\u62e935%\u7684\u5173\u952e\u4eff\u771f\u70b9\u3002", "result": "\u9a8c\u8bc1\u663e\u793a\u5e73\u5747R\u00b2\u4e3a0.901\uff0c\u4eff\u771f\u65f6\u95f4\u51cf\u5c1158.24%\u81f377.78%\uff0c\u4e14\u4e0d\u727a\u7272\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5316GUI\u5de5\u4f5c\u6d41\u5b9e\u73b0\u5feb\u901f\u8bbe\u8ba1\u8fed\u4ee3\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u7ea7RF\u7535\u8def\u3002"}}
{"id": "2507.11948", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11948", "abs": "https://arxiv.org/abs/2507.11948", "authors": ["Carlo Baronio", "Pietro Marsella", "Ben Pan", "Simon Guo", "Silas Alberti"], "title": "Kevin: Multi-Turn RL for Generating CUDA Kernels", "comment": null, "summary": "Writing GPU kernels is a challenging task and critical for AI systems'\nefficiency. It is also highly iterative: domain experts write code and improve\nperformance through execution feedback. Moreover, it presents verifiable\nrewards like correctness and speedup, making it a natural environment to apply\nReinforcement Learning (RL). To explicitly incorporate the iterative nature of\nthis process into training, we develop a flexible multi-turn RL recipe that\naddresses unique challenges encountered in real-world settings, such as\nlearning from long trajectories and effective reward attribution across turns.\nWe present Kevin - K(ernel D)evin, the first model trained with multi-turn RL\nfor CUDA kernel generation and optimization. In our evaluation setup, Kevin\nshows significant gains over its base model (QwQ-32B), improving correctness of\ngenerated kernels (in pure CUDA) from 56% to 82% and mean speedup from 0.53x to\n1.10x of baseline (PyTorch Eager), and surpassing frontier models like o4-mini\n(0.78x). Finally, we study its behavior across test-time scaling axes: we found\nscaling serial refinement more beneficial than parallel sampling. In\nparticular, when given more refinement turns, Kevin shows a higher rate of\nimprovement.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5Kevin\uff0c\u7528\u4e8e\u751f\u6210\u548c\u4f18\u5316CUDA\u5185\u6838\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6b63\u786e\u6027\u548c\u6027\u80fd\u3002", "motivation": "GPU\u5185\u6838\u7f16\u5199\u5bf9AI\u7cfb\u7edf\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5177\u6709\u6311\u6218\u6027\u4e14\u9700\u8981\u8fed\u4ee3\u4f18\u5316\u3002RL\u56e0\u5176\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\uff08\u5982\u6b63\u786e\u6027\u548c\u52a0\u901f\uff09\u6210\u4e3a\u81ea\u7136\u9009\u62e9\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u591a\u8f6eRL\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u957f\u8f68\u8ff9\u5b66\u4e60\u548c\u8de8\u8f6e\u5956\u52b1\u5206\u914d\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "result": "Kevin\u5728\u751f\u6210\u5185\u6838\u7684\u6b63\u786e\u6027\uff0856%\u523082%\uff09\u548c\u5e73\u5747\u52a0\u901f\uff080.53x\u52301.10x\uff09\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u524d\u6cbf\u6a21\u578b\u3002", "conclusion": "\u591a\u8f6eRL\u5728CUDA\u5185\u6838\u4f18\u5316\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e32\u884c\u7ec6\u5316\u6bd4\u5e76\u884c\u91c7\u6837\u66f4\u6709\u6548\uff0c\u4e14\u968f\u7740\u7ec6\u5316\u8f6e\u6b21\u589e\u52a0\uff0c\u6027\u80fd\u63d0\u5347\u66f4\u5feb\u3002"}}
{"id": "2507.11975", "categories": ["cs.LG", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11975", "abs": "https://arxiv.org/abs/2507.11975", "authors": ["Valentin Frank Ingmar Guenter", "Athanasios Sideris"], "title": "Online Training and Pruning of Deep Reinforcement Learning Networks", "comment": "25 pages, 5 figures, 4 tables", "summary": "Scaling deep neural networks (NN) of reinforcement learning (RL) algorithms\nhas been shown to enhance performance when feature extraction networks are used\nbut the gained performance comes at the significant expense of increased\ncomputational and memory complexity. Neural network pruning methods have\nsuccessfully addressed this challenge in supervised learning. However, their\napplication to RL is underexplored. We propose an approach to integrate\nsimultaneous training and pruning within advanced RL methods, in particular to\nRL algorithms enhanced by the Online Feature Extractor Network (OFENet). Our\nnetworks (XiNet) are trained to solve stochastic optimization problems over the\nRL networks' weights and the parameters of variational Bernoulli distributions\nfor 0/1 Random Variables $\\xi$ scaling each unit in the networks. The\nstochastic problem formulation induces regularization terms that promote\nconvergence of the variational parameters to 0 when a unit contributes little\nto the performance. In this case, the corresponding structure is rendered\npermanently inactive and pruned from its network. We propose a cost-aware,\nsparsity-promoting regularization scheme, tailored to the DenseNet architecture\nof OFENets expressing the parameter complexity of involved networks in terms of\nthe parameters of the RVs in these networks. Then, when matching this cost with\nthe regularization terms, the many hyperparameters associated with them are\nautomatically selected, effectively combining the RL objectives and network\ncompression. We evaluate our method on continuous control benchmarks (MuJoCo)\nand the Soft Actor-Critic RL agent, demonstrating that OFENets can be pruned\nconsiderably with minimal loss in performance. Furthermore, our results confirm\nthat pruning large networks during training produces more efficient and higher\nperforming RL agents rather than training smaller networks from scratch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bad\u7ec3\u548c\u526a\u679d\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\uff0c\u4ee5\u51cf\u5c11\u8ba1\u7b97\u548c\u5185\u5b58\u5f00\u9500\u3002", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5185\u5b58\u590d\u6742\u5ea6\u9ad8\u3002\u73b0\u6709\u526a\u679d\u65b9\u6cd5\u5728\u76d1\u7763\u5b66\u4e60\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u5e94\u7528\u4e0d\u8db3\u3002", "method": "\u63d0\u51faXiNet\uff0c\u901a\u8fc7\u53d8\u5206\u4f2f\u52aa\u5229\u5206\u5e03\u548c\u968f\u673a\u53d8\u91cf\u03be\u5bf9\u7f51\u7edc\u5355\u5143\u8fdb\u884c\u526a\u679d\uff0c\u7ed3\u5408\u6210\u672c\u611f\u77e5\u7684\u6b63\u5219\u5316\u65b9\u6848\u3002", "result": "\u5728MuJoCo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cOFENet\u526a\u679d\u540e\u6027\u80fd\u635f\u5931\u6781\u5c0f\uff0c\u4e14\u8bad\u7ec3\u65f6\u526a\u679d\u6bd4\u4ece\u5934\u8bad\u7ec3\u5c0f\u7f51\u7edc\u66f4\u9ad8\u6548\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u548c\u7f51\u7edc\u538b\u7f29\uff0c\u63d0\u5347\u4e86\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.11997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11997", "abs": "https://arxiv.org/abs/2507.11997", "authors": ["Tairan Huang", "Yili Wang"], "title": "Can LLMs Find Fraudsters? Multi-level LLM Enhanced Graph Fraud Detection", "comment": null, "summary": "Graph fraud detection has garnered significant attention as Graph Neural\nNetworks (GNNs) have proven effective in modeling complex relationships within\nmultimodal data. However, existing graph fraud detection methods typically use\npreprocessed node embeddings and predefined graph structures to reveal\nfraudsters, which ignore the rich semantic cues contained in raw textual\ninformation. Although Large Language Models (LLMs) exhibit powerful\ncapabilities in processing textual information, it remains a significant\nchallenge to perform multimodal fusion of processed textual embeddings with\ngraph structures. In this paper, we propose a \\textbf{M}ulti-level \\textbf{L}LM\n\\textbf{E}nhanced Graph Fraud \\textbf{D}etection framework called MLED. In\nMLED, we utilize LLMs to extract external knowledge from textual information to\nenhance graph fraud detection methods. To integrate LLMs with graph structure\ninformation and enhance the ability to distinguish fraudsters, we design a\nmulti-level LLM enhanced framework including type-level enhancer and\nrelation-level enhancer. One is to enhance the difference between the\nfraudsters and the benign entities, the other is to enhance the importance of\nthe fraudsters in different relations. The experiments on four real-world\ndatasets show that MLED achieves state-of-the-art performance in graph fraud\ndetection as a generalized framework that can be applied to existing methods.", "AI": {"tldr": "MLED\u6846\u67b6\u901a\u8fc7\u591a\u7ea7LLM\u589e\u5f3a\uff0c\u7ed3\u5408\u6587\u672c\u4fe1\u606f\u548c\u56fe\u7ed3\u6784\uff0c\u63d0\u5347\u56fe\u6b3a\u8bc8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u7565\u539f\u59cb\u6587\u672c\u4fe1\u606f\u4e2d\u7684\u4e30\u5bcc\u8bed\u4e49\u7ebf\u7d22\uff0c\u4e14\u96be\u4ee5\u5c06LLM\u5904\u7406\u7684\u6587\u672c\u5d4c\u5165\u4e0e\u56fe\u7ed3\u6784\u8fdb\u884c\u591a\u6a21\u6001\u878d\u5408\u3002", "method": "\u63d0\u51faMLED\u6846\u67b6\uff0c\u5229\u7528LLM\u63d0\u53d6\u5916\u90e8\u77e5\u8bc6\uff0c\u8bbe\u8ba1\u7c7b\u578b\u7ea7\u548c\u5173\u7cfb\u7ea7\u589e\u5f3a\u5668\uff0c\u533a\u5206\u6b3a\u8bc8\u8005\u4e0e\u826f\u6027\u5b9e\u4f53\uff0c\u5e76\u7a81\u51fa\u6b3a\u8bc8\u8005\u5728\u4e0d\u540c\u5173\u7cfb\u4e2d\u7684\u91cd\u8981\u6027\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cMLED\u4f5c\u4e3a\u901a\u7528\u6846\u67b6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MLED\u901a\u8fc7\u591a\u7ea7LLM\u589e\u5f3a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u56fe\u6b3a\u8bc8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.12002", "categories": ["cs.LG", "I.2.0; J.4"], "pdf": "https://arxiv.org/pdf/2507.12002", "abs": "https://arxiv.org/abs/2507.12002", "authors": ["Alice Zhang", "Callihan Bertley", "Dawei Liang", "Edison Thomaz"], "title": "Detecting In-Person Conversations in Noisy Real-World Environments with Smartwatch Audio and Motion Sensing", "comment": null, "summary": "Social interactions play a crucial role in shaping human behavior,\nrelationships, and societies. It encompasses various forms of communication,\nsuch as verbal conversation, non-verbal gestures, facial expressions, and body\nlanguage. In this work, we develop a novel computational approach to detect a\nfoundational aspect of human social interactions, in-person verbal\nconversations, by leveraging audio and inertial data captured with a commodity\nsmartwatch in acoustically-challenging scenarios. To evaluate our approach, we\nconducted a lab study with 11 participants and a semi-naturalistic study with\n24 participants. We analyzed machine learning and deep learning models with 3\ndifferent fusion methods, showing the advantages of fusing audio and inertial\ndata to consider not only verbal cues but also non-verbal gestures in\nconversations. Furthermore, we perform a comprehensive set of evaluations\nacross activities and sampling rates to demonstrate the benefits of multimodal\nsensing in specific contexts. Overall, our framework achieved 82.0$\\pm$3.0%\nmacro F1-score when detecting conversations in the lab and 77.2$\\pm$1.8% in the\nsemi-naturalistic setting.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u667a\u80fd\u624b\u8868\u7684\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u9762\u5bf9\u9762\u5bf9\u8bdd\uff0c\u7ed3\u5408\u97f3\u9891\u548c\u60ef\u6027\u6570\u636e\uff0c\u5728\u5b9e\u9a8c\u5ba4\u548c\u534a\u81ea\u7136\u573a\u666f\u4e2d\u5206\u522b\u8fbe\u523082.0%\u548c77.2%\u7684F1\u5206\u6570\u3002", "motivation": "\u793e\u4ea4\u4e92\u52a8\u5bf9\u4eba\u7c7b\u884c\u4e3a\u548c\u793e\u4f1a\u5173\u7cfb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u58f0\u5b66\u73af\u5883\u4e0b\u68c0\u6d4b\u9762\u5bf9\u9762\u5bf9\u8bdd\u7684\u80fd\u529b\u6709\u9650\u3002", "method": "\u5229\u7528\u667a\u80fd\u624b\u8868\u91c7\u96c6\u97f3\u9891\u548c\u60ef\u6027\u6570\u636e\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u591a\u6a21\u6001\u878d\u5408\u65b9\u6cd5\uff0c\u5206\u6790\u5bf9\u8bdd\u4e2d\u7684\u8a00\u8bed\u548c\u975e\u8a00\u8bed\u7ebf\u7d22\u3002", "result": "\u5728\u5b9e\u9a8c\u5ba4\u548c\u534a\u81ea\u7136\u573a\u666f\u4e2d\uff0c\u6a21\u578b\u5206\u522b\u8fbe\u523082.0\u00b13.0%\u548c77.2\u00b11.8%\u7684\u5b8fF1\u5206\u6570\u3002", "conclusion": "\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u8bdd\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.12011", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12011", "abs": "https://arxiv.org/abs/2507.12011", "authors": ["Yao Lu", "Hongyu Gao", "Zhuangzhi Chen", "Dongwei Xu", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "DUSE: A Data Expansion Framework for Low-resource Automatic Modulation Recognition based on Active Learning", "comment": null, "summary": "Although deep neural networks have made remarkable achievements in the field\nof automatic modulation recognition (AMR), these models often require a large\namount of labeled data for training. However, in many practical scenarios, the\navailable target domain data is scarce and difficult to meet the needs of model\ntraining. The most direct way is to collect data manually and perform expert\nannotation, but the high time and labor costs are unbearable. Another common\nmethod is data augmentation. Although it can enrich training samples to a\ncertain extent, it does not introduce new data and therefore cannot\nfundamentally solve the problem of data scarcity. To address these challenges,\nwe introduce a data expansion framework called Dynamic Uncertainty-driven\nSample Expansion (DUSE). Specifically, DUSE uses an uncertainty scoring\nfunction to filter out useful samples from relevant AMR datasets and employs an\nactive learning strategy to continuously refine the scorer. Extensive\nexperiments demonstrate that DUSE consistently outperforms 8 coreset selection\nbaselines in both class-balance and class-imbalance settings. Besides, DUSE\nexhibits strong cross-architecture generalization for unseen models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDUSE\u7684\u6570\u636e\u6269\u5c55\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u51fd\u6570\u548c\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u89e3\u51b3\u81ea\u52a8\u8c03\u5236\u8bc6\u522b\u4e2d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u81ea\u52a8\u8c03\u5236\u8bc6\u522b\uff08AMR\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002\u5b9e\u9645\u573a\u666f\u4e2d\u76ee\u6807\u57df\u6570\u636e\u7a00\u7f3a\uff0c\u624b\u52a8\u6536\u96c6\u6216\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u65e0\u6cd5\u6839\u672c\u89e3\u51b3\u95ee\u9898\u3002", "method": "\u63d0\u51faDUSE\u6846\u67b6\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u8bc4\u5206\u51fd\u6570\u7b5b\u9009\u6709\u7528\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u6301\u7eed\u4f18\u5316\u8bc4\u5206\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDUSE\u5728\u7c7b\u522b\u5e73\u8861\u548c\u4e0d\u5e73\u8861\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e8\u79cd\u6838\u5fc3\u96c6\u9009\u62e9\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e14\u5bf9\u672a\u89c1\u6a21\u578b\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DUSE\u80fd\u6709\u6548\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5177\u5907\u8de8\u67b6\u6784\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.12041", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12041", "abs": "https://arxiv.org/abs/2507.12041", "authors": ["Anmol Kagrecha", "Henrik Marklund", "Potsawee Manakul", "Richard Zeckhauser", "Benjamin Van Roy"], "title": "Granular feedback merits sophisticated aggregation", "comment": "31 pages, 8 figures", "summary": "Human feedback is increasingly used across diverse applications like training\nAI models, developing recommender systems, and measuring public opinion -- with\ngranular feedback often being preferred over binary feedback for its greater\ninformativeness. While it is easy to accurately estimate a population's\ndistribution of feedback given feedback from a large number of individuals,\ncost constraints typically necessitate using smaller groups. A simple method to\napproximate the population distribution is regularized averaging: compute the\nempirical distribution and regularize it toward a prior. Can we do better? As\nwe will discuss, the answer to this question depends on feedback granularity.\n  Suppose one wants to predict a population's distribution of feedback using\nfeedback from a limited number of individuals. We show that, as feedback\ngranularity increases, one can substantially improve upon predictions of\nregularized averaging by combining individuals' feedback in ways more\nsophisticated than regularized averaging.\n  Our empirical analysis using questions on social attitudes confirms this\npattern. In particular, with binary feedback, sophistication barely reduces the\nnumber of individuals required to attain a fixed level of performance. By\ncontrast, with five-point feedback, sophisticated methods match the performance\nof regularized averaging with about half as many individuals.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u6709\u9650\u4e2a\u4f53\u53cd\u9988\u4e0b\u9884\u6d4b\u7fa4\u4f53\u53cd\u9988\u5206\u5e03\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u53cd\u9988\u7c92\u5ea6\u8d8a\u9ad8\uff0c\u590d\u6742\u65b9\u6cd5\u6bd4\u6b63\u5219\u5316\u5e73\u5747\u66f4\u6709\u6548\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u6210\u672c\u9650\u5236\u4e0b\uff0c\u5229\u7528\u5c11\u91cf\u4e2a\u4f53\u53cd\u9988\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7fa4\u4f53\u53cd\u9988\u5206\u5e03\u3002", "method": "\u6bd4\u8f83\u6b63\u5219\u5316\u5e73\u5747\u4e0e\u66f4\u590d\u6742\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u53cd\u9988\u7c92\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u4e94\u7ea7\u53cd\u9988\u4e0b\uff0c\u590d\u6742\u65b9\u6cd5\u6240\u9700\u4e2a\u4f53\u6570\u91cf\u51cf\u534a\uff1b\u4e8c\u5143\u53cd\u9988\u4e0b\u6548\u679c\u4e0d\u660e\u663e\u3002", "conclusion": "\u53cd\u9988\u7c92\u5ea6\u5f71\u54cd\u65b9\u6cd5\u9009\u62e9\uff0c\u9ad8\u7c92\u5ea6\u65f6\u590d\u6742\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6b63\u5219\u5316\u5e73\u5747\u3002"}}
{"id": "2507.12043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12043", "abs": "https://arxiv.org/abs/2507.12043", "authors": ["Wen Wen", "Tieliang Gong", "Yunjiao Zhang", "Zeyu Gao", "Weizhan Zhang", "Yong-Jin Liu"], "title": "Information-Theoretic Generalization Bounds of Replay-based Continual Learning", "comment": null, "summary": "Continual learning (CL) has emerged as a dominant paradigm for acquiring\nknowledge from sequential tasks while avoiding catastrophic forgetting.\nAlthough many CL methods have been proposed to show impressive empirical\nperformance, the theoretical understanding of their generalization behavior\nremains limited, particularly for replay-based approaches. In this paper, we\nestablish a unified theoretical framework for replay-based CL, deriving a\nseries of information-theoretic bounds that explicitly characterize how the\nmemory buffer interacts with the current task to affect generalization.\nSpecifically, our hypothesis-based bounds reveal that utilizing the limited\nexemplars of previous tasks alongside the current task data, rather than\nexhaustive replay, facilitates improved generalization while effectively\nmitigating catastrophic forgetting. Furthermore, our prediction-based bounds\nyield tighter and computationally tractable upper bounds of the generalization\ngap through the use of low-dimensional variables. Our analysis is general and\nbroadly applicable to a wide range of learning algorithms, exemplified by\nstochastic gradient Langevin dynamics (SGLD) as a representative method.\nComprehensive experimental evaluations demonstrate the effectiveness of our\nderived bounds in capturing the generalization dynamics in replay-based CL\nsettings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u56de\u653e\u5f0f\u6301\u7eed\u5b66\u4e60\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u8bba\u754c\u9650\u63ed\u793a\u4e86\u5185\u5b58\u7f13\u51b2\u533a\u5982\u4f55\u4e0e\u5f53\u524d\u4efb\u52a1\u4ea4\u4e92\u4ee5\u5f71\u54cd\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u6301\u7eed\u5b66\u4e60\u4e2d\u56de\u653e\u5f0f\u65b9\u6cd5\u7684\u6cdb\u5316\u884c\u4e3a\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5efa\u7acb\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u63a8\u5bfc\u57fa\u4e8e\u5047\u8bbe\u548c\u9884\u6d4b\u7684\u6cdb\u5316\u754c\u9650\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5b66\u4e60\u7b97\u6cd5\uff08\u5982SGLD\uff09\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u6709\u9650\u56de\u653e\u6837\u672c\u4e0e\u5f53\u524d\u4efb\u52a1\u6570\u636e\u7ed3\u5408\u53ef\u6539\u5584\u6cdb\u5316\u5e76\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u754c\u9650\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u7684\u7406\u8bba\u6846\u67b6\u4e3a\u56de\u653e\u5f0f\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u6cdb\u5316\u884c\u4e3a\u7684\u6df1\u5165\u7406\u89e3\uff0c\u5e76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.12053", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12053", "abs": "https://arxiv.org/abs/2507.12053", "authors": ["Seanglidet Yean", "Jiazu Zhou", "Bu-Sung Lee", "Markus Schl\u00e4pfer"], "title": "FloGAN: Scenario-Based Urban Mobility Flow Generation via Conditional GANs and Dynamic Region Decoupling", "comment": "International Conference on Intelligent Digitization of Systems and\n  Services, Valencia, Spain, 2025 (IDSS 2025)", "summary": "The mobility patterns of people in cities evolve alongside changes in land\nuse and population. This makes it crucial for urban planners to simulate and\nanalyze human mobility patterns for purposes such as transportation\noptimization and sustainable urban development. Existing generative models\nborrowed from machine learning rely heavily on historical trajectories and\noften overlook evolving factors like changes in population density and land\nuse. Mechanistic approaches incorporate population density and facility\ndistribution but assume static scenarios, limiting their utility for future\nprojections where historical data for calibration is unavailable. This study\nintroduces a novel, data-driven approach for generating origin-destination\nmobility flows tailored to simulated urban scenarios. Our method leverages\nadaptive factors such as dynamic region sizes and land use archetypes, and it\nutilizes conditional generative adversarial networks (cGANs) to blend\nhistorical data with these adaptive parameters. The approach facilitates rapid\nmobility flow generation with adjustable spatial granularity based on regions\nof interest, without requiring extensive calibration data or complex behavior\nmodeling. The promising performance of our approach is demonstrated by its\napplication to mobile phone data from Singapore, and by its comparison with\nexisting methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u52a8\u6001\u533a\u57df\u5927\u5c0f\u548c\u571f\u5730\u5229\u7528\u7c7b\u578b\uff0c\u5229\u7528\u6761\u4ef6\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08cGANs\uff09\u751f\u6210\u57ce\u5e02\u6a21\u62df\u573a\u666f\u4e2d\u7684\u51fa\u884c\u6d41\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u4f9d\u8d56\u5386\u53f2\u8f68\u8ff9\uff0c\u5ffd\u7565\u52a8\u6001\u56e0\u7d20\uff08\u5982\u4eba\u53e3\u5bc6\u5ea6\u548c\u571f\u5730\u5229\u7528\u53d8\u5316\uff09\uff0c\u800c\u673a\u68b0\u65b9\u6cd5\u5047\u8bbe\u9759\u6001\u573a\u666f\uff0c\u65e0\u6cd5\u9002\u5e94\u672a\u6765\u9884\u6d4b\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u533a\u57df\u5927\u5c0f\u548c\u571f\u5730\u5229\u7528\u7c7b\u578b\uff0c\u5229\u7528cGANs\u878d\u5408\u5386\u53f2\u6570\u636e\u4e0e\u52a8\u6001\u53c2\u6570\uff0c\u5feb\u901f\u751f\u6210\u53ef\u8c03\u7a7a\u95f4\u7c92\u5ea6\u7684\u51fa\u884c\u6d41\u3002", "result": "\u5728\u65b0\u52a0\u5761\u624b\u673a\u6570\u636e\u4e0a\u7684\u5e94\u7528\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e0\u9700\u5927\u91cf\u6821\u51c6\u6570\u636e\u6216\u590d\u6742\u884c\u4e3a\u5efa\u6a21\uff0c\u9002\u7528\u4e8e\u672a\u6765\u57ce\u5e02\u89c4\u5212\u548c\u4ea4\u901a\u4f18\u5316\u3002"}}
{"id": "2507.12070", "categories": ["cs.LG", "I.5.1; F.1.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.12070", "abs": "https://arxiv.org/abs/2507.12070", "authors": ["George Bird"], "title": "Emergence of Quantised Representations Isolated to Anisotropic Functions", "comment": "36 pages, 31 figures", "summary": "This paper describes a novel methodology for determining representational\nalignment, developed upon the existing Spotlight Resonance method. Using this,\nit is found that algebraic symmetries of network primitives are a strong\npredictor for task-agnostic structure in representations. Particularly, this\nnew tool is used to gain insight into how discrete representations can form and\narrange in autoencoder models, through an ablation study where only the\nactivation function is altered. Representations are found to tend to discretise\nwhen the activation functions are defined through a discrete algebraic\npermutation-equivariant symmetry. In contrast, they remain continuous under a\ncontinuous algebraic orthogonal-equivariant definition. These findings\ncorroborate the hypothesis that functional form choices can carry unintended\ninductive biases which produce task-independent artefactual structures in\nrepresentations, particularly that contemporary forms induce discretisation of\notherwise continuous structure -- a quantisation effect. Moreover, this\nsupports a general causal model for one mode in which discrete representations\nmay form, and could constitute a prerequisite for downstream interpretability\nphenomena, including grandmother neurons, discrete coding schemes, general\nlinear features and possibly Superposition. Hence, this tool and proposed\nmechanism for the influence of functional form on representations may provide\nseveral insights into emergent interpretability research. Finally, preliminary\nresults indicate that quantisation of representations appears to correlate with\na measurable increase in reconstruction error, reinforcing previous conjectures\nthat this collapse can be detrimental.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSpotlight Resonance\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u786e\u5b9a\u8868\u5f81\u5bf9\u9f50\uff0c\u53d1\u73b0\u7f51\u7edc\u539f\u8bed\u7684\u4ee3\u6570\u5bf9\u79f0\u6027\u662f\u4efb\u52a1\u65e0\u5173\u8868\u5f81\u7ed3\u6784\u7684\u5f3a\u9884\u6d4b\u56e0\u5b50\u3002\u901a\u8fc7\u6539\u53d8\u6fc0\u6d3b\u51fd\u6570\u7684\u7814\u7a76\uff0c\u63ed\u793a\u4e86\u79bb\u6563\u8868\u5f81\u7684\u5f62\u6210\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u7406\u89e3\u529f\u80fd\u5f62\u5f0f\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u8868\u5f81\u7ed3\u6784\uff0c\u7279\u522b\u662f\u6fc0\u6d3b\u51fd\u6570\u5982\u4f55\u5bfc\u81f4\u79bb\u6563\u5316\u6216\u8fde\u7eed\u6027\u8868\u5f81\u7684\u5f62\u6210\u3002", "method": "\u91c7\u7528\u57fa\u4e8eSpotlight Resonance\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u53d8\u6fc0\u6d3b\u51fd\u6570\u8fdb\u884c\u6d88\u878d\u7814\u7a76\uff0c\u5206\u6790\u8868\u5f81\u7684\u79bb\u6563\u5316\u4e0e\u8fde\u7eed\u6027\u3002", "result": "\u53d1\u73b0\u79bb\u6563\u4ee3\u6570\u7f6e\u6362\u7b49\u53d8\u5bf9\u79f0\u6027\u6fc0\u6d3b\u51fd\u6570\u5bfc\u81f4\u8868\u5f81\u79bb\u6563\u5316\uff0c\u800c\u8fde\u7eed\u4ee3\u6570\u6b63\u4ea4\u7b49\u53d8\u5bf9\u79f0\u6027\u5219\u4fdd\u6301\u8fde\u7eed\u6027\u3002\u529f\u80fd\u5f62\u5f0f\u7684\u9009\u62e9\u53ef\u80fd\u5f15\u5165\u610f\u5916\u7684\u5f52\u7eb3\u504f\u5dee\u3002", "conclusion": "\u529f\u80fd\u5f62\u5f0f\u5bf9\u8868\u5f81\u7ed3\u6784\u6709\u663e\u8457\u5f71\u54cd\uff0c\u79bb\u6563\u5316\u53ef\u80fd\u5bfc\u81f4\u91cd\u5efa\u8bef\u5dee\u589e\u52a0\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u8868\u5f81\u5f62\u6210\u673a\u5236\u548c\u4e0b\u6e38\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.12094", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12094", "abs": "https://arxiv.org/abs/2507.12094", "authors": ["Yiding Feng", "Wei Tang"], "title": "Measuring Informativeness Gap of (Mis)Calibrated Predictors", "comment": null, "summary": "In many applications, decision-makers must choose between multiple predictive\nmodels that may all be miscalibrated. Which model (i.e., predictor) is more\n\"useful\" in downstream decision tasks? To answer this, our first contribution\nintroduces the notion of the informativeness gap between any two predictors,\ndefined as the maximum normalized payoff advantage one predictor offers over\nthe other across all decision-making tasks. Our framework strictly generalizes\nseveral existing notions: it subsumes U-Calibration [KLST-23] and Calibration\nDecision Loss [HW-24], which compare a miscalibrated predictor to its\ncalibrated counterpart, and it recovers Blackwell informativeness [Bla-51,\nBla-53] as a special case when both predictors are perfectly calibrated. Our\nsecond contribution is a dual characterization of the informativeness gap,\nwhich gives rise to a natural informativeness measure that can be viewed as a\nrelaxed variant of the earth mover's distance (EMD) between two prediction\ndistributions. We show that this measure satisfies natural desiderata: it is\ncomplete and sound, and it can be estimated sample-efficiently in the\nprediction-only access setting. Along the way, we also obtain novel\ncombinatorial structural results when applying this measure to perfectly\ncalibrated predictors.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u79f0\u4e3a\u2018\u4fe1\u606f\u5dee\u8ddd\u2019\u7684\u65b0\u6982\u5ff5\uff0c\u7528\u4e8e\u6bd4\u8f83\u591a\u4e2a\u9884\u6d4b\u6a21\u578b\u5728\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u2018\u6709\u7528\u6027\u2019\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53cc\u91cd\u8868\u5f81\u548c\u81ea\u7136\u7684\u4fe1\u606f\u5ea6\u91cf\u65b9\u6cd5\u3002", "motivation": "\u5728\u591a\u4e2a\u9884\u6d4b\u6a21\u578b\u53ef\u80fd\u90fd\u5b58\u5728\u6821\u51c6\u95ee\u9898\u7684\u60c5\u51b5\u4e0b\uff0c\u51b3\u7b56\u8005\u9700\u8981\u9009\u62e9\u6700\u2018\u6709\u7528\u2019\u7684\u6a21\u578b\u3002", "method": "\u5f15\u5165\u4fe1\u606f\u5dee\u8ddd\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e86\u4e24\u4e2a\u9884\u6d4b\u5668\u5728\u6240\u6709\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u6700\u5927\u5f52\u4e00\u5316\u6536\u76ca\u4f18\u52bf\uff0c\u5e76\u63d0\u4f9b\u4e86\u53cc\u91cd\u8868\u5f81\u548c\u4e00\u79cd\u7c7b\u4f3cEMD\u7684\u5ea6\u91cf\u65b9\u6cd5\u3002", "result": "\u8be5\u6846\u67b6\u6cdb\u5316\u4e86\u73b0\u6709\u6982\u5ff5\uff08\u5982U-Calibration\u548cBlackwell\u4fe1\u606f\u6027\uff09\uff0c\u5e76\u8bc1\u660e\u4e86\u65b0\u5ea6\u91cf\u7684\u5b8c\u5907\u6027\u3001\u5408\u7406\u6027\u548c\u6837\u672c\u9ad8\u6548\u6027\u3002", "conclusion": "\u4fe1\u606f\u5dee\u8ddd\u4e3a\u6bd4\u8f83\u9884\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u6821\u51c6\u548c\u975e\u6821\u51c6\u573a\u666f\uff0c\u5e76\u5177\u6709\u5b9e\u9645\u53ef\u64cd\u4f5c\u6027\u3002"}}
{"id": "2507.12127", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12127", "abs": "https://arxiv.org/abs/2507.12127", "authors": ["Ngoc Duy Pham", "Thusitha Dayaratne", "Viet Vo", "Shangqi Lai", "Sharif Abuadbba", "Hajime Suzuki", "Xingliang Yuan", "Carsten Rudolph"], "title": "Self-Adaptive and Robust Federated Spectrum Sensing without Benign Majority for Cellular Networks", "comment": null, "summary": "Advancements in wireless and mobile technologies, including 5G advanced and\nthe envisioned 6G, are driving exponential growth in wireless devices. However,\nthis rapid expansion exacerbates spectrum scarcity, posing a critical\nchallenge. Dynamic spectrum allocation (DSA)--which relies on sensing and\ndynamically sharing spectrum--has emerged as an essential solution to address\nthis issue. While machine learning (ML) models hold significant potential for\nimproving spectrum sensing, their adoption in centralized ML-based DSA systems\nis limited by privacy concerns, bandwidth constraints, and regulatory\nchallenges. To overcome these limitations, distributed ML-based approaches such\nas Federated Learning (FL) offer promising alternatives. This work addresses\ntwo key challenges in FL-based spectrum sensing (FLSS). First, the scarcity of\nlabeled data for training FL models in practical spectrum sensing scenarios is\ntackled with a semi-supervised FL approach, combined with energy detection,\nenabling model training on unlabeled datasets. Second, we examine the security\nvulnerabilities of FLSS, focusing on the impact of data poisoning attacks. Our\nanalysis highlights the shortcomings of existing majority-based defenses in\ncountering such attacks. To address these vulnerabilities, we propose a novel\ndefense mechanism inspired by vaccination, which effectively mitigates data\npoisoning attacks without relying on majority-based assumptions. Extensive\nexperiments on both synthetic and real-world datasets validate our solutions,\ndemonstrating that FLSS can achieve near-perfect accuracy on unlabeled datasets\nand maintain Byzantine robustness against both targeted and untargeted data\npoisoning attacks, even when a significant proportion of participants are\nmalicious.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u7684\u52a8\u6001\u9891\u8c31\u5206\u914d\uff08DSA\uff09\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u9891\u8c31\u611f\u77e5\u4e2d\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u548c\u5b89\u5168\u6f0f\u6d1e\u95ee\u9898\u3002", "motivation": "\u65e0\u7ebf\u8bbe\u5907\u7684\u5feb\u901f\u589e\u957f\u5bfc\u81f4\u9891\u8c31\u7a00\u7f3a\uff0c\u4f20\u7edf\u96c6\u4e2d\u5f0f\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u56e0\u9690\u79c1\u548c\u5e26\u5bbd\u9650\u5236\u96be\u4ee5\u5e94\u7528\uff0c\u5206\u5e03\u5f0f\u65b9\u6cd5\u5982FL\u6210\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u534a\u76d1\u7763FL\u7ed3\u5408\u80fd\u91cf\u68c0\u6d4b\uff0c\u89e3\u51b3\u6570\u636e\u6807\u6ce8\u4e0d\u8db3\u95ee\u9898\uff1b\u63d0\u51fa\u57fa\u4e8e\u75ab\u82d7\u63a5\u79cd\u542f\u53d1\u7684\u9632\u5fa1\u673a\u5236\uff0c\u5e94\u5bf9\u6570\u636e\u6295\u6bd2\u653b\u51fb\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FLSS\u5728\u672a\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u7684\u9ad8\u51c6\u786e\u6027\u548c\u5bf9\u6076\u610f\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "FLSS\u662f\u4e00\u79cd\u6709\u6548\u7684\u9891\u8c31\u611f\u77e5\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.12142", "categories": ["cs.LG", "cs.CL", "cs.NA", "math.DG", "math.NA", "68T07, 65F55, 53Z50"], "pdf": "https://arxiv.org/pdf/2507.12142", "abs": "https://arxiv.org/abs/2507.12142", "authors": ["Vladimir Bogachev", "Vladimir Aletov", "Alexander Molozhavenko", "Denis Bobkov", "Vera Soboleva", "Aibek Alanov", "Maxim Rakhuba"], "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization", "comment": null, "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRiemannLoRA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06LoRA\u77e9\u9635\u89c6\u4e3a\u5149\u6ed1\u6d41\u5f62\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u521d\u59cb\u5316\u7b56\u7565\u548c\u4f4e\u79e9\u77e9\u9635\u5206\u89e3\u4e2d\u7684\u8fc7\u53c2\u6570\u5316\u95ee\u9898\u3002", "motivation": "LoRA\u5728\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4ecd\u9762\u4e34\u521d\u59cb\u5316\u7b56\u7565\u4f18\u5316\u548c\u8fc7\u53c2\u6570\u5316\u7684\u6311\u6218\u3002", "method": "\u5c06\u56fa\u5b9a\u79e9LoRA\u77e9\u9635\u89c6\u4e3a\u5149\u6ed1\u6d41\u5f62\uff0c\u901a\u8fc7\u6d41\u5f62\u4e0a\u7684\u6700\u5feb\u635f\u5931\u4e0b\u964d\u65b9\u5411\u786e\u5b9a\u521d\u59cb\u5316\uff0c\u5e76\u91c7\u7528\u6570\u503c\u7a33\u5b9a\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u5b9e\u73b0\u3002", "result": "\u5728LLM\u548c\u6269\u6563\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRiemannLoRA\u5728\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u4f18\u4e8e\u6807\u51c6LoRA\u53ca\u5176\u5148\u8fdb\u6539\u8fdb\u3002", "conclusion": "RiemannLoRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86LoRA\u7684\u4e24\u5927\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2507.12144", "categories": ["cs.LG", "physics.ao-ph", "86-10, 68T07", "I.2.1; I.6.5; G.3"], "pdf": "https://arxiv.org/pdf/2507.12144", "abs": "https://arxiv.org/abs/2507.12144", "authors": ["Boris Bonev", "Thorsten Kurth", "Ankur Mahesh", "Mauro Bisson", "Jean Kossaifi", "Karthik Kashinath", "Anima Anandkumar", "William D. Collins", "Michael S. Pritchard", "Alexander Keller"], "title": "FourCastNet 3: A geometric approach to probabilistic machine-learning weather forecasting at scale", "comment": null, "summary": "FourCastNet 3 advances global weather modeling by implementing a scalable,\ngeometric machine learning (ML) approach to probabilistic ensemble forecasting.\nThe approach is designed to respect spherical geometry and to accurately model\nthe spatially correlated probabilistic nature of the problem, resulting in\nstable spectra and realistic dynamics across multiple scales. FourCastNet 3\ndelivers forecasting accuracy that surpasses leading conventional ensemble\nmodels and rivals the best diffusion-based methods, while producing forecasts 8\nto 60 times faster than these approaches. In contrast to other ML approaches,\nFourCastNet 3 demonstrates excellent probabilistic calibration and retains\nrealistic spectra, even at extended lead times of up to 60 days. All of these\nadvances are realized using a purely convolutional neural network architecture\ntailored for spherical geometry. Scalable and efficient large-scale training on\n1024 GPUs and more is enabled by a novel training paradigm for combined model-\nand data-parallelism, inspired by domain decomposition methods in classical\nnumerical models. Additionally, FourCastNet 3 enables rapid inference on a\nsingle GPU, producing a 90-day global forecast at 0.25{\\deg}, 6-hourly\nresolution in under 20 seconds. Its computational efficiency, medium-range\nprobabilistic skill, spectral fidelity, and rollout stability at subseasonal\ntimescales make it a strong candidate for improving meteorological forecasting\nand early warning systems through large ensemble predictions.", "AI": {"tldr": "FourCastNet 3\u901a\u8fc7\u51e0\u4f55\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u5168\u7403\u5929\u6c14\u5efa\u6a21\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u7684\u6982\u7387\u96c6\u5408\u9884\u62a5\u3002", "motivation": "\u6539\u8fdb\u4f20\u7edf\u5929\u6c14\u6a21\u578b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6982\u7387\u6821\u51c6\u548c\u9891\u8c31\u771f\u5b9e\u6027\u3002", "method": "\u91c7\u7528\u7eaf\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7ed3\u5408\u6a21\u578b\u548c\u6570\u636e\u5e76\u884c\u8bad\u7ec3\u8303\u5f0f\uff0c\u9002\u5e94\u7403\u9762\u51e0\u4f55\u3002", "result": "\u9884\u62a5\u901f\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb8\u81f360\u500d\uff0c\u7cbe\u5ea6\u8d85\u8d8a\u4f20\u7edf\u6a21\u578b\uff0c\u5ab2\u7f8e\u6269\u6563\u65b9\u6cd5\u3002", "conclusion": "FourCastNet 3\u5728\u8ba1\u7b97\u6548\u7387\u3001\u6982\u7387\u6280\u80fd\u548c\u9891\u8c31\u4fdd\u771f\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u6c14\u8c61\u9884\u62a5\u548c\u9884\u8b66\u7cfb\u7edf\u3002"}}
{"id": "2507.12145", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12145", "abs": "https://arxiv.org/abs/2507.12145", "authors": ["Muhammad Azlan Qazi", "Alexandros Iosifidis", "Qi Zhang"], "title": "PRISM: Distributed Inference for Foundation Models at Edge", "comment": null, "summary": "Foundation models (FMs) have achieved remarkable success across a wide range\nof applications, from image classification to natural langurage processing, but\npose significant challenges for deployment at edge. This has sparked growing\ninterest in developing practical and efficient strategies for bringing\nfoundation models to edge environments. In this work, we propose PRISM, a\ncommunication-efficient and compute-aware strategy for distributed Transformer\ninference on edge devices. Our method leverages a Segment Means representation\nto approximate intermediate output features, drastically reducing inter-device\ncommunication. Additionally, we restructure the self-attention mechanism to\neliminate redundant computations caused by per-device Key/Value calculation in\nposition-wise partitioning and design a partition-aware causal masking scheme\ntailored for autoregressive models. We evaluate PRISM on ViT, BERT, and GPT-2\nacross diverse datasets, namely CIFAR-10, CIFAR-100, ImageNet-1k, GLUE, and\nCBT. Our results demonstrate substantial reductions in communication overhead\n(up to 99.2% for BERT at compression rate CR = 128) and per-device computation\n(51.24% for BERT at the same setting), with only minor accuracy degradation.\nThis method offers a scalable and practical solution for deploying foundation\nmodels in distributed resource-constrained environments.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u901a\u4fe1\u4f18\u5316\u7684\u5206\u5e03\u5f0fTransformer\u63a8\u7406\u7b56\u7565\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u548c\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5728\u8fb9\u7f18\u90e8\u7f72\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7b56\u7565\u4ee5\u652f\u6301\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002", "method": "\u91c7\u7528Segment Means\u8fd1\u4f3c\u4e2d\u95f4\u7279\u5f81\uff0c\u91cd\u6784\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4ee5\u51cf\u5c11\u5197\u4f59\u8ba1\u7b97\uff0c\u5e76\u8bbe\u8ba1\u5206\u533a\u611f\u77e5\u7684\u56e0\u679c\u63a9\u7801\u3002", "result": "\u5728ViT\u3001BERT\u548cGPT-2\u4e0a\u6d4b\u8bd5\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c11\u9ad8\u8fbe99.2%\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c1151.24%\uff0c\u7cbe\u5ea6\u635f\u5931\u8f83\u5c0f\u3002", "conclusion": "PRISM\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u57fa\u7840\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12165", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12165", "abs": "https://arxiv.org/abs/2507.12165", "authors": ["Fouad Oubari", "Mohamed El-Baha", "Raphael Meunier", "Rodrigue D\u00e9catoire", "Mathilde Mougeot"], "title": "Multi-Component VAE with Gaussian Markov Random Field", "comment": null, "summary": "Multi-component datasets with intricate dependencies, like industrial\nassemblies or multi-modal imaging, challenge current generative modeling\ntechniques. Existing Multi-component Variational AutoEncoders typically rely on\nsimplified aggregation strategies, neglecting critical nuances and consequently\ncompromising structural coherence across generated components. To explicitly\naddress this gap, we introduce the Gaussian Markov Random Field Multi-Component\nVariational AutoEncoder , a novel generative framework embedding Gaussian\nMarkov Random Fields into both prior and posterior distributions. This design\nchoice explicitly models cross-component relationships, enabling richer\nrepresentation and faithful reproduction of complex interactions. Empirically,\nour GMRF MCVAE achieves state-of-the-art performance on a synthetic Copula\ndataset specifically constructed to evaluate intricate component relationships,\ndemonstrates competitive results on the PolyMNIST benchmark, and significantly\nenhances structural coherence on the real-world BIKED dataset. Our results\nindicate that the GMRF MCVAE is especially suited for practical applications\ndemanding robust and realistic modeling of multi-component coherence", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u7ec4\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08GMRF MCVAE\uff09\uff0c\u901a\u8fc7\u5d4c\u5165\u9ad8\u65af\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\u6765\u5efa\u6a21\u7ec4\u4ef6\u95f4\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u7684\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u7ec4\u4ef6\u6570\u636e\u5efa\u6a21\u4e2d\u5ffd\u7565\u4e86\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u7f3a\u4e4f\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "method": "\u5728\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\u4e2d\u5d4c\u5165\u9ad8\u65af\u9a6c\u5c14\u53ef\u592b\u968f\u673a\u573a\uff0c\u663e\u5f0f\u5efa\u6a21\u7ec4\u4ef6\u95f4\u5173\u7cfb\u3002", "result": "\u5728\u5408\u6210Copula\u6570\u636e\u96c6\u3001PolyMNIST\u57fa\u51c6\u548c\u771f\u5b9eBIKED\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u7ed3\u6784\u4e00\u81f4\u6027\u65b9\u9762\u663e\u8457\u63d0\u5347\u3002", "conclusion": "GMRF MCVAE\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u4fdd\u771f\u591a\u7ec4\u4ef6\u5efa\u6a21\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.12192", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12192", "abs": "https://arxiv.org/abs/2507.12192", "authors": ["Victor F. Lopes de Souza", "Karima Bakhti", "Sofiane Ramdani", "Denis Mottet", "Abdelhak Imoussaten"], "title": "Explainable Evidential Clustering", "comment": null, "summary": "Unsupervised classification is a fundamental machine learning problem.\nReal-world data often contain imperfections, characterized by uncertainty and\nimprecision, which are not well handled by traditional methods. Evidential\nclustering, based on Dempster-Shafer theory, addresses these challenges. This\npaper explores the underexplored problem of explaining evidential clustering\nresults, which is crucial for high-stakes domains such as healthcare. Our\nanalysis shows that, in the general case, representativity is a necessary and\nsufficient condition for decision trees to serve as abductive explainers.\nBuilding on the concept of representativity, we generalize this idea to\naccommodate partial labeling through utility functions. These functions enable\nthe representation of \"tolerable\" mistakes, leading to the definition of\nevidential mistakeness as explanation cost and the construction of explainers\ntailored to evidential classifiers. Finally, we propose the Iterative\nEvidential Mistake Minimization (IEMM) algorithm, which provides interpretable\nand cautious decision tree explanations for evidential clustering functions. We\nvalidate the proposed algorithm on synthetic and real-world data. Taking into\naccount the decision-maker's preferences, we were able to provide an\nexplanation that was satisfactory up to 93% of the time.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8eDempster-Shafer\u7406\u8bba\u7684\u8bc1\u636e\u805a\u7c7b\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u91ca\u805a\u7c7b\u7ed3\u679c\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7IEMM\u7b97\u6cd5\u751f\u6210\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u6811\u3002", "motivation": "\u73b0\u5b9e\u6570\u636e\u5e38\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u7cbe\u786e\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u3002\u8bc1\u636e\u805a\u7c7b\u80fd\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u89e3\u91ca\u805a\u7c7b\u7ed3\u679c\u7684\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u4e2d\u3002", "method": "\u901a\u8fc7\u4ee3\u8868\u6027\u6982\u5ff5\uff0c\u5c06\u51b3\u7b56\u6811\u4f5c\u4e3a\u89e3\u91ca\u5de5\u5177\uff0c\u5e76\u5f15\u5165\u6548\u7528\u51fd\u6570\u5904\u7406\u90e8\u5206\u6807\u7b7e\u95ee\u9898\uff0c\u5b9a\u4e49\u89e3\u91ca\u6210\u672c\uff0c\u63d0\u51faIEMM\u7b97\u6cd5\u751f\u6210\u89e3\u91ca\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86IEMM\u7b97\u6cd5\uff0c93%\u7684\u60c5\u51b5\u4e0b\u80fd\u63d0\u4f9b\u6ee1\u610f\u7684\u89e3\u91ca\u3002", "conclusion": "IEMM\u7b97\u6cd5\u4e3a\u8bc1\u636e\u805a\u7c7b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u8c28\u614e\u7684\u51b3\u7b56\u6811\u89e3\u91ca\uff0c\u9002\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\u3002"}}
{"id": "2507.12218", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.12218", "abs": "https://arxiv.org/abs/2507.12218", "authors": ["Tomohisa Okazaki"], "title": "Physics-Informed Linear Model (PILM): Analytical Representations and Application to Crustal Strain Rate Estimation", "comment": null, "summary": "Many physical systems are described by partial differential equations (PDEs),\nand solving these equations and estimating their coefficients or boundary\nconditions (BCs) from observational data play a crucial role in understanding\nthe associated phenomena. Recently, a machine learning approach known as\nphysics-informed neural network, which solves PDEs using neural networks by\nminimizing the sum of residuals from the PDEs, BCs, and data, has gained\nsignificant attention in the scientific community. In this study, we\ninvestigate a physics-informed linear model (PILM) that uses linear\ncombinations of basis functions to represent solutions, thereby enabling an\nanalytical representation of optimal solutions. The PILM was formulated and\nverified for illustrative forward and inverse problems including cases with\nuncertain BCs. Furthermore, the PILM was applied to estimate crustal strain\nrates using geodetic data. Specifically, physical regularization that enforces\nelastic equilibrium on the velocity fields was compared with mathematical\nregularization that imposes smoothness constraints. From a Bayesian\nperspective, mathematical regularization exhibited superior performance. The\nPILM provides an analytically solvable framework applicable to linear forward\nand inverse problems, underdetermined systems, and physical regularization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ebf\u6027\u57fa\u51fd\u6570\u7ec4\u5408\u7684\u7269\u7406\u4fe1\u606f\u7ebf\u6027\u6a21\u578b\uff08PILM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDEs\uff09\u7684\u6b63\u53cd\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5730\u58f3\u5e94\u53d8\u7387\u4f30\u8ba1\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edfPDE\u6c42\u89e3\u65b9\u6cd5\u590d\u6742\uff0c\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u867d\u53d7\u5173\u6ce8\u4f46\u7f3a\u4e4f\u89e3\u6790\u89e3\u3002PILM\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u53ef\u89e3\u6790\u6c42\u89e3\u7684\u7ebf\u6027\u6a21\u578b\u6846\u67b6\u3002", "method": "PILM\u4f7f\u7528\u7ebf\u6027\u57fa\u51fd\u6570\u7ec4\u5408\u8868\u793a\u89e3\uff0c\u901a\u8fc7\u6700\u5c0f\u5316PDE\u6b8b\u5dee\u3001\u8fb9\u754c\u6761\u4ef6\u548c\u6570\u636e\u8bef\u5dee\u6765\u6c42\u89e3\uff0c\u9002\u7528\u4e8e\u6b63\u53cd\u95ee\u9898\u53ca\u4e0d\u786e\u5b9a\u8fb9\u754c\u6761\u4ef6\u3002", "result": "PILM\u5728\u5730\u58f3\u5e94\u53d8\u7387\u4f30\u8ba1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u6570\u5b66\u6b63\u5219\u5316\u5728\u8d1d\u53f6\u65af\u89c6\u89d2\u4e0b\u8868\u73b0\u4f18\u4e8e\u7269\u7406\u6b63\u5219\u5316\u3002", "conclusion": "PILM\u4e3a\u7ebf\u6027\u6b63\u53cd\u95ee\u9898\u3001\u6b20\u5b9a\u7cfb\u7edf\u548c\u7269\u7406\u6b63\u5219\u5316\u63d0\u4f9b\u4e86\u53ef\u89e3\u6790\u6c42\u89e3\u7684\u6846\u67b6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.12224", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12224", "abs": "https://arxiv.org/abs/2507.12224", "authors": ["Razvan Pascanu", "Clare Lyle", "Ionut-Vlad Modoranu", "Naima Elosegui Borras", "Dan Alistarh", "Petar Velickovic", "Sarath Chandar", "Soham De", "James Martens"], "title": "Optimizers Qualitatively Alter Solutions And We Should Leverage This", "comment": null, "summary": "Due to the nonlinear nature of Deep Neural Networks (DNNs), one can not\nguarantee convergence to a unique global minimum of the loss when using\noptimizers relying only on local information, such as SGD. Indeed, this was a\nprimary source of skepticism regarding the feasibility of DNNs in the early\ndays of the field. The past decades of progress in deep learning have revealed\nthis skepticism to be misplaced, and a large body of empirical evidence shows\nthat sufficiently large DNNs following standard training protocols exhibit\nwell-behaved optimization dynamics that converge to performant solutions. This\nsuccess has biased the community to use convex optimization as a mental model\nfor learning, leading to a focus on training efficiency, either in terms of\nrequired iteration, FLOPs or wall-clock time, when improving optimizers. We\nargue that, while this perspective has proven extremely fruitful, another\nperspective specific to DNNs has received considerably less attention: the\noptimizer not only influences the rate of convergence, but also the qualitative\nproperties of the learned solutions. Restated, the optimizer can and will\nencode inductive biases and change the effective expressivity of a given class\nof models. Furthermore, we believe the optimizer can be an effective way of\nencoding desiderata in the learning process. We contend that the community\nshould aim at understanding the biases of already existing methods, as well as\naim to build new optimizers with the explicit intent of inducing certain\nproperties of the solution, rather than solely judging them based on their\nconvergence rates. We hope our arguments will inspire research to improve our\nunderstanding of how the learning process can impact the type of solution we\nconverge to, and lead to a greater recognition of optimizers design as a\ncritical lever that complements the roles of architecture and data in shaping\nmodel outcomes.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86DNN\u4f18\u5316\u5668\u7684\u89d2\u8272\u4e0d\u4ec5\u9650\u4e8e\u6536\u655b\u901f\u5ea6\uff0c\u8fd8\u5f71\u54cd\u5b66\u4e60\u89e3\u7684\u6027\u8d28\uff0c\u547c\u5401\u5173\u6ce8\u4f18\u5316\u5668\u8bbe\u8ba1\u5bf9\u6a21\u578b\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "motivation": "\u65e9\u671f\u5bf9DNN\u53ef\u884c\u6027\u7684\u6000\u7591\u6e90\u4e8e\u5176\u975e\u7ebf\u6027\u7279\u6027\u5bfc\u81f4\u65e0\u6cd5\u4fdd\u8bc1\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u4f46\u5b9e\u9645\u7ecf\u9a8c\u8868\u660e\u6807\u51c6\u8bad\u7ec3\u534f\u8bae\u4e0bDNN\u8868\u73b0\u826f\u597d\u3002\u7136\u800c\uff0c\u793e\u533a\u8fc7\u4e8e\u5173\u6ce8\u4f18\u5316\u6548\u7387\uff0c\u5ffd\u89c6\u4e86\u4f18\u5316\u5668\u5bf9\u5b66\u4e60\u89e3\u6027\u8d28\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u4f18\u5316\u5668\u7684\u884c\u4e3a\uff0c\u63d0\u51fa\u4f18\u5316\u5668\u4e0d\u4ec5\u5f71\u54cd\u6536\u655b\u901f\u5ea6\uff0c\u8fd8\u80fd\u901a\u8fc7\u5f15\u5165\u5f52\u7eb3\u504f\u7f6e\u6539\u53d8\u6a21\u578b\u7684\u6709\u6548\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u4f18\u5316\u5668\u53ef\u4ee5\u7f16\u7801\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u671f\u671b\u7279\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u52a0\u901f\u6536\u655b\u3002", "conclusion": "\u547c\u5401\u793e\u533a\u5173\u6ce8\u4f18\u5316\u5668\u8bbe\u8ba1\u5bf9\u6a21\u578b\u7ed3\u679c\u7684\u5f71\u54cd\uff0c\u5c06\u5176\u89c6\u4e3a\u4e0e\u67b6\u6784\u548c\u6570\u636e\u540c\u7b49\u91cd\u8981\u7684\u56e0\u7d20\u3002"}}
{"id": "2507.12297", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12297", "abs": "https://arxiv.org/abs/2507.12297", "authors": ["Yuan-Chen Shu", "Zhiwei Lin", "Yongtao Wang"], "title": "RegCL: Continual Adaptation of Segment Anything Model via Model Merging", "comment": null, "summary": "To address the performance limitations of the Segment Anything Model (SAM) in\nspecific domains, existing works primarily adopt adapter-based one-step\nadaptation paradigms. However, some of these methods are specific developed for\nspecific domains. If used on other domains may lead to performance degradation.\nThis issue of catastrophic forgetting severely limits the model's scalability.\nTo address this issue, this paper proposes RegCL, a novel non-replay continual\nlearning (CL) framework designed for efficient multi-domain knowledge\nintegration through model merging. Specifically, RegCL incorporates the model\nmerging algorithm into the continual learning paradigm by merging the\nparameters of SAM's adaptation modules (e.g., LoRA modules) trained on\ndifferent domains. The merging process is guided by weight optimization, which\nminimizes prediction discrepancies between the merged model and each of the\ndomain-specific models. RegCL effectively consolidates multi-domain knowledge\nwhile maintaining parameter efficiency, i.e., the model size remains constant\nregardless of the number of tasks, and no historical data storage is required.\nExperimental results demonstrate that RegCL achieves favorable continual\nlearning performance across multiple downstream datasets, validating its\neffectiveness in dynamic scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRegCL\u7684\u65b0\u578b\u975e\u56de\u653e\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u5b9e\u73b0\u591a\u9886\u57df\u77e5\u8bc6\u7684\u9ad8\u6548\u6574\u5408\uff0c\u89e3\u51b3\u4e86Segment Anything Model\uff08SAM\uff09\u5728\u7279\u5b9a\u9886\u57df\u6027\u80fd\u53d7\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u91c7\u7528\u57fa\u4e8e\u9002\u914d\u5668\u7684\u4e00\u6b65\u9002\u5e94\u8303\u5f0f\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u9886\u57df\u8bbe\u8ba1\uff0c\u8de8\u9886\u57df\u4f7f\u7528\u65f6\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "RegCL\u5c06\u6a21\u578b\u5408\u5e76\u7b97\u6cd5\u5f15\u5165\u6301\u7eed\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u5408\u5e76\u4e0d\u540c\u9886\u57df\u8bad\u7ec3\u7684SAM\u9002\u914d\u6a21\u5757\uff08\u5982LoRA\u6a21\u5757\uff09\u53c2\u6570\uff0c\u5e76\u4ee5\u6743\u91cd\u4f18\u5316\u4e3a\u6307\u5bfc\uff0c\u6700\u5c0f\u5316\u5408\u5e76\u6a21\u578b\u4e0e\u5404\u9886\u57df\u7279\u5b9a\u6a21\u578b\u4e4b\u95f4\u7684\u9884\u6d4b\u5dee\u5f02\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRegCL\u5728\u591a\u4e2a\u4e0b\u6e38\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u6301\u7eed\u5b66\u4e60\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "RegCL\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u9ad8\u6548\u6574\u5408\u591a\u9886\u57df\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u6548\u7387\uff08\u6a21\u578b\u5927\u5c0f\u4e0d\u968f\u4efb\u52a1\u6570\u91cf\u589e\u52a0\u800c\u53d8\u5316\uff0c\u4e14\u65e0\u9700\u5b58\u50a8\u5386\u53f2\u6570\u636e\uff09\u3002"}}
{"id": "2507.12305", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.12305", "abs": "https://arxiv.org/abs/2507.12305", "authors": ["M. Anwar Ma'sum", "Mahardhika Pratama", "Savitha Ramasamy", "Lin Liu", "Habibullah Habibullah", "Ryszard Kowalczyk"], "title": "PROL : Rehearsal Free Continual Learning in Streaming Data via Prompt Online Learning", "comment": "ICCV 2025", "summary": "The data privacy constraint in online continual learning (OCL), where the\ndata can be seen only once, complicates the catastrophic forgetting problem in\nstreaming data. A common approach applied by the current SOTAs in OCL is with\nthe use of memory saving exemplars or features from previous classes to be\nreplayed in the current task. On the other hand, the prompt-based approach\nperforms excellently in continual learning but with the cost of a growing\nnumber of trainable parameters. The first approach may not be applicable in\npractice due to data openness policy, while the second approach has the issue\nof throughput associated with the streaming data. In this study, we propose a\nnovel prompt-based method for online continual learning that includes 4 main\ncomponents: (1) single light-weight prompt generator as a general knowledge,\n(2) trainable scaler-and-shifter as specific knowledge, (3) pre-trained model\n(PTM) generalization preserving, and (4) hard-soft updates mechanism. Our\nproposed method achieves significantly higher performance than the current\nSOTAs in CIFAR100, ImageNet-R, ImageNet-A, and CUB dataset. Our complexity\nanalysis shows that our method requires a relatively smaller number of\nparameters and achieves moderate training time, inference time, and throughput.\nFor further study, the source code of our method is available at\nhttps://github.com/anwarmaxsum/PROL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63d0\u793a\u7684\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u9690\u79c1\u548c\u53c2\u6570\u589e\u957f\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u4e2d\u6570\u636e\u9690\u79c1\u7ea6\u675f\u548c\u53c2\u6570\u589e\u957f\u95ee\u9898\u9650\u5236\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u3002", "method": "\u7ed3\u5408\u8f7b\u91cf\u7ea7\u63d0\u793a\u751f\u6210\u5668\u3001\u53ef\u8bad\u7ec3\u7684\u7f29\u653e\u79fb\u4f4d\u5668\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u6cdb\u5316\u4fdd\u62a4\u53ca\u786c\u8f6f\u66f4\u65b0\u673a\u5236\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53c2\u6570\u8f83\u5c11\u4e14\u8bad\u7ec3\u63a8\u7406\u65f6\u95f4\u9002\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.12314", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.12314", "abs": "https://arxiv.org/abs/2507.12314", "authors": ["Zihao Xue", "Zhen Bi", "Long Ma", "Zhenlin Hu", "Yan Wang", "Zhenfang Liu", "Qing Sheng", "Jie Xiao", "Jungang Lou"], "title": "Thought Purity: Defense Paradigm For Chain-of-Thought Attack", "comment": null, "summary": "While reinforcement learning-trained Large Reasoning Models (LRMs, e.g.,\nDeepseek-R1) demonstrate advanced reasoning capabilities in the evolving Large\nLanguage Models (LLMs) domain, their susceptibility to security threats remains\na critical vulnerability. This weakness is particularly evident in\nChain-of-Thought (CoT) generation processes, where adversarial methods like\nbackdoor prompt attacks can systematically subvert the model's core reasoning\nmechanisms. The emerging Chain-of-Thought Attack (CoTA) reveals this\nvulnerability through exploiting prompt controllability, simultaneously\ndegrading both CoT safety and task performance with low-cost interventions. To\naddress this compounded security-performance vulnerability, we propose Thought\nPurity (TP): a defense paradigm that systematically strengthens resistance to\nmalicious content while preserving operational efficacy. Our solution achieves\nthis through three synergistic components: (1) a safety-optimized data\nprocessing pipeline (2) reinforcement learning-enhanced rule constraints (3)\nadaptive monitoring metrics. Our approach establishes the first comprehensive\ndefense mechanism against CoTA vulnerabilities in reinforcement\nlearning-aligned reasoning systems, significantly advancing the\nsecurity-functionality equilibrium for next-generation AI architectures.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faThought Purity\uff08TP\uff09\u9632\u5fa1\u8303\u5f0f\uff0c\u9488\u5bf9\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728Chain-of-Thought\uff08CoT\uff09\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u5904\u7406\u3001\u5f3a\u5316\u89c4\u5219\u7ea6\u675f\u548c\u81ea\u9002\u5e94\u76d1\u63a7\uff0c\u63d0\u5347\u5b89\u5168\u6027\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08\u5982Deepseek-R1\uff09\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u5728Chain-of-Thought\u751f\u6210\u8fc7\u7a0b\u4e2d\u6613\u53d7\u5b89\u5168\u5a01\u80c1\uff08\u5982\u540e\u95e8\u63d0\u793a\u653b\u51fb\uff09\uff0c\u5bfc\u81f4\u63a8\u7406\u673a\u5236\u88ab\u7cfb\u7edf\u6027\u7834\u574f\uff0c\u4e9f\u9700\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faThought Purity\uff08TP\uff09\u9632\u5fa1\u8303\u5f0f\uff0c\u5305\u542b\u4e09\u4e2a\u534f\u540c\u7ec4\u4ef6\uff1a\u5b89\u5168\u4f18\u5316\u7684\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3001\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684\u89c4\u5219\u7ea6\u675f\u548c\u81ea\u9002\u5e94\u76d1\u63a7\u6307\u6807\u3002", "result": "TP\u662f\u9996\u4e2a\u9488\u5bf9CoTA\u6f0f\u6d1e\u7684\u5168\u9762\u9632\u5fa1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b89\u5168\u6027\u4e0e\u529f\u80fd\u6027\u7684\u5e73\u8861\u3002", "conclusion": "TP\u4e3a\u4e0b\u4e00\u4ee3AI\u67b6\u6784\u63d0\u4f9b\u4e86\u5b89\u5168\u6027\u4e0e\u6027\u80fd\u517c\u987e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u63a8\u7406\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u53d1\u5c55\u3002"}}
{"id": "2507.12341", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12341", "abs": "https://arxiv.org/abs/2507.12341", "authors": ["Antoine Saillenfest", "Pirmin Lemberger"], "title": "Nonlinear Concept Erasure: a Density Matching Approach", "comment": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)", "summary": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLEOPARD\u7684\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u6295\u5f71\u4ece\u6587\u672c\u8868\u793a\u4e2d\u79fb\u9664\u654f\u611f\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u4ed6\u8bed\u4e49\u4fe1\u606f\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u516c\u5e73\u6027\u8981\u6c42\u4e0b\u65e0\u6cd5\u63a8\u65ad\u654f\u611f\u4fe1\u606f\uff08\u5982\u6027\u522b\u6216\u79cd\u65cf\uff09\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6b63\u4ea4\u6295\u5f71\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5b66\u4e60\uff0c\u4f7f\u76ee\u6807\u6982\u5ff5\u7684\u7c7b\u6761\u4ef6\u7279\u5f81\u5206\u5e03\u5728\u6295\u5f71\u540e\u65e0\u6cd5\u533a\u5206\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u6295\u5f71\u79e9\u63a7\u5236\u4fe1\u606f\u79fb\u9664\u7a0b\u5ea6\u3002", "result": "LEOPARD\u5728\u7ecf\u5178NLP\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u79bb\u6563\u5c5e\u6027\u975e\u7ebf\u6027\u64e6\u9664\u6027\u80fd\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u6df1\u5ea6\u975e\u7ebf\u6027\u5206\u7c7b\u5668\u7684\u504f\u89c1\u3002", "conclusion": "LEOPARD\u65b9\u6cd5\u5728\u63d0\u5347\u516c\u5e73\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u654f\u611f\u4fe1\u606f\u79fb\u9664\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12380", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12380", "abs": "https://arxiv.org/abs/2507.12380", "authors": ["Maximilian Krahn", "Vikas Garg"], "title": "Heat Kernel Goes Topological", "comment": null, "summary": "Topological neural networks have emerged as powerful successors of graph\nneural networks. However, they typically involve higher-order message passing,\nwhich incurs significant computational expense. We circumvent this issue with a\nnovel topological framework that introduces a Laplacian operator on\ncombinatorial complexes (CCs), enabling efficient computation of heat kernels\nthat serve as node descriptors. Our approach captures multiscale information\nand enables permutation-equivariant representations, allowing easy integration\ninto modern transformer-based architectures.\n  Theoretically, the proposed method is maximally expressive because it can\ndistinguish arbitrary non-isomorphic CCs. Empirically, it significantly\noutperforms existing topological methods in terms of computational efficiency.\nBesides demonstrating competitive performance with the state-of-the-art\ndescriptors on standard molecular datasets, it exhibits superior capability in\ndistinguishing complex topological structures and avoiding blind spots on\ntopological benchmarks. Overall, this work advances topological deep learning\nby providing expressive yet scalable representations, thereby opening up\nexciting avenues for molecular classification and property prediction tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u5408\u590d\u5f62\uff08CCs\uff09\u7684\u65b0\u578b\u62d3\u6251\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u548c\u70ed\u6838\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u5c3a\u5ea6\u4fe1\u606f\u6355\u6349\u548c\u7f6e\u6362\u7b49\u53d8\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u62d3\u6251\u795e\u7ecf\u7f51\u7edc\u56e0\u9ad8\u9636\u6d88\u606f\u4f20\u9012\u5bfc\u81f4\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "\u5728\u7ec4\u5408\u590d\u5f62\u4e0a\u5f15\u5165\u62c9\u666e\u62c9\u65af\u7b97\u5b50\uff0c\u8ba1\u7b97\u70ed\u6838\u4f5c\u4e3a\u8282\u70b9\u63cf\u8ff0\u7b26\uff0c\u652f\u6301\u591a\u5c3a\u5ea6\u4fe1\u606f\u6355\u6349\u548c\u7f6e\u6362\u7b49\u53d8\u8868\u793a\u3002", "result": "\u7406\u8bba\u4e0a\u5177\u6709\u6700\u5927\u8868\u8fbe\u80fd\u529b\uff0c\u80fd\u533a\u5206\u4efb\u610f\u975e\u540c\u6784\u7ec4\u5408\u590d\u5f62\uff1b\u5b9e\u9a8c\u4e0a\u8ba1\u7b97\u6548\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u62d3\u6251\u65b9\u6cd5\uff0c\u5e76\u5728\u5206\u5b50\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u62d3\u6251\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u8868\u8fbe\u80fd\u529b\u5f3a\u7684\u8868\u793a\uff0c\u4e3a\u5206\u5b50\u5206\u7c7b\u548c\u6027\u8d28\u9884\u6d4b\u4efb\u52a1\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.12383", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12383", "abs": "https://arxiv.org/abs/2507.12383", "authors": ["Mohit Prashant", "Arvind Easwaran"], "title": "Improving Reinforcement Learning Sample-Efficiency using Local Approximation", "comment": "Preprint", "summary": "In this study, we derive Probably Approximately Correct (PAC) bounds on the\nasymptotic sample-complexity for RL within the infinite-horizon Markov Decision\nProcess (MDP) setting that are sharper than those in existing literature. The\npremise of our study is twofold: firstly, the further two states are from each\nother, transition-wise, the less relevant the value of the first state is when\nlearning the $\\epsilon$-optimal value of the second; secondly, the amount of\n'effort', sample-complexity-wise, expended in learning the $\\epsilon$-optimal\nvalue of a state is independent of the number of samples required to learn the\n$\\epsilon$-optimal value of a second state that is a sufficient number of\ntransitions away from the first. Inversely, states within each other's vicinity\nhave values that are dependent on each other and will require a similar number\nof samples to learn. By approximating the original MDP using smaller MDPs\nconstructed using subsets of the original's state-space, we are able to reduce\nthe sample-complexity by a logarithmic factor to $O(SA \\log A)$ timesteps,\nwhere $S$ and $A$ are the state and action space sizes. We are able to extend\nthese results to an infinite-horizon, model-free setting by constructing a\nPAC-MDP algorithm with the aforementioned sample-complexity. We conclude with\nshowing how significant the improvement is by comparing our algorithm against\nprior work in an experimental setting.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5728\u65e0\u9650\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u4e2d\uff0c\u6bd4\u73b0\u6709\u6587\u732e\u66f4\u7cbe\u786e\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6837\u672c\u590d\u6742\u5ea6\u7684PAC\u8fb9\u754c\u3002\u901a\u8fc7\u8fd1\u4f3c\u539f\u59cbMDP\uff0c\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4e\u5230O(SA log A)\u65f6\u95f4\u6b65\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u4e24\u70b9\uff1a\u4e00\u662f\u72b6\u6001\u95f4\u7684\u8ddd\u79bb\u5f71\u54cd\u5b66\u4e60\u6548\u7387\uff1b\u4e8c\u662f\u5b66\u4e60\u4e00\u4e2a\u72b6\u6001\u7684\u03b5\u6700\u4f18\u503c\u6240\u9700\u7684\u6837\u672c\u590d\u6742\u5ea6\u4e0e\u8fdc\u79bb\u5b83\u7684\u72b6\u6001\u65e0\u5173\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u6784\u5efa\u539f\u59cbMDP\u72b6\u6001\u7a7a\u95f4\u7684\u5b50\u96c6\u6765\u8fd1\u4f3cMDP\uff0c\u4ece\u800c\u51cf\u5c11\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u6269\u5c55\u5230\u65e0\u9650\u65f6\u95f4\u3001\u65e0\u6a21\u578b\u8bbe\u7f6e\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6837\u672c\u590d\u6742\u5ea6\u964d\u4f4e\u5230O(SA log A)\u65f6\u95f4\u6b65\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6539\u8fdb\u7684\u663e\u8457\u6027\u3002", "conclusion": "\u7ed3\u8bba\u662f\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.12384", "categories": ["cs.LG", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.12384", "abs": "https://arxiv.org/abs/2507.12384", "authors": ["Bo Wen", "Guoyun Gao", "Zhicheng Xu", "Ruibin Mao", "Xiaojuan Qi", "X. Sharon Hu", "Xunzhao Yin", "Can Li"], "title": "Trustworthy Tree-based Machine Learning by $MoS_2$ Flash-based Analog CAM with Inherent Soft Boundaries", "comment": null, "summary": "The rapid advancement of artificial intelligence has raised concerns\nregarding its trustworthiness, especially in terms of interpretability and\nrobustness. Tree-based models like Random Forest and XGBoost excel in\ninterpretability and accuracy for tabular data, but scaling them remains\ncomputationally expensive due to poor data locality and high data dependence.\nPrevious efforts to accelerate these models with analog content addressable\nmemory (CAM) have struggled, due to the fact that the difficult-to-implement\nsharp decision boundaries are highly susceptible to device variations, which\nleads to poor hardware performance and vulnerability to adversarial attacks.\nThis work presents a novel hardware-software co-design approach using $MoS_2$\nFlash-based analog CAM with inherent soft boundaries, enabling efficient\ninference with soft tree-based models. Our soft tree model inference\nexperiments on $MoS_2$ analog CAM arrays show this method achieves exceptional\nrobustness against device variation and adversarial attacks while achieving\nstate-of-the-art accuracy. Specifically, our fabricated analog CAM arrays\nachieve $96\\%$ accuracy on Wisconsin Diagnostic Breast Cancer (WDBC) database,\nwhile maintaining decision explainability. Our experimentally calibrated model\nvalidated only a $0.6\\%$ accuracy drop on the MNIST dataset under $10\\%$ device\nthreshold variation, compared to a $45.3\\%$ drop for traditional decision\ntrees. This work paves the way for specialized hardware that enhances AI's\ntrustworthiness and efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e$MoS_2$\u95ea\u5b58\u6a21\u62dfCAM\u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u6811\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u5f15\u53d1\u4e86\u5bf9\u5176\u53ef\u4fe1\u5ea6\u7684\u62c5\u5fe7\uff0c\u5c24\u5176\u662f\u5728\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u3002\u4f20\u7edf\u6811\u6a21\u578b\u5728\u6269\u5c55\u65f6\u9762\u4e34\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u6a21\u62dfCAM\u65b9\u6cd5\u56e0\u8bbe\u5907\u53d8\u5316\u654f\u611f\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u91c7\u7528$MoS_2$\u95ea\u5b58\u6a21\u62dfCAM\u7684\u8f6f\u8fb9\u754c\u7279\u6027\uff0c\u7ed3\u5408\u8f6f\u6811\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728WDBC\u6570\u636e\u96c6\u4e0a\u8fbe\u523096%\u51c6\u786e\u7387\uff0c\u4e14\u5728\u8bbe\u5907\u9608\u503c\u53d8\u531610%\u65f6\u4ec5\u635f\u59310.6%\u51c6\u786e\u7387\uff0c\u8fdc\u4f18\u4e8e\u4f20\u7edf\u51b3\u7b56\u6811\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63d0\u5347AI\u53ef\u4fe1\u5ea6\u548c\u6548\u7387\u7684\u4e13\u7528\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.12412", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12412", "abs": "https://arxiv.org/abs/2507.12412", "authors": ["Dzung Dinh", "Boqi Chen", "Marc Niethammer", "Junier Oliva"], "title": "NOCTA: Non-Greedy Objective Cost-Tradeoff Acquisition for Longitudinal Data", "comment": null, "summary": "In many critical applications, resource constraints limit the amount of\ninformation that can be gathered to make predictions. For example, in\nhealthcare, patient data often spans diverse features ranging from lab tests to\nimaging studies. Each feature may carry different information and must be\nacquired at a respective cost of time, money, or risk to the patient. Moreover,\ntemporal prediction tasks, where both instance features and labels evolve over\ntime, introduce additional complexity in deciding when or what information is\nimportant. In this work, we propose NOCTA, a Non-Greedy Objective Cost-Tradeoff\nAcquisition method that sequentially acquires the most informative features at\ninference time while accounting for both temporal dynamics and acquisition\ncost. We first introduce a cohesive estimation target for our NOCTA setting,\nand then develop two complementary estimators: 1) a non-parametric method based\non nearest neighbors to guide the acquisition (NOCTA-NP), and 2) a parametric\nmethod that directly predicts the utility of potential acquisitions (NOCTA-P).\nExperiments on synthetic and real-world medical datasets demonstrate that both\nNOCTA variants outperform existing baselines.", "AI": {"tldr": "NOCTA\u662f\u4e00\u79cd\u975e\u8d2a\u5a6a\u7684\u76ee\u6807\u6210\u672c\u6743\u8861\u83b7\u53d6\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u63a8\u7406\u65f6\u52a8\u6001\u83b7\u53d6\u6700\u5177\u4fe1\u606f\u91cf\u7684\u7279\u5f81\uff0c\u540c\u65f6\u8003\u8651\u65f6\u95f4\u52a8\u6001\u548c\u83b7\u53d6\u6210\u672c\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5e94\u7528\u4e2d\uff08\u5982\u533b\u7597\uff09\uff0c\u9700\u8981\u9ad8\u6548\u83b7\u53d6\u4fe1\u606f\u4ee5\u8fdb\u884c\u9884\u6d4b\uff0c\u540c\u65f6\u8003\u8651\u6210\u672c\u548c\u65f6\u95f4\u52a8\u6001\u3002", "method": "\u63d0\u51faNOCTA\u65b9\u6cd5\uff0c\u5305\u62ec\u975e\u53c2\u6570\uff08NOCTA-NP\uff09\u548c\u53c2\u6570\uff08NOCTA-P\uff09\u4e24\u79cd\u4f30\u8ba1\u5668\uff0c\u5206\u522b\u57fa\u4e8e\u6700\u8fd1\u90bb\u548c\u76f4\u63a5\u9884\u6d4b\u83b7\u53d6\u6548\u7528\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u533b\u7597\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNOCTA\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "NOCTA\u80fd\u6709\u6548\u5e73\u8861\u4fe1\u606f\u83b7\u53d6\u4e0e\u6210\u672c\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u9884\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2507.12419", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12419", "abs": "https://arxiv.org/abs/2507.12419", "authors": ["Andrea Perin", "Giacomo Lagomarsini", "Claudio Gallicchio", "Giuseppe Nuti"], "title": "Mixture of Raytraced Experts", "comment": "Preliminary version (pre-submission)", "summary": "We introduce a Mixture of Raytraced Experts, a stacked Mixture of Experts\n(MoE) architecture which can dynamically select sequences of experts, producing\ncomputational graphs of variable width and depth. Existing MoE architectures\ngenerally require a fixed amount of computation for a given sample. Our\napproach, in contrast, yields predictions with increasing accuracy as the\ncomputation cycles through the experts' sequence. We train our model by\niteratively sampling from a set of candidate experts, unfolding the sequence\nakin to how Recurrent Neural Networks are trained. Our method does not require\nload-balancing mechanisms, and preliminary experiments show a reduction in\ntraining epochs of 10\\% to 40\\% with a comparable/higher accuracy. These\nresults point to new research directions in the field of MoEs, allowing the\ndesign of potentially faster and more expressive models. The code is available\nat https://github.com/nutig/RayTracing", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5e8f\u5217\u7684\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff08Mixture of Raytraced Experts\uff09\uff0c\u80fd\u591f\u751f\u6210\u53ef\u53d8\u5bbd\u5ea6\u548c\u6df1\u5ea6\u7684\u8ba1\u7b97\u56fe\uff0c\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u901a\u5e38\u9700\u8981\u56fa\u5b9a\u8ba1\u7b97\u91cf\uff0c\u65e0\u6cd5\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u8d44\u6e90\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e13\u5bb6\u5e8f\u5217\uff0c\u5b9e\u73b0\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "method": "\u901a\u8fc7\u8fed\u4ee3\u91c7\u6837\u5019\u9009\u4e13\u5bb6\u5e8f\u5217\uff0c\u7c7b\u4f3c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u65b9\u5f0f\uff0c\u65e0\u9700\u8d1f\u8f7d\u5747\u8861\u673a\u5236\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\u8bad\u7ec3\u5468\u671f\u51cf\u5c1110%\u81f340%\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\uff0c\u53ef\u80fd\u8bbe\u8ba1\u51fa\u66f4\u5feb\u3001\u8868\u8fbe\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\u3002"}}
{"id": "2507.12435", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12435", "abs": "https://arxiv.org/abs/2507.12435", "authors": ["Yi Li", "David Mccoy", "Nolan Gunter", "Kaitlyn Lee", "Alejandro Schuler", "Mark van der Laan"], "title": "Targeted Deep Architectures: A TMLE-Based Framework for Robust Causal Inference in Neural Networks", "comment": null, "summary": "Modern deep neural networks are powerful predictive tools yet often lack\nvalid inference for causal parameters, such as treatment effects or entire\nsurvival curves. While frameworks like Double Machine Learning (DML) and\nTargeted Maximum Likelihood Estimation (TMLE) can debias machine-learning fits,\nexisting neural implementations either rely on \"targeted losses\" that do not\nguarantee solving the efficient influence function equation or computationally\nexpensive post-hoc \"fluctuations\" for multi-parameter settings. We propose\nTargeted Deep Architectures (TDA), a new framework that embeds TMLE directly\ninto the network's parameter space with no restrictions on the backbone\narchitecture. Specifically, TDA partitions model parameters - freezing all but\na small \"targeting\" subset - and iteratively updates them along a targeting\ngradient, derived from projecting the influence functions onto the span of the\ngradients of the loss with respect to weights. This procedure yields plug-in\nestimates that remove first-order bias and produce asymptotically valid\nconfidence intervals. Crucially, TDA easily extends to multi-dimensional causal\nestimands (e.g., entire survival curves) by merging separate targeting\ngradients into a single universal targeting update. Theoretically, TDA inherits\nclassical TMLE properties, including double robustness and semiparametric\nefficiency. Empirically, on the benchmark IHDP dataset (average treatment\neffects) and simulated survival data with informative censoring, TDA reduces\nbias and improves coverage relative to both standard neural-network estimators\nand prior post-hoc approaches. In doing so, TDA establishes a direct, scalable\npathway toward rigorous causal inference within modern deep architectures for\ncomplex multi-parameter targets.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTDA\u7684\u65b0\u6846\u67b6\uff0c\u5c06TMLE\u76f4\u63a5\u5d4c\u5165\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u7a7a\u95f4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u504f\u5dee\u95ee\u9898\uff0c\u5e76\u9002\u7528\u4e8e\u591a\u53c2\u6570\u8bbe\u7f6e\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u9884\u6d4b\u65b9\u9762\u5f3a\u5927\uff0c\u4f46\u5728\u56e0\u679c\u53c2\u6570\u63a8\u65ad\uff08\u5982\u6cbb\u7597\u6548\u679c\u6216\u751f\u5b58\u66f2\u7ebf\uff09\u4e0a\u7f3a\u4e4f\u6709\u6548\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u4fdd\u8bc1\u89e3\u51b3\u9ad8\u6548\u5f71\u54cd\u51fd\u6570\u65b9\u7a0b\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "TDA\u901a\u8fc7\u51bb\u7ed3\u5927\u90e8\u5206\u53c2\u6570\u5e76\u8fed\u4ee3\u66f4\u65b0\u4e00\u5c0f\u90e8\u5206\u201c\u76ee\u6807\u201d\u53c2\u6570\uff0c\u5229\u7528\u76ee\u6807\u68af\u5ea6\u6d88\u9664\u4e00\u9636\u504f\u5dee\uff0c\u751f\u6210\u6e10\u8fd1\u6709\u6548\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u5728IHDP\u6570\u636e\u96c6\u548c\u6a21\u62df\u751f\u5b58\u6570\u636e\u4e0a\uff0cTDA\u51cf\u5c11\u4e86\u504f\u5dee\u5e76\u63d0\u9ad8\u4e86\u8986\u76d6\u7387\uff0c\u4f18\u4e8e\u6807\u51c6\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u5668\u548c\u73b0\u6709\u540e\u5904\u7406\u65b9\u6cd5\u3002", "conclusion": "TDA\u4e3a\u590d\u6742\u591a\u53c2\u6570\u76ee\u6807\u7684\u6df1\u5ea6\u67b6\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u63a5\u3001\u53ef\u6269\u5c55\u7684\u4e25\u683c\u56e0\u679c\u63a8\u65ad\u9014\u5f84\u3002"}}
{"id": "2507.12439", "categories": ["cs.LG", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.12439", "abs": "https://arxiv.org/abs/2507.12439", "authors": ["Daniel Commey", "Rebecca A. Sarpong", "Griffith S. Klogo", "Winful Bagyl-Bac", "Garth V. Crosby"], "title": "A Bayesian Incentive Mechanism for Poison-Resilient Federated Learning", "comment": null, "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients while preserving data privacy. However, its\nopen-participation nature exposes it to data-poisoning attacks, in which\nmalicious actors submit corrupted model updates to degrade the global model.\nExisting defenses are often reactive, relying on statistical aggregation rules\nthat can be computationally expensive and that typically assume an honest\nmajority. This paper introduces a proactive, economic defense: a lightweight\nBayesian incentive mechanism that makes malicious behavior economically\nirrational. Each training round is modeled as a Bayesian game of incomplete\ninformation in which the server, acting as the principal, uses a small, private\nvalidation dataset to verify update quality before issuing payments. The design\nsatisfies Individual Rationality (IR) for benevolent clients, ensuring their\nparticipation is profitable, and Incentive Compatibility (IC), making poisoning\nan economically dominated strategy. Extensive experiments on non-IID partitions\nof MNIST and FashionMNIST demonstrate robustness: with 50% label-flipping\nadversaries on MNIST, the mechanism maintains 96.7% accuracy, only 0.3\npercentage points lower than in a scenario with 30% label-flipping adversaries.\nThis outcome is 51.7 percentage points better than standard FedAvg, which\ncollapses under the same 50% attack. The mechanism is computationally light,\nbudget-bounded, and readily integrates into existing FL frameworks, offering a\npractical route to economically robust and sustainable FL ecosystems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8d1d\u53f6\u65af\u6fc0\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u7ecf\u6d4e\u624b\u6bb5\u9632\u6b62\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u786e\u4fdd\u6076\u610f\u884c\u4e3a\u5728\u7ecf\u6d4e\u4e0a\u4e0d\u5212\u7b97\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u7684\u5f00\u653e\u53c2\u4e0e\u6027\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u591a\u4e3a\u88ab\u52a8\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8d1d\u53f6\u65af\u535a\u5f08\u6a21\u578b\uff0c\u670d\u52a1\u5668\u4f7f\u7528\u5c0f\u578b\u9a8c\u8bc1\u6570\u636e\u96c6\u9a8c\u8bc1\u66f4\u65b0\u8d28\u91cf\u5e76\u53d1\u653e\u5956\u52b1\uff0c\u6ee1\u8db3\u4e2a\u4f53\u7406\u6027\uff08IR\uff09\u548c\u6fc0\u52b1\u76f8\u5bb9\u6027\uff08IC\uff09\u3002", "result": "\u5728MNIST\u548cFashionMNIST\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u4e0a\uff0c\u5373\u4f7f\u670950%\u7684\u6807\u7b7e\u7ffb\u8f6c\u653b\u51fb\uff0c\u6a21\u578b\u4ecd\u4fdd\u630196.7%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u673a\u5236\u8ba1\u7b97\u8f7b\u91cf\u3001\u9884\u7b97\u53ef\u63a7\uff0c\u53ef\u96c6\u6210\u5230\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u4e3a\u7ecf\u6d4e\u7a33\u5065\u7684\u8054\u90a6\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2507.12453", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12453", "abs": "https://arxiv.org/abs/2507.12453", "authors": ["Qian Xie", "Linda Cai", "Alexander Terenin", "Peter I. Frazier", "Ziv Scully"], "title": "Cost-aware Stopping for Bayesian Optimization", "comment": null, "summary": "In automated machine learning, scientific discovery, and other applications\nof Bayesian optimization, deciding when to stop evaluating expensive black-box\nfunctions is an important practical consideration. While several adaptive\nstopping rules have been proposed, in the cost-aware setting they lack\nguarantees ensuring they stop before incurring excessive function evaluation\ncosts. We propose a cost-aware stopping rule for Bayesian optimization that\nadapts to varying evaluation costs and is free of heuristic tuning. Our rule is\ngrounded in a theoretical connection to state-of-the-art cost-aware acquisition\nfunctions, namely the Pandora's Box Gittins Index (PBGI) and log expected\nimprovement per cost. We prove a theoretical guarantee bounding the expected\ncumulative evaluation cost incurred by our stopping rule when paired with these\ntwo acquisition functions. In experiments on synthetic and empirical tasks,\nincluding hyperparameter optimization and neural architecture size search, we\nshow that combining our stopping rule with the PBGI acquisition function\nconsistently matches or outperforms other acquisition-function--stopping-rule\npairs in terms of cost-adjusted simple regret, a metric capturing trade-offs\nbetween solution quality and cumulative evaluation cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6210\u672c\u611f\u77e5\u7684\u8d1d\u53f6\u65af\u4f18\u5316\u505c\u6b62\u89c4\u5219\uff0c\u65e0\u9700\u542f\u53d1\u5f0f\u8c03\u53c2\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u8bc4\u4f30\u6210\u672c\uff0c\u5e76\u4e0ePBGI\u548clog EI per cost\u4e24\u79cd\u83b7\u53d6\u51fd\u6570\u7ed3\u5408\uff0c\u7406\u8bba\u4fdd\u8bc1\u5176\u6210\u672c\u63a7\u5236\u80fd\u529b\u3002\u5b9e\u9a8c\u8868\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "motivation": "\u5728\u8d1d\u53f6\u65af\u4f18\u5316\u4e2d\uff0c\u5982\u4f55\u5728\u9ad8\u6210\u672c\u7684\u9ed1\u76d2\u51fd\u6570\u8bc4\u4f30\u4e2d\u9002\u65f6\u505c\u6b62\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u6210\u672c\u63a7\u5236\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6210\u672c\u611f\u77e5\u505c\u6b62\u89c4\u5219\uff0c\u7ed3\u5408PBGI\u548clog EI per cost\u83b7\u53d6\u51fd\u6570\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6210\u672c\u63a7\u5236\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u505c\u6b62\u89c4\u5219\u4e0ePBGI\u7ed3\u5408\u65f6\uff0c\u5728\u6210\u672c\u8c03\u6574\u540e\u6094\u5ea6\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d1d\u53f6\u65af\u4f18\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7406\u8bba\u4fdd\u969c\u7684\u505c\u6b62\u7b56\u7565\u3002"}}
