<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 14]
- [cs.LG](#cs.LG) [Total: 90]
- [stat.ML](#stat.ML) [Total: 12]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Newton-Direction-Based ReLU-Thresholding Methods for Nonnegative Sparse Signal Recovery](https://arxiv.org/abs/2602.15880)
*Ning Bian,Zhong-Feng Sun,Yun-Bin Zhao,Jin-Chuan Zhou,Nan Meng*

Main category: eess.SP

TL;DR: 提出两种基于牛顿方向和ReLU的非负稀疏信号恢复算法：NDRT和NDRTP，理论证明在一定条件下能精确恢复，实验显示NDRTP在噪声和无噪声场景下性能优异。


<details>
  <summary>Details</summary>
Motivation: 非负稀疏信号恢复具有广泛应用，现有方法结合ReLU技术可提升恢复算法性能，需要进一步改进现有方法。

Method: 将牛顿型阈值化与ReLU方法结合，提出两种算法：牛顿方向ReLU阈值化(NDRT)及其增强版本牛顿方向ReLU阈值化追踪(NDRTP)。

Result: 理论分析表明当测量矩阵满足特定条件时，两种算法都能保证精确恢复非负稀疏信号。数值实验显示NDRTP在噪声和无噪声场景下相比多种现有方法具有竞争力。

Conclusion: 提出的NDRT和NDRTP算法有效结合了牛顿方向和ReLU技术，为稀疏信号恢复提供了新的高效方法，NDRTP尤其在实际应用中表现优异。

Abstract: Nonnegative sparse signal recovery has been extensively studied due to its broad applications. Recent work has integrated rectified linear unit (ReLU) techniques to enhance existing recovery algorithms. We merge Newton-type thresholding with ReLU-based approaches to propose two algorithms: Newton-Direction-Based ReLU-Thresholding (NDRT) and its enhanced variant, Newton-Direction-Based ReLU-Thresholding Pursuit (NDRTP). Theoretical analysis iindicates that both algorithms can guarantee exact recovery of nonnegative sparse signals when the measurement matrix satisfies a certain condition.. Numerical experiments demonstrate NDRTP achieves competitive performance compared to several existing methods in both noisy and noiseless scenarios.

</details>


### [2] [NeuroSleep: Neuromorphic Event-Driven Single-Channel EEG Sleep Staging for Edge-Efficient Sensing](https://arxiv.org/abs/2602.15888)
*Boyu Li,Xingchun Zhu,Yonghui Wu*

Main category: eess.SP

TL;DR: NeuroSleep是一个用于可穿戴设备上高效睡眠分期的集成事件驱动传感与推理系统，通过神经形态编码和状态感知建模，在保持准确性的同时大幅降低计算负载。


<details>
  <summary>Details</summary>
Motivation: 可穿戴边缘平台上可靠的连续神经传感对于长期健康监测至关重要，但基于EEG的睡眠监测需要密集的高频处理，在严格的能量预算下计算成本过高。

Method: 1. 使用残差自适应多尺度Delta调制(R-AMSDM)将原始EEG转换为互补的多尺度双极事件流；2. 采用分层推理架构，包括事件自适应多尺度响应(EAMR)模块、局部时间注意力模块(LTAM)和周期泄漏积分发放(ELIF)模块。

Result: 在Sleep-EDF Expanded数据集上，NeuroSleep实现了74.2%的平均准确率，仅需0.932M参数，稀疏调整有效操作比密集处理减少约53.6%。与密集Transformer基线相比，准确率提高7.5%，计算负载减少45.8%。

Conclusion: 通过桥接神经形态编码和状态感知建模，NeuroSleep为资源受限的可穿戴场景提供了可扩展的始终在线睡眠分析解决方案。

Abstract: Reliable, continuous neural sensing on wearable edge platforms is fundamental to long-term health monitoring; however, for electroencephalography (EEG)-based sleep monitoring, dense high-frequency processing is often computationally prohibitive under tight energy budgets. To address this bottleneck, this paper proposes NeuroSleep, an integrated event-driven sensing and inference system for energy-efficient sleep staging. NeuroSleep first converts raw EEG into complementary multi-scale bipolar event streams using Residual Adaptive Multi-Scale Delta Modulation (R-AMSDM), enabling an explicit fidelity-sparsity trade-off at the sensing front end. Furthermore, NeuroSleep adopts a hierarchical inference architecture that comprises an Event-based Adaptive Multi-scale Response (EAMR) module for local feature extraction, a Local Temporal-Attention Module (LTAM) for context aggregation, and an Epoch-Leaky Integrate-and-Fire (ELIF) module to capture long-term state persistence. Experimental results using subject-independent 5-fold cross-validation on the Sleep-EDF Expanded dataset demonstrate that NeuroSleep achieves a mean accuracy of 74.2% with only 0.932 M parameters while reducing sparsity-adjusted effective operations by approximately 53.6% relative to dense processing. Compared with the representative dense Transformer baseline, NeuroSleep improves accuracy by 7.5% with a 45.8% reduction in computational load. By bridging neuromorphic encoding with state-aware modeling, NeuroSleep provides a scalable solution for always-on sleep analysis in resource-constrained wearable scenarios.

</details>


### [3] [Advancing Industry 4.0: Multimodal Sensor Fusion for AI-Based Fault Detection in 3D Printing](https://arxiv.org/abs/2602.16108)
*Muhammad Fasih Waheed,Shonda Bernadin,Ali Hassan*

Main category: eess.SP

TL;DR: 开发低成本便携式多模态传感器融合AI系统，用于FDM 3D打印实时故障检测


<details>
  <summary>Details</summary>
Motivation: FDM 3D打印存在喷嘴堵塞、耗材耗尽、层错位等故障，传统检测方法成本高、耗时长且无法实时干预，需要实时监控解决方案

Method: 集成声学、振动和热传感器构建非侵入式架构，将多模态信号处理为频谱图及时频特征，使用卷积神经网络进行智能故障分类

Result: 提出低成本、便携、可扩展的实时监控系统，提高故障检测精度，减少浪费，支持可持续自适应制造

Conclusion: 该系统推进工业4.0目标，为FDM 3D打印提供实用监控解决方案，实现实时干预和过程优化

Abstract: Additive manufacturing, particularly fused deposition modeling, is transforming modern production by enabling rapid prototyping and complex part fabrication. However, its layer-by-layer process remains vulnerable to faults such as nozzle clogging, filament runout, and layer misalignment, which compromise print quality and reliability. Traditional inspection methods are costly, time-intensive, and often limited to post-process analysis, making them unsuitable for real-time intervention. In this current study, the authors developed a novel, low-cost, and portable faultdetection system that leverages multimodal sensor fusion and artificial intelligence for real-time monitoring in FDM-based 3D printing. The system integrates acoustic, vibration, and thermal sensing into a non-intrusive architecture, capturing complementary data streams that reflect both mechanical and process-related anomalies. Acoustic and thermal sensors operate in a fully contactless manner, while the vibration sensor requires minimal attachment such that it will not interfere with printer hardware, thereby preserving portability and ease of deployment. The multimodal signals are processed into spectrograms and time-frequency features, which are classified using convolutional neural networks for intelligent fault detection. The proposed system advances Industry 4.0 objectives by offering an affordable, scalable, and practical monitoring solution that improves faultdetection accuracy, reduces waste, and supports sustainable, adaptive manufacturing.

</details>


### [4] [Real time fault detection in 3D printers using Convolutional Neural Networks and acoustic signals](https://arxiv.org/abs/2602.16118)
*Muhammad Fasih Waheed,Shonda Bernadin*

Main category: eess.SP

TL;DR: 利用卷积神经网络对3D打印过程中的音频信号进行实时分析，实现机械故障的无接触式检测


<details>
  <summary>Details</summary>
Motivation: 传统3D打印故障检测方法依赖视觉检查和硬件传感器，成本高且范围有限，需要更经济、可扩展的实时监控方案

Method: 通过控制实验收集音频数据，应用卷积神经网络进行实时音频分类，检测喷嘴堵塞、线材断裂、滑轮打滑等常见机械故障

Result: 初步结果表明，结合机器学习技术分析音频信号能够提供可靠且经济高效的实时故障检测手段

Conclusion: 音频信号分析为3D打印过程提供了一种可扩展、无接触的实时故障检测方法，相比传统方法更具成本效益

Abstract: The reliability and quality of 3D printing processes are critically dependent on the timely detection of mechanical faults. Traditional monitoring methods often rely on visual inspection and hardware sensors, which can be both costly and limited in scope. This paper explores a scalable and contactless method for the use of real-time audio signal analysis for detecting mechanical faults in 3D printers. By capturing and classifying acoustic emissions during the printing process, we aim to identify common faults such as nozzle clogging, filament breakage, pully skipping and various other mechanical faults. Utilizing Convolutional neural networks, we implement algorithms capable of real-time audio classification to detect these faults promptly. Our methodology involves conducting a series of controlled experiments to gather audio data, followed by the application of advanced machine learning models for fault detection. Additionally, we review existing literature on audio-based fault detection in manufacturing and 3D printing to contextualize our research within the broader field. Preliminary results demonstrate that audio signals, when analyzed with machine learning techniques, provide a reliable and cost-effective means of enhancing real-time fault detection.

</details>


### [5] [In-Situ Analysis of Vibration and Acoustic Data in Additive Manufacturing](https://arxiv.org/abs/2602.16119)
*Muhammad Fasih Waheed,Shonda Bernadin*

Main category: eess.SP

TL;DR: 该研究使用加速度计和声学传感器监测FDM 3D打印机的振动和声音，发现连续运动时振动较低，但在锯齿形运动时因急加速/减速导致振动峰值


<details>
  <summary>Details</summary>
Motivation: 3D打印中的振动会损害制造部件质量并降低输出质量，因此需要分析打印机振动模式以进行故障检测和控制

Method: 使用加速度计和声学传感器测量MakerBot Method X打印机的振动和声音，分别分析传感器输出数据

Result: 连续运动时振动水平相对较低（主要出现在部件过渡边缘），而锯齿形运动时由于急加速和减速导致振动达到峰值

Conclusion: 通过传感器监测可以确定打印机状态，振动分析对于3D打印质量控制至关重要，特别是在运动模式变化时

Abstract: Vibration from an erroneous disturbance harms the manufactured components and lowers the output quality of an FDM printer. For moving machinery, vibration analysis and control are crucial. Additive manufacturing is the basis of 3D printing, which utilizes mechanical movement of the extruder to fabricate objects, and faults occur due to unwanted vibrations. Therefore, it is vital to examine the vibration patterns of a 3D printer. In this work, we observe these parameters of an FDM printer, exemplified by the MakerBot Method X. To analyze the system, it is necessary to understand the motion it generates and select appropriate sensors to detect those motions. The sensor measurement values can be used to determine the condition of the printer. We used an accelerometer and an acoustic sensor to measure the vibration and sound produced by the printer. The outputs from these sensors were examined individually. The findings show that vibration occurs at relatively low levels during continuous motion because it mainly appears at component transition edges. Due to abrupt acceleration and deceleration during zigzag motion, vibration reaches its peak.

</details>


### [6] [Pinching Antennas-Aided Integrated Sensing and Multicast Communication Systems](https://arxiv.org/abs/2602.16244)
*Shan Shan,Chongjun Ouyang,Xiaohang Yang,Yong Li,Zhiqin Wang,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出基于可移动天线（PAs）的集成感知与组播通信框架，优化天线布局以平衡通信和感知性能


<details>
  <summary>Details</summary>
Motivation: 传统固定天线系统在集成感知与通信（ISAC）中性能受限，需要探索可移动天线布局优化来同时提升组播通信速率和感知精度

Method: 提出三种设计准则：通信中心（C-C）、感知中心（S-C）和帕累托最优设计；针对单PA情况推导闭式解，针对多PA情况提出基于元素交替优化的方法，并结合增广拉格朗日框架和速率剖面公式求解

Result: PAs系统显著优于固定天线基线，组播增益随用户密度增加而增强，感知精度随部署PA数量增加而提高

Conclusion: 可移动天线布局优化能有效提升集成感知与组播通信系统的性能，为未来ISAC系统设计提供了新思路

Abstract: A pinching antennas (PAs)-aided integrated sensing and multicast communication framework is proposed. In this framework, the communication performance is measured by the multicast rate considering max-min fairness. Moreover, the sensing performance is quantified by the Bayesian Cramér-Rao bound (BCRB), where a Gauss-Hermite quadrature-based approach is proposed to compute the Bayesian Fisher information matrix. Based on these metrics, PA placement is optimized under three criteria: communications-centric (C-C), sensing-centric (S-C), and Pareto-optimal designs. These designs are investigated in two scenarios: the single-PA case and the multi-PA case. 1) For the single-PA case, a closed-form solution is derived for the location of the C-C transmit PA, while the S-C design yields optimal transmit and receive PA placements that are symmetric about the target location. Leveraging this geometric insight, the Pareto-optimal design is solved by enforcing this PA placement symmetry, thereby reducing the joint transmit and receive PA placement to the transmit PA optimization. 2) For the general multi-PA case, the PA placements constitute a highly non-convex optimization problem. To solve this, an element-wise alternating optimization-based method is proposed to sequentially optimize all PA placements for the S-C design, and is further incorporated into an augmented Lagrangian (AL) framework and a rate-profile formulation to solve the C-C and Pareto-optimal design problems, respectively. Numerical results show that: i) PASS substantially outperforms fixed-antenna baselines in both multicast rate and sensing accuracy; ii) the multicasting gain becomes more pronounced as the user density increases; and iii) the sensing accuracy improves with the number of deployed PAs.

</details>


### [7] [SeaSpoofFinder -- Potential GNSS Spoofing Event Detection Using AIS](https://arxiv.org/abs/2602.16257)
*Jón Winkel,Tom Willems,Cillian O'Driscoll,Ignacio Fernandez-Hernandez*

Main category: eess.SP

TL;DR: 开发SeaSpoofFinder框架，通过处理全球AIS数据检测GNSS欺骗活动，使用两阶段方法识别异常位置跳跃和空间聚类模式，发现多个区域存在潜在欺骗事件。


<details>
  <summary>Details</summary>
Motivation: 研究是否可以通过海事自动识别系统(AIS)位置报告推断大规模GNSS欺骗活动，为大规模监测和识别潜在欺骗行为提供方法。

Method: 开发SeaSpoofFinder数据处理框架，采用两阶段检测方法：第一阶段使用运动学和数据质量过滤器识别不合理的位置跳跃；第二阶段通过多船舶空间一致性聚类减少误报。

Result: 在波罗的海、黑海、摩尔曼斯克、莫斯科和海法地区发现重复出现的潜在欺骗事件模式，受影响区域可覆盖大面积海域，同时识别出非欺骗性伪影。

Conclusion: AIS监测可为大规模识别和表征潜在欺骗活动提供有用证据，但仅凭AIS证据无法提供确定性归因，需要结合其他数据源进行验证。

Abstract: This paper investigates whether large-scale GNSS spoofing activity can be inferred from maritime Automatic Identification System (AIS) position reports. A data-processing framework, called SeaSpoofFinder, available here: seaspooffinder.github.io/ais_data, was developed to ingest and post-process global AIS streams and to detect candidate anomalies through a two-stage procedure. In Stage 1, implausible position jumps are identified using kinematic and data-quality filters; in Stage 2, events are retained only when multiple vessels exhibit spatially consistent source and target clustering, thereby reducing false positives from single-vessel artifacts. The resulting final potential spoofing events (FPSEs) reveal recurrent patterns in several regions, including the Baltic Sea, the Black Sea, Murmansk, Moscow, and the Haifa area, with affected footprints that can span large maritime areas. The analysis also highlights recurring non-spoofing artifacts (e.g., back-to-port jumps and data gaps) that can still pass heuristic filters in dense traffic regions. These results indicate that AIS-based monitoring can provide useful evidence for identifying and characterizing potential spoofing activity at scale, while emphasizing that AIS-only evidence does not provide definitive attribution.

</details>


### [8] [Impact of Preprocessing on Neural Network-Based RSS/AoA Positioning](https://arxiv.org/abs/2602.16271)
*Omid Abbassi Aghda,Slavisa Tomic,Oussama Ben Haj Belkacem,Joao Guerreiro,Nuno Souto,Michal Szczachor,Rui Dinis*

Main category: eess.SP

TL;DR: 提出基于多层感知器的神经网络方法，直接将RSS-AoA测量映射到3D位置，优于传统线性方法


<details>
  <summary>Details</summary>
Motivation: 混合RSS-AoA定位虽然成本低且角度分辨率高，但存在固有非线性、几何相关噪声和传统线性估计器次优加权等问题，限制了精度

Method: 使用多层感知器神经网络直接映射RSS-AoA测量到3D位置，比较原始测量与基于线性化方法预处理特征两种输入表示

Result: 学习型方法在所有噪声水平下均优于现有线性方法，在高AoA噪声下达到或超越最先进性能；预处理测量比原始数据有明显优势

Conclusion: 神经网络方法能有效捕获传统方法难以建模的非线性关系，几何感知的特征提取对性能提升有显著帮助

Abstract: Hybrid received signal strength (RSS)-angle of arrival (AoA)-based positioning offers low-cost distance estimation and high-resolution angular measurements. Still, it comes at a cost of inherent nonlinearities, geometry-dependent noise, and suboptimal weighting in conventional linear estimators that might limit accuracy. In this paper, we propose a neural network-based approach using a multilayer perceptron (MLP) to directly map RSS-AoA measurements to 3D positions, capturing nonlinear relationships that are difficult to model with traditional methods. We evaluate the impact of input representation by comparing networks trained on raw measurements versus preprocessed features derived from a linearization method. Simulation results show that the learning-based approach consistently outperforms existing linear methods under RSS noise across all noise levels, and matches or surpasses state-of-the-art performance under increasing AoA noise. Furthermore, preprocessing measurements using the linearization method provides a clear advantage over raw data, demonstrating the benefit of geometry-aware feature extraction.

</details>


### [9] [Joint beamforming and mode optimization for multi-functional STAR-RIS-aided integrated sensing and communication networks](https://arxiv.org/abs/2602.16383)
*Ziming Liu,Tao Chen,Giacinto Gelli,Vincenzo Galdi,Francesco Verde*

Main category: eess.SP

TL;DR: 本文提出了一种基于STAR-RIS辅助的集成感知与通信系统设计，采用两阶段协议实现方向估计和通信增强，通过联合优化基站波束成形、STAR-RIS系数和模式划分，在保证感知精度的同时最大化通信和速率。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信系统的发展，集成感知与通信(ISAC)成为6G关键技术之一。传统RIS只能反射信号，而STAR-RIS能同时透射和反射信号，为室内外用户提供更灵活的覆盖。然而，如何在STAR-RIS辅助下平衡感知精度和通信性能仍是一个挑战。

Method: 提出两阶段ISAC协议：准备阶段在反射空间进行室外用户方向估计，同时在透射空间维持室内外用户通信；通信阶段利用估计方向增强信息传输。将方向建模为高斯随机变量以捕捉估计不确定性，并设计联合优化问题，同时优化基站波束成形向量、STAR-RIS透反射系数、以及能量分割和仅传输模式的表面划分。采用分数规划、拉格朗日对偶重构和逐次凸逼近解决非凸问题，通过连续松弛和投影二值化恢复二进制划分。

Result: 数值结果表明，所提设计在感知精度和通信吞吐量之间实现了有效权衡，显著优于传统的STAR-RIS辅助ISAC方案。系统能够同时保证所需的感知性能并最大化通信和速率。

Conclusion: STAR-RIS辅助的ISAC系统通过两阶段协议和联合优化设计，能够有效平衡感知和通信性能。所提方法为未来6G网络中多功能可编程超表面的应用提供了有前景的解决方案，实现了室内外用户的协同服务。

Abstract: This paper investigates the design of integrated sensing and communication (ISAC) systems assisted by simultaneously transmitting and reflecting reconfigurable intelligent surfaces (STAR-RISs), which act as multi-functional programmable metasurfaces capable of supporting concurrent communication and sensing within a unified architecture. We propose a two-stage ISAC protocol, in which the preparation phase performs direction estimation for outdoor users located in the reflection space, while maintaining communication with both outdoor and indoor users in the transmission space. The subsequent communication phase exploits the estimated directions to enhance information transfer. The directions of outdoor users are modeled as Gaussian random variables to capture estimation uncertainty, and the corresponding average communication performance is incorporated into the design. Building on this framework, we formulate a performance-balanced optimization problem that maximizes the communication sum-rate while guaranteeing the required sensing accuracy, jointly determining the beamforming vectors at the base station (BS), the STAR-RIS transmission and reflection coefficients, and the metasurface partition between energy-splitting and transmit-only modes. The physical constraints of STAR-RIS elements and the required sensing performance are explicitly enforced. To address the non-convex nature of the problem, we combine fractional programming, Lagrangian dual reformulation, and successive convex approximation. The binary metasurface partition is ultimately recovered via continuous relaxation followed by projection-based binarization. Numerical results demonstrate that the proposed design achieves an effective trade-off between sensing accuracy and communication throughput, by significantly outperforming conventional STAR-RIS-aided ISAC schemes.

</details>


### [10] [Reconstruction of Piecewise-Constant Sparse Signals for Modulo Sampling](https://arxiv.org/abs/2602.16418)
*Haruka Kobayashi,Ryo Hayakawa*

Main category: eess.SP

TL;DR: 提出一种直接重构残差信号的模数采样算法，避免传统方法中误差传播问题


<details>
  <summary>Details</summary>
Motivation: 传统模数采样方法通过估计残差信号的差值并计算累积和来重构原始信号，但每个估计误差都会在后续时间样本中传播，需要解决这个误差传播问题

Method: 利用模数样本的高频特性和残差信号及其差值的稀疏性，直接重构残差信号，而不是通过估计差值再累积的方式

Result: 仿真结果显示，与基于残差信号差值的传统方法相比，所提方法能更准确地重构原始信号

Conclusion: 通过直接重构残差信号而非间接估计差值，有效消除了误差传播问题，提高了模数采样信号重构的准确性

Abstract: Modulo sampling is a promising technology to preserve amplitude information that exceeds the observable range of analog-to-digital converters during the digitization of analog signals. Since conventional methods typically reconstruct the original signal by estimating the differences of the residual signal and computing their cumulative sum, each estimation error inevitably propagates through subsequent time samples. In this paper, to eliminate this error-propagation problem, we propose an algorithm that reconstructs the residual signal directly. The proposed method takes advantage of the high-frequency characteristics of the modulo samples and the sparsity of both the residual signal and its difference. Simulation results show that the proposed method reconstructs the original signal more accurately than a conventional method based on the differences of the residual signal.

</details>


### [11] [Proof of Concept: Local TX Real-Time Phase Calibration in MIMO Systems](https://arxiv.org/abs/2602.16441)
*Carl Collmann,Ahmad Nimr,Gerhard Fettweis*

Main category: eess.SP

TL;DR: 提出并验证了一种用于数字阵列的简单本地实时相位校准方法，比较了瞬时和平滑两种校准方法，实现了2.1 ps到124 fs范围内的抖动精度。


<details>
  <summary>Details</summary>
Motivation: MIMO系统中的信道测量依赖于精确同步。虽然时间和频率同步方法已经成熟，但保持实时相位相干性仍然是许多MIMO系统的开放需求。相位相干性对于数字阵列中的波束成形至关重要，并支持精确的参数估计，如到达/离开角。

Method: 提出并验证了一种简单的本地实时相位校准方法，比较了两种不同方法：瞬时校准和平滑校准，以确定同步程序之间的最佳间隔。使用两个指标定量评估校准性能：平均波束成形功率损失和RMS周期到周期抖动。

Result: 结果表明两种相位校准方法都有效，对于不同的SDR模型，抖动RMS在2.1 ps到124 fs范围内。这种精度水平使得在常见的SDR平台上实现相干传输成为可能。

Conclusion: 该相位校准方法能够在实际测试平台上实现高级MIMO技术和发射波束成形的研究，为数字阵列提供了有效的实时相位同步解决方案。

Abstract: Channel measurements in MIMO systems hinge on precise synchronization. While methods for time and frequency synchronization are well established, maintaining real-time phase coherence remains an open requirement for many MIMO systems. Phase coherence in MIMO systems is crucial for beamforming in digital arrays and enables precise parameter estimates such as Angle-of-Arrival/Departure. This work presents and validates a simple local real-time phase calibration method for a digital array. We compare two different approaches, instantaneous and smoothed calibration, to determine the optimal interval between synchronization procedures. To quantitatively assess calibration performance, we use two metrics: the average beamforming power loss and the RMS cycle-to-cycle jitter. Our results indicate that both approaches for phase calibration are effective and yield RMS of jitter in the 2.1 ps to 124 fs range for different SDR models. This level of precision enables coherent transmission on commonly available SDR platforms, allowing investigation on advanced MIMO techniques and transmit beamforming in practical testbeds.

</details>


### [12] [Failure-Aware Access Point Selection for Resilient Cell-Free Massive MIMO Networks](https://arxiv.org/abs/2602.16546)
*Mostafa Rahmani Ghourtani,Junbo Zhao,Yi Chu,Hamed Ahmadi,David Grace,Alister G. Burr*

Main category: eess.SP

TL;DR: FAAS方法通过联合考虑信道强度和AP故障概率来选择接入点，在故障条件下显著提升CF-mMIMO网络的硬件弹性，减少85%以上的中断概率并改善最差用户速率。


<details>
  <summary>Details</summary>
Motivation: 提升无小区大规模MIMO网络的硬件弹性，传统方法在AP故障时性能下降严重，需要一种能考虑故障概率的AP选择方法来增强网络可靠性。

Method: 提出故障感知接入点选择方法，为每个用户选择AP时联合考虑信道强度和每个AP的故障概率，使用可调参数α缩放故障概率以模拟不同网络压力水平。

Result: 在故障条件下，FAAS相比故障无关聚类方法性能显著更好，在高故障水平下减少85%以上的中断概率，并改善最差用户速率。

Conclusion: FAAS是构建更可靠CF-mMIMO网络的实用高效解决方案，通过考虑故障概率的AP选择能显著提升网络弹性。

Abstract: This paper presents a Failure-Aware Access Point Selection (FAAS) method aimed at improving hardware resilience in cell-free massive MIMO (CF-mMIMO) networks. FAAS selects APs for each user by jointly considering channel strength and the failure probability of each AP. A tunable parameter \(α\in [0,1]\) scales these failure probabilities to model different levels of network stress. We evaluate resilience using two key metrics: the minimum-user spectral efficiency, which captures worst-case user performance, and the outage probability, defined as the fraction of users left without any active APs. Simulation results show that FAAS maintains significantly better performance under failure conditions compared to failure-agnostic clustering. At high failure levels, FAAS reduces outage by over 85\% and improves worst-case user rates. These results confirm that FAAS is a practical and efficient solution for building more reliable CF-mMIMO networks.

</details>


### [13] [WindDensity-MBIR: Model-Based Iterative Reconstruction for Wind Tunnel 3D Density Estimation](https://arxiv.org/abs/2602.16621)
*Karl J. Weisenburger,Gregery T. Buzzard,Charles A. Bouman,Matthew R. Kemnetz*

Main category: eess.SP

TL;DR: 提出WindDensity-MBIR方法，基于贝叶斯稀疏视角层析重建，用于风洞中非侵入式3D密度场测量，在数据稀疏、小投影视场和有限角度范围等困难场景下仍能恢复高阶特征。


<details>
  <summary>Details</summary>
Motivation: 现有风洞成像技术难以进行非侵入式3D密度测量，而现有波前层析方法需要强假设（如样条基表示）来处理病态问题，需要更鲁棒的方法。

Method: 将问题建模为贝叶斯稀疏视角层析重建问题，开发基于模型的迭代重建算法(WindDensity-MBIR)，使用多视角波前测量重建3D密度场。

Result: 在模拟数据测试中，即使在波前测量去除倾斜和活塞分量后，WindDensity-MBIR在数据稀疏、小投影视场和有限角度范围等困难场景下仍能以10%-25%误差恢复高阶特征。

Conclusion: WindDensity-MBIR为风洞湍流研究提供了一种有效的非侵入式3D密度测量方法，能够处理传统方法难以应对的稀疏数据场景。

Abstract: Experimentalists often use wind tunnels to study aerodynamic turbulence, but most wind tunnel imaging techniques are limited in their ability to take non-invasive 3D density measurements of turbulence. Wavefront tomography is a technique that uses multiple wavefront measurements from various viewing angles to non-invasively measure the 3D density field of a turbulent medium. Existing methods make strong assumptions, such as a spline basis representation, to address the ill-conditioned nature of this problem. We formulate this problem as a Bayesian, sparse-view tomographic reconstruction problem and develop a model-based iterative reconstruction algorithm for measuring the volumetric 3D density field inside a wind tunnel. We call this method WindDensity-MBIR and apply it using simulated data to difficult reconstruction scenarios with sparse data, small projection field of view, and limited angular extent. WindDensity-MBIR can recover high-order features in these scenarios within 10% to 25% error even when the tip, tilt, and piston are removed from the wavefront measurements.

</details>


### [14] [Active RIS-Assisted MIMO System for Vital Signs Extraction: ISAC Modeling, Deep Learning, and Prototype Measurements](https://arxiv.org/abs/2602.16637)
*De-Ming Chian,Chao-Kai Wen,Feng-Ji Chen,Yi-Jie Sun,Fu-Kang Wang*

Main category: eess.SP

TL;DR: RIS-VSign系统：基于主动可重构智能表面的MIMO-OFDM框架，用于集成感知与通信场景下的生命体征提取，通过DMTNet深度学习框架配置RIS相位，实验验证可提高呼吸检测可靠性并支持高阶调制。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信（ISAC）场景下，传统无线系统在同时进行呼吸检测和通信时面临挑战：无RIS时呼吸检测不可靠且仅支持低阶调制。需要开发一种能同时提升感知性能和通信能力的框架。

Method: 系统分为两阶段：1) RIS相位选择器：集成Möbius变换差分的深度学习框架DMTNet，联合配置多个主动RIS单元，仅使用仿真数据训练；2) 呼吸率提取：多天线测量通过DC偏移校准和DeepMining-MMV处理，结合CA-CFAR检测和牛顿法优化。

Result: 原型实验表明：主动RIS部署显著提高呼吸可检测性，同时支持高阶调制；无RIS时呼吸检测不可靠且仅支持低阶调制。相位选择器通过仿真训练并在实验中验证有效。

Conclusion: RIS-VSign系统成功实现了ISAC场景下的生命体征提取，主动RIS能同时提升感知性能和通信能力，DMTNet框架有效解决了同步引起的相位漂移问题，为无线健康监测提供了新方案。

Abstract: We present the RIS-VSign system, an active reconfigurable intelligent surface (RIS)-assisted multiple-input multiple-output orthogonal frequency division multiplexing (MIMO-OFDM) framework for vital signs extraction under an integrated sensing and communication (ISAC) model. The system consists of two stages: the phase selector of RIS and the extraction of respiration rate. To mitigate synchronization-induced common phase drifts, the difference of Möbius transformation (DMT) is integrated into the deep learning framework, named DMTNet, to jointly configure multiple active RIS elements. Notably, the training data are generated in simulation without collecting real-world measurements, and the resulting phase selector is validated experimentally. For sensing, multi-antenna measurements are fused by the DC-offset calibration and the DeepMining-MMV processing with CA-CFAR detection and Newton's refinements. Prototype experiments indicate that active RIS deployment improves respiration detectability while simultaneously enabling higher-order modulation; without RIS, respiration detection is unreliable and only lower-order modulation is supported.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [15] [A Koopman-Bayesian Framework for High-Fidelity, Perceptually Optimized Haptic Surgical Simulation](https://arxiv.org/abs/2602.15834)
*Rohit Kaushik,Eva Kaushik*

Main category: cs.LG

TL;DR: 提出一个结合非线性动力学、感知心理物理学和高频触觉渲染的统一框架，用于增强手术模拟的真实感，通过Koopman算子实现线性预测控制，贝叶斯校准模块优化力信号，在多种手术任务中实现低延迟、高精度和感知改进。


<details>
  <summary>Details</summary>
Motivation: 传统手术模拟系统在触觉渲染方面存在局限性，无法同时处理非线性组织动力学和人类感知限制，需要一种统一框架来提升手术训练的真实感和效果。

Method: 1. 使用Koopman算子将非线性组织动力学提升到增广状态空间，实现线性预测和控制；2. 基于Weber-Fechner和Stevens标度定律的贝叶斯校准模块，根据个体感知阈值调整力信号；3. 高频触觉渲染架构支持多种手术任务（触诊、切口、骨磨削）。

Result: 系统平均渲染延迟4.3ms，力误差小于2.8%，感知辨别能力提升20%。多元统计分析（MANOVA和回归）显示系统性能显著优于传统的弹簧阻尼器和基于能量的渲染方法。

Conclusion: 该框架显著提升了手术模拟的真实感和训练效果，对手术培训和VR医学教育具有重要影响，未来可向闭环神经反馈触觉接口方向发展。

Abstract: We introduce a unified framework that combines nonlinear dynamics, perceptual psychophysics and high frequency haptic rendering to enhance realism in surgical simulation. The interaction of the surgical device with soft tissue is elevated to an augmented state space with a Koopman operator formulation, allowing linear prediction and control of the dynamics that are nonlinear by nature. To make the rendered forces consistent with human perceptual limits, we put forward a Bayesian calibration module based on WeberFechner and Stevens scaling laws, which progressively shape force signals relative to each individual's discrimination thresholds. For various simulated surgical tasks such as palpation, incision, and bone milling, the proposed system attains an average rendering latency of 4.3 ms, a force error of less than 2.8% and a 20% improvement in perceptual discrimination. Multivariate statistical analyses (MANOVA and regression) reveal that the system's performance is significantly better than that of conventional spring-damper and energy, based rendering methods. We end by discussing the potential impact on surgical training and VR, based medical education, as well as sketching future work toward closed, loop neural feedback in haptic interfaces.

</details>


### [16] [Memes-as-Replies: Can Models Select Humorous Manga Panel Responses?](https://arxiv.org/abs/2602.15842)
*Ryosuke Kohita,Seiichiro Yoshioka*

Main category: cs.LG

TL;DR: 论文提出了Meme Reply Selection任务和MaMe-Re基准，包含10万个人工标注的漫画面板与社交媒体帖子配对，用于研究表情包在对话中创造幽默的动态使用。研究发现LLMs能捕捉复杂社交线索但视觉信息无帮助，选择情境幽默回复仍是当前模型的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有计算研究主要关注表情包的内在属性，而表情包在对话中动态、情境化地创造幽默的使用方式仍是网络科学中研究不足的领域。需要填补这一空白，研究表情包如何作为互动回复在对话中创造幽默。

Method: 引入Meme Reply Selection任务，创建MaMe-Re基准数据集，包含10万个人工标注配对（来自2,325名标注者的50万条标注），使用开放许可的日本漫画面板和社交媒体帖子。分析大型语言模型在捕捉复杂社交线索和视觉信息利用方面的表现。

Result: 1) LLMs初步显示出捕捉夸张等复杂社交线索的能力，超越表面语义匹配；2) 视觉信息的加入并未提升性能，揭示理解视觉内容与有效利用其创造情境幽默之间存在差距；3) LLMs在受控环境中能匹配人类判断，但难以区分语义相似候选回复中微妙的机智差异。

Conclusion: 选择情境幽默回复对当前模型仍是开放挑战。虽然LLMs能捕捉复杂社交线索，但视觉信息利用不足且难以区分微妙幽默差异，表明需要更深入理解表情包在对话中的动态使用。

Abstract: Memes are a popular element of modern web communication, used not only as static artifacts but also as interactive replies within conversations. While computational research has focused on analyzing the intrinsic properties of memes, the dynamic and contextual use of memes to create humor remains an understudied area of web science. To address this gap, we introduce the Meme Reply Selection task and present MaMe-Re (Manga Meme Reply Benchmark), a benchmark of 100,000 human-annotated pairs (500,000 total annotations from 2,325 unique annotators) consisting of openly licensed Japanese manga panels and social media posts. Our analysis reveals three key insights: (1) large language models (LLMs) show preliminary evidence of capturing complex social cues such as exaggeration, moving beyond surface-level semantic matching; (2) the inclusion of visual information does not improve performance, revealing a gap between understanding visual content and effectively using it for contextual humor; (3) while LLMs can match human judgments in controlled settings, they struggle to distinguish subtle differences in wit among semantically similar candidates. These findings suggest that selecting contextually humorous replies remains an open challenge for current models.

</details>


### [17] [Kalman-Inspired Runtime Stability and Recovery in Hybrid Reasoning Systems](https://arxiv.org/abs/2602.15855)
*Barak Or*

Main category: cs.LG

TL;DR: 该论文从卡尔曼滤波角度研究混合推理系统的运行时稳定性，提出认知漂移作为可测量现象，并开发了基于创新信号统计的稳定性监测与恢复框架。


<details>
  <summary>Details</summary>
Motivation: 当前结合学习组件和模型推理的混合系统在工具增强决策循环中广泛应用，但其在部分可观测和持续证据不匹配下的运行时行为缺乏理解。实践中故障常表现为内部推理动态的逐渐发散而非孤立预测错误。

Method: 从卡尔曼滤波视角将推理建模为随机推理过程，引入认知漂移作为可测量运行时现象。提出基于创新信号统计的运行时稳定性框架，监测创新统计量、检测新兴不稳定性，并触发恢复感知控制机制。

Result: 在多步工具增强推理任务实验中，该框架能在任务失败前可靠检测不稳定性，并在可行时通过恢复机制在有限时间内重建有界内部行为。

Conclusion: 运行时稳定性应作为不确定性下可靠推理的系统级要求，而非仅关注任务级正确性。该研究为混合推理系统的鲁棒性提供了新视角和实用框架。

Abstract: Hybrid reasoning systems that combine learned components with model-based inference are increasingly deployed in tool-augmented decision loops, yet their runtime behavior under partial observability and sustained evidence mismatch remains poorly understood. In practice, failures often arise as gradual divergence of internal reasoning dynamics rather than as isolated prediction errors. This work studies runtime stability in hybrid reasoning systems from a Kalman-inspired perspective. We model reasoning as a stochastic inference process driven by an internal innovation signal and introduce cognitive drift as a measurable runtime phenomenon. Stability is defined in terms of detectability, bounded divergence, and recoverability rather than task-level correctness. We propose a runtime stability framework that monitors innovation statistics, detects emerging instability, and triggers recovery-aware control mechanisms. Experiments on multi-step, tool-augmented reasoning tasks demonstrate reliable instability detection prior to task failure and show that recovery, when feasible, re-establishes bounded internal behavior within finite time. These results emphasize runtime stability as a system-level requirement for reliable reasoning under uncertainty.

</details>


### [18] [Genetic Generalized Additive Models](https://arxiv.org/abs/2602.15877)
*Kaaustaaub Shankar,Kelly Cohen*

Main category: cs.LG

TL;DR: 使用多目标遗传算法NSGA-II自动优化广义可加模型，在预测误差和模型复杂度之间取得平衡，提高模型性能的同时增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 广义可加模型在预测准确性和可解释性之间取得了良好平衡，但手动配置其结构具有挑战性。需要一种自动化方法来优化GAMs，同时考虑预测性能和模型复杂度。

Method: 使用多目标遗传算法NSGA-II自动优化GAMs，联合最小化预测误差（RMSE）和复杂度惩罚项（包含稀疏性、平滑性和不确定性）。

Result: 在加州住房数据集上的实验表明，NSGA-II发现的GAMs在准确性上优于基线LinearGAMs，或以显著更低的复杂度匹配其性能。生成的模型更简单、更平滑，置信区间更窄，增强了可解释性。

Conclusion: 该框架为自动化优化透明、高性能模型提供了一种通用方法，能够自动发现平衡预测准确性和可解释性的GAMs结构。

Abstract: Generalized Additive Models (GAMs) balance predictive accuracy and interpretability, but manually configuring their structure is challenging. We propose using the multi-objective genetic algorithm NSGA-II to automatically optimize GAMs, jointly minimizing prediction error (RMSE) and a Complexity Penalty that captures sparsity, smoothness, and uncertainty. Experiments on the California Housing dataset show that NSGA-II discovers GAMs that outperform baseline LinearGAMs in accuracy or match performance with substantially lower complexity. The resulting models are simpler, smoother, and exhibit narrower confidence intervals, enhancing interpretability. This framework provides a general approach for automated optimization of transparent, high-performing models. The code can be found at https://github.com/KaaustaaubShankar/GeneticAdditiveModels.

</details>


### [19] [IT-OSE: Exploring Optimal Sample Size for Industrial Data Augmentation](https://arxiv.org/abs/2602.15878)
*Mingchun Sun,Rongqiang Zhao,Zhennan Huang,Songyu Ding,Jie Liu*

Main category: cs.LG

TL;DR: 提出信息理论最优样本量估计(IT-OSE)方法，为工业数据增强提供可靠的最优样本量估计，并设计区间覆盖偏差(ICD)评分来评估估计准确性。


<details>
  <summary>Details</summary>
Motivation: 工业场景中数据增强能提升模型性能，但缺乏最优样本量的理论研究和评估方法，现有经验估计不够可靠，需要系统性的理论框架。

Method: 基于信息理论提出IT-OSE方法估计最优样本量，设计ICD评分评估估计准确性，理论分析OSS与主导因素的关系以增强可解释性。

Result: 相比经验估计，分类任务准确率平均提升4.38%，回归任务MAPE平均降低18.80%；相比穷举搜索，计算成本降低83.97%，数据成本降低93.46%；ICD评分偏差平均降低49.30%。

Conclusion: IT-OSE方法能有效估计工业数据增强的最优样本量，显著提升下游模型性能稳定性，降低计算和数据成本，在典型传感器工业场景中具有通用性。

Abstract: In industrial scenarios, data augmentation is an effective approach to improve model performance. However, its benefits are not unidirectionally beneficial. There is no theoretical research or established estimation for the optimal sample size (OSS) in augmentation, nor is there an established metric to evaluate the accuracy of OSS or its deviation from the ground truth. To address these issues, we propose an information-theoretic optimal sample size estimation (IT-OSE) to provide reliable OSS estimation for industrial data augmentation. An interval coverage and deviation (ICD) score is proposed to evaluate the estimated OSS intuitively. The relationship between OSS and dominant factors is theoretically analyzed and formulated, thereby enhancing the interpretability. Experiments show that, compared to empirical estimation, the IT-OSE increases accuracy in classification tasks across baseline models by an average of 4.38%, and reduces MAPE in regression tasks across baseline models by an average of 18.80%. The improvements in downstream model performance are more stable. ICDdev in the ICD score is also reduced by an average of 49.30%. The determinism of OSS is enhanced. Compared to exhaustive search, the IT-OSE achieves the same OSS while reducing computational and data costs by an average of 83.97% and 93.46%. Furthermore, practicality experiments demonstrate that the IT-OSE exhibits generality across representative sensor-based industrial scenarios.

</details>


### [20] [BamaER: A Behavior-Aware Memory-Augmented Model for Exercise Recommendation](https://arxiv.org/abs/2602.15879)
*Qing Yang,Yuhao Jiang,Rui Wang,Jipeng Guo,Yejiang Wang,Xinghe Cheng,Zezheng Wu,Jiapu Wang,Jingwei Zhang*

Main category: cs.LG

TL;DR: BamaER是一个行为感知的记忆增强型练习推荐框架，通过三向混合编码捕获学生交互行为，使用动态记忆矩阵建模知识状态，并通过河马优化算法进行多样性感知的候选练习筛选。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将学生学习表示为练习序列，忽略了丰富的交互行为信息，导致学习进度估计存在偏差。同时，固定长度的序列分割限制了早期学习经验的纳入，阻碍了长期依赖建模和知识掌握程度的准确估计。

Method: BamaER包含三个核心模块：1) 学习进度预测模块，通过三向混合编码方案捕获异构学生交互行为；2) 记忆增强知识追踪模块，维护动态记忆矩阵联合建模历史和当前知识状态；3) 练习筛选模块，将候选选择建模为多样性感知优化问题，使用河马优化算法减少冗余并提高推荐覆盖率。

Result: 在五个真实世界教育数据集上的实验表明，BamaER在一系列评估指标上始终优于最先进的基线方法。

Conclusion: BamaER通过整合行为感知和记忆增强机制，有效解决了现有练习推荐方法的局限性，能够更准确地估计学习进度和知识掌握程度，提供更个性化的练习推荐。

Abstract: Exercise recommendation focuses on personalized exercise selection conditioned on students' learning history, personal interests, and other individualized characteristics. Despite notable progress, most existing methods represent student learning solely as exercise sequences, overlooking rich behavioral interaction information. This limited representation often leads to biased and unreliable estimates of learning progress. Moreover, fixed-length sequence segmentation limits the incorporation of early learning experiences, thereby hindering the modeling of long-term dependencies and the accurate estimation of knowledge mastery. To address these limitations, we propose BamaER, a Behavior-aware memory-augmented Exercise Recommendation framework that comprises three core modules: (i) the learning progress prediction module that captures heterogeneous student interaction behaviors via a tri-directional hybrid encoding scheme; (ii) the memory-augmented knowledge tracing module that maintains a dynamic memory matrix to jointly model historical and current knowledge states for robust mastery estimation; and (iii) the exercise filtering module that formulates candidate selection as a diversity-aware optimization problem, solved via the Hippopotamus Optimization Algorithm to reduce redundancy and improve recommendation coverage. Experiments on five real-world educational datasets show that BamaER consistently outperforms state-of-the-art baselines across a range of evaluation metrics.

</details>


### [21] [Distributed physics-informed neural networks via domain decomposition for fast flow reconstruction](https://arxiv.org/abs/2602.15883)
*Yixiao Qian,Jiaxu Liu,Zewei Xia,Song Chen,Chao Xu,Shengze Cai*

Main category: cs.LG

TL;DR: 提出一个分布式PINNs框架，通过时空域分解实现高效流场重建，解决了压力不确定性问题，并利用CUDA图加速训练。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在大规模时空域流场重建中存在计算瓶颈和优化不稳定问题，需要可扩展的分布式解决方案。

Method: 采用时空域分解的分布式PINNs框架，引入参考锚点归一化策略和去耦非对称加权，通过从主节点到相邻节点的单向信息流消除压力不确定性，并使用CUDA图和JIT编译加速训练。

Result: 在复杂流场基准测试中实现了接近线性的强扩展性和高保真重建，为流场重建提供了可扩展且物理严谨的途径。

Conclusion: 提出的分布式PINNs框架成功解决了大规模流场重建中的压力不确定性和计算效率问题，为复杂流体动力学的理解和重建提供了有效工具。

Abstract: Physics-Informed Neural Networks (PINNs) offer a powerful paradigm for flow reconstruction, seamlessly integrating sparse velocity measurements with the governing Navier-Stokes equations to recover complete velocity and latent pressure fields. However, scaling such models to large spatiotemporal domains is hindered by computational bottlenecks and optimization instabilities. In this work, we propose a robust distributed PINNs framework designed for efficient flow reconstruction via spatiotemporal domain decomposition. A critical challenge in such distributed solvers is pressure indeterminacy, where independent sub-networks drift into inconsistent local pressure baselines. We address this issue through a reference anchor normalization strategy coupled with decoupled asymmetric weighting. By enforcing a unidirectional information flow from designated master ranks where the anchor point lies to neighboring ranks, our approach eliminates gauge freedom and guarantees global pressure uniqueness while preserving temporal continuity. Furthermore, to mitigate the Python interpreter overhead associated with computing high-order physics residuals, we implement a high-performance training pipeline accelerated by CUDA graphs and JIT compilation. Extensive validation on complex flow benchmarks demonstrates that our method achieves near-linear strong scaling and high-fidelity reconstruction, establishing a scalable and physically rigorous pathway for flow reconstruction and understanding of complex hydrodynamics.

</details>


### [22] [Adaptive Semi-Supervised Training of P300 ERP-BCI Speller System with Minimum Calibration Effort](https://arxiv.org/abs/2602.15955)
*Shumeng Chen,Jane E. Huggins,Tianwen Ma*

Main category: cs.LG

TL;DR: 提出基于自适应半监督EM-GMM算法的P300 BCI拼写器框架，减少校准数据需求，提高拼写效率


<details>
  <summary>Details</summary>
Motivation: 传统P300 BCI拼写器需要大量校准数据构建二元分类器，降低了整体效率，需要减少校准工作量

Method: 提出统一框架，使用少量标注校准数据，采用自适应半监督EM-GMM算法更新二元分类器

Result: 15名参与者中，9人达到0.7字符级准确率，其中7人自适应方法优于基准方法

Conclusion: 该半监督学习框架为实时BCI拼写系统提供了实用高效的选择，特别适用于标注数据有限的情况

Abstract: A P300 ERP-based Brain-Computer Interface (BCI) speller is an assistive communication tool. It searches for the P300 event-related potential (ERP) elicited by target stimuli, distinguishing it from the neural responses to non-target stimuli embedded in electroencephalogram (EEG) signals. Conventional methods require a lengthy calibration procedure to construct the binary classifier, which reduced overall efficiency. Thus, we proposed a unified framework with minimum calibration effort such that, given a small amount of labeled calibration data, we employed an adaptive semi-supervised EM-GMM algorithm to update the binary classifier. We evaluated our method based on character-level prediction accuracy, information transfer rate (ITR), and BCI utility. We applied calibration on training data and reported results on testing data. Our results indicate that, out of 15 participants, 9 participants exceed the minimum character-level accuracy of 0.7 using either on our adaptive method or the benchmark, and 7 out of these 9 participants showed that our adaptive method performed better than the benchmark. The proposed semi-supervised learning framework provides a practical and efficient alternative to improve the overall spelling efficiency in the real-time BCI speller system, particularly in contexts with limited labeled data.

</details>


### [23] [R$^2$Energy: A Large-Scale Benchmark for Robust Renewable Energy Forecasting under Diverse and Extreme Conditions](https://arxiv.org/abs/2602.15961)
*Zhi Sheng,Yuan Yuan,Guozhen Zhang,Yong Li*

Main category: cs.LG

TL;DR: R²Energy是一个用于NWP辅助可再生能源预测的大规模基准数据集，包含中国4个省份902个风能和太阳能电站的1070万小时记录，建立了标准化、无泄漏的预测范式，并揭示了极端天气下的"鲁棒性差距"。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源（特别是风能和太阳能）的快速扩张，可靠的预测对电力系统运行变得至关重要。虽然深度学习模型在平均精度上表现良好，但气候驱动的极端天气事件频率和强度不断增加，对电网稳定性和运行安全构成严重威胁，因此开发能够应对波动条件的鲁棒预测模型成为首要挑战。

Method: 提出了R²Energy基准数据集，包含中国4个省份902个风能和太阳能电站的1070万小时高保真记录；建立了标准化、无泄漏的预测范式，确保所有模型都能公平访问未来数值天气预报信号；采用基于专家标注极端天气的分区评估方法，揭示模型鲁棒性差距。

Result: 研究发现了一个关键的"鲁棒性差距"，这一差距通常被平均指标所掩盖。揭示了鲁棒性与复杂性之间的权衡：在极端条件下，模型的可靠性取决于其气象集成策略而非架构复杂性。R²Energy为安全关键电力系统应用的预测模型评估和开发提供了原则性基础。

Conclusion: R²Energy为可再生能源预测提供了一个大规模、标准化的基准平台，特别关注极端天气条件下的模型鲁棒性评估。研究表明，在安全关键的电力系统应用中，气象集成策略比模型架构复杂性更为重要，为未来鲁棒预测模型的开发提供了重要指导。

Abstract: The rapid expansion of renewable energy, particularly wind and solar power, has made reliable forecasting critical for power system operations. While recent deep learning models have achieved strong average accuracy, the increasing frequency and intensity of climate-driven extreme weather events pose severe threats to grid stability and operational security. Consequently, developing robust forecasting models that can withstand volatile conditions has become a paramount challenge. In this paper, we present R$^2$Energy, a large-scale benchmark for NWP-assisted renewable energy forecasting. It comprises over 10.7 million high-fidelity hourly records from 902 wind and solar stations across four provinces in China, providing the diverse meteorological conditions necessary to capture the wide-ranging variability of renewable generation. We further establish a standardized, leakage-free forecasting paradigm that grants all models identical access to future Numerical Weather Prediction (NWP) signals, enabling fair and reproducible comparison across state-of-the-art representative forecasting architectures. Beyond aggregate accuracy, we incorporate regime-wise evaluation with expert-aligned extreme weather annotations, uncovering a critical ``robustness gap'' typically obscured by average metrics. This gap reveals a stark robustness-complexity trade-off: under extreme conditions, a model's reliability is driven by its meteorological integration strategy rather than its architectural complexity. R$^2$Energy provides a principled foundation for evaluating and developing forecasting models for safety-critical power system applications.

</details>


### [24] [B-DENSE: Branching For Dense Ensemble Network Learning](https://arxiv.org/abs/2602.15971)
*Cherish Puniani,Tushar Kumar,Arnav Bendre,Gaurav Kumar,Shree Singhi*

Main category: cs.LG

TL;DR: 提出B-DENSE框架，通过多分支轨迹对齐改进扩散模型蒸馏，解决稀疏监督导致的结构信息丢失和离散化误差问题


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量好，但迭代采样导致推理延迟高。现有蒸馏技术加速采样但丢弃中间轨迹步骤，稀疏监督导致结构信息丢失和显著离散化误差

Method: 提出B-DENSE框架，修改学生架构输出K倍扩展通道，每个子集对应教师轨迹中的特定离散中间步骤。训练这些分支同时映射到教师目标时间步的整个序列，实现密集中间轨迹对齐

Result: 学生模型从训练早期就学会在解空间中导航，相比基线蒸馏框架展现出更优的图像生成质量

Conclusion: B-DENSE通过多分支轨迹对齐有效解决了扩散模型蒸馏中的稀疏监督问题，提高了生成质量

Abstract: Inspired by non-equilibrium thermodynamics, diffusion models have achieved state-of-the-art performance in generative modeling. However, their iterative sampling nature results in high inference latency. While recent distillation techniques accelerate sampling, they discard intermediate trajectory steps. This sparse supervision leads to a loss of structural information and introduces significant discretization errors. To mitigate this, we propose B-DENSE, a novel framework that leverages multi-branch trajectory alignment. We modify the student architecture to output $K$-fold expanded channels, where each subset corresponds to a specific branch representing a discrete intermediate step in the teacher's trajectory. By training these branches to simultaneously map to the entire sequence of the teacher's target timesteps, we enforce dense intermediate trajectory alignment. Consequently, the student model learns to navigate the solution space from the earliest stages of training, demonstrating superior image generation quality compared to baseline distillation frameworks.

</details>


### [25] [Fast Online Learning with Gaussian Prior-Driven Hierarchical Unimodal Thompson Sampling](https://arxiv.org/abs/2602.15972)
*Tianchi Zhao,He Liu,Hongyin Shi,Jinliang Li*

Main category: cs.LG

TL;DR: 提出基于高斯先验的汤普森采样聚类算法（TSCG）和单峰汤普森采样聚类算法（UTSCG），用于解决高斯奖励反馈的多臂老虎机问题，通过利用聚类结构降低遗憾界。


<details>
  <summary>Details</summary>
Motivation: 许多实际问题（如毫米波通信和投资组合管理）中的奖励反馈服从高斯分布，且臂之间存在聚类结构。现有汤普森采样算法未充分利用这种层次结构，导致遗憾界不够紧。

Method: 基于高斯先验的汤普森采样（TSG），提出TSCG算法专门处理2级层次结构。进一步针对单峰奖励场景，提出UTSCG算法。两种算法都利用了臂的聚类信息。

Result: 理论证明TSCG相比普通TSG能获得更低的遗憾界，UTSCG在单峰奖励下能达到更低的遗憾界。数值实验验证了所提算法的优势。

Conclusion: 通过利用多臂老虎机中的聚类结构，提出的TSCG和UTSCG算法能有效降低遗憾界，在具有高斯奖励反馈的实际问题中具有应用价值。

Abstract: We study a type of Multi-Armed Bandit (MAB) problems in which arms with a Gaussian reward feedback are clustered. Such an arm setting finds applications in many real-world problems, for example, mmWave communications and portfolio management with risky assets, as a result of the universality of the Gaussian distribution. Based on the Thompson Sampling algorithm with Gaussian prior (TSG) algorithm for the selection of the optimal arm, we propose our Thompson Sampling with Clustered arms under Gaussian prior (TSCG) specific to the 2-level hierarchical structure. We prove that by utilizing the 2-level structure, we can achieve a lower regret bound than we do with ordinary TSG. In addition, when the reward is Unimodal, we can reach an even lower bound on the regret by our Unimodal Thompson Sampling algorithm with Clustered Arms under Gaussian prior (UTSCG). Each of our proposed algorithms are accompanied by theoretical evaluation of the upper regret bound, and our numerical experiments confirm the advantage of our proposed algorithms.

</details>


### [26] [Verifier-Constrained Flow Expansion for Discovery Beyond the Data](https://arxiv.org/abs/2602.15984)
*Riccardo De Santi,Kimon Protopapas,Ya-Ping Hsieh,Andreas Krause*

Main category: cs.LG

TL;DR: 提出Flow Expander方法，通过验证器约束的熵最大化来扩展预训练流模型的采样范围，使其超越原始数据分布，同时保持样本有效性。


<details>
  <summary>Details</summary>
Motivation: 现有流和扩散模型通常在有限数据上预训练，只能从可行域的一小部分生成样本。这对于科学发现应用（如分子设计）是一个根本限制，因为目标通常是采样超出可用数据分布的有效设计。

Method: 引入强验证器和弱验证器的形式化概念，提出概率空间优化的全局和局部流扩展算法框架。提出Flow Expander（FE），一种可扩展的镜像下降方案，通过验证器约束的熵最大化来处理这两个问题。

Result: 在理想化和一般假设下提供了收敛保证的理论分析。在视觉可解释设置和分子设计任务上的实证评估表明，FE能够扩展预训练流模型，增加构象多样性同时保持有效性。

Conclusion: Flow Expander方法能够有效扩展预训练流模型的采样范围，使其超越原始数据分布，同时通过验证器约束保持样本有效性，为科学发现应用提供了有前景的解决方案。

Abstract: Flow and diffusion models are typically pre-trained on limited available data (e.g., molecular samples), covering only a fraction of the valid design space (e.g., the full molecular space). As a consequence, they tend to generate samples from only a narrow portion of the feasible domain. This is a fundamental limitation for scientific discovery applications, where one typically aims to sample valid designs beyond the available data distribution. To this end, we address the challenge of leveraging access to a verifier (e.g., an atomic bonds checker), to adapt a pre-trained flow model so that its induced density expands beyond regions of high data availability, while preserving samples validity. We introduce formal notions of strong and weak verifiers and propose algorithmic frameworks for global and local flow expansion via probability-space optimization. Then, we present Flow Expander (FE), a scalable mirror descent scheme that provably tackles both problems by verifier-constrained entropy maximization over the flow process noised state space. Next, we provide a thorough theoretical analysis of the proposed method, and state convergence guarantees under both idealized and general assumptions. Ultimately, we empirically evaluate our method on both illustrative, yet visually interpretable settings, and on a molecular design task showcasing the ability of FE to expand a pre-trained flow model increasing conformer diversity while preserving validity.

</details>


### [27] [ASPEN: Spectral-Temporal Fusion for Cross-Subject Brain Decoding](https://arxiv.org/abs/2602.16147)
*Megan Lee,Seung Ha Hwang,Inhyeok Choi,Shreyas Darade,Mengchun Zhang,Kateryna Shapovalenko*

Main category: cs.LG

TL;DR: ASPEN：一种通过乘法多模态融合结合频谱和时域特征的混合架构，在EEG脑机接口中实现跨被试泛化


<details>
  <summary>Details</summary>
Motivation: EEG脑机接口中的跨被试泛化具有挑战性，因为神经信号存在个体差异。研究发现频谱特征比时域波形具有更高的跨被试相似性，这启发了设计能够利用频谱稳定性的方法。

Method: 提出ASPEN混合架构，通过乘法融合结合频谱和时域特征流，要求跨模态一致性才能传播特征。该架构能够根据范式动态实现最优的频谱-时域平衡。

Result: 在六个基准数据集上的实验表明，ASPEN在三个数据集上取得了最佳未见被试准确率，在其他数据集上表现具有竞争力。乘法多模态融合能够有效实现跨被试泛化。

Conclusion: 频谱特征比时域信号具有更高的跨被试稳定性，通过乘法融合频谱和时域特征的ASPEN架构能够有效提升EEG脑机接口的跨被试泛化能力。

Abstract: Cross-subject generalization in EEG-based brain-computer interfaces (BCIs) remains challenging due to individual variability in neural signals. We investigate whether spectral representations offer more stable features for cross-subject transfer than temporal waveforms. Through correlation analyses across three EEG paradigms (SSVEP, P300, and Motor Imagery), we find that spectral features exhibit consistently higher cross-subject similarity than temporal signals. Motivated by this observation, we introduce ASPEN, a hybrid architecture that combines spectral and temporal feature streams via multiplicative fusion, requiring cross-modal agreement for features to propagate. Experiments across six benchmark datasets reveal that ASPEN is able to dynamically achieve the optimal spectral-temporal balance depending on the paradigm. ASPEN achieves the best unseen-subject accuracy on three of six datasets and competitive performance on others, demonstrating that multiplicative multimodal fusion enables effective cross-subject generalization.

</details>


### [28] [Anatomy of Capability Emergence: Scale-Invariant Representation Collapse and Top-Down Reorganization in Neural Networks](https://arxiv.org/abs/2602.15997)
*Jayadev Billa*

Main category: cs.LG

TL;DR: 研究发现神经网络训练中的能力涌现具有几何特征：表示会先普遍坍缩到任务特定的尺度不变地板值，然后自上而下传播，几何特征可预测硬任务的涌现，但无法预测细粒度时序


<details>
  <summary>Details</summary>
Motivation: 神经网络训练过程中能力涌现的机制仍然不透明，需要从几何角度理解表示空间如何变化以及这些变化如何预测能力涌现

Method: 追踪五个几何指标，涵盖五个模型规模（405K-85M参数）、120多个涌现事件、八个算法任务和三个Pythia语言模型（160M-2.8B），分析表示几何与能力涌现的关系

Result: 发现：1）训练开始时表示普遍坍缩到任务特定的尺度不变地板值；2）坍缩自上而下传播；3）表示几何是硬任务涌现的前兆（75-100%前兆率），而局部学习系数同步，Hessian指标滞后；几何指标能编码粗略任务难度但无法预测细粒度时序

Conclusion: 贡献在于揭示了能力涌现的几何解剖及其边界条件，而非预测工具；几何模式在Pythia中可复制，但每任务前兆信号需要任务训练对齐，自然预训练无法提供

Abstract: Capability emergence during neural network training remains mechanistically opaque. We track five geometric measures across five model scales (405K-85M parameters), 120+ emergence events in eight algorithmic tasks, and three Pythia language models (160M-2.8B). We find: (1) training begins with a universal representation collapse to task-specific floors that are scale-invariant across a 210X parameter range (e.g., modular arithmetic collapses to RANKME ~ 2.0 regardless of model size); (2) collapse propagates top-down through layers (32/32 task X model consistency), contradicting bottom-up feature-building intuition; (3) a geometric hierarchy in which representation geometry leads emergence (75-100% precursor rate for hard tasks), while the local learning coefficient is synchronous (0/24 precursor) and Hessian measures lag. We also delineate prediction limits: geometric measures encode coarse task difficulty but not fine-grained timing (within-class concordance 27%; when task ordering reverses across scales, prediction fails at 26%). On Pythia, global geometric patterns replicate but per-task precursor signals do not -- the precursor relationship requires task-training alignment that naturalistic pre-training does not provide. Our contribution is the geometric anatomy of emergence and its boundary conditions, not a prediction tool.

</details>


### [29] [Geometry-Aware Uncertainty Quantification via Conformal Prediction on Manifolds](https://arxiv.org/abs/2602.16015)
*Marzieh Amiri Shahbazi,Ali Baheri*

Main category: cs.LG

TL;DR: 提出自适应测地线保形预测框架，用测地线非一致性分数替代欧几里得残差，通过交叉验证难度估计器处理异方差噪声，在流形数据上获得更均匀的条件覆盖


<details>
  <summary>Details</summary>
Motivation: 现有保形预测方法假设欧几里得输出空间，当响应位于黎曼流形上时，预测区域校准效果差，需要专门处理流形数据的保形预测方法

Method: 提出自适应测地线保形预测框架：1) 用测地线距离替代欧几里得残差作为非一致性分数；2) 引入交叉验证难度估计器对分数进行归一化处理异方差噪声；3) 在球面上生成测地线帽作为预测区域

Result: 在合成球面实验（强异方差性）和真实世界地磁场预测任务（IGRF-14卫星数据）中，自适应方法显著降低条件覆盖变异性，将最坏情况覆盖率提升至接近名义水平，而基于坐标的基线方法因图表失真浪费大量覆盖面积

Conclusion: 自适应测地线保形预测框架有效处理流形数据的保形预测问题，通过测地线距离和难度自适应机制，在保持分布自由覆盖保证的同时，显著改善条件覆盖均匀性

Abstract: Conformal prediction provides distribution-free coverage guaranties for regression; yet existing methods assume Euclidean output spaces and produce prediction regions that are poorly calibrated when responses lie on Riemannian manifolds. We propose \emph{adaptive geodesic conformal prediction}, a framework that replaces Euclidean residuals with geodesic nonconformity scores and normalizes them by a cross-validated difficulty estimator to handle heteroscedastic noise. The resulting prediction regions, geodesic caps on the sphere, have position-independent area and adapt their size to local prediction difficulty, yielding substantially more uniform conditional coverage than non-adaptive alternatives. In a synthetic sphere experiment with strong heteroscedasticity and a real-world geomagnetic field forecasting task derived from IGRF-14 satellite data, the adaptive method markedly reduces conditional coverage variability and raises worst-case coverage much closer to the nominal level, while coordinate-based baselines waste a large fraction of coverage area due to chart distortion.

</details>


### [30] [Can Generative Artificial Intelligence Survive Data Contamination? Theoretical Guarantees under Contaminated Recursive Training](https://arxiv.org/abs/2602.16065)
*Kevin Wang,Hongqian Niu,Didong Li*

Main category: cs.LG

TL;DR: 该论文研究生成式AI递归训练中的数据污染问题，发现在一般框架下递归训练仍然收敛，收敛速率等于基线模型收敛速率与每轮真实数据比例的最小值。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI（如大语言模型）的普及，网络数据中AI生成内容与人类生成内容混合，导致后续模型训练时面临数据污染问题。现有理论研究仅限于离散或高斯分布等简化场景，而真实数据分布和现代生成模型要复杂得多。

Method: 建立一般性理论框架，对真实数据分布做最小假设，允许生成模型为通用近似器。分析递归训练过程，并扩展到存在采样偏差的数据收集场景，同时用实证研究支持理论结果。

Result: 证明在数据污染的递归训练中，训练过程仍然收敛，收敛速率等于基线模型收敛速率与每轮迭代中真实数据比例的最小值。这是首个在没有数据分布假设下关于递归训练的正面理论结果。

Conclusion: 即使在数据污染的递归训练场景下，只要每轮迭代包含一定比例的真实数据，训练过程仍然能够收敛。该理论结果为理解生成式AI递归训练提供了更一般化的分析框架。

Abstract: Generative Artificial Intelligence (AI), such as large language models (LLMs), has become a transformative force across science, industry, and society. As these systems grow in popularity, web data becomes increasingly interwoven with this AI-generated material and it is increasingly difficult to separate them from naturally generated content. As generative models are updated regularly, later models will inevitably be trained on mixtures of human-generated data and AI-generated data from earlier versions, creating a recursive training process with data contamination. Existing theoretical work has examined only highly simplified settings, where both the real data and the generative model are discrete or Gaussian, where it has been shown that such recursive training leads to model collapse. However, real data distributions are far more complex, and modern generative models are far more flexible than Gaussian and linear mechanisms. To fill this gap, we study recursive training in a general framework with minimal assumptions on the real data distribution and allow the underlying generative model to be a general universal approximator. In this framework, we show that contaminated recursive training still converges, with a convergence rate equal to the minimum of the baseline model's convergence rate and the fraction of real data used in each iteration. To the best of our knowledge, this is the first (positive) theoretical result on recursive training without distributional assumptions on the data. We further extend the analysis to settings where sampling bias is present in data collection and support all theoretical results with empirical studies.

</details>


### [31] [MolCrystalFlow: Molecular Crystal Structure Prediction via Flow Matching](https://arxiv.org/abs/2602.16020)
*Cheng Zeng,Harry W. Sullivan,Thomas Egg,Maya M. Martirossyan,Philipp Höllmer,Jirui Jin,Richard G. Hennig,Adrian Roitberg,Stefano Martiniani,Ellad B. Tadmor,Mingjie Liu*

Main category: cs.LG

TL;DR: MolCrystalFlow是一个基于流的生成模型，用于分子晶体结构预测，通过将分子作为刚体嵌入，在黎曼流形上学习晶格矩阵、分子取向和质心位置，解决了周期性分子晶体结构预测的挑战。


<details>
  <summary>Details</summary>
Motivation: 分子晶体结构预测是计算化学的重大挑战，因为分子尺寸大且存在复杂的分子内和分子间相互作用。虽然生成模型已革新了分子、无机固体和金属有机框架的结构发现，但将其扩展到完全周期性的分子晶体仍然难以实现。

Method: 提出MolCrystalFlow框架，将分子作为刚体嵌入，解耦分子内复杂性和分子间堆积。联合学习晶格矩阵、分子取向和质心位置，其中质心和取向在其原生黎曼流形上表示，允许构建测地流和使用尊重几何对称性的图神经网络操作。

Result: 在两个开源分子晶体数据集上，与最先进的周期性晶体生成模型和基于规则的结构生成方法进行基准测试。展示了将MolCrystalFlow模型与通用机器学习势能结合，加速分子晶体结构预测。

Conclusion: MolCrystalFlow为数据驱动的分子晶体生成发现铺平了道路，通过流式生成模型成功解决了周期性分子晶体结构预测的挑战。

Abstract: Molecular crystal structure prediction represents a grand challenge in computational chemistry due to large sizes of constituent molecules and complex intra- and intermolecular interactions. While generative modeling has revolutionized structure discovery for molecules, inorganic solids, and metal-organic frameworks, extending such approaches to fully periodic molecular crystals is still elusive. Here, we present MolCrystalFlow, a flow-based generative model for molecular crystal structure prediction. The framework disentangles intramolecular complexity from intermolecular packing by embedding molecules as rigid bodies and jointly learning the lattice matrix, molecular orientations, and centroid positions. Centroids and orientations are represented on their native Riemannian manifolds, allowing geodesic flow construction and graph neural network operations that respects geometric symmetries. We benchmark our model against state-of-the-art generative models for large-size periodic crystals and rule-based structure generation methods on two open-source molecular crystal datasets. We demonstrate an integration of MolCrystalFlow model with universal machine learning potential to accelerate molecular crystal structure prediction, paving the way for data-driven generative discovery of molecular crystals.

</details>


### [32] [Feature-based morphological analysis of shape graph data](https://arxiv.org/abs/2602.16120)
*Murad Hossen,Demetrio Labate,Nicolas Charon*

Main category: cs.LG

TL;DR: 提出一个用于分析形状图数据集的计算流程，通过提取拓扑、几何和方向特征来同时捕捉网络连接结构和分支几何差异，并在多个真实数据集上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统图分析主要关注抽象连接结构，但形状图（嵌入2D/3D空间的几何网络）还需要分析分支的几何差异。需要一种能同时捕捉拓扑和几何变化的分析方法。

Method: 提出一个计算流程，提取专门设计的拓扑、几何和方向特征集，这些特征满足关键不变性要求。利用得到的特征表示进行群体比较、聚类和分类任务。

Result: 在多个真实数据集（城市道路网络、神经元追踪、星形胶质细胞成像）上评估特征表示的有效性，并与多种特征基和非特征基方法进行基准比较。

Conclusion: 提出的特征表示方法能有效分析形状图数据集，同时捕捉拓扑和几何变化，在多种分析任务上优于或与现有方法相当。

Abstract: This paper introduces and demonstrates a computational pipeline for the statistical analysis of shape graph datasets, namely geometric networks embedded in 2D or 3D spaces. Unlike traditional abstract graphs, our purpose is not only to retrieve and distinguish variations in the connectivity structure of the data but also geometric differences of the network branches. Our proposed approach relies on the extraction of a specifically curated and explicit set of topological, geometric and directional features, designed to satisfy key invariance properties. We leverage the resulting feature representation for tasks such as group comparison, clustering and classification on cohorts of shape graphs. The effectiveness of this representation is evaluated on several real-world datasets including urban road/street networks, neuronal traces and astrocyte imaging. These results are benchmarked against several alternative methods, both feature-based and not.

</details>


### [33] [AI-CARE: Carbon-Aware Reporting Evaluation Metric for AI Models](https://arxiv.org/abs/2602.16042)
*KC Santosh,Srikanth Baride,Rodrigue Rizk*

Main category: cs.LG

TL;DR: AI-CARE：一个评估机器学习模型能耗和碳排放的工具，引入碳-性能权衡曲线，推动研究社区向透明、多目标评估转变


<details>
  <summary>Details</summary>
Motivation: 随着机器学习快速发展，模型训练和推理的环境成本已成为关键社会问题。现有基准主要关注准确率等标准性能指标，而忽略了能耗和碳排放。这种单目标评估范式与大规模部署的实际需求（特别是在移动设备、发展中地区等能源受限环境）越来越不匹配。

Method: 提出AI-CARE评估工具，用于报告机器学习模型的能耗和碳排放。引入碳-性能权衡曲线，可视化性能与碳成本之间的帕累托前沿。通过理论分析和代表性ML工作负载的实证验证来展示碳感知基准测试的效果。

Result: 碳感知基准测试改变了模型的相对排名，鼓励同时具备准确性和环境责任的架构。研究表明，考虑环境因素后，模型评估结果会发生显著变化。

Conclusion: AI-CARE旨在推动研究社区向透明、多目标评估转变，使机器学习进展与全球可持续发展目标保持一致。工具和文档已在GitHub上开源。

Abstract: As machine learning (ML) continues its rapid expansion, the environmental cost of model training and inference has become a critical societal concern. Existing benchmarks overwhelmingly focus on standard performance metrics such as accuracy, BLEU, or mAP, while largely ignoring energy consumption and carbon emissions. This single-objective evaluation paradigm is increasingly misaligned with the practical requirements of large-scale deployment, particularly in energy-constrained environments such as mobile devices, developing regions, and climate-aware enterprises. In this paper, we propose AI-CARE, an evaluation tool for reporting energy consumption, and carbon emissions of ML models. In addition, we introduce the carbon-performance tradeoff curve, an interpretable tool that visualizes the Pareto frontier between performance and carbon cost. We demonstrate, through theoretical analysis and empirical validation on representative ML workloads, that carbon-aware benchmarking changes the relative ranking of models and encourages architectures that are simultaneously accurate and environmentally responsible. Our proposal aims to shift the research community toward transparent, multi-objective evaluation and align ML progress with global sustainability goals. The tool and documentation are available at https://github.com/USD-AI-ResearchLab/ai-care.

</details>


### [34] [Bayesian Quadrature: Gaussian Processes for Integration](https://arxiv.org/abs/2602.16218)
*Maren Mahsereci,Toni Karvonen*

Main category: cs.LG

TL;DR: 这是一篇关于贝叶斯求积法的系统性综述论文，全面回顾了该概率数值积分方法的数学基础、分类框架、理论保证，并进行了数值研究。


<details>
  <summary>Details</summary>
Motivation: 尽管贝叶斯求积法在20世纪80年代就已流行，但缺乏系统性和全面的文献综述。本文旨在填补这一空白，为这一概率数值积分方法提供完整的理论框架和实践指导。

Method: 论文从多个角度回顾贝叶斯求积法的数学基础；提出了一个三维分类框架（建模、推断、采样）；收集了理论保证；进行了受控数值研究；评估了实际应用挑战；并提供了近乎详尽的文献综述。

Result: 论文提供了贝叶斯求积法的系统性分类框架，展示了不同方法选择对性能的影响，识别了实际应用中的挑战和限制，并整理了跨越机器学习、统计学、数学和工程等多个领域的全面文献。

Conclusion: 这篇综述填补了贝叶斯求积法领域的文献空白，为研究人员和从业者提供了全面的理论框架、分类系统和实践指导，有助于推动这一概率数值积分方法的发展和应用。

Abstract: Bayesian quadrature is a probabilistic, model-based approach to numerical integration, the estimation of intractable integrals, or expectations. Although Bayesian quadrature was popularised already in the 1980s, no systematic and comprehensive treatment has been published. The purpose of this survey is to fill this gap. We review the mathematical foundations of Bayesian quadrature from different points of view; present a systematic taxonomy for classifying different Bayesian quadrature methods along the three axes of modelling, inference, and sampling; collect general theoretical guarantees; and provide a controlled numerical study that explores and illustrates the effect of different choices along the axes of the taxonomy. We also provide a realistic assessment of practical challenges and limitations to application of Bayesian quadrature methods and include an up-to-date and nearly exhaustive bibliography that covers not only machine learning and statistics literature but all areas of mathematics and engineering in which Bayesian quadrature or equivalent methods have seen use.

</details>


### [35] [MoE-Spec: Expert Budgeting for Efficient Speculative Decoding](https://arxiv.org/abs/2602.16052)
*Bradley McDanel,Steven Li,Sruthikesh Surineni,Harshit Khaitan*

Main category: cs.LG

TL;DR: MoE-Spec：一种针对MoE模型的训练免费验证时专家预算方法，通过固定专家容量限制，仅加载对验证贡献最大的专家，显著提升推测解码吞吐量


<details>
  <summary>Details</summary>
Motivation: 针对MoE模型推测解码中的瓶颈问题：大草案树会激活大量独特专家，显著增加内存压力并降低推测解码相对于自回归解码的速度优势

Method: 提出MoE-Spec方法，在验证时实施专家预算，通过每层固定专家容量限制，仅加载对验证贡献最大的专家，丢弃使用频率低的长尾专家

Result: 实验显示该方法在多个模型规模和数据集上，相比最先进的推测解码基线（EAGLE-3）获得10-30%更高的吞吐量，且保持可比较的质量

Conclusion: MoE-Spec通过解耦推测深度与内存成本，提供了一种灵活的方法来权衡精度与延迟，显著提升了MoE模型推测解码的效率

Abstract: Speculative decoding accelerates Large Language Model (LLM) inference by verifying multiple drafted tokens in parallel. However, for Mixture-of-Experts (MoE) models, this parallelism introduces a severe bottleneck: large draft trees activate many unique experts, significantly increasing memory pressure and diminishing speedups from speculative decoding relative to autoregressive decoding. Prior methods reduce speculation depth when MoE verification becomes expensive. We propose MoE-Spec, a training-free verification-time expert budgeting method that decouples speculation depth from memory cost by enforcing a fixed expert capacity limit at each layer, loading only the experts that contribute most to verification and dropping the long tail of rarely used experts that drive bandwidth overhead. Experiments across multiple model scales and datasets show that this method yields 10--30\% higher throughput than state-of-the-art speculative decoding baselines (EAGLE-3) at comparable quality, with flexibility to trade accuracy for further latency reductions through tighter budgets.

</details>


### [36] [Regret and Sample Complexity of Online Q-Learning via Concentration of Stochastic Approximation with Time-Inhomogeneous Markov Chains](https://arxiv.org/abs/2602.16274)
*Rahul Singh,Siddharth Chandak,Eric Moulines,Vivek S. Borkar,Nicholas Bambos*

Main category: cs.LG

TL;DR: 首次为经典在线Q学习在无限时域折扣MDP中提供高概率遗憾界，不依赖乐观或奖励项。分析Boltzmann Q学习发现遗憾对MDP次优性间隙敏感，提出结合ε-greedy和Boltzmann的平滑探索方案，获得间隙鲁棒的近O(N^{9/10})遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有在线Q学习的高概率遗憾界通常依赖乐观方法或奖励项，本文旨在为经典非乐观Q学习建立理论保证，特别是研究探索策略对遗憾界的影响。

Method: 1) 分析带衰减温度的Boltzmann Q学习；2) 提出平滑ε_n-greedy探索方案，结合ε-greedy和Boltzmann探索；3) 开发用于收缩马尔可夫随机逼近的高概率集中界分析工具。

Result: 1) Boltzmann Q学习的遗憾对MDP次优性间隙敏感：大间隙时亚线性，小间隙时接近线性；2) 平滑ε_n-greedy方案获得间隙鲁棒的遗憾界~O(N^{9/10})；3) 开发了新的高概率集中界分析工具。

Conclusion: 本文首次为经典在线Q学习建立了高概率遗憾界，揭示了探索策略对遗憾性能的关键影响，提出的平滑探索方案实现了间隙鲁棒的遗憾界，开发的分析工具对相关研究有独立价值。

Abstract: We present the first high-probability regret bound for classical online Q-learning in infinite-horizon discounted Markov decision processes, without relying on optimism or bonus terms. We first analyze Boltzmann Q-learning with decaying temperature and show that its regret depends critically on the suboptimality gap of the MDP: for sufficiently large gaps, the regret is sublinear, while for small gaps it deteriorates and can approach linear growth. To address this limitation, we study a Smoothed $ε_n$-Greedy exploration scheme that combines $ε_n$-greedy and Boltzmann exploration, for which we prove a gap-robust regret bound of near-$\tilde{O}(N^{9/10})$. To analyze these algorithms, we develop a high-probability concentration bound for contractive Markovian stochastic approximation with iterate- and time-dependent transition dynamics. This bound may be of independent interest as the contraction factor in our bound is governed by the mixing time and is allowed to converge to one asymptotically.

</details>


### [37] [Multi-Objective Alignment of Language Models for Personalized Psychotherapy](https://arxiv.org/abs/2602.16053)
*Mehrab Beikzadeh,Yasaman Asadollah Salmanpour,Ashima Suvarna,Sriram Sankararaman,Matteo Malgaroli,Majid Sarrafzadeh,Saadia Gabriel*

Main category: cs.LG

TL;DR: 该研究开发了一种用于心理健康AI治疗系统的多目标对齐框架，通过直接偏好优化平衡患者偏好与临床安全，相比单目标优化在共情与安全之间取得更好平衡。


<details>
  <summary>Details</summary>
Motivation: 全球超过10亿人受心理健康问题影响，但护理可及性受限于人力短缺和成本约束。现有AI系统采用独立目标优化，未能平衡患者偏好与临床安全需求。

Method: 调查335名有心理健康经历个体的治疗维度偏好，开发多目标对齐框架，使用直接偏好优化训练六个标准的奖励模型（共情、安全、积极倾听、自我激励改变、信任/融洽、患者自主性），系统比较多目标与单目标优化方法。

Result: 多目标DPO在共情(77.6%)与安全(62.6%)间取得更好平衡，优于单目标优化(93.6%共情，47.8%安全)；治疗标准比一般沟通原则表现好17.2%；盲法临床评估确认MODPO始终被偏好，LLM评估者一致性接近临床医生间可靠性。

Conclusion: 多目标对齐框架能有效平衡心理健康AI治疗中的患者偏好与临床安全，为开发更安全有效的AI治疗系统提供了可行方法。

Abstract: Mental health disorders affect over 1 billion people worldwide, yet access to care remains limited by workforce shortages and cost constraints. While AI systems show therapeutic promise, current alignment approaches optimize objectives independently, failing to balance patient preferences with clinical safety. We survey 335 individuals with lived mental health experience to collect preference rankings across therapeutic dimensions, then develop a multi-objective alignment framework using direct preference optimization. We train reward models for six criteria -- empathy, safety, active listening, self-motivated change, trust/rapport, and patient autonomy -- and systematically compare multi-objective approaches against single-objective optimization, supervised fine-tuning, and parameter merging. Multi-objective DPO (MODPO) achieves superior balance (77.6% empathy, 62.6% safety) compared to single-objective optimization (93.6% empathy, 47.8% safety), and therapeutic criteria outperform general communication principles by 17.2%. Blinded clinician evaluation confirms MODPO is consistently preferred, with LLM-evaluator agreement comparable to inter-clinician reliability.

</details>


### [38] [The Implicit Bias of Adam and Muon on Smooth Homogeneous Neural Networks](https://arxiv.org/abs/2602.16340)
*Eitan Gronich,Gal Vardi*

Main category: cs.LG

TL;DR: 该论文研究了动量优化器在齐次模型中的隐式偏好，证明动量算法（如Muon、MomentumGD、Signum）近似于最速下降轨迹，具有最大化相应范数间隔的偏好。


<details>
  <summary>Details</summary>
Motivation: 研究动量优化器在齐次模型中的隐式偏好，扩展先前关于最速下降在齐次模型中偏好的工作，并理解不同动量算法（如Muon、MomentumGD、Signum、Adam）优化时的内在偏好方向。

Method: 首先将齐次模型中最速下降的隐式偏好结果扩展到带可选学习率调度的归一化最速下降。然后证明对于光滑齐次模型，动量最速下降算法（Muon、MomentumGD、Signum）在衰减学习率调度下近似于最速下降轨迹。进一步分析Adam（无稳定性常数）、Muon-Signum和Muon-Adam等算法的偏好特性。

Result: 证明了动量最速下降算法具有偏向相应间隔最大化问题KKT点的偏好。Adam最大化ℓ∞间隔，Muon-Signum和Muon-Adam最大化混合范数。实验验证了理论，显示优化器选择决定了最大化的间隔类型。

Conclusion: 该研究扩展了齐次模型中最速下降和线性模型中动量优化器的工作线，系统揭示了不同动量优化器在齐次模型中的隐式偏好特性，为理解优化算法偏好提供了理论框架。

Abstract: We study the implicit bias of momentum-based optimizers on homogeneous models. We first extend existing results on the implicit bias of steepest descent in homogeneous models to normalized steepest descent with an optional learning rate schedule. We then show that for smooth homogeneous models, momentum steepest descent algorithms like Muon (spectral norm), MomentumGD ($\ell_2$ norm), and Signum ($\ell_\infty$ norm) are approximate steepest descent trajectories under a decaying learning rate schedule, proving that these algorithms too have a bias towards KKT points of the corresponding margin maximization problem. We extend the analysis to Adam (without the stability constant), which maximizes the $\ell_\infty$ margin, and to Muon-Signum and Muon-Adam, which maximize a hybrid norm. Our experiments corroborate the theory and show that the identity of the margin maximized depends on the choice of optimizer. Overall, our results extend earlier lines of work on steepest descent in homogeneous models and momentum-based optimizers in linear models.

</details>


### [39] [Extracting and Analyzing Rail Crossing Behavior Signatures from Videos using Tensor Methods](https://arxiv.org/abs/2602.16057)
*Dawon Ahn,Het Patel,Aemal Khattak,Jia Chen,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: 提出多视角张量分解框架分析铁路道口行为模式，发现道口位置比时间对行为影响更大，接近阶段行为最具区分性


<details>
  <summary>Details</summary>
Motivation: 传统方法单独分析每个铁路道口，难以发现跨地点的共享行为模式。铁路道口安全挑战复杂，驾驶员行为随地点、时间和条件变化

Method: 使用多视角张量分解框架，将行为分为三个时间阶段：接近阶段（警告激活到栏杆下降）、等待阶段（栏杆下降到火车通过）、清空阶段（火车通过到栏杆升起）。利用TimeSformer嵌入表示每个阶段，构建阶段特定相似性矩阵，应用非负对称CP分解发现具有不同时间特征的潜在行为成分

Result: 张量分析显示道口位置比一天中的时间对行为模式影响更大，接近阶段行为提供特别有区分性的特征。学习到的成分空间可视化确认了基于位置的聚类，某些道口形成独特的行为聚类

Conclusion: 该自动化框架支持跨多个道口的可扩展模式发现，为按行为相似性分组道口提供基础，从而指导有针对性的安全干预措施

Abstract: Railway crossings present complex safety challenges where driver behavior varies by location, time, and conditions. Traditional approaches analyze crossings individually, limiting the ability to identify shared behavioral patterns across locations. We propose a multi-view tensor decomposition framework that captures behavioral similarities across three temporal phases: Approach (warning activation to gate lowering), Waiting (gates down to train passage), and Clearance (train passage to gate raising). We analyze railway crossing videos from multiple locations using TimeSformer embeddings to represent each phase. By constructing phase-specific similarity matrices and applying non-negative symmetric CP decomposition, we discover latent behavioral components with distinct temporal signatures. Our tensor analysis reveals that crossing location appears to be a stronger determinant of behavior patterns than time of day, and that approach-phase behavior provides particularly discriminative signatures. Visualization of the learned component space confirms location-based clustering, with certain crossings forming distinct behavioral clusters. This automated framework enables scalable pattern discovery across multiple crossings, providing a foundation for grouping locations by behavioral similarity to inform targeted safety interventions.

</details>


### [40] [Learning with Locally Private Examples by Inverse Weierstrass Private Stochastic Gradient Descent](https://arxiv.org/abs/2602.16436)
*Jean Dufraiche,Paul Mangold,Michaël Perrot,Marc Tommasi*

Main category: cs.LG

TL;DR: 提出基于Weierstrass变换的LDP数据偏差校正方法，构建IWP-SGD算法实现无偏估计，在二元分类中达到O(1/n)收敛速率


<details>
  <summary>Details</summary>
Motivation: 非交互式本地差分隐私(LDP)下一次性发布数据可实现完全数据重用，但引入的噪声会在后续分析中产生偏差，需要校正方法

Method: 利用Weierstrass变换刻画LDP噪声在二元分类中的偏差，通过逆变换实现偏差校正，构建IWP-SGD随机梯度下降算法

Result: IWP-SGD收敛到真实总体风险最小化器，收敛速率为O(1/n)，在合成和真实数据集上验证了二元分类任务的有效性

Conclusion: 通过Weierstrass变换逆变换校正LDP噪声偏差的方法有效，IWP-SGD算法为LDP下数据重用提供了理论保证和实用工具

Abstract: Releasing data once and for all under noninteractive Local Differential Privacy (LDP) enables complete data reusability, but the resulting noise may create bias in subsequent analyses. In this work, we leverage the Weierstrass transform to characterize this bias in binary classification. We prove that inverting this transform leads to a bias-correction method to compute unbiased estimates of nonlinear functions on examples released under LDP. We then build a novel stochastic gradient descent algorithm called Inverse Weierstrass Private SGD (IWP-SGD). It converges to the true population risk minimizer at a rate of $\mathcal{O}(1/n)$, with $n$ the number of examples. We empirically validate IWP-SGD on binary classification tasks using synthetic and real-world datasets.

</details>


### [41] [GICDM: Mitigating Hubness for Reliable Distance-Based Generative Model Evaluation](https://arxiv.org/abs/2602.16449)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: GICDM方法通过修正嵌入空间中的hubness现象，改进生成模型评估中的距离度量可靠性


<details>
  <summary>Details</summary>
Motivation: 生成模型评估通常依赖高维嵌入空间计算样本间距离，但这些空间存在hubness现象，扭曲最近邻关系并偏置基于距离的度量

Method: 基于经典Iterative Contextual Dissimilarity Measure (ICDM)，提出Generative ICDM (GICDM)，修正真实和生成数据的邻域估计，并引入多尺度扩展改进经验行为

Result: 在合成和真实基准测试中，GICDM解决了hubness引起的失败，恢复了可靠的度量行为，并提高了与人类判断的一致性

Conclusion: GICDM通过修正嵌入空间中的hubness现象，显著改进了生成模型评估的可靠性和与人类感知的一致性

Abstract: Generative model evaluation commonly relies on high-dimensional embedding spaces to compute distances between samples. We show that dataset representations in these spaces are affected by the hubness phenomenon, which distorts nearest neighbor relationships and biases distance-based metrics. Building on the classical Iterative Contextual Dissimilarity Measure (ICDM), we introduce Generative ICDM (GICDM), a method to correct neighborhood estimation for both real and generated data. We introduce a multi-scale extension to improve empirical behavior. Extensive experiments on synthetic and real benchmarks demonstrate that GICDM resolves hubness-induced failures, restores reliable metric behavior, and improves alignment with human judgment.

</details>


### [42] [Omni-iEEG: A Large-Scale, Comprehensive iEEG Dataset and Benchmark for Epilepsy Research](https://arxiv.org/abs/2602.16072)
*Chenda Duan,Yipeng Zhang,Sotaro Kanai,Yuanyi Ding,Atsuro Daida,Pengyue Yu,Tiancheng Zheng,Naoto Kuroda,Shaun A. Hussain,Eishi Asano,Hiroki Nariai,Vwani Roychowdhury*

Main category: cs.LG

TL;DR: Omni-iEEG是一个大规模、标准化的颅内脑电图资源，包含302名患者、178小时高分辨率记录，提供超过36K专家验证的病理事件标注，旨在解决癫痫研究中数据格式不一致、缺乏标准化基准的问题。


<details>
  <summary>Details</summary>
Motivation: 癫痫影响全球超过5000万人，其中三分之一患者患有药物耐药性癫痫，手术是最佳治疗选择。然而，临床工作流程受限于劳动密集型的手动审查，现有数据驱动方法通常基于单中心数据集，存在格式不一致、缺乏标准化基准、很少发布病理事件标注等问题，阻碍了研究的可重复性、跨中心验证和临床相关性。

Method: 通过协调公开可用来源中的异构iEEG格式、元数据和记录，创建了Omni-iEEG资源。该数据集包含302名患者、178小时高分辨率记录，提供统一的临床元数据（如癫痫发作起始区、切除区域和手术结果）和超过36K专家验证的病理事件标注。定义了基于临床先验的统一评估指标和临床相关任务。

Result: 建立了大规模、标准化的iEEG资源，展示了在长iEEG片段上进行端到端建模的潜力，并突出了在非神经生理学领域预训练表示的可迁移性。为可重复、可泛化和临床可转化的癫痫研究奠定了基础。

Conclusion: Omni-iEEG作为机器学习和癫痫研究之间的桥梁，通过提供标准化数据集、临床相关任务和统一评估指标，促进了可重复、可泛化和临床可转化的癫痫研究，为癫痫定位和治疗的进一步研究提供了重要基础。

Abstract: Epilepsy affects over 50 million people worldwide, and one-third of patients suffer drug-resistant seizures where surgery offers the best chance of seizure freedom. Accurate localization of the epileptogenic zone (EZ) relies on intracranial EEG (iEEG). Clinical workflows, however, remain constrained by labor-intensive manual review. At the same time, existing data-driven approaches are typically developed on single-center datasets that are inconsistent in format and metadata, lack standardized benchmarks, and rarely release pathological event annotations, creating barriers to reproducibility, cross-center validation, and clinical relevance. With extensive efforts to reconcile heterogeneous iEEG formats, metadata, and recordings across publicly available sources, we present $\textbf{Omni-iEEG}$, a large-scale, pre-surgical iEEG resource comprising $\textbf{302 patients}$ and $\textbf{178 hours}$ of high-resolution recordings. The dataset includes harmonized clinical metadata such as seizure onset zones, resections, and surgical outcomes, all validated by board-certified epileptologists. In addition, Omni-iEEG provides over 36K expert-validated annotations of pathological events, enabling robust biomarker studies. Omni-iEEG serves as a bridge between machine learning and epilepsy research. It defines clinically meaningful tasks with unified evaluation metrics grounded in clinical priors, enabling systematic evaluation of models in clinically relevant settings. Beyond benchmarking, we demonstrate the potential of end-to-end modeling on long iEEG segments and highlight the transferability of representations pretrained on non-neurophysiological domains. Together, these contributions establish Omni-iEEG as a foundation for reproducible, generalizable, and clinically translatable epilepsy research. The project page with dataset and code links is available at omni-ieeg.github.io/omni-ieeg.

</details>


### [43] [Sequential Membership Inference Attacks](https://arxiv.org/abs/2602.16596)
*Thomas Michel,Debabrota Basu,Emilie Kaufmann*

Main category: cs.LG

TL;DR: 提出SeMI*攻击方法，利用模型更新序列进行成员推理攻击，相比仅使用最终模型的现有方法能获得更强的攻击效果和更严格的隐私审计。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型会经历多次更新，现有成员推理攻击主要针对静态模型且假设无限样本，缺乏对模型动态更新的最优攻击方法分析。

Method: 开发SeMI*攻击方法，利用模型更新序列来检测特定更新步骤中插入的目标数据，推导了在有限样本下（无论是否使用隐私保护）的最优攻击能力。

Result: SeMI*避免了现有攻击中随着训练数据积累而信号稀释的问题，攻击者可通过调整插入时间和目标数据获得更严格的隐私审计，实验证明其在不同数据分布和DP-SGD训练模型上优于基线方法。

Conclusion: 利用模型更新序列的SeMI*攻击方法比仅使用最终模型的现有攻击更有效，能提供更严格的隐私审计，对理解动态模型的隐私风险具有重要意义。

Abstract: Modern AI models are not static. They go through multiple updates in their lifecycles. Thus, exploiting the model dynamics to create stronger Membership Inference (MI) attacks and tighter privacy audits are timely questions. Though the literature empirically shows that using a sequence of model updates can increase the power of MI attacks, rigorous analysis of the `optimal' MI attacks is limited to static models with infinite samples. Hence, we develop an `optimal' MI attack, SeMI*, that uses the sequence of model updates to identify the presence of a target inserted at a certain update step. For the empirical mean computation, we derive the optimal power of SeMI*, while accessing a finite number of samples with or without privacy. Our results retrieve the existing asymptotic analysis. We observe that having access to the model sequence avoids the dilution of MI signals unlike the existing attacks on the final model, where the MI signal vanishes as training data accumulates. Furthermore, an adversary can use SeMI* to tune both the insertion time and the canary to yield tighter privacy audits. Finally, we conduct experiments across data distributions and models trained or fine-tuned with DP-SGD demonstrating that practical variants of SeMI* lead to tighter privacy audits than the baselines.

</details>


### [44] [Why Any-Order Autoregressive Models Need Two-Stream Attention: A Structural-Semantic Tradeoff](https://arxiv.org/abs/2602.16092)
*Patrick Pynadath,Ruqi Zhang*

Main category: cs.LG

TL;DR: 研究发现双流注意力在任意顺序自回归模型中的成功不仅源于位置与内容的分离，而是为了规避任意顺序生成中固有的结构-语义权衡问题。


<details>
  <summary>Details</summary>
Motivation: 任意顺序自回归模型（AO-ARMs）通过支持原生键值缓存为高效掩码扩散提供了前景，但竞争性性能通常需要双流注意力机制。本研究旨在探究双流注意力是否在扮演更微妙的角色，而不仅仅是分离位置和内容。

Method: 提出了解耦RoPE（Decoupled RoPE），这是一种对旋转位置嵌入的修改，可以在不泄露目标内容的情况下提供目标位置信息。通过这种方法，可以隔离结构-语义权衡问题与位置-内容分离问题。

Result: 解耦RoPE在短序列长度下表现具有竞争力（此时语义和结构邻近性重合），但随着序列长度增加和两种顺序的分离，性能会下降。这表明双流注意力的成功源于规避任意顺序生成中的结构-语义权衡。

Conclusion: 双流注意力的成功不仅在于分离位置与内容，更重要的是它能够规避任意顺序生成中固有的结构-语义权衡问题，其中隐藏表示需要同时关注语义信息丰富的标记进行预测和结构上最近的标记进行总结。

Abstract: Any-order autoregressive models (AO-ARMs) offer a promising path toward efficient masked diffusion by enabling native key-value caching, but competitive performance has so far required two-stream attention, typically motivated as a means of decoupling token content from position. In this work, we argue that two-stream attention may be serving a more subtle role. We identify a structural-semantic tradeoff in any-order generation: the hidden representation at each step must simultaneously attend to semantically informative tokens for prediction and structurally recent tokens for summarization, objectives that compete for attention capacity in a single stream but can specialize across two streams. To isolate this tradeoff from position-content separation, we propose Decoupled RoPE, a modification to rotary position embeddings that provides target position information without revealing target content. Decoupled RoPE performs competitively at short sequence lengths--where semantic and structural proximity coincide--but degrades as sequence length increases and the two orderings diverge. These results suggest that the success of two-stream attention stems not merely from separating position from content, but from circumventing the deeper structural-semantic tradeoff inherent to any-order generation.

</details>


### [45] [Axle Sensor Fusion for Online Continual Wheel Fault Detection in Wayside Railway Monitoring](https://arxiv.org/abs/2602.16101)
*Afonso Lourenço,Francisca Osório,Diogo Risca,Goreti Marreiros*

Main category: cs.LG

TL;DR: 提出一种语义感知、标签高效的持续学习框架，用于铁路故障诊断，结合VAE编码、语义元数据融合和梯度提升分类器，适应不断变化的运行条件。


<details>
  <summary>Details</summary>
Motivation: 铁路轮轨界面易磨损和故障，传统预测维护方法需要手动特征工程，深度学习模型在在线环境下容易因运行模式变化而性能下降，需要能适应未知运行条件的故障诊断方法。

Method: 1) 使用变分自编码器(VAE)无监督编码加速度计信号；2) 通过AI驱动的峰值检测从光纤布拉格光栅传感器提取语义元数据；3) 融合VAE嵌入和语义元数据；4) 使用轻量级梯度提升分类器进行异常评分；5) 采用基于回放的持续学习策略适应演化领域。

Result: 模型能够检测平坦化和多边形化等微小缺陷，并能适应列车类型、速度、载荷和轨道轮廓等运行条件的变化，仅使用单个加速度计和应变计进行路边监测。

Conclusion: 该语义感知持续学习框架实现了标签高效的铁路故障诊断，能适应不断变化的运行条件，避免灾难性遗忘，为铁路安全维护提供了可靠且经济有效的解决方案。

Abstract: Reliable and cost-effective maintenance is essential for railway safety, particularly at the wheel-rail interface, which is prone to wear and failure. Predictive maintenance frameworks increasingly leverage sensor-generated time-series data, yet traditional methods require manual feature engineering, and deep learning models often degrade in online settings with evolving operational patterns. This work presents a semantic-aware, label-efficient continual learning framework for railway fault diagnostics. Accelerometer signals are encoded via a Variational AutoEncoder into latent representations capturing the normal operational structure in a fully unsupervised manner. Importantly, semantic metadata, including axle counts, wheel indexes, and strain-based deformations, is extracted via AI-driven peak detection on fiber Bragg grating sensors (resistant to electromagnetic interference) and fused with the VAE embeddings, enhancing anomaly detection under unknown operational conditions. A lightweight gradient boosting supervised classifier stabilizes anomaly scoring with minimal labels, while a replay-based continual learning strategy enables adaptation to evolving domains without catastrophic forgetting. Experiments show the model detects minor imperfections due to flats and polygonization, while adapting to evolving operational conditions, such as changes in train type, speed, load, and track profiles, captured using a single accelerometer and strain gauge in wayside monitoring.

</details>


### [46] [On the Power of Source Screening for Learning Shared Feature Extractors](https://arxiv.org/abs/2602.16125)
*Leo,Wang,Connor Mclaughlin,Lili Su*

Main category: cs.LG

TL;DR: 论文研究了在多源学习中如何选择信息量最大的数据源子集进行联合训练，即使丢弃大量数据也能达到统计最优的表示学习效果。


<details>
  <summary>Details</summary>
Motivation: 现有多源学习方法通常同时训练所有相关数据源，但低相关性或低质量的数据源会阻碍表示学习。即使对于传统认为的"好"数据源集合（各源具有相似相关性和质量），也需要进一步筛选哪些源应该联合学习。

Method: 在线性设置下，假设数据源共享低维子空间。提出源筛选方法，识别信息量最大的子集。开发了算法和实用启发式方法来识别这样的子集，包括理论分析和实际实现。

Result: 理论证明对于广泛的问题实例，在精心选择的源子集上训练足以达到极小极大最优性，即使丢弃大量数据。在合成和真实数据集上的实证评估验证了方法的有效性。

Conclusion: 源筛选在多源学习的统计最优子空间估计中起核心作用。识别信息性子集的方法能够提高表示学习效率，即使丢弃部分数据也能保持最优性能。

Abstract: Learning with shared representation is widely recognized as an effective way to separate commonalities from heterogeneity across various heterogeneous sources. Most existing work includes all related data sources via simultaneously training a common feature extractor and source-specific heads. It is well understood that data sources with low relevance or poor quality may hinder representation learning. In this paper, we further dive into the question of which data sources should be learned jointly by focusing on the traditionally deemed ``good'' collection of sources, in which individual sources have similar relevance and qualities with respect to the true underlying common structure. Towards tractability, we focus on the linear setting where sources share a low-dimensional subspace. We find that source screening can play a central role in statistically optimal subspace estimation. We show that, for a broad class of problem instances, training on a carefully selected subset of sources suffices to achieve minimax optimality, even when a substantial portion of data is discarded. We formalize the notion of an informative subpopulation, develop algorithms and practical heuristics for identifying such subsets, and validate their effectiveness through both theoretical analysis and empirical evaluations on synthetic and real-world datasets.

</details>


### [47] [Investigating GNN Convergence on Large Randomly Generated Graphs with Realistic Node Feature Correlations](https://arxiv.org/abs/2602.16145)
*Mohammed Zain Ali Ahmed*

Main category: cs.LG

TL;DR: 该论文提出了一种生成具有相关节点特征的随机图的新方法，挑战了现有GNN收敛性研究的局限性，表明在现实图中GNN可能比先前研究认为的更具表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有GNN收敛性研究大多未考虑节点特征之间的相关性，而这些相关性在现实网络中自然存在。这导致对GNN表达能力的研究结论不能真实反映GNN在实际图上的表现。

Method: 提出了一种生成具有相关节点特征的随机图的新方法，节点特征的采样方式确保相邻节点之间存在相关性。采样方案的设计借鉴了现实图（特别是Barabási-Albert模型）的特性。

Result: 理论分析表明在某些情况下可以避免收敛，这在大规模随机图上得到了经验验证。观察到的发散行为表明GNN在现实图上可能比初始研究认为的更具表达能力。

Conclusion: GNN在具有相关节点特征的现实图上可能比先前收敛性研究认为的更具表达能力，需要重新评估GNN在真实网络中的表现潜力。

Abstract: There are a number of existing studies analysing the convergence behaviour of graph neural networks on large random graphs. Unfortunately, the majority of these studies do not model correlations between node features, which would naturally exist in a variety of real-life networks. Consequently, the derived limitations of GNNs, resulting from such convergence behaviour, is not truly reflective of the expressive power of GNNs when applied to realistic graphs. In this paper, we will introduce a novel method to generate random graphs that have correlated node features. The node features will be sampled in such a manner to ensure correlation between neighbouring nodes. As motivation for our choice of sampling scheme, we will appeal to properties exhibited by real-life graphs, particularly properties that are captured by the Barabási-Albert model. A theoretical analysis will strongly indicate that convergence can be avoided in some cases, which we will empirically validate on large random graphs generated using our novel method. The observed divergent behaviour provides evidence that GNNs may be more expressive than initial studies would suggest, especially on realistic graphs.

</details>


### [48] [Differentially Private Non-convex Distributionally Robust Optimization](https://arxiv.org/abs/2602.16155)
*Difei Xu,Meng Ding,Zebin Ma,Huanyi Xie,Youming Tao,Aicha Slaitane,Di Wang*

Main category: cs.LG

TL;DR: 论文研究了差分隐私下的分布鲁棒优化（DP-DRO），针对非凸损失和ψ-散度，提出了两种隐私保护优化方法，在理论和实验上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实部署面临分布偏移、群体不平衡和对抗扰动，传统经验风险最小化（ERM）在这些情况下性能严重下降。分布鲁棒优化（DRO）通过优化不确定性分布集上的最坏情况损失提供鲁棒性，但训练数据包含敏感信息，需要差分隐私保护。然而DP-DRO研究较少，因其具有不确定性约束的极小极大优化结构。

Method: 1. 针对一般ψ-散度的DRO，将其重构为最小化问题，提出DP Double-Spider方法；2. 针对KL散度的DRO，将其转化为组合有限和优化问题，提出DP Recursive-Spider方法。两种方法均为(ε,δ)-DP优化算法。

Result: 理论分析表明：DP Double-Spider达到梯度范数效用界O(1/√n + (√d log(1/δ)/nε)^{2/3})；DP Recursive-Spider（KL散度）达到O((√d log(1/δ)/nε)^{2/3})，与非凸DP-ERM的最佳已知结果匹配。实验显示所提方法优于现有DP极小极大优化方法。

Conclusion: 本文填补了DP-DRO研究的空白，针对非凸损失和ψ-散度提出了有效的差分隐私优化方法，在理论和实验上均表现出色，为隐私保护的分布鲁棒优化提供了系统解决方案。

Abstract: Real-world deployments routinely face distribution shifts, group imbalances, and adversarial perturbations, under which the traditional Empirical Risk Minimization (ERM) framework can degrade severely.
  Distributionally Robust Optimization (DRO) addresses this issue by optimizing the worst-case expected loss over an uncertainty set of distributions, offering a principled approach to robustness.
  Meanwhile, as training data in DRO always involves sensitive information, safeguarding it against leakage under Differential Privacy (DP) is essential.
  In contrast to classical DP-ERM, DP-DRO has received much less attention due to its minimax optimization structure with uncertainty constraint.
  To bridge the gap, we provide a comprehensive study of DP-(finite-sum)-DRO with $ψ$-divergence and non-convex loss.
  First, we study DRO with general $ψ$-divergence by reformulating it as a minimization problem, and develop a novel $(\varepsilon, δ)$-DP optimization method, called DP Double-Spider, tailored to this structure.
  Under mild assumptions, we show that it achieves a utility bound of $\mathcal{O}(\frac{1}{\sqrt{n}}+ (\frac{\sqrt{d \log (1/δ)}}{n \varepsilon})^{2/3})$ in terms of the gradient norm, where $n$ denotes the data size and $d$ denotes the model dimension.
  We further improve the utility rate for specific divergences.
  In particular, for DP-DRO with KL-divergence, by transforming the problem into a compositional finite-sum optimization problem, we develop a DP Recursive-Spider method and show that it achieves a utility bound of $\mathcal{O}((\frac{\sqrt{d \log(1/δ)}}{n\varepsilon})^{2/3} )$, matching the best-known result for non-convex DP-ERM.
  Experimentally, we demonstrate that our proposed methods outperform existing approaches for DP minimax optimization.

</details>


### [49] [HiPER: Hierarchical Reinforcement Learning with Explicit Credit Assignment for Large Language Model Agents](https://arxiv.org/abs/2602.16165)
*Jiangweizhi Peng,Yuanxin Liu,Ruida Zhou,Charles Fleming,Zhaoran Wang,Alfredo Garcia,Mingyi Hong*

Main category: cs.LG

TL;DR: HiPER：分层规划-执行强化学习框架，通过高层规划与底层执行分离，结合分层优势估计，提升LLM智能体在稀疏奖励长时任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有RL方法将LLM智能体建模为单一时间尺度的平面策略，在稀疏奖励的长时任务中，需要在整个轨迹上传播信用，导致优化不稳定和信用分配效率低下

Method: 提出HiPER分层框架：高层规划器提出子目标，底层执行器在多个动作步骤中执行子目标；引入分层优势估计(HAE)技术，在规划和执行两个层面分配信用

Result: 在ALFWorld上达到97.4%成功率，WebShop上达到83.3%成功率（分别比之前最佳方法提升6.6%和8.3%），在需要多个依赖子任务的长时任务上提升尤其显著

Conclusion: 显式的分层分解对于多轮LLM智能体的可扩展RL训练至关重要，HiPER框架通过分离规划与执行、改进信用分配，显著提升了稀疏奖励长时任务的性能

Abstract: Training LLMs as interactive agents for multi-turn decision-making remains challenging, particularly in long-horizon tasks with sparse and delayed rewards, where agents must execute extended sequences of actions before receiving meaningful feedback. Most existing reinforcement learning (RL) approaches model LLM agents as flat policies operating at a single time scale, selecting one action at each turn. In sparse-reward settings, such flat policies must propagate credit across the entire trajectory without explicit temporal abstraction, which often leads to unstable optimization and inefficient credit assignment.
  We propose HiPER, a novel Hierarchical Plan-Execute RL framework that explicitly separates high-level planning from low-level execution. HiPER factorizes the policy into a high-level planner that proposes subgoals and a low-level executor that carries them out over multiple action steps. To align optimization with this structure, we introduce a key technique called hierarchical advantage estimation (HAE), which carefully assigns credit at both the planning and execution levels. By aggregating returns over the execution of each subgoal and coordinating updates across the two levels, HAE provides an unbiased gradient estimator and provably reduces variance compared to flat generalized advantage estimation.
  Empirically, HiPER achieves state-of-the-art performance on challenging interactive benchmarks, reaching 97.4\% success on ALFWorld and 83.3\% on WebShop with Qwen2.5-7B-Instruct (+6.6\% and +8.3\% over the best prior method), with especially large gains on long-horizon tasks requiring multiple dependent subtasks. These results highlight the importance of explicit hierarchical decomposition for scalable RL training of multi-turn LLM agents.

</details>


### [50] [Muon with Spectral Guidance: Efficient Optimization for Scientific Machine Learning](https://arxiv.org/abs/2602.16167)
*Binghang Lu,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: 提出SpecMuon优化器，结合Muon的正交化几何与模态松弛标量辅助变量机制，通过奇异模态分解和RSAV更新，改善物理信息神经网络和神经算子的优化问题


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络和神经算子常因梯度病态、多尺度谱行为和物理约束导致的刚度而面临优化困难。Muon优化器通过奇异向量基的正交化更新改善了几何条件，但其单位奇异值更新可能导致步长过于激进，且缺乏稳定性保证

Method: 提出SpecMuon优化器，将矩阵值梯度分解为奇异模态，沿主导谱方向单独应用RSAV更新，根据全局损失能量自适应调节步长，同时保持Muon的尺度平衡特性。将优化解释为多模态梯度流，实现对刚性谱分量的原理性控制

Result: 建立了SpecMuon的严格理论性质，包括修正的能量耗散定律、辅助变量的正性和有界性，以及在Polyak-Lojasiewicz条件下的全局线性收敛率。在一维Burgers方程和分数偏微分方程等基准问题上，SpecMuon相比Adam、AdamW和原始Muon优化器实现了更快的收敛和更好的稳定性

Conclusion: SpecMuon通过谱感知优化机制有效解决了物理信息学习中的优化困难，结合了Muon的正交化几何优势与RSAV的稳定性控制，为物理信息神经网络和神经算子提供了更可靠、高效的优化方案

Abstract: Physics-informed neural networks and neural operators often suffer from severe optimization difficulties caused by ill-conditioned gradients, multi-scale spectral behavior, and stiffness induced by physical constraints. Recently, the Muon optimizer has shown promise by performing orthogonalized updates in the singular-vector basis of the gradient, thereby improving geometric conditioning. However, its unit-singular-value updates may lead to overly aggressive steps and lack explicit stability guarantees when applied to physics-informed learning. In this work, we propose SpecMuon, a spectral-aware optimizer that integrates Muon's orthogonalized geometry with a mode-wise relaxed scalar auxiliary variable (RSAV) mechanism. By decomposing matrix-valued gradients into singular modes and applying RSAV updates individually along dominant spectral directions, SpecMuon adaptively regulates step sizes according to the global loss energy while preserving Muon's scale-balancing properties. This formulation interprets optimization as a multi-mode gradient flow and enables principled control of stiff spectral components. We establish rigorous theoretical properties of SpecMuon, including a modified energy dissipation law, positivity and boundedness of auxiliary variables, and global convergence with a linear rate under the Polyak-Lojasiewicz condition. Numerical experiments on physics-informed neural networks, DeepONets, and fractional PINN-DeepONets demonstrate that SpecMuon achieves faster convergence and improved stability compared with Adam, AdamW, and the original Muon optimizer on benchmark problems such as the one-dimensional Burgers equation and fractional partial differential equations.

</details>


### [51] [Discrete Stochastic Localization for Non-autoregressive Generation](https://arxiv.org/abs/2602.16169)
*Yunshu Wu,Jiayi Cheng,Partha Thakuria,Rob Brekelmans,Evangelos E. Papalexakis,Greg Ver Steeg*

Main category: cs.LG

TL;DR: 本文提出DSL方法，通过单一SNR不变的去噪器训练，显著提升掩码扩散语言模型的采样效率，在低步数预算下获得更好的生成质量。


<details>
  <summary>Details</summary>
Motivation: 非自回归生成虽然能并行预测多个token减少解码延迟，但迭代优化常面临错误累积和自生成草稿下的分布偏移问题。掩码扩散语言模型及其重掩码采样器可视为现代NAR迭代优化方法，但需要提高其步数效率。

Method: 提出DSL（离散随机定位）方法，训练单一SNR不变的去噪器跨越连续的信噪比水平，在一个扩散Transformer中桥接中间草稿噪声和掩码式端点损坏。

Result: 在OpenWebText上，DSL微调在低步数预算下获得显著MAUVE提升，相比MDLM+ReMDM基线减少约4倍去噪器评估次数，在高预算下匹配自回归生成质量。分析显示改进了自校正和不确定性校准。

Conclusion: DSL通过改进训练方法显著提升了重掩码采样的计算效率，使掩码扩散语言模型在保持高质量的同时大幅减少推理成本。

Abstract: Non-autoregressive (NAR) generation reduces decoding latency by predicting many tokens in parallel, but iterative refinement often suffers from error accumulation and distribution shift under self-generated drafts. Masked diffusion language models (MDLMs) and their remasking samplers (e.g., ReMDM) can be viewed as modern NAR iterative refinement, where generation repeatedly revises a partially observed draft. In this work we show that \emph{training alone} can substantially improve the step-efficiency of MDLM/ReMDM sampling. We propose \textsc{DSL} (Discrete Stochastic Localization), which trains a single SNR-invariant denoiser across a continuum of corruption levels, bridging intermediate draft noise and mask-style endpoint corruption within one Diffusion Transformer. On OpenWebText, \textsc{DSL} fine-tuning yields large MAUVE gains at low step budgets, surpassing the MDLM+ReMDM baseline with \(\sim\)4$\times$ fewer denoiser evaluations, and matches autoregressive quality at high budgets. Analyses show improved self-correction and uncertainty calibration, making remasking markedly more compute-efficient.

</details>


### [52] [Towards Secure and Scalable Energy Theft Detection: A Federated Learning Approach for Resource-Constrained Smart Meters](https://arxiv.org/abs/2602.16181)
*Diego Labate,Dipanwita Thakur,Giancarlo Fortino*

Main category: cs.LG

TL;DR: 提出一种基于联邦学习的隐私保护能源盗窃检测框架，使用轻量级MLP模型和差分隐私技术，在保护用户隐私的同时实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 能源盗窃对智能电网稳定性和效率构成重大威胁，传统集中式机器学习方法需要聚合用户数据，引发隐私和安全担忧。智能电表设备资源受限，无法运行重型模型。

Method: 提出隐私保护联邦学习框架，采用轻量级多层感知器（MLP）模型适合低功耗智能电表部署，集成基本差分隐私（DP）技术，在本地模型更新聚合前注入高斯噪声。

Result: 在真实智能电表数据集上评估，在IID和非IID数据分布下均表现出色，达到有竞争力的准确率、精确率、召回率和AUC分数，同时保持隐私和效率。

Conclusion: 该解决方案实用且可扩展，适用于下一代智能电网基础设施的安全能源盗窃检测，在保护隐私的同时不损害学习性能。

Abstract: Energy theft poses a significant threat to the stability and efficiency of smart grids, leading to substantial economic losses and operational challenges. Traditional centralized machine learning approaches for theft detection require aggregating user data, raising serious concerns about privacy and data security. These issues are further exacerbated in smart meter environments, where devices are often resource-constrained and lack the capacity to run heavy models. In this work, we propose a privacy-preserving federated learning framework for energy theft detection that addresses both privacy and computational constraints. Our approach leverages a lightweight multilayer perceptron (MLP) model, suitable for deployment on low-power smart meters, and integrates basic differential privacy (DP) by injecting Gaussian noise into local model updates before aggregation. This ensures formal privacy guarantees without compromising learning performance. We evaluate our framework on a real-world smart meter dataset under both IID and non-IID data distributions. Experimental results demonstrate that our method achieves competitive accuracy, precision, recall, and AUC scores while maintaining privacy and efficiency. This makes the proposed solution practical and scalable for secure energy theft detection in next-generation smart grid infrastructures.

</details>


### [53] [Deep TPC: Temporal-Prior Conditioning for Time Series Forecasting](https://arxiv.org/abs/2602.16188)
*Filippos Bellos,NaveenJohn Premkumar,Yannis Avrithis,Nam H. Nguyen,Jason J. Corso*

Main category: cs.LG

TL;DR: 提出Temporal-Prior Conditioning (TPC)方法，将时间作为第一类模态，通过多层次的跨注意力机制将时间信息注入LLM，在保持低参数预算的同时提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM处理时间序列的方法通常只在输入层浅层地注入时间信息（如位置编码或提示），导致时间信息在深层网络中逐渐退化，限制了模型的时间推理能力。

Method: TPC将时间提升为第一类模态：1) 在patch流中添加可学习的时间序列token；2) 在选定层中，这些token通过跨注意力机制关注由同一冻结LLM编码的紧凑、人类可读时间描述符生成的时间嵌入；3) 通过自注意力将时间上下文反馈回模型，实现时间序列信号与时间信息的解耦。

Result: TPC在仅训练跨注意力模块的情况下，在多个数据集上超越了完全微调和浅层条件策略，在长期预测任务中取得了最先进的性能。

Conclusion: 通过将时间作为第一类模态进行多层次条件化，并解耦时间序列信号与时间信息，TPC能够更有效地进行时间推理，在保持低参数预算的同时显著提升预测性能。

Abstract: LLM-for-time series (TS) methods typically treat time shallowly, injecting positional or prompt-based cues once at the input of a largely frozen decoder, which limits temporal reasoning as this information degrades through the layers. We introduce Temporal-Prior Conditioning (TPC), which elevates time to a first-class modality that conditions the model at multiple depths. TPC attaches a small set of learnable time series tokens to the patch stream; at selected layers these tokens cross-attend to temporal embeddings derived from compact, human-readable temporal descriptors encoded by the same frozen LLM, then feed temporal context back via self-attention. This disentangles time series signal and temporal information while maintaining a low parameter budget. We show that by training only the cross-attention modules and explicitly disentangling time series signal and temporal information, TPC consistently outperforms both full fine-tuning and shallow conditioning strategies, achieving state-of-the-art performance in long-term forecasting across diverse datasets. Code available at: https://github.com/fil-mp/Deep_tpc

</details>


### [54] [Rethinking Input Domains in Physics-Informed Neural Networks via Geometric Compactification Mappings](https://arxiv.org/abs/2602.16193)
*Zhenzhen Huang,Haoyu Bian,Jiaquan Zhang,Yibei Liu,Kuien Liu,Caiyan Qin,Guoqing Wang,Yang Yang,Chaoning Zhang*

Main category: cs.LG

TL;DR: 提出GC-PINN框架，通过几何紧致化映射解决PINN在多尺度PDE中的梯度刚性问题，无需修改PINN架构即可提升训练稳定性和求解精度。


<details>
  <summary>Details</summary>
Motivation: 多尺度PDE同时包含平滑低频分量和局部高频结构，传统PINN使用固定坐标系输入会导致几何错位，引发梯度刚性和病态条件问题，阻碍收敛。

Method: 引入几何紧致化映射范式，通过可微分几何紧致化映射重塑输入坐标，将PDE的几何结构与残差算子的谱特性耦合。提出三种映射策略：周期性边界、远场尺度扩展和局部奇异结构处理。

Result: 在代表性1D和2D PDE上的广泛实验表明，该方法能产生更均匀的残差分布和更高的求解精度，同时改善训练稳定性和收敛速度。

Conclusion: GC-PINN框架通过几何映射有效解决了PINN在多尺度PDE中的收敛问题，为处理复杂物理系统提供了新的范式，无需修改底层PINN架构。

Abstract: Several complex physical systems are governed by multi-scale partial differential equations (PDEs) that exhibit both smooth low-frequency components and localized high-frequency structures. Existing physics-informed neural network (PINN) methods typically train with fixed coordinate system inputs, where geometric misalignment with these structures induces gradient stiffness and ill-conditioning that hinder convergence. To address this issue, we introduce a mapping paradigm that reshapes the input coordinates through differentiable geometric compactification mappings and couples the geometric structure of PDEs with the spectral properties of residual operators. Based on this paradigm, we propose Geometric Compactification (GC)-PINN, a framework that introduces three mapping strategies for periodic boundaries, far-field scale expansion, and localized singular structures in the input domain without modifying the underlying PINN architecture. Extensive empirical evaluation demonstrates that this approach yields more uniform residual distributions and higher solution accuracy on representative 1D and 2D PDEs, while improving training stability and convergence speed.

</details>


### [55] [Graphon Mean-Field Subsampling for Cooperative Heterogeneous Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.16196)
*Emile Anand,Richard Hoffmann,Sarah Liaw,Adam Wierman*

Main category: cs.LG

TL;DR: 提出GMFS框架，通过子采样κ个代理来近似图权平均场，实现可扩展的异构多智能体强化学习，样本复杂度为poly(κ)，最优性差距为O(1/√κ)。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，随着智能体数量增加，联合状态-动作空间呈指数级增长。现有平均场方法假设同质交互，而基于图的方法虽然能捕捉异质性但计算成本高。因此需要一种既能处理异质性又能保持可扩展性的方法。

Method: 提出GMFS（图权平均场子采样）框架，通过根据交互强度子采样κ个智能体，近似图权平均场，从而学习策略。该方法将样本复杂度降低到poly(κ)级别。

Result: 理论分析表明，GMFS的样本复杂度为poly(κ)，最优性差距为O(1/√κ)。在机器人协调任务的数值模拟中验证了该框架，显示GMFS能够实现接近最优的性能。

Conclusion: GMFS框架成功解决了异构多智能体强化学习的可扩展性问题，通过子采样方法在保持异质性建模能力的同时显著降低了计算复杂度，为大规模协作MARL提供了实用解决方案。

Abstract: Coordinating large populations of interacting agents is a central challenge in multi-agent reinforcement learning (MARL), where the size of the joint state-action space scales exponentially with the number of agents. Mean-field methods alleviate this burden by aggregating agent interactions, but these approaches assume homogeneous interactions. Recent graphon-based frameworks capture heterogeneity, but are computationally expensive as the number of agents grows. Therefore, we introduce $\texttt{GMFS}$, a $\textbf{G}$raphon $\textbf{M}$ean-$\textbf{F}$ield $\textbf{S}$ubsampling framework for scalable cooperative MARL with heterogeneous agent interactions. By subsampling $κ$ agents according to interaction strength, we approximate the graphon-weighted mean-field and learn a policy with sample complexity $\mathrm{poly}(κ)$ and optimality gap $O(1/\sqrtκ)$. We verify our theory with numerical simulations in robotic coordination, showing that $\texttt{GMFS}$ achieves near-optimal performance.

</details>


### [56] [ModalImmune: Immunity Driven Unlearning via Self Destructive Training](https://arxiv.org/abs/2602.16197)
*Rong Fu,Jia Yee Tan,Wenxin Zhang,Zijian Zhang,Ziming Wang,Zhaolu Kang,Muge Qi,Shuning Zhang,Simon Fong*

Main category: cs.LG

TL;DR: ModalImmune训练框架通过可控地破坏模态信息，使多模态系统对输入通道丢失具有鲁棒性


<details>
  <summary>Details</summary>
Motivation: 多模态系统在部署时容易受到部分或完全输入通道丢失的影响，这会削弱其在真实世界环境中的可靠性

Method: 结合频谱自适应崩溃正则化、信息增益引导的控制器、曲率感知梯度掩码和认证的Neumann截断超梯度程序，在训练中可控地破坏选定模态信息

Result: 在标准多模态基准测试中，ModalImmune提高了对模态移除和损坏的鲁棒性，同时保持了收敛稳定性和重建能力

Conclusion: ModalImmune框架通过强制模态免疫性，使多模态模型能够学习对破坏性模态影响具有鲁棒性的联合表示

Abstract: Multimodal systems are vulnerable to partial or complete loss of input channels at deployment, which undermines reliability in real-world settings. This paper presents ModalImmune, a training framework that enforces modality immunity by intentionally and controllably collapsing selected modality information during training so the model learns joint representations that are robust to destructive modality influence. The framework combines a spectrum-adaptive collapse regularizer, an information-gain guided controller for targeted interventions, curvature-aware gradient masking to stabilize destructive updates, and a certified Neumann-truncated hyper-gradient procedure for automatic meta-parameter adaptation. Empirical evaluation on standard multimodal benchmarks demonstrates that ModalImmune improves resilience to modality removal and corruption while retaining convergence stability and reconstruction capacity.

</details>


### [57] [Training-Free Adaptation of Diffusion Models via Doob's $h$-Transform](https://arxiv.org/abs/2602.16198)
*Qijie Zhu,Zeqi Ye,Han Liu,Zhaoran Wang,Minshuo Chen*

Main category: cs.LG

TL;DR: DOIT：一种无需训练、计算高效的扩散模型适应方法，利用Doob's h-transform实现生成分布到高奖励目标分布的转换，适用于不可微分的通用奖励函数。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型适应方法存在计算开销大、依赖奖励函数可微分性假设、缺乏理论保证等问题。需要一种无需训练、计算高效、适用于通用非可微分奖励函数且有理论保证的适应方法。

Method: 提出DOIT方法，基于测度传输框架，利用Doob's h-transform将预训练生成分布传输到高奖励目标分布。通过动态修正扩散采样过程实现，无需修改预训练模型，支持基于模拟的高效计算。

Result: 理论上建立了到目标高奖励分布的高概率收敛保证；实证上在D4RL离线RL基准测试中持续优于最先进基线方法，同时保持采样效率。

Conclusion: DOIT是一种无需训练、计算高效的扩散模型适应方法，适用于通用非可微分奖励函数，具有理论保证，在离线RL任务中表现出色。

Abstract: Adaptation methods have been a workhorse for unlocking the transformative power of pre-trained diffusion models in diverse applications. Existing approaches often abstract adaptation objectives as a reward function and steer diffusion models to generate high-reward samples. However, these approaches can incur high computational overhead due to additional training, or rely on stringent assumptions on the reward such as differentiability. Moreover, despite their empirical success, theoretical justification and guarantees are seldom established. In this paper, we propose DOIT (Doob-Oriented Inference-time Transformation), a training-free and computationally efficient adaptation method that applies to generic, non-differentiable rewards. The key framework underlying our method is a measure transport formulation that seeks to transport the pre-trained generative distribution to a high-reward target distribution. We leverage Doob's $h$-transform to realize this transport, which induces a dynamic correction to the diffusion sampling process and enables efficient simulation-based computation without modifying the pre-trained model. Theoretically, we establish a high probability convergence guarantee to the target high-reward distribution via characterizing the approximation error in the dynamic Doob's correction. Empirically, on D4RL offline RL benchmarks, our method consistently outperforms state-of-the-art baselines while preserving sampling efficiency.

</details>


### [58] [Linked Data Classification using Neurochaos Learning](https://arxiv.org/abs/2602.16204)
*Pooja Honna,Ayush Patravali,Nithin Nagaraj,Nanjangud C. Narendra*

Main category: cs.LG

TL;DR: 该论文将神经混沌学习应用于知识图谱数据，通过节点聚合将图数据输入到ChaosNet架构中，在同性图和异性图数据集上测试性能。


<details>
  <summary>Details</summary>
Motivation: 神经混沌学习在小样本学习和低计算需求方面表现出优势，但之前主要应用于可分离数据和时序数据。本研究旨在将NL扩展到链接数据领域，特别是知识图谱数据。

Method: 在知识图谱上实现节点聚合，将聚合后的节点特征输入到最简单的NL架构ChaosNet中。在同性图和不同异性程度的异性图数据集上进行测试。

Result: 该方法在同性图上的效果优于异性图。研究提供了结果分析并提出了未来工作建议。

Conclusion: 成功将神经混沌学习应用于知识图谱数据，证明了NL在链接数据上的可行性，特别是在同性图数据集上表现更好。

Abstract: Neurochaos Learning (NL) has shown promise in recent times over traditional deep learning due to its two key features: ability to learn from small sized training samples, and low compute requirements. In prior work, NL has been implemented and extensively tested on separable and time series data, and demonstrated its superior performance on both classification and regression tasks. In this paper, we investigate the next step in NL, viz., applying NL to linked data, in particular, data that is represented in the form of knowledge graphs. We integrate linked data into NL by implementing node aggregation on knowledge graphs, and then feeding the aggregated node features to the simplest NL architecture: ChaosNet. We demonstrate the results of our implementation on homophilic graph datasets as well as heterophilic graph datasets of verying heterophily. We show better efficacy of our approach on homophilic graphs than on heterophilic graphs. While doing so, we also present our analysis of the results, as well as suggestions for future work.

</details>


### [59] [Geometric Neural Operators via Lie Group-Constrained Latent Dynamics](https://arxiv.org/abs/2602.16209)
*Jiaquan Zhang,Fachrina Dewi Puspitasari,Songbo Zhang,Yibei Liu,Kuien Liu,Caiyan Qin,Fan Mo,Peng Wang,Yang Yang,Chaoning Zhang*

Main category: cs.LG

TL;DR: 提出MCL方法，通过低秩李代数参数化约束流形，将群作用更新应用于神经算子的潜在表示，提升长期预测稳定性


<details>
  <summary>Details</summary>
Motivation: 现有神经算子在多层迭代和长期推演中存在不稳定性，源于无约束的欧几里得潜在空间更新违反了几何和守恒定律

Method: 提出基于李群的流形约束方法(MCL)，使用低秩李代数参数化约束流形，在潜在表示上执行群作用更新，作为即插即用模块增强现有神经算子

Result: 在1-D Burgers和2-D Navier-Stokes等偏微分方程上的广泛实验表明，该方法在仅增加2.26%参数的情况下，能将相对预测误差降低30-50%

Conclusion: MCL方法通过引入几何归纳偏置，为神经算子更新中缺失的几何约束提供了可扩展解决方案，显著提升了长期预测保真度

Abstract: Neural operators offer an effective framework for learning solutions of partial differential equations for many physical systems in a resolution-invariant and data-driven manner. Existing neural operators, however, often suffer from instability in multi-layer iteration and long-horizon rollout, which stems from the unconstrained Euclidean latent space updates that violate the geometric and conservation laws. To address this challenge, we propose to constrain manifolds with low-rank Lie algebra parameterization that performs group action updates on the latent representation. Our method, termed Manifold Constraining based on Lie group (MCL), acts as an efficient \emph{plug-and-play} module that enforces geometric inductive bias to existing neural operators. Extensive experiments on various partial differential equations, such as 1-D Burgers and 2-D Navier-Stokes, over a wide range of parameters and steps demonstrate that our method effectively lowers the relative prediction error by 30-50\% at the cost of 2.26\% of parameter increase. The results show that our approach provides a scalable solution for improving long-term prediction fidelity by addressing the principled geometric constraints absent in the neural operator updates.

</details>


### [60] [Graph neural network for colliding particles with an application to sea ice floe modeling](https://arxiv.org/abs/2602.16213)
*Ruibiao Zhu*

Main category: cs.LG

TL;DR: 提出基于图神经网络的海冰建模新方法，利用海冰自然图结构（节点为冰片，边为物理相互作用），结合数据同化技术，在合成数据上验证了轨迹模拟的加速效果。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法计算密集且扩展性差，需要更高效的海冰动力学建模方法，特别是在边缘冰区（MIZ）的预测中。

Method: 使用图神经网络（GNN）构建碰撞捕获网络（CN），将海冰建模为图结构（节点=冰片，边=物理相互作用），结合数据同化（DA）技术，在一维框架中开发基础模型。

Result: 在合成数据上验证，无论有无观测数据点，模型都能在不损失精度的情况下加速轨迹模拟，为边缘冰区预测提供更高效工具。

Conclusion: 该方法展示了机器学习与数据同化结合在海冰建模中的潜力，为高效准确的海冰动力学预测提供了新途径。

Abstract: This paper introduces a novel approach to sea ice modeling using Graph Neural Networks (GNNs), utilizing the natural graph structure of sea ice, where nodes represent individual ice pieces, and edges model the physical interactions, including collisions. This concept is developed within a one-dimensional framework as a foundational step. Traditional numerical methods, while effective, are computationally intensive and less scalable. By utilizing GNNs, the proposed model, termed the Collision-captured Network (CN), integrates data assimilation (DA) techniques to effectively learn and predict sea ice dynamics under various conditions. The approach was validated using synthetic data, both with and without observed data points, and it was found that the model accelerates the simulation of trajectories without compromising accuracy. This advancement offers a more efficient tool for forecasting in marginal ice zones (MIZ) and highlights the potential of combining machine learning with data assimilation for more effective and efficient modeling.

</details>


### [61] [UCTECG-Net: Uncertainty-aware Convolution Transformer ECG Network for Arrhythmia Detection](https://arxiv.org/abs/2602.16216)
*Hamzeh Asgharnezhad,Pegah Tabarisaadi,Abbas Khosravi,Roohallah Alizadehsani,U. Rajendra Acharya*

Main category: cs.LG

TL;DR: 提出UCTECG-Net，一种结合1D卷积和Transformer编码器的混合架构，用于ECG分类，并集成不确定性量化方法以提高预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在ECG分类中表现优异，但缺乏对预测可靠性的洞察，限制了其在安全关键场景中的应用。需要开发能提供不确定性估计的模型。

Method: 提出UCTECG-Net混合架构，结合1D卷积和Transformer编码器处理原始ECG信号及其频谱图。集成三种不确定性量化方法：蒙特卡洛Dropout、深度集成和集成蒙特卡洛Dropout。使用不确定性感知混淆矩阵和衍生指标进行评估。

Result: 在MIT-BIH和PTB数据集上，UCTECG-Net在准确率、精确率、召回率和F1分数上均优于LSTM、CNN1D和Transformer基线，MIT-BIH准确率达98.58%，PTB达99.14%。UCTECG-Net（特别是集成或EMCD方法）提供更可靠、更好对齐的不确定性估计。

Conclusion: UCTECG-Net结合不确定性量化方法，为风险感知的ECG决策支持提供了更可靠的基础，在安全关键医疗应用中具有潜力。

Abstract: Deep learning has improved automated electrocardiogram (ECG) classification, but limited insight into prediction reliability hinders its use in safety-critical settings. This paper proposes UCTECG-Net, an uncertainty-aware hybrid architecture that combines one-dimensional convolutions and Transformer encoders to process raw ECG signals and their spectrograms jointly. Evaluated on the MIT-BIH Arrhythmia and PTB Diagnostic datasets, UCTECG-Net outperforms LSTM, CNN1D, and Transformer baselines in terms of accuracy, precision, recall and F1 score, achieving up to 98.58% accuracy on MIT-BIH and 99.14% on PTB. To assess predictive reliability, we integrate three uncertainty quantification methods (Monte Carlo Dropout, Deep Ensembles, and Ensemble Monte Carlo Dropout) into all models and analyze their behavior using an uncertainty-aware confusion matrix and derived metrics. The results show that UCTECG-Net, particularly with Ensemble or EMCD, provides more reliable and better-aligned uncertainty estimates than competing architectures, offering a stronger basis for risk-aware ECG decision support.

</details>


### [62] [Multi-Class Boundary Extraction from Implicit Representations](https://arxiv.org/abs/2602.16217)
*Jash Vira,Andrew Myers,Simon Ratcliffe*

Main category: cs.LG

TL;DR: 提出首个用于多类别隐式神经表示的拓扑一致、无空洞边界提取算法，支持细节约束，在地质建模数据上验证


<details>
  <summary>Details</summary>
Motivation: 目前没有能从多类别隐式表示中提取拓扑正确、无空洞表面的方法，需要解决这一技术空白

Method: 开发了2D多类别边界提取算法，保证拓扑一致性和水密性，允许设置最小细节约束

Result: 算法在地质建模数据上评估，展示了其适应性和处理复杂拓扑结构的能力

Conclusion: 为多类别隐式表示的表面提取奠定了基础，提供了拓扑一致且水密的边界提取解决方案

Abstract: Surface extraction from implicit neural representations modelling a single class surface is a well-known task. However, there exist no surface extraction methods from an implicit representation of multiple classes that guarantee topological correctness and no holes. In this work, we lay the groundwork by introducing a 2D boundary extraction algorithm for the multi-class case focusing on topological consistency and water-tightness, which also allows for setting minimum detail restraint on the approximation. Finally, we evaluate our algorithm using geological modelling data, showcasing its adaptiveness and ability to honour complex topology.

</details>


### [63] [SEMixer: Semantics Enhanced MLP-Mixer for Multiscale Mixing and Long-term Time Series Forecasting](https://arxiv.org/abs/2602.16220)
*Xu Zhang,Qitong Wang,Peng Wang,Wei Wang*

Main category: cs.LG

TL;DR: SEMixer是一个轻量级多尺度时间序列预测模型，通过随机注意力机制和多尺度渐进混合链有效捕捉多尺度模式，在多个公开数据集和实际无线网络数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 时间序列中的冗余和噪声，以及非相邻尺度间的语义鸿沟，使得多尺度时间依赖的高效对齐和整合变得困难，需要设计更有效的多尺度建模方法。

Method: 提出SEMixer模型，包含两个核心组件：1) 随机注意力机制(RAM)：在训练时捕捉多样化的时间片段交互，在推理时通过dropout集成聚合，增强片段级语义；2) 多尺度渐进混合链(MPMC)：以内存高效的方式堆叠RAM和MLP-Mixer，实现更有效的时间混合。

Result: 在10个公开数据集上验证了SEMixer的有效性，并在基于21GB真实无线网络数据的2025 CCF AlOps挑战赛中获得第三名。

Conclusion: SEMixer通过创新的随机注意力机制和多尺度渐进混合链，有效解决了多尺度时间序列预测中的语义鸿沟问题，实现了轻量级且高效的多尺度建模。

Abstract: Modeling multiscale patterns is crucial for long-term time series forecasting (TSF). However, redundancy and noise in time series, together with semantic gaps between non-adjacent scales, make the efficient alignment and integration of multi-scale temporal dependencies challenging. To address this, we propose SEMixer, a lightweight multiscale model designed for long-term TSF. SEMixer features two key components: a Random Attention Mechanism (RAM) and a Multiscale Progressive Mixing Chain (MPMC). RAM captures diverse time-patch interactions during training and aggregates them via dropout ensemble at inference, enhancing patch-level semantics and enabling MLP-Mixer to better model multi-scale dependencies. MPMC further stacks RAM and MLP-Mixer in a memory-efficient manner, achieving more effective temporal mixing. It addresses semantic gaps across scales and facilitates better multiscale modeling and forecasting performance. We not only validate the effectiveness of SEMixer on 10 public datasets, but also on the \textit{2025 CCF AlOps Challenge} based on 21GB real wireless network data, where SEMixer achieves third place. The code is available at the link https://github.com/Meteor-Stars/SEMixer.

</details>


### [64] [Amortized Predictability-aware Training Framework for Time Series Forecasting and Classification](https://arxiv.org/abs/2602.16224)
*Xu Zhang,Peng Wang,Yichen Li,Wei Wang*

Main category: cs.LG

TL;DR: 提出APTF框架，通过分层可预测性感知损失和摊销模型，动态识别并惩罚低可预测性样本，提升时间序列预测和分类性能


<details>
  <summary>Details</summary>
Motivation: 时间序列数据常包含噪声和低可预测性模式，这些样本会偏离正常数据分布，导致训练不稳定或陷入局部最优，现有深度学习模型很少考虑如何识别和惩罚这些样本

Method: 提出APTF框架，包含两个关键设计：1) 分层可预测性感知损失(HPL)，动态识别低可预测性样本并逐步扩大其损失惩罚；2) 摊销模型，减轻模型偏差导致的可预测性估计误差

Result: APTF在时间序列预测和分类任务上均表现出色，代码已开源

Conclusion: APTF通过动态识别和适当惩罚低可预测性样本，有效提升了时间序列分析任务的性能，为训练视角的改进提供了新思路

Abstract: Time series data are prone to noise in various domains, and training samples may contain low-predictability patterns that deviate from the normal data distribution, leading to training instability or convergence to poor local minima. Therefore, mitigating the adverse effects of low-predictability samples is crucial for time series analysis tasks such as time series forecasting (TSF) and time series classification (TSC). While many deep learning models have achieved promising performance, few consider how to identify and penalize low-predictability samples to improve model performance from the training perspective. To fill this gap, we propose a general Amortized Predictability-aware Training Framework (APTF) for both TSF and TSC. APTF introduces two key designs that enable the model to focus on high-predictability samples while still learning appropriately from low-predictability ones: (i) a Hierarchical Predictability-aware Loss (HPL) that dynamically identifies low-predictability samples and progressively expands their loss penalty as training evolves, and (ii) an amortization model that mitigates predictability estimation errors caused by model bias, further enhancing HPL's effectiveness. The code is available at https://github.com/Meteor-Stars/APTF.

</details>


### [65] [Factored Latent Action World Models](https://arxiv.org/abs/2602.16229)
*Zizhao Wang,Chang Shi,Jiaheng Hu,Kevin Rohling,Roberto Martín-Martín,Amy Zhang,Peter Stone*

Main category: cs.LG

TL;DR: FLAM提出了一种分解的潜在动作模型，将场景分解为独立因子，每个因子推断自己的潜在动作并预测下一步因子值，在复杂多实体环境中优于传统的单一潜在动作模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用单一潜在动作控制整个场景，在复杂多实体同时行动的环境中表现不佳。需要一种能够分解场景、独立建模各实体动态的方法。

Method: 提出分解潜在动作模型（FLAM），将场景分解为独立因子，每个因子推断自己的潜在动作并预测下一步因子值，实现多实体动态的精确建模。

Result: 在仿真和真实世界多实体数据集上的实验表明，FLAM在预测准确性和表示质量上优于先前工作，并能促进下游策略学习。

Conclusion: 分解的潜在动作模型在复杂多实体环境中具有优势，能提高视频生成质量并支持更好的控制接口。

Abstract: Learning latent actions from action-free video has emerged as a powerful paradigm for scaling up controllable world model learning. Latent actions provide a natural interface for users to iteratively generate and manipulate videos. However, most existing approaches rely on monolithic inverse and forward dynamics models that learn a single latent action to control the entire scene, and therefore struggle in complex environments where multiple entities act simultaneously. This paper introduces Factored Latent Action Model (FLAM), a factored dynamics framework that decomposes the scene into independent factors, each inferring its own latent action and predicting its own next-step factor value. This factorized structure enables more accurate modeling of complex multi-entity dynamics and improves video generation quality in action-free video settings compared to monolithic models. Based on experiments on both simulation and real-world multi-entity datasets, we find that FLAM outperforms prior work in prediction accuracy and representation quality, and facilitates downstream policy learning, demonstrating the benefits of factorized latent action models.

</details>


### [66] [Online Prediction of Stochastic Sequences with High Probability Regret Bounds](https://arxiv.org/abs/2602.16236)
*Matthias Frey,Jonathan H. Manton,Jingge Zhu*

Main category: cs.LG

TL;DR: 该论文研究了有限时间T已知情况下随机序列的通用预测问题，提出了高概率的遗憾界，补充了现有文献中的期望界。


<details>
  <summary>Details</summary>
Motivation: 现有文献主要提供了通用预测的期望遗憾界，但缺乏高概率保证。作者希望填补这一空白，为随机序列预测提供具有高概率保证的遗憾界。

Method: 针对可数字母表上的随机过程通用预测问题，作者提出了高概率遗憾界，其形式与先前的期望界相似，但包含概率参数δ。

Result: 对于可数字母表上的随机过程，作者获得了收敛率为O(T^{-1/2} δ^{-1/2})的高概率遗憾界，概率至少为1-δ。同时证明了在不做额外假设的情况下，无法改进δ的指数。

Conclusion: 该工作为随机序列的通用预测提供了高概率遗憾界，填补了现有文献的空白，并建立了这些界限的紧致性，表明在不增加假设的情况下无法改进δ的指数。

Abstract: We revisit the classical problem of universal prediction of stochastic sequences with a finite time horizon $T$ known to the learner. The question we investigate is whether it is possible to derive vanishing regret bounds that hold with high probability, complementing existing bounds from the literature that hold in expectation. We propose such high-probability bounds which have a very similar form as the prior expectation bounds. For the case of universal prediction of a stochastic process over a countable alphabet, our bound states a convergence rate of $\mathcal{O}(T^{-1/2} δ^{-1/2})$ with probability as least $1-δ$ compared to prior known in-expectation bounds of the order $\mathcal{O}(T^{-1/2})$. We also propose an impossibility result which proves that it is not possible to improve the exponent of $δ$ in a bound of the same form without making additional assumptions.

</details>


### [67] [Prediction of Major Solar Flares Using Interpretable Class-dependent Reward Framework with Active Region Magnetograms and Domain Knowledge](https://arxiv.org/abs/2602.16264)
*Zixian Wu,Xuebao Li,Yanfang Zheng,Rui Wang,Shunhuang Zhang,Jinfang Wei,Yongshang Lv,Liang Dong,Zamri Zainal Abidin,Noraisyah Mohamed Shah,Hongwei Ye,Pengchao Yan,Xuefeng Li,Xiaojia Ji,Xusheng Huang,Xiaotian Wang,Honglei Jin*

Main category: cs.LG

TL;DR: 首次开发了基于类别依赖奖励（CDR）的监督分类框架来预测24小时内≥M级太阳耀斑，使用多种数据集和深度学习模型，CDR-Transformer在所有模型中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 需要改进太阳耀斑预测的准确性和可靠性，特别是在24小时内预测≥M级耀斑的能力，以更好地支持空间天气预报和灾害防范。

Method: 构建了包含知识驱动特征和视线磁图的数据集，应用了三种深度学习模型（CNN、CNN-BiLSTM、Transformer）及其CDR版本，进行了特征重要性分析、模型性能比较、奖励工程敏感性分析和SHAP可解释性分析。

Result: 1) R_VALUE和AREA_ACR是最重要的视线磁场参数；2) 结合视线和矢量磁场数据的Transformer表现更好；3) 知识驱动特征模型优于磁图模型；4) CDR-Transformer在所有模型中表现最佳；5) CDR模型对奖励选择不敏感；6) CDR模型更重视TOTUSJH，Transformer更重视R_VALUE；7) CDR-Transformer优于NASA/CCMC。

Conclusion: CDR-Transformer框架在太阳耀斑预测中表现出色，特别是在使用知识驱动特征时，为空间天气预报提供了有效的工具，并且其预测性能优于现有NASA/CCMC系统。

Abstract: In this work, we develop, for the first time, a supervised classification framework with class-dependent rewards (CDR) to predict $\geq$MM flares within 24 hr. We construct multiple datasets, covering knowledge-informed features and line-of sight (LOS) magnetograms. We also apply three deep learning models (CNN, CNN-BiLSTM, and Transformer) and three CDR counterparts (CDR-CNN, CDR-CNN-BiLSTM, and CDR-Transformer). First, we analyze the importance of LOS magnetic field parameters with the Transformer, then compare its performance using LOS-only, vector-only, and combined magnetic field parameters. Second, we compare flare prediction performance based on CDR models versus deep learning counterparts. Third, we perform sensitivity analysis on reward engineering for CDR models. Fourth, we use the SHAP method for model interpretability. Finally, we conduct performance comparison between our models and NASA/CCMC. The main findings are: (1)Among LOS feature combinations, R_VALUE and AREA_ACR consistently yield the best results. (2)Transformer achieves better performance with combined LOS and vector magnetic field data than with either alone. (3)Models using knowledge-informed features outperform those using magnetograms. (4)While CNN and CNN-BiLSTM outperform their CDR counterparts on magnetograms, CDR-Transformer is slightly superior to its deep learning counterpart when using knowledge-informed features. Among all models, CDR-Transformer achieves the best performance. (5)The predictive performance of the CDR models is not overly sensitive to the reward choices.(6)Through SHAP analysis, the CDR model tends to regard TOTUSJH as more important, while the Transformer tends to prioritize R_VALUE more.(7)Under identical prediction time and active region (AR) number, the CDR-Transformer shows superior predictive capabilities compared to NASA/CCMC.

</details>


### [68] [Fast KV Compaction via Attention Matching](https://arxiv.org/abs/2602.16284)
*Adam Zweiger,Xinghong Fu,Han Guo,Yoon Kim*

Main category: cs.LG

TL;DR: 提出Attention Matching方法，通过匹配注意力输出来快速构建紧凑的KV缓存，实现高效的长上下文压缩


<details>
  <summary>Details</summary>
Motivation: 长上下文语言模型受限于KV缓存大小，现有基于摘要的压缩方法损失严重，而Cartridges方法虽然效果好但训练成本高，需要快速高效的压缩方案

Method: 提出Attention Matching框架，通过构建紧凑的键值来重现注意力输出并保持注意力质量，该方法可分解为简单子问题，部分有闭式解，实现快速上下文压缩

Result: 在多个数据集上实现高达50倍的压缩，仅需数秒时间且质量损失很小，显著提升了压缩时间与质量的帕累托前沿

Conclusion: Attention Matching提供了一种快速高效的上下文压缩方法，在保持性能的同时大幅减少KV缓存大小，解决了长上下文部署的瓶颈问题

Abstract: Scaling language models to long contexts is often bottlenecked by the size of the key-value (KV) cache. In deployed settings, long contexts are typically managed through compaction in token space via summarization. However, summarization can be highly lossy, substantially harming downstream performance. Recent work on Cartridges has shown that it is possible to train highly compact KV caches in latent space that closely match full-context performance, but at the cost of slow and expensive end-to-end optimization. This work describes an approach for fast context compaction in latent space through Attention Matching, which constructs compact keys and values to reproduce attention outputs and preserve attention mass at a per-KV-head level. We show that this formulation naturally decomposes into simple subproblems, some of which admit efficient closed-form solutions. Within this framework, we develop a family of methods that significantly push the Pareto frontier of compaction time versus quality, achieving up to 50x compaction in seconds on some datasets with little quality loss.

</details>


### [69] [A Graph Meta-Network for Learning on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.16316)
*Guy Bar-Shalom,Ami Tavory,Itay Evron,Maya Bechler-Speicher,Ido Guy,Haggai Maron*

Main category: cs.LG

TL;DR: 提出了首个针对KANs的权重空间架构WS-KAN，利用KAN的图表示和对称性，在多种任务上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有权重空间模型主要针对标准神经网络，缺乏专门针对Kolmogorov-Arnold Networks (KANs)的架构设计。虽然先前工作利用标准网络的置换对称性，但KANs尚未有类似分析和定制架构。

Method: 1) 证明KANs与MLPs具有相同的置换对称性；2) 提出KAN-graph作为KAN的计算图表示；3) 基于此开发WS-KAN，首个专门学习KANs的权重空间架构；4) 分析WS-KAN的表达能力，证明其能复制输入KAN的前向传播。

Result: 构建了包含多种任务的KANs训练"动物园"作为基准测试。在所有任务中，WS-KAN始终优于结构无关的基线方法，且优势通常显著。

Conclusion: 成功开发了首个专门针对KANs的权重空间架构WS-KAN，有效利用KAN的对称性，为KANs的权重空间学习提供了理论基础和实用工具。

Abstract: Weight-space models learn directly from the parameters of neural networks, enabling tasks such as predicting their accuracy on new datasets. Naive methods -- like applying MLPs to flattened parameters -- perform poorly, making the design of better weight-space architectures a central challenge. While prior work leveraged permutation symmetries in standard networks to guide such designs, no analogous analysis or tailored architecture yet exists for Kolmogorov-Arnold Networks (KANs). In this work, we show that KANs share the same permutation symmetries as MLPs, and propose the KAN-graph, a graph representation of their computation. Building on this, we develop WS-KAN, the first weight-space architecture that learns on KANs, which naturally accounts for their symmetry. We analyze WS-KAN's expressive power, showing it can replicate an input KAN's forward pass - a standard approach for assessing expressiveness in weight-space architectures. We construct a comprehensive ``zoo'' of trained KANs spanning diverse tasks, which we use as benchmarks to empirically evaluate WS-KAN. Across all tasks, WS-KAN consistently outperforms structure-agnostic baselines, often by a substantial margin. Our code is available at https://github.com/BarSGuy/KAN-Graph-Metanetwork.

</details>


### [70] [Guide-Guard: Off-Target Predicting in CRISPR Applications](https://arxiv.org/abs/2602.16327)
*Joseph Bingham,Netanel Arussy,Saman Zonouz*

Main category: cs.LG

TL;DR: 提出Guide-Guard机器学习模型，用于预测CRISPR基因编辑中gRNA的脱靶行为，准确率达84%


<details>
  <summary>Details</summary>
Motivation: 随着CRISPR等基因编辑技术的发展，研究人员需要更好的工具来预测脱靶效应，这是基因编辑安全性的关键问题

Method: 从数据驱动角度探索生物学和化学模型，开发名为Guide-Guard的机器学习解决方案

Result: Guide-Guard模型在预测CRISPR基因编辑过程中gRNA行为时达到84%的准确率，能够同时训练多个不同基因并保持准确性

Conclusion: Guide-Guard为CRISPR基因编辑的脱靶行为预测提供了有效的机器学习解决方案，有助于提高基因编辑的安全性和可靠性

Abstract: With the introduction of cyber-physical genome sequencing and editing technologies, such as CRISPR, researchers can more easily access tools to investigate and create remedies for a variety of topics in genetics and health science (e.g. agriculture and medicine). As the field advances and grows, new concerns present themselves in the ability to predict the off-target behavior. In this work, we explore the underlying biological and chemical model from a data driven perspective. Additionally, we present a machine learning based solution named \textit{Guide-Guard} to predict the behavior of the system given a gRNA in the CRISPR gene-editing process with 84\% accuracy. This solution is able to be trained on multiple different genes at the same time while retaining accuracy.

</details>


### [71] [HAWX: A Hardware-Aware FrameWork for Fast and Scalable ApproXimation of DNNs](https://arxiv.org/abs/2602.16336)
*Samira Nazari,Mohammad Saeed Almasi,Mahdi Taheri,Ali Azarpeyvand,Ali Mokhtari,Ali Mahani,Christian Herglotz*

Main category: cs.LG

TL;DR: HAWX是一个硬件感知的可扩展探索框架，通过多级敏感性评分指导异构近似计算块的集成，使用预测模型加速配置评估，在保持精度的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型部署中需要在精度和硬件效率（功耗、面积）之间权衡，传统方法评估近似计算块配置耗时且复杂，需要更高效的硬件感知探索框架。

Method: 提出HAWX框架，采用多级敏感性评分（算子、滤波器、层、模型）指导异构近似计算块的选择性集成，结合精度、功耗、面积的预测模型加速候选配置评估。

Result: 在层级搜索中实现超过23倍加速，在滤波器级搜索中实现超过3×10^6倍加速（仅LeNet-5），同时保持与穷举搜索相当的精度。在VGG-11、ResNet-18、EfficientNetLite等基准测试中，效率优势随网络规模呈指数级扩展。

Conclusion: HAWX是一个高效的硬件感知探索框架，支持空间和时间加速器架构，可利用现成或定制化近似组件，显著加速近似计算块配置搜索，为深度学习模型部署提供有效的精度-效率权衡方案。

Abstract: This work presents HAWX, a hardware-aware scalable exploration framework that employs multi-level sensitivity scoring at different DNN abstraction levels (operator, filter, layer, and model) to guide selective integration of heterogeneous AxC blocks. Supported by predictive models for accuracy, power, and area, HAWX accelerates the evaluation of candidate configurations, achieving over 23* speedup in a layer-level search with two candidate approximate blocks and more than (3*106)* speedup at the filter-level search only for LeNet-5, while maintaining accuracy comparable to exhaustive search. Experiments across state-of-the-art DNN benchmarks such as VGG-11, ResNet-18, and EfficientNetLite demonstrate that the efficiency benefits of HAWX scale exponentially with network size. The HAWX hardware-aware search algorithm supports both spatial and temporal accelerator architectures, leveraging either off-the-shelf approximate components or customized designs.

</details>


### [72] [Explainability for Fault Detection System in Chemical Processes](https://arxiv.org/abs/2602.16341)
*Georgios Gravanis,Dimitrios Kyriakou,Spyros Voutetakis,Simira Papadopoulou,Konstantinos Diamantaras*

Main category: cs.LG

TL;DR: 本文应用并比较了两种先进的XAI方法（IG和SHAP），用于解释LSTM分类器在田纳西伊士曼过程故障诊断中的决策，发现XAI能帮助识别故障发生的子系统，SHAP在某些情况下能更接近故障根本原因。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于应用可解释人工智能方法解释深度学习模型在复杂工业过程故障诊断中的决策，帮助识别故障发生的具体子系统，提高诊断系统的透明度和可信度。

Method: 采用两种先进的模型无关XAI方法（集成梯度和SHAP），应用于高精度的LSTM分类器，该分类器在田纳西伊士曼过程这一基准非线性化学过程上进行故障检测训练。

Result: 研究发现XAI方法能有效识别故障发生的子系统；在大多数情况下，两种方法指示的最重要特征相同；但在某些情况下，SHAP方法提供的信息更丰富，更接近故障的根本原因。

Conclusion: 由于使用的XAI方法是模型无关的，所提出的方法不仅限于特定过程，也可用于类似问题，为工业过程故障诊断提供了可解释的解决方案。

Abstract: In this work, we apply and compare two state-of-the-art eXplainability Artificial Intelligence (XAI) methods, the Integrated Gradients (IG) and the SHapley Additive exPlanations (SHAP), that explain the fault diagnosis decisions of a highly accurate Long Short-Time Memory (LSTM) classifier. The classifier is trained to detect faults in a benchmark non-linear chemical process, the Tennessee Eastman Process (TEP). It is highlighted how XAI methods can help identify the subsystem of the process where the fault occurred. Using our knowledge of the process, we note that in most cases the same features are indicated as the most important for the decision, while insome cases the SHAP method seems to be more informative and closer to the root cause of the fault. Finally, since the used XAI methods are model-agnostic, the proposed approach is not limited to the specific process and can also be used in similar problems.

</details>


### [73] [Optical Inversion and Spectral Unmixing of Spectroscopic Photoacoustic Images with Physics-Informed Neural Networks](https://arxiv.org/abs/2602.16357)
*Sarkis Ter Martirosyan,Xinyue Huang,David Qin,Anthony Yu,Stanislav Emelianov*

Main category: cs.LG

TL;DR: SPOI-AE是一种用于光谱光声成像的自动编码器，能够解决非线性光学反演和光谱解混问题，在活体小鼠淋巴结成像中表现出优于传统算法的性能。


<details>
  <summary>Details</summary>
Motivation: 光谱光声成像中色团浓度估计存在非线性和病态性问题，传统方法难以准确估计，需要开发不依赖线性假设的新方法。

Method: 提出SPOI-AE（光谱光声光学反演自动编码器），在未知真实色团浓度的活体小鼠淋巴结图像上进行训练和测试，通过模拟小鼠淋巴结体模验证解混准确性。

Result: SPOI-AE比传统算法更好地重建输入光声像素，同时提供生物学上一致的光学参数、色团浓度和组织氧饱和度估计，在模拟体模上验证了解混准确性。

Conclusion: SPOI-AE能够有效解决光谱光声成像中的非线性光学反演和光谱解混问题，为生理过程的结构、功能和分子信息分析提供了新工具。

Abstract: Accurate estimation of the relative concentrations of chromophores in a spectroscopic photoacoustic (sPA) image can reveal immense structural, functional, and molecular information about physiological processes. However, due to nonlinearities and ill-posedness inherent to sPA imaging, concentration estimation is intractable. The Spectroscopic Photoacoustic Optical Inversion Autoencoder (SPOI-AE) aims to address the sPA optical inversion and spectral unmixing problems without assuming linearity. Herein, SPOI-AE was trained and tested on \textit{in vivo} mouse lymph node sPA images with unknown ground truth chromophore concentrations. SPOI-AE better reconstructs input sPA pixels than conventional algorithms while providing biologically coherent estimates for optical parameters, chromophore concentrations, and the percent oxygen saturation of tissue. SPOI-AE's unmixing accuracy was validated using a simulated mouse lymph node phantom ground truth.

</details>


### [74] [Improved Bounds for Reward-Agnostic and Reward-Free Exploration](https://arxiv.org/abs/2602.16363)
*Oran Ridel,Alon Cohen*

Main category: cs.LG

TL;DR: 该论文研究了无奖励和奖励不可知探索，提出新算法显著放宽了精度参数ε的要求，并建立了紧下界


<details>
  <summary>Details</summary>
Motivation: 研究在无外部奖励观测情况下的探索问题，旨在为后续揭示的任何奖励函数找到ε最优策略，同时针对奖励不可知设置（奖励来自有限小类）放宽现有算法对精度参数ε的限制要求

Method: 提出新算法，采用精心设计奖励的在线学习过程构建探索策略，收集足够数据进行准确动态估计，在奖励揭示后计算ε最优策略

Result: 算法显著放宽了对精度参数ε的要求，建立了无奖励探索的紧下界，填补了已知上下界之间的差距

Conclusion: 新算法在技术上有创新性，解决了奖励不可知探索中ε限制过严的问题，同时为无奖励探索建立了完整的理论界限

Abstract: We study reward-free and reward-agnostic exploration in episodic finite-horizon Markov decision processes (MDPs), where an agent explores an unknown environment without observing external rewards. Reward-free exploration aims to enable $ε$-optimal policies for any reward revealed after exploration, while reward-agnostic exploration targets $ε$-optimality for rewards drawn from a small finite class. In the reward-agnostic setting, Li, Yan, Chen, and Fan achieve minimax sample complexity, but only for restrictively small accuracy parameter $ε$. We propose a new algorithm that significantly relaxes the requirement on $ε$. Our approach is novel and of technical interest by itself. Our algorithm employs an online learning procedure with carefully designed rewards to construct an exploration policy, which is used to gather data sufficient for accurate dynamics estimation and subsequent computation of an $ε$-optimal policy once the reward is revealed. Finally, we establish a tight lower bound for reward-free exploration, closing the gap between known upper and lower bounds.

</details>


### [75] [Easy Data Unlearning Bench](https://arxiv.org/abs/2602.16400)
*Roy Rinberg,Pol Puigdemont,Martin Pawelczyk,Volkan Cevher*

Main category: cs.LG

TL;DR: 提出统一的机器遗忘算法评估框架，使用KLoM指标简化评估流程，提供预计算模型和基础设施


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘方法的评估存在技术挑战，现有基准测试需要复杂设置和大量工程开销，缺乏统一标准

Method: 开发统一的扩展性基准测试套件，采用KLoM（边缘KL散度）指标，提供预计算模型集成、oracle输出和简化基础设施

Result: 创建了标准化评估框架，支持可复现、可扩展和公平的机器遗忘方法比较，代码和数据已公开

Conclusion: 该基准测试为机器遗忘研究提供了实用基础，旨在加速研究进展并促进最佳实践

Abstract: Evaluating machine unlearning methods remains technically challenging, with recent benchmarks requiring complex setups and significant engineering overhead. We introduce a unified and extensible benchmarking suite that simplifies the evaluation of unlearning algorithms using the KLoM (KL divergence of Margins) metric. Our framework provides precomputed model ensembles, oracle outputs, and streamlined infrastructure for running evaluations out of the box. By standardizing setup and metrics, it enables reproducible, scalable, and fair comparison across unlearning methods. We aim for this benchmark to serve as a practical foundation for accelerating research and promoting best practices in machine unlearning. Our code and data are publicly available.

</details>


### [76] [Intra-Fairness Dynamics: The Bias Spillover Effect in Targeted LLM Alignment](https://arxiv.org/abs/2602.16438)
*Eva Paraschou,Line Harder Clemmensen,Sneha Das*

Main category: cs.LG

TL;DR: 针对LLM公平性对齐的研究发现，单一属性对齐可能导致偏见溢出，即在改善目标属性公平性的同时，恶化其他敏感属性的公平性，特别是在模糊语境下。


<details>
  <summary>Details</summary>
Motivation: 当前LLM公平性对齐主要关注单一敏感属性，忽视了公平性本质上是多维且上下文相关的价值。这种方法可能导致系统在达到狭窄的公平性指标的同时，加剧未针对属性的差异，即偏见溢出现象。虽然偏见溢出在机器学习中已有广泛研究，但在LLM对齐中仍严重未被探索。

Method: 研究针对性别对齐如何影响三个先进LLM（Mistral 7B、Llama 3.1 8B、Qwen 2.5 7B）在九个敏感属性上的公平性。使用直接偏好优化和BBQ基准，在模糊和非模糊语境下评估公平性。

Result: 研究发现明显的偏见溢出：虽然总体结果有所改善，但上下文感知分析显示在模糊语境下公平性显著下降，特别是在外貌（所有模型p<0.001）、性取向和残疾状况方面。研究表明，改善一个属性的公平性可能在不确定性下无意中恶化其他属性的差异。

Conclusion: 强调需要上下文感知、多属性公平性评估框架的必要性。公平性对齐必须考虑多维属性和具体语境，避免单一属性优化导致的偏见溢出问题。

Abstract: Conventional large language model (LLM) fairness alignment largely focuses on mitigating bias along single sensitive attributes, overlooking fairness as an inherently multidimensional and context-specific value. This approach risks creating systems that achieve narrow fairness metrics while exacerbating disparities along untargeted attributes, a phenomenon known as bias spillover. While extensively studied in machine learning, bias spillover remains critically underexplored in LLM alignment. In this work, we investigate how targeted gender alignment affects fairness across nine sensitive attributes in three state-of-the-art LLMs (Mistral 7B, Llama 3.1 8B, Qwen 2.5 7B). Using Direct Preference Optimization and the BBQ benchmark, we evaluate fairness under ambiguous and disambiguous contexts. Our findings reveal noticeable bias spillover: while aggregate results show improvements, context-aware analysis exposes significant degradations in ambiguous contexts, particularly for physical appearance ($p< 0.001$ across all models), sexual orientation, and disability status. We demonstrate that improving fairness along one attribute can inadvertently worsen disparities in others under uncertainty, highlighting the necessity of context-aware, multi-attribute fairness evaluation frameworks.

</details>


### [77] [Hardware-accelerated graph neural networks: an alternative approach for neuromorphic event-based audio classification and keyword spotting on SoC FPGA](https://arxiv.org/abs/2602.16442)
*Kamil Jeziorek,Piotr Wzorek,Krzysztof Blachut,Hiroshi Nakano,Manon Dampfhoffer,Thomas Mesquida,Hiroaki Nishi,Thomas Dalgaty,Tomasz Kryjak*

Main category: cs.LG

TL;DR: FPGA实现的事件图神经网络用于音频处理，通过人工耳蜗将时间序列信号转换为稀疏事件数据，在资源受限的边缘设备上实现高效、低延迟、低功耗的音频分类和关键词检测。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式边缘传感器数据量增加，特别是神经形态设备产生的离散事件流，需要硬件感知的神经架构来实现高效、低延迟、低功耗的本地处理。

Method: 使用人工耳蜗将时间序列信号转换为稀疏事件数据，在SoC FPGA上实现事件图神经网络架构，结合图卷积层和循环序列建模，进行量化和硬件优化。

Result: 在SHD数据集上达到92.7%准确率（仅比SOTA低2.4%），参数减少10-67倍；在SSC上达到66.9-71.0%准确率；量化模型达到92.3%准确率，比FPGA SNNs高19.3%；关键词检测系统达到95%词尾检测准确率，延迟仅10.53微秒，功耗1.18W。

Conclusion: 该工作展示了事件图神经网络在FPGA上的高效实现，为资源受限的边缘设备提供了低延迟、低功耗的音频处理解决方案，建立了能效事件驱动关键词检测的强基准。

Abstract: As the volume of data recorded by embedded edge sensors increases, particularly from neuromorphic devices producing discrete event streams, there is a growing need for hardware-aware neural architectures that enable efficient, low-latency, and energy-conscious local processing. We present an FPGA implementation of event-graph neural networks for audio processing. We utilise an artificial cochlea that converts time-series signals into sparse event data, reducing memory and computation costs. Our architecture was implemented on a SoC FPGA and evaluated on two open-source datasets. For classification task, our baseline floating-point model achieves 92.7% accuracy on SHD dataset - only 2.4% below the state of the art - while requiring over 10x and 67x fewer parameters. On SSC, our models achieve 66.9-71.0% accuracy. Compared to FPGA-based spiking neural networks, our quantised model reaches 92.3% accuracy, outperforming them by up to 19.3% while reducing resource usage and latency. For SSC, we report the first hardware-accelerated evaluation. We further demonstrate the first end-to-end FPGA implementation of event-audio keyword spotting, combining graph convolutional layers with recurrent sequence modelling. The system achieves up to 95% word-end detection accuracy, with only 10.53 microsecond latency and 1.18 W power consumption, establishing a strong benchmark for energy-efficient event-driven KWS.

</details>


### [78] [Beyond SGD, Without SVD: Proximal Subspace Iteration LoRA with Diagonal Fractional K-FAC](https://arxiv.org/abs/2602.16456)
*Abdulla Jasem Almansoori,Maria Ivanova,Andrey Veprikov,Aleksandr Beznosikov,Samuel Horváth,Martin Takáč*

Main category: cs.LG

TL;DR: LoRSum：一种内存高效的LoRA优化方法，通过交替最小二乘更新解决近端子问题，匹配或超越LoRA基线，同时保持参数效率。


<details>
  <summary>Details</summary>
Motivation: 解决使用低秩投影（SVDLoRA）进行全步长训练与LoRA微调之间的差距，同时保持LoRA的参数效率优势。

Method: 提出LoRSum方法，将LoRA优化转化为近端子问题，使用交替最小二乘更新高效求解（被证明是隐式块幂方法）。还提出了使用K-FAC和Shampoo等结构化度量的缩放变体，仅存储对角线保持内存效率。

Result: 在合成任务、CIFAR-100以及GLUE、SQuAD v2、WikiText-103等语言模型微调任务上，LoRSum能够匹配或改进LoRA基线，计算开销适中，避免全矩阵SVD投影，保持LoRA式参数效率。

Conclusion: LoRSum填补了全步长低秩投影训练与LoRA微调之间的差距，提供了一种内存高效的优化方法，能够恢复多种最近提出的LoRA预处理方法作为特例，并在保持参数效率的同时实现更好的性能。

Abstract: Low-Rank Adaptation (LoRA) fine-tunes large models by learning low-rank updates on top of frozen weights, dramatically reducing trainable parameters and memory. In this work, we address the gap between training with full steps with low-rank projections (SVDLoRA) and LoRA fine-tuning. We propose LoRSum, a memory-efficient subroutine that closes this gap for gradient descent by casting LoRA optimization as a proximal sub-problem and solving it efficiently with alternating least squares updates, which we prove to be an implicit block power method. We recover several recently proposed preconditioning methods for LoRA as special cases, and show that LoRSum can also be used for updating a low-rank momentum. In order to address full steps with preconditioned gradient descent, we propose a scaled variant of LoRSum that uses structured metrics such as K-FAC and Shampoo, and we show that storing the diagonal of these metrics still allows them to perform well while remaining memory-efficient. Experiments on a synthetic task, CIFAR-100, and language-model fine-tuning on GLUE, SQuAD v2, and WikiText-103, show that our method can match or improve LoRA baselines given modest compute overhead, while avoiding full-matrix SVD projections and retaining LoRA-style parameter efficiency.

</details>


### [79] [HPMixer: Hierarchical Patching for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.16468)
*Jung Min Choi,Vijaya Krishna Yalavarthi,Lars Schmidt-Thieme*

Main category: cs.LG

TL;DR: HPMixer：一种用于长期多元时间序列预测的层次化补丁混合模型，通过解耦的周期性建模和多尺度残差学习实现高效预测


<details>
  <summary>Details</summary>
Motivation: 在长期多元时间序列预测中，需要同时有效捕捉周期性模式和残差动态。现有方法在标准深度学习基准设置下，对这两方面的建模不够充分或耦合度过高，需要一种能够解耦但互补地建模周期性和残差的方法。

Method: 提出层次化补丁混合器（HPMixer），包含：1）周期性组件使用可学习循环模块增强非线性通道MLP；2）残差组件通过可学习平稳小波变换提取稳定频域表示；3）通道混合编码器建模通道间依赖；4）两级非重叠层次化补丁机制捕捉粗粒度和细粒度残差变化

Result: 在标准多元基准测试上的大量实验表明，HPMixer相比近期基线方法取得了竞争性或最先进的性能

Conclusion: 通过将解耦的周期性建模与结构化多尺度残差学习相结合，HPMixer为长期多元时间序列预测提供了一个有效的框架

Abstract: In long-term multivariate time series forecasting, effectively capturing both periodic patterns and residual dynamics is essential. To address this within standard deep learning benchmark settings, we propose the Hierarchical Patching Mixer (HPMixer), which models periodicity and residuals in a decoupled yet complementary manner. The periodic component utilizes a learnable cycle module [7] enhanced with a nonlinear channel-wise MLP for greater expressiveness. The residual component is processed through a Learnable Stationary Wavelet Transform (LSWT) to extract stable, shift-invariant frequency-domain representations. Subsequently, a channel-mixing encoder models explicit inter-channel dependencies, while a two-level non-overlapping hierarchical patching mechanism captures coarse- and fine-scale residual variations. By integrating decoupled periodicity modeling with structured, multi-scale residual learning, HPMixer provides an effective framework. Extensive experiments on standard multivariate benchmarks demonstrate that HPMixer achieves competitive or state-of-the-art performance compared to recent baselines.

</details>


### [80] [Synthesis and Verification of Transformer Programs](https://arxiv.org/abs/2602.16473)
*Hongjian Jiang,Matthew Hague,Philipp Rümmer,Anthony Widjaja Lin*

Main category: cs.LG

TL;DR: 本文提出了C-RASP程序验证和学习的新算法：通过连接Lustre同步数据流程序验证实现自动验证，以及使用局部搜索从示例中学习C-RASP程序。


<details>
  <summary>Details</summary>
Motivation: C-RASP是一种能表达transformer概念的简单编程语言，但缺乏自动验证和学习的方法，限制了其在transformer程序优化和约束学习中的应用。

Method: 1. 建立C-RASP与Lustre同步数据流程序的连接，利用现有模型检查器和优化的SMT求解器进行验证；2. 提出基于局部搜索的算法，从示例中学习C-RASP程序。

Result: 实现了C-RASP验证和学习系统，在文献中的C-RASP基准测试中表现有效，特别是在transformer程序优化和基于部分规范的transformer程序约束学习两个应用场景中。

Conclusion: 本文为C-RASP提供了实用的验证和学习工具，扩展了C-RASP在transformer程序分析和综合中的应用潜力。

Abstract: C-RASP is a simple programming language that was recently shown to capture concepts expressible by transformers. In this paper, we develop new algorithmic techniques for automatically verifying C-RASPs. To this end, we establish a connection to the verification of synchronous dataflow programs in Lustre, which enables us to exploit state-of-the-art model checkers utilizing highly optimized SMT-solvers. Our second contribution addresses learning a C-RASP program in the first place. To this end, we provide a new algorithm for learning a C-RASP from examples using local search. We demonstrate efficacy of our implementation for benchmarks of C-RASPs in the literature, in particular in connection to the following applications: (1) transformer program optimization, and (2) constrained learning of transformer programs (based on a partial specification).

</details>


### [81] [Fast and Scalable Analytical Diffusion](https://arxiv.org/abs/2602.16498)
*Xinyi Shang,Peng Sun,Jingyu Lin,Zhiqiang Shen*

Main category: cs.LG

TL;DR: GoldDiff提出了一种训练免费的动态时间感知黄金子集扩散框架，通过渐进后验集中现象实现推理复杂度与数据集大小的解耦，在AFHQ上实现71倍加速并在ImageNet-1K上首次成功扩展分析扩散模型。


<details>
  <summary>Details</summary>
Motivation: 分析扩散模型虽然具有数学透明性，但标准实现需要在每个时间步扫描整个数据集，计算复杂度随数据集规模线性增长，这严重限制了其可扩展性。作者旨在解决这一可扩展性瓶颈。

Method: 提出Dynamic Time-Aware Golden Subset Diffusion (GoldDiff)框架：1) 发现后验渐进集中现象——去噪分数的有效黄金支撑会随着信噪比增加从全局流形收缩到局部邻域；2) 采用粗到细机制动态定位"黄金子集"进行推理，而非静态检索；3) 理论上推导了稀疏逼近收敛到精确分数的严格边界。

Result: 在AFHQ数据集上实现71倍加速，同时匹配甚至优于全扫描基线性能；首次成功将分析扩散模型扩展到ImageNet-1K，为大规模生成建模提供了可扩展的训练免费范式。

Conclusion: GoldDiff通过利用后验渐进集中现象，成功解决了分析扩散模型的可扩展性瓶颈，实现了推理复杂度与数据集大小的解耦，为大规模生成建模开辟了新的训练免费范式。

Abstract: Analytical diffusion models offer a mathematically transparent path to generative modeling by formulating the denoising score as an empirical-Bayes posterior mean. However, this interpretability comes at a prohibitive cost: the standard formulation necessitates a full-dataset scan at every timestep, scaling linearly with dataset size. In this work, we present the first systematic study addressing this scalability bottleneck. We challenge the prevailing assumption that the entire training data is necessary, uncovering the phenomenon of Posterior Progressive Concentration: the effective golden support of the denoising score is not static but shrinks asymptotically from the global manifold to a local neighborhood as the signal-to-noise ratio increases. Capitalizing on this, we propose Dynamic Time-Aware Golden Subset Diffusion (GoldDiff), a training-free framework that decouples inference complexity from dataset size. Instead of static retrieval, GoldDiff uses a coarse-to-fine mechanism to dynamically pinpoint the ''Golden Subset'' for inference. Theoretically, we derive rigorous bounds guaranteeing that our sparse approximation converges to the exact score. Empirically, GoldDiff achieves a $\bf 71 \times$ speedup on AFHQ while matching or achieving even better performance than full-scan baselines. Most notably, we demonstrate the first successful scaling of analytical diffusion to ImageNet-1K, unlocking a scalable, training-free paradigm for large-scale generative modeling.

</details>


### [82] [Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects](https://arxiv.org/abs/2602.16503)
*Vasilis Gkolemis,Loukas Kavouras,Dimitrios Kyriakopoulos,Konstantinos Tsopelas,Dimitrios Rontogiannis,Giuseppe Casalicchio,Theodore Dalamagas,Christos Diou*

Main category: cs.LG

TL;DR: CALMs在保持GAMs可解释性的同时，通过局部加性模型捕捉特征交互，达到与GA²Ms相当的预测精度。


<details>
  <summary>Details</summary>
Motivation: GAMs虽然可解释但无法处理特征交互导致欠拟合，GA²Ms加入成对交互提高了精度但牺牲了可解释性。需要一种平衡可解释性和准确性的模型。

Method: 提出CALMs模型：为每个特征设置多个一元形状函数，每个函数在不同输入子区域激活；区域由简单阈值条件定义；采用蒸馏训练流程识别同质区域并通过区域感知反向拟合拟合可解释形状函数。

Result: 在多种分类和回归任务上，CALMs始终优于GAMs，且准确性与GA²Ms相当。

Conclusion: CALMs在预测准确性和可解释性之间提供了有吸引力的权衡，平衡了GAMs的可解释性和GA²Ms的准确性。

Abstract: Generalized additive models (GAMs) offer interpretability through independent univariate feature effects but underfit when interactions are present in data. GA$^2$Ms add selected pairwise interactions which improves accuracy, but sacrifices interpretability and limits model auditing. We propose \emph{Conditionally Additive Local Models} (CALMs), a new model class, that balances the interpretability of GAMs with the accuracy of GA$^2$Ms. CALMs allow multiple univariate shape functions per feature, each active in different regions of the input space. These regions are defined independently for each feature as simple logical conditions (thresholds) on the features it interacts with. As a result, effects remain locally additive while varying across subregions to capture interactions. We further propose a principled distillation-based training pipeline that identifies homogeneous regions with limited interactions and fits interpretable shape functions via region-aware backfitting. Experiments on diverse classification and regression tasks show that CALMs consistently outperform GAMs and achieve accuracy comparable with GA$^2$Ms. Overall, CALMs offer a compelling trade-off between predictive accuracy and interpretability.

</details>


### [83] [Small molecule retrieval from tandem mass spectrometry: what are we optimizing for?](https://arxiv.org/abs/2602.16507)
*Gaetan De Waele,Marek Wydmuch,Krzysztof Dembczyński,Wojciech Kotłowski,Willem Waegeman*

Main category: cs.LG

TL;DR: 研究LC-MS/MS数据分析中深度学习模型损失函数对性能的影响，发现指纹相似性和分子检索之间存在根本性权衡


<details>
  <summary>Details</summary>
Motivation: 在LC-MS/MS数据分析中，使用深度学习预测分子指纹进行化合物识别时，不同损失函数对模型性能的影响尚不明确，需要系统研究

Method: 研究常用损失函数，推导新颖的遗憾界限，分析贝叶斯最优决策在不同目标下的差异，考察候选集相似性结构的影响

Result: 发现指纹相似性和分子检索之间存在根本性权衡：优化指纹预测准确性通常会恶化检索结果，反之亦然；权衡取决于候选集的相似性结构

Conclusion: 损失函数和指纹选择应基于候选集的相似性结构，为LC-MS/MS数据分析中的模型训练提供理论指导

Abstract: One of the central challenges in the computational analysis of liquid chromatography-tandem mass spectrometry (LC-MS/MS) data is to identify the compounds underlying the output spectra. In recent years, this problem is increasingly tackled using deep learning methods. A common strategy involves predicting a molecular fingerprint vector from an input mass spectrum, which is then used to search for matches in a chemical compound database. While various loss functions are employed in training these predictive models, their impact on model performance remains poorly understood. In this study, we investigate commonly used loss functions, deriving novel regret bounds that characterize when Bayes-optimal decisions for these objectives must diverge. Our results reveal a fundamental trade-off between the two objectives of (1) fingerprint similarity and (2) molecular retrieval. Optimizing for more accurate fingerprint predictions typically worsens retrieval results, and vice versa. Our theoretical analysis shows this trade-off depends on the similarity structure of candidate sets, providing guidance for loss function and fingerprint selection.

</details>


### [84] [Reinforcement Learning for Parameterized Quantum State Preparation: A Comparative Study](https://arxiv.org/abs/2602.16523)
*Gerhard Stenzel,Isabella Debelic,Michael Kölle,Tobias Rohe,Leo Sünkel,Julian Hager,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 将DQCS强化学习从离散门选择扩展到包含连续单比特旋转的参数化量子态制备，比较单阶段和两阶段训练方法，PPO在稳定超参数下成功，但可扩展性在λ≈3-4时饱和。


<details>
  <summary>Details</summary>
Motivation: 扩展定向量子电路合成的强化学习方法，从纯离散门选择扩展到包含连续旋转的参数化量子态制备，以更全面地处理量子电路合成问题。

Method: 使用Gymnasium和PennyLane，比较两种训练机制：1) 单阶段代理联合选择门类型、作用量子比特和旋转角度；2) 两阶段变体先提出离散电路，再用Adam优化旋转角度。评估PPO和A2C算法在2-10量子比特系统上。

Result: A2C未学习有效策略，PPO在稳定超参数下成功（单阶段学习率约5×10⁻⁴，两阶段约10⁻⁴）。两种方法可靠重构计算基态（83-99%成功率）和贝尔态（61-77%成功率），但可扩展性在λ≈3-4时饱和，无法扩展到10量子比特目标。两阶段方法仅提供边际精度提升但需要约3倍运行时间。

Conclusion: 在固定计算预算下推荐单阶段PPO策略，提供显式合成电路，并与经典变分基线对比，为改进可扩展性指明方向。

Abstract: We extend directed quantum circuit synthesis (DQCS) with reinforcement learning from purely discrete gate selection to parameterized quantum state preparation with continuous single-qubit rotations \(R_x\), \(R_y\), and \(R_z\). We compare two training regimes: a one-stage agent that jointly selects the gate type, the affected qubit(s), and the rotation angle; and a two-stage variant that first proposes a discrete circuit and subsequently optimizes the rotation angles with Adam using parameter-shift gradients. Using Gymnasium and PennyLane, we evaluate Proximal Policy Optimization (PPO) and Advantage Actor--Critic (A2C) on systems comprising two to ten qubits and on targets of increasing complexity with \(λ\) ranging from one to five. Whereas A2C does not learn effective policies in this setting, PPO succeeds under stable hyperparameters (one-stage: learning rate approximately \(5\times10^{-4}\) with a self-fidelity-error threshold of 0.01; two-stage: learning rate approximately \(10^{-4}\)). Both approaches reliably reconstruct computational basis states (between 83\% and 99\% success) and Bell states (between 61\% and 77\% success). However, scalability saturates for \(λ\) of approximately three to four and does not extend to ten-qubit targets even at \(λ=2\). The two-stage method offers only marginal accuracy gains while requiring around three times the runtime. For practicality under a fixed compute budget, we therefore recommend the one-stage PPO policy, provide explicit synthesized circuits, and contrast with a classical variational baseline to outline avenues for improved scalability.

</details>


### [85] [Capacity-constrained demand response in smart grids using deep reinforcement learning](https://arxiv.org/abs/2602.16525)
*Shafagh Abband Pashaki,Sepehr Maleki,Amir Badiee*

Main category: cs.LG

TL;DR: 提出基于容量约束的激励型需求响应方法，采用分层架构和深度强化学习优化实时激励费率，有效降低住宅智能电网的峰值负荷


<details>
  <summary>Details</summary>
Motivation: 住宅智能电网中需要维持电网容量限制并防止拥塞，同时考虑服务提供商和终端用户的财务利益

Method: 采用分层架构，服务提供商根据批发电价和聚合住宅负荷调整小时激励费率；使用深度强化学习在明确容量约束下学习最优实时激励费率；通过设备级家庭能源管理系统和不满成本建模异构用户偏好

Result: 使用三个家庭的实际用电和价格数据进行仿真，结果显示该方法有效降低峰值需求并平滑聚合负荷曲线，相比无需求响应情况，峰均比降低约22.82%

Conclusion: 提出的容量约束激励型需求响应方法能有效管理住宅智能电网负荷，平衡电网容量限制与各方财务利益

Abstract: This paper presents a capacity-constrained incentive-based demand response approach for residential smart grids. It aims to maintain electricity grid capacity limits and prevent congestion by financially incentivising end users to reduce or shift their energy consumption. The proposed framework adopts a hierarchical architecture in which a service provider adjusts hourly incentive rates based on wholesale electricity prices and aggregated residential load. The financial interests of both the service provider and end users are explicitly considered. A deep reinforcement learning approach is employed to learn optimal real-time incentive rates under explicit capacity constraints. Heterogeneous user preferences are modelled through appliance-level home energy management systems and dissatisfaction costs. Using real-world residential electricity consumption and price data from three households, simulation results show that the proposed approach effectively reduces peak demand and smooths the aggregated load profile. This leads to an approximately 22.82% reduction in the peak-to-average ratio compared to the no-demand-response case.

</details>


### [86] [FEKAN: Feature-Enriched Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.16530)
*Sidharth S. Menon,Ameya D. Jagtap*

Main category: cs.LG

TL;DR: FEKAN是一种特征增强的Kolmogorov-Arnold网络，通过特征富集提高计算效率和预测精度，同时保持KAN的可解释性优势，不增加可训练参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有KAN架构（如样条、小波、径向基等变体）存在计算成本高、收敛慢的问题，限制了可扩展性和实际应用性，需要改进。

Method: 提出特征增强KAN（FEKAN），通过特征富集技术增强网络表示能力，保持KAN所有优势的同时提高计算效率和预测精度，不增加可训练参数。

Result: 在函数逼近、物理信息PDE求解和神经算子等任务中，FEKAN相比各种KAN变体（FastKAN、WavKAN等）表现出显著更快的收敛速度和更高的逼近精度。

Conclusion: FEKAN通过特征富集有效解决了传统KAN的计算效率问题，在保持可解释性的同时实现了更好的性能和收敛特性，为KAN的实际应用提供了可行方案。

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a compelling alternative to multilayer perceptrons, offering enhanced interpretability via functional decomposition. However, existing KAN architectures, including spline-, wavelet-, radial-basis variants, etc., suffer from high computational cost and slow convergence, limiting scalability and practical applicability. Here, we introduce Feature-Enriched Kolmogorov-Arnold Networks (FEKAN), a simple yet effective extension that preserves all the advantages of KAN while improving computational efficiency and predictive accuracy through feature enrichment, without increasing the number of trainable parameters. By incorporating these additional features, FEKAN accelerates convergence, increases representation capacity, and substantially mitigates the computational overhead characteristic of state-of-the-art KAN architectures. We investigate FEKAN across a comprehensive set of benchmarks, including function-approximation tasks, physics-informed formulations for diverse partial differential equations (PDEs), and neural operator settings that map between input and output function spaces. For function approximation, we systematically compare FEKAN against a broad family of KAN variants, FastKAN, WavKAN, ReLUKAN, HRKAN, ChebyshevKAN, RBFKAN, and the original SplineKAN. Across all tasks, FEKAN demonstrates substantially faster convergence and consistently higher approximation accuracy than the underlying baseline architectures. We also establish the theoretical foundations for FEKAN, showing its superior representation capacity compared to KAN, which contributes to improved accuracy and efficiency.

</details>


### [87] [Transfer Learning of Linear Regression with Multiple Pretrained Models: Benefiting from More Pretrained Models via Overparameterization Debiasing](https://arxiv.org/abs/2602.16531)
*Daniel Boharon,Yehuda Dar*

Main category: cs.LG

TL;DR: 研究多個過參數化最小二乘預訓練模型在線性回歸任務中的遷移學習，分析何時使用更多預訓練模型能改善性能，並提出消除過參數化偏差的方法。


<details>
  <summary>Details</summary>
Motivation: 探討在線性回歸任務中，使用多個可能過參數化的預訓練模型進行遷移學習時，何時增加預訓練模型數量能帶來益處，以及如何克服過參數化帶來的偏差問題。

Method: 將目標學習任務公式化為在目標數據集上最小化平方誤差，同時懲罰學習模型與預訓練模型之間的距離。分析學習模型的測試誤差，提出通過乘法校正因子來減少過參數化偏差的簡單去偏方法。

Result: 闡明當預訓練模型過參數化時，使用足夠多的預訓練模型對有益的遷移學習至關重要。然而，學習可能受到預訓練模型過參數化偏差的影響，即最小ℓ2範數解在高維參數空間中受限於訓練樣本張成的小子空間。提出的去偏方法能減少這種偏差並更好地利用更多預訓練模型。

Conclusion: 在過參數化預訓練模型的情況下，使用足夠多的模型對遷移學習有益，但需要解決過參數化偏差問題。提出的簡單去偏方法能有效減少偏差，使學習能更好地利用更多預訓練模型來學習目標預測器。

Abstract: We study transfer learning for a linear regression task using several least-squares pretrained models that can be overparameterized.
  We formulate the target learning task as optimization that minimizes squared errors on the target dataset with penalty on the distance of the learned model from the pretrained models. We analytically formulate the test error of the learned target model and provide the corresponding empirical evaluations.
  Our results elucidate when using more pretrained models can improve transfer learning. Specifically, if the pretrained models are overparameterized, using sufficiently many of them is important for beneficial transfer learning. However, the learning may be compromised by overparameterization bias of pretrained models, i.e., the minimum $\ell_2$-norm solution's restriction to a small subspace spanned by the training examples in the high-dimensional parameter space. We propose a simple debiasing via multiplicative correction factor that can reduce the overparameterization bias and leverage more pretrained models to learn a target predictor.

</details>


### [88] [Vulnerability Analysis of Safe Reinforcement Learning via Inverse Constrained Reinforcement Learning](https://arxiv.org/abs/2602.16543)
*Jialiang Fan,Shixiong Jiang,Mengyu Liu,Fanxin Kong*

Main category: cs.LG

TL;DR: 提出一种针对安全强化学习策略的黑盒对抗攻击框架，利用专家演示和环境交互学习约束模型和代理策略，无需受害者策略的内部梯度或真实安全约束信息。


<details>
  <summary>Details</summary>
Motivation: 现有安全强化学习方法假设环境友好，易受现实世界中常见对抗扰动影响；现有基于梯度的攻击方法通常需要策略梯度信息，这在现实场景中不切实际。

Method: 使用专家演示和黑盒环境交互学习约束模型和代理（学习者）策略，从而能够在不需要受害者策略内部梯度或真实安全约束的情况下进行基于梯度的攻击优化。

Result: 在多个安全强化学习基准测试中验证了方法的有效性，证明了在有限特权访问下的攻击能力。

Conclusion: 提出的对抗攻击框架能够有效揭示安全强化学习策略的脆弱性，为实际应用中的安全评估提供了实用工具。

Abstract: Safe reinforcement learning (Safe RL) aims to ensure policy performance while satisfying safety constraints. However, most existing Safe RL methods assume benign environments, making them vulnerable to adversarial perturbations commonly encountered in real-world settings. In addition, existing gradient-based adversarial attacks typically require access to the policy's gradient information, which is often impractical in real-world scenarios. To address these challenges, we propose an adversarial attack framework to reveal vulnerabilities of Safe RL policies. Using expert demonstrations and black-box environment interaction, our framework learns a constraint model and a surrogate (learner) policy, enabling gradient-based attack optimization without requiring the victim policy's internal gradients or the ground-truth safety constraints. We further provide theoretical analysis establishing feasibility and deriving perturbation bounds. Experiments on multiple Safe RL benchmarks demonstrate the effectiveness of our approach under limited privileged access.

</details>


### [89] [RIDER: 3D RNA Inverse Design with Reinforcement Learning-Guided Diffusion](https://arxiv.org/abs/2602.16548)
*Tianmeng Hu,Yongzheng Cui,Biao Luo,Ke Li*

Main category: cs.LG

TL;DR: RIDER：基于强化学习的RNA三维结构逆设计框架，直接优化结构相似性而非序列恢复率


<details>
  <summary>Details</summary>
Motivation: 现有RNA三维结构逆设计方法主要优化和评估原生序列恢复率，但这是一个有限的结构保真度代理指标，因为不同序列可以折叠成相似的三维结构，高恢复率不一定代表正确折叠

Method: 1. 开发并预训练基于GNN的条件扩散生成模型，以目标三维结构为条件；2. 使用改进的策略梯度算法和基于三维自一致性度量的四个任务特定奖励函数进行微调

Result: 预训练模型在原生序列恢复率上比最先进方法提高9%；强化学习微调后，在所有三维结构相似性指标上提高超过100%，并能发现与原生序列不同的设计

Conclusion: RIDER通过直接优化三维结构相似性而非序列恢复率，显著提高了RNA逆设计的结构保真度，为合成生物学和治疗应用中的功能性RNA工程提供了更有效的设计框架

Abstract: The inverse design of RNA three-dimensional (3D) structures is crucial for engineering functional RNAs in synthetic biology and therapeutics. While recent deep learning approaches have advanced this field, they are typically optimized and evaluated using native sequence recovery, which is a limited surrogate for structural fidelity, since different sequences can fold into similar 3D structures and high recovery does not necessarily indicate correct folding. To address this limitation, we propose RIDER, an RNA Inverse DEsign framework with Reinforcement learning that directly optimizes for 3D structural similarity. First, we develop and pre-train a GNN-based generative diffusion model conditioned on the target 3D structure, achieving a 9% improvement in native sequence recovery over state-of-the-art methods. Then, we fine-tune the model with an improved policy gradient algorithm using four task-specific reward functions based on 3D self-consistency metrics. Experimental results show that RIDER improves structural similarity by over 100% across all metrics and discovers designs that are distinct from native sequences.

</details>


### [90] [Illustration of Barren Plateaus in Quantum Computing](https://arxiv.org/abs/2602.16558)
*Gerhard Stenzel,Tobias Rohe,Michael Kölle,Leo Sünkel,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 参数共享在变分量子电路中虽然能减少参数量并可能缓解贫瘠高原现象，但会通过欺骗性梯度改变优化景观，增加优化难度，导致传统梯度优化器性能下降。


<details>
  <summary>Details</summary>
Motivation: 研究参数共享在变分量子电路中的复杂权衡，特别是它如何通过欺骗性梯度改变优化景观，这一问题在现有研究中被忽视。

Method: 通过系统实验分析，研究不同参数共享程度对优化景观的影响，引入梯度欺骗性检测算法和量子电路优化难度量化框架。

Result: 参数共享程度增加会产生更复杂的解景观，具有更高的梯度幅度和可测量的欺骗性比率，导致传统梯度优化器收敛性能下降，性能高度依赖超参数选择。

Conclusion: 参数共享虽然能提高电路表达能力，但代价是显著增加景观欺骗性，揭示了经典优化策略与量子参数景观之间的根本性不匹配，为实际应用中的量子电路设计提供了重要考量。

Abstract: Variational Quantum Circuits (VQCs) have emerged as a promising paradigm for quantum machine learning in the NISQ era. While parameter sharing in VQCs can reduce the parameter space dimensionality and potentially mitigate the barren plateau phenomenon, it introduces a complex trade-off that has been largely overlooked. This paper investigates how parameter sharing, despite creating better global optima with fewer parameters, fundamentally alters the optimization landscape through deceptive gradients -- regions where gradient information exists but systematically misleads optimizers away from global optima. Through systematic experimental analysis, we demonstrate that increasing degrees of parameter sharing generate more complex solution landscapes with heightened gradient magnitudes and measurably higher deceptiveness ratios. Our findings reveal that traditional gradient-based optimizers (Adam, SGD) show progressively degraded convergence as parameter sharing increases, with performance heavily dependent on hyperparameter selection. We introduce a novel gradient deceptiveness detection algorithm and a quantitative framework for measuring optimization difficulty in quantum circuits, establishing that while parameter sharing can improve circuit expressivity by orders of magnitude, this comes at the cost of significantly increased landscape deceptiveness. These insights provide important considerations for quantum circuit design in practical applications, highlighting the fundamental mismatch between classical optimization strategies and quantum parameter landscapes shaped by parameter sharing.

</details>


### [91] [A Scalable Approach to Solving Simulation-Based Network Security Games](https://arxiv.org/abs/2602.16564)
*Michael Lanier,Yevgeniy Vorobeychik*

Main category: cs.LG

TL;DR: MetaDOAR：一种轻量级元控制器，通过分区感知过滤层和Q值缓存增强Double Oracle/PSRO范式，实现大规模网络环境下的可扩展多智能体强化学习。


<details>
  <summary>Details</summary>
Motivation: 解决大规模网络环境中多智能体强化学习的可扩展性问题，传统方法在大型网络拓扑上存在内存使用和训练时间方面的扩展瓶颈。

Method: 1. 学习紧凑的状态投影，从节点结构嵌入快速评分并选择设备子集（top-k分区）；2. 低层执行器在选定分区上进行聚焦束搜索；3. 使用批处理critic前向传播评估候选动作；4. 建立LRU缓存（键为量化状态投影和局部动作标识符）；5. 采用保守的k跳缓存失效策略。

Result: 在大型网络拓扑上获得比SOTA基线更高的玩家收益，在内存使用和训练时间方面没有显著的扩展问题。

Conclusion: MetaDOAR为大规模网络决策问题提供了一个实用、理论上有依据的高效分层策略学习路径。

Abstract: We introduce MetaDOAR, a lightweight meta-controller that augments the Double Oracle / PSRO paradigm with a learned, partition-aware filtering layer and Q-value caching to enable scalable multi-agent reinforcement learning on very large cyber-network environments. MetaDOAR learns a compact state projection from per node structural embeddings to rapidly score and select a small subset of devices (a top-k partition) on which a conventional low-level actor performs focused beam search utilizing a critic agent. Selected candidate actions are evaluated with batched critic forwards and stored in an LRU cache keyed by a quantized state projection and local action identifiers, dramatically reducing redundant critic computation while preserving decision quality via conservative k-hop cache invalidation. Empirically, MetaDOAR attains higher player payoffs than SOTA baselines on large network topologies, without significant scaling issues in terms of memory usage or training time. This contribution provide a practical, theoretically motivated path to efficient hierarchical policy learning for large-scale networked decision problems.

</details>


### [92] [Steering diffusion models with quadratic rewards: a fine-grained analysis](https://arxiv.org/abs/2602.16570)
*Ankur Moitra,Andrej Risteski,Dhruv Rohatgi*

Main category: cs.LG

TL;DR: 该论文分析了从奖励倾斜扩散模型中采样的计算复杂性，重点关注二次奖励函数。证明了线性奖励倾斜总是可高效采样，低秩正定二次倾斜也可高效采样，但负定二次倾斜即使秩为1也是难解的。


<details>
  <summary>Details</summary>
Motivation: 推理时算法作为新兴范式，使用预训练模型作为子程序解决下游任务。然而，当前实践中部署的方法多是启发式的，存在各种失败模式，且对这些启发式何时能被高效改进的理解很少。

Method: 研究从奖励倾斜扩散模型采样的计算复杂性，即从 p*(x) ∝ p(x)exp(r(x)) 采样，其中 r 是奖励函数。使用线性奖励倾斜作为构建模块，结合新的概念性工具——Hubbard-Stratonovich变换，分析二次奖励函数 r(x) = x⊤Ax + b⊤x 的采样复杂性。

Result: 1. 线性奖励倾斜总是可高效采样；2. 低秩正定二次倾斜（A正定且秩为O(1)）可通过高效算法采样；3. 负定二次倾斜（r(x) = -x⊤Ax，A正定）即使秩为1也是难解的（需要指数级大条目）。

Conclusion: 该工作为推理时算法提供了精细的计算复杂性分析，揭示了不同奖励函数类型下采样问题的可解性边界，为理解何时启发式方法可被高效改进提供了理论基础。

Abstract: Inference-time algorithms are an emerging paradigm in which pre-trained models are used as subroutines to solve downstream tasks. Such algorithms have been proposed for tasks ranging from inverse problems and guided image generation to reasoning. However, the methods currently deployed in practice are heuristics with a variety of failure modes -- and we have very little understanding of when these heuristics can be efficiently improved.
  In this paper, we consider the task of sampling from a reward-tilted diffusion model -- that is, sampling from $p^{\star}(x) \propto p(x) \exp(r(x))$ -- given a reward function $r$ and pre-trained diffusion oracle for $p$. We provide a fine-grained analysis of the computational tractability of this task for quadratic rewards $r(x) = x^\top A x + b^\top x$. We show that linear-reward tilts are always efficiently sampleable -- a simple result that seems to have gone unnoticed in the literature. We use this as a building block, along with a conceptually new ingredient -- the Hubbard-Stratonovich transform -- to provide an efficient algorithm for sampling from low-rank positive-definite quadratic tilts, i.e. $r(x) = x^\top A x$ where $A$ is positive-definite and of rank $O(1)$. For negative-definite tilts, i.e. $r(x) = - x^\top A x$ where $A$ is positive-definite, we prove that the problem is intractable even if $A$ is of rank 1 (albeit with exponentially-large entries).

</details>


### [93] [MoDE-Boost: Boosting Shared Mobility Demand with Edge-Ready Prediction Models](https://arxiv.org/abs/2602.16573)
*Antonios Tziorvas,George S. Theodoropoulos,Yannis Theodoridis*

Main category: cs.LG

TL;DR: 提出两种梯度提升模型（分类和回归）用于城市需求预测，支持5分钟到1小时的时间范围，在五个大都市区的共享微出行数据上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 城市需求预测对智能交通系统中的路线优化、调度和拥堵管理至关重要。通过数据融合和分析技术，交通需求预测可作为识别新兴时空需求模式的关键中间措施，帮助改善共享微出行服务的效率。

Method: 提出两种梯度提升模型变体：一个用于分类，一个用于回归。两种模型都能生成从5分钟到1小时不同时间范围的需求预测。整体方法有效整合了时间和上下文特征，实现准确预测。

Result: 使用五个大都市区的电动滑板车和电动自行车网络的开源共享出行数据进行评估。与最先进方法以及基于生成式AI的模型进行比较，证明该方法能有效捕捉现代城市出行的复杂性。

Conclusion: 该方法为城市微出行管理提供了新颖见解，有助于应对快速城市化带来的挑战，从而促进更可持续、高效和宜居的城市发展。

Abstract: Urban demand forecasting plays a critical role in optimizing routing, dispatching, and congestion management within Intelligent Transportation Systems. By leveraging data fusion and analytics techniques, traffic demand forecasting serves as a key intermediate measure for identifying emerging spatial and temporal demand patterns. In this paper, we tackle this challenge by proposing two gradient boosting model variations, one for classiffication and one for regression, both capable of generating demand forecasts at various temporal horizons, from 5 minutes up to one hour. Our overall approach effectively integrates temporal and contextual features, enabling accurate predictions that are essential for improving the efficiency of shared (micro-) mobility services. To evaluate its effectiveness, we utilize open shared mobility data derived from e-scooter and e-bike networks in five metropolitan areas. These real-world datasets allow us to compare our approach with state-of-the-art methods as well as a Generative AI-based model, demonstrating its effectiveness in capturing the complexities of modern urban mobility. Ultimately, our methodology offers novel insights on urban micro-mobility management, helping to tackle the challenges arising from rapid urbanization and thus, contributing to more sustainable, efficient, and livable cities.

</details>


### [94] [AIFL: A Global Daily Streamflow Forecasting Model Using Deterministic LSTM Pre-trained on ERA5-Land and Fine-tuned on IFS](https://arxiv.org/abs/2602.16579)
*Maria Luisa Taccari,Kenza Tazi,Oisín M. Morrison,Andreas Grafberger,Juan Colonese,Corentin Carton de Wiart,Christel Prudhomme,Cinzia Mazzetti,Matthew Chantry,Florian Pappenberger*

Main category: cs.LG

TL;DR: AIFL是一个基于LSTM的确定性全球日径流预测模型，采用两阶段训练策略解决再分析到预报的领域偏移问题，在CARAVAN数据集上训练，性能优于现有全球系统。


<details>
  <summary>Details</summary>
Motivation: 数据驱动模型从历史再分析数据过渡到业务预报产品时存在性能差距，需要可靠的全球径流预测用于洪水准备和水资源管理。

Method: 提出AIFL模型，采用两阶段训练：先在ERA5-Land再分析数据（1980-2019）上预训练，然后在IFS控制预报（2016-2019）上微调，使用CARAVAN数据集中的18,588个流域。

Result: 在独立测试集（2021-2024）上，AIFL获得中位修正Kling-Gupta效率0.66和中位Nash-Sutcliffe效率0.53，与当前最先进全球系统竞争，在极端事件检测中表现优异。

Conclusion: AIFL是首个在CARAVAN生态系统中端到端训练的全球模型，为全球水文社区提供了透明、可复现且业务稳健的基线系统。

Abstract: Reliable global streamflow forecasting is essential for flood preparedness and water resource management, yet data-driven models often suffer from a performance gap when transitioning from historical reanalysis to operational forecast products. This paper introduces AIFL (Artificial Intelligence for Floods), a deterministic LSTM-based model designed for global daily streamflow forecasting. Trained on 18,588 basins curated from the CARAVAN dataset, AIFL utilises a novel two-stage training strategy to bridge the reanalysis-to-forecast domain shift. The model is first pre-trained on 40 years of ERA5-Land reanalysis (1980-2019) to capture robust hydrological processes, then fine-tuned on operational Integrated Forecasting System (IFS) control forecasts (2016-2019) to adapt to the specific error structures and biases of operational numerical weather prediction. To our knowledge, this is the first global model trained end-to-end within the CARAVAN ecosystem. On an independent temporal test set (2021-2024), AIFL achieves high predictive skill with a median modified Kling-Gupta Efficiency (KGE') of 0.66 and a median Nash-Sutcliffe Efficiency (NSE) of 0.53. Benchmarking results show that AIFL is highly competitive with current state-of-the-art global systems, achieving comparable accuracy while maintaining a transparent and reproducible forcing pipeline. The model demonstrates exceptional reliability in extreme-event detection, providing a streamlined and operationally robust baseline for the global hydrological community.

</details>


### [95] [Predicting The Cop Number Using Machine Learning](https://arxiv.org/abs/2602.16600)
*Meagan Mann,Christian Muise,Erin Meger*

Main category: cs.LG

TL;DR: 该研究探索使用机器学习方法预测图的警察数，发现树模型和图神经网络都能准确预测，且可解释性分析显示节点连通性、聚类、团结构和宽度参数是最具预测性的特征。


<details>
  <summary>Details</summary>
Motivation: 警察与强盗游戏中的警察数计算在计算上很困难，传统算法通常局限于小型图族。本研究旨在探索机器学习方法是否能从图的结构特性准确预测警察数，并识别哪些特性对预测影响最大。

Method: 使用经典机器学习方法（特别是基于树的模型）和图神经网络来预测图的警察数。通过可解释性分析识别最具预测性的图结构特征。

Result: 基于树的模型在处理类别不平衡的情况下实现了高精度预测，图神经网络在没有显式特征工程的情况下也取得了可比的结果。可解释性分析显示，最具预测性的特征与节点连通性、聚类、团结构和宽度参数相关。

Conclusion: 机器学习方法可以与现有的警察数算法互补使用，在计算不可行的情况下提供可扩展的近似解。预测模型识别出的关键特征与已知的理论结果一致。

Abstract: Cops and Robbers is a pursuit evasion game played on a graph, first introduced independently by Quilliot \cite{quilliot1978jeux} and Nowakowski and Winkler \cite{NOWAKOWSKI1983235} over four decades ago. A main interest in recent the literature is identifying the cop number of graph families. The cop number of a graph, $c(G)$, is defined as the minimum number of cops required to guarantee capture of the robber. Determining the cop number is computationally difficult and exact algorithms for this are typically restricted to small graph families. This paper investigates whether classical machine learning methods and graph neural networks can accurately predict a graph's cop number from its structural properties and identify which properties most strongly influence this prediction. Of the classical machine learning models, tree-based models achieve high accuracy in prediction despite class imbalance, whereas graph neural networks achieve comparable results without explicit feature engineering. The interpretability analysis shows that the most predictive features are related to node connectivity, clustering, clique structure, and width parameters, which aligns with known theoretical results. Our findings suggest that machine learning approaches can be used in complement with existing cop number algorithms by offering scalable approximations where computation is infeasible.

</details>


### [96] [A Systematic Evaluation of Sample-Level Tokenization Strategies for MEG Foundation Models](https://arxiv.org/abs/2602.16626)
*SungJun Cho,Chetan Gohil,Rukuang Huang,Oiwi Parker Jones,Mark W. Woolrich*

Main category: cs.LG

TL;DR: 本文系统评估了神经影像数据（MEG）的tokenization策略，比较了可学习和不可学习方法，发现两者在重建精度和下游任务表现上相当，表明简单的固定tokenization策略足以用于神经基础模型开发。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理成功推动大规模基础模型发展，神经影像数据也需要类似方法。然而，连续神经时间序列数据的离散化（tokenization）策略对模型性能的影响尚不明确，需要系统评估不同tokenization方法的效果。

Method: 提出了基于样本级别的tokenization策略系统评估框架，比较可学习和不可学习方法。对于可学习方法，引入了基于自编码器的新方法。在三个公开MEG数据集上进行实验，评估信号重建保真度、基础模型性能（token预测、生成数据生物学合理性、主体特异性信息保留、下游任务表现）。

Result: 实验结果显示，可学习和不可学习的离散化方案都能达到高重建精度，在大多数评估标准上表现相当。这表明简单的固定样本级别tokenization策略可以用于神经基础模型开发。

Conclusion: 神经影像数据的tokenization策略选择相对灵活，简单的固定tokenization方法已经足够有效，为神经基础模型的开发提供了实用指导。

Abstract: Recent success in natural language processing has motivated growing interest in large-scale foundation models for neuroimaging data. Such models often require discretization of continuous neural time series data, a process referred to as 'tokenization'. However, the impact of different tokenization strategies for neural data is currently poorly understood. In this work, we present a systematic evaluation of sample-level tokenization strategies for transformer-based large neuroimaging models (LNMs) applied to magnetoencephalography (MEG) data. We compare learnable and non-learnable tokenizers by examining their signal reconstruction fidelity and their impact on subsequent foundation modeling performance (token prediction, biological plausibility of generated data, preservation of subject-specific information, and performance on downstream tasks). For the learnable tokenizer, we introduce a novel approach based on an autoencoder. Experiments were conducted on three publicly available MEG datasets spanning different acquisition sites, scanners, and experimental paradigms. Our results show that both learnable and non-learnable discretization schemes achieve high reconstruction accuracy and broadly comparable performance across most evaluation criteria, suggesting that simple fixed sample-level tokenization strategies can be used in the development of neural foundation models. The code is available at https://github.com/OHBA-analysis/Cho2026_Tokenizer.

</details>


### [97] [Almost Sure Convergence of Differential Temporal Difference Learning for Average Reward Markov Decision Processes](https://arxiv.org/abs/2602.16629)
*Ethan Blaser,Jiuqi Wang,Shangtong Zhang*

Main category: cs.LG

TL;DR: 本文证明了在标准递减学习率下，无需局部时钟的n步差分TD算法的几乎必然收敛，为平均奖励RL提供了更实用的理论保证。


<details>
  <summary>Details</summary>
Motivation: 差分TD学习算法是平均奖励强化学习的重要进展，但现有收敛保证需要基于状态访问计数的局部时钟学习率，这与实际应用不符且无法扩展到表格之外。本文旨在解决这一限制。

Method: 证明了在标准递减学习率下，无需局部时钟的on-policy n步差分TD算法的几乎必然收敛。然后推导了三个充分条件，确保off-policy n步差分TD算法在无局部时钟时也能收敛。

Result: 成功证明了on-policy n步差分TD算法在标准递减学习率下的几乎必然收敛，并确定了off-policy收敛的三个充分条件，将理论分析更贴近实际实现。

Conclusion: 这些结果加强了差分TD算法的理论基础，使其收敛分析更接近实际实现，为平均奖励强化学习提供了更实用的理论保证。

Abstract: The average reward is a fundamental performance metric in reinforcement learning (RL) focusing on the long-run performance of an agent. Differential temporal difference (TD) learning algorithms are a major advance for average reward RL as they provide an efficient online method to learn the value functions associated with the average reward in both on-policy and off-policy settings. However, existing convergence guarantees require a local clock in learning rates tied to state visit counts, which practitioners do not use and does not extend beyond tabular settings. We address this limitation by proving the almost sure convergence of on-policy $n$-step differential TD for any $n$ using standard diminishing learning rates without a local clock. We then derive three sufficient conditions under which off-policy $n$-step differential TD also converges without a local clock. These results strengthen the theoretical foundations of differential TD and bring its convergence analysis closer to practical implementations.

</details>


### [98] [Optimizer choice matters for the emergence of Neural Collapse](https://arxiv.org/abs/2602.16642)
*Jim Zhao,Tin Sum Cheng,Wojciech Masarczyk,Aurelien Lucchi*

Main category: cs.LG

TL;DR: 本文挑战了神经坍缩（NC）与优化器无关的假设，证明优化器选择对NC出现至关重要，并引入新诊断指标NC0来分析不同优化器（SGD、Adam、AdamW）对NC的影响。


<details>
  <summary>Details</summary>
Motivation: 现有神经坍缩理论大多忽略优化器的作用，认为NC是普遍现象。本文挑战这一假设，旨在揭示优化器选择对NC出现的关键影响，特别是权重衰减耦合机制在优化器隐式偏置中的作用。

Method: 引入新诊断指标NC0（收敛到零是NC的必要条件），理论分析SGD、带耦合权重衰减的SignGD（Adam特例）、带解耦权重衰减的SignGD（AdamW特例）的NC0动态差异。通过3900次训练实验验证，涵盖多种数据集、架构、优化器和超参数。

Result: 理论证明：1）自适应优化器中的解耦权重衰减（如AdamW）无法产生NC；2）不同优化器（SGD、Adam、AdamW）的NC0动态存在本质差异；3）动量在SGD训练中能加速NC（超越训练损失收敛）。实验结果全面支持理论分析。

Conclusion: 这是首个解释优化器依赖的NC出现的理论工作，揭示了权重衰减耦合机制在塑造优化器隐式偏置中的关键作用，挑战了NC与优化器无关的普遍假设。

Abstract: Neural Collapse (NC) refers to the emergence of highly symmetric geometric structures in the representations of deep neural networks during the terminal phase of training. Despite its prevalence, the theoretical understanding of NC remains limited. Existing analyses largely ignore the role of the optimizer, thereby suggesting that NC is universal across optimization methods. In this work, we challenge this assumption and demonstrate that the choice of optimizer plays a critical role in the emergence of NC. The phenomenon is typically quantified through NC metrics, which, however, are difficult to track and analyze theoretically. To overcome this limitation, we introduce a novel diagnostic metric, NC0, whose convergence to zero is a necessary condition for NC. Using NC0, we provide theoretical evidence that NC cannot emerge under decoupled weight decay in adaptive optimizers, as implemented in AdamW. Concretely, we prove that SGD, SignGD with coupled weight decay (a special case of Adam), and SignGD with decoupled weight decay (a special case of AdamW) exhibit qualitatively different NC0 dynamics. Also, we show the accelerating effect of momentum on NC (beyond convergence of train loss) when trained with SGD, being the first result concerning momentum in the context of NC. Finally, we conduct extensive empirical experiments consisting of 3,900 training runs across various datasets, architectures, optimizers, and hyperparameters, confirming our theoretical results. This work provides the first theoretical explanation for optimizer-dependent emergence of NC and highlights the overlooked role of weight-decay coupling in shaping the implicit biases of optimizers.

</details>


### [99] [Factorization Machine with Quadratic-Optimization Annealing for RNA Inverse Folding and Evaluation of Binary-Integer Encoding and Nucleotide Assignment](https://arxiv.org/abs/2602.16643)
*Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: 提出使用因子分解机与二次优化退火(FMQA)解决RNA逆折叠问题，并系统分析核苷酸-整数分配和二进制编码方法对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有RNA逆折叠方法通常需要大量序列评估，实验验证成本高。FMQA作为一种离散黑盒优化方法，能以有限评估次数获得高质量解，但其在RNA逆折叠中的应用尚未充分探索，特别是核苷酸编码方式对性能的影响。

Method: 建立FMQA框架用于RNA逆折叠，系统评估24种核苷酸-整数分配方式(0-3)与四种二进制编码方法(one-hot、domain-wall、binary、unary)的组合效果。

Result: one-hot和domain-wall编码在归一化集成缺陷值上优于binary和unary编码。在domain-wall编码中，边界整数(0和3)对应的核苷酸出现频率更高。将鸟嘌呤(G)和胞嘧啶(C)分配给边界整数能促进它们在茎区富集，获得比one-hot编码更热力学稳定的二级结构。

Conclusion: 成功建立FMQA框架用于RNA逆折叠，并证明编码方式选择对优化性能有显著影响。domain-wall编码结合特定核苷酸分配策略能生成更稳定的RNA二级结构。

Abstract: The RNA inverse folding problem aims to identify nucleotide sequences that preferentially adopt a given target secondary structure. While various heuristic and machine learning-based approaches have been proposed, many require a large number of sequence evaluations, which limits their applicability when experimental validation is costly. We propose a method to solve the problem using a factorization machine with quadratic-optimization annealing (FMQA). FMQA is a discrete black-box optimization method reported to obtain high-quality solutions with a limited number of evaluations. Applying FMQA to the problem requires converting nucleotides into binary variables. However, the influence of integer-to-nucleotide assignments and binary-integer encoding on the performance of FMQA has not been thoroughly investigated, even though such choices determine the structure of the surrogate model and the search landscape, and thus can directly affect solution quality. Therefore, this study aims both to establish a novel FMQA framework for RNA inverse folding and to analyze the effects of these assignments and encoding methods. We evaluated all 24 possible assignments of the four nucleotides to the ordered integers (0-3), in combination with four binary-integer encoding methods. Our results demonstrated that one-hot and domain-wall encodings outperform binary and unary encodings in terms of the normalized ensemble defect value. In domain-wall encoding, nucleotides assigned to the boundary integers (0 and 3) appeared with higher frequency. In the RNA inverse folding problem, assigning guanine and cytosine to these boundary integers promoted their enrichment in stem regions, which led to more thermodynamically stable secondary structures than those obtained with one-hot encoding.

</details>


### [100] [Neighborhood Stability as a Measure of Nearest Neighbor Searchability](https://arxiv.org/abs/2602.16673)
*Thomas Vecchiato,Sebastian Bruch*

Main category: cs.LG

TL;DR: 提出了两种衡量高维数据聚类搜索能力的指标：聚类邻域稳定性度量（clustering-NSM）和点邻域稳定性度量（point-NSM），用于预测聚类近似最近邻搜索的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于聚类的近似最近邻搜索（ANNS）很流行，但缺乏分析工具来确定数据集是否适合这种搜索方法（即"可搜索性"）。现有方法无法评估聚类ANNS对特定数据集的适用性。

Method: 提出了两种度量：1）聚类邻域稳定性度量（clustering-NSM）作为聚类质量的内在度量，可预测ANNS的准确性；2）点邻域稳定性度量（point-NSM）作为数据集可聚类性的度量，可预测clustering-NSM。两者都基于点之间的最近邻关系而非距离，适用于包括内积在内的各种距离函数。

Result: 开发了两种能够仅通过数据点本身就能确定数据集是否适合聚类ANNS搜索的度量工具，填补了该领域的分析空白。

Conclusion: 提出的clustering-NSM和point-NSM为评估高维数据集的聚类搜索能力提供了理论工具，有助于在实际应用前预测聚类ANNS的性能表现。

Abstract: Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its popularity, there are virtually no analytical tools to determine the suitability of clustering-based ANNS for a given dataset -- what we call "searchability." To address that gap, we present two measures for flat clusterings of high-dimensional points in Euclidean space. First is Clustering-Neighborhood Stability Measure (clustering-NSM), an internal measure of clustering quality -- a function of a clustering of a dataset -- that we show to be predictive of ANNS accuracy. The second, Point-Neighborhood Stability Measure (point-NSM), is a measure of clusterability -- a function of the dataset itself -- that is predictive of clustering-NSM. The two together allow us to determine whether a dataset is searchable by clustering-based ANNS given only the data points. Importantly, both are functions of nearest neighbor relationships between points, not distances, making them applicable to various distance functions including inner product.

</details>


### [101] [Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition](https://arxiv.org/abs/2602.16684)
*Bo Pan,Peter Zhiping Zhang,Hao-Wei Pang,Alex Zhu,Xiang Yu,Liying Zhang,Liang Zhao*

Main category: cs.LG

TL;DR: 提出基于匹配分子对（MMP）的变量到变量生成框架，通过大规模MMP变换训练基础模型，结合提示机制和检索增强生成，实现可控的类似物设计。


<details>
  <summary>Details</summary>
Motivation: 现有ML方法要么在分子层面操作编辑可控性有限，要么从小规模受限设置中学习MMP式编辑，需要更灵活可控的类似物生成方法。

Method: 采用变量到变量生成框架，在大规模MMP变换上训练基础模型；开发提示机制让用户指定偏好变换模式；引入MMPT-RAG检索增强框架，利用外部参考类似物作为上下文指导。

Result: 在通用化学语料库和专利数据集上实验显示，方法在多样性、新颖性和可控性方面均有提升，能够在实际发现场景中恢复真实的类似物结构。

Conclusion: 提出的MMP基础模型结合提示机制和检索增强生成，为药物化学家提供了更灵活可控的类似物设计工具，在实际药物发现场景中具有实用价值。

Abstract: Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.

</details>


### [102] [Protecting the Undeleted in Machine Unlearning](https://arxiv.org/abs/2602.16697)
*Aloni Cohen,Refael Kohen,Kobbi Nissim,Uri Stemmer*

Main category: cs.LG

TL;DR: 论文指出机器学习中的"完美再训练"遗忘方法存在严重隐私风险，攻击者通过少量数据点就能重构整个数据集，提出了新的安全定义来保护未删除数据。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在从训练模型中移除特定数据点，通常追求"完美再训练"——即产生从未包含删除数据时训练得到的模型。然而，这种方法及其安全定义对剩余（未删除）数据点存在重大隐私风险。

Method: 1）展示重建攻击：证明对于某些可通过安全计算完成的任务，遵循完美再训练的机制允许攻击者仅控制少量数据点就能通过删除请求重建几乎整个数据集；2）调查现有机器遗忘定义，发现它们要么易受此类攻击，要么过于严格无法支持基本功能；3）提出新的安全定义，专门保护未删除数据免受其他点删除导致的泄漏。

Result: 发现完美再训练方法存在严重安全漏洞，攻击者可通过少量数据点重构整个数据集。现有定义要么不安全要么功能受限。提出的新安全定义能够支持公告板、求和、统计学习等基本功能。

Conclusion: 机器遗忘的"完美再训练"方法存在重大隐私风险，需要新的安全定义来保护未删除数据。提出的定义在安全性和功能性之间取得平衡，支持多种基本机器学习任务。

Abstract: Machine unlearning aims to remove specific data points from a trained model, often striving to emulate "perfect retraining", i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $ω(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.

</details>


### [103] [Causality is Key for Interpretability Claims to Generalise](https://arxiv.org/abs/2602.16698)
*Shruti Joshi,Aaron Mueller,David Klindt,Wieland Brendel,Patrik Reizinger,Dhanya Sridhar*

Main category: cs.LG

TL;DR: 该论文提出使用因果推理框架来规范大语言模型可解释性研究，通过因果层次结构明确不同证据能支持的结论类型，并引入因果表示学习来指导可解释性方法的选择和评估。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型可解释性研究存在两个主要问题：1）研究发现难以泛化；2）因果解释超出了证据支持的范围。需要建立更严谨的框架来确保可解释性研究的有效性和可靠性。

Method: 采用Pearl的因果层次结构来区分不同层次的证据支持：观察性证据建立关联，干预性证据（如消融或激活修补）支持因果效应，而反事实主张需要额外监督。结合因果表示学习来指定从激活中可恢复的变量及其假设条件。

Result: 提出了一个诊断框架，帮助研究者根据想要支持的结论类型选择相应的方法和评估指标，确保研究发现能够泛化，避免因果解释超出证据支持的范围。

Conclusion: 因果推理为LLM可解释性研究提供了严谨的理论基础，通过因果层次结构和因果表示学习的结合，可以建立更可靠、可泛化的解释框架，指导实践者做出证据与主张相匹配的研究设计。

Abstract: Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl's causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.

</details>


### [104] [Knowledge-Embedded Latent Projection for Robust Representation Learning](https://arxiv.org/abs/2602.16709)
*Weijing Tang,Ming Yuan,Zongqi Xia,Tianxi Cai*

Main category: cs.LG

TL;DR: 提出了一种知识嵌入的潜在投影模型，利用外部语义嵌入来正则化表示学习，解决高维离散数据矩阵在维度不平衡时的估计挑战。


<details>
  <summary>Details</summary>
Motivation: 在电子健康记录等应用中，患者-特征矩阵存在维度不平衡问题（患者维度远小于特征维度），传统潜在空间模型估计困难。同时，临床概念等语义嵌入的可用性不断增加，为利用外部知识提供了机会。

Method: 提出知识嵌入潜在投影模型，将列嵌入建模为语义嵌入的平滑函数（通过再生核希尔伯特空间映射）。开发了两步估计方法：1）通过核主成分分析进行语义引导的子空间构建；2）可扩展的投影梯度下降优化。

Result: 建立了估计误差界，刻画了统计误差与核投影诱导的近似误差之间的权衡。提供了非凸优化过程的局部收敛保证。模拟研究和真实电子健康记录应用验证了方法的有效性。

Conclusion: 该方法通过利用外部语义知识来正则化表示学习，有效解决了高维离散数据矩阵在维度不平衡情况下的估计问题，在电子健康记录分析等应用中具有实用价值。

Abstract: Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [105] [Generalized Leverage Score for Scalable Assessment of Privacy Vulnerability](https://arxiv.org/abs/2602.15919)
*Valentin Dorseuil,Jamal Atif,Olivier Cappé*

Main category: stat.ML

TL;DR: 论文提出了一种无需重新训练模型或模拟攻击即可评估数据点隐私漏洞的方法，通过建立成员推理攻击风险与数据点对模型影响之间的理论联系，并提出了计算高效的深度学习泛化杠杆评分。


<details>
  <summary>Details</summary>
Motivation: 传统评估数据点隐私漏洞需要重新训练模型或模拟攻击，计算成本高。本文旨在寻找一种更高效的方法，在不进行这些操作的情况下评估个体数据点的隐私风险。

Method: 在线性模型设置中，建立了成员推理攻击风险与杠杆评分之间的理论对应关系。基于此，提出了深度学习环境下杠杆评分的计算高效泛化方法，用于评估数据点的隐私漏洞。

Result: 实证评估显示，提出的评分与成员推理攻击成功率之间存在强相关性，验证了该评分作为个体隐私风险评估的实用替代指标的有效性。

Conclusion: 数据点的成员推理攻击风险本质上由其对该模型的影响决定，杠杆评分可作为评估个体隐私漏洞的原则性指标，无需训练影子模型的计算负担，为隐私风险评估提供了高效实用的方法。

Abstract: Can the privacy vulnerability of individual data points be assessed without retraining models or explicitly simulating attacks? We answer affirmatively by showing that exposure to membership inference attack (MIA) is fundamentally governed by a data point's influence on the learned model. We formalize this in the linear setting by establishing a theoretical correspondence between individual MIA risk and the leverage score, identifying it as a principled metric for vulnerability. This characterization explains how data-dependent sensitivity translates into exposure, without the computational burden of training shadow models. Building on this, we propose a computationally efficient generalization of the leverage score for deep learning. Empirical evaluations confirm a strong correlation between the proposed score and MIA success, validating this metric as a practical surrogate for individual privacy risk assessment.

</details>


### [106] [Including Node Textual Metadata in Laplacian-constrained Gaussian Graphical Models](https://arxiv.org/abs/2602.15920)
*Jianhua Wang,Killian Cressant,Pedro Braconnot Velloso,Arnaud Breloy*

Main category: stat.ML

TL;DR: 提出一种基于拉普拉斯约束高斯图模型的图学习方法，联合利用节点信号和元数据，通过MM算法优化，在金融数据集上显著提升图聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统高斯图模型学习通常忽略节点相关的辅助元数据（如文本描述），而这些信息可能对图结构学习有重要价值。需要填补这一空白，充分利用信号和元数据两种信息源。

Method: 提出基于拉普拉斯约束高斯图模型的图学习方法，将节点信号和元数据联合建模。设计了一个优化问题，并开发了高效的主最小化算法，每次迭代都有闭式解。

Result: 在真实金融数据集上的实验表明，该方法相比仅使用信号或元数据的现有方法，显著提升了图聚类性能，验证了融合两种信息源的价值。

Conclusion: 通过联合利用节点信号和元数据，基于拉普拉斯约束高斯图模型的方法能够有效提升图学习性能，特别是在图聚类任务中表现出显著优势。

Abstract: This paper addresses graph learning in Gaussian Graphical Models (GGMs). In this context, data matrices often come with auxiliary metadata (e.g., textual descriptions associated with each node) that is usually ignored in traditional graph estimation processes. To fill this gap, we propose a graph learning approach based on Laplacian-constrained GGMs that jointly leverages the node signals and such metadata. The resulting formulation yields an optimization problem, for which we develop an efficient majorization-minimization (MM) algorithm with closed-form updates at each iteration. Experimental results on a real-world financial dataset demonstrate that the proposed method significantly improves graph clustering performance compared to state-of-the-art approaches that use either signals or metadata alone, thus illustrating the interest of fusing both sources of information.

</details>


### [107] [Robust Stochastic Gradient Posterior Sampling with Lattice Based Discretisation](https://arxiv.org/abs/2602.15925)
*Zier Mensch,Lars Holdijk,Samuel Duffield,Maxwell Aifer,Patrick J. Coles,Max Welling,Miranda C. N. Cheng*

Main category: stat.ML

TL;DR: 提出SGLRW方法，通过仅在更新协方差非对角元素引入随机噪声，提高对minibatch大小的鲁棒性，相比SGLD在重尾梯度噪声下更稳定。


<details>
  <summary>Details</summary>
Motivation: 随机梯度MCMC方法虽然能实现可扩展的贝叶斯后验采样，但对minibatch大小和梯度噪声敏感，需要更鲁棒的算法。

Method: 扩展Lattice Random Walk离散化，提出SGLRW方法，仅在更新协方差的非对角元素引入随机噪声，同时分析了使用梯度裁剪的SGLD类比方法。

Result: 在贝叶斯回归和分类实验中，SGLRW在SGLD失效的情况下保持稳定，包括存在重尾梯度噪声时，预测性能相当或更好。

Conclusion: SGLRW通过创新的噪声引入方式，提供了对minibatch大小更鲁棒的随机梯度MCMC方法，在保持渐近正确性的同时提高了稳定性。

Abstract: Stochastic-gradient MCMC methods enable scalable Bayesian posterior sampling but often suffer from sensitivity to minibatch size and gradient noise. To address this, we propose Stochastic Gradient Lattice Random Walk (SGLRW), an extension of the Lattice Random Walk discretization. Unlike conventional Stochastic Gradient Langevin Dynamics (SGLD), SGLRW introduces stochastic noise only through the off-diagonal elements of the update covariance; this yields greater robustness to minibatch size while retaining asymptotic correctness. Furthermore, as comparison we analyze a natural analogue of SGLD utilizing gradient clipping. Experimental validation on Bayesian regression and classification demonstrates that SGLRW remains stable in regimes where SGLD fails, including in the presence of heavy-tailed gradient noise, and matches or improves predictive performance.

</details>


### [108] [Partial Identification under Missing Data Using Weak Shadow Variables from Pretrained Models](https://arxiv.org/abs/2602.16061)
*Hongyu Chen,David Simchi-Levi,Ruoxuan Xiong*

Main category: stat.ML

TL;DR: 提出一个部分识别框架，利用预训练模型（如LLM）的预测作为弱影子变量，通过线性规划获得估计量的锐利边界，在MNAR数据下提供有效覆盖。


<details>
  <summary>Details</summary>
Motivation: 在平台评估和社会科学中，从用户反馈估计总体数量（如平均结果）是基础工作，但反馈通常是"非随机缺失"（MNAR）的：意见更强的用户更可能回应，导致标准估计量有偏且在没有额外假设时无法识别。现有方法通常依赖强参数假设或定制辅助变量，而这些在实践中可能不可用。

Method: 开发部分识别框架，通过求解一对线性规划获得估计量的锐利边界，约束条件编码观测数据结构。框架自然整合预训练模型（包括LLM）的预测作为额外线性约束来收紧可行集。这些预测被称为"弱影子变量"：满足关于缺失的条件独立性假设，但不需要满足经典影子变量方法的完备性条件。在有限样本中，提出集合扩展估计量，在集合识别机制下实现慢于√n的收敛速率，在点识别下实现标准√n速率。

Result: 在模拟和半合成实验（客户服务对话）中，发现LLM预测对于经典影子变量方法通常条件不良，但在本框架中仍然非常有效。它们将识别区间缩小75-83%，同时在现实的MNAR机制下保持有效覆盖。

Conclusion: 提出的部分识别框架利用预训练模型预测作为弱影子变量，在MNAR数据下提供稳健的估计边界。当预测足够信息丰富时，边界收敛到单点，恢复标准识别作为特例。该方法在保持有效覆盖的同时显著缩小识别区间。

Abstract: Estimating population quantities such as mean outcomes from user feedback is fundamental to platform evaluation and social science, yet feedback is often missing not at random (MNAR): users with stronger opinions are more likely to respond, so standard estimators are biased and the estimand is not identified without additional assumptions. Existing approaches typically rely on strong parametric assumptions or bespoke auxiliary variables that may be unavailable in practice. In this paper, we develop a partial identification framework in which sharp bounds on the estimand are obtained by solving a pair of linear programs whose constraints encode the observed data structure. This formulation naturally incorporates outcome predictions from pretrained models, including large language models (LLMs), as additional linear constraints that tighten the feasible set. We call these predictions weak shadow variables: they satisfy a conditional independence assumption with respect to missingness but need not meet the completeness conditions required by classical shadow-variable methods. When predictions are sufficiently informative, the bounds collapse to a point, recovering standard identification as a special case. In finite samples, to provide valid coverage of the identified set, we propose a set-expansion estimator that achieves slower-than-$\sqrt{n}$ convergence rate in the set-identified regime and the standard $\sqrt{n}$ rate under point identification. In simulations and semi-synthetic experiments on customer-service dialogues, we find that LLM predictions are often ill-conditioned for classical shadow-variable methods yet remain highly effective in our framework. They shrink identification intervals by 75--83\% while maintaining valid coverage under realistic MNAR mechanisms.

</details>


### [109] [Empirical Cumulative Distribution Function Clustering for LLM-based Agent System Analysis](https://arxiv.org/abs/2602.16131)
*Chihiro Watanabe,Jingyu Sun*

Main category: stat.ML

TL;DR: 提出基于经验累积分布函数(ECDF)的LLM代理评估框架，通过响应与参考答案的余弦相似度分布进行更细致的质量评估，并使用聚类分析揭示不同配置下的响应分布差异。


<details>
  <summary>Details</summary>
Motivation: 传统LLM代理评估方法（如多数投票聚合）会掩盖原始响应的质量和分布特征，需要更细致的评估框架来揭示不同配置下的响应质量分布差异。

Method: 1) 基于响应与参考答案余弦相似度的ECDF评估框架；2) 使用距离度量和k-medoids算法对ECDF进行聚类分析，以识别不同代理配置下的响应分布模式。

Result: 在QA数据集上的实验表明，ECDF能够区分具有相似最终准确率但质量分布不同的代理设置，聚类分析揭示了温度、角色设定和问题主题对响应分布的可解释影响。

Conclusion: 提出的ECDF评估框架提供了比传统聚合方法更细致的LLM代理响应质量分析，有助于理解不同配置参数对响应分布的影响，为模型优化提供更深入的洞察。

Abstract: Large language models (LLMs) are increasingly used as agents to solve complex tasks such as question answering (QA), scientific debate, and software development. A standard evaluation procedure aggregates multiple responses from LLM agents into a single final answer, often via majority voting, and compares it against reference answers. However, this process can obscure the quality and distributional characteristics of the original responses. In this paper, we propose a novel evaluation framework based on the empirical cumulative distribution function (ECDF) of cosine similarities between generated responses and reference answers. This enables a more nuanced assessment of response quality beyond exact match metrics. To analyze the response distributions across different agent configurations, we further introduce a clustering method for ECDFs using their distances and the $k$-medoids algorithm. Our experiments on a QA dataset demonstrate that ECDFs can distinguish between agent settings with similar final accuracies but different quality distributions. The clustering analysis also reveals interpretable group structures in the responses, offering insights into the impact of temperature, persona, and question topics.

</details>


### [110] [Conjugate Learning Theory: Uncovering the Mechanisms of Trainability and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.16177)
*Binchuan Qi*

Main category: stat.ML

TL;DR: 提出基于凸共轭对偶的共轭学习理论框架，从有限样本角度定义实用可学习性，分析SGD训练DNN的全局最优性、模型架构影响、经验风险下界，以及基于广义条件熵的泛化误差确定性/概率性界。


<details>
  <summary>Details</summary>
Motivation: 现有学习理论多关注渐近性能，缺乏对有限样本场景下实用可学习性的严格理论框架。需要建立能够同时解释深度神经网络优化和泛化行为的统一理论，特别是分析SGD训练非凸DNN达到全局最优的条件，以及量化模型架构、数据特性对训练和泛化的影响。

Method: 提出基于凸共轭对偶的共轭学习理论框架，从有限样本角度定义实用可学习性。通过控制结构矩阵极端特征值和梯度能量，证明SGD训练DNN能达到经验风险全局最优。推导模型无关的经验风险下界，并基于广义条件熵建立泛化误差的确定性和概率性界。

Result: 理论证明：1) SGD训练DNN在控制结构矩阵特征值和梯度能量条件下可达全局最优；2) 数据决定训练性的根本极限；3) 泛化误差界明确量化了模型不可逆性、最大损失值、特征-标签广义条件熵三个关键因素的影响；4) 批大小、模型架构深度/参数/稀疏性/跳跃连接等影响非凸优化。

Conclusion: 提出的共轭学习理论框架为有限样本下的实用可学习性提供了统一的理论基础，成功解释了DNN训练和泛化的关键机制，并通过实验验证了理论预测的正确性和一致性，为理解深度学习的理论基础提供了新视角。

Abstract: In this work, we propose a notion of practical learnability grounded in finite sample settings, and develop a conjugate learning theoretical framework based on convex conjugate duality to characterize this learnability property. Building on this foundation, we demonstrate that training deep neural networks (DNNs) with mini-batch stochastic gradient descent (SGD) achieves global optima of empirical risk by jointly controlling the extreme eigenvalues of a structure matrix and the gradient energy, and we establish a corresponding convergence theorem. We further elucidate the impact of batch size and model architecture (including depth, parameter count, sparsity, skip connections, and other characteristics) on non-convex optimization. Additionally, we derive a model-agnostic lower bound for the achievable empirical risk, theoretically demonstrating that data determines the fundamental limit of trainability. On the generalization front, we derive deterministic and probabilistic bounds on generalization error based on generalized conditional entropy measures. The former explicitly delineates the range of generalization error, while the latter characterizes the distribution of generalization error relative to the deterministic bounds under independent and identically distributed (i.i.d.) sampling conditions. Furthermore, these bounds explicitly quantify the influence of three key factors: (i) information loss induced by irreversibility in the model, (ii) the maximum attainable loss value, and (iii) the generalized conditional entropy of features with respect to labels. Moreover, they offer a unified theoretical lens for understanding the roles of regularization, irreversible transformations, and network depth in shaping the generalization behavior of deep neural networks. Extensive experiments validate all theoretical predictions, confirming the framework's correctness and consistency.

</details>


### [111] [On sparsity, extremal structure, and monotonicity properties of Wasserstein and Gromov-Wasserstein optimal transport plans](https://arxiv.org/abs/2602.16265)
*Titouan Vayer*

Main category: stat.ML

TL;DR: 本文系统比较了Gromov-Wasserstein距离与线性最优传输的差异，重点探讨了GW最优传输计划的稀疏性、置换支撑条件以及循环单调性等关键性质。


<details>
  <summary>Details</summary>
Motivation: GW距离作为度量空间之间的最优传输框架，相比线性OT具有更复杂的数学性质。作者旨在系统梳理GW距离的重要特性，特别是其最优传输计划的结构特征，以深化对这一重要工具的理论理解。

Method: 采用理论分析方法，基于条件负半定性质这一关键数学条件，推导GW最优传输计划的结构特性。通过对比线性OT框架，系统探讨稀疏性、置换支撑和循环单调性等性质。

Result: 证明了当条件负半定性质成立时，存在稀疏的GW最优传输计划，且这些计划可以支撑在置换上。这一结果为GW距离的理论分析和实际应用提供了重要基础。

Conclusion: GW距离在特定条件下具有与线性OT类似的结构性质，条件负半定性质是保证GW最优传输计划稀疏性和置换支撑的关键条件，这为GW距离的理论发展和实际应用提供了重要指导。

Abstract: This note gives a self-contained overview of some important properties of the Gromov-Wasserstein (GW) distance, compared with the standard linear optimal transport (OT) framework. More specifically, I explore the following questions: are GW optimal transport plans sparse? Under what conditions are they supported on a permutation? Do they satisfy a form of cyclical monotonicity? In particular, I present the conditionally negative semi-definite property and show that, when it holds, there are GW optimal plans that are sparse and supported on a permutation.

</details>


### [112] [Machine Learning in Epidemiology](https://arxiv.org/abs/2602.16352)
*Marvin N. Wright,Lukas Burk,Pegah Golchian,Jan Kapar,Niklas Koenen,Sophie Hanna Langbein*

Main category: stat.ML

TL;DR: 本章为流行病学中应用机器学习提供方法论基础，涵盖监督/无监督学习原理、重要方法、模型评估策略、超参数优化和可解释机器学习，并配有R语言代码示例。


<details>
  <summary>Details</summary>
Motivation: 数字流行病学时代，流行病学家面临数据量激增、复杂性和维度增加的挑战，需要机器学习工具来分析海量复杂数据。

Method: 建立流行病学中机器学习的应用框架，包括监督学习与无监督学习原理、重要机器学习方法、模型评估策略、超参数优化方法，并引入可解释机器学习概念。

Result: 提供完整的机器学习在流行病学中的应用方法论，通过心脏病数据集的实际R代码示例，使理论内容与实践应用相结合。

Conclusion: 本章为流行病学家提供了系统化的机器学习应用指南，通过理论结合实践的方式，帮助研究人员有效利用机器学习工具处理复杂的流行病学数据。

Abstract: In the age of digital epidemiology, epidemiologists are faced by an increasing amount of data of growing complexity and dimensionality. Machine learning is a set of powerful tools that can help to analyze such enormous amounts of data. This chapter lays the methodological foundations for successfully applying machine learning in epidemiology. It covers the principles of supervised and unsupervised learning and discusses the most important machine learning methods. Strategies for model evaluation and hyperparameter optimization are developed and interpretable machine learning is introduced. All these theoretical parts are accompanied by code examples in R, where an example dataset on heart disease is used throughout the chapter.

</details>


### [113] [Learning Preference from Observed Rankings](https://arxiv.org/abs/2602.16476)
*Yu-Chang Chen,Chen Chian Fuh,Shang En Tsai*

Main category: stat.ML

TL;DR: 该论文提出了一个从部分排序信息中学习消费者偏好的灵活框架，通过将观察到的排序解释为具有逻辑选择概率的成对比较，并校正选择偏差。


<details>
  <summary>Details</summary>
Motivation: 估计消费者偏好是经济学和市场营销中的核心问题。现有方法需要处理部分排序信息、解释性、跨消费者和商品的信息共享，以及选择偏差（只有进入考虑集的商品才会被比较）。

Method: 将观察到的排序建模为具有逻辑选择概率的成对比较。潜在效用包括可解释的产品属性、商品固定效应和低秩用户-商品因子结构。通过商品级可观察倾向的乘积建模成对可观察性，使用逻辑模型估计边际概率。偏好参数通过最大化逆概率加权（IPW）、岭正则化的对数似然函数进行估计，将观察到的比较重新加权到目标比较群体。为扩展计算，提出了基于逆概率重采样的随机梯度下降（SGD）算法。

Result: 在在线葡萄酒零售商的交易数据应用中，该方法相对于基于流行度的基准提高了样本外推荐性能，特别是在预测先前未消费产品的购买方面表现出显著优势。

Conclusion: 该框架能够从部分排序信息中灵活学习消费者偏好，同时处理选择偏差并保持可解释性，在实际应用中显示出改进的推荐性能。

Abstract: Estimating consumer preferences is central to many problems in economics and marketing. This paper develops a flexible framework for learning individual preferences from partial ranking information by interpreting observed rankings as collections of pairwise comparisons with logistic choice probabilities. We model latent utility as the sum of interpretable product attributes, item fixed effects, and a low-rank user-item factor structure, enabling both interpretability and information sharing across consumers and items. We further correct for selection in which comparisons are observed: a comparison is recorded only if both items enter the consumer's consideration set, inducing exposure bias toward frequently encountered items. We model pair observability as the product of item-level observability propensities and estimate these propensities with a logistic model for the marginal probability that an item is observable. Preference parameters are then estimated by maximizing an inverse-probability-weighted (IPW), ridge-regularized log-likelihood that reweights observed comparisons toward a target comparison population. To scale computation, we propose a stochastic gradient descent (SGD) algorithm based on inverse-probability resampling, which draws comparisons in proportion to their IPW weights. In an application to transaction data from an online wine retailer, the method improves out-of-sample recommendation performance relative to a popularity-based benchmark, with particularly strong gains in predicting purchases of previously unconsumed products.

</details>


### [114] [Functional Decomposition and Shapley Interactions for Interpreting Survival Models](https://arxiv.org/abs/2602.16505)
*Sophie Hanna Langbein,Hubert Baniecki,Fabian Fumagalli,Niklas Koenen,Marvin N. Wright,Julia Herbinger*

Main category: stat.ML

TL;DR: SurvFD和SurvSHAP-IQ为生存分析模型提供交互作用和时间感知的解释方法，解决传统加性解释在生存函数中的局限性。


<details>
  <summary>Details</summary>
Motivation: 生存函数和风险函数是时间到事件预测中自然且可解释的目标，但其固有的非加性特性限制了标准加性解释方法的应用，需要新的解释框架来处理特征交互作用。

Method: 提出生存函数分解(SurvFD)方法，将高阶效应分解为时间依赖和时间独立分量；基于此提出SurvSHAP-IQ，将Shapley交互作用扩展到时间索引函数，提供高阶时间依赖交互作用的实用估计器。

Result: 建立了一个交互作用和时间感知的生存建模可解释性方法，能够明确表征加性解释何时以及为何失败，为时间到事件预测任务提供广泛适用的解释框架。

Conclusion: SurvFD和SurvSHAP-IQ共同为生存分析模型提供了理论分解和实用估计器，解决了传统加性解释的局限性，为时间到事件预测任务提供了新的可解释性视角。

Abstract: Hazard and survival functions are natural, interpretable targets in time-to-event prediction, but their inherent non-additivity fundamentally limits standard additive explanation methods. We introduce Survival Functional Decomposition (SurvFD), a principled approach for analyzing feature interactions in machine learning survival models. By decomposing higher-order effects into time-dependent and time-independent components, SurvFD offers a previously unrecognized perspective on survival explanations, explicitly characterizing when and why additive explanations fail. Building on this theoretical decomposition, we propose SurvSHAP-IQ, which extends Shapley interactions to time-indexed functions, providing a practical estimator for higher-order, time-dependent interactions. Together, SurvFD and SurvSHAP-IQ establish an interaction- and time-aware interpretability approach for survival modeling, with broad applicability across time-to-event prediction tasks.

</details>


### [115] [Error Propagation and Model Collapse in Diffusion Models: A Theoretical Study](https://arxiv.org/abs/2602.16601)
*Nail B. Khelifa,Richard E. Turner,Ramji Venkataramanan*

Main category: stat.ML

TL;DR: 该论文理论分析了在扩散模型上递归训练合成数据导致的性能退化现象，量化了生成分布与目标分布之间的累积偏差，并识别了不同的漂移机制。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型越来越多地在合成数据上进行训练或微调，但递归训练会导致性能显著下降，表现为逐渐偏离目标分布。需要从理论上分析这一现象，特别是在基于分数的扩散模型场景中。

Method: 在每轮训练使用合成数据和目标分布新鲜样本混合的现实场景下，通过理论分析获得生成分布与目标分布之间累积偏差的上界和下界。使用合成数据和图像进行实证验证。

Result: 理论分析揭示了不同的漂移机制，取决于分数估计误差和每轮生成中使用的新鲜数据比例。实证结果在合成数据和图像上验证了理论发现。

Conclusion: 递归训练合成数据会导致分布漂移，但通过控制分数估计误差和新鲜数据比例可以管理这一现象。研究为理解合成数据递归训练的局限性提供了理论框架。

Abstract: Machine learning models are increasingly trained or fine-tuned on synthetic data. Recursively training on such data has been observed to significantly degrade performance in a wide range of tasks, often characterized by a progressive drift away from the target distribution. In this work, we theoretically analyze this phenomenon in the setting of score-based diffusion models. For a realistic pipeline where each training round uses a combination of synthetic data and fresh samples from the target distribution, we obtain upper and lower bounds on the accumulated divergence between the generated and target distributions. This allows us to characterize different regimes of drift, depending on the score estimation error and the proportion of fresh data used in each generation. We also provide empirical results on synthetic data and images to illustrate the theory.

</details>


### [116] [Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models](https://arxiv.org/abs/2602.16634)
*Yu Xie,Ludwig Winkler,Lixin Sun,Sarah Lewis,Adam E. Foster,José Jiménez Luna,Tim Hempel,Michael Gastegger,Yaoyi Chen,Iryna Zaporozhets,Cecilia Clementi,Christopher M. Bishop,Frank Noé*

Main category: stat.ML

TL;DR: 本文提出增强扩散采样方法，通过精确引导协议生成偏置系综，再通过精确重加权恢复平衡统计，解决了扩散模型在稀有事件采样中的剩余问题。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型（如BioEmu）已成为强大的平衡采样器，能够从复杂分子分布中生成独立样本，但在计算依赖于平衡状态下稀有状态的观测值（如折叠自由能）时，采样问题仍然存在。

Method: 提出增强扩散采样框架，通过精确引导协议生成偏置系综，然后使用精确重加权恢复平衡统计。具体实现了三种算法：UmbrellaDiff（使用扩散模型的伞形采样）、ΔG-Diff（通过倾斜系综计算自由能差）和MetaDiff（元动力学的批处理版本）。

Result: 在玩具系统、蛋白质折叠景观和折叠自由能计算中，这些方法能够在GPU分钟到小时级别实现快速、准确、可扩展的平衡性质估计，填补了扩散模型平衡采样器出现后仍然存在的稀有事件采样空白。

Conclusion: 增强扩散采样方法有效解决了分子动力学中剩余的稀有事件采样问题，使扩散模型能够高效探索稀有事件区域，同时保持无偏的热力学估计器。

Abstract: The rare-event sampling problem has long been the central limiting factor in molecular dynamics (MD), especially in biomolecular simulation. Recently, diffusion models such as BioEmu have emerged as powerful equilibrium samplers that generate independent samples from complex molecular distributions, eliminating the cost of sampling rare transition events. However, a sampling problem remains when computing observables that rely on states which are rare in equilibrium, for example folding free energies. Here, we introduce enhanced diffusion sampling, enabling efficient exploration of rare-event regions while preserving unbiased thermodynamic estimators. The key idea is to perform quantitatively accurate steering protocols to generate biased ensembles and subsequently recover equilibrium statistics via exact reweighting. We instantiate our framework in three algorithms: UmbrellaDiff (umbrella sampling with diffusion models), $Δ$G-Diff (free-energy differences via tilted ensembles), and MetaDiff (a batchwise analogue for metadynamics). Across toy systems, protein folding landscapes and folding free energies, our methods achieve fast, accurate, and scalable estimation of equilibrium properties within GPU-minutes to hours per system -- closing the rare-event sampling gap that remained after the advent of diffusion-model equilibrium samplers.

</details>
