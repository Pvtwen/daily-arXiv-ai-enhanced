<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 16]
- [cs.LG](#cs.LG) [Total: 140]
- [stat.ML](#stat.ML) [Total: 13]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [AFDM for LEO Inter-Satellite Links: Path-Level CSI Prediction and CRLB-Guided Pre-Equalization](https://arxiv.org/abs/2601.00819)
*Houtianfu Wang,Ozgur Akan*

Main category: eess.SP

TL;DR: 提出用于低轨卫星间链路的AFDM双阶段ISAC框架，包含路径级信道预测和感知增强预均衡器设计


<details>
  <summary>Details</summary>
Motivation: 低轨卫星间链路面临强双选择性信道和过时信道状态信息问题，现有AFDM设计假设理想CSI且仅优化通信性能，缺乏在预测CSI下的感知-通信联合优化方案

Method: 提出两阶段框架：第一阶段对少数主导镜面路径参数进行序列预测并重构AFDM DD域核；第二阶段设计感知增强预均衡器，在经典MMSE基础上加入基于CRLB型灵敏度度量的正则项

Result: 路径级预测器相比AFDM无关基线改善了有效核重构；在预测CSI下，感知增强预均衡器显著提升感知指标，同时保持符号错误率接近通信导向的MMSE设计

Conclusion: 所提框架在低轨卫星间链路中实现了通信与感知的有效权衡，路径级预测和感知增强预均衡器设计具有实用价值

Abstract: Low-Earth-orbit (LEO) inter-satellite links must cope with strongly doubly selective channels and aged channel state information (CSI). In this paper, the term ``sensing'' refers to the receiver-side identifiability of a small set of dominant delay--Doppler path parameters, quantified via CRLB-type proxies, rather than a full-fledged target-sensing pipeline. Affine frequency division multiplexing (AFDM) provides a sparse delay--Doppler (DD) representation well suited to such channels, yet most existing AFDM designs assume ideal CSI, operate on grid-based channel coefficients, and optimize only communication performance. This paper proposes a two-stage AFDM-based ISAC framework for mobile LEO ISLs that explicitly operates under predicted CSI. In Stage~I, we model the channel by a small number of dominant specular paths and perform sequence prediction directly on their complex gains, delays, and Dopplers, from which we reconstruct the AFDM DD-domain kernel used as the sole instantaneous CSI at the transmitter. In Stage~II, we design a sensing-aware AFDM pre-equalizer by augmenting the classical minimum mean-square error (MMSE) solution with a term obtained from Cramér--Rao-type sensitivity measures evaluated under the predicted channel model, leading to a first-order surrogate of a CRLB-regularized pre-equalizer with a single tuning parameter that controls the communication--sensing tradeoff. Simulation results for representative LEO ISL trajectories show that the proposed path-level predictor improves effective-kernel reconstruction over AFDM-unaware baselines, and that, under predicted CSI, the sensing-aware pre-equalizer significantly improves sensing-oriented metrics over outdated-CSI baselines while keeping symbol error rates close to a communication-oriented MMSE design with only modest additional complexity.

</details>


### [2] [Environment-to-Link ISAC with Space-Weather Sensing for Ka-Band LEO Downlinks](https://arxiv.org/abs/2601.00820)
*Houtianfu Wang,Haofan Dong,Hanlin Cai,Ozgur B. Akan*

Main category: eess.SP

TL;DR: 提出一种基于双载波相位观测的预测控制器，用于Ka波段LEO下行链路在电离层扰动期间的可靠通信，通过预测VTEC变化提前触发调制编码方案切换，提升链路性能。


<details>
  <summary>Details</summary>
Motivation: Ka波段LEO下行链路在电离层扰动期间会出现秒级可靠性崩溃，传统的固定衰减裕度和反应式自适应编码调制要么过于保守，要么响应太慢，需要更有效的预测性控制方法。

Method: 采用GNSS-free、链路内部的预测控制器，通过10Hz的双载波相位观测值感知下行链路，使用高通滤波和模板检测器识别扰动起始，通过四状态近恒定速度卡尔曼滤波器估计ΔVTEC及其变化率，基于60秒前瞻预测计算中断概率，触发离散调制编码方案切换和导频时间更新。

Result: 在60秒压力窗口内，控制器相比无自适应基线将峰值BLER降低25-30%，吞吐量提升0.10-0.15 bps/Hz，每个0.1秒周期的计算复杂度为O(1)，实测约0.042毫秒，适合星上实现。

Conclusion: 该预测控制器能有效应对电离层扰动，提升Ka波段LEO下行链路的可靠性和吞吐量，计算效率高适合星载实现，并讨论了色散主导事件的应用范围和部署考虑。

Abstract: Ka-band low-Earth-orbit (LEO) downlinks can suffer second-scale reliability collapses during flare-driven ionospheric disturbances, where fixed fade margins and reactive adaptive coding and modulation (ACM) are either overly conservative or too slow. This paper presents a GNSS-free, link-internal predictive controller that senses the same downlink via a geometry-free dual-carrier phase observable at 10~Hz: a high-pass filter and template-based onset detector, followed by a four-state nearly-constant-velocity Kalman filter, estimate $Δ$VTEC and its rate, and a short look-ahead (60~s) yields an endpoint outage probability used as a risk gate to trigger one-step discrete MCS down-switch and pilot-time update with hysteresis. Evaluation uses physics-informed log replay driven by real GOES X-ray flare morphologies under a disjoint-day frozen-calibration protocol, with uncertainty reported via paired moving-block bootstrap. Across stressed 60~s windows, the controller reduces peak BLER by 25--30\% and increases goodput by 0.10--0.15~bps/Hz versus no-adaptation baselines under a unified link-level abstraction. The loop runs in $\mathcal{O}(1)$ per 0.1~s epoch (about 0.042~ms measured), making on-board implementation feasible, and scope and deployment considerations for dispersion-dominated events are discussed.

</details>


### [3] [Dynamic Accuracy Estimation in a Wi-Fi-based Positioning System](https://arxiv.org/abs/2601.00999)
*Marcin Kolakowski,Vitomir Djaja-Josko*

Main category: eess.SP

TL;DR: 提出了一种动态精度估计方法，通过定位算法使用的测量结果来推导定位误差，在Wi-Fi室内定位系统中验证了多种回归方法，随机森林回归表现最佳，平均绝对误差为0.72米。


<details>
  <summary>Details</summary>
Motivation: 室内定位系统中需要准确估计定位误差，传统方法往往无法动态评估精度，因此需要开发一种基于实际测量结果的动态精度估计方法。

Method: 提出动态精度估计概念，利用定位算法使用的测量结果来推导定位误差。在Wi-Fi室内定位系统中实验验证，测试了线性回归、随机森林、k近邻和神经网络等多种回归方法。

Result: 随机森林回归方法表现最佳，定位误差估计的平均绝对误差为0.72米，优于其他测试的回归方法。

Conclusion: 动态精度估计方法可行且有效，随机森林回归在Wi-Fi室内定位系统中能够准确估计定位误差，为定位系统提供了可靠的精度评估手段。

Abstract: The paper presents a concept of a dynamic accuracy estimation method, in which the localization errors are derived based on the measurement results used by the positioning algorithm. The concept was verified experimentally in a Wi\nobreakdash-Fi based indoor positioning system, where several regression methods were tested (linear regression, random forest, k-nearest neighbors, and neural networks). The highest positioning error estimation accuracy was achieved for random forest regression, with a mean absolute error of 0.72 m.

</details>


### [4] [System-Level Comparison of Multimodal and In-Band mmWave Sensing for Beam Prediction in 6G ISAC](https://arxiv.org/abs/2601.01033)
*Abidemi Orimogunje,Hyunwoo Park,Igbafe Orikumhi,Sunwoo Kim,Dejan Vukobratovic*

Main category: eess.SP

TL;DR: 本文提出了一个系统级框架，用于评估多模态传感器融合在6G ISAC辅助的V2I波束预测中的性能，使用轻量级神经网络预测64波束索引，并建立了校准基准。


<details>
  <summary>Details</summary>
Motivation: 毫米波V2I链路中的波束训练开销较大，ISAC可以通过带内感知减少这种开销，而外部传感器（摄像头、LiDAR、雷达等）可以进一步提高预测精度。需要建立一个系统级框架来评估各种传感器及其融合在波束预测中的性能。

Method: 开发了一个系统级框架，使用DeepSense-6G Scenario-33数据集评估摄像头、LiDAR、雷达、GPS和带内毫米波功率传感器的性能。采用轻量级CNN和MLP编码器组成的延迟感知神经网络来预测64波束索引。

Result: 毫米波功率向量本身就是一个强大的独立预测器，与外部传感器融合后保持高性能：毫米波单独以及毫米波+LiDAR/GPS/雷达达到98%的Top-5准确率，毫米波+摄像头达到94%的Top-5准确率。建立了6G ISAC辅助V2I波束预测的校准基准。

Conclusion: 提出的框架为6G ISAC辅助的V2I波束预测建立了校准基准，证明了毫米波功率向量作为强预测器的有效性，以及多模态传感器融合在保持高性能方面的潜力。

Abstract: Integrated sensing and communication (ISAC) can reduce beam-training overhead in mmWave vehicle-to-infrastructure (V2I) links by enabling in-band sensing-based beam prediction, while exteroceptive sensors can further enhance the prediction accuracy. This work develop a system-level framework that evaluates camera, LiDAR, radar, GPS, and in-band mmWave power, both individually and in multimodal fusion using the DeepSense-6G Scenario-33 dataset. A latency-aware neural network composed of lightweight convolutional (CNN) and multilayer-perceptron (MLP) encoders predict a 64-beam index. We assess performance using Top-k accuracy alongside spectral-efficiency (SE) gap, signal-to-noise-ratio (SNR) gap, rate loss, and end-to-end latency. Results show that the mmWave power vector is a strong standalone predictor, and fusing exteroceptive sensors with it preserves high performance: mmWave alone and mmWave+LiDAR/GPS/Radar achieve 98% Top-5 accuracy, while mmWave+camera achieves 94% Top-5 accuracy. The proposed framework establishes calibrated baselines for 6G ISAC-assisted beam prediction in V2I systems.

</details>


### [5] [Towards a Theoretical Framework for Robust Node Deployment in Cooperative ISAC Networks](https://arxiv.org/abs/2601.01152)
*Haojin Li,Kaiqian Qu,Chen Sun,Anbang Zhang,Xiaoxue Wang,Wenqi Zhang,Haijun Zhang*

Main category: eess.SP

TL;DR: 该论文研究了ISAC网络中用于鲁棒多节点协作定位的节点部署策略，提出基于距离加权相关度量的部署优化框架，并使用遗传算法求解，显著提升了最坏情况下的定位性能。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信网络中，多节点协作定位的性能受到节点部署策略的显著影响。传统方法未充分考虑不同位置间导向矢量相关性对定位性能的影响，特别是在最坏情况下的网络鲁棒性不足。

Method: 首先分析导向矢量相关性对定位性能的影响，提出距离加权相关性度量；然后建立最小化最大加权导向矢量相关性的部署优化框架，同时优化节点位置和阵列方向；最后开发遗传算法求解该min-max优化问题。

Result: 通过MUSIC和神经网络两种定位方法的广泛仿真验证，所提方法能显著提升鲁棒定位性能，优化后的节点部署策略在最坏情况下表现出更好的定位精度和网络鲁棒性。

Conclusion: 论文提出的基于距离加权相关性度量的节点部署优化框架有效提升了ISAC网络中多节点协作定位的鲁棒性，为实际网络部署提供了理论指导和技术方案。

Abstract: This paper investigates node deployment strategies for robust multi-node cooperative localization in integrated sensing and communication (ISAC) networks.We first analyze how steering vector correlation across different positions affects localization performance and introduce a novel distance-weighted correlation metric to characterize this effect. Building upon this insight, we propose a deployment optimization framework that minimizes the maximum weighted steering vector correlation by optimizing simultaneously node positions and array orientations, thereby enhancing worst-case network robustness. Then, a genetic algorithm (GA) is developed to solve this min-max optimization, yielding optimized node positions and array orientations. Extensive simulations using both multiple signal classification (MUSIC) and neural-network (NN)-based localization validate the effectiveness of the proposed methods, demonstrating significant improvements in robust localization performance.

</details>


### [6] [NeuroSSM: Multiscale Differential State-Space Modeling for Context-Aware fMRI Analysis](https://arxiv.org/abs/2601.01229)
*Furkan Genç,Boran İsmet Macun,Sait Sarper Özaslan,Emine U. Saritas,Tolga Çukur*

Main category: eess.SP

TL;DR: NeuroSSM：一种用于原始BOLD信号fMRI分析的多元状态空间架构，通过多尺度状态空间主干和并行差分分支，同时捕捉快速瞬态动态和缓慢全局趋势。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以在长fMRI时间序列中联合捕捉多尺度时间结构（从快速瞬态动态到缓慢大规模波动）。Transformer计算成本高，而现有状态空间模型通常基于功能连接表示且采用单尺度处理，限制了同时表示快慢动态的能力。

Method: 提出NeuroSSM架构：1）多尺度状态空间主干，通过选择性状态空间模型同时捕捉快速和缓慢动态；2）并行差分分支，增强对瞬态状态变化的敏感性。该架构直接处理原始BOLD信号进行端到端分析。

Result: 在临床和非临床数据集上的实验表明，NeuroSSM在性能和效率方面均优于最先进的fMRI分析方法。

Conclusion: NeuroSSM通过多尺度状态空间建模和差分增强，有效解决了fMRI分析中多尺度时间结构捕捉的挑战，为原始BOLD信号分析提供了高效且性能优越的解决方案。

Abstract: Accurate fMRI analysis requires sensitivity to temporal structure across multiple scales, as BOLD signals encode cognitive processes that emerge from fast transient dynamics to slower, large-scale fluctuations. Existing deep learning (DL) approaches to temporal modeling face challenges in jointly capturing these dynamics over long fMRI time series. Among current DL models, transformers address long-range dependencies by explicitly modeling pairwise interactions through attention, but the associated quadratic computational cost limits effective integration of temporal dependencies across long fMRI sequences. Selective state-space models (SSMs) instead model long-range temporal dependencies implicitly through latent state evolution in a dynamical system, enabling efficient propagation of dependencies over time. However, recent SSM-based approaches for fMRI commonly operate on derived functional connectivity representations and employ single-scale temporal processing. These design choices constrain the ability to jointly represent fast transient dynamics and slower global trends within a single model. We propose NeuroSSM, a selective state-space architecture designed for end-to-end analysis of raw BOLD signals in fMRI time series. NeuroSSM addresses the above limitations through two complementary design components: a multiscale state-space backbone that captures fast and slow dynamics concurrently, and a parallel differencing branch that increases sensitivity to transient state changes. Experiments on clinical and non-clinical datasets demonstrate that NeuroSSM achieves competitive performance and efficiency against state-of-the-art fMRI analysis methods.

</details>


### [7] [A 2.5 $μ$W 30 nV/$\surd$Hz Instrumentation Amplifier for Bioimpedance Sensors with Source Degenerated Current Mirror and DTMOS Transistor](https://arxiv.org/abs/2601.01232)
*Yu Xue,Kwantae Kim*

Main category: eess.SP

TL;DR: 提出一种用于生物阻抗传感的低功耗低噪声仪表放大器，采用增益提升的FVF跨导级，结合源极退化电流镜和动态阈值MOSFET技术降低噪声，在28nm CMOS工艺下实现30nV/√Hz噪声和2.5μW功耗。


<details>
  <summary>Details</summary>
Motivation: 生物阻抗传感应用需要低功耗、低噪声的仪表放大器，传统设计在噪声性能和电压裕度之间存在权衡，需要优化设计以在有限电源电压下实现更好的噪声性能。

Method: 基于增益提升的翻转电压跟随器(FVF)跨导级，采用两种互补技术：1) 源极退化电流镜(SDCM)优化噪声与电压裕度平衡；2) 动态阈值MOSFET(DTMOS)增强有效跨导。在28nm CMOS工艺中仿真验证。

Result: 输入参考噪声30nV/√Hz，带宽1.44MHz，功耗仅2.5μW（0.8V电源），相比基准设计功耗降低32.4%且不牺牲噪声性能。SDCM降低噪声7.95%，DTMOS进一步降低11.66%。

Conclusion: 提出的低功耗低噪声仪表放大器适合生物阻抗传感应用，通过创新电路技术有效平衡噪声与功耗，设计参数开源以确保可重复性和促进未来发展。

Abstract: This paper proposes a low-power and low-noise instrumentation amplifier (IA) tailored for bioimpedance sensing applications. The design originates from a gain-boosted flipped voltage follower (FVF) transconductance (TC) stage and integrates two complementary circuit techniques to improve the noise performance. To achieve an optimal balance between input-referred noise and available voltage headroom, a source-degenerated current mirror (SDCM) is adopted, resulting in reducing the input-referred noise by 7.95% compared with a conventional current mirror structure. In addition, a dynamic threshold MOSFET (DTMOS) scheme is employed to enhance the effective transconductance, leading to a further 11.66% reduction in input-referred noise. Simulated in a 28 nm CMOS process demonstrate that the proposed IA achieves an input-referred noise floor of 30 nV/$\surd$Hz and a bandwidth of 1.44 MHz, while consuming only 2.5 $μ$W from a 0.8 V supply. Compared to the baseline design, the proposed approach achieves a 32.4% reduction in power consumption without degrading noise performance. The complete design parameters are open-sourced in this paper, to ensure reproducibility and facilitate future developments.

</details>


### [8] [Pinching Antennas in Blockage-Aware Environments: Modeling, Design, and Optimization](https://arxiv.org/abs/2601.01277)
*Ximing Xie,Fang Fang,Zhiguo Ding,Xianbin Wang*

Main category: eess.SP

TL;DR: 本文研究了在存在障碍物的环境中夹持天线系统的性能优化，提出了确定性模型和两种优化算法，显著提高了系统吞吐量和视距连接性。


<details>
  <summary>Details</summary>
Motivation: 现有夹持天线系统研究大多假设理想无障碍环境，但实际室内部署往往障碍物丰富，视距阻塞会显著降低性能。需要研究在阻塞感知环境中的夹持天线系统优化。

Method: 1. 建立圆柱形障碍物的确定性模型精确描述视距条件；2. 针对单用户服务场景，使用匈牙利算法进行波导-用户分配，采用代理辅助块坐标搜索优化天线位置；3. 针对多用户服务场景，提出WMMSE-DDPG方法联合优化波束赋形和天线位置。

Result: 仿真结果表明，所提算法相比基准方法显著提高了系统吞吐量和视距连接性。此外，夹持天线系统能有效利用障碍物抑制同信道干扰，将潜在阻塞转化为性能增益。

Conclusion: 本文提出的确定性模型和优化算法为障碍物丰富的室内环境中夹持天线系统的部署提供了有效解决方案，展示了利用障碍物改善系统性能的新思路。

Abstract: Pinching-antenna (PA) systems have recently emerged as a promising member of the flexible-antenna family due to their ability to dynamically establish line-of-sight (LoS) links. While most existing studies assume ideal environments without obstacles, practical indoor deployments are often obstacle-rich, where LoS blockage significantly degrades performance. This paper investigates pinching-antenna systems in blockage-aware environments by developing a deterministic model for cylinder-shaped obstacles that precisely characterizes LoS conditions without relying on stochastic approximations. Based on this model, a special case is first studied where each PA serves a single user and can only be deployed at discrete positions along the waveguide. In this case, the waveguide-user assignment is obtained via the Hungarian algorithm, and PA positions are refined using a surrogate-assisted block-coordinate search. Then, a general case is considered where each PA serves all users and can be continuously placed along the waveguide. In this case, beamforming and PA positions are jointly optimized by a weighted minimum mean square error integrated deep deterministic policy gradient (WMMSE-DDPG) approach to address non-smooth LoS transitions. Simulation results demonstrate that the proposed algorithms significantly improve system throughput and LoS connectivity compared with benchmark methods. Moreover, the results reveal that pinching-antenna systems can effectively leverage obstacles to suppress co-channel interference, converting potential blockages into performance gains.

</details>


### [9] [KAN-AE with Non-Linearity Score and Symbolic Regression for Energy-Efficient Channel Coding](https://arxiv.org/abs/2601.01598)
*Anthony Joseph Perre,Parker Huggins,Alphan Sahin*

Main category: eess.SP

TL;DR: KAN-AE结合符号回归实现能效信道编码，通过非线性评分选择低复杂度表达式，相比MLP-AE节能1.38倍


<details>
  <summary>Details</summary>
Motivation: 研究如何通过Kolmogorov-Arnold网络和符号回归技术实现能量高效的信道编码，解决传统深度学习模型在无线电设备中能耗过高的问题

Method: 使用Kolmogorov-Arnold网络自编码器（KAN-AE）结合符号回归（SR），引入非线性评分项选择低复杂度表达式，将神经网络转换为符号表达式以实现低复杂度实现

Result: KAN-AE在保持竞争性BLER性能的同时，通过符号回归显著提升能效，相比多层感知机自编码器（MLP-AE）节能1.38倍

Conclusion: KAN-AE结合符号回归是实现能量高效深度学习信道编码的有前景方案，特别适合对能耗敏感的无线电应用

Abstract: In this paper, we investigate Kolmogorov-Arnold network-based autoencoders (KAN-AEs) with symbolic regression (SR) for energy-efficient channel coding. By using SR, we convert KAN-AEs into symbolic expressions, which enables low-complexity implementation and improved energy efficiency at the radios. To further enhance the efficiency, we introduce a new non-linearity score term in the SR process to help select lower-complexity equations when possible. Through numerical simulations, we demonstrate that KAN-AEs achieve competitive BLER performance while improving energy efficiency when paired with SR. We score the energy efficiency of a KAN-AE implementation using the proposed non-linearity metric and compare it to a multi-layer perceptron-based autoencoder (MLP-AE). Our experiment shows that the KAN-AE paired with SR uses 1.38 times less energy than the MLP-AE, supporting that KAN-AEs are a promising choice for energy-efficient deep learning-based channel coding.

</details>


### [10] [Joint Sparsity and Beamforming Design for RDARS-Aided Systems](https://arxiv.org/abs/2601.01773)
*Chengwang Ji,Haiquan Lu,Qiaoyan Peng,Jintao Wang,Shaodan Ma*

Main category: eess.SP

TL;DR: 本文提出了一种RDARS辅助通信系统，通过优化连接元件的稀疏性来简化模式配置并扩大物理阵列孔径，实现了和速率最大化。


<details>
  <summary>Details</summary>
Motivation: RDARS架构在通信和感知性能增强方面具有潜力，但动态工作模式选择带来的低复杂度元件配置仍然是一个未解决的问题。需要找到既能简化模式配置又能扩大物理阵列孔径的方法。

Method: 提出RDARS辅助通信系统，将连接元件形成均匀稀疏阵列以简化模式配置。通过联合优化主动/被动波束形成矩阵和连接元件阵列的稀疏性来最大化系统和速率。针对单用户和双用户场景推导了最优稀疏设计的闭式解，针对任意用户数提出了基于加权最小均方误差的交替优化算法。

Result: 数值结果表明优化稀疏性的重要性以及低复杂度稀疏优化算法的有效性。针对特殊情况的闭式解和针对一般情况的AO算法都能有效解决非凸优化问题。

Conclusion: 通过优化连接元件的稀疏性，RDARS系统能够在简化模式配置的同时扩大物理阵列孔径，实现通信性能的显著提升。提出的优化方法为解决RDARS配置复杂度问题提供了有效解决方案。

Abstract: Reconfigurable distributed antennas and reflecting surface (RDARS) has emerged as a promising architecture for communication and sensing performance enhancement. In particular, the new selection gain can be achieved by leveraging the dynamic working mode selection between connection and reflection modes, whereas low-complexity element configuration remains an open issue. In this paper, we consider a RDARS-assisted communication system, where the connected elements are formed as a uniform sparse array for simplified mode configuration while achieving enlarged physical array aperture. The sum rate maximization problem is then formulated by jointly optimizing the active and passive beamforming matrices and sparsity of connected element array. For the special cases of a single user equipment (UE) and two UEs, the optimal sparsity designs are derived in closed-form. Then, for an arbitrary number of UEs, a weighted minimum mean-square error-based alternating optimization (AO) algorithm is proposed to tackle the non-convex optimization problem. Numerical results demonstrate the importance of optimizing the sparsity and the effectiveness of low-complexity sparsity optimization.

</details>


### [11] [Rethinking Secure Semantic Communications in the Age of Generative and Agentic AI: Threats and Opportunities](https://arxiv.org/abs/2601.01791)
*Shunpu Tang,Yuanyuan Jia,Zijiu Yang,Qianqian Yang,Ruichen Zhang,Jun Du,Jihong Park,Zhiguo Shi,Khaled B. Letaief*

Main category: eess.SP

TL;DR: 本文系统分析了生成式AI和智能体AI时代下语义通信系统的安全与隐私威胁，提出了威胁模型分类，并探讨了利用AI技术增强隐私保护的机遇。


<details>
  <summary>Details</summary>
Motivation: 语义通信通过传输任务相关信息而非原始比特提高通信效率，是6G网络的关键技术。生成式AI进一步增强了语义编码和解码能力，但这些效率提升也引入了新的安全和隐私漏洞。无线信道的广播特性使窃听者也能使用强大的生成式AI语义解码器从截获信号中恢复私有信息。此外，智能体AI的快速发展使窃听者能够通过整合记忆、外部知识和推理能力进行长期自适应推理，进一步推断用户私有行为和意图。

Method: 本文首先系统性地对语义通信系统中的窃听威胁模型进行分类，然后深入分析生成式AI和智能体AI如何增强窃听威胁，同时探讨利用这些AI技术设计隐私保护语义通信系统的潜在机遇。

Result: 提出了语义通信系统中窃听威胁模型的系统分类，阐明了生成式AI和智能体AI对窃听能力的增强机制，并指出了利用AI技术构建隐私保护语义通信系统的研究方向。

Conclusion: 在生成式AI和智能体AI时代，语义通信系统面临新的安全和隐私挑战，需要重新思考系统设计。通过系统分析威胁模型和AI增强机制，本文为构建更安全的语义通信系统提供了理论框架和研究方向，强调了利用AI技术本身来增强隐私保护的机遇。

Abstract: Semantic communication (SemCom) improves communication efficiency by transmitting task-relevant information instead of raw bits and is expected to be a key technology for 6G networks. Recent advances in generative AI (GenAI) further enhance SemCom by enabling robust semantic encoding and decoding under limited channel conditions. However, these efficiency gains also introduce new security and privacy vulnerabilities. Due to the broadcast nature of wireless channels, eavesdroppers can also use powerful GenAI-based semantic decoders to recover private information from intercepted signals. Moreover, rapid advances in agentic AI enable eavesdroppers to perform long-term and adaptive inference through the integration of memory, external knowledge, and reasoning capabilities. This allows eavesdroppers to further infer user private behavior and intent beyond the transmitted content. Motivated by these emerging challenges, this paper comprehensively rethinks the security and privacy of SemCom systems in the age of generative and agentic AI. We first present a systematic taxonomy of eavesdropping threat models in SemCom systems. Then, we provide insights into how GenAI and agentic AI can enhance eavesdropping threats. Meanwhile, we also highlight potential opportunities for leveraging GenAI and agentic AI to design privacy-preserving SemCom systems.

</details>


### [12] [On the Performance of Lossless Reciprocal MiLAC Architectures in Multi-User Networks](https://arxiv.org/abs/2601.01834)
*Tianyu Fang,Xiaohua Zhou,Yijie Mao*

Main category: eess.SP

TL;DR: 微波线性模拟计算机辅助波束成形在MU-MISO网络中无法达到数字波束成形的性能，但通过联合优化功率分配和散射矩阵，仍能实现良好的和速率性能。


<details>
  <summary>Details</summary>
Motivation: 微波线性模拟计算机作为全数字和混合波束成形的替代方案，在单用户MIMO网络中已证明能达到与数字波束成形相同的容量，但其在多用户场景下的性能尚不清楚。

Method: 基于微波网络理论，首先证明无损互易MiLAC在一般MU-MISO网络中无法达到数字波束成形性能；然后建立和速率最大化问题，开发联合优化功率分配和MiLAC散射矩阵的高效优化框架。

Result: 理论分析得到验证，数值结果表明MiLAC在超大规模MIMO系统中具有良好性能潜力。

Conclusion: MiLAC是多用户MIMO网络中一种有前景的架构，虽然无法达到数字波束成形的理论性能，但通过优化设计仍能实现高效传输。

Abstract: Microwave linear analog computer (MiLAC)-aided beamforming, which processes the transmitted symbols fully in the analog domain, has recently emerged as a promising alternative to fully digital and hybrid beamforming architectures for multiple-input multiple-output (MIMO) systems. While prior studies have shown that lossless and reciprocal MiLAC can achieve the same capacity as digital beamforming in a single-user MIMO network, its performance in multi-user scenarios remains unknown. To answer this question, in this work, we establish a downlink multi-user multiple-input single-output (MU-MISO) network with a MiLAC-aided transmitter, and investigate its sum-rate performance. Based on the microwave network theory, we first prove that lossless and reciprocal MiLAC cannot achieve the same performance as digital beamforming in a general MU-MISO network. Then, we formulate a sum-rate maximization problem and develop an efficient optimization framework to jointly optimize the power allocation and the scattering matrix for MiLAC. Numerical results validate our theoretical analysis and demonstrate that MiLAC is a promising architecture for future extremely large-scale MIMO systems.

</details>


### [13] [Doppler-Resilient LEO Satellite OFDM Transmission with Affine Frequency Domain Pilot](https://arxiv.org/abs/2601.01956)
*Tang Shuntian,Wu Xiaomei,Wang Xinyi,Zhao Le,Yang Guang,Liu Zilong,Liu Fan,Fei Zesong*

Main category: eess.SP

TL;DR: 提出一种在仿射频率域嵌入导频的OFDM传输方案，利用LSTM预测器替代传统插值，显著提升高多普勒场景下的卫星通信性能。


<details>
  <summary>Details</summary>
Motivation: LEO卫星OFDM系统面临严重多普勒频移问题，而现有的AFDM传输方案虽然对多普勒有鲁棒性，但数据检测处理复杂度极高。需要探索新的方法来提升高移动性下的OFDM传输性能。

Method: 提出一种新颖的仿射频率域导频嵌入方案，利用相邻信道的自回归特性，设计基于LSTM的预测器来替代传统OFDM信道估计中的插值操作。

Result: 仿真结果表明，在高多普勒场景下，所提出的传输方案在误码率方面显著优于传统OFDM方案。

Conclusion: 该方案为下一代非地面网络通信系统的设计开辟了新途径，能够有效应对高移动性环境下的通信挑战。

Abstract: Orthogonal frequency division multiplexing (OFDM) based low Earth orbit (LEO) satellite communication system suffers from severe Doppler shifts, while {the Doppler-resilient affine frequency-division multiplexing (AFDM) transmission suffers from significantly high processing complexity in data detection}. In this paper, we explore the channel estimation gain of affine frequency (AF) domain pilot to enhance the OFDM transmission under high mobility. Specifically, we propose a novel AF domain pilot embedding scheme for satellite-ground downlink OFDM systems for capturing the channel characteristics. By exploiting the autoregressive (AR) property of adjacent channels, a long short-term memory (LSTM) based predictor is designed to replace conventional interpolation operation in OFDM channel estimation. Simulation results show that the proposed transmission scheme significantly outperforms conventional OFDM scheme in terms of bit error rate (BER) under high Doppler scenarios, thus paving a new way for the design of next generation non-terrestrial network (NTN) communication systems.

</details>


### [14] [Beam-Brainstorm: A Generative Site-Specific Beamforming Approach](https://arxiv.org/abs/2601.02219)
*Zihao Zhou,Zhaolin Wang,Yuanwei Liu*

Main category: eess.SP

TL;DR: 提出GenSSBF框架，通过联合结构建模生成站点特定波束，显著降低波束扫描开销，在低信噪比环境下实现接近最优的波束赋形增益


<details>
  <summary>Details</summary>
Motivation: 传统站点特定波束赋形（SSBF）面临传播环境理解不准确的挑战，需要从非结构化预测转向联合结构建模

Method: 提出GenSSBF统一框架，包含站点配置文件、无线提示模块和生成器。具体实现BBS方案：通过DFT将信道数据转换到可逆潜在空间构建站点配置文件；使用少量DFT波束测量的RSRP构建无线提示；采用定制条件扩散模型作为生成器

Result: 在精确射线追踪数据集上的仿真表明，BBS能够实现接近最优的波束赋形增益，同时大幅减少波束扫描开销，即使在低信噪比环境下也能保持性能

Conclusion: GenSSBF框架代表了从传统非结构化预测到联合结构建模的范式转变，BBS作为其实例化方案，能够直接生成多样化、高保真的用户特定波束，显著提升波束赋形效率

Abstract: Accurately understanding the propagation environment is a fundamental challenge in site-specific beamforming (SSBF). This paper proposes a novel generative SSBF (GenSSBF) solution, which represents a paradigm shift from conventional unstructured prediction to joint-structure modeling. First, considering the fundamental differences between beam generation and conventional image synthesis, a unified GenSSBF framework is proposed, which includes a site profile, a wireless prompting module, and a generator. Second, a beam-brainstorm (BBS) solution is proposed as an instantiation of this GenSSBF framework. Specifically, the site profile is configured by transforming channel data from spatial domain to a reversible latent space via discrete Fourier transform (DFT). To facilitate practical deployment, the wireless prompt is constructed from the reference signal received power (RSRP) measured using a small number of DFT-beams. Finally, the generator is developed using a customized conditional diffusion model. Rather than relying on a meticulously designed global codebook, BBS directly generates diverse and high-fidelity user-specific beams guided by the wireless prompts. Simulation results on accurate ray-tracing datasets demonstrate that BBS can achieve near-optimal beamforming gain while drastically reducing the beam sweeping overhead, even in low signal-to-noise ratio (SNR) environments.

</details>


### [15] [Backscatter-Assisted High-Speed Rail Communications in Straight Tunnel Environments: Effects of Tag Number and Phase Control](https://arxiv.org/abs/2601.02225)
*Yunping Mu,Gongpu Wang,Ruisi He,Theodoros A. Tsiftsis,Saman Atapattu,Chintha Tellambura*

Main category: eess.SP

TL;DR: 研究隧道环境中多标签反向散射通信的信道增益，分析标签数量和相位调整对系统性能的影响，推导相位可调与随机相位下的链路增益概率表达式。


<details>
  <summary>Details</summary>
Motivation: 反向散射通信能增强隧道环境中的信号强度，但标签数量和相位调整对系统性能的影响仍是一个挑战性问题，需要深入研究。

Method: 通过高斯和伽马近似推导相位可调与随机相位假设下反向散射链路增益超过直接链路增益的概率表达式，并进行仿真验证。

Result: 仿真结果表明：相位可调标签相比随机相位情况能显著提高反向散射链路信道增益；标签数量存在有效部署模式的上限阈值。

Conclusion: 研究结果为隧道环境中反向散射通信系统的高效设计提供了有价值的指导原则。

Abstract: Backscatter communication is a promising technology to enhance the signal strength received by the receiver in straight tunnel environments. The impact of the number of tags and their phase adjustment on system performance remains a challenging issue though. Therefore, in this paper, we investigate the channel gain of backscatter-assisted communication with multiple tags in straight tunnels. In particular, we derive the probabilities that the backscatter link gain is greater than the direct link under adjustable and random phase assumptions by applying the Gaussian and Gamma approximations to derive tractable expressions. The simulation results show that phaseadjustable tags significantly improve the channel gain of the backscatter links compared to the random phase case. Moreover, the number of tags has an upper threshold for an effective tag deployment pattern. These insights provide valuable guidelines for the efficient design of backscatter communication systems in tunnel environments.

</details>


### [16] [Ultra-low-power Monostatic Backscatter Platform with Phase-Aware Channel Estimation and System-Level Validation](https://arxiv.org/abs/2601.02227)
*Hanyeol Ryu,Sangkil Kim*

Main category: eess.SP

TL;DR: 提出一种用于单天线单站系统的信道估计方法，通过补偿残余相位漂移和优化导频分配，实现超低功耗（320 pJ/bit）的多媒体级反向散射物联网链路。


<details>
  <summary>Details</summary>
Motivation: 解决反向散射链路中的残余相位漂移问题，开发硬件-软件协同设计的可扩展部署方案，实现超低功耗的多媒体数据传输。

Method: 采用半被动标签、SDR阅读器和2x1平面八木天线阵列；开发考虑往返传播和时间相关性的反向散射衰落模型；提出资源最优导频分配策略；在接收端使用优化的LS和LMMSE信道估计，结合导频辅助CFO补偿和ZF均衡器抑制ISI。

Result: 原型系统在1米距离实现500 kbps速率，功耗158 μW（SDR基带）和10 μW（RF开关），能量效率达320 pJ/bit；OOK和BPSK调制的EVM分别为2.97%和4.02%；通过BER测量和全彩色图像无线传输验证性能。

Conclusion: 该研究展示了超低功耗、支持多媒体传输的反向散射物联网链路，为可扩展部署提供了实用的硬件-软件协同设计指导。

Abstract: This paper presents a novel channel-estimation (CE) method that mitigates residual phase drifts in backscatter links and a full hardware and signal-processing pipeline for a single-antenna monostatic system. The platform comprises a semi-passive tag, a software-defined radio (SDR) reader, and a 2x1 planar Yagi-Uda array (7 dBi with higher than 30 dB isolation) operating at 2.4 ~ 2.5 GHz. The developed backscatter fading model accounts for round-trip propagation and temporal correlation, and employs an analytically derived resource-optimal pilot allocation strategy. At the receiver, optimized least square (LS) and linear minimum mean square error (LMMSE) CE with pilot-aided carrier frequency offset (CFO) compensation feed a zero-forcing (ZF) equalizer to suppress ISI. The prototype delivers 500 kbps at 1 m with power of 158 uW (SDR baseband) and 10 uW (RF switch), yielding 320 pJ/bit. OOK and BPSK modulations achieve measured EVMs of 2.97 % and 4.02 %, respectively. Performance is validated by BER measurements and successful reconstruction of a full-color image in an over-the-air experiment. The results demonstrate an ultra-low-power, multimedia-capable backscatter IoT link and provide practical hardware-software co-design guidance for scalable deployments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [17] [Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study](https://arxiv.org/abs/2601.01016)
*Ata Akbari Asanjan,Milad Memarzadeh,Bryan Matthews,Nikunj Oza*

Main category: cs.LG

TL;DR: 研究使用随机傅里叶变换(RFT)改进自编码器和变分自编码器的训练与推理，通过频率原理分析发现RFT模型能同时学习高低频特征，而传统DNN只能从低频开始逐步学习高频特征。


<details>
  <summary>Details</summary>
Motivation: 探索傅里叶变换在深度神经网络训练中的作用，特别是如何改进自编码器和变分自编码器在异常检测中的性能，解决传统DNN只能逐步学习高频特征的局限性。

Method: 使用随机傅里叶变换(RFT)增强自编码器和变分自编码器，通过频率原理分析训练行为，并引入可训练的RFT变体。在低维合成数据集和高维航空安全数据集(Dashlink)上进行重建式异常检测实验。

Result: 傅里叶变换模型在重建式异常检测中优于传统模型，但可训练RFT与随机RFT相比的优势尚不明确。RFT模型能同时学习高低频特征，而传统DNN只能从低频开始逐步学习。

Conclusion: 傅里叶变换能显著改进自编码器和变分自编码器的性能，特别是在同时学习高低频特征方面具有优势，但可训练RFT相对于随机RFT的额外价值需要进一步研究。

Abstract: In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequency Principle (F-Principle) analysis and show that models with RFT turn to learn low frequency and high frequency at the same time, whereas conventional DNNs start from low frequency and gradually learn (if successful) high-frequency features. We focus on reconstruction-based anomaly detection using autoencoder and variational autoencoder and investigate the RFT's role. We also introduced a trainable variant of RFT that uses the existing computation graph to train the expansion of RFT instead of it being random. We showcase our findings with two low-dimensional synthetic datasets for data representation, and an aviation safety dataset, called Dashlink, for high-dimensional reconstruction-based anomaly detection. The results indicate the superiority of models with Fourier transformation compared to the conventional counterpart and remain inconclusive regarding the benefits of using trainable Fourier transformation in contrast to the Random variant.

</details>


### [18] [Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco](https://arxiv.org/abs/2601.01065)
*Achraf Hsain,Yahya Zaki,Othman Abaakil,Hibat-allah Bekkar,Yousra Chtouki*

Main category: cs.LG

TL;DR: 该论文提出将基于TinyML的低功耗边缘设备集成到水产养殖系统中，实现实时自动化监测和控制，以解决传统人工监测效率低、响应慢的问题。


<details>
  <summary>Details</summary>
Motivation: 水产养殖业面临水质波动、疾病爆发和饲料管理效率低等挑战，传统人工监测方法耗时且可能导致问题处理延迟，需要更高效的自动化解决方案。

Method: 采用TinyML（微型机器学习）技术结合低功耗边缘设备，设计实时监测系统，通过传感器收集pH值、温度、溶解氧、氨氮等参数数据，实现异常检测和自动报警。

Result: 系统能够提供实时水质监测数据，实现异常预警，收集的数据可用于优化水处理过程、饲料分配和饲料效率分析，降低运营成本。

Conclusion: 研究表明TinyML技术在水产养殖监测中具有可行性，能够促进更可持续和高效的养殖实践发展，同时考虑了传感器选择、算法设计、硬件约束和伦理因素。

Abstract: Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.

</details>


### [19] [Distributed Federated Learning by Alternating Periods of Training](https://arxiv.org/abs/2601.01793)
*Shamik Bhattacharyya,Rachel Kalpana Kalaimani*

Main category: cs.LG

TL;DR: 提出分布式联邦学习框架，用多服务器替代单一中心服务器，通过交替进行本地训练和服务器间全局训练，解决可扩展性和容错性问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习依赖单一中心服务器，在大规模客户端场景下存在可扩展性限制和单点故障风险。需要解决这些关键限制。

Method: 设计分布式联邦学习框架，包含多个具有互连通信能力的服务器，每个服务器关联一组不相交的客户端。提出DFL算法，交替进行客户端数据本地训练和服务器间全局训练。

Result: 在适当参数选择下，DFL算法确保所有服务器收敛到共同模型值，且与理想模型的误差在可接受范围内，有效整合了本地和全局训练模型。

Conclusion: 分布式联邦学习框架通过多服务器架构解决了传统联邦学习的可扩展性和容错性问题，理论分析和数值模拟验证了方法的有效性。

Abstract: Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.

</details>


### [20] [Distribution Matching for Graph Quantification Under Structural Covariate Shift](https://arxiv.org/abs/2601.00864)
*Clemens Damke,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: 将KDEy量化学习方法扩展到图数据，通过结构重要性采样处理训练和测试数据之间的结构偏移问题


<details>
  <summary>Details</summary>
Motivation: 在社交网络等图数据中，预测标签分布（量化学习）时，传统方法基于先验概率偏移假设，但该假设在图结构偏移情况下不成立。需要处理训练和测试数据来自图的不同区域时的结构偏移问题。

Method: 将结构重要性采样的思想扩展到最先进的KDEy量化学习方法中，通过重要性采样来适应图结构偏移。

Result: 提出的方法能够适应结构偏移，并且在性能上优于标准的量化学习方法。

Conclusion: 通过将结构重要性采样与KDEy方法结合，成功解决了图数据量化学习中的结构偏移问题，提高了标签分布预测的准确性。

Abstract: Graphs are commonly used in machine learning to model relationships between instances. Consider the task of predicting the political preferences of users in a social network; to solve this task one should consider, both, the features of each individual user and the relationships between them. However, oftentimes one is not interested in the label of a single instance but rather in the distribution of labels over a set of instances; e.g., when predicting the political preferences of users, the overall prevalence of a given opinion might be of higher interest than the opinion of a specific person. This label prevalence estimation task is commonly referred to as quantification learning (QL). Current QL methods for tabular data are typically based on the so-called prior probability shift (PPS) assumption which states that the label-conditional instance distributions should remain equal across the training and test data. In the graph setting, PPS generally does not hold if the shift between training and test data is structural, i.e., if the training data comes from a different region of the graph than the test data. To address such structural shifts, an importance sampling variant of the popular adjusted count quantification approach has previously been proposed. In this work, we extend the idea of structural importance sampling to the state-of-the-art KDEy quantification approach. We show that our proposed method adapts to structural shifts and outperforms standard quantification approaches.

</details>


### [21] [Hierarchical topological clustering](https://arxiv.org/abs/2601.00892)
*Ana Carpio,Gema Duro*

Main category: cs.LG

TL;DR: 提出一种分层拓扑聚类算法，适用于任意距离选择，能识别任意形状的聚类和异常值


<details>
  <summary>Details</summary>
Motivation: 拓扑方法能够在不假设数据结构的情况下探索数据云，需要一种能处理任意形状聚类和异常值的聚类算法

Method: 分层拓扑聚类算法，可使用任意距离度量，通过生成的层次结构推断异常值和任意形状聚类的持久性

Result: 在图像、医疗和经济数据等选定数据集上展示了算法的潜力，这些数据中异常值起重要作用

Conclusion: 该方法能在其他技术失败的情况下提供有意义的聚类，具有处理复杂数据结构的优势

Abstract: Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.

</details>


### [22] [Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment](https://arxiv.org/abs/2601.00908)
*Chorok Lee*

Main category: cs.LG

TL;DR: 研究发现在分布偏移下，保形预测的覆盖率会下降，特别是在特征严重偏移且重要性集中于单一特征时。通过COVID-19供应链任务的实证分析，提出了基于SHAP特征重要性集中度的决策框架来指导模型重训练。


<details>
  <summary>Details</summary>
Motivation: 保形预测在分布偏移下的性能保证会恶化，但不同任务间的恶化程度差异很大。研究旨在理解这种差异的原因，并开发实用的决策框架来指导模型维护。

Method: 使用COVID-19作为自然实验，分析8个供应链任务在严重特征偏移（Jaccard≈0）下的表现。通过SHAP分析特征重要性分布，计算特征集中度指标。进行季度重训练实验，并额外分析4个具有中等特征稳定性的任务。

Result: 覆盖率下降幅度从0%到86.7%不等，差异达两个数量级。灾难性失败与单一特征依赖性高度相关（ρ=0.714, p=0.047）。灾难性任务的特征重要性高度集中（增加4.5倍），而稳健任务则分散到多个特征（10-20倍）。季度重训练可将灾难性任务的覆盖率从22%提升到41%，但对稳健任务无益。

Conclusion: 提出了基于SHAP特征重要性集中度的决策框架：部署前监控SHAP集中度；如果集中度>40%（易受攻击）则进行季度重训练；如果任务稳健则可跳过重训练。特征稳定性而非集中度决定了中等偏移下的稳健性。

Abstract: Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.

</details>


### [23] [Horizon Reduction as Information Loss in Offline Reinforcement Learning](https://arxiv.org/abs/2601.00831)
*Uday Kumar Nidadala,Venkata Bhumika Guthi*

Main category: cs.LG

TL;DR: 论文证明离线强化学习中常用的视野缩减策略会导致不可恢复的信息损失，即使有无限数据和完美函数逼近，最优策略也可能与次优策略统计不可区分。


<details>
  <summary>Details</summary>
Motivation: 尽管经验证据表明视野缩减能改善离线RL的扩展性，但其理论影响尚未充分研究。本文旨在揭示视野缩减可能导致的基本信息损失问题。

Method: 将视野缩减形式化为从固定长度轨迹片段中学习，通过最小反例马尔可夫决策过程识别三种结构失效模式：前缀不可区分性、截断回报导致的目标误设、离线数据集支持和表示混叠。

Result: 证明在固定长度轨迹片段的学习范式下，最优策略可能与次优策略统计不可区分，即使有无限数据和完美函数逼近。建立了视野缩减安全性的必要条件。

Conclusion: 视野缩减在离线RL中存在固有局限性，无法仅通过算法改进克服，这补充了关于保守目标和分布偏移的算法工作，揭示了离线RL困难的不同维度。

Abstract: Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).

</details>


### [24] [Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs](https://arxiv.org/abs/2601.01069)
*Jing Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文重新审视非平稳参数化赌博机中的加权策略，提出一个精炼的分析框架，简化算法设计并改进遗憾界，应用于线性赌博机、广义线性赌博机、自协调赌博机以及具有函数近似的MDP。


<details>
  <summary>Details</summary>
Motivation: 非平稳参数化赌博机中，加权策略在实际应用中很常见，但之前的理论研究显示其分析复杂、算法计算效率低或统计次优。本文旨在解决这些问题。

Method: 提出一个精炼的分析框架，简化加权策略的推导，产生更简单的基于权重的算法。该框架应用于线性赌博机、广义线性赌博机、自协调赌博机，并扩展到具有函数近似的MDP（线性混合MDP和多项Logit混合MDP）。

Result: 在线性赌博机中，新算法与窗口/重启算法同样高效，且保持相同遗憾界。在广义线性赌博机中，获得改进的遗憾界$\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$，优于之前的$\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$。框架还成功扩展到MDP。

Conclusion: 精炼的分析框架解决了加权策略在非平稳参数化赌博机中的理论缺陷，简化了算法设计，改进了遗憾界，并成功扩展到更复杂的强化学习设置。

Abstract: Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_μ$ and $c_μ$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.

</details>


### [25] [ShrimpXNet: A Transfer Learning Framework for Shrimp Disease Classification with Augmented Regularization, Adversarial Training, and Explainable AI](https://arxiv.org/abs/2601.00832)
*Israk Hasan Jone,D. M. Rafiun Bin Masud,Promit Sarker,Sayed Fuad Al Labib,Nazmul Islam,Farhad Billah*

Main category: cs.LG

TL;DR: 本文提出基于深度学习的虾病自动分类方法，使用6种预训练模型在包含1149张图像的4类疾病数据集上进行评估，ConvNeXt-Tiny取得最佳性能（96.88%准确率）。


<details>
  <summary>Details</summary>
Motivation: 虾是全球重要的水产养殖物种，但疾病爆发严重威胁其可持续生产。传统方法难以实现及时准确的疾病检测，需要自动化分类方法来解决这一挑战。

Method: 1) 使用包含1149张图像的4类疾病数据集；2) 评估6种预训练深度学习模型（ResNet50、EfficientNet、DenseNet201、MobileNet、ConvNeXt-Tiny、Xception）；3) 图像预处理包括背景去除和Keras图像管道标准化；4) 使用FGSM进行对抗训练增强鲁棒性；5) 采用CutMix和MixUp等高级数据增强策略防止过拟合；6) 应用Grad-CAM、Grad-CAM++和XGrad-CAM等后解释方法可视化模型关注区域。

Result: ConvNeXt-Tiny模型表现最佳，在测试集上达到96.88%准确率。经过1000次迭代后，模型99%置信区间为[0.953,0.971]。对抗训练和高级数据增强策略有效提升了模型鲁棒性和泛化能力。

Conclusion: 深度学习模型能够有效实现虾病的自动分类，ConvNeXt-Tiny在多种评估模型中表现最优。该方法为虾养殖业的疾病监测提供了及时准确的解决方案，有助于促进可持续水产养殖发展。

Abstract: Shrimp is one of the most widely consumed aquatic species globally, valued for both its nutritional content and economic importance. Shrimp farming represents a significant source of income in many regions; however, like other forms of aquaculture, it is severely impacted by disease outbreaks. These diseases pose a major challenge to sustainable shrimp production. To address this issue, automated disease classification methods can offer timely and accurate detection. This research proposes a deep learning-based approach for the automated classification of shrimp diseases. A dataset comprising 1,149 images across four disease classes was utilized. Six pretrained deep learning models, ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, and Xception were deployed and evaluated for performance. The images background was removed, followed by standardized preprocessing through the Keras image pipeline. Fast Gradient Sign Method (FGSM) was used for enhancing the model robustness through adversarial training. While advanced augmentation strategies, including CutMix and MixUp, were implemented to mitigate overfitting and improve generalization. To support interpretability, and to visualize regions of model attention, post-hoc explanation methods such as Grad-CAM, Grad-CAM++, and XGrad-CAM were applied. Exploratory results demonstrated that ConvNeXt-Tiny achieved the highest performance, attaining a 96.88% accuracy on the test dataset. After 1000 iterations, the 99% confidence interval for the model is [0.953,0.971].

</details>


### [26] [Wittgenstein's Family Resemblance Clustering Algorithm](https://arxiv.org/abs/2601.01127)
*Golbahar Amanpour,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: 基于维特根斯坦家族相似性概念的聚类算法，无需预设簇数量或形状假设


<details>
  <summary>Details</summary>
Motivation: 将维特根斯坦的家族相似性哲学概念应用于机器学习聚类问题，解决传统聚类方法需要预设簇数量和形状假设的限制

Method: 提出WFR算法及其核变体，计算相邻数据实例间的相似度得分，通过阈值处理后构建相似图，图的连通分量形成最终聚类

Result: 在基准数据集上的仿真表明WFR是一种有效的非线性聚类算法，无需簇数量先验知识或形状假设

Conclusion: 维特根斯坦的家族相似性概念为机器学习聚类提供了新的哲学基础，WFR算法展示了哲学思想与机器学习方法结合的价值

Abstract: This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.

</details>


### [27] [Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds](https://arxiv.org/abs/2601.00834)
*Julian Evan Chrisnanto,Salsabila Rahma Alia,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: 提出IM-PINN框架，直接在连续参数域中求解复杂非欧几里得流形上的非线性反应扩散方程，无需网格生成，嵌入黎曼度量张量解析重构拉普拉斯-贝尔特拉米算子。


<details>
  <summary>Details</summary>
Motivation: 模拟复杂非欧几里得流形上的非线性反应扩散动力学面临两大挑战：高保真网格生成成本高，以及离散时间步进方案中的辛漂移问题。传统自适应细化方法在处理极端高斯曲率波动时无法解析各向异性图灵不稳定性。

Method: 提出内在度量物理信息神经网络(IM-PINN)：1) 将黎曼度量张量嵌入自动微分图，解析重构拉普拉斯-贝尔特拉米算子；2) 使用双流架构和傅里叶特征嵌入缓解谱偏差；3) 在连续参数域中直接求解偏微分方程，解耦解复杂度与几何离散化。

Result: 在具有极端高斯曲率波动(K∈[-2489,3580])的"随机布料"流形上验证，IM-PINN恢复了Gray-Scott模型的"分裂斑点"和"迷宫"机制。相比表面有限元法(SFEM)，IM-PINN实现全局质量守恒误差约0.157(SFEM为0.258)，作为热力学一致的全局求解器消除了半隐式积分固有的质量漂移。

Conclusion: IM-PINN为在演化表面上模拟生物模式形成提供了内存高效、分辨率无关的范式，弥合了微分几何与物理信息机器学习之间的鸿沟，解决了传统方法在复杂几何上的局限性。

Abstract: Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a "Stochastic Cloth" manifold with extreme Gaussian curvature fluctuations ($K \in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the "splitting spot" and "labyrinthine" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\mathcal{E}_{mass} \approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.

</details>


### [28] [Sparse Bayesian Message Passing under Structural Uncertainty](https://arxiv.org/abs/2601.01207)
*Yoonhyuk Choi,Jiho Choi,Chanran Kim,Yumin Lee,Hawon Shin,Yeowon Jeon,Minjeong Kim,Jiwoo Kang*

Main category: cs.LG

TL;DR: 提出一种基于符号图后验分布的半监督学习方法，通过建模符号邻接矩阵的后验分布来处理异质性和边噪声，使用稀疏符号消息传递网络提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实世界图中的半监督学习常面临异质性挑战，现有图神经网络要么依赖固定邻接结构，要么通过正则化处理结构噪声，缺乏对结构不确定性的显式建模。

Method: 建模符号邻接矩阵的后验分布（边可为正、负或缺失），提出稀疏符号消息传递网络，结合符号图结构的后验边缘化和稀疏符号消息聚合。

Result: 在异质性基准测试中，该方法在合成和真实世界结构噪声下均优于强基线模型。

Conclusion: 通过显式建模结构不确定性，提供了一种处理边噪声和异质性的原则性方法，稀疏符号消息传递网络具有贝叶斯解释和鲁棒性优势。

Abstract: Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.

</details>


### [29] [SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes](https://arxiv.org/abs/2601.00841)
*Bharath Nunepalli*

Main category: cs.LG

TL;DR: 该研究将RAG系统的查询控制建模为离散动作选择问题，通过离线数据集训练策略来优化服务级别目标，发现固定基线策略表现良好，学习策略主要在质量优先场景下提供成本节省。


<details>
  <summary>Details</summary>
Motivation: RAG系统需要根据每个查询动态调整检索深度和生成模式以满足成本、拒绝率和幻觉风险等服务级别目标，但现有研究缺乏对这种控制问题的系统性分析。

Method: 将每个查询的控制建模为离散动作选择（检索深度、生成模式或拒绝），使用SQuAD 2.0构建离线数据集，记录准确率、token成本、幻觉/拒绝指标和SLO加权奖励，评估两种策略学习目标：监督分类和奖励加权变体。

Result: 固定基线策略（低k、防护提示）表现竞争力强；学习策略主要在质量优先的SLO下提供额外成本节省，在廉价SLO下当拒绝被重奖时可能出现拒绝崩溃现象。

Conclusion: 本研究提供了RAG管道SLO感知控制的可重复案例研究，强调失败模式和报告规范而非提出新的检索器或语言模型，为实际部署中的控制策略设计提供参考。

Abstract: Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.

</details>


### [30] [Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data](https://arxiv.org/abs/2601.01223)
*Marzieh Amiri Shahbazi,Ali Baheri,Nasibeh Azadeh-Fard*

Main category: cs.LG

TL;DR: 提出混合贝叶斯-保形框架，结合贝叶斯层次随机森林与组感知保形校准，为医疗预测提供分布自由的覆盖保证和风险自适应精度


<details>
  <summary>Details</summary>
Motivation: 临床决策需要不确定性量化，既要分布自由的覆盖保证，又要风险自适应精度，现有方法无法同时满足这两个要求

Method: 集成贝叶斯层次随机森林与组感知保形校准，使用后验不确定性加权保形分数，同时保持严格的覆盖有效性

Result: 在61,538例入院、3,793家美国医院和4个区域评估，达到目标覆盖率（94.3% vs 95%目标），低不确定性病例区间窄21%，高风险预测适当加宽，纯贝叶斯不确定性严重欠覆盖（14.1%）

Conclusion: 该框架支持风险分层临床协议、高效资源规划和高置信度预测，为不确定病例提供增强监督的保守分配，为多样化医疗环境提供不确定性感知决策支持

Abstract: Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.

</details>


### [31] [Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning](https://arxiv.org/abs/2601.01023)
*João Morais,Sadjad Alikhani,Akshay Malhotra,Shahab Hamidi-Rad,Ahmed Alkhateeb*

Main category: cs.LG

TL;DR: 提出任务和模型感知的无线数据集相似性度量框架，用于预测跨数据集可迁移性，在CSI压缩和波束预测任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 无线通信中需要比较不同数据集的相似性，以支持数据集选择/增强、仿真到真实迁移、任务特定合成数据生成等应用，但缺乏有效的度量方法。

Method: 提出任务和模型感知框架，通过数据集距离预测模型可迁移性；使用UMAP嵌入结合Wasserstein和欧氏距离；针对监督任务集成监督UMAP和数据集不平衡惩罚。

Result: 在CSI压缩任务中，UMAP嵌入结合Wasserstein和欧氏距离获得超过0.85的Pearson相关性；在波束预测任务中，标签感知距离优于传统基线，与模型可迁移性相关性更强。

Conclusion: 该框架能够有效度量无线数据集的相似性，支持任务相关的数据集比较，为数据集选择、模型训练和适应新部署提供决策依据。

Abstract: This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.

</details>


### [32] [Value-guided action planning with JEPA world models](https://arxiv.org/abs/2601.00844)
*Matthieu Destrade,Oumayma Bounou,Quentin Le Lidec,Jean Ponce,Yann LeCun*

Main category: cs.LG

TL;DR: 提出一种增强JEPA世界模型规划能力的方法，通过塑造表示空间使负目标条件值函数近似于状态嵌入间的距离，从而显著提升简单控制任务中的规划性能。


<details>
  <summary>Details</summary>
Motivation: 虽然JEPA框架能通过自监督预测目标学习环境动态表示，但其支持有效行动规划的能力有限，需要增强其规划能力。

Method: 通过塑造JEPA表示空间，使特定环境中到达成本的负目标条件值函数近似于状态嵌入间的距离（或准距离），并在训练中强制执行此约束。

Result: 相比标准JEPA模型，该方法在简单控制任务上显著提升了规划性能。

Conclusion: 通过约束表示空间使值函数近似于嵌入距离，能有效增强JEPA世界模型的规划能力，为构建能推理环境动态的深度学习模型提供新途径。

Abstract: Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.

</details>


### [33] [Learning with Monotone Adversarial Corruptions](https://arxiv.org/abs/2601.02193)
*Kasper Green Larsen,Chirag Pabbaraju,Abhishek Shetty*

Main category: cs.LG

TL;DR: 研究机器学习算法对数据可交换性和独立性的依赖程度，通过引入单调对抗性污染模型，发现已知最优分类算法在看似有益的单调污染下会表现不佳，而基于一致收敛的算法则不受影响。


<details>
  <summary>Details</summary>
Motivation: 探索标准机器学习算法在多大程度上依赖于数据的可交换性和独立性。当前算法通常假设数据是独立同分布的，但现实世界中数据可能受到各种形式的污染或操纵，需要理解算法在这种非理想条件下的表现。

Method: 引入单调对抗性污染模型：攻击者观察干净的i.i.d.数据集后，可以插入自己选择的"污染"点，但这些污染点必须符合单调性约束，即按照真实目标函数进行标记。在此模型下分析不同算法的表现。

Result: 发现所有已知的最优二分类学习算法在单调污染设置下，在新独立测试点上的期望误差都会变得次优。相反，基于一致收敛的算法在这种污染下仍能保持原有的性能保证。

Conclusion: 研究揭示了最优学习算法在面对看似有益的单调污染时会失效，暴露了它们对数据可交换性的过度依赖。而基于一致收敛的算法更具鲁棒性，这为理解算法在非理想数据条件下的表现提供了重要见解。

Abstract: We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a "clean" i.i.d. dataset, inserts additional "corrupted" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.

</details>


### [34] [You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference](https://arxiv.org/abs/2601.00847)
*Ryan Shamim*

Main category: cs.LG

TL;DR: MFEE框架将推理重构为控制平面决策问题，选择性执行transformer推理，实现78.1%的执行减少同时保持100%准确率


<details>
  <summary>Details</summary>
Motivation: 现代AI推理系统将transformer执行视为强制性的，混淆了模型能力与执行必要性。需要重新构建推理为控制平面决策问题，确定何时需要执行vs何时可以通过替代路径保持正确性

Method: 引入Meaning-First Execution (MFEE)控制平面架构，作为现有堆栈之上的门控层，不修改模型、权重或参数，通过语义分析选择性调用transformer推理

Result: 在1000个多样化提示下，MFEE实现78.1%执行减少，同时保持100%精确匹配等价性。相比基于模式的路由器最多53.3%避免率且有正确性失败，MFEE通过语义分析达到100%避免率且零失败

Conclusion: 通过定理1证明仅基于有限特征图的路由器无法同时保证零假跳过和正避免率。执行治理应成为ML系统基础设施的基础层，与模型级优化技术正交

Abstract: Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. We reframe inference as a control-plane decision problem: determining when execution is necessary versus when correctness can be preserved through alternative pathways. We introduce Meaning-First Execution (MFEE), a control-plane architecture implementing this framework, selectively invoking transformer inference only when required. MFEE operates as a gating layer above existing stacks without modifying models, weights, or parameters. Across 1,000 diverse prompts under deterministic decoding, MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence for invoked executions. Comparative evaluation reveals pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures through semantic analysis. We prove this limitation via Theorem 1: any router operating solely on finite feature maps cannot simultaneously guarantee zero false skips and positive avoidance on feature-collision pairs. These results establish execution governance as a foundational layer in ML systems infrastructure, orthogonal to model-level optimization techniques.

</details>


### [35] [Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry](https://arxiv.org/abs/2601.01616)
*Md Istiauk Hossain Rifat,Moin Khan,Mohammad Zunaed*

Main category: cs.LG

TL;DR: 本文针对孟加拉国纺织业能耗监测落后问题，提出基于非侵入式负载监测的实时工业监控框架，重点研究相同电机驱动的纺织切割机，开发硬件系统并创建新数据集，评估MATNILM模型在工业环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国纺织业作为高能耗行业，其能耗监测方法落后，导致能源使用效率低下和运营成本高昂。现有监控实践无法满足工业应用需求，需要开发实时、非侵入式的负载监测解决方案。

Method: 开发包含电压电流传感器、Arduino Mega和ESP8266的硬件系统，采集总负载和单个负载数据，通过云平台存储处理。创建包含三台相同感应电机和辅助负载的新数据集（超过18万个样本），评估MATNILM模型在工业条件下的性能。

Result: 总能耗估计相对准确，但单个设备分解在多个相同机器同时运行时面临困难。集成系统通过Blynk应用实现了实用的实时远程监控，展示了NILM在工业应用中的潜力与局限性。

Conclusion: NILM在工业环境中具有应用潜力但面临挑战，特别是处理相同负载时的分解困难。未来改进方向包括更高频率数据采集、更大规模数据集以及更先进的深度学习方法来处理相同负载问题。

Abstract: The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads.

</details>


### [36] [EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference](https://arxiv.org/abs/2601.00850)
*Aayush Kumar*

Main category: cs.LG

TL;DR: EdgeJury是一个轻量级集成框架，使用小型指令调优语言模型（3B-8B）通过四阶段协作流程提升问答的真实性和鲁棒性，在边缘计算环境中显著减少幻觉错误。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘部署场景中，前沿规模模型或检索管道可能不切实际，而幻觉问题严重影响了问答系统的可靠性，需要一种轻量级解决方案来提升真实性和鲁棒性。

Method: EdgeJury采用四阶段框架：1) 并行角色专业化生成；2) 匿名交叉评审，包含结构化批评和排名；3) 主席合成，整合最强内容并解决标记问题；4) 基于模型间一致性的声明级一致性标注。

Result: 在TruthfulQA（MC1）上达到76.2%准确率，相对单个8B基线提升21.4%；在对抗性EdgeCases集上获得48.2%相对增益；人工分析显示事实性幻觉错误减少约55%；在Cloudflare Workers AI上实现8.4秒中位端到端延迟。

Conclusion: 协调的小型模型集成可以在不依赖外部检索或专有大型模型API的情况下，显著提升对误解密集问答基准的真实性，为资源受限的边缘部署提供了可行的解决方案。

Abstract: Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.

</details>


### [37] [FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments](https://arxiv.org/abs/2601.00853)
*Sameer Rahil,Zain Abdullah Ahmad,Talha Asif*

Main category: cs.LG

TL;DR: FedSCAM是一种联邦学习算法，通过基于客户端异质性动态调整SAM扰动半径和聚合权重，解决非IID数据下的收敛和泛化问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中客户端数据的统计异质性（非IID标签分布）对收敛和泛化构成挑战。现有SAM方法对所有客户端使用统一扰动半径，忽略了客户端特定的异质性。

Method: 提出FedSCAM算法：1）计算每个客户端的异质性指标；2）根据异质性分数反向调制SAM扰动半径，防止高方差客户端破坏全局模型；3）引入异质性感知的加权聚合机制，优先考虑与全局优化方向一致的客户端更新。

Result: 在CIFAR-10和Fashion-MNIST数据集上，使用不同程度的狄利克雷标签偏斜进行实验，FedSCAM在收敛速度和最终测试准确率方面与FedSAM、FedLESAM等先进基线方法相比具有竞争力。

Conclusion: FedSCAM通过动态调整扰动半径和异质性感知聚合，有效解决了联邦学习中的统计异质性问题，提升了模型在非IID数据下的性能。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.

</details>


### [38] [Harvesting AlphaEarth: Benchmarking the Geospatial Foundation Model for Agricultural Downstream Tasks](https://arxiv.org/abs/2601.00857)
*Yuchi Ma,Yawen Shen,Anu Swatantran,David B. Lobell*

Main category: cs.LG

TL;DR: 评估AlphaEarth Foundation地理空间基础模型在农业监测任务中的表现，发现其在作物产量预测和耕作制图方面表现良好，但存在空间可迁移性、可解释性和时间敏感性等限制。


<details>
  <summary>Details</summary>
Motivation: 尽管AlphaEarth Foundation等地理空间基础模型在土地覆盖分类任务中表现出色，但缺乏在农业监测关键下游任务中的深入评估，以及与传统遥感模型的全面比较。

Method: 在美国三个农业下游任务中评估AEF嵌入：作物产量预测、耕作制图和覆盖作物制图。使用公共和私人数据源，在不同尺度和位置进行综合评估，并训练传统遥感模型作为对比。

Result: AEF模型在所有任务中表现良好，在产量预测和县级耕作制图方面与专门构建的遥感模型竞争力相当。但存在空间可迁移性有限、可解释性低和时间敏感性不足等局限性。

Conclusion: AEF嵌入在农业应用中需要谨慎使用，特别是在时间敏感性、泛化能力和可解释性重要的场景中。虽然表现良好，但当前版本存在明显限制。

Abstract: Geospatial foundation models (GFMs) have emerged as a promising approach to overcoming the limitations in existing featurization methods. More recently, Google DeepMind has introduced AlphaEarth Foundation (AEF), a GFM pre-trained using multi-source EOs across continuous time. An annual and global embedding dataset is produced using AEF that is ready for analysis and modeling. The internal experiments show that AEF embeddings have outperformed operational models in 15 EO tasks without re-training. However, those experiments are mostly about land cover and land use classification. Applying AEF and other GFMs to agricultural monitoring require an in-depth evaluation in critical agricultural downstream tasks. There is also a lack of comprehensive comparison between the AEF-based models and traditional remote sensing (RS)-based models under different scenarios, which could offer valuable guidance for researchers and practitioners. This study addresses some of these gaps by evaluating AEF embeddings in three agricultural downstream tasks in the U.S., including crop yield prediction, tillage mapping, and cover crop mapping. Datasets are compiled from both public and private sources to comprehensively evaluate AEF embeddings across tasks at different scales and locations, and RS-based models are trained as comparison models. AEF-based models generally exhibit strong performance on all tasks and are competitive with purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data. However, we also find several limitations in current AEF embeddings, such as limited spatial transferability compared to RS-based models, low interpretability, and limited time sensitivity. These limitations recommend caution when applying AEF embeddings in agriculture, where time sensitivity, generalizability, and interpretability is important.

</details>


### [39] [Path Integral Solution for Dissipative Generative Dynamics](https://arxiv.org/abs/2601.00860)
*Xidi Wang*

Main category: cs.LG

TL;DR: 该论文证明纯力学系统通过耗散量子动力学和非局域上下文聚合可以生成智能语言，而守恒定律会导致根本性失败。


<details>
  <summary>Details</summary>
Motivation: 探究纯力学系统是否能生成智能语言，研究耗散与守恒在语言生成中的根本作用。

Method: 使用具有封闭形式路径积分传播子的Koopman算子，分析耗散量子动力学中的非局域上下文聚合机制。

Result: 谱分析揭示了特征值结构分为衰减模式（遗忘）、增长模式（放大）和中性模式（保持），这是定向信息流的基本要素。哈密顿约束会消除这些耗散模式，导致性能下降。

Conclusion: 语言生成本质上是耗散量子场论，力学系统通过耗散和非局域性的结合获得智能，而不是通过守恒。

Abstract: Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.

</details>


### [40] [Universal Battery Degradation Forecasting Driven by Foundation Model Across Diverse Chemistries and Conditions](https://arxiv.org/abs/2601.00862)
*Joey Chan,Huan Wang,Haoyu Pan,Wei Wu,Zirong Wang,Zhen Chen,Ershun Pan,Min Xie,Lifeng Xi*

Main category: cs.LG

TL;DR: 提出统一的电池容量衰减预测框架，利用时间序列基础模型和参数高效微调技术，在涵盖1704个电池的大规模数据集上实现跨化学体系、容量尺度和工况的稳定预测性能。


<details>
  <summary>Details</summary>
Motivation: 电池容量衰减预测对储能系统的安全、可靠和长期效率至关重要，但不同化学体系、外形尺寸和工况的强异质性使得单一模型难以泛化到训练域之外。

Method: 整合20个公开老化数据集构建大规模语料库（1704个电池，396万次充放电循环段），采用时间序列基础模型（TSFM）作为骨干网络，结合参数高效的低秩适应（LoRA）和物理引导的对比表示学习来捕捉共享的衰减模式。

Result: 单一统一模型在已见和刻意保留的未见数据集上均取得竞争性或优于各数据集专用基线的精度，同时在训练中未包含的化学体系、容量尺度和工况上保持稳定性能。

Conclusion: 基于TSFM的架构展示了作为电池管理系统容量衰减预测的可扩展和可迁移解决方案的潜力，能够应对实际应用中的多样性挑战。

Abstract: Accurate forecasting of battery capacity fade is essential for the safety, reliability, and long-term efficiency of energy storage systems. However, the strong heterogeneity across cell chemistries, form factors, and operating conditions makes it difficult to build a single model that generalizes beyond its training domain. This work proposes a unified capacity forecasting framework that maintains robust performance across diverse chemistries and usage scenarios. We curate 20 public aging datasets into a large-scale corpus covering 1,704 cells and 3,961,195 charge-discharge cycle segments, spanning temperatures from $-5\,^{\circ}\mathrm{C}$ to $45\,^{\circ}\mathrm{C}$, multiple C-rates, and application-oriented profiles such as fast charging and partial cycling. On this corpus, we adopt a Time-Series Foundation Model (TSFM) backbone and apply parameter-efficient Low-Rank Adaptation (LoRA) together with physics-guided contrastive representation learning to capture shared degradation patterns. Experiments on both seen and deliberately held-out unseen datasets show that a single unified model achieves competitive or superior accuracy compared with strong per-dataset baselines, while retaining stable performance on chemistries, capacity scales, and operating conditions excluded from training. These results demonstrate the potential of TSFM-based architectures as a scalable and transferable solution for capacity degradation forecasting in real battery management systems.

</details>


### [41] [Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery](https://arxiv.org/abs/2601.00863)
*Markus J. Buehler*

Main category: cs.LG

TL;DR: 论文提出"材料音乐"生成框架，将物质层次结构与音乐创作逻辑连接，通过可逆映射将分子振动、蜘蛛网等物理结构转化为音乐元素，揭示科学与艺术在约束下的创造性共性。


<details>
  <summary>Details</summary>
Motivation: 探索物质结构与音乐创作之间的深层联系，建立跨尺度的统一框架，将振动作为组织结构的共享语法，通过听觉作为科学探测的新模式。

Method: 使用可逆映射方法：1) 分子光谱映射到音调；2) 三维网络映射到可演奏乐器；3) 枚举所有2^12音乐音阶分析文化系统分布；4) 基于群体智能的AI模型作曲。

Result: 发现文化重要的音乐系统聚集在中熵、中缺陷区域，与材料科学中的Hall-Petch最优缺陷密度直接对应；AI作曲展现出类似人类的小世界连接、模块化整合等结构特征。

Conclusion: 科学与艺术都是在约束下的生成性世界构建行为，振动是跨尺度组织结构的共享语法，选择性不完美是平衡连贯性与适应性的机制，听觉可作为科学探索的新模式。

Abstract: We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.

</details>


### [42] [A-PINN: Auxiliary Physics-informed Neural Networks for Structural Vibration Analysis in Continuous Euler-Bernoulli Beam](https://arxiv.org/abs/2601.00866)
*Shivani Saini,Ramesh Kumar Vats,Arup Kumar Sahoo*

Main category: cs.LG

TL;DR: 提出了一种改进的辅助物理信息神经网络（A-PINN）框架，结合平衡自适应优化器，用于结构振动分析，在数值稳定性和预测精度方面相比基线提升至少40%。


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络及其变体在求解微分方程控制的正反问题中表现出色，但需要改进以更准确地表征结构系统、捕捉振动现象并确保可靠的预测分析，这对深入理解科学机器学习模型在振动问题中的鲁棒性至关重要。

Method: 提出改进的辅助物理信息神经网络（A-PINN）框架，结合平衡自适应优化器，用于分析结构振动问题。通过数值模拟近似欧拉-伯努利梁方程在不同场景下的解来评估模型性能。

Result: 数值结果证实了模型在数值稳定性和预测精度方面的增强性能，相比基线模型提升至少40%。

Conclusion: 改进的A-PINN框架在结构振动问题分析中表现出优越性能，为科学机器学习模型在振动问题求解中的鲁棒性提供了深入见解。

Abstract: Recent advancements in physics-informed neural networks (PINNs) and their variants have garnered substantial focus from researchers due to their effectiveness in solving both forward and inverse problems governed by differential equations. In this research, a modified Auxiliary physics-informed neural network (A-PINN) framework with balanced adaptive optimizers is proposed for the analysis of structural vibration problems. In order to accurately represent structural systems, it is critical for capturing vibration phenomena and ensuring reliable predictive analysis. So, our investigations are crucial for gaining deeper insight into the robustness of scientific machine learning models for solving vibration problems. Further, to rigorously evaluate the performance of A-PINN, we conducted different numerical simulations to approximate the Euler-Bernoulli beam equations under the various scenarios. The numerical results substantiate the enhanced performance of our model in terms of both numerical stability and predictive accuracy. Our model shows improvement of at least 40% over the baselines.

</details>


### [43] [SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation](https://arxiv.org/abs/2601.00868)
*Aditya Sreevatsa K,Arun Kumar Raveendran,Jesrael K Mani,Prakash G Shigli,Rajkumar Rangadore,Narayana Darapaneni,Anwesh Reddy Paduri*

Main category: cs.LG

TL;DR: SmartFlow是一个多层框架，结合强化学习和Agentic AI解决城市共享单车动态再平衡问题，通过战略DQN学习策略、战术模块优化调度、通信层生成可执行指令，显著降低网络不平衡并提高运营效率。


<details>
  <summary>Details</summary>
Motivation: 解决城市共享单车系统中的动态再平衡问题，传统方法难以应对复杂的城市交通网络和实时需求变化，需要一种能够减少闲置时间、提高单车可用性并降低运营成本的智能解决方案。

Method: 采用多层框架：战略层使用DQN在纽约Citi Bike网络的高保真模拟中训练，将问题建模为马尔可夫决策过程；战术层确定性模块优化多段行程和调度；通信层基于Agentic AI和LLM将物流计划转化为可执行指令。

Result: 评估显示SmartFlow能减少95%以上的网络不平衡，同时最小化车队行驶距离，实现高卡车利用率，显著降低闲置时间，提高单车可用性，减少运营成本。

Conclusion: SmartFlow通过整合机器智能与人工操作，为复杂城市移动网络提供了可扩展、可解释的AI驱动物流解决方案，为智能城市交通管理提供了蓝图。

Abstract: SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.

</details>


### [44] [Quantum Machine Learning Approaches for Coordinated Stealth Attack Detection in Distributed Generation Systems](https://arxiv.org/abs/2601.00873)
*Osasumwen Cedric Ogiesoba-Eguakun,Suman Rath*

Main category: cs.LG

TL;DR: 量子机器学习用于检测微电网分布式发电单元的协同隐蔽攻击，混合量子经典模型在低维数据集上表现最佳


<details>
  <summary>Details</summary>
Motivation: 协同隐蔽攻击是分布式发电系统的严重网络安全威胁，它们修改控制和测量信号但保持接近正常行为，使得传统入侵检测方法难以发现

Method: 使用三个特征（DG1的无功功率、相对于标称值的频率偏差、端电压幅值）创建平衡二元分类数据集，评估经典机器学习基线、完全量子变分分类器和混合量子经典模型

Result: 混合量子经典模型（量子特征嵌入结合经典RBF支持向量机）在低维数据集上获得最佳整体性能，在准确率和F1分数上比经典SVM基线有适度提升

Conclusion: 完全量子模型由于训练不稳定性和当前NISQ硬件限制表现较差，而混合模型训练更可靠，表明量子特征映射可以增强入侵检测，即使完全量子学习尚不实用

Abstract: Coordinated stealth attacks are a serious cybersecurity threat to distributed generation systems because they modify control and measurement signals while remaining close to normal behavior, making them difficult to detect using standard intrusion detection methods. This study investigates quantum machine learning approaches for detecting coordinated stealth attacks on a distributed generation unit in a microgrid. High-quality simulated measurements were used to create a balanced binary classification dataset using three features: reactive power at DG1, frequency deviation relative to the nominal value, and terminal voltage magnitude. Classical machine learning baselines, fully quantum variational classifiers, and hybrid quantum classical models were evaluated. The results show that a hybrid quantum classical model combining quantum feature embeddings with a classical RBF support vector machine achieves the best overall performance on this low dimensional dataset, with a modest improvement in accuracy and F1 score over a strong classical SVM baseline. Fully quantum models perform worse due to training instability and limitations of current NISQ hardware. In contrast, hybrid models train more reliably and demonstrate that quantum feature mapping can enhance intrusion detection even when fully quantum learning is not yet practical.

</details>


### [45] [LLMize: A Framework for Large Language Model-Based Numerical Optimization](https://arxiv.org/abs/2601.00874)
*M. Rizki Oktavian*

Main category: cs.LG

TL;DR: LLMize是一个开源Python框架，利用大语言模型进行优化，通过自然语言迭代生成和评估候选解，支持多种优化策略，特别适用于复杂领域特定问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型展现出超越传统语言任务的推理能力，这激发了将其用于数值优化的动机。传统优化方法需要数学编程或元启发式设计专业知识，而LLMize旨在提供更易访问的优化方法。

Method: LLMize将优化建模为黑盒过程：通过自然语言生成候选解，由外部目标函数评估，利用解-分数反馈进行迭代优化。支持多种策略，包括优化提示（OPRO）以及受进化算法和模拟退火启发的混合方法。

Result: 在凸优化、线性规划、旅行商问题、神经网络超参数调优和核燃料晶格优化等任务上评估。结果显示，对于简单问题，LLM优化不如经典求解器有竞争力，但对于复杂、领域特定的任务，它提供了实用且易访问的方法。

Conclusion: LLMize为复杂优化问题提供了一个实用框架，特别适用于约束和启发式难以形式化的领域特定任务，通过自然语言描述直接注入领域知识，降低了优化门槛。

Abstract: Large language models (LLMs) have recently shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. This paper presents LLMize, an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning. LLMize formulates optimization as a black-box process in which candidate solutions are generated in natural language, evaluated by an external objective function, and refined over successive iterations using solution-score feedback. The framework supports multiple optimization strategies, including Optimization by Prompting (OPRO) and hybrid LLM-based methods inspired by evolutionary algorithms and simulated annealing. A key advantage of LLMize is the ability to inject constraints, rules, and domain knowledge directly through natural language descriptions, allowing practitioners to define complex optimization problems without requiring expertise in mathematical programming or metaheuristic design. LLMize is evaluated on convex optimization, linear programming, the Traveling Salesman Problem, neural network hyperparameter tuning, and nuclear fuel lattice optimization. Results show that while LLM-based optimization is not competitive with classical solvers for simple problems, it provides a practical and accessible approach for complex, domain-specific tasks where constraints and heuristics are difficult to formalize.

</details>


### [46] [LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification](https://arxiv.org/abs/2601.00877)
*Thomas Andrews,Mark Law,Sara Ahmadi-Abhari,Alessandra Russo*

Main category: cs.LG

TL;DR: LearnAD是一种神经符号方法，用于从脑MRI数据预测阿尔茨海默病，学习完全可解释的规则。它结合统计模型、决策树、随机森林或GNN识别相关脑连接，然后使用FastLAS学习全局规则，在保持完全可解释性的同时达到与SVM相当的准确率。


<details>
  <summary>Details</summary>
Motivation: 在临床神经科学中，深度学习模型（如GNN）虽然性能优异但缺乏可解释性，而传统统计模型可解释性较好但性能有限。需要一种既能保持高性能又能提供完全可解释性的方法来预测阿尔茨海默病，以帮助临床医生理解模型决策依据。

Method: 采用神经符号混合方法：1）使用统计模型、决策树、随机森林或GNN识别相关的脑连接特征；2）应用FastLAS（一种逻辑编程系统）从这些特征中学习全局可解释规则；3）通过符号学习生成人类可理解的决策规则。

Result: 最佳实例性能优于决策树，与支持向量机准确率相当，仅略低于使用所有特征的随机森林和GNN，同时保持完全可解释性。消融研究表明神经符号方法在保持可比性能的同时显著提高了可解释性。

Conclusion: LearnAD展示了符号学习如何加深对GNN在临床神经科学中行为的理解，为开发既高性能又完全可解释的医疗AI模型提供了可行路径，有助于临床决策和疾病机制理解。

Abstract: We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.

</details>


### [47] [Outlier Detection Using Vector Cosine Similarity by Adding a Dimension](https://arxiv.org/abs/2601.00883)
*Zhongyang Shen*

Main category: cs.LG

TL;DR: 提出基于向量余弦相似度的多维异常检测方法，通过添加零值维度构建新数据集，利用观测点与测量点之间的向量相似性识别异常数据


<details>
  <summary>Details</summary>
Motivation: 现有的多维异常检测方法在处理复杂数据时存在局限性，需要更有效的方法来识别多维空间中的异常点

Method: 1. 在原始数据中添加一个零值维度构建新数据集；2. 选择测量点并创建观测点（原点），观测点与测量点仅在新维度上有差异；3. 构建从观测点到测量点及其他数据点的向量；4. 通过比较这些向量的余弦相似度来识别异常数据

Result: 开发了优化实现MDOD，已在PyPI上发布为开源工具包，可用于实际的多维异常检测任务

Conclusion: 该方法提供了一种基于向量相似度的有效多维异常检测方案，通过创新的维度扩展和向量比较机制，能够准确识别异常数据点

Abstract: We propose a new outlier detection method for multi-dimensional data. The method detects outliers based on vector cosine similarity, using a new dataset constructed by adding a dimension with zero values to the original data. When a point in the new dataset is selected as the measured point, an observation point is created as the origin, differing only in the new dimension by having a non-zero value compared to the measured point. Vectors are then formed from the observation point to the measured point and to other points in the dataset. By comparing the cosine similarities of these vectors, abnormal data can be identified. An optimized implementation (MDOD) is available on PyPI: https://pypi.org/project/mdod/.

</details>


### [48] [FANoS: Friction-Adaptive Nosé--Hoover Symplectic Momentum for Stiff Objectives](https://arxiv.org/abs/2601.00889)
*Nalin Dhiman*

Main category: cs.LG

TL;DR: FANoS是一种受物理启发的优化器，结合了二阶动力系统、Nosé-Hoover恒温器和辛积分器，在某些非凸问题上表现良好，但总体上不如现代基线方法。


<details>
  <summary>Details</summary>
Motivation: 将分子动力学中的结构保持积分和恒温器思想应用于优化问题，开发一种物理启发的优化启发式方法。

Method: FANoS结合了：(1) 离散化二阶动力系统的动量更新，(2) 使用动能反馈自适应调整摩擦系数的Nosé-Hoover型恒温器变量，(3) 半隐式辛积分器，可选RMS对角预处理器。

Result: 在Rosenbrock-100D基准测试中，FANoS-RMS达到1.74×10⁻²，优于AdamW(48.50)和SGD+momentum(90.76)，但不如带梯度裁剪的AdamW(1.87×10⁻³)和L-BFGS(≈4.4×10⁻¹⁰)。在病态凸二次问题和PINN预热测试中表现不稳定。

Conclusion: FANoS是对现有思想的解释性综合，在某些刚性非凸谷问题上可能有帮助，但不是现代基线的通用替代品，其行为对温度调度和超参数选择敏感。

Abstract: We study a physics-inspired optimizer, \emph{FANoS} (Friction-Adaptive Nosé--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nosé--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal RMS preconditioner. The method is motivated by structure-preserving integration and thermostat ideas from molecular dynamics, but is used here purely as an optimization heuristic.
  We provide the algorithm and limited theoretical observations in idealized settings. On the deterministic Rosenbrock-100D benchmark with 3000 gradient evaluations, FANoS-RMS attains a mean final objective value of $1.74\times 10^{-2}$, improving substantially over unclipped AdamW ($48.50$) and SGD+momentum ($90.76$) in this protocol. However, AdamW with gradient clipping is stronger, reaching $1.87\times 10^{-3}$, and L-BFGS reaches $\approx 4.4\times 10^{-10}$. On ill-conditioned convex quadratics and in a small PINN warm-start suite (Burgers and Allen--Cahn), the default FANoS configuration underperforms AdamW and can be unstable or high-variance.
  Overall, the evidence supports a conservative conclusion: FANoS is an interpretable synthesis of existing ideas that can help on some stiff nonconvex valleys, but it is not a generally superior replacement for modern baselines, and its behavior is sensitive to temperature-schedule and hyperparameter choices.

</details>


### [49] [When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training](https://arxiv.org/abs/2601.00894)
*Gihyeon Sim*

Main category: cs.LG

TL;DR: PonderTTT：基于自监督重建损失的训练免费门控策略，用于选择性触发测试时训练更新，在代码语言建模中显著优于随机跳过基线


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对所有输入采用统一计算，不考虑难度差异。需要一种方法能根据输入难度选择性应用计算资源，提高效率

Method: 提出PonderTTT门控策略，利用TTT层的自监督重建损失选择性触发测试时训练更新。门控决策完全训练免费，无需学习分类器或辅助网络，仅需单个标量阈值，在无标签数据上初始校准并通过EMA持续调整以维持目标更新率

Result: 在GPT-2模型（124M到1.5B）的代码语言建模实验中，该方法实现了82-89%的Oracle恢复率，完全训练免费，显著优于随机跳过基线（在OOD语言上损失降低达16%）

Conclusion: PonderTTT展示了自监督重建损失作为推理兼容信号的有效性，无需真实标签，为选择性计算提供了训练免费的解决方案

Abstract: Large language models apply uniform computation to all inputs, regardless of difficulty. We propose PonderTTT, a gating strategy using the TTT layer's self-supervised reconstruction loss to selectively trigger Test-Time Training (TTT) updates. The gating decision itself is training-free--requiring no learned classifier or auxiliary networks; only a single scalar threshold is initially calibrated on unlabeled data and continuously adapted via EMA to maintain target update rates. Our experiments with GPT-2 models (124M to 1.5B) on code language modeling (The Stack v2, teacher-forced perplexity) demonstrate that this signal is inference-compatible, requiring no ground-truth labels. Our Reconstruction Gating achieves 82-89% Oracle Recovery while being fully training-free, significantly outperforming Random Skip baselines (up to 16% lower loss on OOD languages).

</details>


### [50] [Dichotomous Diffusion Policy Optimization](https://arxiv.org/abs/2601.00898)
*Ruiming Liang,Yinan Zheng,Kexin Zheng,Tianyi Tan,Jianxiong Li,Liyuan Mao,Zhihao Wang,Guang Chen,Hangjun Ye,Jingjing Liu,Jinqiao Wang,Xianyuan Zhan*

Main category: cs.LG

TL;DR: 提出DIPOLE算法，通过将最优策略分解为奖励最大化和最小化两个对立策略，实现稳定可控的扩散策略优化


<details>
  <summary>Details</summary>
Motivation: 现有扩散策略在强化学习中面临训练不稳定或计算效率低的问题，需要一种既能稳定训练又能灵活控制贪婪度的扩散策略优化方法

Method: 重新审视KL正则化目标，提出贪婪化策略正则化方案，将最优策略分解为奖励最大化和最小化两个对立策略，推理时通过线性组合分数实现贪婪度控制

Result: 在ExORL和OGBench的离线和离线到在线RL设置中验证有效性，并成功训练大型视觉-语言-动作模型用于端到端自动驾驶，在NAVSIM基准上表现优异

Conclusion: DIPOLE算法为扩散策略提供了稳定可控的优化框架，在复杂决策任务和真实世界应用中展现出巨大潜力

Abstract: Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.

</details>


### [51] [Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles](https://arxiv.org/abs/2601.00915)
*Jacquelyn Shelton,Przemyslaw Polewski,Alexander Robel,Matthew Hoffman,Stephen Price*

Main category: cs.LG

TL;DR: 提出LC-CVAE方法，通过强制潜在空间在锚点位置的一致性，从有限的气候模型运行中生成统计一致的时空气候变量新实现


<details>
  <summary>Details</summary>
Motivation: 大型气候模型集合计算成本高昂，但许多下游分析需要更多统计一致的时空气候变量实现。现有方法在有限运行集上训练时，潜在空间碎片化且无法泛化到未见过的集合成员

Method: 提出潜在约束条件变分自编码器(LC-CVAE)，在共享地理"锚点"位置强制跨实现的潜在嵌入同质性，然后使用多输出高斯过程回归在潜在空间预测未采样位置的坐标，最后解码生成完整时间序列场

Result: 实验表明：(1)在单个实现上训练不稳定；(2)纳入约五个实现后收益递减；(3)空间覆盖范围与重建质量之间存在权衡，这与潜在空间中的平均邻居距离密切相关

Conclusion: LC-CVAE方法能够从有限的气候模型运行中生成统计一致的新实现，解决了传统CVAE在跨实现训练时潜在空间碎片化的问题，为气候模型集合扩展提供了有效解决方案

Abstract: Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.

</details>


### [52] [Attention Needs to Focus: A Unified Perspective on Attention Allocation](https://arxiv.org/abs/2601.00919)
*Zichuan Fu,Wentao Song,Guojing Li,Yejing Wang,Xian Wu,Yimin Deng,Hanyu Yan,Yefeng Zheng,Xiangyu Zhao*

Main category: cs.LG

TL;DR: 本文提出Lazy Attention机制，通过解决注意力过载和注意力欠载问题来统一处理Transformer中的表征塌缩和注意力沉没问题，实现更聚焦的注意力分布。


<details>
  <summary>Details</summary>
Motivation: Transformer架构虽然成功，但标准注意力机制存在表征塌缩和注意力沉没两个已知问题。先前研究往往孤立处理这些问题，掩盖了它们之间的深层联系。本文认为这两个问题都源于共同的根源——不恰当的注意力分配。

Method: 提出Lazy Attention机制：1) 针对注意力过载问题，采用跨头和跨维度的位置区分来锐化token区分度；2) 针对注意力欠载问题，引入Elastic-Softmax归一化函数，放松标准softmax约束以抑制对无关token的关注。

Result: 在FineWeb-Edu语料库上的实验表明，Lazy Attention成功缓解了注意力沉没问题，在九个不同基准测试中与标准注意力和现代架构相比具有竞争力，同时达到高达59.58%的注意力稀疏度。

Conclusion: Lazy Attention通过统一视角解决注意力分配问题，为Transformer注意力机制的改进提供了新思路，既能缓解注意力沉没问题，又能保持竞争性能并实现高稀疏度。

Abstract: The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.

</details>


### [53] [MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs](https://arxiv.org/abs/2601.00920)
*Xingsheng Chen,Regina Zhang,Bo Gao,Xingwei He,Xiaofeng Liu,Pietro Lio,Kwok-Yan Lam,Siu-Ming Yiu*

Main category: cs.LG

TL;DR: MODE：一个统一的时间序列预测框架，结合低秩神经ODE和增强Mamba架构，高效处理长程依赖和不规则采样数据


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法在效率、可扩展性和准确性之间难以平衡，特别是在处理长程依赖和不规则采样数据时存在挑战

Method: 1. 线性标记化层处理输入序列；2. 多个Mamba编码器块，每个包含增强Mamba层（因果卷积、SiLU激活、低秩神经ODE增强）；3. 分段选择性扫描机制，基于伪ODE动态自适应关注重要子序列

Result: 在基准数据集上的广泛实验表明，MODE在预测准确性和计算效率方面均超越现有基线方法

Conclusion: MODE提供了一个统一高效的长时序建模架构，通过Mamba选择性扫描与低秩神经ODE的集成，在效率和可扩展性方面取得显著改进

Abstract: Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.

</details>


### [54] [Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease](https://arxiv.org/abs/2601.00921)
*Azadeh Alavi,Hamidreza Khalili,Stanley H. Chan,Fatemeh Kouchmeshki,Ross Vlahos*

Main category: cs.LG

TL;DR: 该研究使用量子核方法和几何感知描述符，在慢性阻塞性肺疾病的小样本临床前数据中，预测肌肉功能指标，相比传统方法获得性能提升。


<details>
  <summary>Details</summary>
Motivation: 慢性阻塞性肺疾病常伴随骨骼肌功能障碍，与全身和气道炎症密切相关。研究旨在通过微创生物标志物纵向预测肌肉结局，解决小样本生物医学预测问题。

Method: 使用213只动物的临床前数据集，比较调优的经典基线方法、几何感知对称正定描述符（使用Stein散度）和量子核模型。在肌肉重量预测中，使用四个可解释输入（血液C反应蛋白、中性粒细胞计数、支气管肺泡灌洗细胞性和条件）的量子核岭回归。

Result: 量子核岭回归在肌肉重量预测中取得测试均方根误差4.41 mg和决定系数0.605，优于相同特征集的岭回归基线（4.70 mg和0.553）。几何感知Stein散度原型距离在仅使用生物标志物时也获得一致增益（4.55 mg vs 4.79 mg）。筛查式评估中，检测低肌肉重量的ROC-AUC最高达0.90。

Conclusion: 几何和量子核提升方法在小数据、低特征的生物医学预测问题中能提供可测量的优势，同时保持可解释性和透明的模型选择。

Abstract: Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.

</details>


### [55] [Complexity-based code embeddings](https://arxiv.org/abs/2601.00924)
*Rares Folea,Radu Iacob,Emil Slusanschi,Traian Rebedea*

Main category: cs.LG

TL;DR: 提出一种将算法源代码转换为数值嵌入的通用方法，通过动态分析程序在不同输入下的行为，为分析指标定制多个通用复杂度函数，基于r-Complexity构建嵌入，并在Codeforces真实代码片段数据集上实现XGBoost分类


<details>
  <summary>Details</summary>
Motivation: 需要一种通用方法将算法源代码转换为数值表示（嵌入），以便进行机器学习分析。现有方法可能无法充分捕捉程序的行为特性和复杂度特征

Method: 1. 动态分析程序在不同输入下的行为；2. 为分析指标定制多个通用复杂度函数；3. 基于r-Complexity构建代码嵌入；4. 使用这些嵌入实现XGBoost算法进行多标签分类

Result: 在包含11个类别的多标签数据集上（基于Codeforces平台真实代码片段构建）实现了平均F1分数，表明该方法有效

Conclusion: 提出的通用代码嵌入方法能够有效将算法源代码转换为数值表示，支持机器学习应用，在真实编程竞赛代码数据集上表现出良好的分类性能

Abstract: This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.

</details>


### [56] [Enhanced Data-Driven Product Development via Gradient Based Optimization and Conformalized Monte Carlo Dropout Uncertainty Estimation](https://arxiv.org/abs/2601.00932)
*Andrea Thomas Nava,Lijo Johny,Fabio Azzalini,Johannes Schneider,Arianna Casanova*

Main category: cs.LG

TL;DR: 提出一个数据驱动的产品开发框架，使用联合神经网络优化多属性产品设计，并引入ConfMC方法提供不确定性估计和覆盖保证。


<details>
  <summary>Details</summary>
Motivation: 传统产品开发中，同时优化多个相关属性具有挑战性，需要捕捉属性间的相互依赖关系，并且现有方法缺乏可靠的不确定性估计和覆盖保证。

Method: 1) 使用联合神经网络学习产品设计规格与多属性之间的关系；2) 应用投影梯度下降寻找最优设计特征；3) 提出ConfMC方法，结合嵌套保形预测和蒙特卡洛dropout提供模型无关的有限样本覆盖保证。

Result: 在五个真实数据集上的实验表明，该方法在性能上达到最先进水平，同时提供自适应、非均匀的预测区间，并且调整覆盖水平时无需重新训练模型。

Conclusion: 该框架为多属性产品优化提供了有效的解决方案，结合了性能优化和可靠的不确定性估计，具有实际应用价值。

Abstract: Data-Driven Product Development (DDPD) leverages data to learn the relationship between product design specifications and resulting properties. To discover improved designs, we train a neural network on past experiments and apply Projected Gradient Descent to identify optimal input features that maximize performance. Since many products require simultaneous optimization of multiple correlated properties, our framework employs joint neural networks to capture interdependencies among targets. Furthermore, we integrate uncertainty estimation via \emph{Conformalised Monte Carlo Dropout} (ConfMC), a novel method combining Nested Conformal Prediction with Monte Carlo dropout to provide model-agnostic, finite-sample coverage guarantees under data exchangeability. Extensive experiments on five real-world datasets show that our method matches state-of-the-art performance while offering adaptive, non-uniform prediction intervals and eliminating the need for retraining when adjusting coverage levels.

</details>


### [57] [LOFA: Online Influence Maximization under Full-Bandit Feedback using Lazy Forward Selection](https://arxiv.org/abs/2601.00933)
*Jinyu Xu,Abhishek K. Umrawal*

Main category: cs.LG

TL;DR: 提出LOFA算法解决在线影响力最大化问题，在完全强盗反馈下利用影响力函数的子模性实现更低经验遗憾


<details>
  <summary>Details</summary>
Motivation: 在线影响力最大化问题中，现有算法虽然利用子模性实现了低遗憾，但仍有改进空间。作者希望进一步利用子模性设计更高效的算法来降低经验遗憾。

Method: 提出懒惰在线前向算法（LOFA），在完全强盗反馈模型下，利用影响力函数的子模性设计更高效的在线学习策略。

Result: 在真实社交网络上的实验表明，LOFA在累积遗憾和瞬时奖励方面均优于现有的强盗算法。

Conclusion: 通过进一步利用影响力函数的子模性，LOFA算法在在线影响力最大化问题上实现了更优的性能表现。

Abstract: We study the problem of influence maximization (IM) in an online setting, where the goal is to select a subset of nodes$\unicode{x2014}$called the seed set$\unicode{x2014}$at each time step over a fixed time horizon, subject to a cardinality budget constraint, to maximize the expected cumulative influence. We operate under a full-bandit feedback model, where only the influence of the chosen seed set at each time step is observed, with no additional structural information about the network or diffusion process. It is well-established that the influence function is submodular, and existing algorithms exploit this property to achieve low regret. In this work, we leverage this property further and propose the Lazy Online Forward Algorithm (LOFA), which achieves a lower empirical regret. We conduct experiments on a real-world social network to demonstrate that LOFA achieves superior performance compared to existing bandit algorithms in terms of cumulative regret and instantaneous reward.

</details>


### [58] [Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures](https://arxiv.org/abs/2601.00942)
*Kabir Grover*

Main category: cs.LG

TL;DR: 稀疏MoE架构在随机解码下的可靠性研究表明，指令微调而非架构稀疏性是决定模型在确定性任务上对解码随机性鲁棒性的主要因素。


<details>
  <summary>Details</summary>
Motivation: 随着稀疏MoE架构在大型语言模型中的普及，需要研究其在随机解码下的可靠性。虽然条件计算提高了计算效率，但稀疏路由与基于温度的采样之间的相互作用是否会损害输出稳定性尚不清楚。

Method: 评估三个代表性模型：OLMoE-7B（稀疏基础模型）、Mixtral-8x7B（稀疏指令微调模型）和Qwen2.5-3B（密集指令微调模型）。在具有客观可验证答案的确定性算术推理任务上进行测试，涵盖四种解码配置（从贪婪解码到T=1.0），评估准确性、格式合规性、重复生成输出一致性和置信度指标，总计9,360次模型生成。

Result: 稀疏指令微调模型在所有解码温度下表现出与密集指令微调模型相当的稳定性，而稀疏基础模型随着温度升高显示系统性性能下降。这表明指令微调而非架构稀疏性是决定模型在确定性任务上对解码随机性鲁棒性的主要因素。

Conclusion: 指令微调是确保稀疏语言模型在可靠性关键应用中输出稳定性的关键因素。在适当指令微调的情况下，稀疏架构可以在不牺牲输出稳定性的情况下安全采用。

Abstract: The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.

</details>


### [59] [Adapting Feature Attenuation to NLP](https://arxiv.org/abs/2601.00965)
*Tianshuo Yang,Ryan Rabinowitz,Terrance E. Boult,Jugal Kalita*

Main category: cs.LG

TL;DR: 本文研究文本开放集识别，将计算机视觉中的特征衰减假设移植到Transformer模型，评估COSTARR框架在BERT和GPT-2上的表现，发现其相比MaxLogit和MSP无显著优势，揭示了将视觉OSR方法移植到语言模型的局限。


<details>
  <summary>Details</summary>
Motivation: Transformer分类器（如BERT）在闭集任务中表现出色，但在面对未见类别输入时表现脆弱，而这是部署NLP系统常见场景。需要研究文本开放集识别方法。

Method: 将计算机视觉中的COSTARR框架移植到两个语言模型（BERT-base和GPT-2），在176个arXiv学科领域分类任务上评估。同时评估MSP、MaxLogit和温度缩放自由能分数，使用OOSA和AUOSCR指标。

Result: COSTARR可扩展到NLP领域且无需重新训练，但与MaxLogit或MSP相比无统计学显著优势；自由能分数在高类别数量设置下落后于其他所有分数。

Conclusion: 研究显示了将视觉中心OSR思想移植到语言模型的潜力和当前局限性，指出需要更大的骨干网络和任务定制的衰减策略。

Abstract: Transformer classifiers such as BERT deliver impressive closed-set accuracy, yet they remain brittle when confronted with inputs from unseen categories--a common scenario for deployed NLP systems. We investigate Open-Set Recognition (OSR) for text by porting the feature attenuation hypothesis from computer vision to transformers and by benchmarking it against state-of-the-art baselines. Concretely, we adapt the COSTARR framework--originally designed for classification in computer vision--to two modest language models (BERT (base) and GPT-2) trained to label 176 arXiv subject areas. Alongside COSTARR, we evaluate Maximum Softmax Probability (MSP), MaxLogit, and the temperature-scaled free-energy score under the OOSA and AUOSCR metrics. Our results show (i) COSTARR extends to NLP without retraining but yields no statistically significant gain over MaxLogit or MSP, and (ii) free-energy lags behind all other scores in this high-class-count setting. The study highlights both the promise and the current limitations of transplanting vision-centric OSR ideas to language models, and points toward the need for larger backbones and task-tailored attenuation strategies.

</details>


### [60] [Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks](https://arxiv.org/abs/2601.00968)
*Longwei Wang,Mohammad Navid Nayyem,Abdullah Al Rakin,KC Santosh,Chaowei Zhang,Yang Zhou*

Main category: cs.LG

TL;DR: 提出一种基于LIME解释的对抗鲁棒性训练框架，通过抑制虚假特征提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 在医疗和自动驾驶等安全关键领域，深度学习模型需要同时具备对抗鲁棒性和决策透明度。研究发现LIME识别出的虚假、不稳定或语义无关特征会显著增加对抗脆弱性

Method: 提出属性引导的精炼框架：1) 特征掩码抑制虚假特征；2) 敏感感知正则化；3) 对抗增强；4) 闭环精炼流程。该框架无需额外数据集或模型架构，可无缝集成到标准对抗训练中

Result: 在CIFAR-10、CIFAR-10-C和CIFAR-100数据集上，对抗鲁棒性和分布外泛化能力均有显著提升

Conclusion: 将LIME从被动诊断工具转变为主动训练信号，建立了可解释性与鲁棒性之间的直接联系，为开发既鲁棒又可解释的深度学习模型提供了新途径

Abstract: The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.

</details>


### [61] [Zero-shot Forecasting by Simulation Alone](https://arxiv.org/abs/2601.00970)
*Boris N. Oreshkin,Mayank Jauhari,Ravi Kiran Selvam,Malcolm Wolff,Wenhao Pan,Shankar Ramasubramanian,Kin G. Olivares,Tatiana Konstantinova,Andres Potapczynski,Mengfei Cao,Dmitry Efimov,Michael W. Mahoney,Andrew G. Wilson*

Main category: cs.LG

TL;DR: 提出SarSim0时间序列模拟器，基于SARIMA模型实现快速数据生成，支持零样本预测，在M-Series和GiftEval基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决零样本时间序列预测面临的挑战：数据有限且存在偏差、评估易泄露、隐私和许可限制，需要实用的时间序列模拟方法。

Method: 基于SARIMA模型的三步模拟流程：1)从特征多项式稳定区域采样良好轨迹；2)叠加多个路径创建多季节性序列；3)添加基于速率的厚尾噪声模型捕捉突发性和间歇性。

Result: SarSim0比基于核的生成器快几个数量级，可生成约10亿个独特模拟序列，训练后的神经网络在零样本预测中超越统计方法和现有基线，在GiftEval上甚至超过生成过程AutoARIMA。

Conclusion: SarSim0为时间序列模拟提供了实用解决方案，支持高效零样本预测训练，在工业应用中表现出色，展现了"学生超越老师"的效果。

Abstract: Zero-shot time-series forecasting holds great promise, but is still in its infancy, hindered by limited and biased data corpora, leakage-prone evaluation, and privacy and licensing constraints. Motivated by these challenges, we propose the first practical univariate time series simulation pipeline which is simultaneously fast enough for on-the-fly data generation and enables notable zero-shot forecasting performance on M-Series and GiftEval benchmarks that capture trend/seasonality/intermittency patterns, typical of industrial forecasting applications across a variety of domains. Our simulator, which we call SarSim0 (SARIMA Simulator for Zero-Shot Forecasting), is based off of a seasonal autoregressive integrated moving average (SARIMA) model as its core data source. Due to instability in the autoregressive component, naive SARIMA simulation often leads to unusable paths. Instead, we follow a three-step procedure: (1) we sample well-behaved trajectories from its characteristic polynomial stability region; (2) we introduce a superposition scheme that combines multiple paths into rich multi-seasonality traces; and (3) we add rate-based heavy-tailed noise models to capture burstiness and intermittency alongside seasonalities and trends. SarSim0 is orders of magnitude faster than kernel-based generators, and it enables training on circa 1B unique purely simulated series, generated on the fly; after which well-established neural network backbones exhibit strong zero-shot generalization, surpassing strong statistical forecasters and recent foundation baselines, while operating under strict zero-shot protocol. Notably, on GiftEval we observe a "student-beats-teacher" effect: models trained on our simulations exceed the forecasting accuracy of the AutoARIMA generating processes.

</details>


### [62] [Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations](https://arxiv.org/abs/2601.01003)
*Amin Abyaneh,Charlotte Morissette,Mohamad H. Danesh,Anas El Houssaini,David Meger,Gregory Dudek,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: 提出收缩扩散策略(CDPs)，通过在扩散采样动力学中引入收缩行为来增强离线策略学习的鲁棒性，减少求解器和得分匹配误差的影响


<details>
  <summary>Details</summary>
Motivation: 扩散策略在离线策略学习中表现出色，但其基于得分的SDE建模会带来求解器误差、得分匹配误差、大数据需求以及动作生成不一致等问题。这些在图像生成中不太关键的问题在连续控制环境中会累积并导致失败

Method: 引入收缩扩散策略(CDPs)，在扩散采样动力学中诱导收缩行为。收缩会将相邻的流拉近，从而增强对求解器和得分匹配误差的鲁棒性，同时减少不必要的动作方差。提供了理论分析和实用的实现方法，可以最小化修改和计算成本地集成到现有扩散策略架构中

Result: 在仿真和真实世界环境中进行了广泛实验评估。在多个基准测试中，CDPs通常优于基线策略，在数据稀缺情况下表现出更明显的优势

Conclusion: CDPs通过引入收缩行为有效解决了扩散策略在连续控制中的误差累积问题，提高了离线策略学习的鲁棒性和性能，特别是在数据稀缺情况下表现优异

Abstract: Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.

</details>


### [63] [Data-Driven Assessment of Concrete Mixture Compositions on Chloride Transport via Standalone Machine Learning Algorithms](https://arxiv.org/abs/2601.01009)
*Mojtaba Aliasghar-Mamaghani,Mohammadreza Khalafi*

Main category: cs.LG

TL;DR: 使用多种机器学习算法预测混凝土混合物成分对氯离子渗透时间演化的影响，KRR、GPR和MLP表现最佳，揭示了成分与氯离子含量的相关性。


<details>
  <summary>Details</summary>
Motivation: 评估混凝土结构在侵蚀环境下的使用寿命，需要理解混凝土混合物成分如何影响氯离子渗透的时间演化，这对基础设施耐久性至关重要。

Method: 采用多种简单和复杂的机器学习算法：线性回归、KNN回归、核岭回归、支持向量回归、高斯过程回归、多层感知机和门控循环单元，基于综合数据集进行预测分析。

Result: KRR、GPR和MLP表现出高精度，GRU因数据多样性未能准确预测。大多数混合物成分与氯离子含量呈负相关，少数呈正相关。GPR模型揭示了清晰的潜在相关性趋势。

Conclusion: 机器学习方法可作为替代方法有效描述氯离子渗透的物理过程和相关关系，有助于增强基础设施的服务寿命，其中GPR、MLP、SVR和KRR提供了可接受的趋势估计。

Abstract: This paper employs a data-driven approach to determine the impact of concrete mixture compositions on the temporal evolution of chloride in concrete structures. This is critical for assessing the service life of civil infrastructure subjected to aggressive environments. The adopted methodology relies on several simple and complex standalone machine learning (ML) algorithms, with the primary objective of establishing confidence in the unbiased prediction of the underlying hidden correlations. The simple algorithms include linear regression (LR), k-nearest neighbors (KNN) regression, and kernel ridge regression (KRR). The complex algorithms entail support vector regression (SVR), Gaussian process regression (GPR), and two families of artificial neural networks, including a feedforward network (multilayer perceptron, MLP) and a gated recurrent unit (GRU). The MLP architecture cannot explicitly handle sequential data, a limitation addressed by the GRU. A comprehensive dataset is considered. The performance of ML algorithms is evaluated, with KRR, GPR, and MLP exhibiting high accuracy. Given the diversity of the adopted concrete mixture proportions, the GRU was unable to accurately reproduce the response in the test set. Further analyses elucidate the contributions of mixture compositions to the temporal evolution of chloride. The results obtained from the GPR model unravel latent correlations through clear and explainable trends. The MLP, SVR, and KRR also provide acceptable estimates of the overall trends. The majority of mixture components exhibit an inverse relation with chloride content, while a few components demonstrate a direct correlation. These findings highlight the potential of surrogate approaches for describing the physical processes involved in chloride ingress and the associated correlations, toward the ultimate goal of enhancing the service life of civil infrastructure.

</details>


### [64] [Geometric and Dynamic Scaling in Deep Transformers](https://arxiv.org/abs/2601.01014)
*Haoran Su,Chenyu You*

Main category: cs.LG

TL;DR: 该论文提出深度Transformer中的表示崩溃本质上是几何问题，而非优化问题，并提出了Manifold-Geometric Transformer (MGT)架构，通过流形约束超连接和深度增量学习来防止表示退化。


<details>
  <summary>Details</summary>
Motivation: 现有研究将深度Transformer的表示崩溃归因于优化不稳定或梯度消失，但这些解释无法说明为何在现代归一化和初始化方案下崩溃仍然存在。作者认为这本质上是一个几何问题：标准残差更新缺乏约束更新方向或擦除过时信息的机制，导致表示退化。

Method: 提出了统一的几何框架，包含两个正交原则：1) 流形约束超连接，将残差更新限制在有效的局部切向方向，防止不受控制的流形漂移；2) 深度增量学习，引入数据依赖的非单调更新，能够反射和擦除冗余特征而非无条件累积。

Result: 提出的MGT架构将特征更新的方向和符号解耦，实现了跨深度的稳定几何演化。分析预测，强制几何有效性同时允许动态擦除对于避免超深网络中的秩崩溃至关重要。

Conclusion: 深度Transformer的崩溃是几何问题而非深度本身。通过流形约束和动态擦除机制，可以构建超过100层的稳定Transformer架构，几何有效性是深度表示学习的关键限制因素。

Abstract: Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.

</details>


### [65] [Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations](https://arxiv.org/abs/2601.01021)
*Dai Shi,Lequan Lin,Andi Han,Luke Thompson,José Miguel Hernández-Lobato,Zhiyong Wang,Junbin Gao*

Main category: cs.LG

TL;DR: 基于Wiener混沌展开设计神经算子架构，用于学习SDE/SPDE的解算子，通过正交Hermite特征投影噪声路径，用神经算子参数化确定性混沌系数，实现单次前向传播从噪声重构完整解轨迹。


<details>
  <summary>Details</summary>
Motivation: 随机微分方程(SDEs)和随机偏微分方程(SPDEs)是建模随机动力学的基础工具。开发深度学习模型来逼近它们的解算子不仅有望提供快速实用的求解器，还可能启发从新视角解决经典学习任务的模型。

Method: 基于经典Wiener混沌展开(WCE)，将驱动噪声路径投影到正交Wick Hermite特征上，用神经算子参数化得到的确定性混沌系数，从而可以从噪声单次前向传播重构完整解轨迹。理论方面研究了多维SDE和半线性SPDE的WCE结果，明确写出其混沌系数的耦合ODE/PDE系统。

Result: 在多个问题上验证了模型：经典SPDE基准、图像上的扩散一步采样、图上的拓扑插值、金融外推、参数估计以及洪水预测的流形SDE，展示了竞争性准确度和广泛适用性。

Conclusion: WCE-based神经算子为跨多个领域学习SDE/SPDE解算子提供了实用且可扩展的方法，表明该方法具有广泛的应用潜力。

Abstract: Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.

</details>


### [66] [Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI](https://arxiv.org/abs/2601.01045)
*Tatsuaki Tsuruyama*

Main category: cs.LG

TL;DR: 提出基于信息论Lyapunov函数V-δ的投影反向扩散方法，用于控制生成图像中的粗粒度统计量（如分块强度），在保持像素级精度和视觉质量的同时，将分块质量误差控制在预设容差内。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型和基于分数的生成模型缺乏理论来描述粗粒度统计量（如图像分块强度或类别比例）在反向扩散过程中的演化规律。需要一种能够显式控制这些粗粒度量的反向扩散设计方法。

Method: 将信息论Lyapunov函数V-δ框架移植到生成模型的反向扩散过程，提出V-δ投影反向扩散方案。扩展V的单调性到时间非齐次的分块保持马尔可夫核，证明在小泄漏和V-δ投影下，V-δ可作为近似Lyapunov函数。

Result: 在由分块常数图像和简化反向核组成的玩具模型上数值验证：所提方法能将分块质量误差和泄漏容差势函数控制在预设容差内，同时达到与非投影动力学相当的像素级精度和视觉质量。

Conclusion: 将生成采样重新解释为从噪声到数据的信息势函数下降过程，为具有显式粗粒度量控制的反向扩散过程提供了设计原则，实现了对图像分块统计量的精确调控。

Abstract: Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses.
  In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.

</details>


### [67] [A UCB Bandit Algorithm for General ML-Based Estimators](https://arxiv.org/abs/2601.01061)
*Yajing Liu,Erkao Bao,Linqi Song*

Main category: cs.LG

TL;DR: ML-UCB：一种将任意机器学习模型集成到多臂老虎机框架中的广义上置信界算法，通过直接建模底层估计器的学习曲线行为来克服传统方法需要可处理集中不等式的问题。


<details>
  <summary>Details</summary>
Motivation: 在序列决策中部署复杂ML模型时，缺乏可处理的集中不等式是一个基本挑战，这限制了ML模型在多臂老虎机框架中的原则性探索。

Method: 假设均方误差随训练样本数呈幂律下降，推导出广义集中不等式，并基于此开发ML-UCB算法，可直接集成任何学习曲线可经验表征的ML模型。

Result: 理论证明ML-UCB可实现次线性遗憾，实验验证在协同过滤推荐系统中使用在线矩阵分解时，相比LinUCB有显著改进。

Conclusion: ML-UCB为将任意ML模型集成到多臂老虎机框架提供了原则性方法，无需模型特定的理论分析，只需经验表征学习曲线即可。

Abstract: We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB

</details>


### [68] [SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models](https://arxiv.org/abs/2601.01062)
*Yunlin Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种端到端的视觉播客生成管道，通过合成到真实的训练策略微调Qwen3-VL-32B模型，在对话自然性和叙事深度上显著优于更大的基础模型。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在描述性任务上表现出色，但在生成引人入胜的长篇叙事（特别是多人播客对话）方面仍未被充分探索且难以评估。传统指标无法捕捉对话自然性、个性和叙事流程的细微差别。

Method: 1. 提出端到端视觉播客生成管道；2. 在4000个图像-对话对数据集上微调Qwen3-VL-32B模型；3. 采用合成到真实的训练策略：在SPoRC高质量播客对话与合成生成图像上训练，在VIST真实世界照片序列上评估；4. 提出综合评估框架，使用AI作为评判和新型风格指标。

Result: 微调的32B模型在对话自然性上显著优于235B基础模型（胜率>80%），叙事深度增加50%（平均对话轮次长度），同时保持相同的视觉基础能力（CLIPScore: 20.39）。

Conclusion: 通过合成到真实的训练策略和综合评估框架，可以有效提升视觉语言模型在生成引人入胜的多人播客对话方面的能力，即使较小模型也能在对话自然性和叙事深度上超越更大模型。

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\% win rate) and narrative depth (+50\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).

</details>


### [69] [Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments](https://arxiv.org/abs/2601.01075)
*Hansen Jin Lillemark,Benhao Huang,Fangneng Zhan,Yilun Du,Thomas Anderson Keller*

Main category: cs.LG

TL;DR: 提出Flow Equivariant World Models框架，将自运动和外部物体运动统一为单参数李群"流"，利用群等变性实现稳定潜在世界表示，在部分观测视频世界建模基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有神经网络世界模型忽略了世界体验中的连续流对称性结构，反复从数据中重新学习相同变换，缺乏对内在和外部运动对称性的结构化利用。

Method: 将自运动和外部物体运动统一为单参数李群"流"，实现对这些变换的群等变性，构建具有稳定潜在表示的世界模型框架。

Result: 在2D和3D部分观测视频世界建模基准上，显著优于当前最先进的基于扩散和记忆增强的世界建模架构，特别是在智能体当前视野外存在可预测世界动态时表现更优，对长时滚动预测具有良好泛化能力。

Conclusion: 通过将世界模型表示结构与内部和外部运动对齐，流等变性为数据高效、对称性引导的具身智能提供了一条可扩展路径。

Abstract: Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.

</details>


### [70] [Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces](https://arxiv.org/abs/2601.01082)
*Bryon Tjanaka,Henry Chen,Matthew C. Fontaine,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: DMS（Discount Model Search）是一种新的质量多样性优化算法，使用连续模型替代直方图来指导探索，解决了高维度量空间中相似解被归入同一单元导致探索停滞的问题。


<details>
  <summary>Details</summary>
Motivation: 现有QD算法（如CMA-MAE）在高维度量空间中表现不佳，因为许多解会映射到相似的度量值，被归入直方图的同一单元，导致探索停滞。需要一种能处理高维度量空间的方法。

Method: 提出Discount Model Search（DMS），使用连续模型替代直方图来提供平滑、连续的折扣值表示。该模型能区分度量相似但不同的解，从而在高维空间中持续探索。

Result: DMS在高维基准测试和两个新应用领域（度量空间为图像空间）中优于CMA-MAE和其他黑盒QD算法，实现了更好的探索性能。

Conclusion: DMS通过连续折扣模型解决了高维度量空间中的探索问题，拓展了QD算法的应用范围，特别是在用户可通过图像数据集指定度量而非手动设计度量函数的场景中。

Abstract: Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.

</details>


### [71] [Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding](https://arxiv.org/abs/2601.01089)
*Nobuyuki Ota*

Main category: cs.LG

TL;DR: CDT是一个整合DNA、RNA和蛋白质预训练模型的架构，遵循中心法则方向性逻辑，通过跨注意力机制产生统一虚拟细胞嵌入，在CRISPRi增强子扰动数据上达到0.503皮尔逊相关性。


<details>
  <summary>Details</summary>
Motivation: 虽然领域特定的基础模型在DNA、RNA和蛋白质各自模态上取得成功，但它们仍然孤立，限制了我们对整合细胞过程的建模能力。需要一种能够整合中心法则三个分子系统的架构。

Method: 提出中心法则变换器（CDT），整合DNA、RNA和蛋白质的预训练语言模型，采用方向性跨注意力机制：DNA到RNA注意力建模转录调控，RNA到蛋白质注意力建模翻译关系，产生统一的虚拟细胞嵌入。

Result: 在K562细胞的CRISPRi增强子扰动数据上，CDT v1（使用固定RNA和蛋白质嵌入的概念验证实现）达到0.503皮尔逊相关性，占交叉实验变异性理论上限（r=0.797）的63%。注意力和梯度分析提供了互补的解释窗口。

Conclusion: 与生物信息流对齐的AI架构既能实现预测准确性，又能获得机制可解释性。CDT展示了整合中心法则三个分子系统的潜力。

Abstract: Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein following the directional logic of the Central Dogma. CDT employs directional cross-attention mechanisms - DNA-to-RNA attention models transcriptional regulation, while RNA-to-Protein attention models translational relationships - producing a unified Virtual Cell Embedding that integrates all three modalities. We validate CDT v1 - a proof-of-concept implementation using fixed (non-cell-specific) RNA and protein embeddings - on CRISPRi enhancer perturbation data from K562 cells, achieving a Pearson correlation of 0.503, representing 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). Attention and gradient analyses provide complementary interpretive windows: in detailed case studies, these approaches highlight largely distinct genomic regions, with gradient analysis identifying a CTCF binding site that Hi-C data showed as physically contacting both enhancer and target gene. These results suggest that AI architectures aligned with biological information flow can achieve both predictive accuracy and mechanistic interpretability.

</details>


### [72] [Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings](https://arxiv.org/abs/2601.01119)
*Muhammad Ashad Kabir,Sirajam Munira,Dewan Tasnia Azad,Saleh Mohammed Ikram,Mohammad Habibur Rahman Sarker,Syed Manzoor Ahmed Hanifi*

Main category: cs.LG

TL;DR: 开发了一个针对孟加拉国和南亚人群的可解释机器学习框架，用于社区早期慢性肾病筛查，相比现有工具显著提高了准确性和敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有慢性肾病筛查工具主要基于高收入国家人群开发，在孟加拉国和南亚地区表现不佳，且大多依赖简单评分函数和晚期患者数据，无法捕捉风险因素的复杂交互作用，难以预测早期肾病。

Method: 使用孟加拉国社区数据集（南亚首个此类数据集），评估12种机器学习分类器，应用10种互补特征选择技术识别稳健可泛化的预测因子，采用10折交叉验证，并在印度、阿联酋和孟加拉国的三个独立数据集上进行外部验证，使用SHAP提供模型可解释性。

Result: 基于RFECV选择特征子集的ML模型平衡准确率达90.40%，最小非病理测试特征集平衡准确率达89.23%，通常优于更大或完整特征集。相比现有筛查工具，准确性、敏感性显著提高且所需输入更少。外部验证显示敏感性78%-98%，SHAP识别出与已知CKD风险因素一致的临床意义预测因子。

Conclusion: 该研究开发了一个针对低资源环境的可解释机器学习框架，能够有效筛查早期慢性肾病，特别适用于孟加拉国和南亚人群，具有高准确性、强泛化能力和临床可解释性。

Abstract: Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.

</details>


### [73] [Learning from Historical Activations in Graph Neural Networks](https://arxiv.org/abs/2601.01123)
*Yaniv Galron,Hadar Sinai,Haggai Maron,Moshe Eliasof*

Main category: cs.LG

TL;DR: HISTOGRAPH提出了一种基于注意力的两阶段图池化方法，利用中间层激活历史来改进图分类性能，特别在深层GNN中表现稳健。


<details>
  <summary>Details</summary>
Motivation: 现有图池化方法仅使用最后一层GNN特征，未能充分利用前向传播过程中产生的中间层激活（历史图激活）。这在节点表示可能随层数变化显著的情况下尤其明显，且图特有的挑战如深度架构中的过平滑问题会加剧这一问题。

Method: HISTOGRAPH是一种新颖的两阶段基于注意力的最终聚合层：首先在中间激活上应用统一的层间注意力，然后进行节点间注意力。通过建模节点表示在层间的演化，该方法利用节点的激活历史和图结构来优化用于最终预测的特征。

Result: 在多个图分类基准测试上的实证结果表明，HISTOGRAPH提供了强大的性能，持续改进传统技术，在深度GNN中表现出特别强的鲁棒性。

Conclusion: 通过利用历史图激活，HISTOGRAPH能够更有效地聚合图表示，特别是在深层架构中，为解决图池化中的信息利用不足问题提供了有效方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.

</details>


### [74] [Self-Training the Neurochaos Learning Algorithm](https://arxiv.org/abs/2601.01146)
*Anusree M,Akhila Henry,Pramod P Nair*

Main category: cs.LG

TL;DR: 该研究提出了一种结合神经混沌学习与自训练的混合半监督学习架构，在标记数据稀缺的情况下显著提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 在许多实际应用中，获取大量标记数据既困难又昂贵，而未标记数据则容易获得。传统的监督学习方法在标记数据少或数据集不平衡的情况下表现不佳。

Method: 提出了一种混合半监督学习架构，将神经混沌学习（NL）与基于阈值的自训练（ST）方法相结合。NL将输入特征转换为混沌发放率表示，捕捉数据中的非线性关系；ST则利用高置信度的伪标记样本逐步扩大标记集。

Result: 在10个基准数据集和5个机器学习分类器上评估，使用85%未标记数据和15%标记数据进行训练。提出的NL+ST架构相比独立ST模型获得显著性能提升，特别是在有限、非线性和不平衡数据集上：Iris（188.66%）、Wine（158.58%）和Glass Identification（110.48%）。

Conclusion: 结果表明，将基于混沌的特征提取与半监督学习结合使用，可以在低数据环境下提高泛化能力、鲁棒性和分类准确性。

Abstract: In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.

</details>


### [75] [Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification](https://arxiv.org/abs/2601.01150)
*Wenbin Pei,Ruohao Dai,Bing Xue,Mengjie Zhang,Qiang Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: Evo-TFS：一种结合时域和频域特征的进化过采样方法，用于不平衡时间序列分类，通过强类型遗传编程生成多样化的高质量时间序列样本。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法假设数据分布平衡，在不平衡数据中会忽略少数类；传统过采样方法依赖线性插值，难以保持时间动态特征和生成多样化样本。

Method: 使用强类型遗传编程，结合时域和频域特征的适应度函数，进化生成多样化的高质量时间序列样本。

Result: 在不平衡时间序列数据集上的实验表明，Evo-TFS优于现有过采样方法，显著提升了时域和频域分类器的性能。

Conclusion: Evo-TFS通过整合时域和频域特征的进化方法，有效解决了不平衡时间序列分类问题，生成了更高质量和多样性的少数类样本。

Abstract: Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.

</details>


### [76] [Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models](https://arxiv.org/abs/2601.01162)
*Zihua Yang,Xin Liao,Yiqun Zhang,Yiu-ming Cheung*

Main category: cs.LG

TL;DR: ARISE利用大语言模型为分类数据提供语义嵌入，增强聚类效果


<details>
  <summary>Details</summary>
Motivation: 分类数据聚类面临相似性度量挑战，传统方法将无序属性值视为等距，造成语义鸿沟。现有方法依赖数据集内共现模式，在样本有限时不可靠，未能充分利用数据语义上下文。

Method: 提出ARISE方法，利用大语言模型获取外部语义知识，为分类属性值构建语义感知表示，将LLM增强的嵌入与原始数据结合，探索语义突出的聚类结构。

Result: 在8个基准数据集上实验，相比7个代表性方法获得19-27%的性能提升。

Conclusion: ARISE通过整合外部语义知识有效弥补了分类数据聚类的语义鸿沟，显著提升了聚类质量。

Abstract: Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE

</details>


### [77] [MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches](https://arxiv.org/abs/2601.01206)
*Soroush Elyasi,Arya VarastehNezhad,Fattaneh Taghiyareh*

Main category: cs.LG

TL;DR: 使用多类型严肃游戏框架结合机器学习预测软件开发岗位适合度，通过游戏行为特征替代传统人格问卷，达到97%精度和94%准确率


<details>
  <summary>Details</summary>
Motivation: 传统职业评估中的人格问卷存在回应偏差、疲劳和故意扭曲等问题，需要一种更客观、可扩展且减少偏见的替代方法。游戏化评估通过捕捉游戏过程中的隐式行为信号，为职业适合度评估提供了新途径。

Method: 1. 通过文献综述和软件工程师实证研究确定与开发者相关的人格和行为特征；2. 设计定制移动游戏来激发问题解决、规划、适应性、坚持性、时间管理和信息寻求等行为；3. 收集细粒度游戏事件数据；4. 采用两阶段建模策略，仅使用游戏行为特征预测适合度。

Result: 模型达到最高97%的精确度和94%的准确率。行为分析显示，合适的候选人表现出独特的游戏模式：在解谜游戏中更多获胜、完成更多支线挑战、更频繁导航菜单、更少暂停、重试和放弃行为。

Conclusion: 游戏过程中捕捉的隐式行为痕迹能够有效预测软件开发适合度，无需显式人格测试。严肃游戏作为职业评估工具具有可扩展性、趣味性和较少偏见的特点，是传统问卷的有前景替代方案。

Abstract: Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.

</details>


### [78] [The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context](https://arxiv.org/abs/2601.01231)
*Md Muhtasim Munif Fahim,Humyra Ankona,Md Monimul Huq,Md Rezaul Karim*

Main category: cs.LG

TL;DR: 该研究提出"依赖鸿沟"框架，发现在基础设施脆弱的环境中，高度投入的学生反而更容易因网络故障而受挫，挑战了传统数字鸿沟理论。


<details>
  <summary>Details</summary>
Motivation: 传统数字鸿沟框架无法解释在资源受限环境中，拥有同等网络连接的学生对数字学习平台的满意度差异。研究旨在探索为什么高度投入的学生在基础设施故障时反而更脆弱。

Method: 对孟加拉国396名大学生进行横断面研究，采用三阶段分析方法：1) K-prototypes聚类识别学生档案；2) 档案特异性随机森林模型分析满意度驱动因素；3) 倾向得分匹配的正式交互分析检验依赖鸿沟假设。

Result: 识别出三类学生档案：偶尔投入(58%)、高效学习者(35%)、高度投入(7%)。发现教育设备使用时间与网络可靠性之间存在显著交互作用，证实了依赖鸿沟假设。高度投入学生最脆弱，针对性可靠性改进的回报是统一干预的2.06倍。

Conclusion: 在脆弱基础设施环境中，能力可能变成负担。数字转型政策应优先保障高依赖用户的可靠性，建立应急系统，并教育学生了解依赖风险，而非单纯鼓励投入。

Abstract: Background: While digital access has expanded rapidly in resource-constrained contexts, satisfaction with digital learning platforms varies significantly among students with seemingly equal connectivity. Traditional digital divide frameworks fail to explain these variations.
  Purpose: This study introduces the "Dependency Divide", a novel framework proposing that highly engaged students become conditionally vulnerable to infrastructure failures, challenging assumptions that engagement uniformly benefits learners in post-access environments.
  Methods: We conducted a cross-sectional study of 396 university students in Bangladesh using a three-stage analytical approach: (1) stability-validated K-prototypes clustering to identify student profiles, (2) profile-specific Random Forest models with SHAP and ALE analysis to determine satisfaction drivers, and (3) formal interaction analysis with propensity score matching to test the Dependency Divide hypothesis.
  Results: Three distinct profiles emerged: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). A significant interaction between educational device time and internet reliability (\b{eta} = 0.033, p = 0.028) confirmed the Dependency Divide: engagement increased satisfaction only when infrastructure remained reliable. Hyper-Engaged students showed greatest vulnerability despite or because of their sophisticated digital workflows. Policy simulations demonstrated that targeted reliability improvements for high-dependency users yielded 2.06 times greater returns than uniform interventions.
  Conclusions: In fragile infrastructure contexts, capability can become liability. Digital transformation policies must prioritize reliability for dependency-prone users, establish contingency systems, and educate students about dependency risks rather than uniformly promoting engagement.

</details>


### [79] [Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers on Long-Context Dyadic Sessions](https://arxiv.org/abs/2601.01237)
*Abidemi Koledoye,Chinemerem Unachukwu,Gold Nwobu,Hasin Rana*

Main category: cs.LG

TL;DR: 该论文比较了Mamba SSM与LLaMA Transformer在长上下文序列建模上的性能，发现SSM在计算复杂度上具有线性优势，但在不同场景下各有优劣。


<details>
  <summary>Details</summary>
Motivation: State Space Models (SSMs) 作为Transformer的替代方案，在长上下文序列建模中展现出线性计算复杂度的优势，但需要在实际应用中验证其相对于Transformer的具体优势条件和场景。

Method: 使用治疗会话作为测试案例，从两个维度评估Mamba SSM和LLaMA Transformer：1) 计算效率（内存使用和推理速度，测试512到8,192个token）；2) 表示效率（分析隐藏状态动态和注意力模式）。

Result: 研究提供了SSM相对于Transformer的优势条件的具体洞察，确定了在不同上下文长度下两种架构的性能表现和适用场景。

Conclusion: SSM在长上下文序列建模中确实具有计算复杂度优势，但需要根据具体应用场景选择合适架构，论文为实践者提供了实用的指导原则。

Abstract: State Space Models (SSMs) have emerged as a promising alternative to Transformers for long-context sequence modeling, offering linear $O(N)$ computational complexity compared to the Transformer's quadratic $O(N^2)$ scaling. This paper presents a comprehensive benchmarking study comparing the Mamba SSM against the LLaMA Transformer on long-context sequences, using dyadic therapy sessions as a representative test case. We evaluate both architectures across two dimensions: (1) computational efficiency, where we measure memory usage and inference speed from 512 to 8,192 tokens, and (2) representational efficiency, where we analyze hidden state dynamics and attention patterns. Our findings provide actionable insights for practitioners working with long-context applications, establishing precise conditions under which SSMs offer advantages over Transformers.

</details>


### [80] [Accelerated Full Waveform Inversion by Deep Compressed Learning](https://arxiv.org/abs/2601.01268)
*Maayan Gelboim,Amir Adler,Mauricio Araya-Polo*

Main category: cs.LG

TL;DR: 提出一种通过深度神经网络和压缩学习降低全波形反演数据维度的方法，利用二值化感知层学习关键地震采集布局，通过自编码器和K-means聚类进一步选择最相关数据，显著优于随机采样。


<details>
  <summary>Details</summary>
Motivation: 现代地震采集系统产生的数据量达到太字节级别，工业级全波形反演计算成本过高，使得复杂地下情况分析或多场景探索变得不可行，需要降低数据维度以减轻计算负担。

Method: 使用带二值化感知层的深度神经网络进行压缩学习，从大量地下模型中学习关键地震采集布局；对大型地震数据集，网络先选择子集，然后通过自编码器计算潜在表示，再用K-means聚类进一步筛选最相关数据，形成分层选择机制。

Result: 该方法在仅使用10%数据的情况下，在2D全波形反演中始终优于随机数据采样，为大规模3D反演加速铺平了道路。

Conclusion: 提出的分层数据选择方法能有效降低全波形反演的计算成本，通过智能选择关键数据而非随机采样，显著提升反演效率，有望推动大规模3D反演的实际应用。

Abstract: We propose and test a method to reduce the dimensionality of Full Waveform Inversion (FWI) inputs as computational cost mitigation approach. Given modern seismic acquisition systems, the data (as input for FWI) required for an industrial-strength case is in the teraflop level of storage, therefore solving complex subsurface cases or exploring multiple scenarios with FWI become prohibitive. The proposed method utilizes a deep neural network with a binarized sensing layer that learns by compressed learning a succinct but consequential seismic acquisition layout from a large corpus of subsurface models. Thus, given a large seismic data set to invert, the trained network selects a smaller subset of the data, then by using representation learning, an autoencoder computes latent representations of the data, followed by K-means clustering of the latent representations to further select the most relevant data for FWI. Effectively, this approach can be seen as a hierarchical selection. The proposed approach consistently outperforms random data sampling, even when utilizing only 10% of the data for 2D FWI, these results pave the way to accelerating FWI in large scale 3D inversion.

</details>


### [81] [The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification](https://arxiv.org/abs/2601.01290)
*Harshita Narnoli,Mihai Surdeanu*

Main category: cs.LG

TL;DR: 本文通过比较上下文学习(ICL)与在相同演示上训练的监督分类器的行为，研究了ICL的工作机制，发现当演示相关性高时，LLM行为类似于kNN分类器，相关性低时LLM表现更好，因为它们可以利用参数记忆。


<details>
  <summary>Details</summary>
Motivation: 尽管上下文学习(ICL)已成为快速定制LLM的重要范式，但其工作机制仍不清楚。本文旨在通过比较ICL与监督分类器的行为来理解ICL的工作原理。

Method: 使用文本分类作为用例，在六个数据集和三个LLM上，将ICL的行为与基于梯度下降(GD)和k近邻(kNN)的监督分类器进行比较，分析三种研究问题。

Result: 当演示相关性高时，LLM行为与这些分类器相似，且ICL更接近kNN而非逻辑回归；当演示相关性低时，LLM表现优于这些分类器，因为LLM可以利用其参数记忆。

Conclusion: ICL机制更类似于kNN而非梯度下降，LLM在演示相关性低时的优势源于其参数记忆能力，这为理解ICL工作机制提供了实证证据。

Abstract: In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.

</details>


### [82] [Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space](https://arxiv.org/abs/2601.01295)
*Changhoon Song,Seungchan Ko,Youngjoon Hong*

Main category: cs.LG

TL;DR: 提出对数加权Barron空间，比传统Barron空间要求更弱的正则性条件，证明深度ReLU网络在该空间中具有显式深度依赖的逼近能力，解释深度如何降低高效表示所需的正则性要求。


<details>
  <summary>Details</summary>
Motivation: 传统通用逼近定理无法解释深度学习在高维数据上的实际成功，因为参数数量可能随维度指数增长。经典Barron空间虽然能提供O(n^{-1/2})的逼近误差，但仍需要比Sobolev空间更强的正则性条件，且现有深度敏感结果通常有约束限制（如sL ≤ 1/2）。

Method: 1. 引入对数加权Barron空间B^log，其假设条件比任何s>0的B^s空间都弱；2. 研究该空间的嵌入性质并通过Rademacher复杂度进行统计分析；3. 证明B^log中的函数可由深度ReLU网络以显式深度依赖方式逼近；4. 定义B^{s,log}族，建立H^1范数下的逼近界，并确定保持这些速率的最大深度尺度。

Result: 建立了对数加权Barron空间中深度ReLU网络的逼近理论，证明了深度如何降低高效表示所需的函数正则性要求，为深度架构在超越经典Barron设置下的性能提供了更精确的解释。

Conclusion: 对数加权Barron空间为理解深度网络在高维问题中的成功提供了更精细的理论框架，表明深度可以显著降低表示复杂函数所需的正则性条件，这有助于解释为什么深度模型在实际高维应用中表现优异。

Abstract: Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \le 1/2$. In this paper, we introduce a log-weighted Barron space $\mathscr{B}^{\log}$, which requires a strictly weaker assumption than $\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\mathscr{B}^{\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\mathscr{B}^{s,\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.

</details>


### [83] [ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System](https://arxiv.org/abs/2601.01297)
*Anantha Sharma*

Main category: cs.LG

TL;DR: Argus框架将高维数据流中的分布漂移检测重新定义为在数据流形固定空间分区上跟踪局部统计量，解决了现有方法在可扩展性、几何结构保持和身份稳定性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 高维数据流中的分布漂移检测面临三个主要挑战：全局比较方法扩展性差，基于投影的方法丢失几何结构，重新聚类方法存在身份不稳定性。需要一种既能保持高维结构又具有计算效率的检测方法。

Method: Argus框架基于数据流形的固定空间分区（Voronoi细分），通过跟踪局部统计量来检测漂移。使用规范正交基上的Voronoi细分确保正交变换不变性，引入图论方法表征漂移传播，采用乘积量化细分扩展到超高维度（d>500）。

Result: 理论证明框架具有正交变换不变性，实现O(N)复杂度并提供细胞级空间定位，实验验证框架能正确识别坐标旋转下的漂移，而现有方法会产生误报。

Conclusion: Argus为分布监控提供了原则性的几何基础，既保持了高维结构又避免了成对比较的计算负担，通过固定空间分区实现了高效、稳定的漂移检测。

Abstract: Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.
  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.
  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.

</details>


### [84] [Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware](https://arxiv.org/abs/2601.01298)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: Warp Cortex是一个异步多智能体LLM框架，通过解耦智能体逻辑与物理内存，将内存复杂度从O(N*L)降至O(1)权重和O(N*k)上下文，在单张RTX 4090上实现100个并发智能体，理论支持1000+智能体。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体LLM框架存在线性内存扩展问题，导致"系统2"并行推理在消费级硬件上不实用，限制了大规模智能体系统的部署。

Method: 采用异步架构，通过Singleton权重共享和基于拓扑数据分析的拓扑突触技术，将KV缓存视为潜在空间中的点云，应用witness-complex稀疏化来保持上下文流形的持久同调特征，并引入非侵入式KV缓存更新机制Referential Injection。

Result: 在单张NVIDIA RTX 4090上，仅使用2.2GB VRAM实现100个并发智能体，理论容量超过1000个智能体，计算延迟成为主要瓶颈而非内存。

Conclusion: Warp Cortex通过创新的内存解耦和拓扑稀疏化技术，实现了百万级智能体的理论扩展能力，为大规模多智能体LLM系统在消费级硬件上的部署提供了可行方案。

Abstract: Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering "System 2" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.

</details>


### [85] [Towards a Principled Muon under $μ\mathsf{P}$: Ensuring Spectral Conditions throughout Training](https://arxiv.org/abs/2601.01306)
*John Zhao*

Main category: cs.LG

TL;DR: 提出Muon++优化器，在μP框架下保证谱条件，实现矩阵优化器在长时训练中的实用部署


<details>
  <summary>Details</summary>
Motivation: μP理论要求权重矩阵满足谱条件以确保宽度无关的学习动态，但现有Muon优化器要么无法保证整个训练过程的谱条件，要么需要频繁的谱归一化导致计算开销大，限制了矩阵优化器在LLM训练中的实际应用

Method: 提出Muon++优化器，核心洞察是对于中等大模型，仅需在优化器更新层面维持谱控制即可保持μP兼容的缩放，无需显式权重谱归一化；并首次引入数据依赖的自适应谱条件

Result: 开发出Muon++变体，在整个训练过程中满足谱条件，填补了μP理论承诺与矩阵优化器实际部署之间的差距

Conclusion: 该方法为矩阵优化器在长时LLM训练中的实用部署提供了可靠方案，并开启了自适应谱条件的研究方向

Abstract: The $μ$-parameterization ($μ$P) provides a principled foundation for large language model (LLM) training by prescribing width-independent learning dynamics, which in turn enables predictable scaling behavior and robust hyperparameter transfer across model sizes. A central requirement of $μ$P is the satisfaction of certain spectral conditions on weight matrices, which ensure consistent feature learning and optimization behavior as model width grows. While these conditions are well understood in theory, guaranteeing their validity in practical training for matrix-based optimizers such as Muon is still under studied. Existing works that study Muon under $μ$P exhibit important limitations: they either do not ensure that the spectral conditions hold throughout the entire training horizon, or require repeated spectral normalization (or Newton-Schulz iterations) applied to both weights and updates, leading to significant computational overhead and reduced practicality. In this work, we show how to reliably guarantee the spectral conditions required by $μ$P for Muon during the entire training process. Our key insight is that for moderately large models, maintaining spectral control at the level of optimizer updates alone is sufficient to preserve $μ$P-compatible scaling, eliminating the need for explicit spectral normalization of the weights. Based on this principle, we develop a variant of Muon, namely Muon++, that satisfies spectral condition throughout the training process. Our results bridge the gap between the theoretical promises of $μ$P and the practical deployment of matrix-based optimizers in long-horizon training. We also take the first step towards an adaptive spectral condition by incorporating data-dependent effects, making it better suited for long-horizon LLM training.

</details>


### [86] [Spectral-Window Hybrid (SWH)](https://arxiv.org/abs/2601.01313)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: SWH是一种混合架构，通过并行全局分支（利用卷积定理建模长程衰减动态）和局部分支（滑动窗口注意力）来平衡计算效率与表示能力，实现线性扩展到长序列。


<details>
  <summary>Details</summary>
Motivation: 将序列建模扩展到极长上下文时，需要在计算效率和表示表达能力之间取得平衡。Transformer虽然通过注意力机制提供精确检索，但其二次方复杂度限制了在长序列任务中的应用。

Method: 提出Spectral-Window Hybrid (SWH)架构，将序列建模解耦为两个并行流：1) 全局分支利用卷积定理在O(T log T)时间内建模长程衰减动态；2) 局部分支采用滑动窗口注意力处理有界上下文内的token交互。通过聚合这些表示，避免了全局注意力的计算瓶颈。

Result: SWH在短上下文上能达到标准Transformer的困惑度水平，同时能够高效地线性扩展到长序列。

Conclusion: SWH架构通过结合全局频谱建模和局部注意力机制，在保持局部精度的同时避免了全局注意力的计算瓶颈，为长序列建模提供了有效的解决方案。

Abstract: Scaling sequence modeling to extreme contexts requires balancing computational efficiency with representational expressivity. While Transformers provide precise retrieval via the attention mechanism, their quadratic $\mathcal{O}(T^2)$ complexity limits their application to long-horizon tasks. In this work, we propose the \textbf{Spectral-Window Hybrid (SWH)}, an architecture that decouples sequence modeling into two \textit{parallel} streams: a global branch utilizing the Convolution Theorem to model long-range decay dynamics in $\mathcal{O}(T \log T)$ time, and a local branch employing sliding-window attention for token interactions within a bounded context. By aggregating these representations, SWH avoids the computational bottleneck of global attention while retaining local precision. We demonstrate that SWH matches the perplexity of standard Transformers on short contexts while enabling efficient linear scaling to extended sequences. The code is available at https://github.com/VladimerKhasia/SWH

</details>


### [87] [From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion](https://arxiv.org/abs/2601.01347)
*Yuyan Pi,Min Jin,Wentao Xie,Xinhua Liu*

Main category: cs.LG

TL;DR: GM-MLG：基于图-基序特征融合和多标签生成的开放端药物不良反应预测新范式，将多标签分类转化为Transformer解码器多标签生成，预测空间从200扩展到10000+类型


<details>
  <summary>Details</summary>
Motivation: 当前药物不良反应预测方法面临数据稀缺导致的冷启动问题、封闭标签集限制以及标签依赖关系建模不足等挑战，需要新的开放端预测范式

Method: 1. 构建原子级、局部分子级（BRICS算法动态提取细粒度基序）和全局分子级的双图表示架构；2. 将ADR预测从多标签分类转化为基于Transformer解码器的多标签生成；3. 将ADR标签视为离散标记序列，使用位置嵌入显式捕获标签依赖关系；4. 通过自回归解码动态扩展预测空间

Result: GM-MLG实现了最高38%的性能提升，平均增益20%，预测空间从200种扩展到10000+类型；通过逆合成基序分析阐明了ADR与基序之间的非线性构效关系

Conclusion: GM-MLG为药物安全性系统风险降低提供了可解释的创新支持，通过开放端生成范式有效解决了冷启动和标签依赖建模问题，显著扩展了ADR预测能力

Abstract: Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.

</details>


### [88] [Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows](https://arxiv.org/abs/2601.01357)
*Ke Xiao,Haoze Zhang,Runze Mao,Han Li,Zhi X. Chen*

Main category: cs.LG

TL;DR: FlamePilot是一个专为燃烧建模设计的LLM智能体，能够自动执行CFD工作流，从科学文献中学习并自主配置、运行和优化模拟，在基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂科学领域（如燃烧建模）的应用存在关键缺口，需要将领域文献知识与专业工具（如CFD代码）的执行能力无缝集成，以实现实用的AI研究助手。

Method: FlamePilot采用基于原子工具的架构，确保在OpenFOAM和DeepFlame等框架中稳健地设置和执行复杂模拟。系统能够从科学文章中学习，提取关键信息来指导从初始设置到优化结果的整个模拟过程。

Result: 在公开基准测试中，FlamePilot获得了完美的1.0可执行性分数和0.438的成功率，超过了之前最佳代理的0.625和0.250分数。在MILD燃烧模拟案例研究中，系统能够自主将研究论文转化为配置的模拟，执行模拟，后处理结果，提出基于证据的改进，并在最少人工干预下管理多步骤参数研究直至收敛。

Conclusion: FlamePilot通过透明可解释的范式，为AI赋能的燃烧建模建立了基础框架，促进了研究人员与智能体之间的协作伙伴关系，让智能体管理工作流编排，使研究人员能够专注于高层次分析。

Abstract: The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.

</details>


### [89] [Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach](https://arxiv.org/abs/2601.01368)
*Mujin Zhou,Junzhe Zhang*

Main category: cs.LG

TL;DR: 提出基于f-GAN框架的因果发现方法，学习与具体权重值无关的二元因果结构，通过最小化贝叶斯自由能量实现结构学习


<details>
  <summary>Details</summary>
Motivation: 在存在未测量混杂因素的数据中进行因果发现是一个具有挑战性的问题，需要开发能够学习独立于具体权重值的因果结构的方法

Method: 将结构学习问题重新表述为最小化贝叶斯自由能量，证明该问题等价于最小化真实数据分布与模型生成分布之间的f-散度；利用f-GAN框架将目标转化为min-max对抗优化问题；使用Gumbel-Softmax松弛在离散图空间实现梯度搜索

Result: 该方法能够从存在未测量混杂因素的数据中学习二元因果结构，且学习到的结构独立于具体的权重参数值

Conclusion: 提出的基于f-GAN框架的因果发现方法为解决存在未测量混杂因素的因果结构学习问题提供了一种有效的解决方案

Abstract: Causal discovery from data with unmeasured confounding factors is a challenging problem. This paper proposes an approach based on the f-GAN framework, learning the binary causal structure independent of specific weight values. We reformulate the structure learning problem as minimizing Bayesian free energy and prove that this problem is equivalent to minimizing the f-divergence between the true data distribution and the model-generated distribution. Using the f-GAN framework, we transform this objective into a min-max adversarial optimization problem. We implement the gradient search in the discrete graph space using Gumbel-Softmax relaxation.

</details>


### [90] [Data Complexity-aware Deep Model Performance Forecasting](https://arxiv.org/abs/2601.01383)
*Yen-Chia Chen,Hsing-Kuo Pao,Hanjuan Huang*

Main category: cs.LG

TL;DR: 提出一个轻量级的两阶段框架，在训练前根据数据集特性和模型结构预测模型性能，用于指导架构选择和数据质量评估。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型架构选择通常依赖重复试错过程，耗时耗资源且难以自动化。现有性能预测方法要么需要大量计算开销，要么缺乏泛化能力。

Method: 提出两阶段框架：第一阶段基于数据集可测量属性预测基线性能；第二阶段结合模型架构和超参数细节调整估计。框架可泛化到不同数据集和模型类型。

Result: 框架不仅能预测模型性能，还能指导架构选择、告知必要预处理步骤，并在训练开始前检测潜在问题数据集。数据集方差等特征可作为数据质量的早期指标。

Conclusion: 该轻量级框架提供了一种高效替代传统试错方法的方式，能够在训练前预测性能并指导模型选择，同时有助于数据质量评估。

Abstract: Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.

</details>


### [91] [Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning](https://arxiv.org/abs/2601.01387)
*Yongzhe Li,Lin Guan,Zihan Cai,Zuxian Lin,Jiyu Huang,Liukai Chen*

Main category: cs.LG

TL;DR: 提出SaMPFA框架，通过局部拓扑切片采样和多任务图学习，提升电力潮流分析模型对系统规模变化的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 开发对拓扑变化具有强适应性的深度学习模型对潮流分析具有重要实践意义。现有模型在系统规模变化时性能受限，分支功率预测鲁棒性不足。

Method: 1) 提出SaMPFA框架；2) 引入局部拓扑切片采样技术，从完整电网中提取不同尺度的子图；3) 设计无参考多任务图学习模型，预测母线电压和分支功率而非相角；4) 损失函数中加入额外项，鼓励模型学习相角差和功率传输的物理模式。

Result: 在IEEE 39节点系统和实际省级电网上的仿真表明，所提模型在可变系统规模下具有优越的适应性和泛化能力，准确率分别提升4.47%和36.82%。

Conclusion: SaMPFA框架通过局部拓扑切片和多任务学习，有效提升了潮流分析模型对系统规模变化的适应性和预测鲁棒性，避免了相角预测中的误差放大问题。

Abstract: Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.

</details>


### [92] [A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble](https://arxiv.org/abs/2601.01403)
*Zewei Yu,Jianqiu Xu,Caimin Li*

Main category: cs.LG

TL;DR: 提出GDME：基于图模型的无监督在线时间序列异常检测框架，通过动态模型池和图结构进行模型集成，有效处理异构流数据


<details>
  <summary>Details</summary>
Motivation: 工业系统中流数据量不断增加，在线异常检测成为关键任务。现有方法多为离线设计或难以有效处理异构流数据，且数据模式多样且快速演变带来挑战

Method: GDME框架：1）维护动态模型池，持续更新（剪枝表现不佳模型，引入新模型）；2）使用动态图结构表示模型间关系；3）通过图上的社区检测选择合适子集进行集成；4）利用图结构变化监测概念漂移，适应流数据演变

Result: 在7个异构时间序列数据集上，GDME优于现有在线异常检测方法，提升达24%；集成策略优于单个模型和平均集成，计算效率有竞争力

Conclusion: GDME通过动态图结构和模型集成有效解决了在线时间序列异常检测中的异构流数据挑战，具有优越的检测性能和适应性

Abstract: With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.

</details>


### [93] [A Depth Hierarchy for Computing the Maximum in ReLU Networks via Extremal Graph Theory](https://arxiv.org/abs/2601.01417)
*Itay Safran*

Main category: cs.LG

TL;DR: 论文证明了ReLU神经网络计算d个实数最大值函数时存在深度层次结构：对于3≤k≤log₂(log₂(d))的深度，需要宽度Ω(d^{1+1/(2^{k-2}-1)})才能表示最大值函数，这是首个针对k≥3深度的无条件超线性下界。


<details>
  <summary>Details</summary>
Motivation: 最大值函数是神经网络中的基本算子，但对其计算复杂度的理解有限。现有研究主要关注浅层网络，对于深层网络表示最大值函数所需资源的下界缺乏理论结果。本文旨在填补这一空白，揭示最大值函数的内在复杂性。

Method: 采用组合论证方法，将最大值函数的不可微分脊线与计算网络第一隐藏层诱导的图中的团相关联。利用极值图论中的Turán定理，证明足够窄的网络无法捕捉最大值函数的非线性特性。

Result: 证明了对于3≤k≤log₂(log₂(d))的深度，表示d个实数最大值函数需要宽度Ω(d^{1+1/(2^{k-2}-1)})。这是首个针对k≥3深度的无条件超线性下界，即使深度随d增长也成立。

Conclusion: 尽管最大值函数看似简单，但其不可微分超平面的几何结构赋予了内在复杂性。该证明技术为深度神经网络下界证明提供了新方法，揭示了深度层次结构在神经网络表示能力中的重要性。

Abstract: We consider the problem of exact computation of the maximum function over $d$ real inputs using ReLU neural networks. We prove a depth hierarchy, wherein width $Ω\big(d^{1+\frac{1}{2^{k-2}-1}}\big)$ is necessary to represent the maximum for any depth $3\le k\le \log_2(\log_2(d))$. This is the first unconditional super-linear lower bound for this fundamental operator at depths $k\ge3$, and it holds even if the depth scales with $d$. Our proof technique is based on a combinatorial argument and associates the non-differentiable ridges of the maximum with cliques in a graph induced by the first hidden layer of the computing network, utilizing Turán's theorem from extremal graph theory to show that a sufficiently narrow network cannot capture the non-linearities of the maximum. This suggests that despite its simple nature, the maximum function possesses an inherent complexity that stems from the geometric structure of its non-differentiable hyperplanes, and provides a novel approach for proving lower bounds for deep neural networks.

</details>


### [94] [Unveiling the Heart-Brain Connection: An Analysis of ECG in Cognitive Performance](https://arxiv.org/abs/2601.01424)
*Akshay Sasi,Malavika Pradeep,Nusaibah Farrukh,Rahul Venugopal,Elizabeth Sherly*

Main category: cs.LG

TL;DR: ECG信号可以替代EEG作为认知负荷监测的可穿戴解决方案，通过跨模态机器学习框架将ECG特征映射到EEG认知空间，实现准确的认知状态分类。


<details>
  <summary>Details</summary>
Motivation: 虽然EEG是评估心理工作负荷的金标准，但其便携性有限限制了实际应用。广泛可用的ECG穿戴设备提供了一个实用的替代方案，需要研究ECG是否能可靠反映认知负荷并作为EEG指标的代理。

Method: 收集工作记忆和被动听力任务的多模态数据，提取ECG时域HRV指标和Catch22描述符，以及EEG频谱和Catch22特征。提出跨模态XGBoost框架，将ECG特征投影到EEG代表的认知空间，实现仅用ECG进行工作负荷推断。

Result: ECG衍生的投影能够表达性地捕捉认知状态的变化，为准确分类提供良好支持。ECG可以解释性地反映认知负荷变化。

Conclusion: ECG作为一种可解释、实时、可穿戴的解决方案，适用于日常认知监测，可以替代EEG进行认知负荷评估。

Abstract: Understanding the interaction of neural and cardiac systems during cognitive activity is critical to advancing physiological computing. Although EEG has been the gold standard for assessing mental workload, its limited portability restricts its real-world use. Widely available ECG through wearable devices proposes a pragmatic alternative. This research investigates whether ECG signals can reliably reflect cognitive load and serve as proxies for EEG-based indicators. In this work, we present multimodal data acquired from two different paradigms involving working-memory and passive-listening tasks. For each modality, we extracted ECG time-domain HRV metrics and Catch22 descriptors against EEG spectral and Catch22 features, respectively. We propose a cross-modal XGBoost framework to project the ECG features onto EEG-representative cognitive spaces, thereby allowing workload inferences using only ECG. Our results show that ECG-derived projections expressively capture variation in cognitive states and provide good support for accurate classification. Our findings underpin ECG as an interpretable, real-time, wearable solution for everyday cognitive monitoring.

</details>


### [95] [Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models](https://arxiv.org/abs/2601.01452)
*Jian Feng,Zhihong Huang*

Main category: cs.LG

TL;DR: BSZO是一种贝叶斯子空间零阶优化方法，通过卡尔曼滤波结合多个扰动方向的有限差分信息，相比传统ZO方法提高了收敛速度，在保持接近推理基线内存使用的同时，在多个LLM上取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零阶优化方法依赖于单步随机扰动的梯度估计，这限制了优化效率和收敛速度。需要一种能够更有效地利用有限差分信息的方法来改进大型语言模型的微调效果。

Method: 提出贝叶斯子空间零阶优化（BSZO），将每个有限差分测量视为噪声观测，通过卡尔曼滤波在多个扰动方向上构建投影梯度的后验分布，并使用基于残差的自适应机制调整扰动尺度。

Result: 理论分析显示BSZO相比标准ZO方法收敛速度提高了k/γ倍。在RoBERTa、Mistral和OPT模型上的实验表明，BSZO优于MeZO、MeZO-Adam和HiZOO，在OPT-13B上实现了最高6.67%的绝对平均改进，同时内存使用保持在推理基线的1.00-1.08倍。

Conclusion: BSZO通过贝叶斯方法有效整合多个扰动方向的信息，显著提高了零阶优化的收敛速度和性能，为大型语言模型的高效微调提供了一种内存友好的解决方案。

Abstract: Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations, but existing methods rely on one-step gradient estimates from random perturbations. We introduce Bayesian Subspace Zeroth-Order optimization (BSZO), a ZO optimizer that applies Kalman filtering to combine finite-difference information across multiple perturbation directions. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adjust perturbation scales. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/γ$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms MeZO, MeZO-Adam, and HiZOO across various tasks, achieving up to 6.67\% absolute average improvement on OPT-13B while keeping memory usage close to inference-only baselines (1.00$\times$--1.08$\times$ of MeZO).

</details>


### [96] [Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD](https://arxiv.org/abs/2601.01465)
*Ze Peng,Jian Zhang,Yisen Wang,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.LG

TL;DR: 该论文提出了一种新的信息论泛化界，更好地利用了SGD的平坦性偏好，能够反映平坦性改善时的泛化提升，并在数值上更紧致。


<details>
  <summary>Details</summary>
Motivation: 现有信息论泛化界虽然理论上依赖于数据和算法特性，但未能有效捕捉SGD平坦性偏好对泛化的改善作用，且在数值上较为宽松。

Method: 提出"全知轨迹"技术，推导出更充分利用平坦性偏好的信息论泛化界，该界表明当最终权重协方差的大方差方向在损失景观中具有小局部曲率时，模型泛化更好。

Result: 在深度神经网络上的实验表明，新界不仅正确反映了平坦性改善时的泛化提升，数值上也更紧致；应用于凸-Lipschitz-有界问题的梯度下降极小化超额风险时，将代表性信息论界的Ω(1)速率改进为O(1/√n)。

Conclusion: 新提出的信息论泛化界成功利用了SGD的平坦性偏好，能够更准确地刻画平坦性对泛化的影响，并暗示了可以绕过记忆-泛化权衡。

Abstract: Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD's generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called "omniscient trajectory". When applied to Gradient Descent's minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds' $Ω(1)$ rates to $O(1/\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs.

</details>


### [97] [Accelerating Storage-Based Training for Graph Neural Networks](https://arxiv.org/abs/2601.01473)
*Myung-Hwan Jang,Jeong-Min Park,Yunyong Ko,Sang-Wook Kim*

Main category: cs.LG

TL;DR: AGNES是一个基于存储的GNN训练框架，通过块状存储I/O处理和超批次处理技术，解决了大规模图神经网络训练中的I/O瓶颈问题，相比现有方法提速最高达4.1倍。


<details>
  <summary>Details</summary>
Motivation: 现有基于存储的GNN训练方法在处理大规模图时面临严重的数据准备瓶颈，主要问题在于如何处理大量的小规模存储I/O操作，这限制了存储设备的带宽利用率。

Method: AGNES采用块状存储I/O处理技术来充分利用高性能存储设备的I/O带宽，同时基于真实图特性设计了超批次处理策略，进一步提升了每个存储I/O的效率。

Result: 在五个真实世界图数据集上的综合实验表明，AGNES始终优于四种最先进的方法，比最佳竞争对手快达4.1倍。

Conclusion: AGNES通过创新的存储I/O处理策略有效解决了大规模GNN训练中的I/O瓶颈问题，为单机处理网络规模图提供了高效的解决方案。

Abstract: Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, \textit{a storage-based approach to GNN training} has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: \textit{how to handle a large number of small storage I/Os}. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named \textsf{AGNES}, that employs a method of \textit{block-wise storage I/O processing} to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, \textsf{AGNES} employs a simple yet effective strategy, \textit{hyperbatch-based processing} based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that \textsf{AGNES} consistently outperforms four state-of-the-art methods, by up to 4.1$\times$ faster than the best competitor. Our code is available at https://github.com/Bigdasgit/agnes-kdd26.

</details>


### [98] [Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts](https://arxiv.org/abs/2601.01475)
*Ruofeng Yang,Yongcan Li,Bo Jiang,Cheng Chen,Shuai Li*

Main category: cs.LG

TL;DR: 本文提出MoLR-MoG建模方法，将数据建模为K个线性子空间的并集，每个子空间采用混合高斯潜变量，解决了传统扩散模型在高维数据中的维度诅咒问题，并实现了更优的生成效果。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在高维数据中存在维度诅咒问题（n^{-1/D}），且现有方法虽然考虑了数据的多流形特性，但使用高斯潜变量无法捕捉潜流形的多模态特性。

Method: 提出混合子空间低秩混合高斯（MoLR-MoG）建模：将目标数据建模为K个线性子空间的并集，每个子空间采用混合高斯潜变量（n_k个模态，维度d_k）。对应的得分函数具有混合专家（MoE）结构，能捕捉多模态信息并包含非线性特性。

Result: 1. 实验表明MoE潜变量MoG神经网络的生成结果远优于MoE潜变量高斯得分；2. MoE潜变量MoG神经网络与参数多10倍的MoE潜变量Unet性能相当；3. 理论分析得到R^4√(Σn_k)√(Σn_kd_k)/√n的估计误差，摆脱了维度诅咒；4. 证明了在MoLR-MoG建模下的优化收敛保证。

Conclusion: MoLR-MoG建模能有效捕捉真实世界数据的多模态特性，解释了为什么扩散模型只需少量训练样本和快速优化过程就能获得优异性能，为扩散模型的理论理解提供了新视角。

Abstract: Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. However, the estimation error of diffusion models suffers from the curse of dimensionality $n^{-1/D}$ with the data dimension $D$. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear property. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \times$ parameters. These results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\sqrt{Σ_{k=1}^Kn_k}\sqrt{Σ_{k=1}^Kn_kd_k}/\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.

</details>


### [99] [SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines](https://arxiv.org/abs/2601.01484)
*Itai Morad,Nir Shlezinger,Yonina C. Eldar*

Main category: cs.LG

TL;DR: 本文从贝叶斯视角分析知识蒸馏，证明从贝叶斯分类概率学习能降低方差、提升收敛稳定性，并实验验证贝叶斯教师模型能让学生获得更高准确率和更稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 知识蒸馏在应用中表现出色，但其理论基础尚未完全理解。本文旨在从贝叶斯角度严格分析知识蒸馏的收敛行为，特别是当教师提供贝叶斯分类概率时的影响。

Method: 采用贝叶斯视角分析知识蒸馏，研究两种监督模式：1）教师提供精确的贝叶斯分类概率；2）监督使用带噪声的贝叶斯分类概率近似。分析随机梯度下降的收敛行为，并实验验证贝叶斯深度学习模型作为教师的效果。

Result: 分析表明：从贝叶斯分类概率学习能实现方差降低，相比one-hot监督能消除收敛边界中的邻域项。噪声水平影响泛化和准确性。实验显示：从贝叶斯教师蒸馏的学生不仅准确率更高（最高+4.27%），而且收敛更稳定（噪声减少达30%）。

Conclusion: 贝叶斯视角为知识蒸馏提供了理论理解，证明贝叶斯分类概率能改善学生模型的收敛和泛化。建议使用贝叶斯深度学习模型作为教师，因其能提供更好的贝叶斯分类概率估计，从而提升知识蒸馏效果。

Abstract: Knowledge Distillation (KD) is a central paradigm for transferring knowledge from a large teacher network to a typically smaller student model, often by leveraging soft probabilistic outputs. While KD has shown strong empirical success in numerous applications, its theoretical underpinnings remain only partially understood. In this work, we adopt a Bayesian perspective on KD to rigorously analyze the convergence behavior of students trained with Stochastic Gradient Descent (SGD). We study two regimes: $(i)$ when the teacher provides the exact Bayes Class Probabilities (BCPs); and $(ii)$ supervision with noisy approximations of the BCPs. Our analysis shows that learning from BCPs yields variance reduction and removes neighborhood terms in the convergence bounds compared to one-hot supervision. We further characterize how the level of noise affects generalization and accuracy. Motivated by these insights, we advocate the use of Bayesian deep learning models, which typically provide improved estimates of the BCPs, as teachers in KD. Consistent with our analysis, we experimentally demonstrate that students distilled from Bayesian teachers not only achieve higher accuracies (up to +4.27%), but also exhibit more stable convergence (up to 30% less noise), compared to students distilled from deterministic teachers.

</details>


### [100] [Accelerating Decentralized Optimization via Overlapping Local Steps](https://arxiv.org/abs/2601.01493)
*Yijie Zhou,Shi Pu*

Main category: cs.LG

TL;DR: OLDSGD通过计算-通信重叠加速去中心化训练，减少网络空闲时间，保持与Local SGD相同的平均更新，同时提升实际训练速度。


<details>
  <summary>Details</summary>
Motivation: 去中心化优化在分布式学习中很重要，但现有方法因频繁节点同步而面临通信瓶颈，导致训练效率低下。

Method: 提出重叠本地去中心化SGD（OLDSGD），通过精心设计的更新机制实现计算和通信重叠，避免通信引起的停滞，同时保持与Local SGD相同的平均更新。

Result: 理论证明OLDSGD在光滑非凸目标上具有非渐近收敛率，保持与标准Local Decentralized SGD相同的迭代复杂度，但显著减少每轮迭代的实际运行时间。实验显示在不同通信延迟下都能提升实际收敛速度。

Conclusion: OLDSGD通过对现有框架的最小修改，提供了一种不牺牲理论保证的实用解决方案，能显著加速去中心化学习。

Abstract: Decentralized optimization has emerged as a critical paradigm for distributed learning, enabling scalable training while preserving data privacy through peer-to-peer collaboration. However, existing methods often suffer from communication bottlenecks due to frequent synchronization between nodes. We present Overlapping Local Decentralized SGD (OLDSGD), a novel approach to accelerate decentralized training by computation-communication overlapping, significantly reducing network idle time. With a deliberately designed update, OLDSGD preserves the same average update as Local SGD while avoiding communication-induced stalls. Theoretically, we establish non-asymptotic convergence rates for smooth non-convex objectives, showing that OLDSGD retains the same iteration complexity as standard Local Decentralized SGD while improving per-iteration runtime. Empirical results demonstrate OLDSGD's consistent improvements in wall-clock time convergence under different levels of communication delays. With minimal modifications to existing frameworks, OLDSGD offers a practical solution for faster decentralized learning without sacrificing theoretical guarantees.

</details>


### [101] [Advanced Global Wildfire Activity Modeling with Hierarchical Graph ODE](https://arxiv.org/abs/2601.01501)
*Fan Xu,Wei Gong,Hao Wu,Lilan Peng,Nan Wang,Qingsong Wen,Xian Wu,Kun Wang,Xibin Zhao*

Main category: cs.LG

TL;DR: HiGO：一种用于全球野火预测的多尺度连续时间图神经网络框架，通过分层图ODE建模地球系统动态，在长期预测上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 野火是地球系统的重要组成部分，受大气、海洋和陆地过程的多尺度相互作用影响。虽然深度学习在天气预报方面取得了突破，但在全球野火行为预测方面的潜力尚未充分探索。现有方法难以有效捕捉野火的多尺度连续时间动态特性。

Method: 提出分层图ODE（HiGO）框架：1）将地球系统表示为多层图层次结构；2）设计自适应滤波消息传递机制，实现层内和层间信息流；3）在多个层次集成GNN参数化的神经ODE模块，显式学习每个尺度的连续动态。

Result: 在SeasFire Cube数据集上的实验表明，HiGO在长期野火预测任务上显著优于现有最先进基线方法。其连续时间预测展现出强大的观测一致性，显示了实际应用的潜力。

Conclusion: HiGO通过多尺度连续时间建模成功解决了全球野火预测的挑战，为理解地球系统中野火的复杂动态提供了新框架，具有重要的实际应用价值。

Abstract: Wildfires, as an integral component of the Earth system, are governed by a complex interplay of atmospheric, oceanic, and terrestrial processes spanning a vast range of spatiotemporal scales. Modeling their global activity on large timescales is therefore a critical yet challenging task. While deep learning has recently achieved significant breakthroughs in global weather forecasting, its potential for global wildfire behavior prediction remains underexplored. In this work, we reframe this problem and introduce the Hierarchical Graph ODE (HiGO), a novel framework designed to learn the multi-scale, continuous-time dynamics of wildfires. Specifically, we represent the Earth system as a multi-level graph hierarchy and propose an adaptive filtering message passing mechanism for both intra- and inter-level information flow, enabling more effective feature extraction and fusion. Furthermore, we incorporate GNN-parameterized Neural ODE modules at multiple levels to explicitly learn the continuous dynamics inherent to each scale. Through extensive experiments on the SeasFire Cube dataset, we demonstrate that HiGO significantly outperforms state-of-the-art baselines on long-range wildfire forecasting. Moreover, its continuous-time predictions exhibit strong observational consistency, highlighting its potential for real-world applications.

</details>


### [102] [Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings](https://arxiv.org/abs/2601.01558)
*Pengfei Qu,Wenyu Ouyang,Chi Zhang,Yikai Chai,Shuolong Xu,Lei Ye,Yongri Piao,Miao Zhang,Huchuan Lu*

Main category: cs.LG

TL;DR: 卫星图像学习的环境嵌入比传统流域属性更能有效预测无径流记录地区的河流流量


<details>
  <summary>Details</summary>
Motivation: 预测无径流记录地区的河流流量具有挑战性，因为流域对气候、地形、植被和土壤的响应各不相同。传统流域属性无法完全代表自然环境的复杂性。

Method: 使用AlphaEarth Foundation嵌入（从大量卫星图像中学习而非专家设计）来描述流域特征，这些嵌入总结了植被、地表特性和长期环境动态的模式。研究还探讨了如何选择适当的捐赠流域来影响无测站地区的预测。

Result: 使用嵌入的模型在未用于训练的流域中实现了更高的预测精度，表明它们比传统属性更有效地捕捉了关键的物理差异。基于嵌入的相似性有助于识别具有可比环境和水文行为的流域，提高性能，而添加许多不相似的流域则会降低准确性。

Conclusion: 卫星信息化的环境表征可以加强水文预报，支持开发更容易适应不同景观的模型。

Abstract: Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.

</details>


### [103] [The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs](https://arxiv.org/abs/2601.01580)
*Zibo Zhao,Yuanting Zha,Haipeng Zhang,Xingcheng Xu*

Main category: cs.LG

TL;DR: 本文通过梯度归因属性分析RL后训练如何使大语言模型产生自我反思能力，提出两阶段决策-采样假设，解释了RL相比SFT在自我修正方面的优势机制。


<details>
  <summary>Details</summary>
Motivation: 尽管RL后训练能让大语言模型产生自我反思能力，但统一的优化目标如何同时产生生成解决方案和评估何时修订这两种不同功能，其机制尚不明确。

Method: 引入梯度归因属性来分析奖励梯度在策略组件中的分布，形式化为两阶段决策-采样假设，将策略分解为用于生成的采样策略和用于验证的决策策略。

Result: 理论证明替代奖励表现出平衡梯度归因，而SFT和KL惩罚表现出不平衡梯度归因，长度加权创建了不对称正则化，约束采样策略而让决策策略优化不足。

Conclusion: RL的优越泛化能力主要来自改进的决策能力而非采样能力，这为思维模型中的自我修正提供了基于第一性原理的机制解释。

Abstract: Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.

</details>


### [104] [REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training](https://arxiv.org/abs/2601.01605)
*Xin Di,Xinglin Piao,Fei Wang,Guodong Jing,Yong Zhang*

Main category: cs.LG

TL;DR: 提出REE-TTT模型，通过时空测试时训练机制改进雷达回波外推，增强跨区域极端降水预测的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的雷达回波外推方法依赖高质量本地训练数据和静态参数，导致泛化能力差，难以适应不同区域和极端天气事件

Method: 提出REE-TTT模型，引入自适应测试时训练机制，设计时空测试时训练块，用任务特定注意力机制替代标准线性投影，增强对非平稳气象分布的适应能力

Result: 在跨区域极端降水场景实验中，REE-TTT在预测精度和泛化能力上显著优于现有基准模型，对数据分布偏移表现出卓越适应性

Conclusion: REE-TTT通过测试时训练机制有效解决了雷达回波外推的泛化问题，为降水临近预报提供了更稳健的解决方案

Abstract: Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.

</details>


### [105] [Communication-Efficient Federated AUC Maximization with Cyclic Client Participation](https://arxiv.org/abs/2601.01649)
*Umesh Vangapally,Wenhan Wu,Chen Chen,Zhishuai Guo*

Main category: cs.LG

TL;DR: 该论文提出了针对周期性客户端参与场景的高效联邦AUC最大化算法，在平方替代损失下达到最优通信复杂度，并在一般成对损失下建立了理论保证。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦AUC最大化方法通常假设所有客户端都可用，但这在实际联邦学习系统中不现实。真实场景中客户端通常按固定循环时间表参与训练，这种周期性参与给非可分解的AUC目标带来了独特的优化挑战。

Method: 研究了两种设置：1) 使用平方替代损失的AUC最大化，将其重新表述为非凸-强凹极小极大优化问题，利用Polyak-Łojasiewicz条件；2) 一般成对AUC损失。为这两种设置开发了通信高效的算法。

Result: 在平方替代损失下实现了最优通信复杂度$\widetilde{O}(1/ε^{1/2})$和迭代复杂度$\widetilde{O}(1/ε)$；在一般成对损失下建立了$O(1/ε^3)$通信复杂度和$O(1/ε^4)$迭代复杂度，在PL条件下可提升至$\widetilde{O}(1/ε^{1/2})$和$\widetilde{O}(1/ε)$。

Conclusion: 提出的方法在图像分类、医学影像和欺诈检测等基准任务上表现出优越的效率和效果，为周期性客户端参与场景下的联邦AUC最大化提供了有效的解决方案。

Abstract: Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-Łojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/ε^3)$ and an iteration complexity of $O(1/ε^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.

</details>


### [106] [Learning Resilient Elections with Adversarial GNNs](https://arxiv.org/abs/2601.01653)
*Hao Xiang Li,Yash Shah,Lorenzo Giusti*

Main category: cs.LG

TL;DR: 本文提出一种基于图神经网络和对抗训练的投票规则学习方法，通过二部图表示选举，提高投票规则的鲁棒性和社会福利。


<details>
  <summary>Details</summary>
Motivation: 选举在现代民主、市场调节和推荐系统中至关重要，但设计满足所有场景的通用投票规则仍具挑战性。现有基于集合不变架构的学习方法在应用到真实世界时面临鲁棒性不足等问题。

Method: 使用二部图表示选举，采用图神经网络学习投票规则，结合对抗训练提高对策略性投票的鲁棒性，同时最大化社会福利。

Result: 在合成和真实数据集上评估了方法的有效性，解决了先前工作在学习投票规则方面的关键限制，为机器学习应用于真实选举开辟了新前沿。

Conclusion: 该方法通过改进神经网络架构和对抗训练，提高了投票规则的表达能力和鲁棒性，为机器学习在真实世界选举中的应用提供了新途径。

Abstract: In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.

</details>


### [107] [Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths](https://arxiv.org/abs/2601.01663)
*He Sun,Jiwoong Shin,Ravi Dhar*

Main category: cs.LG

TL;DR: 提出长度感知采样(LAS)方法，通过按长度分组轨迹来减少批次内长度异质性，改善轨迹生成模型的分布匹配性能


<details>
  <summary>Details</summary>
Motivation: 研究可变长度轨迹的生成建模，用于下游仿真和反事实分析。标准小批量训练在轨迹长度高度异质时不稳定，这会降低轨迹派生统计量的分布匹配质量

Method: 提出长度感知采样(LAS)：按轨迹长度分组，从单一长度桶中采样批次，减少批次内长度异质性。将LAS集成到具有辅助时间对齐损失的条件下轨迹GAN中

Result: 在温和有界假设下提供派生变量的分布级保证；通过IPM/Wasserstein机制解释LAS如何通过消除仅长度捷径批评器来改善分布匹配。在多商场购物者轨迹和多样公共序列数据集上，LAS在派生变量分布匹配方面持续优于随机采样

Conclusion: LAS是一种简单的批处理策略，无需改变模型类别即可减少批次内长度异质性，显著改善轨迹生成模型的分布匹配性能，适用于多种实际应用场景

Abstract: We study generative modeling of \emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \emph{distribution matching} for trajectory-derived statistics. We propose \textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.

</details>


### [108] [Who is the Winning Algorithm? Rank Aggregation for Comparative Studies](https://arxiv.org/abs/2601.01664)
*Amichai Painsky*

Main category: cs.LG

TL;DR: 提出新框架利用完整排名信息估计机器学习算法在未见数据集上的获胜概率，相比仅统计获胜次数的最大似然方法有显著改进


<details>
  <summary>Details</summary>
Motivation: 现有方法仅统计算法获胜次数，忽略了完整排名信息（如第二名、第三名等），而这些信息可能对预测未来数据集上的表现更有价值

Method: 引入新的概念框架，利用算法在基准数据集上的完整排名信息来估计每个算法在未见数据集上的获胜概率

Result: 在合成和真实世界示例中，所提框架显著优于现有已知方法

Conclusion: 利用完整排名信息而非仅获胜次数能更准确地估计机器学习算法在未来数据集上的获胜概率

Abstract: Consider a collection of m competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to ``win'' (rank highest) on a future, unseen dataset. The standard maximum likelihood approach suggests counting the number of wins per each algorithm. In this work, we argue that there is much more information in the complete rankings. That is, the number of times that each algorithm finished second, third and so forth. Yet, it is not entirely clear how to effectively utilize this information for our purpose. In this work we introduce a novel conceptual framework for estimating the win probability for each of the m algorithms, given their complete rankings over a benchmark of datasets. Our proposed framework significantly improves upon currently known methods in synthetic and real-world examples.

</details>


### [109] [Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives](https://arxiv.org/abs/2601.01665)
*Wei Liu,Yaoxin Wu,Yingqian Zhang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: 提出一个面向鲁棒性的偏好条件深度强化学习求解器框架，用于多目标组合优化问题，包含偏好对抗攻击生成困难实例和硬度感知偏好选择的防御策略。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的多目标组合优化问题求解器的鲁棒性研究不足，特别是在多样复杂问题分布下的表现需要深入探索。

Method: 1) 提出统一鲁棒性框架；2) 开发偏好对抗攻击生成困难实例；3) 引入硬度感知偏好选择的防御策略进行对抗训练。

Result: 攻击方法成功为不同求解器生成困难实例，防御方法显著增强神经求解器的鲁棒性和泛化能力，在困难或分布外实例上表现优异。

Conclusion: 该框架有效评估和提升多目标组合优化问题中学习型求解器的鲁棒性，为实际应用提供了更可靠的解决方案。

Abstract: Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.

</details>


### [110] [HeurekaBench: A Benchmarking Framework for AI Co-scientist](https://arxiv.org/abs/2601.01678)
*Siba Smarak Panigrahi,Jovana Videnović,Maria Brbić*

Main category: cs.LG

TL;DR: HeurekaBench是一个用于评估科学代理系统的基准框架，通过半自动化流程从真实科学研究中创建开放式研究问题，并在单细胞生物学领域实例化为sc-HeurekaBench，用于分析代理系统设计选择。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM驱动的科学代理系统面临挑战，需要真实的端到端研究场景来整合数据分析、解释和从实验数据中生成新见解。现有基准缺乏这种现实性和综合性。

Method: 开发了一个半自动化管道，利用多个LLM从科学研究和对应代码仓库中提取见解并生成候选工作流，然后根据报告的研究发现进行验证，创建基于真实科学研究的开放式研究问题基准。

Result: 在单细胞生物学领域实例化为sc-HeurekaBench，用于比较最先进的单细胞代理。发现添加批评模块可以将开源LLM代理的不良响应改善高达22%，缩小与闭源代理的差距。

Conclusion: HeurekaBench为科学代理的严格端到端评估设定了路径，将基准构建基于真实的科学工作流程，有助于系统分析当前代理系统设计选择。

Abstract: LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.

</details>


### [111] [DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors](https://arxiv.org/abs/2601.01688)
*Yash Thesia,Meera Suthar*

Main category: cs.LG

TL;DR: DiMEx利用预训练潜在扩散模型的语义先验，通过潜在空间贝叶斯优化绕过数据自由模型窃取中的"冷启动"问题，显著提升攻击效率；同时提出混合状态集成防御来检测这种攻击的时间特征。


<details>
  <summary>Details</summary>
Motivation: 数据自由模型提取(DFME)攻击面临"冷启动"问题：基于GAN的攻击者需要数千次查询才能从随机噪声收敛到有意义的数据，这限制了攻击效率。需要一种能立即生成高质量查询的方法来绕过这一初始化障碍。

Method: 提出DiMEx框架：利用预训练潜在扩散模型的丰富语义先验，在生成器的潜在空间中采用随机嵌入贝叶斯优化(REMBO)，立即合成高保真查询，绕过冷启动问题。同时提出混合状态集成(HSE)防御，通过识别潜在空间攻击的独特"优化轨迹"来检测攻击。

Result: DiMEx在SVHN数据集上仅用2000次查询就达到52.1%的协议率，比最先进的GAN基线高出16%以上。HSE防御能够将攻击成功率抑制到21.6%，且延迟可忽略不计，同时能有效检测静态分布检测器无法发现的DiMEx攻击。

Conclusion: DiMEx通过利用预训练扩散模型的语义先验，显著提升了数据自由模型窃取的效率，但同时也暴露了新的攻击特征。提出的HSE防御能够有效检测这种基于潜在空间的攻击，为MLaaS安全提供了新的防御思路。

Abstract: Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the "Cold Start" problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator's latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique "optimization trajectory" of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.

</details>


### [112] [Enhanced Multi-model Online Conformal Prediction](https://arxiv.org/abs/2601.01692)
*Erfan Hajihashemi,Yanning Shen*

Main category: cs.LG

TL;DR: 提出一种新颖的多模型在线共形预测算法，通过二分图选择有效模型子集，降低计算复杂度并提升预测效率


<details>
  <summary>Details</summary>
Motivation: 传统共形预测依赖单一固定模型，在在线环境中可能表现不稳定；现有多模型选择方法计算成本高，且候选模型中的低效模型会影响整体性能

Method: 在每个时间步生成二分图来识别有效模型子集，从中选择模型构建预测集，减少计算复杂度

Result: 实验表明该方法在预测集大小和计算效率方面均优于现有的多模型共形预测技术

Conclusion: 提出的多模型在线共形预测算法能有效提升预测效率并降低计算成本，解决了传统方法的局限性

Abstract: Conformal prediction is a framework for uncertainty quantification that constructs prediction sets for previously unseen data, guaranteeing coverage of the true label with a specified probability. However, the efficiency of these prediction sets, measured by their size, depends on the choice of the underlying learning model. Relying on a single fixed model may lead to suboptimal performance in online environments, as a single model may not consistently perform well across all time steps. To mitigate this, prior work has explored selecting a model from a set of candidates. However, this approach becomes computationally expensive as the number of candidate models increases. Moreover, poorly performing models in the set may also hinder the effectiveness. To tackle this challenge, this work develops a novel multi-model online conformal prediction algorithm that reduces computational complexity and improves prediction efficiency. At each time step, a bipartite graph is generated to identify a subset of effective models, from which a model is selected to construct the prediction set. Experiments demonstrate that our method outperforms existing multi-model conformal prediction techniques in terms of both prediction set size and computational efficiency.

</details>


### [113] [Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT](https://arxiv.org/abs/2601.01701)
*Mohammed Ayalew Belay,Adil Rasheed,Pierluigi Salvo Rossi*

Main category: cs.LG

TL;DR: 提出数字孪生集成联邦学习(DTFL)方法，通过五种新颖方法解决工业异常检测中的数据隐私、通信效率和标签数据有限等问题，显著提升通信效率。


<details>
  <summary>Details</summary>
Motivation: 工业系统异常检测对安全、可靠性和效率至关重要，但现有统计和机器学习方法面临依赖真实传感器数据、标签数据有限、高误报率和隐私问题等挑战。

Method: 提出数字孪生集成联邦学习(DTFL)方法，包括五种新方法：数字孪生元学习(DTML)、联邦参数融合(FPF)、分层参数交换(LPE)、循环权重适应(CWA)和数字孪生知识蒸馏(DTKd)。这些方法结合合成和真实世界知识，平衡泛化能力和通信开销。

Result: 在公开的赛博物理异常检测数据集上实验，目标准确率80%时，CWA在33轮达到目标，FPF在41轮，LPE在48轮，DTML在87轮，而标准FedAvg和DTKd在100轮内未达到目标。CWA比DTML减少62%轮数，比LPE减少31%轮数。

Conclusion: 将数字孪生知识集成到联邦学习中，能显著加速收敛到操作上有意义的准确率阈值，为工业物联网异常检测提供通信效率高且保护隐私的解决方案。

Abstract: Anomaly detection is increasingly becoming crucial for maintaining the safety, reliability, and efficiency of industrial systems. Recently, with the advent of digital twins and data-driven decision-making, several statistical and machine-learning methods have been proposed. However, these methods face several challenges, such as dependence on only real sensor datasets, limited labeled data, high false alarm rates, and privacy concerns. To address these problems, we propose a suite of digital twin-integrated federated learning (DTFL) methods that enhance global model performance while preserving data privacy and communication efficiency. Specifically, we present five novel approaches: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). Each method introduces a unique mechanism to combine synthetic and real-world knowledge, balancing generalization with communication overhead. We conduct an extensive experiment using a publicly available cyber-physical anomaly detection dataset. For a target accuracy of 80%, CWA reaches the target in 33 rounds, FPF in 41 rounds, LPE in 48 rounds, and DTML in 87 rounds, whereas the standard FedAvg baseline and DTKD do not reach the target within 100 rounds. These results highlight substantial communication-efficiency gains (up to 62% fewer rounds than DTML and 31% fewer than LPE) and demonstrate that integrating DT knowledge into FL accelerates convergence to operationally meaningful accuracy thresholds for IIoT anomaly detection.

</details>


### [114] [Entropy-Aligned Decoding of LMs for Better Writing and Reasoning](https://arxiv.org/abs/2601.01714)
*Kareem Ahmed,Sameer Singh*

Main category: cs.LG

TL;DR: EPIC是一种超参数自由的解码方法，通过将未来轨迹的熵纳入语言模型解码，在每一步生成时显式调节不确定性表达，使采样分布的熵与数据不确定性对齐。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型解码算法依赖贪婪启发式方法，导致短视扭曲，产生同质化、重复和不连贯的句子。需要一种能更好地对齐数据分布不确定性的解码方法。

Method: EPIC通过熵感知懒惰Gumbel-Max采样，将未来轨迹的熵纳入解码过程，显式调节每一步生成的不确定性表达，使采样分布的熵与数据不确定性对齐。该方法精确且高效，每步仅需亚线性次数的熵评估。

Result: 在创意写作和摘要任务中，EPIC在LM-as-judge偏好胜率上持续优于广泛使用的解码策略。自动指标显示EPIC产生更多样化的生成和更忠实的摘要。在数学推理任务中，EPIC也优于所有基线方法。

Conclusion: EPIC是一种有效的解码方法，通过熵感知机制更好地对齐语言模型采样分布与数据分布的不确定性，在多个任务上表现出优于现有方法的性能。

Abstract: Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.

</details>


### [115] [Context-Free Recognition with Transformers](https://arxiv.org/abs/2601.01754)
*Selim Jerad,Anej Svete,Sophie Hao,Ryan Cotterell,William Merrill*

Main category: cs.LG

TL;DR: 循环Transformer通过O(log n)循环层和O(n^6)填充标记可以识别所有上下文无关语言，但实际应用中可通过限制为无歧义CFL将填充需求降至O(n^3)。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理具有语法结构的输入（如自然语言和代码）方面表现出色，但其处理语法结构的能力尚不清楚。理论上，标准Transformer无法识别上下文无关语言（CFL），甚至无法识别正则语言。虽然已有研究显示循环层能帮助识别正则语言，但CFL识别问题仍未解决。

Method: 提出循环Transformer模型，使用O(log n)循环层和O(n^6)填充标记来识别所有CFL。进一步研究无歧义CFL子类，证明其仅需O(n^3)填充标记。通过实验验证循环机制在需要对数深度的语言上的有效性。

Result: 理论上证明循环Transformer能够识别所有CFL，但需要大量填充标记（O(n^6)）。对于无歧义CFL，填充需求降至O(n^3)。实验表明循环机制确实有助于处理需要对数深度的语言。

Conclusion: Transformer识别CFL具有复杂性：通用识别可能需要大量填充标记而不实用，但通过自然约束（如无歧义性）可以获得高效的识别算法。循环机制为Transformer处理语法结构提供了理论可能性。

Abstract: Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\mathcal{O}(\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\mathcal{O}(\log n)$ looping layers and $\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.

</details>


### [116] [UnPII: Unlearning Personally Identifiable Information with Quantifiable Exposure Risk](https://arxiv.org/abs/2601.01786)
*Intae Jeon,Yujeong Kwon,Hyungjoon Koo*

Main category: cs.LG

TL;DR: UnPII：首个基于PII风险优先级的遗忘方法，通过PII风险指数评估不同属性的隐私风险，实现差异化遗忘策略


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融、医疗等关键领域的应用引发PII隐私担忧，GDPR等法规要求删除PII数据。现有遗忘技术采用统一策略，未考虑不同PII属性的隐私风险和业务风险差异。

Method: 提出PII风险指数（PRI），综合可识别性、敏感性、可用性、可链接性、持久性、可暴露性和合规性七个风险维度。基于PRI优先级实施差异化遗忘，与Gradient Ascent、Negative Preference Optimization等现有遗忘算法无缝集成。

Result: 在合成PII数据集（1700个实例）上测试，UnPII相比基线方法在准确率提升11.8%，效用提升6.3%，泛化性提升12.4%，平均仅增加27.5%的微调开销。

Conclusion: UnPII首次实现基于PII风险的差异化遗忘，有效平衡隐私保护与模型效用，为合规性数据删除提供实用解决方案。

Abstract: The ever-increasing adoption of Large Language Models in critical sectors like finance, healthcare, and government raises privacy concerns regarding the handling of sensitive Personally Identifiable Information (PII) during training. In response, regulations such as European Union's General Data Protection Regulation (GDPR) mandate the deletion of PII upon requests, underscoring the need for reliable and cost-effective data removal solutions. Machine unlearning has emerged as a promising direction for selectively forgetting data points. However, existing unlearning techniques typically apply a uniform forgetting strategy that neither accounts for the varying privacy risks posed by different PII attributes nor reflects associated business risks. In this work, we propose UnPII, the first PII-centric unlearning approach that prioritizes forgetting based on the risk of individual or combined PII attributes. To this end, we introduce the PII risk index (PRI), a composite metric that incorporates multiple dimensions of risk factors: identifiability, sensitivity, usability, linkability, permanency, exposability, and compliancy. The PRI enables a nuanced evaluation of privacy risks associated with PII exposures and can be tailored to align with organizational privacy policies. To support realistic assessment, we systematically construct a synthetic PII dataset (e.g., 1,700 PII instances) that simulates realistic exposure scenarios. UnPII seamlessly integrates with established unlearning algorithms, such as Gradient Ascent, Negative Preference Optimization, and Direct Preference Optimization, without modifying their underlying principles. Our experimental results demonstrate that UnPII achieves the improvements of accuracy up to 11.8%, utility up to 6.3%, and generalizability up to 12.4%, respectively, while incurring a modest fine-tuning overhead of 27.5% on average during unlearning.

</details>


### [117] [HyperCLOVA X 8B Omni](https://arxiv.org/abs/2601.01792)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.LG

TL;DR: HyperCLOVA X 8B Omni是首个支持文本、音频、视觉任意输入输出的全模态模型，通过统一的多模态序列预测实现跨模态理解与生成。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型通常采用分离的模态特定管道，缺乏统一的任意到任意模态转换能力。作者旨在开发一个统一的8B规模全模态助手，支持文本、音频、视觉的任意组合输入输出。

Method: 模型通过共享的下一个token预测接口统一处理交错的多模态序列，使用视觉和音频编码器注入连续嵌入以实现细粒度理解和基础。将多模态理解和生成整合到单一模型中，而非分离的模态特定管道。

Result: 实证评估显示，在韩语和英语的文本、音频、视觉多种输入输出组合上，模型性能与同等规模模型相比具有竞争力。

Conclusion: HyperCLOVA X 8B Omni作为8B规模的全模态路径探索点，其开源权重发布将支持广泛的研究和部署场景，推动实用任意到任意全模态助手的发展。

Abstract: In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.

</details>


### [118] [Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving](https://arxiv.org/abs/2601.01800)
*Qi Wei,Junchao Fan,Zhao Yang,Jianhua Wang,Jingkai Mao,Xiaolin Chang*

Main category: cs.LG

TL;DR: CARRL提出了一种针对自动驾驶的鲁棒强化学习方法，通过关键性感知的对抗训练来处理稀疏的安全关键风险，相比现有方法至少降低22.66%的碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习在自动驾驶中存在对扰动的脆弱性，而传统对抗训练方法存在两个主要问题：1）将交互建模为零和博弈，忽略了智能体与对手之间的固有不对称性；2）未能反映安全关键风险的稀疏性，导致在实际自动驾驶场景中鲁棒性不足。

Method: CARRL包含两个交互组件：风险暴露对手（REA）和风险目标鲁棒智能体（RTRA）。1）将REA与RTRA的交互建模为一般和博弈，允许REA专注于暴露安全关键故障；2）REA采用解耦优化机制，在约束预算下更好地识别和利用稀疏的安全关键时刻；3）RTRA通过双回放缓冲区联合利用良性经验和对抗经验，并在扰动下强制执行策略一致性以稳定行为。

Result: 实验结果表明，与最先进的基线方法相比，CARRL在所有情况下至少降低了22.66%的碰撞率。

Conclusion: CARRL通过关键性感知的对抗训练方法，有效解决了自动驾驶中稀疏安全关键风险的鲁棒性问题，显著提高了自动驾驶策略的安全性和实用性。

Abstract: Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\% across all cases compared to state-of-the-art baseline methods.

</details>


### [119] [Moments Matter:Stabilizing Policy Optimization using Return Distributions](https://arxiv.org/abs/2601.01803)
*Dennis Jabs,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: 提出一种基于分布评论家高阶矩的PPO改进方法，通过惩罚极端回报分布来提升策略稳定性


<details>
  <summary>Details</summary>
Motivation: 深度强化学习智能体常学习到相同累积回报但行为差异很大的策略，这源于环境和算法因素。在连续控制任务中，即使小的参数变化也会产生不稳定步态，这既影响算法比较也影响现实世界迁移。先前研究表明，当策略更新穿越噪声邻域时会出现这种不稳定性，而更新后回报分布R(θ)的扩散是衡量这种噪声的有用指标。虽然显式约束策略以保持窄的R(θ)可以改善稳定性，但在高维设置中直接估计R(θ)计算成本很高。

Method: 提出利用环境随机性来减轻更新引起的变异性的替代方法。具体来说，通过分布评论家建模状态-动作回报分布，然后使用该分布的高阶矩（偏度和峰度）来偏置PPO的优势函数。通过惩罚极端尾部行为，该方法阻止策略进入容易不稳定的参数区域。

Result: 在Walker2D环境中，该方法将R(θ)变窄，稳定性提升高达75%，同时保持可比较的评估回报。假设在更新后评论家值与更新后回报对齐不佳的环境中，标准PPO难以产生窄的R(θ)，而本文的基于矩的修正能够缩小R(θ)。

Conclusion: 通过分布评论家的高阶矩偏置PPO优势函数，可以有效提升策略稳定性，减少更新引起的变异性，同时不牺牲性能表现。

Abstract: Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.

</details>


### [120] [RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data](https://arxiv.org/abs/2601.01829)
*Peiyan Hu,Haodong Feng,Hongyuan Liu,Tongtong Yan,Wenhao Deng,Tianrun Gao,Rong Zheng,Haoren Zheng,Chenglei Yu,Chuanrui Wang,Kaiwen Li,Zhi-Ming Ma,Dezhi Zhou,Xingcai Lu,Dixia Fan,Tailin Wu*

Main category: cs.LG

TL;DR: RealPDEBench：首个整合真实世界测量与配对数值模拟的科学机器学习基准，包含5个数据集、3个任务、8个指标和10个基线模型，旨在弥合模拟与真实数据之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前科学机器学习模型面临的关键瓶颈是缺乏昂贵的真实世界数据，导致大多数模型只能在模拟数据上训练和验证，这限制了科学ML的发展、评估以及模拟到真实迁移等关键任务的研究。

Method: 构建包含5个真实世界测量数据集及其配对模拟数据集的基准，定义3个任务（允许真实与模拟数据比较），设计8个评估指标（涵盖数据导向和物理导向），并评估10个代表性基线模型（包括SOTA模型、预训练PDE基础模型和传统方法）。

Result: 实验显示模拟数据与真实世界数据之间存在显著差异，同时表明使用模拟数据进行预训练能持续提高准确性和收敛性。

Conclusion: RealPDEBench通过提供真实世界数据的洞察，推动科学机器学习弥合模拟与真实差距并实现实际部署，为相关研究提供了首个整合真实测量与模拟的基准框架。

Abstract: Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.

</details>


### [121] [FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks](https://arxiv.org/abs/2601.01833)
*Chenyu Hu,Qiming Hu,Sinan Chen,Nianyu Li,Mingyue Zhang,Jialong Li*

Main category: cs.LG

TL;DR: FAROS：一种增强的联邦学习框架，通过自适应差分缩放和鲁棒核心集计算来防御后门攻击，在攻击成功率和主任务准确性方面优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的后门攻击威胁严重，现有防御方法依赖固定参数，存在单点故障风险，难以应对复杂攻击者策略。

Method: 提出FAROS框架，包含自适应差分缩放（ADS）和鲁棒核心集计算（RCC）。ADS根据客户端上传梯度的离散度动态调整防御灵敏度；RCC通过计算最高置信度客户端核心集的质心来缓解单点故障风险。

Result: 在多个数据集、模型和攻击场景下的实验表明，该方法在攻击成功率和主任务准确性方面均优于当前防御方法。

Conclusion: FAROS通过动态自适应机制和鲁棒核心集计算，有效提升了联邦学习对后门攻击的防御能力，解决了现有防御方法的局限性。

Abstract: Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.

</details>


### [122] [Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack](https://arxiv.org/abs/2601.01840)
*Qiantao Yang,Liquan Chen,Mingfu Xue,Songze Li*

Main category: cs.LG

TL;DR: FedCSPACK：基于余弦稀疏化参数打包和双权重聚合的个性化联邦学习方法，有效解决数据异构性同时适应客户端有限资源


<details>
  <summary>Details</summary>
Motivation: 联邦学习中边缘客户端数据异构性严重影响模型性能，现有方法虽然通过模型分割和知识蒸馏增强模型兼容性，但忽略了客户端通信带宽和计算能力不足的问题，未能有效平衡处理数据异构性与适应有限客户端资源之间的矛盾

Method: 提出FedCSPACK方法：1）客户端基于余弦相似性打包模型参数并选择贡献最大的参数包共享，减少带宽需求；2）客户端基于共享参数包生成掩码矩阵，提高稀疏更新在服务器上的对齐和聚合效率；3）在掩码中嵌入方向和分布距离权重，实现加权引导聚合机制

Result: 在四个数据集上使用十种最先进方法的广泛实验表明，FedCSPACK在保持高模型精度的同时，有效提高了通信和计算效率

Conclusion: FedCSPACK通过参数打包和双权重聚合机制，成功解决了联邦学习中数据异构性与客户端资源有限之间的平衡问题，为资源受限环境下的个性化联邦学习提供了有效解决方案

Abstract: Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.

</details>


### [123] [High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation](https://arxiv.org/abs/2601.01860)
*Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: 提出基于因子分解机与二次优化退火(FMQA)的高阶上位性检测方法，将上位性检测转化为黑盒优化问题，利用MDR计算的分类错误率作为目标函数，显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 高阶上位性检测面临组合爆炸的计算挑战，传统MDR方法在基因位点数量或交互阶数增加时计算不可行，需要更高效的方法

Method: 将上位性检测定义为黑盒优化问题，采用因子分解机与二次优化退火(FMQA)算法，以MDR计算的分类错误率(CER)作为黑盒目标函数

Result: 在模拟病例对照数据集上的实验表明，该方法能在有限迭代次数内成功识别预设的高阶上位性，适用于不同交互阶数和基因位点数量

Conclusion: 提出的FMQA方法对于高阶上位性检测既有效又计算高效，为解决遗传关联研究中的组合爆炸问题提供了可行方案

Abstract: Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.

</details>


### [124] [Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance](https://arxiv.org/abs/2601.01887)
*Jiawen Zhang,Lipeng He,Kejia Chen,Jian Lou,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 仅需单个安全示例即可完全恢复安全对齐大语言模型，无需牺牲实用性，成本极低


<details>
  <summary>Details</summary>
Motivation: 微调安全对齐的大语言模型会严重损害其安全性，现有方法需要大量安全样本或校准集，导致计算开销大且模型实用性下降

Method: 发现仅需单个安全示例即可恢复安全对齐，揭示安全梯度的低秩结构，解释高效修正的可能性

Result: 该方法在5个安全对齐LLM和多个数据集上验证有效，无论有害示例数量或模型大小，都能在几个epoch内收敛

Conclusion: 安全对齐可以极低成本高效恢复，无需牺牲模型实用性，具有广泛适用性

Abstract: Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.

</details>


### [125] [FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data](https://arxiv.org/abs/2601.01901)
*Yuexuan Xia,Yinghao Zhang,Yalin Liu,Hong-Ning Dai,Yong Xia*

Main category: cs.LG

TL;DR: FedBiCross：一种个性化的单次联邦学习框架，通过聚类、双层跨集群优化和个性化蒸馏解决非IID数据下预测冲突问题


<details>
  <summary>Details</summary>
Motivation: 现有的单次联邦学习方法在非IID数据下存在预测冲突问题，平均预测会产生接近均匀分布的软标签，导致蒸馏监督信号弱

Method: 三阶段框架：1) 基于模型输出相似性聚类客户端形成连贯子集成；2) 双层跨集群优化学习自适应权重，选择性利用有益跨集群知识并抑制负迁移；3) 个性化蒸馏进行客户端特定适应

Result: 在四个医学图像数据集上的实验表明，FedBiCross在不同非IID程度下始终优于最先进的基线方法

Conclusion: FedBiCross通过聚类和选择性知识转移有效解决了非IID数据下单次联邦学习的预测冲突问题，为隐私敏感的医疗应用提供了有效的个性化解决方案

Abstract: Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.

</details>


### [126] [TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train](https://arxiv.org/abs/2601.01903)
*Ungsik Kim,Suwon Lee*

Main category: cs.LG

TL;DR: TT-FSI：利用矩阵乘积算子（MPO）高效计算忠实Shapley交互指数（FSI）的新方法，将内存需求从O(4^d)降至O(ℓd²)，时间从O(d^ℓ·2^d)降至O(ℓ²d³·2^d)，实现指数级改进。


<details>
  <summary>Details</summary>
Motivation: FSI指数是唯一满足忠实性公理的Shapley交互指数，但现有计算方法需要O(d^ℓ·2^d)时间和O(4^d)内存，计算成本过高，限制了其在实际高维问题中的应用。

Method: 利用FSI的代数结构，通过矩阵乘积算子（MPO）表示线性算子v↦FSI(v)，证明其张量列车秩为O(ℓd)，设计高效的扫描算法，实现时间和内存的指数级改进。

Result: 在6个数据集（d=8到d=20）上实验显示：相比基线加速280倍，相比SHAP-IQ加速85倍，内存减少290倍。TT-FSI可扩展到d=20（100万联盟），而所有竞争方法均失败。

Conclusion: TT-FSI通过张量列车分解和MPO表示，首次实现了高维FSI指数的高效计算，为解释性机器学习中的高阶交互分析提供了实用工具。

Abstract: The Faithful Shapley Interaction (FSI) index uniquely satisfies the faithfulness axiom among Shapley interaction indices, but computing FSI requires $O(d^\ell \cdot 2^d)$ time and existing implementations use $O(4^d)$ memory. We present TT-FSI, which exploits FSI's algebraic structure via Matrix Product Operators (MPO). Our main theoretical contribution is proving that the linear operator $v \mapsto \text{FSI}(v)$ admits an MPO representation with TT-rank $O(\ell d)$, enabling an efficient sweep algorithm with $O(\ell^2 d^3 \cdot 2^d)$ time and $O(\ell d^2)$ core storage an exponential improvement over existing methods. Experiments on six datasets ($d=8$ to $d=20$) demonstrate up to 280$\times$ speedup over baseline, 85$\times$ over SHAP-IQ, and 290$\times$ memory reduction. TT-FSI scales to $d=20$ (1M coalitions) where all competing methods fail.

</details>


### [127] [Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning](https://arxiv.org/abs/2601.01904)
*Yuxuan Li,Harshith Reddy Kethireddy,Srijita Das*

Main category: cs.LG

TL;DR: 该论文研究了强化学习中的偏好学习，重点关注特征依赖性噪声问题，发现现有噪声鲁棒方法在某些特征相关噪声场景下表现不佳，而无需显式去噪的方法反而表现更好。


<details>
  <summary>Details</summary>
Motivation: 偏好学习在强化学习中越来越受关注，但实际应用中偏好通常带有不确定性和噪声，特别是当偏好来自不完美的教师时。现有研究大多关注均匀分布的噪声检测，缺乏对与观察特征相关的噪声的研究。

Method: 提出了特征依赖性噪声的概念，并设计了多种变体：轨迹特征噪声、轨迹相似性噪声、不确定性感知噪声和语言模型噪声。在DMControl和Meta-world的复杂连续控制任务中评估这些噪声类型。

Result: 实验表明，在某些特征依赖性噪声设置下，最先进的噪声鲁棒PbRL方法的学习性能显著恶化，而没有显式去噪的PbRL方法在多数设置中反而表现更好。语言模型产生的噪声表现出与特征依赖性噪声相似的特征。

Conclusion: 特征依赖性噪声对现有噪声鲁棒方法构成挑战，需要进一步研究如何鲁棒地学习具有特征依赖性噪声的偏好。语言模型噪声模拟了现实人类行为，值得深入研究。

Abstract: Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.
  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.
  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.

</details>


### [128] [Distorted Distributional Policy Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2601.01917)
*Ryo Iwaki,Takayuki Osogami*

Main category: cs.LG

TL;DR: 本文提出了一种新的量化扭曲方法，用于解决离线分布强化学习中均匀悲观估计的局限性，通过基于数据可用性的非均匀悲观主义来提高性能。


<details>
  <summary>Details</summary>
Motivation: 虽然分布强化学习在在线场景中表现出色，但在离线场景中的成功仍然有限。作者假设现有离线DRL方法的关键限制在于它们对回报分位数进行均匀低估的方法，这种均匀悲观主义会导致过于保守的价值估计，最终阻碍泛化和性能。

Method: 引入了一个称为"量化扭曲"的新概念，通过根据支持数据的可用性调整保守程度，实现非均匀悲观主义。该方法基于理论分析，并通过实证验证。

Result: 该方法在实证验证中表现出改进的性能，优于均匀悲观主义方法。

Conclusion: 量化扭曲方法通过非均匀悲观主义解决了离线分布强化学习中均匀低估的局限性，提供了更好的泛化和性能。

Abstract: While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.

</details>


### [129] [Theoretical Convergence of SMOTE-Generated Samples](https://arxiv.org/abs/2601.01927)
*Firuz Kamalov,Hana Sulieman,Witold Pedrycz*

Main category: cs.LG

TL;DR: 该论文对SMOTE算法的收敛性进行了严格理论分析，证明了合成变量Z依概率收敛于原始变量X，在紧致条件下有更强的均值收敛，并发现更小的最近邻秩能加速收敛。


<details>
  <summary>Details</summary>
Motivation: 不平衡数据广泛影响机器学习应用，SMOTE是解决该问题最流行的方法之一。需要从理论和实证两方面验证SMOTE的有效性，特别是其收敛特性。

Method: 对SMOTE算法进行严格的理论分析，证明合成随机变量Z向原始随机变量X的收敛性质。包括依概率收敛证明，以及在X紧致条件下的均值收敛证明。同时分析最近邻秩对收敛速度的影响。

Result: 1. 证明了合成随机变量Z依概率收敛于原始随机变量X；2. 在X紧致条件下证明了更强的均值收敛；3. 发现更小的最近邻秩能带来更快的收敛速度；4. 通过真实数据和合成数据的数值实验验证了理论结果。

Conclusion: 该工作为SMOTE算法提供了理论基础，不仅增强了不平衡数据处理技术，也为更广泛的数据增强方法提供了理论支持，为实践者提供了可操作的指导。

Abstract: Imbalanced data affects a wide range of machine learning applications, from healthcare to network security. As SMOTE is one of the most popular approaches to addressing this issue, it is imperative to validate it not only empirically but also theoretically. In this paper, we provide a rigorous theoretical analysis of SMOTE's convergence properties. Concretely, we prove that the synthetic random variable Z converges in probability to the underlying random variable X. We further prove a stronger convergence in mean when X is compact. Finally, we show that lower values of the nearest neighbor rank lead to faster convergence offering actionable guidance to practitioners. The theoretical results are supported by numerical experiments using both real-life and synthetic data. Our work provides a foundational understanding that enhances data augmentation techniques beyond imbalanced data scenarios.

</details>


### [130] [DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems](https://arxiv.org/abs/2601.01931)
*Willem Röpke,Samuel Coward,Andrei Lupu,Thomas Foster,Tim Rocktäschel,Jakob Foerster*

Main category: cs.LG

TL;DR: DéjàQ是一个通过进化合成数学问题来训练推理模型的新框架，它使用LLM驱动的突变策略动态生成训练数据，避免静态数据集导致的记忆化问题


<details>
  <summary>Details</summary>
Motivation: 当前推理模型主要依赖静态数据集，这容易导致模型记忆而非真正理解，限制了泛化能力。需要一种能够动态适应模型能力、促进学习的训练方法

Method: 提出DéjàQ框架，在模型训练过程中联合进化多样化的合成数学问题。采用两种LLM驱动的突变策略：1）改变上下文细节；2）直接修改问题结构。模型自身参与训练数据的生成和优化

Result: 模型能够生成新颖且有意义的数学问题，LLM驱动的突变策略改善了强化学习训练效果。分析了生成问题的有效性和计算开销，证明了动态进化训练数据的潜力

Conclusion: 动态进化训练数据能够增强数学推理能力，具有更广泛的适用性。作者将开源代码以支持进一步研究

Abstract: Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.

</details>


### [131] [SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling](https://arxiv.org/abs/2601.01943)
*Tieu-Long Phan,Nhu-Ngoc Nguyen Song,Peter F. Stadler*

Main category: cs.LG

TL;DR: SynRXN是一个用于计算机辅助合成规划的统一基准测试框架和开放数据资源，将端到端合成规划分解为五个任务家族，提供标准化数据集和评估工作流。


<details>
  <summary>Details</summary>
Motivation: 当前计算机辅助合成规划领域缺乏统一的基准测试框架，数据集异构且评估标准不一致，难以进行公平的方法比较和性能评估。

Method: 将合成规划分解为五个任务家族：反应平衡、原子到原子映射、反应分类、反应性质预测和合成路线设计；从异构公共来源收集反应数据，进行统一表示；提供透明的数据分割函数、标准化评估工作流和指标套件；采用脚本化构建配方确保可复现性。

Result: 创建了一个包含版本化数据集、明确元数据、许可标签和机器可读清单的资源；提供了防泄漏的数据分割和敏感任务的黄金标准测试集；整个资源在宽松开源许可下发布。

Conclusion: SynRXN通过消除数据集异质性并提供透明可重用的评估框架，支持CASP方法的公平纵向比较，降低从业者获得稳健可比性能评估的门槛，促进计算机辅助合成规划领域的发展。

Abstract: We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.

</details>


### [132] [Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior](https://arxiv.org/abs/2601.01966)
*Bo Yin,Qi Li,Runpeng Yu,Xinchao Wang*

Main category: cs.LG

TL;DR: 本文提出RePro框架，用于推断指令微调模型是否基于原始提示或LLM优化版本进行训练，解决训练数据来源的审计问题。


<details>
  <summary>Details</summary>
Motivation: 随着指令微调越来越多地使用LLM优化提示，需要解决训练数据来源的审计问题：当训练数据存在争议时，如何推断模型是基于原始提示还是优化版本进行训练，这对数据集治理和争议解决至关重要。

Method: 提出RePro框架，利用提示优化导致的教师强制token分布稳定变化，融合教师强制似然特征和logit排序信号。通过影子微调学习可迁移表示，使用轻量级线性头在未见过的模型上推断数据来源，无需访问训练数据。

Result: RePro在实验中表现优异且具有良好泛化能力，能够跨不同优化器有效工作，表明其利用了优化器无关的分布变化而非重写风格伪影。

Conclusion: 提示优化会产生可检测的分布变化，RePro框架能够有效解决优化来源推断问题，为训练数据审计提供实用工具，支持数据集治理和争议解决。

Abstract: Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.

</details>


### [133] [SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition](https://arxiv.org/abs/2601.01979)
*Julie Keisler,Anastase Alexandre Charantonis,Yannig Goude,Boutheina Oueslati,Claire Monteleoni*

Main category: cs.LG

TL;DR: SerpentFlow：一种无配对域对齐生成框架，通过将数据分解为共享结构和域特定组件，生成合成训练对，实现无监督域适应


<details>
  <summary>Details</summary>
Motivation: 解决无配对观测情况下域对齐的挑战，传统方法需要配对数据提供跨域监督，而实际应用中往往缺乏这种配对数据

Method: 在潜在空间中将数据分解为共享组件和域特定组件，通过替换域特定组件为随机噪声构建合成训练对，使用分类器自动确定分解频率，采用Flow Matching作为生成管道

Result: 在合成图像、物理过程模拟和气候降尺度任务中，该方法能有效重建与底层低频模式一致的高频结构

Conclusion: 共享结构分解是有效的无配对域对齐策略，SerpentFlow框架能成功学习给定共享表示的目标域条件分布

Abstract: Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.

</details>


### [134] [Prior Diffusiveness and Regret in the Linear-Gaussian Bandit](https://arxiv.org/abs/2601.02022)
*Yifan Zhu,John C. Duchi,Benjamin Van Roy*

Main category: cs.LG

TL;DR: 本文证明了在具有高斯先验的线性高斯赌博机中，Thompson采样算法的贝叶斯遗憾为$\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$，其中先验相关的"预热"项与极小极大遗憾项呈加法关系而非乘法关系。


<details>
  <summary>Details</summary>
Motivation: 现有关于Thompson采样在具有先验的线性赌博机中的遗憾界通常显示先验相关的"预热"项与长期遗憾项呈乘法关系。本文旨在证明这两个项实际上可以解耦为加法关系，从而提供更精确的遗憾分析。

Method: 通过新的"椭圆势能"引理来分析Thompson采样算法在具有$\mathcal{N}(μ_0, Σ_0)$先验分布的线性高斯赌博机中的性能。该方法能够分离先验相关项与时间相关项。

Result: 证明了Thompson采样的贝叶斯遗憾为$\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$，其中$d$是维度，$T$是时间范围，$r$是动作的最大$\ell_2$范数，$σ^2$是噪声方差。这表明先验相关的"预热"项与极小极大遗憾项呈加法关系。同时提供了下界证明预热项是不可避免的。

Conclusion: 在具有高斯先验的线性高斯赌博机中，Thompson采样的遗憾可以分解为加法形式：先验相关的预热项$d r \sqrt{\mathrm{Tr}(Σ_0)}$与长期极小极大遗憾项$σd \sqrt{T}$。这一结果为理解Thompson采样在具有先验信息的环境中的性能提供了新的理论洞见。

Abstract: We prove that Thompson sampling exhibits $\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\mathcal{N}(μ_0, Σ_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\ell_2$ norm of the actions, and $σ^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \sqrt{\mathrm{Tr}(Σ_0)}$ decouples additively from the minimax (long run) regret $σd \sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.

</details>


### [135] [Output Embedding Centering for Stable LLM Pretraining](https://arxiv.org/abs/2601.02031)
*Felix Stollenwerk,Anna Lokrantz,Niclas Hertzberg*

Main category: cs.LG

TL;DR: 本文提出输出嵌入中心化(OEC)方法，通过解决输出嵌入几何问题来防止训练末期的输出logit发散，比现有的z-loss方法更有效。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练成本高且容易出现训练不稳定问题，特别是在训练后期使用大学习率时会出现输出logit发散。现有的z-loss方法只是治标不治本，需要从根本上解决这个问题。

Method: 从输出嵌入的几何角度分析不稳定性原因，提出输出嵌入中心化(OEC)作为新的缓解策略。OEC有两种实现方式：确定性操作μ-centering和正则化方法μ-loss。

Result: 两种OEC变体在训练稳定性和学习率敏感性方面都优于z-loss。它们能确保即使在大学习率下训练也能收敛，而z-loss会失败。μ-loss对正则化超参数调优的敏感性显著低于z-loss。

Conclusion: OEC通过解决输出嵌入几何的根本问题，有效防止训练末期的输出logit发散，比z-loss方法更稳定且对超参数更不敏感。

Abstract: Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.

</details>


### [136] [GDRO: Group-level Reward Post-training Suitable for Diffusion Models](https://arxiv.org/abs/2601.02036)
*Yiyang Wang,Xi Chen,Xiaogang Xu,Yu Liu,Hengshuang Zhao*

Main category: cs.LG

TL;DR: 提出GDRO方法，用于文本到图像校正流扩散模型的群体级奖励对齐，解决了在线RL方法的效率低、依赖随机采样器和奖励黑客问题


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的在线RL方法用于文本到图像校正流扩散模型存在三个主要问题：1) 在线图像采样效率低下，2) 校正流模型本质上是确定性的，3) 存在奖励黑客问题

Method: 提出群体级直接奖励优化(GDRO)，这是一种结合校正流模型特性的后训练范式，支持完全离线训练，不依赖扩散采样器，并引入修正评分来考虑奖励黑客趋势

Result: 在OCR和GenEval任务上的实验表明，GDRO能有效提升扩散模型的奖励分数，同时展现出强大的稳定性和鲁棒性，能缓解奖励黑客问题

Conclusion: GDRO为文本到图像校正流扩散模型提供了一种高效、稳定且鲁棒的群体级奖励对齐方法，解决了现有在线RL方法的局限性

Abstract: Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.

</details>


### [137] [Multivariate Time-series Anomaly Detection via Dynamic Model Pool & Ensembling](https://arxiv.org/abs/2601.02037)
*Wei Hu,Zewei Yu,Jianqiu Xu*

Main category: cs.LG

TL;DR: DMPEAD：一种用于多元时间序列异常检测的动态模型池与集成框架，通过构建多样化模型池、动态更新和集成排名靠前的模型，解决了现有多模型方法在模型选择、集成策略和可扩展性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列异常检测的多模型方法存在三个主要问题：(1) 选择方法依赖单一模型且对策略敏感；(2) 集成方法通常组合所有模型或仅限于单变量数据；(3) 大多数方法依赖固定的数据维度，限制了可扩展性。

Method: 提出DMPEAD框架：1) 通过参数传递和多样性度量构建多样化模型池；2) 使用元模型和基于相似性的策略动态更新模型池，实现自适应扩展、子集选择和池合并；3) 通过代理度量排名和top-k聚合在选定子集中集成排名靠前的模型。

Result: 在8个真实世界数据集上的广泛实验表明，DMPEAD优于所有基线方法，展现出卓越的适应性和可扩展性。

Conclusion: DMPEAD通过动态模型池构建和集成策略，有效解决了多元时间序列异常检测中多模型方法的局限性，提供了一种更灵活、可扩展的解决方案。

Abstract: Multivariate time-series (MTS) anomaly detection is critical in domains such as service monitor, IoT, and network security. While multi-model methods based on selection or ensembling outperform single-model ones, they still face limitations: (i) selection methods rely on a single chosen model and are sensitive to the strategy; (ii) ensembling methods often combine all models or are restricted to univariate data; and (iii) most methods depend on fixed data dimensionality, limiting scalability. To address these, we propose DMPEAD, a Dynamic Model Pool and Ensembling framework for MTS Anomaly Detection. The framework first (i) constructs a diverse model pool via parameter transfer and diversity metric, then (ii) updates it with a meta-model and similarity-based strategy for adaptive pool expansion, subset selection, and pool merging, finally (iii) ensembles top-ranked models through proxy metric ranking and top-k aggregation in the selected subset, outputting the final anomaly detection result. Extensive experiments on 8 real-world datasets show that our model outperforms all baselines, demonstrating superior adaptability and scalability.

</details>


### [138] [Explore the Ideology of Deep Learning in ENSO Forecasts](https://arxiv.org/abs/2601.02050)
*Yanhai Gan,Yipeng Chen,Ning Li,Xingguo Liu,Junyu Dong,Xianyao Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于有界变差函数的数学可解释性框架，通过激活函数饱和区"拯救"死亡神经元来增强模型表达能力，揭示了ENSO可预测性主要来自热带太平洋，并分析了春季可预测性障碍的原因。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习显著提高了ENSO预测技能，但模型的不透明性阻碍了科学信任和业务部署，需要开发数学基础的可解释性框架。

Method: 引入基于有界变差函数的数学可解释性框架，通过从激活函数饱和区"拯救"死亡神经元来增强模型表达能力，进行控制实验验证方法稳健性。

Result: 分析显示ENSO可预测性主要来自热带太平洋，印度洋和大西洋也有贡献；发现春季可预测性障碍期间敏感性扩大但预测性能下降，可能源于次优变量选择。

Conclusion: 该可解释性框架与物理理解一致，建议纳入更多海气变量可能有助于突破春季可预测性障碍限制，推进长期ENSO预测。

Abstract: The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the "dead" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.

</details>


### [139] [The Homogeneity Trap: Spectral Collapse in Doubly-Stochastic Deep Networks](https://arxiv.org/abs/2601.02080)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 论文发现双随机矩阵约束在深度学习架构中会导致"同质性陷阱"，即高熵偏置使混合算子趋向均匀重心，抑制次主导奇异值，限制特征表达能力。


<details>
  <summary>Details</summary>
Motivation: 双随机矩阵在深度学习架构（如最优传输层和Sinkhorn注意力）中用于保持数值稳定性和概率可解释性，但作者发现这些约束会导致谱退化现象，影响网络表达能力。

Method: 通过理论分析证明Sinkhorn投影的最大熵偏置使混合算子趋向均匀重心，推导了σ_2与网络有效深度的谱界，并形式化证明层归一化在噪声主导机制中无法缓解崩溃。

Result: 发现高熵约束将特征变换限制在浅层有效感受野内，当信噪比低于临界阈值时，几何结构会因噪声诱导的正交崩溃而不可逆地丢失。

Conclusion: 双随机矩阵约束网络存在熵稳定性和谱表达能力之间的基本权衡，需要重新考虑这些约束在深度架构中的应用。

Abstract: Doubly-stochastic matrices (DSM) are increasingly utilized in structure-preserving deep architectures -- such as Optimal Transport layers and Sinkhorn-based attention -- to enforce numerical stability and probabilistic interpretability. In this work, we identify a critical spectral degradation phenomenon inherent to these constraints, termed the Homogeneity Trap. We demonstrate that the maximum-entropy bias, typical of Sinkhorn-based projections, drives the mixing operator towards the uniform barycenter, thereby suppressing the subdominant singular value σ_2 and filtering out high-frequency feature components. We derive a spectral bound linking σ_2 to the network's effective depth, showing that high-entropy constraints restrict feature transformation to a shallow effective receptive field. Furthermore, we formally demonstrate that Layer Normalization fails to mitigate this collapse in noise-dominated regimes; specifically, when spectral filtering degrades the Signal-to-Noise Ratio (SNR) below a critical threshold, geometric structure is irreversibly lost to noise-induced orthogonal collapse. Our findings highlight a fundamental trade-off between entropic stability and spectral expressivity in DSM-constrained networks.

</details>


### [140] [A Differentiable Adversarial Framework for Task-Aware Data Subsampling](https://arxiv.org/abs/2601.02081)
*Jiacheng Lyu,Bihua Bao*

Main category: cs.LG

TL;DR: ASSS框架将数据降采样重构为可微分的端到端学习问题，通过选择器网络与任务网络的对抗博弈，为样本分配连续重要性权重，在保持模型性能的同时实现智能去噪。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集给模型训练带来计算挑战，传统数据子采样方法作为静态、任务无关的预处理步骤，通常会丢弃对下游预测至关重要的信息。

Method: 提出对抗性软选择子采样（ASSS）框架，将数据降采样重构为可微分的端到端学习问题。使用选择器网络与任务网络的对抗博弈，选择器网络学习为样本分配连续重要性权重。通过Gumbel-Softmax松弛实现直接优化，使选择器能够在平衡预测保真度和稀疏性的损失函数指导下，识别并保留对特定任务目标信息量最大的样本。

Result: 在四个大规模真实世界数据集上的综合实验表明，ASSS始终优于聚类和最近邻稀疏化等启发式子采样基线方法。值得注意的是，ASSS不仅能匹配，有时甚至能超过使用整个数据集的训练性能，展示了智能去噪的效果。

Conclusion: 这项工作将任务感知数据子采样确立为可学习组件，为有效的大规模数据学习提供了原则性解决方案。理论分析将该框架与信息瓶颈原理联系起来。

Abstract: The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduces the antagonistic soft selection subsampling (ASSS) framework as is a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.

</details>


### [141] [Horizon Activation Mapping for Neural Networks in Time Series Forecasting](https://arxiv.org/abs/2601.02094)
*Hans Krupakar,V A Kandappan*

Main category: cs.LG

TL;DR: 本文提出HAM（Horizon Activation Mapping），一种用于时间序列预测模型的视觉可解释性技术，通过梯度范数平均值分析不同时间子序列的重要性，适用于跨模型家族的模型选择和比较。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型依赖误差指标和特定架构的可解释性方法，这些方法无法在不同模型家族间通用。需要一种模型无关的可解释技术来理解和比较不同预测模型。

Method: HAM受grad-CAM启发，使用梯度范数平均值分析时间序列子序列的重要性。引入因果和反因果模式计算每个时间步的梯度更新范数平均值，并通过比例线表示范数平均值的均匀分布。研究了批量大小、早停、数据划分等优化因素对HAM的影响。

Result: HAM成功应用于多种预测模型（CycleNet、N-Linear、N-HITS、FEDformer、Pyraformer、SpaceTime、Multi-Resolution DDPM），揭示了NHITS的神经逼近定理和SpaceTime的指数自回归活动在HAM图中的趋势。批量大小差异显示存在指数近似关系。

Conclusion: HAM可用于细粒度模型选择、验证集选择和不同神经网络模型家族间的比较，为时间序列预测模型提供了一种通用的可解释性框架。

Abstract: Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.

</details>


### [142] [LION-DG: Layer-Informed Initialization with Deep Gradient Protocols for Accelerated Neural Network Training](https://arxiv.org/abs/2601.02105)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 提出LION-DG初始化方法，针对深度监督架构中的辅助分类器，通过零初始化辅助头实现梯度唤醒，加速收敛且无需超参数调优。


<details>
  <summary>Details</summary>
Motivation: 现有权重初始化方法大多是层无关的，而深度监督架构中的未训练辅助分类器头会通过梯度干扰破坏早期训练稳定性。

Method: 提出LION-DG层感知初始化：对主干网络应用标准He初始化，对辅助分类器头进行零初始化。这实现了梯度唤醒机制——辅助梯度在初始化时为零，随着权重增长自然引入。

Result: 在CIFAR-10和CIFAR-100上的实验显示：DenseNet-DS收敛速度提升8.3%；LSUV与LION-DG结合在CIFAR-10上达到81.92%最佳准确率；ResNet-DS在CIFAR-100上加速11.3%。

Conclusion: LION-DG方法简单、无需超参数、无计算开销，为深度监督架构提供了有效的初始化方案，并给出了架构特定的实践指导。

Abstract: Weight initialization remains decisive for neural network optimization, yet existing methods are largely layer-agnostic. We study initialization for deeply-supervised architectures with auxiliary classifiers, where untrained auxiliary heads can destabilize early training through gradient interference.
  We propose LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads while applying standard He-initialization to the backbone. We prove that this implements Gradient Awakening: auxiliary gradients are exactly zero at initialization, then phase in naturally as weights grow -- providing an implicit warmup without hyperparameters.
  Experiments on CIFAR-10 and CIFAR-100 with DenseNet-DS and ResNet-DS architectures demonstrate: (1) DenseNet-DS: +8.3% faster convergence on CIFAR-10 with comparable accuracy, (2) Hybrid approach: Combining LSUV with LION-DG achieves best accuracy (81.92% on CIFAR-10), (3) ResNet-DS: Positive speedup on CIFAR-100 (+11.3%) with side-tap auxiliary design.
  We identify architecture-specific trade-offs and provide clear guidelines for practitioners. LION-DG is simple, requires zero hyperparameters, and adds no computational overhead.

</details>


### [143] [Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI](https://arxiv.org/abs/2601.02106)
*Ashish Rana,Ammar Shaker,Sascha Saralajew,Takashi Suzuki,Kosuke Yasuda,Shintaro Kato,Toshikazu Wada,Toshiyuki Fujikawa,Toru Kikutsuji*

Main category: cs.LG

TL;DR: ProtoPal框架通过原型学习实现个性化预防医疗，在保证高性能的同时提供可理解和可验证的干预方案展示。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习在医疗领域存在解释性不足的问题，个性化预防医疗需要为所有利益相关者提供可理解和可验证的预测、干预和推荐。

Method: 提出ProtoPal框架，采用原型学习方法，包含前端和后端两种模式，能够直观展示干预措施及其模拟结果。

Result: 在保持优异定量性能的同时，提供了直观的干预方案展示和结果模拟。

Conclusion: 原型学习方法能够有效解决医疗领域个性化预防中的可解释性和可验证性问题，ProtoPal框架为此提供了可行方案。

Abstract: Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.

</details>


### [144] [Edge-aware GAT-based protein binding site prediction](https://arxiv.org/abs/2601.02138)
*Weisen Yang,Hanqing Zhang,Wangren Qiu,Xuan Xiao,Weizhong Lin*

Main category: cs.LG

TL;DR: 提出Edge-aware GAT模型，通过原子级图结构和多维特征集成，实现蛋白质结合位点的高精度预测，在基准测试中达到0.93 ROC-AUC，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 准确识别蛋白质结合位点对于理解生物分子相互作用机制和药物靶点理性设计至关重要。传统预测方法在捕捉复杂空间构象时难以平衡预测精度与计算效率。

Method: 提出Edge-aware Graph Attention Network模型，构建原子级图结构，集成几何描述符、DSSP二级结构、相对溶剂可及性等多维特征。通过将原子间距离和方向向量作为注意力机制中的边特征，增强模型表示能力。采用方向张量传播和残基级注意力池化技术。

Result: 在基准数据集上，蛋白质-蛋白质结合位点预测的ROC-AUC达到0.93，优于多个最先进方法。可视化分析证实了模型的实用性和可解释性。已部署公开可访问的Web服务器。

Conclusion: 该方法为蛋白质功能位点识别提供了新颖高效的解决方案，在预测精度、泛化能力和可解释性之间取得了良好平衡。

Abstract: Accurate identification of protein binding sites is crucial for understanding biomolecular interaction mechanisms and for the rational design of drug targets. Traditional predictive methods often struggle to balance prediction accuracy with computational efficiency when capturing complex spatial conformations. To address this challenge, we propose an Edge-aware Graph Attention Network (Edge-aware GAT) model for the fine-grained prediction of binding sites across various biomolecules, including proteins, DNA/RNA, ions, ligands, and lipids. Our method constructs atom-level graphs and integrates multidimensional structural features, including geometric descriptors, DSSP-derived secondary structure, and relative solvent accessibility (RSA), to generate spatially aware embedding vectors. By incorporating interatomic distances and directional vectors as edge features within the attention mechanism, the model significantly enhances its representation capacity. On benchmark datasets, our model achieves an ROC-AUC of 0.93 for protein-protein binding site prediction, outperforming several state-of-the-art methods. The use of directional tensor propagation and residue-level attention pooling further improves both binding site localization and the capture of local structural details. Visualizations using PyMOL confirm the model's practical utility and interpretability. To facilitate community access and application, we have deployed a publicly accessible web server at http://119.45.201.89:5000/. In summary, our approach offers a novel and efficient solution that balances prediction accuracy, generalization, and interpretability for identifying functional sites in proteins.

</details>


### [145] [Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting](https://arxiv.org/abs/2601.02151)
*Muxi Diao,Lele Yang,Wuxuan Gong,Yutong Zhang,Zhonghao Yan,Yufei Han,Kongming Liang,Weiran Xu,Zhanyu Ma*

Main category: cs.LG

TL;DR: 提出EAFT方法，通过基于熵的门控机制区分认知不确定性和知识冲突，在保持下游性能的同时显著减轻SFT中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 监督微调（SFT）在领域适应中常导致灾难性遗忘，而基于策略的强化学习（RL）能有效保持通用能力。研究发现这种差异源于分布差距：RL与模型内部信念对齐，而SFT强制模型拟合外部监督，导致"自信冲突"令牌（低概率但低熵）引发破坏性梯度更新。

Method: 提出熵自适应微调（EAFT），利用令牌级熵作为门控机制来区分认知不确定性和知识冲突。模型从不确定样本中学习，同时抑制冲突数据的梯度，避免破坏性更新。

Result: 在Qwen和GLM系列模型（4B到32B参数）上，在数学、医疗和智能体领域进行广泛实验。EAFT在匹配标准SFT下游性能的同时，显著减轻了通用能力的退化。

Conclusion: EAFT通过熵自适应机制有效解决了SFT中的灾难性遗忘问题，在保持领域适应性能的同时保护了模型的通用能力，为领域自适应提供了更稳健的方法。

Abstract: Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "Confident Conflicts" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.

</details>


### [146] [ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense](https://arxiv.org/abs/2601.02196)
*Yu Li,Sizhe Tang,Rongqian Chen,Fei Xu Yu,Guangyu Jiang,Mahdi Imani,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: 提出基于蒙特卡洛树搜索和图神经网络的自动化网络防御方法，在CAGE-4挑战中实现样本高效的安全策略学习


<details>
  <summary>Details</summary>
Motivation: 现有基于深度强化学习的自动化网络防御方法在复杂网络中面临探索困难和大状态空间问题，需要大量样本，需要开发样本高效的防御策略

Method: 将ACD建模为基于上下文的部分可观测马尔可夫决策问题，提出基于蒙特卡洛树搜索的规划中心防御策略，使用图神经网络嵌入网络观测为属性图，结合学习到的图嵌入和图编辑动作先验来指导MCTS

Result: 在CC4场景中评估，涉及不同网络结构和对手行为，结果显示基于搜索引导和图嵌入的规划相比最先进的RL基线提高了防御奖励和鲁棒性

Conclusion: 提出的MCTS和图神经网络结合的方法能够有效解决自动化网络防御中的探索-利用权衡问题，实现样本高效的防御策略学习

Abstract: Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.

</details>


### [147] [CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents](https://arxiv.org/abs/2601.02201)
*Keyu Wang,Bingchen Miao,Wendong Bu,Yu Wu,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: CORE是一个基于代码的逆向自训练框架，通过图扩展桥接模仿学习与探索学习，提升虚拟代理的行为多样性和泛化能力，无需人工设计奖励函数。


<details>
  <summary>Details</summary>
Motivation: 当前多模态虚拟代理训练面临两难：行为克隆方法简单有效但行为多样性低，强化学习能发现新策略但严重依赖人工设计的奖励函数。需要解决这两种方法的冲突。

Method: 提出CORE框架：1) 语义代码抽象：从专家演示自动推断可执行的奖励函数（标签函数）；2) 策略图扩展：构建多路径策略图捕捉任务内的行为多样性；3) 轨迹引导外推：利用成功和失败轨迹扩展任务空间，增强域外行为多样性。

Result: 在Web和Android平台上的实验表明，CORE显著提升了整体性能和泛化能力，展示了作为构建强大虚拟代理的鲁棒且可泛化训练范式的潜力。

Conclusion: CORE成功桥接了模仿学习与探索学习，在提升行为多样性的同时消除了对人工奖励设计的依赖，为多模态虚拟代理训练提供了新的有效框架。

Abstract: The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.

</details>


### [148] [Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction](https://arxiv.org/abs/2601.02213)
*Haoyu Zhou,Ping Xue,Tianfan Fu,Hao Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种用于压缩和加速SO(3)-等变图神经网络的低比特量化技术，通过解耦量化方案、分支分离训练策略和注意力归一化机制，在保持精度和等变性的同时显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署3D等变图神经网络面临高计算成本挑战，需要压缩和加速SO(3)-等变GNN以实现在实际化学应用中的部署。

Method: 提出三种创新：1) 幅值-方向解耦量化方案，分别量化等变特征的范数和方向；2) 分支分离量化感知训练策略，在基于注意力的SO(3)-GNN中区别处理不变和等变特征通道；3) 鲁棒性增强的注意力归一化机制，稳定低精度注意力计算。

Result: 在QM9和rMD17分子基准测试中，8位模型在能量和力预测方面达到与全精度基线相当的精度，推理速度提升2.37-2.73倍，模型大小减小4倍，同时保持等变性。

Conclusion: 提出的量化技术使对称感知GNN能够在实际化学应用中部署，在不牺牲精度或物理对称性的情况下显著提升效率和减小模型尺寸。

Abstract: Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.

</details>


### [149] [ELLA: Efficient Lifelong Learning for Adapters in Large Language Models](https://arxiv.org/abs/2601.02232)
*Shristi Das Biswas,Yue Zhang,Anwesan Pal,Radhika Bhargava,Kaushik Roy*

Main category: cs.LG

TL;DR: ELLA：一种基于选择性子空间去相关的持续学习框架，无需数据回放，通过惩罚任务特定方向的对齐来减少灾难性遗忘，同时保留共享表示以实现前向迁移。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在持续学习场景中面临严重的灾难性遗忘问题。现有方法存在根本性限制：基于回放的方法不切实际且侵犯隐私，而严格正交性的方法在规模扩展时会崩溃，因为每个新任务都被投影到正交补空间，逐渐减少剩余自由度并禁止共享表示的重叠，从而消除前向迁移。

Method: 提出ELLA框架，基于选择性子空间去相关原则。不是禁止所有重叠，而是显式表征过去更新的结构，惩罚沿其高能量、任务特定方向的对齐，同时在低能量残差子空间中保留自由度以实现迁移。通过一个轻量级正则化器在单个聚合更新矩阵上实现，这对应于一个各向异性收缩算子，能够限制干扰。

Result: 在三个流行基准测试上实现了最先进的持续学习性能，相对准确率提升高达9.6%，内存占用减少35倍。无需数据回放、架构扩展和可忽略的存储开销。能够稳健地扩展到不同架构，并主动增强模型在未见任务上的零样本泛化性能。

Conclusion: ELLA为大语言模型的建设性终身适应提供了一个原则性且可扩展的解决方案，通过选择性子空间去相关平衡了灾难性遗忘和前向迁移之间的权衡。

Abstract: Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\%$ and a $35\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.

</details>


### [150] [Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission](https://arxiv.org/abs/2601.02253)
*Emrah Mete,Emin Erkan Korkmaz*

Main category: cs.LG

TL;DR: 提出Neuro-Channel Networks (NCN)，一种无需乘法的神经网络架构，通过模拟生物神经系统的离子通道机制，用通道宽度限制信号幅度，用神经递质参数调节信号传输，仅使用加减法和位运算实现前向传播。


<details>
  <summary>Details</summary>
Motivation: 深度学习对GPU等高性能硬件的依赖限制了其在边缘设备的普及，这些硬件昂贵、耗能且供应稀缺。标准人工感知器依赖密集矩阵乘法效率低下，而生物神经系统通过物理离子通道限制和化学神经递质调节实现高效计算，无需算术乘法。

Method: 提出Neuro-Channel Networks (NCN)：1) 用通道宽度取代权重，物理限制信号幅度；2) 引入神经递质参数基于符号逻辑调节信号传输；3) 前向传播仅使用加法、减法和位运算（最小值、符号），完全消除浮点乘法；4) 使用标准反向传播进行训练。

Result: 在概念验证研究中，NCN能够以100%准确率解决非线性可分问题（如XOR和多数函数），证明其无需乘法权重即可形成复杂决策边界的能力。

Conclusion: NCN架构为下一代神经形态硬件提供了高效替代方案，使复杂模型能够在商用CPU或超低功耗芯片上运行，无需依赖昂贵的GPU集群，有望推动AI在边缘设备的普及。

Abstract: The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.

</details>


### [151] [POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network](https://arxiv.org/abs/2601.02264)
*Boris Kriuk,Fedor Kriuk*

Main category: cs.LG

TL;DR: POSEIDON是一个物理信息能量模型，用于统一的多任务地震事件预测，结合了Gutenberg-Richter和Omori-Utsu定律作为可学习约束，在三个任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法通常作为黑箱运行，忽略了已建立的物理定律，而地震预测和地震危险性评估仍然是地球物理学的基本挑战。

Method: 提出了POSEIDON物理信息能量模型，将Gutenberg-Richter震级-频率关系和Omori-Utsu余震衰减定律作为可学习约束嵌入能量建模框架，同时处理余震序列识别、海啸生成潜力和前震检测三个任务。

Result: POSEIDON在所有任务上实现了最先进的性能，平均F1分数最高，优于梯度提升、随机森林和CNN基线。学习到的物理参数收敛到科学可解释的值（b=0.752，p=0.835，c=0.1948天），落在已建立的地震学范围内。

Conclusion: POSEIDON通过将物理定律作为可学习约束嵌入，实现了高性能且物理可解释的地震预测，同时发布了包含280万事件的Poseidon数据集，推动了物理信息地震研究。

Abstract: Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.

</details>


### [152] [Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck](https://arxiv.org/abs/2601.02307)
*Dina El Zein,James Henderson*

Main category: cs.LG

TL;DR: 提出一种基于差分隐私的文本数据共享方法，通过向Transformer嵌入添加噪声来保护隐私，同时保持实用性


<details>
  <summary>Details</summary>
Motivation: Transformer嵌入包含多个向量（每个token一个），容易泄露敏感信息，攻击者可能以较高准确率恢复原始输入数据，需要平衡隐私保护和数据实用性

Method: 提出非参数变分差分隐私（NVDP），在Transformer架构中集成非参数变分信息瓶颈（NVIB）层，向多向量嵌入注入噪声，使用Rényi散度衡量隐私保护，提供贝叶斯差分隐私保证

Result: 在GLUE基准测试中，通过调节噪声水平实现了隐私与准确性的有效权衡，较低噪声水平下模型保持高准确性同时提供强隐私保证

Conclusion: NVDP方法能有效平衡隐私保护和数据实用性，为文本数据的安全共享提供了可行方案

Abstract: We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.

</details>


### [153] [Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay](https://arxiv.org/abs/2601.02310)
*Ahmad Makinde*

Main category: cs.LG

TL;DR: 使用T-KAN网络替代传统LSTM的固定线性权重，通过可学习的B样条激活函数捕捉市场信号的"形状"，在高频交易环境中显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 高频交易环境中的限价订单簿数据噪声大、非线性强，传统模型如DeepLOB随着时间跨度增加预测能力衰减严重，需要更有效的模型来捕捉市场信号的复杂模式。

Method: 提出Temporal Kolmogorov-Arnold Networks (T-KAN)，用可学习的B样条激活函数替代标准LSTM的固定线性权重，使模型能够学习市场信号的"形状"而不仅仅是幅度。

Result: 在k=100时间跨度上相对F1分数提升19.1%；在1.0bps交易成本下，T-KAN获得132.48%回报，而DeepLOB亏损82.76%；模型具有可解释性，样条中的"死区"清晰可见。

Conclusion: T-KAN网络在高频交易预测中显著优于传统模型，不仅性能更好且具有可解释性，同时通过高级综合优化适合低延迟FPGA实现。

Abstract: High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.

</details>


### [154] [Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning](https://arxiv.org/abs/2601.02313)
*Hanzaleh Akbari Nodehi,Viveck R. Cadambe,Mohammad Ali Maddah-Ali*

Main category: cs.LG

TL;DR: 论文提出了一种新的博弈论框架"coding game"，将编码理论扩展到理性对手场景，即使恶意节点占多数也能实现非零概率的数据恢复，并具有Sybil抵抗性。


<details>
  <summary>Details</summary>
Motivation: 传统编码理论假设最坏情况的恶意对手模型，要求诚实节点数量超过恶意节点。但在去中心化机器学习等新兴应用中，节点因贡献而获得奖励，这催生了理性对手（战略行为而非纯粹恶意），需要新的编码方法。

Method: 引入"coding game"博弈论框架，扩展编码理论到信任最小化设置，重点关注重复编码，展示了即使恶意节点占多数也能实现数据恢复，并具有Sybil抵抗性（均衡不随恶意节点数量增加而改变）。

Result: 该框架能够：1）在恶意节点占多数时仍实现非零概率的数据恢复；2）具有Sybil抵抗性，即均衡状态不随恶意节点数量增加而改变；3）处理对手策略未知的情况。

Conclusion: 论文提出了一个创新的博弈论编码框架，解决了去中心化系统中理性对手带来的挑战，为编码理论在信任最小化环境中的应用开辟了新方向，并指出了未来研究的开放问题。

Abstract: Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.
  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.

</details>


### [155] [DatBench: Discriminative, Faithful, and Efficient VLM Evaluations](https://arxiv.org/abs/2601.02316)
*Siddharth Joshi,Haoli Yin,Rishabh Adiga,Ricardo Monti,Aldo Carranza,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Luke Merrick,Parth Doshi,Paul Burstein,Pratyush Maini,Scott Loftin,Spandan Das,Tony Jiang,Vineeth Dorna,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: 论文提出评估视觉语言模型的三个标准（忠实性、可区分性、效率），发现现有评估存在多项缺陷，通过转换和过滤方法改进评估，并发布清理后的评估套件


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型评估方法尚不成熟，存在多种缺陷：多项选择题鼓励猜测、与下游应用脱节、存在可盲目解答的问题、样本标签错误等，且评估计算成本过高

Method: 提出评估应满足的三个标准（忠实性、可区分性、效率），通过将多项选择题转换为生成任务、过滤可盲目解答和错误标签样本来改进现有评估基准

Result: 多项选择题转生成任务后模型能力下降高达35%；过滤问题样本提高可区分性同时降低计算成本；发布DatBench-Full（33个数据集）和DatBench（高效子集，平均加速13倍）

Conclusion: 通过转换和过滤方法可以显著改进视觉语言模型评估的忠实性和可区分性，同时大幅提高效率，为大规模模型评估提供了更严谨和可持续的路径

Abstract: Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.

</details>


### [156] [Heterogeneous Low-Bandwidth Pre-Training of LLMs](https://arxiv.org/abs/2601.02360)
*Yazan Obeidi,Amir Sarfi,Joel Lidin,Paul Janson,Eugene Belilovsky*

Main category: cs.LG

TL;DR: SparseLoCo低通信数据并行方法与低带宽流水线模型并行结合，通过激活和激活梯度压缩实现异构分布式训练，在保持性能的同时显著降低通信开销。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型预训练需要分布式计算，但带宽限制使得在数据中心之外难以扩展，特别是当模型并行需要频繁的大规模设备间通信时。需要研究如何将低通信数据并行方法与低带宽模型并行结合。

Method: 提出异构分布式训练框架：高带宽参与者托管完整副本，资源有限参与者分组使用流水线并行，通过子空间投影进行阶段间通信压缩。将子空间流水线压缩与SparseLoCo方法适配，研究多种调整方案。

Result: 在178M-1B参数的大规模语言建模实验中，激活压缩与SparseLoCo组合成本适中，选择性（异构）压缩相比压缩所有副本能持续改善损失-通信权衡，特别是在高压缩比下效果更明显。

Conclusion: 这些结果表明，将低带宽模型并行和异构参与者纳入LLM预训练是一条可行的实用路径，为资源受限环境下的分布式训练提供了解决方案。

Abstract: Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [157] [Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights](https://arxiv.org/abs/2601.01029)
*Zeyu Bian,Max Biggs,Ruijiang Gao,Zhengling Qi*

Main category: stat.ML

TL;DR: 提出使用观测数据审计AI决策消费者剩余效应的实用框架，通过随机定价数据和累积倾向权重避免需求函数估计与数值积分，引入双重稳健估计器ACPW，并扩展到公平性考量。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过估计需求函数再积分计算消费者剩余，但实践中面临参数模型误设、非参数方法数据需求大收敛慢的问题。需要开发更实用的框架来审计AI驱动决策（如定向定价和算法贷款）的消费者剩余效应。

Method: 利用算法定价中固有的探索-开发随机性，通过随机价格下的购买结果作为需求的无偏估计，使用新颖的累积倾向权重（CPW）重构积分。引入双重稳健的增强累积倾向权重（ACPW）估计器，只需需求模型或历史定价策略分布之一正确设定即可。框架支持灵活机器学习方法，即使估计收敛较慢也能获得快速收敛率。

Result: 开发了避免需求函数显式估计和数值积分的消费者剩余估计方法，ACPW估计器具有双重稳健性。将框架扩展到不平等感知的剩余度量，允许量化利润-公平权衡。通过全面数值研究验证了方法有效性。

Conclusion: 提出了一种实用的观测数据审计框架，能够有效评估AI决策的消费者剩余效应，解决了传统方法的局限性，支持公平性考量，为监管者和企业提供了量化工具。

Abstract: This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies.

</details>


### [158] [Fibonacci-Driven Recursive Ensembles: Algorithms, Convergence, and Learning Dynamics](https://arxiv.org/abs/2601.01055)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: 该论文开发了由斐波那契型更新流驱动的递归集成学习的算法和动力学基础，建立了二阶递归架构的理论框架，相比经典boosting的一阶加法更新，具有记忆能力并能整合历史结构。


<details>
  <summary>Details</summary>
Motivation: 经典boosting使用一阶加法更新，缺乏记忆能力。本文旨在开发具有记忆的动态递归集成学习系统，通过二阶递归架构整合历史信息，提升集成学习的表达能力。

Method: 引入递归权重更新算法族（包括斐波那契、三波那契和高阶递归），建立连续时间极限得到控制集成演化的微分方程组，进行全局收敛条件、谱稳定性准则和非渐近泛化界分析。

Result: 建立了递归集成、结构化加权和动力系统视角的统一理论，实验证明递归流在核岭回归、样条平滑器和随机傅里叶特征模型中持续改善逼近和泛化能力。

Conclusion: 该工作完成了从斐波那契加权、几何加权理论到完全动态递归集成学习系统的三部曲，为具有记忆的动态集成学习提供了理论基础和算法框架。

Abstract: This paper develops the algorithmic and dynamical foundations of recursive ensemble learning driven by Fibonacci-type update flows. In contrast with classical boosting  Freund and Schapire (1997); Friedman (2001), where the ensemble evolves through first-order additive updates, we study second-order recursive architectures in which each predictor depends on its two immediate predecessors. These Fibonacci flows induce a learning dynamic with memory, allowing ensembles to integrate past structure while adapting to new residual information. We introduce a general family of recursive weight-update algorithms encompassing Fibonacci, tribonacci, and higher-order recursions, together with continuous-time limits that yield systems of differential equations governing ensemble evolution. We establish global convergence conditions, spectral stability criteria, and non-asymptotic generalization bounds under Rademacher Bartlett and Mendelson (2002) and algorithmic stability analyses. The resulting theory unifies recursive ensembles, structured weighting, and dynamical systems viewpoints in statistical learning. Experiments with kernel ridge regression Rasmussen and Williams (2006), spline smoothers Wahba (1990), and random Fourier feature models Rahimi and Recht (2007) demonstrate that recursive flows consistently improve approximation and generalization beyond static weighting. These results complete the trilogy begun in Papers I and II: from Fibonacci weighting, through geometric weighting theory, to fully dynamical recursive ensemble learning systems.

</details>


### [159] [Neural Networks on Symmetric Spaces of Noncompact Type](https://arxiv.org/abs/2601.01097)
*Xuan Son Nguyen,Shuo Yang,Aymeric Histace*

Main category: stat.ML

TL;DR: 提出一种在非紧型对称空间（如双曲空间和SPD流形）上构建神经网络的新方法，基于统一的点到超平面距离公式，并应用于多种任务验证。


<details>
  <summary>Details</summary>
Motivation: 现有研究展示了神经网络在双曲空间和SPD流形等非紧型对称空间上的潜力，但需要更统一和通用的方法来在这些流形上构建神经网络层。

Method: 提出统一的点到超平面距离公式，推导出高阶非紧型对称空间上G不变黎曼度量下的闭式表达式，基于此设计全连接层和注意力机制。

Result: 方法在图像分类、EEG信号分类、图像生成和自然语言推理等多个挑战性基准测试中得到验证，展示了有效性。

Conclusion: 提出的统一方法为在非紧型对称空间上构建神经网络提供了有效工具，能够处理多种流形上的机器学习任务。

Abstract: Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference.

</details>


### [160] [Conformal Blindness: A Note on $A$-Cryptic change-points](https://arxiv.org/abs/2601.01147)
*Johan Hallberg Szabadváry*

Main category: stat.ML

TL;DR: 本文揭示了"保形盲区"现象：即使数据交换性发生重大破坏，p值序列仍可保持均匀分布，导致保形测试鞅无法检测到这种变化。


<details>
  <summary>Details</summary>
Motivation: 保形测试鞅是保形预测框架中用于检验数据交换性假设的标准方法，它通过监测p值序列偏离均匀分布的程度来检测交换性破坏。然而，交换性意味着均匀p值，但均匀p值并不一定意味着交换性。这就引出了一个关键问题：是否存在交换性发生显著破坏，但p值仍保持均匀分布的情况，使得保形测试鞅无法检测到这种变化？

Method: 1. 通过理论构造证明"保形盲区"现象的存在
2. 针对理论理想的"预言机"保形度量（由真实条件密度给出），构造A-隐秘变化点
3. 使用二元高斯分布，识别出边际均值变化但不改变保形分数分布的直线
4. 通过模拟验证即使大规模分布偏移也可能对保形测试鞅完全隐秘

Result: 1. 证实了保形盲区现象确实存在：即使数据交换性发生重大破坏，p值序列仍可保持均匀分布
2. 在理论理想的保形度量下，成功构造了A-隐秘变化点
3. 在二元高斯分布中找到了边际均值变化但不改变保形分数分布的特定直线
4. 模拟结果显示，大规模分布偏移可能对保形测试鞅完全隐秘

Conclusion: 保形测试鞅存在根本性局限：即使数据交换性发生重大破坏，只要保形度量与潜在偏移方向"对齐"，p值序列仍可保持均匀分布，导致检测失败。这强调了保形度量与潜在偏移方向对齐的至关重要性。

Abstract: Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \emph{conformal blindness}.
  Through explicit construction, for the theoretically ideal ``oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values.
  Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts.

</details>


### [161] [Evidence Slopes and Effective Dimension in Singular Linear Models](https://arxiv.org/abs/2601.01238)
*Kalyaan Rao*

Main category: stat.ML

TL;DR: 论文研究贝叶斯模型选择中Laplace近似/BIC在奇异模型中的失效问题，提出基于实对数典范阈值(RLCT)的修正方法，在线性高斯模型中理论证明了Laplace/BIC误差随(d/2 - λ)log n线性增长，其中d为参数维度，λ为RLCT。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯模型选择方法（Laplace近似和BIC）假设有效模型维度等于参数数量，但在过参数化或秩不足的奇异模型中这一假设不成立。奇异学习理论用实对数典范阈值(RLCT)作为有效维度，可能严格小于参数数量。需要研究Laplace/BIC在奇异模型中的具体失效机制。

Method: 研究线性高斯秩模型和线性子空间（字典）模型，这些模型的精确边缘似然有闭式解，RLCT可解析计算。理论分析Laplace/BIC误差，提出RLCT感知的修正方法，并通过数值实验验证。

Result: 理论证明Laplace/BIC误差随(d/2 - λ)log n线性增长，其中d为参数维度，λ为RLCT。RLCT感知修正能恢复正确的证据斜率，且对表示相同数据子空间的过完备重参数化具有不变性。在线性简单设置中，证据斜率可作为有效维度的实用估计器。

Conclusion: 研究为奇异模型中Laplace失效提供了具体的有限样本特征描述，表明在简单线性设置中证据斜率可作为有效维度的实用估计器，RLCT感知修正能解决传统方法在奇异模型中的偏差问题。

Abstract: Bayesian model selection commonly relies on Laplace approximation or the Bayesian Information Criterion (BIC), which assume that the effective model dimension equals the number of parameters. Singular learning theory replaces this assumption with the real log canonical threshold (RLCT), an effective dimension that can be strictly smaller in overparameterized or rank-deficient models.
  We study linear-Gaussian rank models and linear subspace (dictionary) models in which the exact marginal likelihood is available in closed form and the RLCT is analytically tractable. In this setting, we show theoretically and empirically that the error of Laplace/BIC grows linearly with (d/2 minus lambda) times log n, where d is the ambient parameter dimension and lambda is the RLCT. An RLCT-aware correction recovers the correct evidence slope and is invariant to overcomplete reparameterizations that represent the same data subspace.
  Our results provide a concrete finite-sample characterization of Laplace failure in singular models and demonstrate that evidence slopes can be used as a practical estimator of effective dimension in simple linear settings.

</details>


### [162] [Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations](https://arxiv.org/abs/2601.01442)
*Dongrong Li,Tianwei Yu,Xiaodan Fan*

Main category: stat.ML

TL;DR: 提出一种用于含缺失观测值的隐马尔可夫模型的折叠吉布斯采样器，通过积分掉缺失观测和对应隐状态来提高采样效率，在缺失值较多时尤其高效。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中经常存在缺失观测值，这使得隐马尔可夫模型的应用变得复杂。现有的EM算法和吉布斯采样器存在非凸性、高计算复杂度和混合速度慢等问题。

Method: 提出一种折叠吉布斯采样器，通过同时积分掉缺失观测值和对应的隐状态，直接从HMM后验分布中采样。该方法在每次迭代中能产生更大的有效样本量，且在缺失值较多时计算复杂度显著降低。

Result: 数值模拟和真实数据分析表明，所提算法在时间复杂度和采样效率（以ESS衡量）方面一致优于现有算法。当缺失条目较多时，计算复杂度显著降低，采样速度更快。

Conclusion: 提出的采样算法在计算和理论上都更快，特别在存在大量缺失条目时具有优势。该折叠吉布斯采样器为含缺失观测的HMM提供了一种高效的后验采样方法。

Abstract: The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS).

</details>


### [163] [Modeling Information Blackouts in Missing Not-At-Random Time Series Data](https://arxiv.org/abs/2601.01480)
*Aman Sunesh,Allan Ma,Siddarth Nilol*

Main category: stat.ML

TL;DR: 提出一个潜在状态空间框架，联合建模交通动态和传感器丢失，通过MNAR建模处理传感器黑屏问题，相比传统方法显著提升插补精度


<details>
  <summary>Details</summary>
Motivation: 大规模交通预测依赖固定传感器网络，但常出现传感器黑屏（连续缺失测量）。传统方法假设数据随机缺失（MAR），但黑屏事件可能与未观测的交通状况相关，需要非随机缺失（MNAR）处理

Method: 提出潜在状态空间框架：1）通过线性动态系统建模交通动态；2）通过伯努利观测通道建模传感器丢失，其概率取决于潜在交通状态。使用扩展卡尔曼滤波进行推断，通过近似EM算法学习参数

Result: 在西雅图感应线圈数据上，潜在动态建模显著优于基线方法：黑屏插补RMSE从7.02（LOCF）和5.02（线性插值+季节朴素）降至4.23（MAR LDS），相对LOCF减少64% MSE。MNAR建模提供额外改进至4.20 RMSE（0.8%提升）。合成实验中，MNAR优势随缺失与潜在状态相关性增强而增加

Conclusion: 时间动态主导性能，MNAR建模提供原则性改进，当缺失确实具有信息性时最有价值。框架有效处理交通传感器黑屏问题，联合建模动态和缺失机制

Abstract: Large-scale traffic forecasting relies on fixed sensor networks that often exhibit blackouts: contiguous intervals of missing measurements caused by detector or communication failures. These outages are typically handled under a Missing At Random (MAR) assumption, even though blackout events may correlate with unobserved traffic conditions (e.g., congestion or anomalous flow), motivating a Missing Not At Random (MNAR) treatment. We propose a latent state-space framework that jointly models (i) traffic dynamics via a linear dynamical system and (ii) sensor dropout via a Bernoulli observation channel whose probability depends on the latent traffic state. Inference uses an Extended Kalman Filter with Rauch-Tung-Striebel smoothing, and parameters are learned via an approximate EM procedure with a dedicated update for detector-specific missingness parameters. On the Seattle inductive loop detector data, introducing latent dynamics yields large gains over naive baselines, reducing blackout imputation RMSE from 7.02 (LOCF) and 5.02 (linear interpolation + seasonal naive) to 4.23 (MAR LDS), corresponding to about a 64% reduction in MSE relative to LOCF. Explicit MNAR modeling provides a consistent but smaller additional improvement on real data (imputation RMSE 4.20; 0.8% RMSE reduction relative to MAR), with similar modest gains for short-horizon post-blackout forecasts (evaluated at 1, 3, and 6 steps). In controlled synthetic experiments, the MNAR advantage increases as the true missingness dependence on latent state strengthens. Overall, temporal dynamics dominate performance, while MNAR modeling offers a principled refinement that becomes most valuable when missingness is genuinely informative.

</details>


### [164] [Variance-Reduced Diffusion Sampling via Conditional Score Expectation Identity](https://arxiv.org/abs/2601.01594)
*Alois Duston,Tan Bui-Thanh*

Main category: stat.ML

TL;DR: 本文提出了条件分数期望(CSE)恒等式，基于此开发了CSE分数估计器，并与Tweedie估计器结合得到方差最小化的混合估计器，在扩散模型和贝叶斯逆问题中提升了样本质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型中的分数估计存在方差问题，需要更有效的分数估计方法来提升样本质量和计算效率。本文旨在通过建立精确的数学关系来改进分数估计。

Method: 1) 提出并证明了仿射扩散过程的条件分数期望(CSE)恒等式；2) 基于CSE开发了使用自归一化重要性采样(SNIS)的统计估计器；3) 分析了CSE与标准Tweedie估计器的关系；4) 推导了方差最小化的混合分数估计器；5) 扩展到贝叶斯逆问题，使用似然信息化的SNIS权重。

Result: 数值实验表明，最优混合估计器相比基线方法减少了方差，在固定计算预算下提升了样本质量。在贝叶斯逆问题中，该方法改善了高维图像重建任务和PDE控制逆问题的重建质量和样本多样性。

Conclusion: CSE恒等式为分数估计提供了新的理论框架，基于此开发的混合估计器在扩散模型和贝叶斯逆问题中均表现出优越性能，为相关领域提供了有效的计算工具。

Abstract: We introduce and prove a \textbf{Conditional Score Expectation (CSE)} identity: an exact relation for the marginal score of affine diffusion processes that links scores across time via a conditional expectation under the forward dynamics. Motivated by this identity, we propose a CSE-based statistical estimator for the score using a Self-Normalized Importance Sampling (SNIS) procedure with prior samples and forward noise. We analyze its relationship to the standard Tweedie estimator, proving anti-correlation for Gaussian targets and establishing the same behavior for general targets in the small time-step regime. Exploiting this structure, we derive a variance-minimizing blended score estimator given by a state--time dependent convex combination of the CSE and Tweedie estimators. Numerical experiments show that this optimal-blending estimator reduces variance and improves sample quality for a fixed computational budget compared to either baseline. We further extend the framework to Bayesian inverse problems via likelihood-informed SNIS weights, and demonstrate improved reconstruction quality and sample diversity on high-dimensional image reconstruction tasks and PDE-governed inverse problems.

</details>


### [165] [Deep Linear Discriminant Analysis Revisited](https://arxiv.org/abs/2601.01619)
*Maxat Tezekbayev,Rustem Takhanov,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 论文提出DNLL损失函数，解决深度线性判别分析在最大似然训练中的病态解问题，同时保持判别性能与概率解释的一致性。


<details>
  <summary>Details</summary>
Motivation: 深度线性判别分析在最大似然训练中会出现病态解（类均值聚合、协方差坍缩），而交叉熵训练虽然准确率高但破坏了生成模型结构，导致参数估计不一致。需要一种方法既能保持生成结构又能获得良好判别性能。

Method: 提出判别性负对数似然（DNLL）损失函数，在LDA对数似然基础上增加对混合密度的惩罚项，明确抑制多个类别同时可能出现的区域。

Result: 使用DNLL训练的深度LDA产生干净、分离良好的潜在空间，在合成数据和标准图像基准测试中匹配softmax分类器的测试准确率，并显著改善预测概率的校准性。

Conclusion: DNLL成功调和了生成结构与判别性能，为深度判别模型恢复了连贯的概率解释，解决了传统训练方法中的病态解和参数不一致问题。

Abstract: We show that for unconstrained Deep Linear Discriminant Analysis (LDA) classifiers, maximum-likelihood training admits pathological solutions in which class means drift together, covariances collapse, and the learned representation becomes almost non-discriminative. Conversely, cross-entropy training yields excellent accuracy but decouples the head from the underlying generative model, leading to highly inconsistent parameter estimates. To reconcile generative structure with discriminative performance, we introduce the \emph{Discriminative Negative Log-Likelihood} (DNLL) loss, which augments the LDA log-likelihood with a simple penalty on the mixture density. DNLL can be interpreted as standard LDA NLL plus a term that explicitly discourages regions where several classes are simultaneously likely. Deep LDA trained with DNLL produces clean, well-separated latent spaces, matches the test accuracy of softmax classifiers on synthetic data and standard image benchmarks, and yields substantially better calibrated predictive probabilities, restoring a coherent probabilistic interpretation to deep discriminant models.

</details>


### [166] [Simplex Deep Linear Discriminant Analysis](https://arxiv.org/abs/2601.01679)
*Maxat Tezekbayev,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: 本文重新审视深度线性判别分析(Deep LDA)，从似然角度分析其训练问题，提出几何约束的Deep LDA模型，在保持竞争力的分类性能同时获得可解释的潜在空间几何结构。


<details>
  <summary>Details</summary>
Motivation: 传统LDA是简单的线性高斯模型，但当将LDA头部连接到神经网络编码器时，如何通过最大似然估计训练这种深度分类器成为一个问题。作者发现无约束的Deep LDA在端到端MLE训练中会出现退化解，导致类别聚类重叠甚至崩溃，分类性能下降。

Method: 提出约束Deep LDA公式：将类别均值固定在潜在空间中正单纯形的顶点上，限制共享协方差为球形，只学习先验和单个方差参数以及编码器。在这些几何约束下，MLE变得稳定，能在潜在空间中产生良好分离的类别聚类。

Result: 在图像数据集(Fashion-MNIST, CIFAR-10, CIFAR-100)上，约束Deep LDA模型达到与softmax基线竞争的分类准确率，同时提供了简单、可解释的潜在几何结构，这些结构在二维投影中清晰可见。

Conclusion: 通过几何约束的Deep LDA模型解决了传统Deep LDA在MLE训练中的退化问题，实现了稳定的训练和良好的类别分离，在保持分类性能的同时获得了可解释的潜在空间表示。

Abstract: We revisit Deep Linear Discriminant Analysis (Deep LDA) from a likelihood-based perspective. While classical LDA is a simple Gaussian model with linear decision boundaries, attaching an LDA head to a neural encoder raises the question of how to train the resulting deep classifier by maximum likelihood estimation (MLE). We first show that end-to-end MLE training of an unconstrained Deep LDA model ignores discrimination: when both the LDA parameters and the encoder parameters are learned jointly, the likelihood admits a degenerate solution in which some of the class clusters may heavily overlap or even collapse, and classification performance deteriorates. Batchwise moment re-estimation of the LDA parameters does not remove this failure mode. We then propose a constrained Deep LDA formulation that fixes the class means to the vertices of a regular simplex in the latent space and restricts the shared covariance to be spherical, leaving only the priors and a single variance parameter to be learned along with the encoder. Under these geometric constraints, MLE becomes stable and yields well-separated class clusters in the latent space. On images (Fashion-MNIST, CIFAR-10, CIFAR-100), the resulting Deep LDA models achieve accuracy competitive with softmax baselines while offering a simple, interpretable latent geometry that is clearly visible in two-dimensional projections.

</details>


### [167] [Sparse Convex Biclustering](https://arxiv.org/abs/2601.01757)
*Jiakun Jiang,Dewei Xiang,Chenliang Gu,Wei Liu,Binhuan Wang*

Main category: stat.ML

TL;DR: 提出SpaCoBi方法，通过凸优化框架和稳定性调优准则，在高维大规模数据中实现更准确稳健的双聚类


<details>
  <summary>Details</summary>
Motivation: 现有双聚类方法在处理现代大规模高维数据时面临挑战：高维特征噪声累积、非凸优化限制、计算复杂度高，导致准确性和稳定性随数据规模增大而下降

Method: 提出稀疏凸双聚类(SpaCoBi)方法，采用凸优化框架，在双聚类过程中惩罚噪声，引入基于稳定性的调优准则，平衡聚类保真度和稀疏性

Result: 综合数值研究（包括模拟实验和小鼠嗅球数据应用）表明，SpaCoBi在准确性方面显著优于现有最先进方法

Conclusion: SpaCoBi为高维大规模数据集的双聚类提供了一个稳健高效的解决方案

Abstract: Biclustering is an essential unsupervised machine learning technique for simultaneously clustering rows and columns of a data matrix, with widespread applications in genomics, transcriptomics, and other high-dimensional omics data. Despite its importance, existing biclustering methods struggle to meet the demands of modern large-scale datasets. The challenges stem from the accumulation of noise in high-dimensional features, the limitations of non-convex optimization formulations, and the computational complexity of identifying meaningful biclusters. These issues often result in reduced accuracy and stability as the size of the dataset increases. To overcome these challenges, we propose Sparse Convex Biclustering (SpaCoBi), a novel method that penalizes noise during the biclustering process to improve both accuracy and robustness. By adopting a convex optimization framework and introducing a stability-based tuning criterion, SpaCoBi achieves an optimal balance between cluster fidelity and sparsity. Comprehensive numerical studies, including simulations and an application to mouse olfactory bulb data, demonstrate that SpaCoBi significantly outperforms state-of-the-art methods in accuracy. These results highlight SpaCoBi as a robust and efficient solution for biclustering in high-dimensional and large-scale datasets.

</details>


### [168] [A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk](https://arxiv.org/abs/2601.01970)
*Ayomide Afolabi,Ebere Ogburu,Symon Kimitei*

Main category: stat.ML

TL;DR: 评估三种信用风险模型（响应、风险、响应-风险）中不同分类器的性能，Extra Trees在响应模型中召回率最高（79.1%），Random Forest在风险模型中特异性最佳（84.1%），在响应-风险多分类模型中准确率最高（83.2%）


<details>
  <summary>Details</summary>
Motivation: 解决信用风险和邮件响应性的具体业务问题，优化不同业务场景下的性能指标，为信用卡邮件营销和违约预测提供有效的分类器选择依据

Method: 使用三种不同模型：响应模型（预测邮件响应）、风险模型（预测违约风险）、响应-风险多分类模型（同时预测响应和风险），评估多种分类器（包括Extra Trees和Random Forest）在不同模型中的性能表现

Result: 1. 响应模型中：Extra Trees分类器召回率最高（79.1%），能有效识别潜在响应者；2. 风险模型中：Random Forest分类器特异性最高（84.1%），能准确识别低风险客户；3. 响应-风险多分类模型中：Random Forest分类器准确率最高（83.2%），能同时有效识别响应者和低风险用户

Conclusion: 不同业务场景需要不同的分类器选择：响应预测优先考虑召回率（Extra Trees），风险预测优先考虑特异性（Random Forest），综合预测则需平衡多项指标（Random Forest），应根据具体业务目标优化相应性能指标

Abstract: This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.

</details>


### [169] [From Mice to Trains: Amortized Bayesian Inference on Graph Data](https://arxiv.org/abs/2601.02241)
*Svenja Jedhoff,Elizaveta Semenova,Aura Raulo,Anne Meyer,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: 将摊销贝叶斯推理（ABI）应用于图数据，通过结合置换不变图编码器和神经后验估计器，在图结构数据上进行节点、边和图级参数的推理。


<details>
  <summary>Details</summary>
Motivation: 图数据在多个领域普遍存在，但图结构数据的推理面临置换不变性、可扩展性和捕捉长程依赖等挑战。需要开发能够处理这些挑战的后验估计方法。

Method: 采用两模块流水线：1) 摘要网络将属性图映射为固定长度表示；2) 推理网络近似参数后验分布。评估多种神经架构作为摘要网络的性能。

Result: 在受控合成设置和两个真实世界领域（生物学和物流）中评估了多种架构，从恢复能力和校准度两方面评估性能。

Conclusion: 将摊销贝叶斯推理成功应用于图数据，为图结构数据的参数推理提供了有效的解决方案，并在多个领域验证了方法的有效性。

Abstract: Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.

</details>
