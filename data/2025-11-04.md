<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 24]
- [cs.LG](#cs.LG) [Total: 184]
- [stat.ML](#stat.ML) [Total: 13]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Model-Free Channel Estimation for Massive MIMO: A Channel Charting-Inspired Approach](https://arxiv.org/abs/2511.00225)
*Pinjun Zheng,Md. Jahangir Hossain,Anas Chaaban*

Main category: eess.SP

TL;DR: 提出了一种结合深度自编码器和LSTM网络的无模型信道估计方法，在massive MIMO系统中显著降低导频开销并提升估计精度


<details>
  <summary>Details</summary>
Motivation: 传统最小二乘法在massive MIMO系统中需要过高的导频开销，而稀疏估计方法依赖精确的信道模型，在实际应用中存在局限性

Method: 使用深度自编码器学习低维信道表示并保持时间相关性，然后通过LSTM网络跟踪这些特征，从有限导频中恢复完整信道信息

Result: 在射线追踪数据集上的仿真结果显示，该方法在病态条件下比最小二乘法实现高达9 dB的归一化均方误差改善，并在不同MIMO配置下保持可扩展性

Conclusion: 该方法为massive MIMO系统提供了一种有效的无模型信道估计解决方案，显著降低了导频开销并提升了估计性能

Abstract: Channel estimation is fundamental to wireless communications, yet it becomes
increasingly challenging in massive multiple-input multiple-output (MIMO)
systems where base stations employ hundreds of antennas. Traditional
least-squares methods require prohibitive pilot overhead that scales with
antenna count, while sparse estimation methods depend on precise channel models
that may not always be practical. This paper proposes a model-free approach
combining deep autoencoders and LSTM networks. The method first learns
low-dimensional channel representations preserving temporal correlation through
augmenting a channel charting-inspired loss function, then tracks these
features to recover full channel information from limited pilots. Simulation
results using ray-tracing datasets show that the proposed approach achieves up
to 9 dB improvement in normalized mean square error compared to the
least-squares methods under ill-conditioned scenarios, while maintaining
scalability across MIMO configurations.

</details>


### [2] [Discrete-Periodic Ambiguity Function of Random Communication Signals](https://arxiv.org/abs/2511.00482)
*Ying Zhang,Fan Liu,Yifeng Xiong,Tao Liu,Shi Jin*

Main category: eess.SP

TL;DR: 本文研究了携带随机数据载荷的通信信号的模糊函数，这是表征ISAC系统感知能力的基本指标。提出了一个统一的分析框架来评估基于任意正交基和独立同分布星座符号构建的通信中心ISAC信号的模糊函数。


<details>
  <summary>Details</summary>
Motivation: 在ISAC系统中，通信信号的模糊函数是评估感知能力的关键指标，但现有研究缺乏对随机数据载荷影响的系统性分析。

Method: 开发统一分析框架评估基于任意正交基和独立同分布星座符号的ISAC信号模糊函数，推导离散周期模糊函数及其期望积分旁瓣水平和平均旁瓣水平的闭式表达式。

Result: 证明了归一化期望积分旁瓣水平在所有星座和调制基之间保持不变，理论发现通过仿真验证。

Conclusion: 建立了ISAC信号模糊函数的统一分析框架，揭示了归一化期望积分旁瓣水平的不变性特性，为ISAC系统设计提供了理论指导。

Abstract: This paper investigates the ambiguity function (AF) of communication signals
carrying random data payloads, which is a fundamental metric characterizing
sensing capability in ISAC systems. We first develop a unified analytical
framework to evaluate the AF of communication-centric ISAC signals constructed
from arbitrary orthonormal bases and independent identically distributed
(i.i.d.) constellation symbols. Subsequently, we derive the discrete periodic
ambiguity function (DP-AF) and provide closed-form expressions for its expected
integrated sidelobe level (EISL) and average sidelobe level. Notably, we prove
that the normalized EISL is invariant across all constellations and modulation
bases. Finally, the theoretical findings are validated through simulations.

</details>


### [3] [Meta-Learning Based Radio Frequency Fingerprinting for GNSS Spoofing Detection](https://arxiv.org/abs/2511.00491)
*Leatile Marata,Juhani Sankari,Eslam Eldeeb,Mikko Valkama,Elena Simona Lohan*

Main category: eess.SP

TL;DR: 提出基于元学习的GNSS接收器欺骗信号检测框架，利用信号在预相关和后相关阶段的射频指纹特征，在多个数据集上实现超过95%的检测准确率。


<details>
  <summary>Details</summary>
Motivation: 随着GNSS技术向低地球轨道卫星发展，欺骗干扰信号的安全威胁日益严重，传统数学方法在处理多样化干扰形式时不够灵活，需要AI解决方案。

Method: 采用元学习框架，利用GNSS信号在预相关和后相关阶段的射频指纹特征来检测各种类型的欺骗信号。

Result: 在TEXBAT和OAKBAT多个数据集上的数值结果显示，提出的解决方案能够显著检测不同形式的欺骗信号，检测准确率超过95%。

Conclusion: 所提出的元学习框架在欺骗信号检测方面具有优越的泛化性能，相比现有最优解决方案表现更好。

Abstract: The rapid development of technology has led to an increase in the number of
devices that rely on position, velocity, and time (PVT) information to perform
their functions. As such, the Global Navigation Satellite Systems (GNSS) have
been adopted as one of the most promising solutions to provide PVT.
Consequently, there are renewed efforts aimed at enhancing GNSS capabilities to
meet emerging use cases and their requirements. For example, GNSS is evolving
to rely on low-earth-orbit satellites, shifting the focus from traditional
medium-earth-orbit satellites. Unfortunately, these developments also bring
forth higher risks of interference signals such as spoofers, which pose serious
security threats. To address this challenge, artificial intelligence
(AI)-inspired solutions are being developed to overcome the limitations of
conventional mathematics-based approaches, which have proven inflexible when
dealing with diverse forms of interference. In this paper, we advance this
direction by proposing a meta-learning framework that enables GNSS receivers to
detect various types of spoofers. Specifically, our approach exploits the radio
frequency fingerprints present in the signal at both the pre-correlation and
post-correlation stages of the receiver. The proposed solution has superior
generalization properties compared to the state-of-the-art solutions. Numerical
results demonstrate that our proposed solution significantly detects spoofers
of different forms, with spoofing detection accuracies of more than 95% on
multiple datasets from the Texas Spoofing Test Battery (TEXBAT) and the Oak
Ridge Spoofing and Interference Test Battery (OAKBAT) repositories

</details>


### [4] [A Multimodal Dataset for Indoor Radio Mapping with 3D Point Clouds and RSSI](https://arxiv.org/abs/2511.00494)
*Ljupcho Milosheski,Kuon Akiyama,Blaž Bertalanič,Jernej Hribar,Ryoichi Shinkuma*

Main category: eess.SP

TL;DR: 本文提出了一个多模态数据集，结合高分辨率3D LiDAR扫描和Wi-Fi RSSI测量，用于研究室内无线信号传播的动态环境效应。


<details>
  <summary>Details</summary>
Motivation: 随着支持带宽密集型应用的智能设备增多，需要可靠的室内无线连接。准确的无线电环境地图估计对于自适应无线网络规划和AP布局优化至关重要，但室内空间复杂性使得生成真实REM具有挑战性。

Method: 创建了一个多模态数据集，在20种不同AP配置下收集高分辨率3D LiDAR扫描和Wi-Fi RSSI测量，包含无人环境和有人环境两种场景。

Result: 提供了支持动态环境效应研究的综合数据集，能够捕捉人类存在对无线信号传播的影响。

Conclusion: 该数据集旨在促进数据驱动的无线建模研究，特别是在新兴高频标准背景下，推动开发鲁棒的高容量室内通信系统。

Abstract: The growing number of smart devices supporting bandwidth-intensive and
latency-sensitive applications, such as real-time video analytics, smart
sensing, and Extended Reality (XR), necessitates reliable wireless connectivity
in indoor environments. Therein, accurate estimation of Radio Environment Maps
(REMs) enables adaptive wireless network planning and optimization of Access
Point (AP) placement. However, generating realistic REMs remains challenging
due to the complexity of indoor spaces. To overcome this challenge, this paper
introduces a multimodal dataset that integrates high-resolution 3D LiDAR scans
with Wi-Fi Received Signal Strength Indicator (RSSI) measurements collected
under 20 distinct AP configurations in a multi-room indoor environment. The
dataset captures two measurement scenarios: the first without human presence in
the environment, and the second with human presence. Thus, the presented
dataset supports the study of dynamic environmental effects on wireless signal
propagation. This resource is designed to facilitate research in data-driven
wireless modeling, particularly in the context of emerging high-frequency
standards such as IEEE 802.11be (Wi-Fi 7), and aims to advance the development
of robust, high-capacity indoor communication systems.

</details>


### [5] [Fast Time-Varying mmWave Channel Estimation: A Rank-Aware Matrix Completion Approach](https://arxiv.org/abs/2511.00607)
*Tianyu Jiang,Yan Yang,Hongjin Liu,Runyu Han,Bo Ai,Mohsen Guizani*

Main category: eess.SP

TL;DR: 提出了一种用于快速时变毫米波MIMO系统的两阶段压缩感知信道估计框架，包含观测矩阵补全和信道矩阵稀疏恢复，通过低秩矩阵补全和秩感知OMP方法实现高精度信道估计。


<details>
  <summary>Details</summary>
Motivation: 解决混合架构毫米波MIMO系统中快速时变信道的高维估计问题，利用信道的低秩和稀疏特性来应对用户移动性导致的秩突变挑战。

Method: 两阶段方法：1) 鲁棒秩一矩阵补全算法完成观测矩阵；2) 秩感知批处理正交匹配追踪进行稀疏信道恢复；建立离散时间自回归模型处理秩变化。

Result: 仿真结果表明所提框架有效，算法在低秩矩阵恢复方面达到最先进性能，并具有理论保证。

Conclusion: 提出的信道估计框架能够有效处理快速时变毫米波MIMO系统中的高维信道估计问题，特别是在应对用户移动性导致的秩变化方面表现出色。

Abstract: We consider the problem of high-dimensional channel estimation in fast
time-varying millimeter-wave MIMO systems with a hybrid architecture. By
exploiting the low-rank and sparsity properties of the channel matrix, we
propose a two-phase compressed sensing framework consisting of observation
matrix completion and channel matrix sparse recovery, respectively. First, we
formulate the observation matrix completion problem as a low-rank matrix
completion (LRMC) problem and develop a robust rank-one matrix completion
(R1MC) algorithm that enables the matrix and its rank to iteratively update.
This approach achieves high-precision completion of the observation matrix and
explicit rank estimation without prior knowledge. Second, we devise a
rank-aware batch orthogonal matching pursuit (OMP) method for achieving
low-latency sparse channel recovery. To handle abrupt rank changes caused by
user mobility, we establish a discrete-time autoregressive (AR) model that
leverages the temporal rank correlation between continuous-time instances to
obtain a complete observation matrix capable of perceiving rank changes for
more accurate channel estimates. Simulation results confirm the effectiveness
of the proposed channel estimation frame and demonstrate that our algorithms
achieve state-of-the-art performance in low-rank matrix recovery with
theoretical guarantees.

</details>


### [6] [Fairness-Aware Secure Communication in ISAC Systems with STAR-RIS and RSMA](https://arxiv.org/abs/2511.00721)
*Thanh Nha To,Hoang Lai Pham,Quynh Nguyen Thi,Tuan Anh Pham,Le Thanh Bang*

Main category: eess.SP

TL;DR: 该论文研究将STAR-RIS与RSMA集成到ISAC系统中，通过联合优化基站波束成形和STAR-RIS系数，在保证感知性能的同时最大化最小保密速率。


<details>
  <summary>Details</summary>
Motivation: 在ISAC系统中，感知目标可能成为潜在窃听者，需要同时保证通信安全性和感知性能，而STAR-RIS和RSMA的结合能有效提升物理层安全性能。

Method: 提出基于交替优化和主化最小化方法的有效算法，联合优化基站波束成形向量和STAR-RIS系数，解决非凸优化问题。

Result: 仿真结果验证了所提算法的快速收敛性，并展示了在安全通信性能方面的显著提升。

Conclusion: STAR-RIS与RSMA的集成能有效提升ISAC系统的物理层安全性能，所提算法具有良好的收敛性和性能优势。

Abstract: In this paper, we investigate the integration of simultaneously transmitting
and reflecting reconfigurable intelligent surfaces (STAR-RIS) with
rate-splitting multiple access (RSMA) for improving physical layer security
(PLS) in integrated sensing and communication (ISAC) systems. Specifically, we
consider a multi-user, multi-sensing target scenario, where each sensing target
is treated as a potential eavesdropper, reflecting realistic deployment
conditions. To enable fairness-aware secure communication among users while
maintaining sensing performance, we formulate a joint optimization problem that
designs the base station beamforming vectors and STAR-RIS coefficients, aiming
to maximize the minimum secrecy rate under a minimum beampattern gain
constraint. To solve the resulting non-convex problem, we propose an efficient
algorithm based on alternating optimization (AO) and the
majorization-minimization (MM) method. Simulation results verify the fast
convergence of the proposed algorithm and demonstrate significant improvements
in secure communication performance.

</details>


### [7] [Target Detection with Tightly-coupled Antennas: Analysis for Unknown Wideband Signals](https://arxiv.org/abs/2511.00779)
*Erfan Khordad,Peter J. Smith,Sachitha C. Bandara,Rajitha Senanayake,Robert W. Heath Jr*

Main category: eess.SP

TL;DR: 该论文分析了使用强耦合天线阵列进行目标检测的方法，提出了针对不同频谱动态特性的检测器，并证明了强耦合天线在目标检测中的优势。


<details>
  <summary>Details</summary>
Motivation: 研究强耦合天线阵列在目标检测中的应用，利用其宽操作带宽的优势，解决在频率变化相关噪声环境下的目标检测问题。

Method: 提出了基于最大似然估计的检测器，针对恒定、慢变和快速变化三种频谱动态特性分别设计检测算法，包括用于慢变频谱的新型检测器和用于快速变化频谱的扩展能量检测器。

Result: 仿真验证表明，提出的检测器在频率变化鲁棒性方面优于基于最大似然估计的检测器，且强耦合天线阵列在目标检测中明显优于弱耦合天线阵列。

Conclusion: 强耦合天线阵列在目标检测中具有显著优势，提出的检测器能有效应对不同频谱动态特性，为相关应用提供了有效的解决方案。

Abstract: This paper presents analysis for target detection using tightly-coupled
antenna (TCA) arrays with high mutual coupling (MC). We show that the wide
operational bandwidth of TCAs is advantageous for target detection. We assume a
sensing receiver equipped with a TCA array that collects joint time and
frequency samples of the target's echo signals. Echoes are assumed to be
unknown wideband signals, and noise at the TCA array follows a
frequency-varying correlation model due to MC. We also assume that the echo
signals are time varying, with no assumption on the temporal variation. We
consider three regimes in frequency as constant, slowly or rapidly varying, to
capture all possible spectral dynamics of the echoes. We propose a novel
detector for the slowly-varying regime, and derive detectors based on maximum
likelihood estimation (MLE) for the other regimes. For the rapidly-varying
regime, we derive an extended energy detector for correlated noise with
frequency and time samples. We analyze the performance of all the detectors. We
also derive and analyze an ideal detector giving an upper bound on performance.
We validate our analysis with simulations and demonstrate that our proposed
detector outperforms the MLE-based detectors in terms of robustness to
frequency variation. Also, we highlight that TCA arrays offer clear advantages
over weakly-coupled antenna arrays in target detection.

</details>


### [8] [Stacked Flexible Intelligent Metasurface Design for Multi-User Wireless Communications](https://arxiv.org/abs/2511.00878)
*Ahmed Magbool,Vaibhav Kumar,Marco Di Renzo,Mark F. Flanagan*

Main category: eess.SP

TL;DR: 本文首次提出了基于堆叠柔性智能超表面（SFIM）的通信系统，通过可变形超表面层提升系统性能，相比刚性SIM系统获得显著性能增益。


<details>
  <summary>Details</summary>
Motivation: 堆叠智能超表面（SIMs）已成为下一代无线网络的有效解决方案，而柔性智能超表面（FIMs）的物理变形能力可进一步增强通信性能。

Method: 提出SFIM系统模型，包括收发信号模型和信道模型，构建在发射功率预算、变形距离和元原子响应单位模约束下的系统和速率最大化优化问题，开发基于梯度投影法的交替优化框架。

Result: 仿真结果表明，所提出的SFIM系统相比刚性SIM对应系统实现了显著的性能增益。

Conclusion: SFIM系统通过超表面层的物理变形能力有效提升了通信性能，为下一代无线网络提供了有前景的解决方案。

Abstract: Stacked intelligent metasurfaces (SIMs) have recently emerged as an effective
solution for next-generation wireless networks. A SIM comprises multiple
metasurface layers that enable signal processing directly in the wave domain.
Moreover, recent advances in flexible metamaterials have highlighted the
potential of flexible intelligent metasurfaces (FIMs), which can be physically
morphed to enhance communication performance. In this paper, we propose a
stacked flexible intelligent metasurface (SFIM)-based communication system for
the first time, where each metasurface layer is deformable to improve the
system's performance. We first present the system model, including the transmit
and receive signal models as well as the channel model, and then formulate an
optimization problem to maximize the system sum rate under constraints on the
transmit power budget, morphing distance, and the unit-modulus condition of the
meta-atom responses. To solve this problem, we develop an alternating
optimization framework based on the gradient projection method. Simulation
results demonstrate that the proposed SFIM-based system achieves significant
performance gains compared to its rigid SIM counterpart.

</details>


### [9] [Towards Channel Charting Enhancement with Non-Reconfigurable Intelligent Surfaces](https://arxiv.org/abs/2511.00919)
*Mahdi Maleki,Reza Agahzadeh Ayoubi,Marouan Mizmizi,Umberto Spagnolini*

Main category: eess.SP

TL;DR: 本文研究了如何通过完全被动电磁表面（EMS）来增强密集城市环境中的信道制图（CC）性能，提出了一种静态EMS相位配置方法，在保持信噪比的同时增加空间多样性，显著降低了定位误差和轨迹中断。


<details>
  <summary>Details</summary>
Motivation: 传统可重构智能表面（RIS）优化仅关注最大化增益，这会抑制位置指纹特征并降低信道制图性能，而随机相位配置虽然增加多样性但会降低信噪比。需要解决这种权衡问题。

Method: 采用两种互补的CC技术（半监督t-SNE和半监督自编码器），设计静态EMS相位配置文件，通过分位数驱动标准针对最差用户情况，提高可信度和连续性。

Result: 在30 GHz的3D射线追踪城市环境中，提出的EMS将90%分位数定位误差从>50米降低到<25米，在15%监督下将严重轨迹中断减少了4倍以上。

Conclusion: 静态预配置EMS是信道制图的实际使能器，无需重新配置开销，在各种评估配置中保持一致的改进效果。

Abstract: We investigate how fully-passive electromagnetic skins (EMSs) can be
engineered to enhance channel charting (CC) in dense urban environments. We
employ two complementary state-of-the-art CC techniques, semi-supervised
t-distributed stochastic neighbor embedding (t-SNE) and a semi-supervised
Autoencoder (AE), to verify the consistency of results across nonparametric and
parametric mappings. We show that the accuracy of CC hinges on a balance
between signal-to-noise ratio (SNR) and spatial dissimilarity: EMS codebooks
that only maximize gain, as in conventional Reconfigurable Intelligent Surface
(RIS) optimization, suppress location fingerprints and degrade CC, while
randomized phases increase diversity but reduce SNR. To address this trade-off,
we design static EMS phase profiles via a quantile-driven criterion that
targets worst-case users and improves both trustworthiness and continuity. In a
3D ray-traced city at 30 GHz, the proposed EMS reduces the 90th-percentile
localization error from > 50 m to < 25 m for both t-SNE and AE-based CC, and
decreases severe trajectory dropouts by over 4x under 15% supervision. The
improvements hold consistently across the evaluated configurations,
establishing static, pre-configured EMS as a practical enabler of CC without
reconfiguration overheads.

</details>


### [10] [Lightweight ResNet-Based Deep Learning for Photoplethysmography Signal Quality Assessment](https://arxiv.org/abs/2511.00943)
*Yangyang Zhao,Matti Kaisti,Olli Lahdenoja,Jonas Sandelin,Arman Anzanpour,Joonas Lehto,Joel Nuotio,Jussi Jaakkola,Arto Relander,Tuija Vasankari,Juhani Airaksinen,Tuomas Kiviniemi,Tero Koivisto*

Main category: eess.SP

TL;DR: 提出了一种轻量级ResNet架构结合SE模块的PPG信号质量评估模型，通过比较不同输入配置（PPG信号及其导数、自相关等），在参数和计算量大幅减少的同时实现了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在可穿戴设备中的应用增长，需要在资源受限平台上开发轻量高效的模型，同时探索不同预处理方法对性能的潜在提升。

Method: 基于ResNet架构结合Squeeze-and-Excitation模块的轻量级深度学习框架，比较了PPG信号、一阶导数、二阶导数、自相关以及这些通道的不同组合作为输入配置。

Result: 在M4M和MIMIC-IV数据集上分别达到96.52%和84.43%的AUC，相比现有研究参数减少超过99%，浮点运算减少超过60%。

Conclusion: 该轻量级模型在保持高性能的同时显著降低了计算复杂度，适用于资源受限的可穿戴设备，为PPG信号质量评估提供了有效的解决方案。

Abstract: With the growing application of deep learning in wearable devices,
lightweight and efficient models are critical to address the computational
constraints in resource-limited platforms. The performance of these approaches
can be potentially improved by using various preprocessing methods. This study
proposes a lightweight ResNet-based deep learning framework with
Squeeze-and-Excitation (SE) modules for photoplethysmography (PPG) signal
quality assessment (SQA) and compares different input configurations, including
the PPG signal alone, its first derivative (FDP), its second derivative (SDP),
the autocorrelation of PPG (ATC), and various combinations of these channels.
Experimental evaluations on the Moore4Medical (M4M) and MIMIC-IV datasets
demonstrate the model's performance, achieving up to 96.52% AUC on the M4M test
dataset and up to 84.43% AUC on the MIMIC-IV dataset. The novel M4M dataset was
collected to explore PPG-based monitoring for detecting atrial fibrillation
(AF) and AF burden in high-risk patients. Compared to the five reproduced
existing studies, our models achieves over 99% reduction in parameters and more
than 60% reduction in floating-point operations (FLOPs).

</details>


### [11] [Optimizing Uncertainty-Aware Deep Learning for On-the-Edge Murmur Detection in Low-Resource Settings](https://arxiv.org/abs/2511.00966)
*Andrea De Simone,Noemi Giordano,Silvia Seoni,Kristen M. Meiburger,Fabrizio Riente*

Main category: eess.SP

TL;DR: 该研究开发了轻量级CNN模型用于心杂音检测，在保持91%准确率的同时大幅减少参数数量，并通过不确定性估计提升临床可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统听诊方法主观性强且依赖专家经验，需要开发AI辅助的心杂音检测系统，特别关注在边缘设备上的资源高效部署。

Method: 比较了三种复杂度递增的CNN架构（轻量、基准、重型），并应用蒙特卡洛Dropout进行不确定性估计，实现选择性分类。

Result: 轻量模型能达到与深层网络相当的准确率（91%），同时参数数量减少两个数量级；结合不确定性分类后灵敏度提升3%。

Conclusion: 证明了在低资源和远程医疗环境中开发计算高效、不确定性感知的心杂音筛查AI系统的可行性。

Abstract: Early and reliable detection of heart murmurs is essential for the timely
diagnosis of cardiovascular diseases, yet traditional auscultation remains
subjective and dependent on expert interpretation. This work investigates
artificial intelligence (AI)-based murmur detection using the CirCor Heart
Sound dataset, with a focus on enabling uncertainty-aware, resource-efficient
deployment on edge devices. Three convolutional neural network (CNN)
architectures of increasing complexity (Light, Baseline, and Heavy) were
compared in terms of classification performance, computational cost, and
suitability for on-device inference. Additionally, Monte Carlo Dropout was
applied for uncertainty estimation, providing confidence measures to improve
prediction sensitivity. Results show that lightweight models can achieve
accuracy comparable to deeper networks (91%) while requiring two orders of
magnitude fewer parameters. Incorporating uncertainty-based selective
classification further improved sensitivity by 3%, enhancing robustness and
clinical reliability. The findings highlight the feasibility of developing
computationally efficient, uncertainty-aware AI systems for heart murmur
screening in low-resource and remote healthcare settings.

</details>


### [12] [Seed-Induced Uniqueness in Transformer Models: Subspace Alignment Governs Subliminal Transfer](https://arxiv.org/abs/2511.01023)
*Ayşe Selin Okatan,Mustafa İlhan Akbaş,Laxima Niure Kandel,Berker Peköz*

Main category: eess.SP

TL;DR: 本文研究了Transformer模型中的潜意识传输现象，发现传输强度取决于特征判别子空间内的对齐程度，而非全局表示相似性。相同种子的学生模型泄漏率更高，而不同种子模型尽管全局CKA>0.9，泄漏率显著降低。


<details>
  <summary>Details</summary>
Motivation: 分析Transformer模型中教师嵌入隐藏特征而学生能够线性解码的现象，挑战传统认为全局表示相似性决定传输性的观点。

Method: 使用解耦公共和私有标签的合成语料库，在匹配和独立随机初始化下蒸馏学生模型，采用子空间级CKA诊断和残差化探针分析。

Result: 相同种子学生泄漏率τ≈0.24，不同种子学生τ≈0.12-0.13，尽管全局CKA>0.9。安全控制措施能减少泄漏而不影响公共任务性能。

Conclusion: 种子诱导的独特性是一种弹性属性，需要子空间感知的诊断方法来确保安全的多模型部署。

Abstract: We analyze subliminal transfer in Transformer models, where a teacher embeds
hidden traits that can be linearly decoded by a student without degrading
main-task performance. Prior work often attributes transferability to global
representational similarity, typically quantified with Centered Kernel
Alignment (CKA). Using synthetic corpora with disentangled public and private
labels, we distill students under matched and independent random
initializations. We find that transfer strength hinges on alignment within a
trait-discriminative subspace: same-seed students inherit this alignment and
show higher leakage {\tau \approx} 0.24, whereas different-seed
students--despite global CKA > 0.9--exhibit substantially reduced excess
accuracy {\tau \approx} 0.12 - 0.13. We formalize this with subspace-level CKA
diagnostic and residualized probes, showing that leakage tracks alignment
within the trait-discriminative subspace rather than global representational
similarity. Security controls (projection penalty, adversarial reversal,
right-for-the-wrong-reasons regularization) reduce leakage in same-base models
without impairing public-task fidelity. These results establish seed-induced
uniqueness as a resilience property and argue for subspace-aware diagnostics
for secure multi-model deployments.

</details>


### [13] [On the Performance of Tri-Hybrid Beamforming Using Pinching Antennas](https://arxiv.org/abs/2511.01099)
*Zhenqiao Cheng,Chongjun Ouyang,Nicola Marchetti*

Main category: eess.SP

TL;DR: PASS系统通过优化压电天线在介质波导上的位置来重构无线信道，结合三混合波束成形技术提升频谱效率和抗路径损耗能力。


<details>
  <summary>Details</summary>
Motivation: 传统混合波束成形在射频链数量有限时性能受限，PASS系统通过动态重构信道来增强波束成形能力。

Method: 提出PASS使能的三混合波束成形系统，优化数字和模拟域的波束成形器以及PA位置，推导信道容量的闭式上下界。

Result: 数值结果验证了推导边界的紧密度，PASS三混合波束成形在相同射频链数量下相比传统混合波束成形有显著性能增益。

Conclusion: PASS系统通过压电波束成形和三混合波束成形的结合，有效提升了无线通信系统的信道容量和频谱效率。

Abstract: The Pinching-Antenna System (PASS) reconfigures wireless channels through
\emph{pinching beamforming}, in which the active positions of pinching antennas
(PAs) along dielectric waveguides are optimized to shape the radiation pattern.
This article investigates the performance of PASS-enabled tri-hybrid
beamforming, where pinched waveguides are integrated with a hybrid
digital-analog beamformer to mitigate path loss and enhance spectral
efficiency. The channel capacity of the proposed system is characterized by
deriving the optimal tri-hybrid beamformer at both the digital and analog
domains, as well as the optimal placement of PAs. Closed-form upper and lower
bounds of the channel capacity are obtained, leading to a capacity scaling law
with respect to the number of PAs. Numerical results verify the tightness of
the derived bounds and demonstrate that applying PASS to tri-hybrid beamforming
yields a significant performance gain over conventional hybrid beamforming
under the same number of radio-frequency chains.

</details>


### [14] [Hi-WaveTST: A Hybrid High-Frequency Wavelet-Transformer for Time-Series Classification](https://arxiv.org/abs/2511.01254)
*Huseyin Goksu*

Main category: eess.SP

TL;DR: Hi-WaveTST是一种混合架构，通过添加可学习的高频小波特征流来增强传统时间序列分类Transformer，在UCI-HAR数据集上显著优于SOTA的PatchTST模型。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer时间序列分类方法（如PatchTST）仅关注原始时间块间关系，忽略了关键的高频信息，这些信息对时间动态具有补充作用。

Method: 提出混合架构Hi-WaveTST，在原始时间块基础上添加可学习的高频小波特征流，使用深度小波包分解和可学习广义均值池化层提取特征。

Result: 在UCI-HAR数据集上达到93.38%的平均准确率（±0.0043），显著优于PatchTST基线（92.59% ±0.0039）。

Conclusion: 混合架构、深度高频小波分解和可学习GeM池化都是实现SOTA性能的关键组件，证明了高频信息对时间序列分类的重要性。

Abstract: Transformers have become state-of-the-art (SOTA) for time-series
classification, with models like PatchTST demonstrating exceptional
performance. These models rely on patching the time series and learning
relationships between raw temporal data blocks. We argue that this approach is
blind to critical, non-obvious high-frequency information that is complementary
to the temporal dynamics. In this letter, we propose Hi-WaveTST, a novel Hybrid
architecture that augments the original temporal patch with a learnable,
High-Frequency wavelet feature stream. Our wavelet stream uses a deep Wavelet
Packet Decomposition (WPD) on each patch and extracts features using a
learnable Generalized Mean (GeM) pooling layer. On the UCI-HAR benchmark
dataset, our hybrid model achieves a mean accuracy of 93.38 percent plus-minus
0.0043, significantly outperforming the SOTA PatchTST baseline (92.59 percent
plus-minus 0.0039). A comprehensive ablation study proves that every component
of our design-the hybrid architecture, the deep high-frequency wavelet
decomposition, and the learnable GeM pooling-is essential for this
state-of-the-art performance.

</details>


### [15] [Classification of motor faults based on transmission coefficient and reflection coefficient of omni-directional antenna using DCNN](https://arxiv.org/abs/2511.01371)
*Sagar Dutta,Banani Basu,Fazal Ahmed Talukdar*

Main category: eess.SP

TL;DR: 提出了一种基于天线反射系数S11和传输系数S21的感应电机故障分类方法，使用深度卷积神经网络对S参数频谱图进行分析，最高达到100%分类准确率。


<details>
  <summary>Details</summary>
Motivation: 感应电机是工业领域最常用的旋转电机，需要有效的故障检测方法来提高系统可靠性。

Method: 利用天线的S11和S21参数，将其频谱图作为特征，应用深度卷积神经网络进行故障分类。

Result: 单独使用S11达到93%准确率，单独使用S21达到98.1%准确率，同时使用两者达到100%准确率。

Conclusion: 基于天线S参数的深度学习方法能够有效识别感应电机故障，且双参数组合可达到完美分类效果。

Abstract: The most commonly used electrical rotary machines in the field are induction
machines. In this paper, we propose an antenna based approach for the
classification of motor faults in induction motors using the reflection
coefficient S11 and the transmission coefficient S21 of the antenna. The
spectrograms of S11 and S21 are seen to possess unique signatures for various
fault conditions that are used for the classification. To learn the required
characteristics and classification boundaries, deep convolution neural network
(DCNN) is applied to the spectrogram of the S-parameter. DCNN has been found to
reach classification accuracy 93% using S11, 98.1% using S21 and 100% using
both S11 and S21. The effect of antenna operating frequency, its location and
duration of signal on the classification accuracy is also presented and
discussed.

</details>


### [16] [CRMD: Complex Robust Modal Decomposition](https://arxiv.org/abs/2511.01398)
*Wang Hao,Kuang Zhang,Hou Chengyu,Tan Chenxing,Cui Weiming,Fu Weifeng,Yao Xinran*

Main category: eess.SP

TL;DR: 本文提出了一种复数域的鲁棒模态分解方法，作为原始实数域RMD的自然扩展，在多个实际应用中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 相比实值信号，复值信号能够更直观地表示真实物理系统的相位信息，在科学和工程领域具有广泛应用价值。

Method: 重新推导复数域RMD的数学原理，并开发了针对复数域的算法版本。

Result: 在合成仿真数据集和多个真实世界数据集上的实验表明，所提出的复数域鲁棒模态分解方法在各种应用中显著提高了性能。

Conclusion: 复数域RMD是实数域RMD的有效扩展，在毫米波雷达生理信号检测、故障轴承检测、射频无人机识别和WiFi CSI呼吸检测等应用中表现出优越性能。

Abstract: Compared to real-valued signals, complex-valued signals provide a unique and
intuitive representation of the phase of real physical systems and processes,
which holds fundamental significance and is widely applied across many fields
of science and engineering. In this paper, we propose a robust modal
decomposition (RMD) in the complex domain as a natural and general extension of
the original real-valued RMD. We revisit and derive the mathematical principles
of RMD in the complex domain, and develop an algorithmic version tailored for
this domain. Extensive experiments are conducted on synthetic simulation
datasets and real-world datasets from diverse fields, including a
millimeter-wave radar physiological signal detection dataset, a faulty bearing
dataset, a radio-frequency unmanned aerial vehicle identification dataset, and
a WiFi CSI-based respiration detection dataset. The results demonstrate that
the proposed complex-domain robust modal decomposition significantly improves
performance across these various applications.

</details>


### [17] [MM-2FSK: Multimodal Frequency Shift Keying for Ultra-Efficient and Robust High-Resolution MIMO Radar Imaging](https://arxiv.org/abs/2511.01405)
*Vanessa Wirth,Johanna Bräunig,Martin Vossiek,Tim Weyrich,Marc Stamminger*

Main category: eess.SP

TL;DR: 提出MM-2FSK方法，结合光学深度传感器提供深度先验，使毫米波MIMO雷达仅使用少量频率即可实现高帧率三维成像，克服传统方法对目标位置先验知识和深度范围的限制。


<details>
  <summary>Details</summary>
Motivation: 解决MIMO雷达在动态场景中因信号处理算法限制导致的高计算强度和大带宽需求问题，这些限制影响了传感器的捕获速率。

Method: 扩展先前工作到多模态领域，使用辅助光学深度传感获取深度先验，结合少量频率的毫米波MIMO雷达实现高帧率捕获。

Result: 在已知地面真实几何形状的各种目标物体上评估，该方法在深度质量方面表现出优越性能，能够与使用多频率的耗时资源密集型测量相媲美。

Conclusion: MM-2FSK方法通过多模态融合成功实现了高帧率三维成像，仅需少量频率即可达到与多频率方法相当的深度质量。

Abstract: Accurate reconstruction of static and rapidly moving targets demands
three-dimensional imaging solutions with high temporal and spatial resolution.
Radar sensors are a promising sensing modality because of their fast capture
rates and their independence from lighting conditions. To achieve high spatial
resolution, MIMO radars with large apertures are required. Yet, they are
infrequently used for dynamic scenarios due to significant limitations in
signal processing algorithms. These limitations impose substantial hardware
constraints due to their computational intensity and reliance on large signal
bandwidths, ultimately restricting the sensor's capture rate.
  One solution of previous work is to use few frequencies only, which enables
faster capture and requires less computation; however, this requires coarse
knowledge of the target's position and works in a limited depth range only. To
address these challenges, we extend previous work into the multimodal domain
with MM-2FSK, which leverages an assistive optical depth sensing modality to
obtain a depth prior, enabling high framerate capture with only few
frequencies.
  We evaluate our method using various target objects with known ground truth
geometry that is spatially registered to real millimeter-wave MIMO radar
measurements. Our method demonstrates superior performance in terms of depth
quality, being able to compete with the time- and resource-intensive
measurements with many frequencies.

</details>


### [18] [AoI-Aware Machine Learning for Constrained Multimodal Sensing-Aided Communications](https://arxiv.org/abs/2511.01406)
*Abolfazl Zakeri,Nhan Thanh Nguyen,Ahmed Alkhateeb,Markku Juntti*

Main category: eess.SP

TL;DR: 提出了一种结合多模态感知和波束预测的框架，在平均感知率约束下通过深度强化学习和神经网络优化波束训练性能。


<details>
  <summary>Details</summary>
Motivation: 环境感知数据可以增强通信波束训练并减少开销，但推理过程中新鲜感知数据的可用性可能受限，需要建立现实的多模态感知模型。

Method: 结合深度Q网络（DQN）和神经网络波束预测器，DQN决定感知决策，神经网络预测最佳波束。引入信息年龄（AoI）概念，使用Lyapunov优化设计奖励函数以强制执行平均感知约束。

Result: 在真实世界数据集上的仿真结果显示，在严格感知约束下，AoI感知训练使top-1和top-3推理准确率分别提高了44.16%和52.96%。

Conclusion: AoI感知训练能显著提升波束预测性能，但随着感知约束的放宽，性能增益会逐渐减小。

Abstract: Using environmental sensory data can enhance communications beam training and
reduce its overhead compared to conventional methods. However, the availability
of fresh sensory data during inference may be limited due to sensing
constraints or sensor failures, necessitating a realistic model for multimodal
sensing. This paper proposes a joint multimodal sensing and beam prediction
framework that operates under a constraint on the average sensing rate, i.e.,
how often fresh sensory data should be obtained. The proposed method combines
deep reinforcement learning, i.e., a deep Q-network (DQN), with a neural
network (NN)-based beam predictor. The DQN determines the sensing decisions,
while the NN predicts the best beam from the codebook. To capture the effect of
limited fresh data during inference, the age of information (AoI) is
incorporated into the training of both the DQN and the beam predictor. Lyapunov
optimization is employed to design a reward function that enforces the average
sensing constraint. Simulation results on a real-world dataset show that
AoI-aware training improves top-1 and top-3 inference accuracy by 44.16% and
52.96%, respectively, under a strict sensing constraint. The performance gain,
however, diminishes as the sensing constraint is relaxed.

</details>


### [19] [Robust Radar Mounting Angle Estimation in Operational Driving Conditions](https://arxiv.org/abs/2511.01431)
*Simin Zhu,Satish Ravindran,Lihui Chen,Alexander Yarovoy,Francesco Fioranelli*

Main category: eess.SP

TL;DR: 提出了一种结合雷达和IMU数据的信号处理流程，用于在真实驾驶场景中准确估计毫米波车载雷达的安装角度。该方法使用神经网络处理稀疏噪声雷达测量，并引入测量模型校正IMU误差，最终通过车辆运动学计算雷达安装角度。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂真实交通条件下准确估计车载雷达安装角度的挑战，避免对受控环境、专用目标或特殊驾驶路线的依赖。

Method: 结合雷达和IMU数据，使用神经网络处理雷达测量并拒绝移动物体检测，引入测量模型校正IMU偏差和比例因子误差，利用车辆运动学从估计的雷达运动和车辆偏航率计算安装角度。

Result: 在RadarScenes数据集（超过79公里真实驾驶）上验证，达到最先进的精度和鲁棒性，约25秒驾驶即可获得可靠估计。

Conclusion: 这是首个证明能在复杂真实交通条件下准确估计车载雷达安装角度的研究，无需受控环境、专用目标或特殊驾驶路线。

Abstract: The robust estimation of the mounting angle for millimeter-wave automotive
radars installed on moving vehicles is investigated. We propose a novel signal
processing pipeline that combines radar and inertial measurement unit (IMU)
data to achieve accurate and reliable performance in realistic driving
scenarios. Unlike previous studies, the method employs neural networks to
process sparse and noisy radar measurements, reject detections from moving
objects, and estimate radar motion. In addition, a measurement model is
introduced to correct IMU bias and scale factor errors. Using vehicle
kinematics, the radar mounting angle is then computed from the estimated radar
motion and the vehicle's yaw rate. To benchmark performance, the proposed
approach is comprehensively compared with two problem formulations and four
estimation techniques reported in the literature. Validation is carried out on
the challenging RadarScenes dataset, covering over 79 km of real-world driving.
Results show that the proposed method achieves state-of-the-art accuracy and
robustness, with reliable estimates obtained within approximately 25 seconds of
driving. To the best of our knowledge, this is the first study to demonstrate
that automotive radar mounting angles can be accurately estimated in complex,
real traffic conditions, without requiring controlled environments, dedicated
targets, or specially designed driving routes.

</details>


### [20] [Optimizing Movable Antenna Position and Transmissive RIS Phase for Efficient Base Station Design](https://arxiv.org/abs/2511.01575)
*Marjan Boloori,Chu Li,Aydin Sezgin*

Main category: eess.SP

TL;DR: 提出了一种将可移动天线与透射式可重构智能表面在近场协同集成的新型基站架构，通过联合优化天线位置和TRIS相位调整来增强信号强度，即使在低分辨率相位偏移器下也能补偿相位量化损失。


<details>
  <summary>Details</summary>
Motivation: 可移动天线和透射式可重构智能表面是两种能显著提升无线通信系统灵活性的创新技术，需要探索它们协同工作的潜力以实现紧凑高效的6G基站设计。

Method: 提出MA-TRIS集成架构，在近场环境中联合优化天线定位和TRIS相位调整，利用该框架系统评估TRIS尺寸和天线位置对系统性能的影响。

Result: 天线移动性提供了额外的自由度来增强所需信号并实现更高信噪比，特别是与TRIS能力结合时效果更显著，即使在1-2位低分辨率相位偏移器下也能获得良好性能。

Conclusion: MA-TRIS集成为紧凑型6G基站提供了一条成本效益高且节能的路径，将硬件简单性与强大的性能增益相结合。

Abstract: Movable antennas (MA) and transmissive reconfigurable intelligent surfaces
(TRIS) represent two innovative technologies that significantly enhance the
flexibility of wireless communication systems. In this paper, we propose a
novel and compact base station architecture that synergistically integrates a
movable antenna with a transmissive RIS in the near field, enabling joint
optimization of antenna positioning and TRIS phase adjustments. The proposed
model compensates for phase quantization loss and significantly enhances signal
strength, even with low-resolution (1-2 bit) phase shifters. Leveraging this
framework, we systematically evaluate system performance as a function of TRIS
size and antenna placement. Our results indicate that antenna mobility provides
an additional degree of freedom to enhance the desired signal and achieve a
higher SNR, particularly when combined with TRIS capabilities. These findings
demonstrate that MA-TRIS integration offers a cost-effective and
energy-efficient pathway toward compact 6G base stations, combining hardware
simplicity with strong performance gains.

</details>


### [21] [Clutter Suppression in Bistatic ISAC with Joint Angle and Doppler Estimation](https://arxiv.org/abs/2511.01599)
*M. Ertug Pihtili,Julia Equi,Ossi Kaltiokallio,Jukka Talvitie,Elena Simona Lohan,Ertugrul Basar,Mikko Valkama*

Main category: eess.SP

TL;DR: 提出了一种基于2D-rootMUSIC的静态背景杂波抑制算法，用于提升6G网络中集成感知与通信系统的感知精度。


<details>
  <summary>Details</summary>
Motivation: 6G网络在更高频率和更大天线阵列下可实现更精确的感知能力，但杂波会显著降低ISAC系统的感知精度，因此需要有效的杂波抑制方法。

Method: 开发了一种基于二维根多重信号分类的算法，专门用于抑制静态背景杂波。

Result: 计算机仿真结果表明，该方法能有效抑制强背景杂波，提供准确的参数估计性能，并在信杂噪比方面显著优于现有基准方法。

Conclusion: 所提出的2D-rootMUSIC算法为ISAC系统中的杂波抑制问题提供了有效的解决方案，提升了系统性能。

Abstract: The coexistence of radar and communications in wireless systems marks a
paradigm shift for the sixth-generation (6G) networks. As 6G systems are
expected to operate at higher frequencies and employ larger antenna arrays than
fifth-generation (5G) systems, they can also enable more accurate sensing
capabilities. To this end, the integrated sensing and communication (ISAC)
paradigm aims to unify the physical and radio frequency (RF) domains by
introducing the sensing functionality into the communication network. However,
the clutter poses a challenge, as it can significantly degrade the sensing
accuracy in ISAC systems. This paper presents a novel two-dimensional root
multiple signal classification (2D-rootMUSIC)-based algorithm for static
background clutter suppression. Computer simulation results indicate that the
proposed method effectively mitigates the strong background clutter, yields
accurate parameter estimation performance, and offers a notable improvement in
the signal-to-clutter-and-noise ratio (SCNR), while outperforming the prior-art
benchmark methods.

</details>


### [22] [AnyPPG: An ECG-Guided PPG Foundation Model Trained on Over 100,000 Hours of Recordings for Holistic Health Profiling](https://arxiv.org/abs/2511.01747)
*Guangkun Nie,Gongzheng Tang,Yujie Xiao,Jun Li,Shun Huang,Deyun Zhang,Qinghao Zhao,Shenda Hong*

Main category: eess.SP

TL;DR: AnyPPG是一个基于PPG信号的基础模型，通过PPG-ECG生理对齐预训练，在生理分析和多器官疾病诊断任务中表现出色，展示了PPG在全面健康评估中的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有PPG研究受限于标注数据的规模和多样性，限制了模型精度和泛化能力。本研究探索通过基础模型技术实现PPG在整体健康分析中的应用。

Method: 提出AnyPPG基础模型，在大规模多源同步PPG-ECG数据上预训练，通过将PPG和ECG表征对齐到共享空间，从未标记信号中学习生理学意义特征。

Result: 在11个生理分析任务中，AnyPPG在回归和分类任务上分别比次优模型平均提升12.8%和9.1%。在多器官疾病诊断中，13种疾病AUC>0.8，137种>0.7，不仅对心血管疾病有效，对帕金森病(AUC=0.78)和慢性肾病(AUC=0.74)等非心血管疾病也有显著诊断价值。

Conclusion: AnyPPG证明通过PPG-ECG生理对齐训练的基础模型能够产生准确鲁棒的信号表征，强调了PPG作为系统性多器官健康全面评估模态的潜力。

Abstract: Background: Photoplethysmography (PPG) offers a noninvasive and accessible
modality for health monitoring beyond clinical settings. However, existing
studies are limited by the scale and diversity of labeled data, constraining
model accuracy, generalizability, and the exploration of broader applications.
This study investigates the potential of PPG for holistic health profiling
through the integration of foundation model techniques.
  Methods: We present AnyPPG, a PPG foundation model pretrained on large-scale,
multi-source synchronized PPG-ECG data. By aligning PPG and ECG representations
within a shared space, AnyPPG learns physiologically meaningful features from
unlabeled signals. Its capability was further evaluated across a diverse set of
downstream tasks, encompassing both conventional physiological analysis and
comprehensive multi-organ disease diagnosis.
  Results: Across eleven physiological analysis tasks spanning six independent
datasets, AnyPPG achieved state-of-the-art performance, with average
improvements of 12.8% in regression and 9.1% in classification tasks over the
next-best model. In multi-organ disease diagnosis, AnyPPG demonstrated broad
cross-system diagnostic potential. Among 1,014 ICD-10 three-digit disease
categories, 13 achieved an AUC above 0.8 and 137 exceeded 0.7. Beyond strong
performance in cardiovascular diseases such as heart failure, valvular
disorders, and hypertension, AnyPPG also showed substantial diagnostic value
for non-cardiovascular conditions, exemplified by Parkinson's disease (AUC =
0.78) and chronic kidney disease (AUC = 0.74).
  Conclusions: AnyPPG demonstrates that a PPG foundation model trained through
physiological alignment with ECG can produce accurate and robust signal
representations. Building on this capability, it underscores the potential of
PPG as a modality for comprehensive assessment of systemic and multi-organ
health.

</details>


### [23] [On Systematic Performance of 3-D Holographic MIMO: Clarke, Kronecker, and 3GPP Models](https://arxiv.org/abs/2511.01780)
*Quan Gao,Shuai S. A. Yuan,Zhanwen Wang,Wanchen Yang,Chongwen Huang,Xiaoming Chen,Wei E. I. Sha*

Main category: eess.SP

TL;DR: 3D全息MIMO通过体积阵列配置克服传统平面阵列的空间相关性和互耦限制，在3GPP城市宏小区信道中相比2D阵列实现约20%的容量提升。


<details>
  <summary>Details</summary>
Motivation: 传统平面全息MIMO在亚波长间距下存在空间相关性和互耦问题，限制了有效自由度和信道容量，需要新的阵列架构来突破这些限制。

Method: 结合电磁特性（互耦和辐射效率）分析3D阵列，在Clarke、Kronecker和标准化3GPP信道模型下进行系统评估，包括解析推导和全波仿真。

Result: 3D架构实现更高的有效自由度、更窄的波束宽度和显著的容量提升，在0.3λ水平间距下相比2D阵列容量提升约20%。

Conclusion: 3D全息MIMO在现实条件下具有鲁棒性和可扩展性，为6G基站阵列设计提供了从理论可行性到实际部署的桥梁。

Abstract: Holographic multiple-input multiple-output (MIMO) has emerged as a key
enabler for 6G networks, yet conventional planar implementations suffer from
spatial correlation and mutual coupling at sub-wavelength spacing, which
fundamentally limit the effective degrees of freedom (EDOF) and channel
capacity. Three-dimensional (3-D) holographic MIMO offers a pathway to overcome
these constraints by exploiting volumetric array configurations that enlarge
the effective aperture and unlock additional spatial modes. This work presents
the first systematic evaluation that jointly incorporates electromagnetic (EM)
characteristics, such as mutual coupling and radiation efficiency, into the
analysis of 3-D arrays under Clarke, Kronecker, and standardized 3rd Generation
Partnership Project (3GPP) channel models. Analytical derivations and full-wave
simulations demonstrate that 3-D architectures achieve higher EDOF, narrower
beamwidths, and notable capacity improvements compared with planar baselines.
In 3GPP urban macro channels with horizontal element spacing of 0.3 lambda, 3-D
configurations yield approximately 20% capacity improvement over conventional
2-D arrays, confirming the robustness and scalability of volumetric designs
under realistic conditions. These findings bridge the gap between theoretical
feasibility and practical deployment, offering design guidance for
next-generation 6G base station arrays.

</details>


### [24] [Practical Approaches to Quantifying Intra-Pair Skew Impact via Insertion Loss Deviation](https://arxiv.org/abs/2511.01787)
*David Nozadze,Zurab Kiguradze,Amendra Koul,Sayed Ashraf Mamun,Mike Sapozhnikov*

Main category: eess.SP

TL;DR: 提出两种新的互易参数SILD和FOM SILD来量化高速互连中的对内偏斜影响，通过224 Gb/s SerDes测试验证了FOM SILD与误码率的强相关性。


<details>
  <summary>Details</summary>
Motivation: AI工作负载和数据中心需求激增，需要超过200 Gb/s的超高速互连。随着单位间隔缩小，即使几皮秒的对内偏斜也会显著降低SerDes性能，而传统时域和频域方法存在局限性。

Method: 引入两种新的互易参数：偏斜引起的插入损耗偏差(SILD)及其互补品质因数(FOM SILD)，使用224 Gb/s SerDes IP和各种不同对内偏斜的通道进行测量。

Result: FOM SILD与误码率(BER)存在强相关性。当FOM SILD低于0.2-0.3 dB时，BER保持稳定；超过0.3 dB时BER明显增加。对3000多条高速双轴电缆的统计分析显示大多数FOM SILD值小于0.1 dB。

Conclusion: 提出的SILD和FOM SILD指标为高速互连评估提供了实用的量化方法，能够有效评估对内偏斜对信号完整性的影响。

Abstract: The surge in AI workloads and escalating data center requirements have
created demand for ultra-high-speed interconnects exceeding 200 Gb/s. As unit
intervals (UI) shrink, even a few picoseconds of intra-pair skew can
significantly degrade serializer-deserializer (SerDes) performance. To quantify
the impact of intra-pair skew, conventional time-domain methods are often
unreliable for coupled interconnects due to skew variations across voltage
levels, while frequency-domain approaches frequently fail to address
reciprocity and symmetry issues. This can result in channels that meet skew
specifications in one direction but not the other, despite the inherently
reciprocal nature of skew impact. To address these limitations, we introduce
two new reciprocal parameters for quantifying intra-pair skew effects:
Skew-Induced Insertion Loss Deviation (SILD) and its complementary Figure of
Merit (FOM SILD). Measurements conducted using 224 Gb/s SerDes IP and a variety
of channels with different intra-pair skews demonstrate a strong correlation
between FOM SILD and bit error rate (BER). Results show that when FOM SILD is
below 0.2-0.3 dB, BER remains stable, indicating minimal signal integrity
degradation; however, BER increases noticeably as FOM SILD exceeds 0.3 dB.
Statistical analysis across more than 3,000 high-speed twinax cables reveals
that the majority exhibit FOM SILD values less than 0.1 dB, underscoring the
practical relevance of the proposed metrics for high-speed interconnect
assessment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [VRScout: Towards Real-Time, Autonomous Testing of Virtual Reality Games](https://arxiv.org/abs/2511.00002)
*Yurun Wu,Yousong Sun,Burkhard Wunsche,Jia Wang,Elliott Wen*

Main category: cs.LG

TL;DR: VRScout是一个基于深度学习的VR游戏自动化测试代理，能够自主导航VR环境并以人类方式实时交互，解决了传统人工测试无法规模化的问题。


<details>
  <summary>Details</summary>
Motivation: VR内容的质量、安全和适用性保证面临挑战，传统人工测试劳动密集且无法跟上行业快速增长，而现有自动化测试方法难以适应VR的高维感官输入和实时性能要求。

Method: 使用增强型动作分块变换器从人类演示中学习，预测多步动作序列，引入动态可调滑动视界来平衡响应性和精确度。

Result: 在商业VR游戏上评估显示，VRScout仅用有限训练数据就达到专家级性能，并在消费级硬件上保持60FPS的实时推理。

Conclusion: VRScout为VR游戏自动化测试提供了一个实用且可扩展的框架，可直接应用于质量保证和安全审计。

Abstract: Virtual Reality (VR) has rapidly become a mainstream platform for gaming and
interactive experiences, yet ensuring the quality, safety, and appropriateness
of VR content remains a pressing challenge. Traditional human-based quality
assurance is labor-intensive and cannot scale with the industry's rapid growth.
While automated testing has been applied to traditional 2D and 3D games,
extending it to VR introduces unique difficulties due to high-dimensional
sensory inputs and strict real-time performance requirements. We present
VRScout, a deep learning-based agent capable of autonomously navigating VR
environments and interacting with virtual objects in a human-like and real-time
manner. VRScout learns from human demonstrations using an enhanced Action
Chunking Transformer that predicts multi-step action sequences. This enables
our agent to capture higher-level strategies and generalize across diverse
environments. To balance responsiveness and precision, we introduce a
dynamically adjustable sliding horizon that adapts the agent's temporal context
at runtime. We evaluate VRScout on commercial VR titles and show that it
achieves expert-level performance with only limited training data, while
maintaining real-time inference at 60 FPS on consumer-grade hardware. These
results position VRScout as a practical and scalable framework for automated VR
game testing, with direct applications in both quality assurance and safety
auditing.

</details>


### [26] [A generative adversarial network optimization method for damage detection and digital twinning by deep AI fault learning: Z24 Bridge structural health monitoring benchmark validation](https://arxiv.org/abs/2511.00099)
*Marios Impraimakis,Evangelia Nektaria Palkanoglou*

Main category: cs.LG

TL;DR: 提出一种基于条件标签生成对抗网络的损伤检测和数字孪生方法，无需系统健康状态先验信息，在Z24桥梁基准测试中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于AI的数字孪生方法在测量数据少、物理知识缺失或损伤状态未知时预测效果不佳，需要开发无需先验信息的无监督框架。

Method: 使用条件标签生成对抗网络，将不同损伤级别的测量数据作为输入，强制模型条件收敛到不同损伤状态，通过比较收敛分数识别不同损伤状态。

Result: 该方法能准确捕获健康测量中的损伤，为基于振动的系统级监测和可扩展基础设施韧性提供了强大工具。

Conclusion: 所提出的无监督框架在损伤检测和数字孪生方面优于现有方法，特别适用于实际工程应用中的不确定性场景。

Abstract: The optimization-based damage detection and damage state digital twinning
capabilities are examined here of a novel conditional-labeled generative
adversarial network methodology. The framework outperforms current approaches
for fault anomaly detection as no prior information is required for the health
state of the system: a topic of high significance for real-world applications.
Specifically, current artificial intelligence-based digital twinning approaches
suffer from the uncertainty related to obtaining poor predictions when a low
number of measurements is available, physics knowledge is missing, or when the
damage state is unknown. To this end, an unsupervised framework is examined and
validated rigorously on the benchmark structural health monitoring measurements
of Z24 Bridge: a post-tensioned concrete highway bridge in Switzerland. In
implementing the approach, firstly, different same damage-level measurements
are used as inputs, while the model is forced to converge conditionally to two
different damage states. Secondly, the process is repeated for a different
group of measurements. Finally, the convergence scores are compared to identify
which one belongs to a different damage state. The process for both
healthy-to-healthy and damage-to-healthy input data creates, simultaneously,
measurements for digital twinning purposes at different damage states, capable
of pattern recognition and machine learning data generation. Further to this
process, a support vector machine classifier and a principal component analysis
procedure is developed to assess the generated and real measurements of each
damage category, serving as a secondary new dynamics learning indicator in
damage scenarios. Importantly, the approach is shown to capture accurately
damage over healthy measurements, providing a powerful tool for vibration-based
system-level monitoring and scalable infrastructure resilience.

</details>


### [27] [Feature-Guided SAE Steering for Refusal-Rate Control using Contrasting Prompts](https://arxiv.org/abs/2511.00029)
*Samaksh Bhargav,Zining Zhu*

Main category: cs.LG

TL;DR: 使用稀疏自编码器(SAE)进行特征选择和定向引导，在Llama-3 8B模型上实现了安全性能提升18.9%同时效用提升11.1%，克服了传统安全-效用权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全部署方法需要调整模型权重且过程昂贵，而现有SAE方法缺乏系统的特征选择方法和安全-效用权衡的量化评估。

Method: 使用对比提示方法和AI生成提示数据集，通过稀疏自编码器识别最佳引导特征，在Llama-3 8B上进行不同引导特征和强度的测试。

Result: 该方法实现了安全性能提升18.9%同时效用提升11.1%，证明了当通过原则性选择方法识别最优特征时，定向SAE引导可以克服传统安全-效用权衡。

Conclusion: 基于稀疏自编码器的定向引导方法能够在不牺牲效用的前提下显著提升LLM的安全性，为LLM安全部署提供了有效解决方案。

Abstract: Large Language Model (LLM) deployment requires guiding the LLM to recognize
and not answer unsafe prompts while complying with safe prompts. Previous
methods for achieving this require adjusting model weights along with other
expensive procedures. While recent advances in Sparse Autoencoders (SAEs) have
enabled interpretable feature extraction from LLMs, existing approaches lack
systematic feature selection methods and principled evaluation of
safety-utility tradeoffs. We explored using different steering features and
steering strengths using Sparse Auto Encoders (SAEs) to provide a solution.
Using an accurate and innovative contrasting prompt method with the
AI-Generated Prompts Dataset from teknium/OpenHermes-2p5-Mistral-7B and Air
Bench eu-dataset to efficiently choose the best features in the model to steer,
we tested this method on Llama-3 8B. We conclude that using this method, our
approach achieves an 18.9% improvement in safety performance while
simultaneously increasing utility by 11.1%, demonstrating that targeted SAE
steering can overcome traditional safety-utility tradeoffs when optimal
features are identified through principled selection methods.

</details>


### [28] [Deep recurrent-convolutional neural network learning and physics Kalman filtering comparison in dynamic load identification](https://arxiv.org/abs/2511.00100)
*Marios Impraimakis*

Main category: cs.LG

TL;DR: 比较门控循环单元、长短时记忆网络和卷积神经网络在动态结构载荷识别中的性能，并与基于物理的残差卡尔曼滤波器进行对比分析。


<details>
  <summary>Details</summary>
Motivation: 解决土木工程应用中因测试数据量少或结构模型不可识别导致的动态载荷识别不确定性问题。

Method: 使用三种神经网络模型和残差卡尔曼滤波器，分别在模拟结构、加州建筑地震响应和IASC-ASCE基准问题上进行测试。

Result: 不同方法在不同载荷场景下表现各异，RKF在物理参数可识别情况下优于神经网络。

Conclusion: 各种方法在不同载荷识别场景中各有优势，需要根据具体应用场景选择合适的方法。

Abstract: The dynamic structural load identification capabilities of the gated
recurrent unit, long short-term memory, and convolutional neural networks are
examined herein. The examination is on realistic small dataset training
conditions and on a comparative view to the physics-based residual Kalman
filter (RKF). The dynamic load identification suffers from the uncertainty
related to obtaining poor predictions when in civil engineering applications
only a low number of tests are performed or are available, or when the
structural model is unidentifiable. In considering the methods, first, a
simulated structure is investigated under a shaker excitation at the top floor.
Second, a building in California is investigated under seismic base excitation,
which results in loading for all degrees of freedom. Finally, the International
Association for Structural Control-American Society of Civil Engineers
(IASC-ASCE) structural health monitoring benchmark problem is examined for
impact and instant loading conditions. Importantly, the methods are shown to
outperform each other on different loading scenarios, while the RKF is shown to
outperform the networks in physically parametrized identifiable cases.

</details>


### [29] [Probing Knowledge Holes in Unlearned LLMs](https://arxiv.org/abs/2511.00030)
*Myeongseob Ko,Hoang Anh Just,Charles Fleming,Ming Jin,Ruoxi Jia*

Main category: cs.LG

TL;DR: 机器遗忘技术虽然能有效移除预训练模型中的不良知识，但会意外产生"知识空洞"——标准基准测试无法检测到的良性知识损失。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘技术在移除不良内容时可能无意中损害其他良性知识，而标准基准测试无法充分揭示这种隐藏的知识损失问题。

Method: 提出了一个测试用例生成框架，探索被遗忘内容附近区域和更广泛的潜在失败区域，以检测知识空洞。

Result: 评估显示机器遗忘存在显著隐藏成本：高达98.7%的测试用例在遗忘模型上产生无关或荒谬的响应，而这些在预训练模型中是可回答的。

Conclusion: 需要重新思考评估机器遗忘中知识保存的传统方法，超越标准静态基准测试。

Abstract: Machine unlearning has emerged as a prevalent technical solution for
selectively removing unwanted knowledge absorbed during pre-training, without
requiring full retraining. While recent unlearning techniques can effectively
remove undesirable content without severely compromising performance on
standard benchmarks, we find that they may inadvertently create ``knowledge
holes'' -- unintended losses of benign knowledge that standard benchmarks fail
to capture. To probe where unlearned models reveal knowledge holes, we propose
a test case generation framework that explores both immediate neighbors of
unlearned content and broader areas of potential failures. Our evaluation
demonstrates significant hidden costs of unlearning: up to 98.7\% of the test
cases yield irrelevant or nonsensical responses from unlearned models, despite
being answerable by the pretrained model. These findings necessitate rethinking
the conventional approach to evaluating knowledge preservation in unlearning,
moving beyond standard, static benchmarks.

</details>


### [30] [LC-Opt: Benchmarking Reinforcement Learning and Agentic AI for End-to-End Liquid Cooling Optimization in Data Centers](https://arxiv.org/abs/2511.00116)
*Avisek Naug,Antonio Guillen,Vineet Kumar,Scott Greenwood,Wesley Brewer,Sahand Ghorbanpour,Ashwin Ramesh Babu,Vineet Gundecha,Ricardo Luna Gutierrez,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: LC-Opt是一个可持续液体冷却基准环境，用于强化学习控制策略，优化高性能计算系统的能源效率。


<details>
  <summary>Details</summary>
Motivation: 随着AI工作负载增加，高密度数据中心需要液体冷却进行热管理，而基于机器学习的控制器对于提高能源效率和可靠性至关重要。

Method: 基于橡树岭国家实验室Frontier超级计算机冷却系统的高保真数字孪生，提供详细的Modelica端到端模型，通过Gymnasium接口让RL代理优化热控制参数。

Result: 创建了多目标实时优化挑战，平衡局部热调节和全局能源效率，并支持额外组件如热回收单元。

Conclusion: LC-Opt使ML社区、运营商和供应商能够开发可持续的数据中心液体冷却控制解决方案，促进用户信任和系统管理简化。

Abstract: Liquid cooling is critical for thermal management in high-density data
centers with the rising AI workloads. However, machine learning-based
controllers are essential to unlock greater energy efficiency and reliability,
promoting sustainability. We present LC-Opt, a Sustainable Liquid Cooling (LC)
benchmark environment, for reinforcement learning (RL) control strategies in
energy-efficient liquid cooling of high-performance computing (HPC) systems.
Built on the baseline of a high-fidelity digital twin of Oak Ridge National
Lab's Frontier Supercomputer cooling system, LC-Opt provides detailed
Modelica-based end-to-end models spanning site-level cooling towers to data
center cabinets and server blade groups. RL agents optimize critical thermal
controls like liquid supply temperature, flow rate, and granular valve
actuation at the IT cabinet level, as well as cooling tower (CT) setpoints
through a Gymnasium interface, with dynamic changes in workloads. This
environment creates a multi-objective real-time optimization challenge
balancing local thermal regulation and global energy efficiency, and also
supports additional components like a heat recovery unit (HRU). We benchmark
centralized and decentralized multi-agent RL approaches, demonstrate policy
distillation into decision and regression trees for interpretable control, and
explore LLM-based methods that explain control actions in natural language
through an agentic mesh architecture designed to foster user trust and simplify
system management. LC-Opt democratizes access to detailed, customizable liquid
cooling models, enabling the ML community, operators, and vendors to develop
sustainable data center liquid cooling control solutions.

</details>


### [31] [From Uniform to Adaptive: General Skip-Block Mechanisms for Efficient PDE Neural Operators](https://arxiv.org/abs/2511.00032)
*Lei Liu,Zhongyi Yu,Hong Wang,Huanshuo Dong,Haiyang Xin,Hongwei Zhao,Bin Li*

Main category: cs.LG

TL;DR: 提出Skip-Block Routing(SBR)框架，用于Transformer类神经算子，通过路由机制学习token复杂度排名，在推理时根据复杂度动态分配计算资源，减少约50%FLOPs，实现2倍推理加速且不损失精度。


<details>
  <summary>Details</summary>
Motivation: 当前神经算子在处理大规模工程问题时存在显著计算开销，且模型对所有区域施加统一计算成本，而物理场复杂度差异很大，这种不匹配导致效率低下。

Method: SBR框架包含路由机制学习token复杂度和排名，在推理时根据排名决定后续层传递的token数量，使模型对复杂区域分配更多计算能力。

Result: SBR可集成到多种神经算子中，减少约50%FLOPs，实现2倍推理加速，同时保持精度不损失。

Conclusion: SBR是一种通用框架，通过动态分配计算资源有效解决了神经算子在处理不同复杂度物理场时的效率问题。

Abstract: In recent years, Neural Operators(NO) have gradually emerged as a popular
approach for solving Partial Differential Equations (PDEs). However, their
application to large-scale engineering tasks suffers from significant
computational overhead. And the fact that current models impose a uniform
computational cost while physical fields exhibit vastly different complexities
constitutes a fundamental mismatch, which is the root of this inefficiency. For
instance, in turbulence flows, intricate vortex regions require deeper network
processing compared to stable flows. To address this, we introduce a framework:
Skip-Block Routing (SBR), a general framework designed for Transformer-based
neural operators, capable of being integrated into their multi-layer
architectures. First, SBR uses a routing mechanism to learn the complexity and
ranking of tokens, which is then applied during inference. Then, in later
layers, it decides how many tokens are passed forward based on this ranking.
This way, the model focuses more processing capacity on the tokens that are
more complex. Experiments demonstrate that SBR is a general framework that
seamlessly integrates into various neural operators. Our method reduces
computational cost by approximately 50% in terms of Floating Point Operations
(FLOPs), while still delivering up to 2x faster inference without sacrificing
accuracy.

</details>


### [32] [DCcluster-Opt: Benchmarking Dynamic Multi-Objective Optimization for Geo-Distributed Data Center Workloads](https://arxiv.org/abs/2511.00117)
*Antonio Guillen-Perez,Avisek Naug,Vineet Gundecha,Sahand Ghorbanpour,Ricardo Luna Gutierrez,Ashwin Ramesh Babu,Munther Salim,Shubhanker Banerjee,Eoin H. Oude Essink,Damien Fay,Soumyendu Sarkar*

Main category: cs.LG

TL;DR: DCcluster-Opt是一个开源的高保真模拟基准，用于可持续的、地理时间任务调度，结合真实世界数据集和物理信息模型，加速地理分布式数据中心可持续计算解决方案的开发。


<details>
  <summary>Details</summary>
Motivation: 大规模AI日益增长的能源需求和碳足迹需要在全球分布式数据中心进行智能工作负载管理，但缺乏能够真实捕捉环境因素、数据中心物理和网络动态相互作用的基准限制了进展。

Method: 结合精选的真实世界数据集（AI工作负载轨迹、电网碳强度、电力市场、天气、云传输成本等）与物理信息模型的数据中心操作，提供模块化奖励系统和Gymnasium API，支持可重复的ML研究。

Result: 提供了一个具有挑战性的调度问题环境，顶层协调代理必须动态重新分配或延迟任务，以优化碳排放、能源成本、服务等级协议和用水等多个目标。

Conclusion: DCcluster-Opt通过提供真实、可配置且可访问的测试平台，加速了地理分布式数据中心下一代可持续计算解决方案的开发和验证。

Abstract: The increasing energy demands and carbon footprint of large-scale AI require
intelligent workload management in globally distributed data centers. Yet
progress is limited by the absence of benchmarks that realistically capture the
interplay of time-varying environmental factors (grid carbon intensity,
electricity prices, weather), detailed data center physics (CPUs, GPUs, memory,
HVAC energy), and geo-distributed network dynamics (latency and transmission
costs). To bridge this gap, we present DCcluster-Opt: an open-source,
high-fidelity simulation benchmark for sustainable, geo-temporal task
scheduling. DCcluster-Opt combines curated real-world datasets, including AI
workload traces, grid carbon intensity, electricity markets, weather across 20
global regions, cloud transmission costs, and empirical network delay
parameters with physics-informed models of data center operations, enabling
rigorous and reproducible research in sustainable computing. It presents a
challenging scheduling problem where a top-level coordinating agent must
dynamically reassign or defer tasks that arrive with resource and service-level
agreement requirements across a configurable cluster of data centers to
optimize multiple objectives. The environment also models advanced components
such as heat recovery. A modular reward system enables an explicit study of
trade-offs among carbon emissions, energy costs, service level agreements, and
water use. It provides a Gymnasium API with baseline controllers, including
reinforcement learning and rule-based strategies, to support reproducible ML
research and a fair comparison of diverse algorithms. By offering a realistic,
configurable, and accessible testbed, DCcluster-Opt accelerates the development
and validation of next-generation sustainable computing solutions for
geo-distributed data centers.

</details>


### [33] [Neural Architecture Search for global multi-step Forecasting of Energy Production Time Series](https://arxiv.org/abs/2511.00035)
*Georg Velev,Stefan Lessmann*

Main category: cs.LG

TL;DR: 提出基于神经架构搜索(NAS)的自动化框架，用于发现平衡计算效率、预测性能和泛化能力的时序模型，以解决能源生产短期预测中的挑战。


<details>
  <summary>Details</summary>
Motivation: 能源行业需要准确且高效的短期预测方法，但手动配置复杂模型耗时且易出错，同时需要考虑时序数据的动态特性和对未见数据的泛化能力。

Method: 设计基于NAS的框架，构建仅包含高效组件的搜索空间，提出考虑时序上下文泛化性能和搜索空间探索的新目标函数。

Result: 在能源生产时序数据上的实验表明，NAS发现的轻量级架构集成在效率和准确性方面均优于Transformer等最先进技术和预训练预测模型。

Conclusion: NAS框架能够自动发现高效且准确的能源预测模型，解决了传统方法在计算效率和泛化能力方面的局限性。

Abstract: The dynamic energy sector requires both predictive accuracy and runtime
efficiency for short-term forecasting of energy generation under operational
constraints, where timely and precise predictions are crucial. The manual
configuration of complex methods, which can generate accurate global multi-step
predictions without suffering from a computational bottleneck, represents a
procedure with significant time requirements and high risk for human-made
errors. A further intricacy arises from the temporal dynamics present in
energy-related data. Additionally, the generalization to unseen data is
imperative for continuously deploying forecasting techniques over time. To
overcome these challenges, in this research, we design a neural architecture
search (NAS)-based framework for the automated discovery of time series models
that strike a balance between computational efficiency, predictive performance,
and generalization power for the global, multi-step short-term forecasting of
energy production time series. In particular, we introduce a search space
consisting only of efficient components, which can capture distinctive patterns
of energy time series. Furthermore, we formulate a novel objective function
that accounts for performance generalization in temporal context and the
maximal exploration of different regions of our high-dimensional search space.
The results obtained on energy production time series show that an ensemble of
lightweight architectures discovered with NAS outperforms state-of-the-art
techniques, such as Transformers, as well as pre-trained forecasting models, in
terms of both efficiency and accuracy.

</details>


### [34] [FTT-GRU: A Hybrid Fast Temporal Transformer with GRU for Remaining Useful Life Prediction](https://arxiv.org/abs/2511.00564)
*Varun Teja Chirukiri,Udaya Bhasker Cheerala,Sandeep Kanta,Abdul Karim,Praveen Damacharla*

Main category: cs.LG

TL;DR: 提出FTT-GRU混合模型，结合快速时序Transformer和GRU，在NASA CMAPSS数据集上实现剩余使用寿命预测，性能优于现有深度基线。


<details>
  <summary>Details</summary>
Motivation: 现有LSTM和CNN方法难以同时建模全局时间依赖性和细粒度退化趋势，需要更高效的混合架构。

Method: 使用快速时序Transformer（基于FFT的线性化注意力）与GRU层结合，构建紧凑的Transformer-RNN混合模型。

Result: 在CMAPSS FD001上获得RMSE 30.76、MAE 18.97、R²=0.45，CPU延迟1.12ms，相比TCN-Attention基线提升RMSE 1.16%、MAE 4.00%。

Conclusion: 紧凑的Transformer-RNN混合架构能够准确高效地进行剩余使用寿命预测，适用于实时工业预测性维护。

Abstract: Accurate prediction of the remaining useful life (RUL) of industrial
machinery is essential for reducing downtime and optimizing maintenance
schedules. Existing approaches, such as long short-term memory (LSTM) networks
and convolutional neural networks (CNNs), often struggle to model both global
temporal dependencies and fine-grained degradation trends in multivariate
sensor data. We propose a hybrid model, FTT-GRU, which combines a Fast Temporal
Transformer (FTT) -- a lightweight Transformer variant using linearized
attention via fast Fourier transform (FFT) -- with a gated recurrent unit (GRU)
layer for sequential modeling. To the best of our knowledge, this is the first
application of an FTT with a GRU for RUL prediction on NASA CMAPSS, enabling
simultaneous capture of global and local degradation patterns in a compact
architecture. On CMAPSS FD001, FTT-GRU attains RMSE 30.76, MAE 18.97, and
$R^2=0.45$, with 1.12 ms CPU latency at batch=1. Relative to the best published
deep baseline (TCN--Attention), it improves RMSE by 1.16\% and MAE by 4.00\%.
Training curves averaged over $k=3$ runs show smooth convergence with narrow
95\% confidence bands, and ablations (GRU-only, FTT-only) support the
contribution of both components. These results demonstrate that a compact
Transformer-RNN hybrid delivers accurate and efficient RUL predictions on
CMAPSS, making it suitable for real-time industrial prognostics.

</details>


### [35] [Semi-Supervised Preference Optimization with Limited Feedback](https://arxiv.org/abs/2511.00040)
*Seonggyun Lee,Sungjun Lim,Seojin Park,Soeun Cheon,Kyungwoo Song*

Main category: cs.LG

TL;DR: 提出半监督偏好优化(SSPO)方法，通过结合少量配对偏好标签和大量未配对样本，显著降低对齐语言模型所需的数据成本


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法严重依赖大量配对反馈数据，导致资源消耗巨大，需要更高效的数据利用方法

Method: 利用理论证明的最优奖励阈值对未配对数据进行伪标签生成，结合少量真实配对标签和大量伪标签进行训练

Result: 在多个数据集上验证了卓越的数据效率，使用1%的UltraFeedback数据训练的SSPO优于使用10%数据训练的基线方法

Conclusion: SSPO能够有效从大规模未配对数据中提取潜在偏好，在保持人类对齐的同时大幅降低数据获取成本

Abstract: The field of preference optimization has made outstanding contributions to
the alignment of language models with human preferences. Despite these
advancements, recent methods still rely heavily on substantial paired (labeled)
feedback data, leading to substantial resource expenditures. To address these
challenges, we study the problem of Semi-Supervised Preference Optimization
(SSPO) in which the idea is to learn from both a small number of pairwise
preference labels and a large pool of unpaired samples simultaneously. Our key
theoretical contribution proves the existence of an optimal reward threshold
capable of separating winning and losing responses with high probability, which
enables a principled pseudo-labeling of unpaired data. By leveraging these
pseudo-labels, SSPO effectively distills latent preferences from large-scale
unpaired data, thus maintaining human alignment while drastically reducing
acquisition costs. Extensive experiments across datasets validate this
remarkable data efficiency; for instance, SSPO trained with Llama3-8B-Instruct
on just 1% of UltraFeedback consistently surpasses strong baselines trained on
10% of UltraFeedback.

</details>


### [36] [Koopman-based Prediction of Connectivity for Flying Ad Hoc Networks](https://arxiv.org/abs/2511.01286)
*Sivaram Krishnan,Jinho Choi,Jihong Park,Gregory Sherman,Benjamin Campbell*

Main category: cs.LG

TL;DR: 本文探索使用数据驱动的Koopman方法来建模无人机自组网中的轨迹动态，通过集中式和分布式两种方法预测信号质量，准确预测通信中断事件。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在静态无线环境中表现良好，但在高度动态的飞行自组网环境中存在局限，需要新的方法来应对不断变化的网络拓扑挑战。

Method: 基于Koopman算子理论，提出了集中式和分布式两种方法，利用无人机轨迹动态建模来预测信号干扰噪声比，确保无人机间的可靠通信。

Result: 这些方法能够准确预测导致通信中断的连接和隔离事件，为无人机基于预测结果调度传输提供了可能。

Conclusion: 数据驱动的Koopman方法能够有效解决动态飞行自组网环境中的通信挑战，提高网络性能。

Abstract: The application of machine learning (ML) to communication systems is expected
to play a pivotal role in future artificial intelligence (AI)-based
next-generation wireless networks. While most existing works focus on ML
techniques for static wireless environments, they often face limitations when
applied to highly dynamic environments, such as flying ad hoc networks
(FANETs). This paper explores the use of data-driven Koopman approaches to
address these challenges. Specifically, we investigate how these approaches can
model UAV trajectory dynamics within FANETs, enabling more accurate predictions
and improved network performance. By leveraging Koopman operator theory, we
propose two possible approaches -- centralized and distributed -- to
efficiently address the challenges posed by the constantly changing topology of
FANETs. To demonstrate this, we consider a FANET performing surveillance with
UAVs following pre-determined trajectories and predict
signal-to-interference-plus-noise ratios (SINRs) to ensure reliable
communication between UAVs. Our results show that these approaches can
accurately predict connectivity and isolation events that lead to modelled
communication outages. This capability could help UAVs schedule their
transmissions based on these predictions.

</details>


### [37] [Physics-Informed Neural Network Frameworks for the Analysis of Engineering and Biological Dynamical Systems Governed by Ordinary Differential Equations](https://arxiv.org/abs/2511.00043)
*Tyrus Whitman,Andrew Particka,Christopher Diers,Ian Griffin,Charuka Wickramasinghe,Pradeep Ranaweera*

Main category: cs.LG

TL;DR: 该研究验证了物理信息神经网络(PINNs)在求解工程和生物动力学系统ODE问题中的预测能力，通过系统调优损失函数平衡和超参数设置，PINNs能够有效处理传统数值方法难以收敛的复杂问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在处理高刚度、冲击、不规则域、奇异摄动、高维或边界不连续等复杂ODE问题时往往难以收敛，需要寻找更强大的替代方法。

Method: 使用经典ODE问题作为受控测试平台，系统评估PINNs框架的准确性、训练效率和泛化能力。通过嵌入物理定律到学习过程中，并精心平衡数据损失、初始条件损失和残差损失，同时系统调优网络深度、层宽、激活函数等超参数。

Result: 研究表明，对于复杂问题要收敛到正确解，必须通过仔细加权适当平衡损失函数各分量。系统调优超参数和嵌入先验知识显著增强了PINNs的预测能力。

Conclusion: PINNs虽然不是通用解决方案，但通过将物理定律直接嵌入学习过程，能够获得优越结果，特别是在处理传统数值方法难以解决的复杂ODE问题时表现出色。

Abstract: In this study, we present and validate the predictive capability of the
Physics-Informed Neural Networks (PINNs) methodology for solving a variety of
engineering and biological dynamical systems governed by ordinary differential
equations (ODEs). While traditional numerical methods a re effective for many
ODEs, they often struggle to achieve convergence in problems involving high
stiffness, shocks, irregular domains, singular perturbations, high dimensions,
or boundary discontinuities. Alternatively, PINNs offer a powerful approach for
handling challenging numerical scenarios. In this study, classical ODE problems
are employed as controlled testbeds to systematically evaluate the accuracy,
training efficiency, and generalization capability under controlled conditions
of the PINNs framework. Although not a universal solution, PINNs can achieve
superior results by embedding physical laws directly into the learning process.
We first analyze the existence and uniqueness properties of several benchmark
problems and subsequently validate the PINNs methodology on these model
systems. Our results demonstrate that for complex problems to converge to
correct solutions, the loss function components data loss, initial condition
loss, and residual loss must be appropriately balanced through careful
weighting. We further establish that systematic tuning of hyperparameters,
including network depth, layer width, activation functions, learning rate,
optimization algorithms, w eight initialization schemes, and collocation point
sampling, plays a crucial role in achieving accurate solutions. Additionally,
embedding prior knowledge and imposing hard constraints on the network
architecture, without loss the generality of the ODE system, significantly
enhances the predictive capability of PINNs.

</details>


### [38] [ReLaX-Net: Reusing Layers for Parameter-Efficient Physical Neural Networks](https://arxiv.org/abs/2511.00044)
*Kohei Tsuchiyama,Andre Roehm,Takatomo Mihana,Ryoichi Horisaki*

Main category: cs.LG

TL;DR: 提出ReLaX-Net架构，通过时间复用层来扩展物理神经网络深度，提高参数利用效率


<details>
  <summary>Details</summary>
Motivation: 物理神经网络在规模上落后数字神经网络几个数量级，需要参数高效架构

Method: 采用层间时间复用方案，仅需在现有PNN基础上添加快速开关

Result: 在图像分类和自然语言处理任务中验证，ReLaX-Net在相同参数数量下性能优于传统RNN/DNN

Conclusion: ReLaX-Net通过简单修改显著提升PNN计算性能，具有良好的扩展性

Abstract: Physical Neural Networks (PNN) are promising platforms for next-generation
computing systems. However, recent advances in digital neural network
performance are largely driven by the rapid growth in the number of trainable
parameters and, so far, demonstrated PNNs are lagging behind by several orders
of magnitude in terms of scale. This mirrors size and performance constraints
found in early digital neural networks. In that period, efficient reuse of
parameters contributed to the development of parameter-efficient architectures
such as convolutional neural networks.
  In this work, we numerically investigate hardware-friendly weight-tying for
PNNs. Crucially, with many PNN systems, there is a time-scale separation
between the fast dynamic active elements of the forward pass and the only
slowly trainable elements implementing weights and biases. With this in mind,we
propose the Reuse of Layers for eXpanding a Neural Network (ReLaX-Net)
architecture, which employs a simple layer-by-layer time-multiplexing scheme to
increase the effective network depth and efficiently use the number of
parameters. We only require the addition of fast switches for existing PNNs. We
validate ReLaX-Nets via numerical experiments on image classification and
natural language processing tasks. Our results show that ReLaX-Net improves
computational performance with only minor modifications to a conventional PNN.
We observe a favorable scaling, where ReLaX-Nets exceed the performance of
equivalent traditional RNNs or DNNs with the same number of parameters.

</details>


### [39] [DynBERG: Dynamic BERT-based Graph neural network for financial fraud detection](https://arxiv.org/abs/2511.00047)
*Omkar Kulkarni,Rohitash Chandra*

Main category: cs.LG

TL;DR: 提出了DynBERG模型，将Graph-BERT与GRU结合用于动态金融欺诈检测，支持有向边和时间演化，在比特币交易数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有Graph-BERT等图Transformer模型主要针对静态无向图，而金融交易网络具有动态性和有向性，需要专门处理时间演化和资金流向的模型。

Method: 结合Graph-BERT与GRU层，修改算法支持有向边，捕捉多时间步的时间演化，适用于动态金融交易分析。

Result: 在Elliptic比特币交易数据集上评估，在Dark Market Shutdown事件前后均表现优异，超越EvolveGCN和GCN等基准方法，消融实验证明GRU对建模时间动态至关重要。

Conclusion: DynBERG能有效适应市场重大变化，为动态金融欺诈检测提供了强有力的解决方案，时间序列深度学习组件的加入显著提升了模型性能。

Abstract: Financial fraud detection is critical for maintaining the integrity of
financial systems, particularly in decentralised environments such as
cryptocurrency networks. Although Graph Convolutional Networks (GCNs) are
widely used for financial fraud detection, graph Transformer models such as
Graph-BERT are gaining prominence due to their Transformer-based architecture,
which mitigates issues such as over-smoothing. Graph-BERT is designed for
static graphs and primarily evaluated on citation networks with undirected
edges. However, financial transaction networks are inherently dynamic, with
evolving structures and directed edges representing the flow of money. To
address these challenges, we introduce DynBERG, a novel architecture that
integrates Graph-BERT with a Gated Recurrent Unit (GRU) layer to capture
temporal evolution over multiple time steps. Additionally, we modify the
underlying algorithm to support directed edges, making DynBERG well-suited for
dynamic financial transaction analysis. We evaluate our model on the Elliptic
dataset, which includes Bitcoin transactions, including all transactions during
a major cryptocurrency market event, the Dark Market Shutdown. By assessing
DynBERG's resilience before and after this event, we analyse its ability to
adapt to significant market shifts that impact transaction behaviours. Our
model is benchmarked against state-of-the-art dynamic graph classification
approaches, such as EvolveGCN and GCN, demonstrating superior performance,
outperforming EvolveGCN before the market shutdown and surpassing GCN after the
event. Additionally, an ablation study highlights the critical role of
incorporating a time-series deep learning component, showcasing the
effectiveness of GRU in modelling the temporal dynamics of financial
transactions.

</details>


### [40] [Adaptive Spatio-Temporal Graphs with Self-Supervised Pretraining for Multi-Horizon Weather Forecasting](https://arxiv.org/abs/2511.00049)
*Yao Liu*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络的自监督学习框架，通过利用时空结构来改进多变量天气预报，在ERA5和MERRA-2数据集上优于传统数值天气预报模型和深度学习方法。


<details>
  <summary>Details</summary>
Motivation: 由于大气系统固有的时空复杂性，准确和稳健的天气预报仍然是一个基本挑战。

Method: 集成图神经网络进行空间推理，采用自监督预训练方案进行表示学习，并利用时空适应机制增强不同预报时长的泛化能力。

Result: 在ERA5和MERRA-2再分析数据集上的广泛实验表明，该方法相比传统数值天气预报模型和最近的深度学习方法具有更优性能。在北京和上海的定量评估和视觉分析证实了模型捕捉细粒度气象模式的能力。

Conclusion: 所提出的框架为未来数据驱动的天气预报系统提供了可扩展且标签高效的解决方案。

Abstract: Accurate and robust weather forecasting remains a fundamental challenge due
to the inherent spatio-temporal complexity of atmospheric systems. In this
paper, we propose a novel self-supervised learning framework that leverages
spatio-temporal structures to improve multi-variable weather prediction. The
model integrates a graph neural network (GNN) for spatial reasoning, a
self-supervised pretraining scheme for representation learning, and a
spatio-temporal adaptation mechanism to enhance generalization across varying
forecasting horizons. Extensive experiments on both ERA5 and MERRA-2 reanalysis
datasets demonstrate that our approach achieves superior performance compared
to traditional numerical weather prediction (NWP) models and recent deep
learning methods. Quantitative evaluations and visual analyses in Beijing and
Shanghai confirm the model's capability to capture fine-grained meteorological
patterns. The proposed framework provides a scalable and label-efficient
solution for future data-driven weather forecasting systems.

</details>


### [41] [Quadratic Direct Forecast for Training Multi-Step Time-Series Forecast Models](https://arxiv.org/abs/2511.00053)
*Hao Wang,Licheng Pan,Yuan Lu,Zhichao Chen,Tianqiao Liu,Shuting He,Zhixuan Chu,Qingsong Wen,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了一种新的二次形式加权训练目标，通过考虑未来步骤间的标签自相关效应和设置异质任务权重，改进时间序列预测模型的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有训练目标（如均方误差）将每个未来步骤视为独立、等权重的任务，这导致两个问题：(1) 忽略未来步骤间的标签自相关效应，造成训练目标偏差；(2) 无法为不同未来步骤的预测任务设置异质权重，限制了预测性能。

Method: 提出二次形式加权训练目标，其中权重矩阵的非对角元素考虑标签自相关效应，非均匀对角元素匹配不同未来步骤预测任务的最优权重。开发了QDF学习算法，使用自适应更新的二次形式权重矩阵训练预测模型。

Result: 实验表明QDF有效提升了各种预测模型的性能，取得了最先进的结果。

Conclusion: 通过考虑标签自相关和异质任务权重的二次形式加权训练目标，能够显著改进时间序列预测模型的训练效果和性能。

Abstract: The design of training objective is central to training time-series
forecasting models. Existing training objectives such as mean squared error
mostly treat each future step as an independent, equally weighted task, which
we found leading to the following two issues: (1) overlook the label
autocorrelation effect among future steps, leading to biased training
objective; (2) fail to set heterogeneous task weights for different forecasting
tasks corresponding to varying future steps, limiting the forecasting
performance. To fill this gap, we propose a novel quadratic-form weighted
training objective, addressing both of the issues simultaneously. Specifically,
the off-diagonal elements of the weighting matrix account for the label
autocorrelation effect, whereas the non-uniform diagonals are expected to match
the most preferable weights of the forecasting tasks with varying future steps.
To achieve this, we propose a Quadratic Direct Forecast (QDF) learning
algorithm, which trains the forecast model using the adaptively updated
quadratic-form weighting matrix. Experiments show that our QDF effectively
improves performance of various forecast models, achieving state-of-the-art
results. Code is available at https://anonymous.4open.science/r/QDF-8937.

</details>


### [42] [FLoRA: Fused forward-backward adapters for parameter efficient fine-tuning and reducing inference-time latencies of LLMs](https://arxiv.org/abs/2511.00050)
*Dhananjaya Gowda,Seoha Song,Junhyun Lee,Harshith Goka*

Main category: cs.LG

TL;DR: 提出了FLoRA方法，一种融合前向-后向适配器的参数高效微调技术，结合了LoRA和并行适配器的优点，在保持低参数预算的同时显著提升了准确性和降低了延迟。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模不断增长，参数高效微调(PEFT)变得愈发重要。虽然已有多种PEFT方法被研究，但仍有大量自由度未被探索。

Method: FLoRA通过融合前向和后向适配器(FFBA)，结合LoRA和并行适配器的思想，并将适配器融合到基础模型的投影层中以最小化延迟。

Result: 实验结果显示，在相似参数预算下，FLoRA在准确性和延迟方面都显著优于广泛使用的LoRA方法。

Conclusion: FLoRA提供了一种有效的参数高效微调解决方案，在保持低参数开销的同时实现了更好的性能和更低的延迟。

Abstract: As the large language models (LLMs) grow in size each day, efficient training
and fine-tuning has never been as important as nowadays. This resulted in the
great interest in parameter efficient fine-tuning (PEFT), and effective methods
including low-rank adapters (LoRA) has emerged. Although the various PEFT
methods have been studied extensively in the recent years, the greater part of
the subject remains unexplored with the huge degree of freedom. In this paper,
we propose FLoRA, a family of fused forward-backward adapters (FFBA) for
parameter-efficient fine-tuning of LLMs on downstream tasks. The FFBA combine
ideas from the popular LoRA and parallel adapters to improve the overall
fine-tuning accuracies. At the same time, latencies are minimized by fusing the
forward and backward adapters into existing projection layers of the base
model. Experimental results show that the proposed FFB adapters perform
significantly better than the popularly used LoRA in both accuracy and latency
for a similar parameter budget.

</details>


### [43] [Diffusion LLMs are Natural Adversaries for any LLM](https://arxiv.org/abs/2511.00203)
*David Lüdke,Tom Wollschläger,Paul Ungermann,Stephan Günnemann,Leo Schwinn*

Main category: cs.LG

TL;DR: 提出将对抗性提示优化转化为高效摊销推理任务的新框架，利用预训练的非自回归生成LLM作为提示搜索的代理，直接条件生成提示，替代昂贵的逐实例离散优化。


<details>
  <summary>Details</summary>
Motivation: 传统的对抗性提示优化需要大量计算资源，本工作旨在通过摊销推理方法提高效率，将问题转化为条件生成任务。

Method: 使用预训练的非自回归生成LLM（如Diffusion LLMs）建模提示-响应对的联合分布，通过少量条件样本来恢复高奖励提示。

Result: 生成的提示具有低困惑度、多样性，并能有效转移到各种黑盒目标模型，包括经过鲁棒训练和专有LLM。

Conclusion: 该框架不仅适用于对抗性提示，还为红队测试、自动化提示优化以及基于Flow和Diffusion的LLM应用开辟了新方向。

Abstract: We introduce a novel framework that transforms the resource-intensive
(adversarial) prompt optimization problem into an \emph{efficient, amortized
inference task}. Our core insight is that pretrained, non-autoregressive
generative LLMs, such as Diffusion LLMs, which model the joint distribution
over prompt-response pairs, can serve as powerful surrogates for prompt search.
This approach enables the direct conditional generation of prompts, effectively
replacing costly, per-instance discrete optimization with a small number of
parallelizable samples. We provide a probabilistic analysis demonstrating that
under mild fidelity assumptions, only a few conditional samples are required to
recover high-reward (harmful) prompts. Empirically, we find that the generated
prompts are low-perplexity, diverse jailbreaks that exhibit strong
transferability to a wide range of black-box target models, including robustly
trained and proprietary LLMs. Beyond adversarial prompting, our framework opens
new directions for red teaming, automated prompt optimization, and leveraging
emerging Flow- and Diffusion-based LLMs.

</details>


### [44] [Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT](https://arxiv.org/abs/2511.00051)
*Da Chang,Peng Xue,Yu Li,Yongxiang Liu,Pengxiang Xu,Shixun Zhang*

Main category: cs.LG

TL;DR: 本文分析了DoRA方法的机制，发现其成功源于增加权重更新矩阵的奇异值熵，并提出更高效的矩阵形式。基于此，作者提出了Pre-Diag和SORA两种新方法，在性能和效率上均优于LoRA和DoRA。


<details>
  <summary>Details</summary>
Motivation: DoRA方法虽然性能优异但机制不明确且计算开销大，需要深入理解其工作原理并开发更高效的参数高效微调方法。

Method: 首先识别DoRA的成功机制，然后将其重新表述为数学等效但更高效的矩阵形式，最后提出统一框架并开发Pre-Diag和SORA两种新方法。

Result: 在自然语言理解和生成任务上的广泛实验表明，提出的方法在性能和效率上都优于LoRA和DoRA。

Conclusion: 通过深入理解DoRA机制并开发新的参数高效微调方法，实现了更优的性能和效率平衡，为设计先进PEFT方法提供了统一框架。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large
pre-trained models. Among these, LoRA is considered a foundational approach.
Building on this, the influential DoRA method enhances performance by
decomposing weight updates into magnitude and direction. However, its
underlying mechanism remains unclear, and it introduces significant
computational overhead. In this work, we first identify that DoRA's success
stems from its capacity to increase the singular value entropy of the weight
update matrix, which promotes a more uniform update distribution akin to full
fine-tuning. We then reformulate DoRA into a mathematically equivalent and more
efficient matrix form, revealing it as a learnable weight conditioning method.
Based on this insight, we propose a unified framework for designing advanced
PEFT methods by exploring two orthogonal dimensions: the architectural
placement and the transformation type of the conditioning matrix. Within this
framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies
a diagonal conditioning matrix before the LoRA update to efficiently calibrate
the pre-trained weights, thereby enhancing performance while reducing training
time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation
\textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient
orthogonal rotation to perform a more powerful, norm-preserving transformation
of the feature space. Extensive experiments on natural language understanding
and generation tasks demonstrate that our proposed methods achieve superior
performance and efficiency compared to both LoRA and DoRA. The code is
available at https://github.com/MaeChd/SORA.

</details>


### [45] [A Tight Lower Bound for Non-stochastic Multi-armed Bandits with Expert Advice](https://arxiv.org/abs/2511.00257)
*Zachary Chase,Shinji Ito,Idan Mehalel*

Main category: cs.LG

TL;DR: 本文确定了非随机多臂老虎机专家建议问题中的极小极大最优期望遗憾，通过证明与Kale(2014)上界匹配的下界，得出最优期望遗憾为Θ(√(TKlog(N/K)))。


<details>
  <summary>Details</summary>
Motivation: 解决非随机多臂老虎机专家建议问题中的最优遗憾界问题，填补现有理论中的空白。

Method: 通过数学证明构建下界，与现有上界进行匹配分析。

Result: 确定了极小极大最优期望遗憾为Θ(√(TKlog(N/K)))，其中K为臂数，N为专家数，T为时间范围。

Conclusion: 该研究完整刻画了非随机多臂老虎机专家建议问题的理论最优性能界限。

Abstract: We determine the minimax optimal expected regret in the classic
non-stochastic multi-armed bandit with expert advice problem, by proving a
lower bound that matches the upper bound of Kale (2014). The two bounds
determine the minimax optimal expected regret to be $\Theta\left( \sqrt{T K
\log (N/K) } \right)$, where $K$ is the number of arms, $N$ is the number of
experts, and $T$ is the time horizon.

</details>


### [46] [Feature-Guided Analysis of Neural Networks: A Replication Study](https://arxiv.org/abs/2511.00052)
*Federico Formica,Stefano Gregis,Aurora Francesca Zanenga,Andrea Rota,Mark Lawford,Claudio Menghi*

Main category: cs.LG

TL;DR: 本文评估了特征引导分析（FGA）在MNIST和LSC数据集上的适用性，发现FGA在基准测试中比文献结果具有更高精度，同时神经网络架构、训练和特征选择对FGA的召回率有显著影响。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络决策原因对其在安全关键应用中使用至关重要，现有特征引导方法需要更多工业应用场景的实证证据。

Method: 在MNIST和LSC数据集组成的基准上评估FGA，分析神经网络架构、训练过程和特征选择对FGA有效性的影响。

Result: FGA在基准测试中比文献结果具有更高精度，特征选择对FGA召回率有显著影响但对精度影响可忽略。

Conclusion: FGA在解释神经网络行为方面具有良好适用性，但需要仔细考虑特征选择以优化召回率。

Abstract: Understanding why neural networks make certain decisions is pivotal for their
use in safety-critical applications. Feature-Guided Analysis (FGA) extracts
slices of neural networks relevant to their tasks. Existing feature-guided
approaches typically monitor the activation of the neural network neurons to
extract the relevant rules. Preliminary results are encouraging and demonstrate
the feasibility of this solution by assessing the precision and recall of
Feature-Guided Analysis on two pilot case studies. However, the applicability
in industrial contexts needs additional empirical evidence.
  To mitigate this need, this paper assesses the applicability of FGA on a
benchmark made by the MNIST and LSC datasets. We assessed the effectiveness of
FGA in computing rules that explain the behavior of the neural network. Our
results show that FGA has a higher precision on our benchmark than the results
from the literature. We also evaluated how the selection of the neural network
architecture, training, and feature selection affect the effectiveness of FGA.
Our results show that the selection significantly affects the recall of FGA,
while it has a negligible impact on its precision.

</details>


### [47] [A Technical Exploration of Causal Inference with Hybrid LLM Synthetic Data](https://arxiv.org/abs/2511.00318)
*Dana Kim,Yichen Xu,Tiffany Lin*

Main category: cs.LG

TL;DR: 本文提出了一种结合LLM生成和因果建模的混合框架，用于生成保持因果结构的合成表格数据，解决了现有方法在保持平均处理效应等关键因果参数方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型和GAN方法在生成合成表格数据时，虽然能达到高预测保真度，但往往无法准确保持因果参数如平均处理效应，这限制了合成数据在因果分析中的应用。

Method: 提出混合生成框架：基于模型的协变量合成（通过距离到最近记录过滤监控）+ 分别学习的倾向评分和结果模型，确保(W, A, Y)三元组保持底层因果结构；引入合成配对策略缓解正性违例问题。

Result: 开发了利用无限合成样本评估传统估计器（IPTW、AIPW、替代）在复杂协变量分布下性能的现实评估协议，证明了所提方法在保持因果结构方面的有效性。

Conclusion: 这项工作为支持稳健因果分析的LLM驱动数据管道奠定了基础，代码已在GitHub开源。

Abstract: Large Language Models (LLMs) offer a flexible means to generate synthetic
tabular data, yet existing approaches often fail to preserve key causal
parameters such as the average treatment effect (ATE). In this technical
exploration, we first demonstrate that state-of-the-art synthetic data
generators, both GAN- and LLM-based, can achieve high predictive fidelity while
substantially misestimating causal effects. To address this gap, we propose a
hybrid generation framework that combines model-based covariate synthesis
(monitored via distance-to-closest-record filtering) with separately learned
propensity and outcome models, thereby ensuring that (W, A, Y) triplets retain
their underlying causal structure. We further introduce a synthetic pairing
strategy to mitigate positivity violations and a realistic evaluation protocol
that leverages unlimited synthetic samples to benchmark traditional estimators
(IPTW, AIPW, substitution) under complex covariate distributions. This work
lays the groundwork for LLM-powered data pipelines that support robust causal
analysis. Our code is available at
https://github.com/Xyc-arch/llm-synthetic-for-causal-inference.git.

</details>


### [48] [Toward Unifying Group Fairness Evaluation from a Sparsity Perspective](https://arxiv.org/abs/2511.00359)
*Zhecheng Sheng,Jiawei Zhang,Enmao Diao*

Main category: cs.LG

TL;DR: 提出基于稀疏性的统一框架来评估算法公平性，该框架与现有公平标准一致，适用于多种机器学习任务。


<details>
  <summary>Details</summary>
Motivation: 机器学习中的算法公平性面临重大挑战，现有公平标准在不同机器学习问题中缺乏通用性。

Method: 研究各种稀疏性度量在促进公平性方面的联系与差异，提出基于稀疏性的统一评估框架。

Result: 通过多种数据集和偏见缓解方法的广泛实验，验证了所提框架作为评估指标的有效性。

Conclusion: 通过稀疏性和社会公平的视角为算法公平性研究提供了新视角，对公平性研究和应用具有广泛影响潜力。

Abstract: Ensuring algorithmic fairness remains a significant challenge in machine
learning, particularly as models are increasingly applied across diverse
domains. While numerous fairness criteria exist, they often lack
generalizability across different machine learning problems. This paper
examines the connections and differences among various sparsity measures in
promoting fairness and proposes a unified sparsity-based framework for
evaluating algorithmic fairness. The framework aligns with existing fairness
criteria and demonstrates broad applicability to a wide range of machine
learning tasks. We demonstrate the effectiveness of the proposed framework as
an evaluation metric through extensive experiments on a variety of datasets and
bias mitigation methods. This work provides a novel perspective to algorithmic
fairness by framing it through the lens of sparsity and social equity, offering
potential for broader impact on fairness research and applications.

</details>


### [49] [SpatialTraceGen: High-Fidelity Traces for Efficient VLM Spatial Reasoning Distillation](https://arxiv.org/abs/2511.00054)
*Gio Huh,Dhruv Sheth,Rayhan Zirvi,Frank Xiao*

Main category: cs.LG

TL;DR: 提出了SpatialTraceGen框架，通过蒸馏大型教师模型的推理过程来生成高质量的多步骤、多工具推理轨迹数据集，解决了视觉语言模型在复杂空间推理任务中缺乏高质量训练数据的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂空间推理方面表现不佳，需要问题分解和策略性工具使用。虽然微调小型模型是高效路径，但缺乏高质量的分步推理数据成为了主要瓶颈。

Method: 开发了SpatialTraceGen框架，包含自动验证器来确保每个推理步骤的保真度，通过蒸馏大型教师模型的推理过程生成多跳、多工具推理轨迹数据集。

Result: 在CLEVR-Humans基准测试中，验证器引导的过程将轨迹平均质量分数提高了17%，同时将质量方差降低了40%以上。

Conclusion: SpatialTraceGen提供了专家级轨迹数据集，为有效微调和样本高效的离线强化学习提供了结构化的分步工具使用示例。

Abstract: While Vision-Language Models (VLMs) excel in many areas, they struggle with
complex spatial reasoning, which requires problem decomposition and strategic
tool use. Fine-tuning smaller, more deployable models offers an efficient path
to strong performance, but this is hampered by a major bottleneck: the absence
of high-quality, step-by-step reasoning data. To address this data-efficiency
gap, we introduce SpatialTraceGen, a framework to distill the reasoning
processes of a large teacher model into a high-quality dataset of multi-hop,
multi-tool reasoning traces. A key innovation is our automated Verifier, which
scalably ensures the fidelity of each reasoning step, providing a
cost-effective alternative to manual human annotation. On the CLEVR-Humans
benchmark, this verifier-guided process improves the average quality score of
traces by 17\% while reducing quality variance by over 40\%. SpatialTraceGen
delivers a dataset of expert traces, providing the structured, step-by-step
examples of tool use necessary for effective fine-tuning and sample-efficient
offline reinforcement learning.

</details>


### [50] [Why Federated Optimization Fails to Achieve Perfect Fitting? A Theoretical Perspective on Client-Side Optima](https://arxiv.org/abs/2511.00469)
*Zhongxiang Lei,Qi Yang,Ping Qiu,Gang Zhang,Yuanchi Ma,Jinyan Liu*

Main category: cs.LG

TL;DR: 本文从理论角度解释了联邦学习中数据异构性导致性能下降的原因，指出异构数据会产生不同的局部最优解，这既提高了全局目标的下界，又导致全局模型在训练后期在区域内振荡而非收敛到单一最优解。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习算法虽然在理论和实践中都能保证收敛，但在数据异构情况下性能下降的原因尚不明确。本文旨在填补这一理论空白，解释非独立同分布数据下性能退化的根本原因。

Method: 提出假设：异构客户端数据会导致不同的局部最优解，并基于此假设分析两个关键后果：1）客户端局部最优解之间的距离提高了全局目标的下界；2）在训练后期，全局模型会在区域内振荡而非收敛到单一最优解。

Result: 理论分析表明，数据异构性使得完美拟合所有客户端数据变得不可能，且限制了模型完全拟合数据的能力。通过多个任务和神经网络架构的实验验证了这些理论结果。

Conclusion: 本文为联邦学习中非独立同分布数据导致的性能退化提供了原则性解释，揭示了异构数据下模型收敛行为的本质特征，相关框架已在GitHub上开源。

Abstract: Federated optimization is a constrained form of distributed optimization that
enables training a global model without directly sharing client data. Although
existing algorithms can guarantee convergence in theory and often achieve
stable training in practice, the reasons behind performance degradation under
data heterogeneity remain unclear. To address this gap, the main contribution
of this paper is to provide a theoretical perspective that explains why such
degradation occurs. We introduce the assumption that heterogeneous client data
lead to distinct local optima, and show that this assumption implies two key
consequences: 1) the distance among clients' local optima raises the lower
bound of the global objective, making perfect fitting of all client data
impossible; and 2) in the final training stage, the global model oscillates
within a region instead of converging to a single optimum, limiting its ability
to fully fit the data. These results provide a principled explanation for
performance degradation in non-iid settings, which we further validate through
experiments across multiple tasks and neural network architectures. The
framework used in this paper is open-sourced at:
https://github.com/NPCLEI/fedtorch.

</details>


### [51] [Exploring Federated Learning for Thermal Urban Feature Segmentation -- A Comparison of Centralized and Decentralized Approaches](https://arxiv.org/abs/2511.00055)
*Leonhard Duda,Khadijeh Alibabaei,Elena Vollmer,Leon Klug,Valentin Kozlov,Lisana Berberi,Mishal Benz,Rebekka Volk,Juan Pedro Gutiérrez Hermosillo Muriedas,Markus Götz,Judith Sáínz-Pardo Díaz,Álvaro López García,Frank Schultmann,Achim Streit*

Main category: cs.LG

TL;DR: 该论文研究了联邦学习在无人机热成像图像分割任务中的实际应用效果，比较了多种FL方法与集中式学习的性能差异。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和技术限制，分布式数据无法集中存储和共享，联邦学习能够绕过传统集中式机器学习的限制，让参与者在本地训练模型而不共享数据。

Method: 在真实部署场景中评估FL算法，比较多种FL方法与集中式学习基线，分析客户端控制和工作流与服务器控制工作流，评估模型精度、训练时间、通信开销和能耗等关键指标。

Result: 研究提供了FL方法在无人机热成像分割任务中实际应用和局限性的有价值参考。

Conclusion: 联邦学习在无人机热成像特征检测中具有实际应用价值，能够有效处理分布式数据，但面临数据分布不一致和特征特性差异等挑战。

Abstract: Federated Learning (FL) is an approach for training a shared Machine Learning
(ML) model with distributed training data and multiple participants. FL allows
bypassing limitations of the traditional Centralized Machine Learning CL if
data cannot be shared or stored centrally due to privacy or technical
restrictions -- the participants train the model locally with their training
data and do not need to share it among the other participants. This paper
investigates the practical implementation and effectiveness of FL in a
real-world scenario, specifically focusing on unmanned aerial vehicle
(UAV)-based thermal images for common thermal feature detection in urban
environments. The distributed nature of the data arises naturally and makes it
suitable for FL applications, as images captured in two German cities are
available. This application presents unique challenges due to non-identical
distribution and feature characteristics of data captured at both locations.
The study makes several key contributions by evaluating FL algorithms in real
deployment scenarios rather than simulation. We compare several FL approaches
with a centralized learning baseline across key performance metrics such as
model accuracy, training time, communication overhead, and energy usage. This
paper also explores various FL workflows, comparing client-controlled workflows
and server-controlled workflows. The findings of this work serve as a valuable
reference for understanding the practical application and limitations of the FL
methods in segmentation tasks in UAV-based imaging.

</details>


### [52] [Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance](https://arxiv.org/abs/2511.00543)
*Yunchuan Guan,Yu Liu,Ke Zhou,Hui Li,Sen Jia,Zhiqi Shen,Ziyang Wang,Xinglin Zhang,Tao Chen,Jenq-Neng Hwang,Lei Li*

Main category: cs.LG

TL;DR: 提出Lo-Hp框架，通过解耦的两阶段权重生成方法解决现有生成式权重优化中的过耦合和长时域问题，提高灵活性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前生成式权重优化方法存在过耦合和长时域问题，前者限制了优化器的灵活性，后者导致推理效率低下和精度不足。

Method: 采用解耦的两阶段权重生成框架，结合混合策略子轨迹平衡目标，整合在线和离线学习来捕捉局部优化策略。

Result: 在迁移学习、少样本学习、领域泛化和大语言模型适应等任务中展现出优越的准确性和推理效率。

Conclusion: 学习局部优化策略可以有效解决长时域问题，同时提升全局最优权重的生成质量。

Abstract: Recent advances in generative modeling enable neural networks to generate
weights without relying on gradient-based optimization. However, current
methods are limited by issues of over-coupling and long-horizon. The former
tightly binds weight generation with task-specific objectives, thereby limiting
the flexibility of the learned optimizer. The latter leads to inefficiency and
low accuracy during inference, caused by the lack of local constraints. In this
paper, we propose Lo-Hp, a decoupled two-stage weight generation framework that
enhances flexibility through learning various optimization policies. It adopts
a hybrid-policy sub-trajectory balance objective, which integrates on-policy
and off-policy learning to capture local optimization policies. Theoretically,
we demonstrate that learning solely local optimization policies can address the
long-horizon issue while enhancing the generation of global optimal weights. In
addition, we validate Lo-Hp's superior accuracy and inference efficiency in
tasks that require frequent weight updates, such as transfer learning, few-shot
learning, domain generalization, and large language model adaptation.

</details>


### [53] [MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling](https://arxiv.org/abs/2511.00056)
*Yuxi Liu,Renjia Deng,Yutong He,Xue Wang,Tao Yao,Kun Yuan*

Main category: cs.LG

TL;DR: 提出了MISA方法，通过将每层划分为更小的模块并分配重要性分数，使用加权随机采样机制激活模块，相比分层采样能有效降低梯度方差，在减少内存需求的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型预训练和微调的内存需求巨大，现有分层优化方法虽然能节省内存，但忽略了层内模块的重要性差异，且内存节省有限。

Method: 将每层划分为小模块，为每个模块分配重要性分数，使用加权随机采样机制选择激活的模块，同时冻结其他模块以节省内存。

Result: 理论证明MISA能降低梯度方差，建立O(1/√K)收敛率，实验验证在多种学习任务上有效，内存分析显示优于基线方法。

Conclusion: MISA是一种有效的模块级重要性采样方法，能在减少内存需求的同时保持模型性能，为大语言模型的高效训练提供了新思路。

Abstract: The substantial memory demands of pre-training and fine-tuning large language
models (LLMs) require memory-efficient optimization algorithms. One promising
approach is layer-wise optimization, which treats each transformer block as a
single layer and optimizes it sequentially, while freezing the other layers to
save optimizer states and activations. Although effective, these methods ignore
the varying importance of the modules within each layer, leading to suboptimal
performance. Moreover, layer-wise sampling provides only limited memory
savings, as at least one full layer must remain active during optimization. To
overcome these limitations, we propose Module-wise Importance SAmpling (MISA),
a novel method that divides each layer into smaller modules and assigns
importance scores to each module. MISA uses a weighted random sampling
mechanism to activate modules, provably reducing gradient variance compared to
layer-wise sampling. Additionally, we establish an \(\mathcal{O}(1/\sqrt{K})\)
convergence rate under non-convex and stochastic conditions, where $K$ is the
total number of block updates, and provide a detailed memory analysis
showcasing MISA's superiority over existing baseline methods. Experiments on
diverse learning tasks validate the effectiveness of MISA. Source code is
available at https://github.com/pkumelon/MISA.

</details>


### [54] [Sparse and nonparametric estimation of equations governing dynamical systems with applications to biology](https://arxiv.org/abs/2511.00579)
*G. Pillonetto,A. Giaretta,A. Aravkin,M. Bisiacco,T. Elston*

Main category: cs.LG

TL;DR: 提出了一种结合稀疏参数估计和非参数技术的新框架，用于从数据中发现复杂动力系统的模型方程，特别适用于系统生物学中难以用传统参数模型准确描述的非线性系统。


<details>
  <summary>Details</summary>
Motivation: 传统参数模型（如Sindy算法）在描述复杂系统的某些非线性特性时存在局限，需要预先知道函数形式或扩展函数库。本文旨在克服这一限制，无需先验信息即可准确捕捉Sindy无法描述的非线性特征。

Method: 开发了一个集成框架，将稀疏参数估计与非参数技术相结合。该方法不需要扩展函数库或预先指定非线性函数形式，就能自动发现和描述系统中的复杂非线性动态。

Result: 该方法在多个复杂生物现象估计的示例中得到了验证，能够成功捕捉传统参数方法无法准确描述的非线性特征，展示了在系统生物学应用中的有效性。

Conclusion: 提出的混合框架为数据驱动的模型发现提供了更强大的工具，特别适用于具有复杂非线性特性的系统，为系统生物学等领域的建模提供了新的可能性。

Abstract: Data-driven discovery of model equations is a powerful approach for
understanding the behavior of dynamical systems in many scientific fields. In
particular, the ability to learn mathematical models from data would benefit
systems biology, where the complex nature of these systems often makes a bottom
up approach to modeling unfeasible. In recent years, sparse estimation
techniques have gained prominence in system identification, primarily using
parametric paradigms to efficiently capture system dynamics with minimal model
complexity. In particular, the Sindy algorithm has successfully used sparsity
to estimate nonlinear systems by extracting from a library of functions only a
few key terms needed to capture the dynamics of these systems. However,
parametric models often fall short in accurately representing certain
nonlinearities inherent in complex systems. To address this limitation, we
introduce a novel framework that integrates sparse parametric estimation with
nonparametric techniques. It captures nonlinearities that Sindy cannot describe
without requiring a priori information about their functional form. That is,
without expanding the library of functions to include the one that is trying to
be discovered. We illustrate our approach on several examples related to
estimation of complex biological phenomena.

</details>


### [55] [Automatically Finding Rule-Based Neurons in OthelloGPT](https://arxiv.org/abs/2511.00059)
*Aditya Singh,Zihang Wen,Srujananjali Medicherla,Adam Karvonen,Can Rager*

Main category: cs.LG

TL;DR: 该论文提出了一种基于决策树的自动化方法，用于识别和解释OthelloGPT模型中编码游戏规则的MLP神经元，发现约一半神经元可被紧凑的规则化决策树准确描述，并通过干预实验验证了这些模式与模型预测能力之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: OthelloGPT模型为可解释性研究提供了理想测试平台——模型足够复杂以展现丰富的计算模式，同时基于规则的游戏逻辑使得逆向工程具有意义。

Method: 使用回归决策树将棋盘状态映射到神经元激活，提取高激活决策路径并转换为人类可读的逻辑形式，通过目标干预验证因果相关性。

Result: 发现第5层中约一半神经元（913/2048）可由紧凑的规则化决策树准确描述（R²>0.7），其余神经元可能参与更分布式或非规则计算。干预实验显示特定模式对应的神经元被消融后，模型预测相关模式合法移动的能力下降5-10倍。

Conclusion: 该方法成功识别了模型中编码规则化游戏逻辑的神经元，验证了其因果相关性，并提供了将游戏行为映射到实现神经元的Python工具，为可解释性研究提供资源。

Abstract: OthelloGPT, a transformer trained to predict valid moves in Othello, provides
an ideal testbed for interpretability research. The model is complex enough to
exhibit rich computational patterns, yet grounded in rule-based game logic that
enables meaningful reverse-engineering. We present an automated approach based
on decision trees to identify and interpret MLP neurons that encode rule-based
game logic. Our method trains regression decision trees to map board states to
neuron activations, then extracts decision paths where neurons are highly
active to convert them into human-readable logical forms. These descriptions
reveal highly interpretable patterns; for instance, neurons that specifically
detect when diagonal moves become legal. Our findings suggest that roughly half
of the neurons in layer 5 can be accurately described by compact, rule-based
decision trees ($R^2 > 0.7$ for 913 of 2,048 neurons), while the remainder
likely participate in more distributed or non-rule-based computations. We
verify the causal relevance of patterns identified by our decision trees
through targeted interventions. For a specific square, for specific game
patterns, we ablate neurons corresponding to those patterns and find an
approximately 5-10 fold stronger degradation in the model's ability to predict
legal moves along those patterns compared to control patterns. To facilitate
future work, we provide a Python tool that maps rule-based game behaviors to
their implementing neurons, serving as a resource for researchers to test
whether their interpretability methods recover meaningful computational
structures.

</details>


### [56] [Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering](https://arxiv.org/abs/2511.00617)
*Eric Bigelow,Daniel Wurgaft,YingQiao Wang,Noah Goodman,Tomer Ullman,Hidenori Tanaka,Ekdeep Singh Lubana*

Main category: cs.LG

TL;DR: 该论文提出了一个统一的贝叶斯框架来解释大语言模型的控制方法，将上下文学习和激活引导视为改变模型对潜在概念信念的不同方式。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM控制方法（上下文学习和激活引导）看似不同，但都旨在控制模型行为，作者希望建立一个统一的理论框架来解释这些方法。

Method: 从贝叶斯视角出发，将上下文学习视为证据积累过程，激活引导视为改变概念先验，建立了一个闭式贝叶斯模型来预测模型行为。

Result: 该模型能准确预测LLM在多种干预下的行为，解释了S型学习曲线等现象，并预测了新的现象如对数信念空间中的干预可加性。

Conclusion: 这项工作为基于提示和激活的LLM行为控制提供了统一的理论解释，并提供了预测这些干预效果的经验方法。

Abstract: Large language models (LLMs) can be controlled at inference time through
prompts (in-context learning) and internal activations (activation steering).
Different accounts have been proposed to explain these methods, yet their
common goal of controlling model behavior raises the question of whether these
seemingly disparate methodologies can be seen as specific instances of a
broader framework. Motivated by this, we develop a unifying, predictive account
of LLM control from a Bayesian perspective. Specifically, we posit that both
context- and activation-based interventions impact model behavior by altering
its belief in latent concepts: steering operates by changing concept priors,
while in-context learning leads to an accumulation of evidence. This results in
a closed-form Bayesian model that is highly predictive of LLM behavior across
context- and activation-based interventions in a set of domains inspired by
prior work on many-shot in-context learning. This model helps us explain prior
empirical phenomena - e.g., sigmoidal learning curves as in-context evidence
accumulates - while predicting novel ones - e.g., additivity of both
interventions in log-belief space, which results in distinct phases such that
sudden and dramatic behavioral shifts can be induced by slightly changing
intervention controls. Taken together, this work offers a unified account of
prompt-based and activation-based control of LLM behavior, and a methodology
for empirically predicting the effects of these interventions.

</details>


### [57] [EVINGCA: Adaptive Graph Clustering with Evolving Neighborhood Statistics](https://arxiv.org/abs/2511.00064)
*Randolph Wiredu-Aidoo*

Main category: cs.LG

TL;DR: EVINGCA是一种基于密度-方差的聚类算法，将聚类形成视为最近邻图上的自适应演化过程，通过广度优先搜索和局部统计反馈来替代固定密度阈值。


<details>
  <summary>Details</summary>
Motivation: 现有聚类算法存在限制性假设：K-Means和高斯混合模型假设凸的、高斯类簇，而DBSCAN和HDBSCAN能捕获非凸性但对参数高度敏感。

Method: EVINGCA在最近邻图上通过广度优先搜索扩展根图，使用持续更新的局部距离和形状统计来指导聚类过程，并利用空间索引实现高效计算。

Result: EVINGCA在平均情况下具有对数线性复杂度，在多种合成、真实世界、低维和高维数据集上展现出与基线方法竞争的性能。

Conclusion: EVINGCA通过自适应演化过程和局部统计反馈机制，克服了传统聚类算法的限制性假设和参数敏感性，提供了一种有效的聚类解决方案。

Abstract: Clustering algorithms often rely on restrictive assumptions: K-Means and
Gaussian Mixtures presuppose convex, Gaussian-like clusters, while DBSCAN and
HDBSCAN capture non-convexity but can be highly sensitive. I introduce EVINGCA
(Evolving Variance-Informed Nonparametric Graph Construction Algorithm), a
density-variance based clustering algorithm that treats cluster formation as an
adaptive, evolving process on a nearest-neighbor graph. EVINGCA expands rooted
graphs via breadth-first search, guided by continuously updated local distance
and shape statistics, replacing fixed density thresholds with local statistical
feedback. With spatial indexing, EVINGCA features log-linear complexity in the
average case and exhibits competitive performance against baselines across a
variety of synthetic, real-world, low-d, and high-d datasets.

</details>


### [58] [Stochastic Shortest Path with Sparse Adversarial Costs](https://arxiv.org/abs/2511.00637)
*Emmeran Johnson,Alberto Rumi,Ciara Pike-Burke,Patrick Rebeschini*

Main category: cs.LG

TL;DR: 本文研究了具有稀疏成本的对抗性随机最短路径问题，提出了ℓ_r-范数正则化器来适应稀疏性，在已知转移设置下实现了与√log M成比例的遗憾，而不是√log SA。


<details>
  <summary>Details</summary>
Motivation: 现有基于负熵正则化的在线镜像下降方法在已知转移设置下的遗憾边界与√log SA成比例，这无法从稀疏性中获益，因为只有少量状态-动作对产生成本。负熵正则化本质上无法适应稀疏性，在稀疏问题上会产生与√log S成比例的遗憾。

Method: 提出了一族ℓ_r-范数正则化器（r∈(1,2)），这些正则化器能够适应稀疏性，在已知转移设置下实现了与√log M成比例的遗憾，其中M是产生成本的状态-动作对数量。

Result: 在已知转移设置下，ℓ_r-范数正则化器实现了与√log M成比例的遗憾，这通过匹配下界证明是最优的，表明M捕获了问题的有效维度而不是SA。在未知转移设置下，稀疏性的好处有限，任何学习者的极小极大遗憾都与SA成多项式比例。

Conclusion: ℓ_r-范数正则化器能够有效适应稀疏成本结构，在已知转移设置下实现了最优的遗憾边界，突出了M作为问题有效维度的重要性。但在未知转移设置下，稀疏性的优势受到限制。

Abstract: We study the adversarial Stochastic Shortest Path (SSP) problem with sparse
costs under full-information feedback. In the known transition setting,
existing bounds based on Online Mirror Descent (OMD) with negative-entropy
regularization scale with $\sqrt{\log S A}$, where $SA$ is the size of the
state-action space. While we show that this is optimal in the worst-case, this
bound fails to capture the benefits of sparsity when only a small number $M \ll
SA$ of state-action pairs incur cost. In fact, we also show that the
negative-entropy is inherently non-adaptive to sparsity: it provably incurs
regret scaling with $\sqrt{\log S}$ on sparse problems. Instead, we propose a
family of $\ell_r$-norm regularizers ($r \in (1,2)$) that adapts to the
sparsity and achieves regret scaling with $\sqrt{\log M}$ instead of
$\sqrt{\log SA}$. We show this is optimal via a matching lower bound,
highlighting that $M$ captures the effective dimension of the problem instead
of $SA$. Finally, in the unknown transition setting the benefits of sparsity
are limited: we prove that even on sparse problems, the minimax regret for any
learner scales polynomially with $SA$.

</details>


### [59] [Aligning Brain Signals with Multimodal Speech and Vision Embeddings](https://arxiv.org/abs/2511.00065)
*Kateryna Shapovalenko,Quentin Auster*

Main category: cs.LG

TL;DR: 该研究探索了预训练模型的不同层如何反映大脑处理语言的分层过程，通过比较wav2vec2和CLIP模型的嵌入与脑电图信号的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 受大脑从原始声音到丰富多模态关联的分层处理过程启发，研究旨在确定预训练模型中哪些层最能反映大脑的这种分层处理机制。

Method: 使用自然语音感知期间记录的EEG数据，通过岭回归和对比解码评估wav2vec2和CLIP模型的嵌入与大脑活动的对齐程度，测试了三种策略：单独层、渐进连接和渐进求和。

Result: 研究发现结合多模态、层感知的表征可能更接近解码大脑如何理解语言，而不仅仅是声音处理。

Conclusion: 多模态和层感知表征的结合可能使我们更接近理解大脑如何将语言处理为经验而不仅仅是声音。

Abstract: When we hear the word "house", we don't just process sound, we imagine walls,
doors, memories. The brain builds meaning through layers, moving from raw
acoustics to rich, multimodal associations. Inspired by this, we build on
recent work from Meta that aligned EEG signals with averaged wav2vec2 speech
embeddings, and ask a deeper question: which layers of pre-trained models best
reflect this layered processing in the brain? We compare embeddings from two
models: wav2vec2, which encodes sound into language, and CLIP, which maps words
to images. Using EEG recorded during natural speech perception, we evaluate how
these embeddings align with brain activity using ridge regression and
contrastive decoding. We test three strategies: individual layers, progressive
concatenation, and progressive summation. The findings suggest that combining
multimodal, layer-aware representations may bring us closer to decoding how the
brain understands language, not just as sound, but as experience.

</details>


### [60] [Diluting Restricted Boltzmann Machines](https://arxiv.org/abs/2511.00648)
*C. Díaz-Faloh,R. Mulet*

Main category: cs.LG

TL;DR: 研究表明，受限玻尔兹曼机(RBMs)在训练前剪枝80%连接仍能保持良好生成性能，但训练后进一步剪枝会导致性能无法完全恢复，且重训练网络表现不如从头训练的同稀疏度网络。


<details>
  <summary>Details</summary>
Motivation: 针对大型神经网络计算和环境成本问题，研究更简单稀疏的网络是否能保持强性能，受彩票假说启发探索RBMs在极端剪枝条件下的表现。

Method: 研究受限玻尔兹曼机在极端剪枝条件下的表现，包括训练前剪枝和训练后剪枝对比，分析网络性能恢复能力和重训练效果。

Result: RBMs在训练前剪枝80%连接仍能保持高质量生成性能，但训练后进一步剪枝会导致性能急剧下降且无法通过重训练完全恢复。重训练网络表现不如从头训练的同稀疏度网络。

Conclusion: 稀疏网络要有效工作，剪枝应在训练早期实施而非训练后尝试。初始条件对网络能力有持久影响，为高效神经网络架构开发提供实践指导。

Abstract: Recent advances in artificial intelligence have relied heavily on
increasingly large neural networks, raising concerns about their computational
and environmental costs. This paper investigates whether simpler, sparser
networks can maintain strong performance by studying Restricted Boltzmann
Machines (RBMs) under extreme pruning conditions. Inspired by the Lottery
Ticket Hypothesis, we demonstrate that RBMs can achieve high-quality generative
performance even when up to 80% of the connections are pruned before training,
confirming that they contain viable sub-networks. However, our experiments
reveal crucial limitations: trained networks cannot fully recover lost
performance through retraining once additional pruning is applied. We identify
a sharp transition above which the generative quality degrades abruptly when
pruning disrupts a minimal core of essential connections. Moreover, re-trained
networks remain constrained by the parameters originally learned performing
worse than networks trained from scratch at equivalent sparsity levels. These
results suggest that for sparse networks to work effectively, pruning should be
implemented early in training rather than attempted afterwards. Our findings
provide practical insights for the development of efficient neural
architectures and highlight the persistent influence of initial conditions on
network capabilities.

</details>


### [61] [Token-Regulated Group Relative Policy Optimization for Stable Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2511.00066)
*Tue Le,Nghi D. Q. Bui,Linh Ngo Van,Trung Le*

Main category: cs.LG

TL;DR: 提出了TR-GRPO方法，通过基于token概率的权重调节解决GRPO中低概率token梯度过度放大的问题，提升强化学习训练的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法存在关键问题：低概率token由于其固有的较大梯度幅度会不成比例地主导梯度更新，导致训练不稳定并抑制高概率token的学习贡献。

Method: TR-GRPO是GRPO的简单有效扩展，通过分配与模型预测概率正相关的token级权重，降低低概率token的权重并强调高概率token，从而减轻梯度过度放大同时保留信息性学习信号。

Result: 大量实验表明TR-GRPO在逻辑、数学和智能体推理等RLVR任务中持续优于GRPO。

Conclusion: 证明了在RL训练中调节token贡献的重要性，并确立了TR-GRPO作为增强LLM推理的稳健框架。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful approach for strengthening the reasoning capabilities of large
language models (LLMs). Among existing algorithms, Group Relative Policy
Optimization (GRPO) has demonstrated strong performance, yet it suffers from a
critical issue: low-probability tokens disproportionately dominate gradient
updates due to their inherently large gradient magnitudes. This imbalance leads
to unstable training and suppresses the contribution of high-probability tokens
that are more reliable for learning. In this work, we introduce Token-Regulated
Group Relative Policy Optimization (TR-GRPO), a simple yet effective extension
of GRPO that assigns token-level weights positively correlated with the model's
predicted probability. By downweighting low-probability tokens and emphasizing
high-probability ones, TR-GRPO mitigates gradient over-amplification while
preserving informative learning signals. Extensive experiments demonstrate that
TR-GRPO consistently outperforms GRPO across RLVR tasks, including logic, math,
and agentic reasoning, highlighting the importance of regulating token
contributions during RL training and establishing TR-GRPO as a robust framework
for enhancing LLM reasoning.

</details>


### [62] [Investigating the Robustness of Knowledge Tracing Models in the Presence of Student Concept Drift](https://arxiv.org/abs/2511.00704)
*Morgan Lee,Artem Frenk,Eamon Worden,Karish Gupta,Thinh Pham,Ethan Croteau,Neil Heffernan*

Main category: cs.LG

TL;DR: 该论文研究了知识追踪模型在在线学习平台中面对概念漂移和学生群体变化时的稳定性问题，发现所有模型都会出现性能下降，其中贝叶斯知识追踪模型最为稳定，而复杂的注意力模型性能下降最快。


<details>
  <summary>Details</summary>
Motivation: 传统知识追踪模型假设学习过程是静态的，但在线学习平台中学生行为和概念内容会随时间变化，需要评估模型对这种变化的适应性。

Method: 使用四种经典知识追踪模型，在五个学年数据上进行测试，分析模型在单学年内和跨学年的性能变化。

Result: 所有知识追踪模型都会出现性能下降，贝叶斯知识追踪模型在应对新数据时最稳定，而复杂的注意力模型预测能力下降最快。

Conclusion: 知识追踪模型容易受到概念漂移的影响，需要更多纵向评估，贝叶斯知识追踪模型在变化环境中表现最为稳健。

Abstract: Knowledge Tracing (KT) has been an established problem in the educational
data mining field for decades, and it is commonly assumed that the underlying
learning process be- ing modeled remains static. Given the ever-changing land-
scape of online learning platforms (OLPs), we investigate how concept drift and
changing student populations can im- pact student behavior within an OLP
through testing model performance both within a single academic year and across
multiple academic years. Four well-studied KT models were applied to five
academic years of data to assess how suscep- tible KT models are to concept
drift. Through our analysis, we find that all four families of KT models can
exhibit de- graded performance, Bayesian Knowledge Tracing (BKT) remains the
most stable KT model when applied to newer data, while more complex, attention
based models lose pre- dictive power significantly faster. To foster more
longitu- dinal evaluations of KT models, the data used to conduct our analysis
is available at https://osf.io/hvfn9/?view_
only=b936c63dfdae4b0b987a2f0d4038f72a

</details>


### [63] [Latent Domain Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2511.00067)
*Zhixing Li,Arsham Gholamzadeh Khoee,Yinan Yu*

Main category: cs.LG

TL;DR: 提出了一种无需显式域标签的领域泛化方法，通过自动发现训练数据中的潜在域，将未见目标域表示为这些潜在域的组合，实现跨域知识自适应迁移。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法大多依赖可能不可用且模糊的域标签，因此研究无需显式域标签的领域泛化设置，这对于视觉语言模型在现实应用中的部署至关重要。

Method: 在图像特征上进行潜在域聚类，并根据输入图像与每个潜在域的相似度融合域特定文本特征，实现自适应知识迁移。

Result: 在四个基准测试上的实验表明，该方法相比基于VLM的基线方法获得了持续的性能提升。

Conclusion: 该方法在领域偏移下提高了模型的鲁棒性，并为改进领域泛化提供了新的思路。

Abstract: The objective of domain generalization (DG) is to enable models to be robust
against domain shift. DG is crucial for deploying vision-language models (VLMs)
in real-world applications, yet most existing methods rely on domain labels
that may not be available and often ambiguous. We instead study the DG setting
where models must generalize well without access to explicit domain labels. Our
key idea is to represent an unseen target domain as a combination of latent
domains automatically discovered from training data, enabling the model to
adaptively transfer knowledge across domains. To realize this, we perform
latent domain clustering on image features and fuse domain-specific text
features based on the similarity between the input image and each latent
domain. Experiments on four benchmarks show that this strategy yields
consistent gains over VLM-based baselines and provides new insights into
improving robustness under domain shift.

</details>


### [64] [Random Spiking Neural Networks are Stable and Spectrally Simple](https://arxiv.org/abs/2511.00904)
*Ernesto Araya,Massimiliano Datres,Gitta Kutyniok*

Main category: cs.LG

TL;DR: 该论文通过布尔函数分析研究LIF-SNN的稳定性，发现宽网络分类器具有平均稳定性，并引入谱简单性概念解释深度网络的简单性偏差。


<details>
  <summary>Details</summary>
Motivation: SNN在能效计算方面有前景，但其理论基础（特别是稳定性和鲁棒性）相比人工神经网络仍有限，需要深入研究。

Method: 使用布尔函数分析研究离散时间LIF-SNN，重点关注噪声敏感性和稳定性，分析输入扰动对输出的影响。

Result: 主要结果表明宽LIF-SNN分类器具有平均稳定性，这可通过其傅里叶谱集中在低频分量来解释。随机LIF-SNN偏向简单函数。

Conclusion: 研究为SNN的稳定性和鲁棒性提供了新见解，实验证实这些稳定性特性在实践中持续存在。

Abstract: Spiking neural networks (SNNs) are a promising paradigm for energy-efficient
computation, yet their theoretical foundations-especially regarding stability
and robustness-remain limited compared to artificial neural networks. In this
work, we study discrete-time leaky integrate-and-fire (LIF) SNNs through the
lens of Boolean function analysis. We focus on noise sensitivity and stability
in classification tasks, quantifying how input perturbations affect outputs.
Our main result shows that wide LIF-SNN classifiers are stable on average, a
property explained by the concentration of their Fourier spectrum on
low-frequency components. Motivated by this, we introduce the notion of
spectral simplicity, which formalizes simplicity in terms of Fourier spectrum
concentration and connects our analysis to the simplicity bias observed in deep
networks. Within this framework, we show that random LIF-SNNs are biased toward
simple functions. Experiments on trained networks confirm that these stability
properties persist in practice. Together, these results provide new insights
into the stability and robustness properties of SNNs.

</details>


### [65] [Benchmarking Generative AI Against Bayesian Optimization for Constrained Multi-Objective Inverse Design](https://arxiv.org/abs/2511.00070)
*Muhammad Bilal Awan,Abdul Razzaq,Abdul Shahid*

Main category: cs.LG

TL;DR: 该研究比较了大型语言模型与贝叶斯优化在约束多目标回归任务中的表现，发现专用贝叶斯优化框架性能最优，但微调后的LLMs是计算速度快的有前景替代方案。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在约束连续高维数值空间中的优化能力，这些任务并非LLMs的原始设计目标，但在材料信息学中具有重要应用价值。

Method: 使用贝叶斯优化框架（BoTorch Ax和qEHVI）与微调LLMs（包括WizardMath-7B）进行对比研究，通过参数高效微调将问题构建为回归任务。

Result: BoTorch qEHVI实现了完美收敛（GD=0.0），而最佳LLM（WizardMath-7B）的GD为1.21，显著优于传统BoTorch Ax基线（GD=15.03）。

Conclusion: 专用BO框架在保证收敛方面仍是性能领导者，但微调LLMs被验证为有前景的计算快速替代方案，为AI驱动优化领域提供了重要比较指标。

Abstract: This paper investigates the performance of Large Language Models (LLMs) as
generative optimizers for solving constrained multi-objective regression tasks,
specifically within the challenging domain of inverse design
(property-to-structure mapping). This problem, critical to materials
informatics, demands finding complex, feasible input vectors that lie on the
Pareto optimal front. While LLMs have demonstrated universal effectiveness
across generative and reasoning tasks, their utility in constrained,
continuous, high-dimensional numerical spaces tasks they weren't explicitly
architected for remains an open research question. We conducted a rigorous
comparative study between established Bayesian Optimization (BO) frameworks and
a suite of fine-tuned LLMs and BERT models. For BO, we benchmarked the
foundational BoTorch Ax implementation against the state-of-the-art q-Expected
Hypervolume Improvement (qEHVI, BoTorchM). The generative approach involved
fine-tuning models via Parameter-Efficient Fine-Tuning (PEFT), framing the
challenge as a regression problem with a custom output head. Our results show
that BoTorch qEHVI achieved perfect convergence (GD=0.0), setting the
performance ceiling. Crucially, the best-performing LLM (WizardMath-7B)
achieved a Generational Distance (GD) of 1.21, significantly outperforming the
traditional BoTorch Ax baseline (GD=15.03). We conclude that specialized BO
frameworks remain the performance leader for guaranteed convergence, but
fine-tuned LLMs are validated as a promising, computationally fast alternative,
contributing essential comparative metrics to the field of AI-driven
optimization. The findings have direct industrial applications in optimizing
formulation design for resins, polymers, and paints, where multi-objective
trade-offs between mechanical, rheological, and chemical properties are
critical to innovation and production efficiency.

</details>


### [66] [The Hidden Power of Normalization: Exponential Capacity Control in Deep Neural Networks](https://arxiv.org/abs/2511.00958)
*Khoat Than*

Main category: cs.LG

TL;DR: 该论文从容量控制角度解释了归一化层在深度神经网络中的作用，证明归一化能指数级减小网络的Lipschitz常数，从而平滑损失景观并约束网络容量，改善优化和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 归一化方法在现代深度神经网络中至关重要，但它们在优化和泛化方面的理论机制仍不清楚，特别是在使用多个归一化层时。

Method: 建立理论框架，从容量控制角度分析归一化作用，证明未归一化网络的Lipschitz常数可能指数级增长，而插入归一化层能指数级减小该常数。

Result: 归一化层能指数级减小Lipschitz常数，这平滑了损失景观（促进优化）并约束了网络容量（提升泛化）。

Conclusion: 该工作为归一化方法在深度学习中的经验成功提供了原则性解释，揭示了其通过指数级减小Lipschitz常数来改善优化和泛化的机制。

Abstract: Normalization methods are fundamental components of modern deep neural
networks (DNNs). Empirically, they are known to stabilize optimization dynamics
and improve generalization. However, the underlying theoretical mechanism by
which normalization contributes to both optimization and generalization remains
largely unexplained, especially when using many normalization layers in a DNN
architecture.
  In this work, we develop a theoretical framework that elucidates the role of
normalization through the lens of capacity control. We prove that an
unnormalized DNN can exhibit exponentially large Lipschitz constants with
respect to either its parameters or inputs, implying excessive functional
capacity and potential overfitting. Such bad DNNs are uncountably many. In
contrast, the insertion of normalization layers provably can reduce the
Lipschitz constant at an exponential rate in the number of normalization
operations. This exponential reduction yields two fundamental consequences: (1)
it smooths the loss landscape at an exponential rate, facilitating faster and
more stable optimization; and (2) it constrains the effective capacity of the
network, thereby enhancing generalization guarantees on unseen data. Our
results thus offer a principled explanation for the empirical success of
normalization methods in deep learning.

</details>


### [67] [Wavelet-Based Feature Extraction and Unsupervised Clustering for Parity Detection: A Feature Engineering Perspective](https://arxiv.org/abs/2511.00071)
*Ertugrul Mutlu*

Main category: cs.LG

TL;DR: 使用小波特征提取和无监督聚类的方法解决奇偶检测问题，无需标签监督达到69.67%的分类准确率


<details>
  <summary>Details</summary>
Motivation: 探索将经典信号处理技术应用于纯离散符号领域，连接符号推理和基于特征的学习

Method: 将整数转换为小波域表示，提取多尺度统计特征，使用k-means算法进行聚类

Result: 在无监督情况下实现了69.67%的分类准确率，特征空间揭示了奇偶数之间的结构性差异

Conclusion: 经典信号处理技术即使在纯离散符号领域也能发现潜在结构，为特征工程和聚类在非常规机器学习问题中的应用提供了新视角

Abstract: This paper explores a deliberately over-engineered approach to the classical
problem of parity detection -- determining whether a number is odd or even --
by combining wavelet-based feature extraction with unsupervised clustering.
Instead of relying on modular arithmetic, integers are transformed into
wavelet-domain representations, from which multi-scale statistical features are
extracted and clustered using the k-means algorithm. The resulting feature
space reveals meaningful structural differences between odd and even numbers,
achieving a classification accuracy of approximately 69.67% without any label
supervision. These results suggest that classical signal-processing techniques,
originally designed for continuous data, can uncover latent structure even in
purely discrete symbolic domains. Beyond parity detection, the study provides
an illustrative perspective on how feature engineering and clustering may be
repurposed for unconventional machine learning problems, potentially bridging
symbolic reasoning and feature-based learning.

</details>


### [68] [Happiness as a Measure of Fairness](https://arxiv.org/abs/2511.01069)
*Georg Pichler,Marco Romanelli,Pablo Piantanida*

Main category: cs.LG

TL;DR: 提出基于幸福度概念的新公平性框架，通过群体从决策结果中获得的效用来衡量公平性，提供更人性化且数学严谨的方法。


<details>
  <summary>Details</summary>
Motivation: 现有公平性定义缺乏直观的人类中心视角，需要一种既能反映群体实际效用感受又具有数学严谨性的公平性框架。

Method: 使用线性规划计算最优的公平后处理策略，该方法高效且可扩展，能够统一和扩展多个已知的公平性定义。

Result: 实证结果表明该方法在多种场景下具有实际优势，能够有效平衡公平性和实用性。

Conclusion: 基于幸福度的公平性框架提供了一个直观、数学严谨且实用的公平性度量方法，能够统一现有公平性概念并具有良好的可扩展性。

Abstract: In this paper, we propose a novel fairness framework grounded in the concept
of happi- ness, a measure of the utility each group gains fromdecisionoutcomes.
Bycapturingfairness through this intuitive lens, we not only offer a more
human-centered approach, but also one that is mathematically rigorous: In order
to compute the optimal, fair post-processing strategy, only a linear program
needs to be solved. This makes our method both efficient and scalable with
existing optimization tools. Furthermore, it unifies and extends several
well-known fairness definitions, and our em- pirical results highlight its
practical strengths across diverse scenarios.

</details>


### [69] [Bridging Vision, Language, and Mathematics: Pictographic Character Reconstruction with Bézier Curves](https://arxiv.org/abs/2511.00076)
*Zihao Wan,Pau Tong Lin Xu,Fuwen Luo,Ziyue Wang,Peng Li,Yang Liu*

Main category: cs.LG

TL;DR: 该研究探索视觉语言模型对图像几何结构的理解能力，通过将象形文字识别转化为程序合成任务，训练模型将光栅图像反编译为由贝塞尔曲线组成的几何程序。


<details>
  <summary>Details</summary>
Motivation: 虽然视觉语言模型在语义理解方面表现出色，但其对视觉信息几何结构的理解能力尚未充分探索。象形文字结合了视觉形式和符号结构，是测试这种能力的理想案例。

Method: 将视觉识别问题在数学领域形式化，每个字符表示为可执行的几何基元程序。训练视觉语言模型将光栅图像反编译为由贝塞尔曲线组成的程序，作为"视觉反编译器"。

Result: 模型性能优于包括GPT-4o在内的强零样本基线。关键发现是仅在现代汉字上训练的模型能够零样本重建古代甲骨文，表明模型获得了抽象且可迁移的几何语法。

Conclusion: 该研究证明视觉语言模型能够超越像素级模式识别，获得结构化的视觉理解能力，掌握可迁移的抽象几何语法。

Abstract: While Vision-language Models (VLMs) have demonstrated strong semantic
capabilities, their ability to interpret the underlying geometric structure of
visual information is less explored. Pictographic characters, which combine
visual form with symbolic structure, provide an ideal test case for this
capability. We formulate this visual recognition challenge in the mathematical
domain, where each character is represented by an executable program of
geometric primitives. This is framed as a program synthesis task, training a
VLM to decompile raster images into programs composed of B\'ezier curves. Our
model, acting as a "visual decompiler", demonstrates performance superior to
strong zero-shot baselines, including GPT-4o. The most significant finding is
that when trained solely on modern Chinese characters, the model is able to
reconstruct ancient Oracle Bone Script in a zero-shot context. This
generalization provides strong evidence that the model acquires an abstract and
transferable geometric grammar, moving beyond pixel-level pattern recognition
to a more structured form of visual understanding.

</details>


### [70] [Regularization Implies balancedness in the deep linear network](https://arxiv.org/abs/2511.01137)
*Kathryn Lindsey,Govind Menon*

Main category: cs.LG

TL;DR: 使用几何不变量理论研究深度线性网络，通过Kempf-Ness定理证明L2正则化器在平衡流形上最小化，将训练动态分解为纤维上的正则化流和平衡流形上的学习流。


<details>
  <summary>Details</summary>
Motivation: 为深度学习和线性系统理论中的平衡性提供统一的数学框架，从模型简化和贝叶斯原理角度解释平衡性。

Method: 应用几何不变量理论和Kempf-Ness定理，将训练动态分解为两个梯度流：纤维上的正则化流（使用矩映射精确求解）和平衡流形上的学习流。

Result: 建立了L2正则化器在平衡流形上最小化的理论，提供了训练动态的分解框架，并实现了正则化流的精确求解。

Conclusion: 该方法为深度学习和线性系统理论中的平衡性概念提供了统一的数学解释框架，连接了模型简化和贝叶斯原理视角。

Abstract: We use geometric invariant theory (GIT) to study the deep linear network
(DLN). The Kempf-Ness theorem is used to establish that the $L^2$ regularizer
is minimized on the balanced manifold. This allows us to decompose the training
dynamics into two distinct gradient flows: a regularizing flow on fibers and a
learning flow on the balanced manifold. We show that the regularizing flow is
exactly solvable using the moment map.
  This approach provides a common mathematical framework for balancedness in
deep learning and linear systems theory. We use this framework to interpret
balancedness in terms of model reduction and Bayesian principles.

</details>


### [71] [flowengineR: A Modular and Extensible Framework for Fair and Reproducible Workflow Design in R](https://arxiv.org/abs/2511.00079)
*Maximilian Willer,Peter Ruckdeschel*

Main category: cs.LG

TL;DR: flowengineR是一个R包，为构建可重现的机器学习流程提供模块化、可扩展的框架，特别关注算法公平性领域。


<details>
  <summary>Details</summary>
Motivation: 算法公平性领域快速发展，新指标、缓解策略和机器学习方法不断涌现，现有工具要么过于专注单一干预，要么将可重现性和可扩展性视为次要考虑。

Method: 引入统一架构，包含数据分割、执行、预处理、训练、处理中、后处理、评估和报告等标准化引擎，每个引擎封装一个方法任务并通过轻量级接口通信。

Result: 构建了透明、可审计且易于扩展的工作流，使研究人员能够在建模流程中集成、比较和评估公平性干预措施。

Conclusion: 虽然受公平性驱动，但该架构可推广到可解释性、鲁棒性和合规性指标，为任何需要可重现性、透明度和可扩展性的工作流上下文提供通用基础设施。

Abstract: flowengineR is an R package designed to provide a modular and extensible
framework for building reproducible algorithmic workflows for general-purpose
machine learning pipelines. It is motivated by the rapidly evolving field of
algorithmic fairness where new metrics, mitigation strategies, and machine
learning methods continuously emerge. A central challenge in fairness, but also
far beyond, is that existing toolkits either focus narrowly on single
interventions or treat reproducibility and extensibility as secondary
considerations rather than core design principles. flowengineR addresses this
by introducing a unified architecture of standardized engines for data
splitting, execution, preprocessing, training, inprocessing, postprocessing,
evaluation, and reporting. Each engine encapsulates one methodological task yet
communicates via a lightweight interface, ensuring workflows remain
transparent, auditable, and easily extensible. Although implemented in R,
flowengineR builds on ideas from workflow languages (CWL, YAWL), graph-oriented
visual programming languages (KNIME), and R frameworks (BatchJobs, batchtools).
Its emphasis, however, is less on orchestrating engines for resilient parallel
execution but rather on the straightforward setup and management of distinct
engines as data structures. This orthogonalization enables distributed
responsibilities, independent development, and streamlined integration. In
fairness context, by structuring fairness methods as interchangeable engines,
flowengineR lets researchers integrate, compare, and evaluate interventions
across the modeling pipeline. At the same time, the architecture generalizes to
explainability, robustness, and compliance metrics without core modifications.
While motivated by fairness, it ultimately provides a general infrastructure
for any workflow context where reproducibility, transparency, and extensibility
are essential.

</details>


### [72] [Analyzing the Power of Chain of Thought through Memorization Capabilities](https://arxiv.org/abs/2511.01190)
*Lijia Yu,Xiao-Shan Gao,Lijun Zhang*

Main category: cs.LG

TL;DR: 本文研究了思维链（CoT）是否在所有推理任务中都能增强Transformer的能力，发现推理本质上是记忆问题，并给出了固定精度Transformer在有无CoT情况下的记忆能力完整描述。


<details>
  <summary>Details</summary>
Motivation: 探索思维链（CoT）是否在所有推理任务中都能增强Transformer的能力，回答CoT是否普遍扩展Transformer推理能力的基本问题。

Method: 将Transformer推理建模为记忆问题，分析固定精度Transformer在有/无CoT情况下的记忆能力，给出记忆有限和无限推理数据集的必要条件、充分条件及参数数量边界。

Result: 证明了有/无CoT的Transformer记忆有限数据集的条件互不蕴含，参数数量边界均为Θ(N)；发现存在CoT无法增强Transformer推理能力的任务；某些简单无限数据集无法被有/无CoT的Transformer记忆。

Conclusion: CoT并不能在所有推理任务中增强Transformer的能力，Transformer的推理能力本质上受限于其记忆能力，存在CoT无法提供帮助的推理任务。

Abstract: It has been shown that the chain of thought (CoT) can enhance the power of
large language models (LLMs) to solve certain mathematical reasoning problems.
However, the capacity of CoT is still not fully explored. As an important
instance, the following basic question has not yet been answered: Does CoT
expand the capability of transformers across all reasoning tasks? We
demonstrate that reasoning with transformers is essentially a memorization
problem for reasoning datasets. Thus, examining the power of CoT across all
reasoning tasks amounts to analyzing the memorization capabilities of CoT
transformers. In this paper, we give a complete description of the memorization
capabilities of fixed-precision transformers with or without CoT and give a
negative answer to the above-mentioned question. Precisely, we first give
necessary and sufficient conditions for fixed-precision transformers with and
without CoT to memorize a finite reasoning dataset and show that these two
conditions do not imply each other. Then, we give lower and upper bounds for
the number of parameters needed for transformers with or without CoT to
memorize a finite reasoning dataset with $N$ elements, which are
$\overline{\Theta}(N)$ in all cases. This implies that there exist reasoning
tasks for which CoT does not enhance the reasoning power of transformers,
leading to a negative answer to the above-mentioned question. Finally, we give
the first results on memorizing infinite reasoning datasets by CoT transformers
and show that some simple infinite datasets cannot be memorized by transformers
with or without CoT.

</details>


### [73] [Fixed-point graph convolutional networks against adversarial attacks](https://arxiv.org/abs/2511.00083)
*Shakib Khan,A. Ben Hamza,Amr Youssef*

Main category: cs.LG

TL;DR: 提出Fix-GCN模型，通过固定点迭代图卷积网络实现对抗攻击鲁棒性，使用频谱调制滤波器选择性衰减高频成分并保留低频结构信息。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击严重威胁图神经网络在图形结构和节点特征任务中的完整性，需要设计无需额外内存或计算复杂度的鲁棒防御机制。

Method: 引入通用频谱调制滤波器，通过固定点迭代推导特征传播规则，实现灵活通滤波方法，迭代更新节点表示。

Result: 在多个基准图数据集上的实验证明该模型能有效抵抗对抗攻击，保持图结构信息的完整性。

Conclusion: Fix-GCN提供了一个灵活高效的框架，在减轻对抗操纵影响的同时保留基本图信息，具有实际应用价值。

Abstract: Adversarial attacks present a significant risk to the integrity and
performance of graph neural networks, particularly in tasks where graph
structure and node features are vulnerable to manipulation. In this paper, we
present a novel model, called fixed-point iterative graph convolutional network
(Fix-GCN), which achieves robustness against adversarial perturbations by
effectively capturing higher-order node neighborhood information in the graph
without additional memory or computational complexity. Specifically, we
introduce a versatile spectral modulation filter and derive the feature
propagation rule of our model using fixed-point iteration. Unlike traditional
defense mechanisms that rely on additional design elements to counteract
attacks, the proposed graph filter provides a flexible-pass filtering approach,
allowing it to selectively attenuate high-frequency components while preserving
low-frequency structural information in the graph signal. By iteratively
updating node representations, our model offers a flexible and efficient
framework for preserving essential graph information while mitigating the
impact of adversarial manipulation. We demonstrate the effectiveness of the
proposed model through extensive experiments on various benchmark graph
datasets, showcasing its resilience against adversarial attacks.

</details>


### [74] [A Saddle Point Remedy: Power of Variable Elimination in Non-convex Optimization](https://arxiv.org/abs/2511.01234)
*Min Gan,Guang-Yong Chen,Yang Yi,Lin Yang*

Main category: cs.LG

TL;DR: 变量消除算法通过将原始优化问题中的鞍点转化为简化问题中的局部最大值，从根本上重塑了临界点结构，从而更有效地导航复杂能量景观。


<details>
  <summary>Details</summary>
Motivation: 理解变量消除算法在实践中表现出的优越收敛性和鲁棒性的原理机制，特别是它们如何有效应对大规模非凸优化中的鞍点问题。

Method: 基于Hessian惯性和Schur补的严谨几何分析，比较原始和简化公式的优化景观，证明变量消除如何重塑临界点结构。

Result: 在非凸矩阵分解、二维参数神经网络和深度残差网络训练中验证了方法的有效性，显著提高了稳定性和收敛到更优极小值的能力。

Conclusion: 通过鞍点变换实现景观简化是一个强大原则，可以指导设计新一代更鲁棒高效的优化算法。

Abstract: The proliferation of saddle points, rather than poor local minima, is
increasingly understood to be a primary obstacle in large-scale non-convex
optimization for machine learning. Variable elimination algorithms, like
Variable Projection (VarPro), have long been observed to exhibit superior
convergence and robustness in practice, yet a principled understanding of why
they so effectively navigate these complex energy landscapes has remained
elusive. In this work, we provide a rigorous geometric explanation by comparing
the optimization landscapes of the original and reduced formulations. Through a
rigorous analysis based on Hessian inertia and the Schur complement, we prove
that variable elimination fundamentally reshapes the critical point structure
of the objective function, revealing that local maxima in the reduced landscape
are created from, and correspond directly to, saddle points in the original
formulation. Our findings are illustrated on the canonical problem of
non-convex matrix factorization, visualized directly on two-parameter neural
networks, and finally validated in training deep Residual Networks, where our
approach yields dramatic improvements in stability and convergence to superior
minima. This work goes beyond explaining an existing method; it establishes
landscape simplification via saddle point transformation as a powerful
principle that can guide the design of a new generation of more robust and
efficient optimization algorithms.

</details>


### [75] [Application of predictive machine learning in pen & paper RPG game design](https://arxiv.org/abs/2511.00084)
*Jolanta Śliwa*

Main category: cs.LG

TL;DR: 该论文探讨了使用序数回归技术预测笔纸RPG中怪物等级的方法，构建了专用数据集并开发了人类启发模型作为基准，以解决当前依赖人工测试和专家评估的耗时问题。


<details>
  <summary>Details</summary>
Motivation: 随着笔纸RPG市场增长，公司寻求AI技术提升玩家体验。当前怪物等级设计依赖人工测试和专家评估，虽然准确但耗时耗力，需要自动化解决方案。

Method: 使用序数回归技术进行等级预测，构建专用数据集，开发人类启发模型作为基准，设计基于领域知识的专门评估程序来比较机器学习算法与传统方法。

Result: 论文提供了最先进方法的概述和评估，建立了用于等级估计的数据集，并开发了基准模型来比较机器学习方法与人类评估方法的表现。

Conclusion: 该研究为笔纸RPG出版商提供了自动化的怪物等级预测方法，通过机器学习技术可以替代传统耗时的人工评估过程，同时设计了专门的评估框架来确保模型性能的可比性。

Abstract: In recent years, the pen and paper RPG market has experienced significant
growth. As a result, companies are increasingly exploring the integration of AI
technologies to enhance player experience and gain a competitive edge.
  One of the key challenges faced by publishers is designing new opponents and
estimating their challenge level. Currently, there are no automated methods for
determining a monster's level; the only approaches used are based on manual
testing and expert evaluation. Although these manual methods can provide
reasonably accurate estimates, they are time-consuming and resource-intensive.
  Level prediction can be approached using ordinal regression techniques. This
thesis presents an overview and evaluation of state-of-the-art methods for this
task. It also details the construction of a dedicated dataset for level
estimation. Furthermore, a human-inspired model was developed to serve as a
benchmark, allowing comparison between machine learning algorithms and the
approach typically employed by pen and paper RPG publishers. In addition, a
specialized evaluation procedure, grounded in domain knowledge, was designed to
assess model performance and facilitate meaningful comparisons.

</details>


### [76] [MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials](https://arxiv.org/abs/2511.00113)
*Huseyin Goksu*

Main category: cs.LG

TL;DR: 提出了MeixnerNet，一种使用离散正交多项式（Meixner多项式）的谱图神经网络，解决了传统基于连续域多项式滤波器与离散图结构不匹配的问题，并具有更好的超参数鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统谱GNN使用连续正交多项式（如Chebyshev）作为滤波器，但这些连续域滤波器应用于离散图结构存在理论不匹配，可能导致性能不佳和对超参数设置的脆弱性。

Method: 引入MeixnerNet架构，使用离散正交Meixner多项式，并将多项式的两个关键形状参数β和c设为可学习参数，使滤波器能够适应特定图的谱特性。通过结合拉普拉斯缩放和逐基LayerNorm的新稳定化技术来解决数值不稳定性问题。

Result: 在最优K=2设置下，MeixnerNet在2/3基准测试中优于强基线ChebyNet。更重要的是，MeixnerNet对多项式阶数K的变化具有异常鲁棒性，而ChebyNet对此高度脆弱，在性能崩溃时MeixnerNet仍保持稳定。

Conclusion: MeixnerNet通过使用离散正交多项式解决了谱GNN中连续域滤波器与离散图结构的不匹配问题，不仅实现了竞争性性能，更重要的是显著提高了对关键超参数（多项式阶数K）的鲁棒性。

Abstract: Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art results
by defining graph convolutions in the spectral domain. A common approach,
popularized by ChebyNet, is to use polynomial filters based on continuous
orthogonal polynomials (e.g., Chebyshev). This creates a theoretical
disconnect, as these continuous-domain filters are applied to inherently
discrete graph structures. We hypothesize this mismatch can lead to suboptimal
performance and fragility to hyperparameter settings.
  In this paper, we introduce MeixnerNet, a novel spectral GNN architecture
that employs discrete orthogonal polynomials -- specifically, the Meixner
polynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters of
the polynomial, beta and c, learnable, allowing the filter to adapt its
polynomial basis to the specific spectral properties of a given graph. We
overcome the significant numerical instability of these polynomials by
introducing a novel stabilization technique that combines Laplacian scaling
with per-basis LayerNorm.
  We demonstrate experimentally that MeixnerNet achieves
competitive-to-superior performance against the strong ChebyNet baseline at the
optimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, we
show that MeixnerNet is exceptionally robust to variations in the polynomial
degree K, a hyperparameter to which ChebyNet proves to be highly fragile,
collapsing in performance where MeixnerNet remains stable.

</details>


### [77] [A Spatio-Temporal Online Robust Tensor Recovery Approach for Streaming Traffic Data Imputation](https://arxiv.org/abs/2511.01267)
*Yiyang Yang,Xiejian Chi,Shanxing Gao,Kaidong Wang,Yao Wang*

Main category: cs.LG

TL;DR: 提出了一种新颖的在线鲁棒张量恢复算法，用于智能交通系统中的交通数据恢复，能够同时处理缺失值和异常值，在保持高恢复精度的同时显著提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统批处理方法在计算和存储资源方面需求大，限制了在持续增长的交通数据量下的可扩展性；现有的在线张量恢复方法在复杂现实场景中性能下降严重，未能充分利用交通数据的内在结构特性。

Method: 将交通数据恢复问题重新表述为流式框架，提出一种同时利用交通数据全局时空相关性和局部一致性的在线鲁棒张量恢复算法。

Result: 在三个真实交通数据集上的实验结果表明，该方法实现了高恢复精度，同时与最先进的批处理方法相比，计算效率提高了三个数量级。

Conclusion: 该方法作为智能交通系统中交通数据质量增强的可扩展有效解决方案具有巨大潜力。

Abstract: Data quality is critical to Intelligent Transportation Systems (ITS), as
complete and accurate traffic data underpin reliable decision-making in traffic
control and management. Recent advances in low-rank tensor recovery algorithms
have shown strong potential in capturing the inherent structure of
high-dimensional traffic data and restoring degraded observations. However,
traditional batch-based methods demand substantial computational and storage
resources, which limits their scalability in the face of continuously expanding
traffic data volumes. Moreover, recent online tensor recovery methods often
suffer from severe performance degradation in complex real-world scenarios due
to their insufficient exploitation of the intrinsic structural properties of
traffic data. To address these challenges, we reformulate the traffic data
recovery problem within a streaming framework, and propose a novel online
robust tensor recovery algorithm that simultaneously leverages both the global
spatio-temporal correlations and local consistency of traffic data, achieving
high recovery accuracy and significantly improved computational efficiency in
large-scale scenarios. Our method is capable of simultaneously handling missing
and anomalous values in traffic data, and demonstrates strong adaptability
across diverse missing patterns. Experimental results on three real-world
traffic datasets demonstrate that the proposed approach achieves high recovery
accuracy while significantly improving computational efficiency by up to three
orders of magnitude compared to state-of-the-art batch-based methods. These
findings highlight the potential of the proposed approach as a scalable and
effective solution for traffic data quality enhancement in ITS.

</details>


### [78] [MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning](https://arxiv.org/abs/2511.00085)
*Peilin Tan,Chuanqi Shi,Dian Tu,Liang Xie*

Main category: cs.LG

TL;DR: 提出MaGNet模型，结合Mamba双向时序建模、双超图框架和时空注意力机制，用于股票趋势预测，在多个股票指数上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有股票预测方法难以有效捕捉时序依赖和动态股票间交互，常忽略横截面市场影响、依赖静态相关性、对节点和边采用统一处理，以及混淆不同关系。

Method: 使用MAGE块（双向Mamba+自适应门控机制+稀疏MoE层+多头注意力）、特征和股票维度的2D时空注意力模块、双超图框架（时序因果超图TCH和全局概率超图GPH）。

Result: 在六个主要股票指数上的广泛实验表明，MaGNet在预测性能和投资回报方面均优于最先进方法，并具有稳健的风险管理能力。

Conclusion: MaGNet通过整合时序建模、关系推理和多尺度关系学习，为股票预测提供了有效的解决方案。

Abstract: Stock trend prediction is crucial for profitable trading strategies and
portfolio management yet remains challenging due to market volatility, complex
temporal dynamics and multifaceted inter-stock relationships. Existing methods
struggle to effectively capture temporal dependencies and dynamic inter-stock
interactions, often neglecting cross-sectional market influences, relying on
static correlations, employing uniform treatments of nodes and edges, and
conflating diverse relationships. This work introduces MaGNet, a novel Mamba
dual-hyperGraph Network for stock prediction, integrating three key
innovations: (1) a MAGE block, which leverages bidirectional Mamba with
adaptive gating mechanisms for contextual temporal modeling and integrates a
sparse Mixture-of-Experts layer to enable dynamic adaptation to diverse market
conditions, alongside multi-head attention for capturing global dependencies;
(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enable
precise fusion of multivariate features and cross-stock dependencies,
effectively enhancing informativeness while preserving intrinsic data
structures, bridging temporal modeling with relational reasoning; and (3) a
dual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)
that captures fine-grained causal dependencies with temporal constraints, and
Global Probabilistic Hypergraph (GPH) that models market-wide patterns through
soft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,
jointly disentangling localized temporal influences from instantaneous global
structures for multi-scale relational learning. Extensive experiments on six
major stock indices demonstrate MaGNet outperforms state-of-the-art methods in
both superior predictive performance and exceptional investment returns with
robust risk management capabilities. Codes available at:
https://github.com/PeilinTime/MaGNet.

</details>


### [79] [Casing Collar Identification using AlexNet-based Neural Networks for Depth Measurement in Oil and Gas Wells](https://arxiv.org/abs/2511.00129)
*Siyu Xiao,Xindi Zhao,Tianhao Mao,Yiwei Wang,Yuqiao Chen,Hongyun Zhang,Jian Wang,Junjie Wang,Shuang Liu,Tupei Chen,Yang Liu*

Main category: cs.LG

TL;DR: 本文提出了一种集成到井下工具中的CCL信号采集系统，用于构建数据集，并评估了数据增强预处理方法对基于AlexNet的套管接箍识别模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 准确的井下深度测量对油气井作业至关重要，但CCL信号识别的预处理方法发展不足，且真实井数据有限，给需要大量数据集的神经网络模型训练带来挑战。

Method: 提出集成到井下工具的CCL信号采集系统，采用标准化、标签分布平滑(LDS)、随机裁剪、标签平滑正则化(LSR)、时间缩放和多重采样等数据增强方法，使用基于AlexNet的神经网络模型进行训练。

Result: 实验表明，标准化、LDS和随机裁剪是模型训练的基本要求，而LSR、时间缩放和多重采样显著提升模型泛化能力。两个基准模型的F1分数从0.937和0.952最大提升至1.0和1.0。

Conclusion: 该工作解决了在CCL数据有限环境下训练套管接箍识别模型的数据增强方法空白，实际CCL波形验证了方法的有效性和实用性。

Abstract: Accurate downhole depth measurement is essential for oil and gas well
operations, directly influencing reservoir contact, production efficiency, and
operational safety. Collar correlation using a casing collar locator (CCL) is
fundamental for precise depth calibration. While neural network-based CCL
signal recognition has achieved significant progress in collar identification,
preprocessing methods for such applications remain underdeveloped. Moreover,
the limited availability of real well data poses substantial challenges for
training neural network models that require extensive datasets. This paper
presents a system integrated into downhole tools for CCL signal acquisition to
facilitate dataset construction. We propose comprehensive preprocessing methods
for data augmentation and evaluate their effectiveness using our AlexNet-based
neural network models. Through systematic experimentation across various
configuration combinations, we analyze the contribution of each augmentation
method. Results demonstrate that standardization, label distribution smoothing
(LDS), and random cropping are fundamental requirements for model training,
while label smoothing regularization (LSR), time scaling, and multiple sampling
significantly enhance model generalization capability. The F1 scores of our two
benchmark models trained with the proposed augmentation methods maximumly
improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance
validation on real CCL waveforms confirms the effectiveness and practical
applicability of our approach. This work addresses the gaps in data
augmentation methodologies for training casing collar recognition models in CCL
data-limited environments.

</details>


### [80] [Estimation of Toeplitz Covariance Matrices using Overparameterized Gradient Descent](https://arxiv.org/abs/2511.01605)
*Daniel Busbib,Ami Wiesel*

Main category: cs.LG

TL;DR: 该论文研究了在Toeplitz结构下的协方差估计问题，通过过参数化梯度下降方法，证明了当参数数量为2P或4P时能够实现全局收敛，并提出了一种加速梯度下降变体。


<details>
  <summary>Details</summary>
Motivation: 受到深度学习中使用过参数化模型和简单梯度下降取得成功的启发，重新审视Toeplitz协方差估计问题，探索简单梯度下降方法在此问题上的潜力。

Method: 将P×P协方差建模为K个复正弦波的求和，使用梯度下降优化参数。研究了不同参数化程度（K=P、2P、4P）的效果，并提出了具有分离学习率的加速梯度下降变体。

Result: 当K=P时梯度下降可能收敛到次优解，但当K=2P或4P时能够从随机初始化实现全局收敛。在固定频率仅优化振幅的情况下，证明了优化景观是渐近良性的。数值实验显示该方法在挑战性设置下能够达到或超过最先进方法的精度。

Conclusion: 过参数化梯度下降为Toeplitz协方差估计提供了一种简单、可扩展且有效的解决方案，在某些情况下能够超越复杂的最先进方法。

Abstract: We consider covariance estimation under Toeplitz structure. Numerous
sophisticated optimization methods have been developed to maximize the Gaussian
log-likelihood under Toeplitz constraints. In contrast, recent advances in deep
learning demonstrate the surprising power of simple gradient descent (GD)
applied to overparameterized models. Motivated by this trend, we revisit
Toeplitz covariance estimation through the lens of overparameterized GD. We
model the $P\times P$ covariance as a sum of $K$ complex sinusoids with
learnable parameters and optimize them via GD. We show that when $K = P$, GD
may converge to suboptimal solutions. However, mild overparameterization ($K =
2P$ or $4P$) consistently enables global convergence from random
initializations. We further propose an accelerated GD variant with separate
learning rates for amplitudes and frequencies. When frequencies are fixed and
only amplitudes are optimized, we prove that the optimization landscape is
asymptotically benign and any stationary point recovers the true covariance.
Finally, numerical experiments demonstrate that overparameterized GD can match
or exceed the accuracy of state-of-the-art methods in challenging settings,
while remaining simple and scalable.

</details>


### [81] [Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph](https://arxiv.org/abs/2511.00086)
*Fali Wang,Jihai Chen,Shuhua Yang,Runxue Bao,Tianxiang Zhao,Zhiwei Zhang,Xianfeng Tang,Hui Liu,Qi He,Suhang Wang*

Main category: cs.LG

TL;DR: 本文提出Agent-REINFORCE框架，用于在固定计算预算下搜索测试时扩展中的最优多LLM协作图和模型组合，解决了传统方法忽视任务特定需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法通常假设固定的协作架构和单一模型使用，忽略了最优架构和模型组合会因任务而异的问题。

Method: 将问题形式化为概率图优化，提出Agent-REINFORCE框架，通过采样-反馈-更新流程，使用文本梯度更新概率图来搜索最优多LLM协作图。

Result: 实验表明Agent-REINFORCE在样本效率和搜索性能上优于传统和基于LLM的基线方法，能有效识别在准确性和推理延迟联合目标下的最优图。

Conclusion: Agent-REINFORCE框架成功解决了测试时扩展中多LLM协作图的优化问题，为不同任务提供了定制化的最优架构和模型组合。

Abstract: Test-Time Scaling (TTS) improves large language models (LLMs) by allocating
additional computation during inference, typically through parallel,
sequential, or hybrid scaling. However, prior studies often assume fixed
collaboration architectures (e.g., topologies) and single-model usage,
overlooking that optimal architectures and model combinations can vary across
tasks. Therefore, we study the novel problem of searching for compute-optimal
model combinations and architectures in TTS under a fixed budget. We formalize
it as a multi-LLM collaboration graph, where nodes encode roles and LLM model
assignments, and edges capture information flow. This problem is challenging
because (i) the combinatorial search space is prohibitively large, and (ii)
task-specific requirements demand tailored designs. To address these, we
reformulate the problem as probabilistic graph optimization and, through pilot
experiments, derive three empirical insights into TTS collaboration graphs.
Guided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented
framework that mirrors the REINFORCE pipeline by mapping
sampling-gradient-update to sampling-feedback-update, where feedback serves as
a textual gradient to update the probabilistic graph and efficiently search for
optimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE
outperforms both traditional and LLM-based baselines in sample efficiency and
search performance, and effectively identifies optimal graphs under joint
objectives of accuracy and inference latency.

</details>


### [82] [Adapt under Attack and Domain Shift: Unified Adversarial Meta-Learning and Domain Adaptation for Robust Automatic Modulation Classification](https://arxiv.org/abs/2511.01172)
*Ali Owfi,Amirmohammad Bamdad,Tolunay Seyfi,Fatemeh Afghah*

Main category: cs.LG

TL;DR: 提出了一种结合元学习和领域自适应的统一框架，使自动调制分类系统能够同时抵抗对抗攻击和环境变化。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自动调制分类中表现出色，但易受对抗攻击和数据分布变化的影响，阻碍了在实际动态环境中的部署。

Method: 采用两阶段策略：离线阶段使用元学习方法在单一源域上训练模型，使其能够泛化防御；在线阶段应用领域自适应将模型特征与新目标域对齐。

Result: 该框架显著提高了调制分类精度，能够有效应对组合威胁。

Conclusion: 该框架为解决现代AMC系统的部署和操作挑战提供了关键解决方案。

Abstract: Deep learning has emerged as a leading approach for Automatic Modulation
Classification (AMC), demonstrating superior performance over traditional
methods. However, vulnerability to adversarial attacks and susceptibility to
data distribution shifts hinder their practical deployment in real-world,
dynamic environments. To address these threats, we propose a novel, unified
framework that integrates meta-learning with domain adaptation, making AMC
systems resistant to both adversarial attacks and environmental changes. Our
framework utilizes a two-phase strategy. First, in an offline phase, we employ
a meta-learning approach to train the model on clean and adversarially
perturbed samples from a single source domain. This method enables the model to
generalize its defense, making it resistant to a combination of previously
unseen attacks. Subsequently, in the online phase, we apply domain adaptation
to align the model's features with a new target domain, allowing it to adapt
without requiring substantial labeled data. As a result, our framework achieves
a significant improvement in modulation classification accuracy against these
combined threats, offering a critical solution to the deployment and
operational challenges of modern AMC systems.

</details>


### [83] [Cross-Treatment Effect Estimation for Multi-Category, Multi-Valued Causal Inference via Dynamic Neural Masking](https://arxiv.org/abs/2511.01641)
*Xiaopeng Ke,Yihan Yu,Ruyue Zhang,Zhishuo Zhou,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: XTNet是一种用于多类别、多值处理效应估计的新型网络架构，通过交叉效应估计模块和动态掩码机制捕捉处理交互，无需限制性结构假设。


<details>
  <summary>Details</summary>
Motivation: 反事实因果推断在面对多类别、多值处理时面临重大挑战，现有方法仅限于二元或单类型处理，存在假设限制、可扩展性不足和评估框架不完善等问题。

Method: XTNet采用分解策略，将基本效应与交叉处理交互分离，引入具有动态掩码机制的交叉效应估计模块，并提出了考虑处理成本和交互效应的MCMV-AUCC评估指标。

Result: 在合成和真实数据集上的广泛实验表明，XTNet在排序准确性和效应估计质量方面始终优于最先进的基线方法，真实A/B测试结果进一步证实了其有效性。

Conclusion: XTNet为复杂干预场景下的多类别、多值处理效应估计提供了有效的解决方案，显著提升了建模能力和评估准确性。

Abstract: Counterfactual causal inference faces significant challenges when extended to
multi-category, multi-valued treatments, where complex cross-effects between
heterogeneous interventions are difficult to model. Existing methodologies
remain constrained to binary or single-type treatments and suffer from
restrictive assumptions, limited scalability, and inadequate evaluation
frameworks for complex intervention scenarios.
  We present XTNet, a novel network architecture for multi-category,
multi-valued treatment effect estimation. Our approach introduces a
cross-effect estimation module with dynamic masking mechanisms to capture
treatment interactions without restrictive structural assumptions. The
architecture employs a decomposition strategy separating basic effects from
cross-treatment interactions, enabling efficient modeling of combinatorial
treatment spaces. We also propose MCMV-AUCC, a suitable evaluation metric that
accounts for treatment costs and interaction effects. Extensive experiments on
synthetic and real-world datasets demonstrate that XTNet consistently
outperforms state-of-the-art baselines in both ranking accuracy and effect
estimation quality. The results of the real-world A/B test further confirm its
effectiveness.

</details>


### [84] [GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation](https://arxiv.org/abs/2511.00097)
*Zihao Guo,Qingyun Sun,Ziwei Zhang,Haonan Yuan,Huiping Zhuang,Xingcheng Fu,Jianxin Li*

Main category: cs.LG

TL;DR: 提出了GraphKeeper方法来解决图域增量学习中的灾难性遗忘问题，通过知识解缠和保持来应对嵌入漂移和决策边界偏差。


<details>
  <summary>Details</summary>
Motivation: 现有的图增量学习方法主要关注单一域内的任务增量和类别增量场景，而图域增量学习在多个图域间更新模型的需求随着图基础模型的发展变得至关重要，但尚未被充分研究。

Method: GraphKeeper方法包括：1）域特定参数高效微调结合域内和域间解缠目标防止嵌入漂移；2）无偏差知识保持来维持稳定决策边界；3）对于不可观测域的图，使用域感知分布判别获取精确嵌入。

Result: 大量实验表明GraphKeeper取得了最先进的结果，相比第二名有6.5%~16.6%的提升，且遗忘可忽略不计。该方法可与各种代表性图基础模型无缝集成。

Conclusion: GraphKeeper有效解决了图域增量学习中的关键挑战，展示了广泛的适用潜力，为图基础模型在多域场景下的持续学习提供了可行方案。

Abstract: Graph incremental learning (GIL), which continuously updates graph models by
sequential knowledge acquisition, has garnered significant interest recently.
However, existing GIL approaches focus on task-incremental and
class-incremental scenarios within a single domain. Graph domain-incremental
learning (Domain-IL), aiming at updating models across multiple graph domains,
has become critical with the development of graph foundation models (GFMs), but
remains unexplored in the literature. In this paper, we propose Graph
Domain-Incremental Learning via Knowledge Dientanglement and Preservation
(GraphKeeper), to address catastrophic forgetting in Domain-IL scenario from
the perspectives of embedding shifts and decision boundary deviations.
Specifically, to prevent embedding shifts and confusion across incremental
graph domains, we first propose the domain-specific parameter-efficient
fine-tuning together with intra- and inter-domain disentanglement objectives.
Consequently, to maintain a stable decision boundary, we introduce
deviation-free knowledge preservation to continuously fit incremental domains.
Additionally, for graphs with unobservable domains, we perform domain-aware
distribution discrimination to obtain precise embeddings. Extensive experiments
demonstrate the proposed GraphKeeper achieves state-of-the-art results with
6.5%~16.6% improvement over the runner-up with negligible forgetting. Moreover,
we show GraphKeeper can be seamlessly integrated with various representative
GFMs, highlighting its broad applicative potential.

</details>


### [85] [Collaborative Large Language Model Inference via Resource-Aware Parallel Speculative Decoding](https://arxiv.org/abs/2511.01695)
*Jungyeon Koh,Hyun Jong Yang*

Main category: cs.LG

TL;DR: 提出统一框架联合优化用户关联和资源分配，支持并行推测解码，在移动边缘计算中实现高效大语言模型推理


<details>
  <summary>Details</summary>
Motivation: 设备端大语言模型推理需求增长，需要高效移动边缘计算方案。推测解码虽能分区生成令牌，但存在通信开销和异步延迟问题

Method: 使用多智能体深度强化学习算法解决用户关联和资源分配问题，通过Sionna模拟器在现实条件下评估

Result: 端到端延迟最多降低28.0%，平均降低23.7%，且不影响推理准确性

Conclusion: 该方法能在移动边缘计算系统中实现可扩展、低延迟的大语言模型服务

Abstract: The growing demand for on-device large language model (LLM) inference
highlights the need for efficient mobile edge computing (MEC) solutions,
especially in resource-constrained settings. Speculative decoding offers a
promising solution by partitioning token generation between a lightweight draft
model on mobile devices and a powerful target model on edge servers, but
suffers from communication overhead and asynchronous delays. This paper is the
first to propose a unified framework that jointly optimizes user association
and resource allocation (UARA) to support efficient parallel speculative
decoding. We solve the UARA problem using a multi-agent deep reinforcement
learning algorithm. To evaluate our approach under realistic conditions, we
conduct experiments using the Sionna simulator. Results show that our method
achieves up to 28.0% and an average of 23.7% reduction in end-to-end latency
without compromising inference accuracy, enabling scalable and low-latency LLM
services in MEC systems.

</details>


### [86] [Fractional Diffusion Bridge Models](https://arxiv.org/abs/2511.01795)
*Gabriel Nobis,Maximilian Springenberg,Arina Belova,Rembert Daems,Christoph Knochenhauer,Manfred Opper,Tolga Birdal,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出了分数扩散桥模型(FDBM)，这是一种基于分数布朗运动近似的新型生成扩散桥框架，能够捕捉现实随机过程中的记忆效应、长程依赖性和异常扩散现象。


<details>
  <summary>Details</summary>
Motivation: 现实随机过程具有记忆效应、时间相关性、长程依赖性和异常扩散等特性，这些在标准扩散或桥建模中由于使用布朗运动而无法捕捉。

Method: 利用分数布朗运动的马尔可夫近似(MA-fBM)构建FDBM，保持分数布朗运动的非马尔可夫特性同时实现可处理的推理。扩展到Schrödinger桥问题并推导出学习非配对数据转换的原则性损失函数。

Result: 在蛋白质构象预测和图像转换任务中，FDBM相比布朗运动基线表现更优：蛋白质结构预测中Cα原子位置的均方根偏差更低，非配对图像转换中Fréchet Inception距离更低。

Conclusion: FDBM框架能够有效建模现实世界数据的复杂动态特性，在多个任务中优于传统基于布朗运动的模型。

Abstract: We present Fractional Diffusion Bridge Models (FDBM), a novel generative
diffusion bridge framework driven by an approximation of the rich and
non-Markovian fractional Brownian motion (fBM). Real stochastic processes
exhibit a degree of memory effects (correlations in time), long-range
dependencies, roughness and anomalous diffusion phenomena that are not captured
in standard diffusion or bridge modeling due to the use of Brownian motion
(BM). As a remedy, leveraging a recent Markovian approximation of fBM (MA-fBM),
we construct FDBM that enable tractable inference while preserving the
non-Markovian nature of fBM. We prove the existence of a coupling-preserving
generative diffusion bridge and leverage it for future state prediction from
paired training data. We then extend our formulation to the Schr\"{o}dinger
bridge problem and derive a principled loss function to learn the unpaired data
translation. We evaluate FDBM on both tasks: predicting future protein
conformations from aligned data, and unpaired image translation. In both
settings, FDBM achieves superior performance compared to the Brownian
baselines, yielding lower root mean squared deviation (RMSD) of C$_\alpha$
atomic positions in protein structure prediction and lower Fr\'echet Inception
Distance (FID) in unpaired image translation.

</details>


### [87] [Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure](https://arxiv.org/abs/2511.01847)
*Zhi Wang,Chicheng Zhang,Ramya Korlakai Vinayak*

Main category: cs.LG

TL;DR: 提出了一个终身表示学习框架，使用多任务经验风险最小化作为子程序，引入任务规避维度来建立样本复杂度界限。


<details>
  <summary>Details</summary>
Motivation: 终身学习中，学习者面临一系列具有共享结构的任务，需要在线学习过程中利用现有知识并持续收集部分信息来识别和利用这种结构。

Method: 使用多任务经验风险最小化作为子程序的简单算法，基于新引入的任务规避维度建立理论框架。

Result: 建立了适用于广泛学习问题的样本复杂度界限，并在分类和回归任务中进行了实例化验证。

Conclusion: 该框架为终身表示学习提供了理论基础，能够处理涉及一般函数类的各种学习问题。

Abstract: In lifelong learning, a learner faces a sequence of tasks with shared
structure and aims to identify and leverage it to accelerate learning. We study
the setting where such structure is captured by a common representation of
data. Unlike multi-task learning or learning-to-learn, where tasks are
available upfront to learn the representation, lifelong learning requires the
learner to make use of its existing knowledge while continually gathering
partial information in an online fashion. In this paper, we consider a
generalized framework of lifelong representation learning. We propose a simple
algorithm that uses multi-task empirical risk minimization as a subroutine and
establish a sample complexity bound based on a new notion we introduce--the
task-eluder dimension. Our result applies to a wide range of learning problems
involving general function classes. As concrete examples, we instantiate our
result on classification and regression tasks under noise.

</details>


### [88] [Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving](https://arxiv.org/abs/2511.00101)
*Yuchen Zhang,Hanyue Du,Chun Cao,Jingwei Xu*

Main category: cs.LG

TL;DR: Loquetier是一个虚拟化的多LoRA框架，将LoRA微调和推理统一在单个运行时中，通过虚拟化模块和优化的计算流程显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA方法在微调和推理之间存在分离，缺乏统一的框架来同时支持这两种操作，导致效率低下。

Method: 提出虚拟化模块来隔离PEFT修改并支持多适配器共享基础模型，以及优化的计算流程通过内核设计合并微调和推理路径。

Result: 在三个任务设置上的实验显示，Loquetier在推理任务上达到最先进共服务系统3.0倍的吞吐量，在统一微调和推理任务上比PEFT高46.4倍的SLO达成率。

Conclusion: Loquetier成功统一了LoRA微调和推理，提供了高性能和灵活性的解决方案，代码已开源。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted parameter-efficient
fine-tuning (PEFT) technique for adapting large language models (LLMs) to
downstream tasks. While prior work has explored strategies for integrating LLM
training and serving, there still remains a gap in unifying fine-tuning and
inference for LoRA-based models. We present Loquetier, a virtualized multi-LoRA
framework that seamlessly integrates LoRA fine-tuning and serving within a
single runtime. Loquetier introduces two key components: (1) a Virtualized
Module that isolates PEFT-based modifications and supports multiple adapters on
a shared base model, and (2) an optimized computation flow with a kernel design
that merges fine-tuning and inference paths in forward propagation, enabling
efficient batching and minimizing kernel invocation overhead. Extensive
experiments across three task settings show that Loquetier consistently
outperforms existing baselines in both performance and flexibility, achieving
up to $3.0\times$ the throughput of the state-of-the-art co-serving system on
inference-only tasks and $46.4\times$ higher SLO attainment than PEFT on
unified fine-tuning and inference tasks. The implementation of Loquetier is
publicly available at https://github.com/NJUDeepEngine/Loquetier.

</details>


### [89] [Automated Discovery of Conservation Laws via Hybrid Neural ODE-Transformers](https://arxiv.org/abs/2511.00102)
*Vivan Doshi*

Main category: cs.LG

TL;DR: 提出了一种混合框架，用于从噪声轨迹数据中自动发现守恒量，结合了神经ODE、Transformer和符号-数值验证器。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中识别守恒定律是科学进步的关键，但目前从轨迹数据中发现这些不变量仍然具有挑战性。

Method: 集成三个组件：神经ODE学习系统动力学的连续模型，Transformer生成符号候选不变量，符号-数值验证器提供数值验证。

Result: 在典型物理系统上测试表明，该框架显著优于直接在轨迹数据上操作的基线方法。

Conclusion: 证明了分离的'先学习后搜索'方法在从不完美数据中发现数学原理方面的鲁棒性。

Abstract: The discovery of conservation laws is a cornerstone of scientific progress.
However, identifying these invariants from observational data remains a
significant challenge. We propose a hybrid framework to automate the discovery
of conserved quantities from noisy trajectory data. Our approach integrates
three components: (1) a Neural Ordinary Differential Equation (Neural ODE) that
learns a continuous model of the system's dynamics, (2) a Transformer that
generates symbolic candidate invariants conditioned on the learned vector
field, and (3) a symbolic-numeric verifier that provides a strong numerical
certificate for the validity of these candidates. We test our framework on
canonical physical systems and show that it significantly outperforms baselines
that operate directly on trajectory data. This work demonstrates the robustness
of a decoupled learn-then-search approach for discovering mathematical
principles from imperfect data.

</details>


### [90] [Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence](https://arxiv.org/abs/2511.00108)
*Yi Zhang,Che Liu,Xiancong Ren,Hanchu Ni,Shuai Zhang,Zeyuan Ding,Jiayu Hu,Hanzhe Shan,Zhenwei Niu,Zhaoyang Liu,Yue Zhao,Junbo Qi,Qinfan Zhang,Dengjie Li,Yidong Wang,Jiachen Luo,Yong Dai,Jian Tang,Xiaozhu Ju*

Main category: cs.LG

TL;DR: Pelican-VL 1.0是当前最大规模的开源具身多模态大脑模型，参数规模从70亿到720亿，在1000+ A800 GPU集群上训练，性能比基础模型提升20.3%，超越100B级开源模型10.6%。


<details>
  <summary>Details</summary>
Motivation: 明确使命是将强大智能嵌入各种具身系统中，推动具身智能的发展。

Method: 采用DPPO（刻意练习策略优化）框架，通过metaloop实现RL-Refine-Diagnose-SFT循环训练机制，从40亿+ token的原始数据中蒸馏高质量数据集。

Result: 在知名具身基准测试中达到与领先专有系统相当的水平，消耗超过50k+ A800 GPU小时每检查点。

Conclusion: Pelican-VL 1.0通过深度整合数据能力和智能自适应学习机制，成功构建了高性能的具身多模态大脑模型。

Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied
brain models with parameter scales ranging from 7 billion to 72 billion. Our
explicit mission is clearly stated as: To embed powerful intelligence into
various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source
embodied multimodal brain model. Its core advantage lies in the in-depth
integration of data power and intelligent adaptive learning mechanisms.
Specifically, metaloop distilled a high-quality dataset from a raw dataset
containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale
cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint.
This translates to a 20.3% performance uplift from its base model and
outperforms 100B-level open-source counterparts by 10.6%, placing it on par
with leading proprietary systems on well-known embodied benchmarks. We
establish a novel framework, DPPO (Deliberate Practice Policy Optimization),
inspired by human metacognition to train Pelican-VL 1.0. We operationalize this
as a metaloop that teaches the AI to practice deliberately, which is a
RL-Refine-Diagnose-SFT loop.

</details>


### [91] [Analysis of Line Break prediction models for detecting defensive breakthrough in football](https://arxiv.org/abs/2511.00121)
*Shoma Yagi,Jun Ichikawa,Genki Ichinose*

Main category: cs.LG

TL;DR: 开发了一个基于机器学习的模型，使用事件和追踪数据来预测足球中的防线突破（Line Break），模型准确率高，揭示了进攻球员速度、防线空隙等关键因素。


<details>
  <summary>Details</summary>
Motivation: 足球中突破对手防线是创造得分机会的关键战术行为，但以往研究主要关注射门或进球机会，缺乏对如何突破防线的系统性分析。

Method: 使用2023年J1联赛赛季的事件和追踪数据，构建包含189个特征（球员位置、速度、空间配置等）的XGBoost分类器模型来预测防线突破概率。

Result: 模型预测准确率很高（AUC 0.982，Brier得分0.015），SHAP分析显示进攻球员速度、防线空隙和进攻球员空间分布是重要影响因素。

Conclusion: 防线突破与创造得分机会密切相关，该研究为理解足球战术动态提供了量化框架。

Abstract: In football, attacking teams attempt to break through the opponent's
defensive line to create scoring opportunities. This action, known as a Line
Break, is a critical indicator of offensive effectiveness and tactical
performance, yet previous studies have mainly focused on shots or goal
opportunities rather than on how teams break the defensive line. In this study,
we develop a machine learning model to predict Line Breaks using event and
tracking data from the 2023 J1 League season. The model incorporates 189
features, including player positions, velocities, and spatial configurations,
and employs an XGBoost classifier to estimate the probability of Line Breaks.
The proposed model achieved high predictive accuracy, with an AUC of 0.982 and
a Brier score of 0.015. Furthermore, SHAP analysis revealed that factors such
as offensive player speed, gaps in the defensive line, and offensive players'
spatial distributions significantly contribute to the occurrence of Line
Breaks. Finally, we found a moderate positive correlation between the predicted
probability of being Line-Broken and the number of shots and crosses conceded
at the team level. These results suggest that Line Breaks are closely linked to
the creation of scoring opportunities and provide a quantitative framework for
understanding tactical dynamics in football.

</details>


### [92] [Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models](https://arxiv.org/abs/2511.00124)
*Sai Niranjan Ramachandran,Manish Krishan Lal,Suvrit Sra*

Main category: cs.LG

TL;DR: 本文使用统计物理学中的交叉涨落分析扩散模型的采样动态，发现样本会经历尖锐的离散转变，形成目标分布，这些转变可通过交叉涨落检测，并能提升采样效率和应用性能。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型中采样动态的演变过程，理解样本从初始分布到目标分布的转变机制，并探索如何利用这些转变提升模型性能。

Method: 使用交叉涨落统计量分析采样动态，推导方差保持SDE的交叉涨落闭式解，检测样本转变过程中的不连续性。

Result: 发现样本经历离散转变，检测这些转变能直接提升采样效率，加速条件生成，改进图像分类和风格迁移等零样本任务，无需网格搜索或重新训练。

Conclusion: 该框架统一了离散马尔可夫链理论与连续动力学，将经典耦合和混合概念扩展到随机SDE和非马尔可夫采样器，连接了相分析与现代生成建模。

Abstract: We analyse how the sampling dynamics of distributions evolve in score-based
diffusion models using cross-fluctuations, a centered-moment statistic from
statistical physics. Specifically, we show that starting from an unbiased
isotropic normal distribution, samples undergo sharp, discrete transitions,
eventually forming distinct events of a desired distribution while
progressively revealing finer structure. As this process is reversible, these
transitions also occur in reverse, where intermediate states progressively
merge, tracing a path back to the initial distribution. We demonstrate that
these transitions can be detected as discontinuities in $n^{\text{th}}$-order
cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for
these cross-fluctuations that is efficiently computable for the reverse
trajectory. We find that detecting these transitions directly boosts sampling
efficiency, accelerates class-conditional and rare-class generation, and
improves two zero-shot tasks--image classification and style transfer--without
expensive grid search or retraining. We also show that this viewpoint unifies
classical coupling and mixing from finite Markov chains with continuous
dynamics while extending to stochastic SDEs and non Markovian samplers. Our
framework therefore bridges discrete Markov chain theory, phase analysis, and
modern generative modeling.

</details>


### [93] [Dynamic Model Selection for Trajectory Prediction via Pairwise Ranking and Meta-Features](https://arxiv.org/abs/2511.00126)
*Lu Bowen*

Main category: cs.LG

TL;DR: 提出动态多专家门控框架，通过内部模型信号自适应选择最佳轨迹预测器，在nuPlan-mini数据集上实现FDE 9.5%的降低，达到oracle性能的57.8%。


<details>
  <summary>Details</summary>
Motivation: 现有深度轨迹预测器在复杂长尾驾驶场景中不可靠，而简单物理模型偶尔能胜过先进网络，需要超越"一刀切"范式。

Method: 使用动态多专家门控框架，基于内部模型信号（稳定性和不确定性）自适应选择物理LSTM、Transformer和微调GameFormer中的最佳预测器，将专家选择建模为成对排序问题。

Result: 在nuPlan-mini数据集上FDE为2.567m，比GameFormer降低9.5%；在左转场景中FDE降低约10%，在离线验证和开环评估中均表现一致改进。

Conclusion: 自适应混合系统能增强安全关键自动驾驶中的轨迹可靠性，为超越静态单模型范式提供了实用途径。

Abstract: Recent deep trajectory predictors (e.g., Jiang et al., 2023; Zhou et al.,
2022) have achieved strong average accuracy but remain unreliable in complex
long-tail driving scenarios. These limitations reveal the weakness of the
prevailing "one-model-fits-all" paradigm, particularly in safety-critical urban
contexts where simpler physics-based models can occasionally outperform
advanced networks (Kalman, 1960). To bridge this gap, we propose a dynamic
multi-expert gating framework that adaptively selects the most reliable
trajectory predictor among a physics-informed LSTM, a Transformer, and a
fine-tuned GameFormer on a per-sample basis.
  Our method leverages internal model signals (meta-features) such as stability
and uncertainty (Gal and Ghahramani, 2016), which we demonstrate to be
substantially more informative than geometric scene descriptors. To the best of
our knowledge, this is the first work to formulate trajectory expert selection
as a pairwise-ranking problem over internal model signals (Burges et al.,
2005), directly optimizing decision quality without requiring post-hoc
calibration.
  Evaluated on the nuPlan-mini dataset (Caesar et al., 2021) with 1,287
samples, our LLM-enhanced tri-expert gate achieves a Final Displacement Error
(FDE) of 2.567 m, representing a 9.5 percent reduction over GameFormer (2.835
m), and realizes 57.8 percent of the oracle performance bound. In open-loop
simulations, after trajectory horizon alignment, the same configuration reduces
FDE on left-turn scenarios by approximately 10 percent, demonstrating
consistent improvements across both offline validation and open-loop
evaluation. These results indicate that adaptive hybrid systems enhance
trajectory reliability in safety-critical autonomous driving, providing a
practical pathway beyond static single-model paradigms.

</details>


### [94] [A Comparative Analysis of LLM Adaptation: SFT, LoRA, and ICL in Data-Scarce Scenarios](https://arxiv.org/abs/2511.00130)
*Bernd Bohnet,Rumen Dangovski,Kevin Swersky,Sherry Moore,Arslan Chaudhry,Kathleen Kenealy,Noah Fiedel*

Main category: cs.LG

TL;DR: 比较分析SFT、LoRA和ICL三种大语言模型适应方法在数据稀缺场景下的表现，发现LoRA在技能获取和通用知识保留之间达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型需要针对特定应用进行定制，但完全微调计算成本高且会导致灾难性遗忘。需要找到既能有效获取新技能又能保留通用推理能力的适应策略。

Method: 在数据稀缺场景下对监督微调(SFT)、低秩适应(LoRA)和上下文学习(ICL)三种方法进行对比分析，评估它们在技能获取和知识保留方面的表现。

Result: LoRA在技能获取和通用知识保留之间达到最佳平衡；SFT擅长技能获取但极易发生灾难性遗忘；ICL适合整合事实知识但难以处理复杂技能。

Conclusion: LoRA是最有效的平衡策略，为选择LLM适应方法提供了实用框架，强调了技能获取与知识整合的关键区别，以及任务特定性能与通用能力保留之间的权衡。

Abstract: The remarkable capabilities of Large Language Models (LLMs) often need to be
tailored for specific applications, requiring the integration of new knowledge
or the acquisition of new skills. While full fine-tuning is a powerful
adaptation method, it is computationally expensive and can lead to a
degradation of general reasoning abilities, a phenomenon known as catastrophic
forgetting. A range of alternative techniques exists, each with its own
trade-offs. In-Context Learning (ICL) is fast but limited by context length,
while Parameter-Efficient Fine-Tuning (PEFT) methods like Low-Rank Adaptation
(LoRA) offer a middle ground by minimizing parameter changes. However, the
challenge of catastrophic forgetting persists, raising questions about the best
adaptation strategy for a given task. This paper presents a comparative
analysis of Supervised Finetuning (SFT), LoRA, and ICL in data-scarce
scenarios. We find that LoRA provides the most effective balance, successfully
instilling new skills with minimal impact on the base model's general
knowledge. In contrast, while SFT excels at skill acquisition, it is highly
susceptible to catastrophic forgetting. ICL is effective for incorporating
factual knowledge but struggles with complex skills. Our findings offer a
practical framework for selecting an LLM adaptation strategy. We highlight the
critical distinction between skill acquisition and knowledge integration,
clarify the trade-offs between task-specific performance and the preservation
of general capabilities.

</details>


### [95] [Feature Importance Guided Random Forest Learning with Simulated Annealing Based Hyperparameter Tuning](https://arxiv.org/abs/2511.00133)
*Kowshik Balasubramanian,Andre Williams,Ismail Butun*

Main category: cs.LG

TL;DR: 提出了一种增强随机森林分类器的新框架，通过概率特征采样和模拟退火超参数调优，显著提升预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统随机森林在捕捉数据相关信号方面的局限性，特别是在信用风险评估、物联网异常检测、早期医学诊断和高维生物数据分析等领域的鲁棒分类挑战。

Method: 结合概率特征采样（强调对分类更有意义的特征）和模拟退火算法进行动态超参数调优。

Result: 在多个应用领域实现了准确性的持续提升，并获得了特征相关性的有意义洞察。

Conclusion: 证明了重要性感知采样与元启发式优化相结合的有效性，为随机森林分类器提供了显著改进。

Abstract: This paper introduces a novel framework for enhancing Random Forest
classifiers by integrating probabilistic feature sampling and hyperparameter
tuning via Simulated Annealing. The proposed framework exhibits substantial
advancements in predictive accuracy and generalization, adeptly tackling the
multifaceted challenges of robust classification across diverse domains,
including credit risk evaluation, anomaly detection in IoT ecosystems,
early-stage medical diagnostics, and high-dimensional biological data analysis.
To overcome the limitations of conventional Random Forests, we present an
approach that places stronger emphasis on capturing the most relevant signals
from data while enabling adaptive hyperparameter configuration. The model is
guided towards features that contribute more meaningfully to classification and
optimizing this with dynamic parameter tuning. The results demonstrate
consistent accuracy improvements and meaningful insights into feature
relevance, showcasing the efficacy of combining importance aware sampling and
metaheuristic optimization.

</details>


### [96] [Physiologically Active Vegetation Reverses Its Cooling Effect in Humid Urban Climates](https://arxiv.org/abs/2511.00134)
*Angana Borah,Adrija Datta,Ashish S. Kumar,Raviraj Dave,Udit Bhatia*

Main category: cs.LG

TL;DR: 该研究分析了印度138个城市中植被对热指数的影响，发现植被在特定条件下会从降温转为增温效应，特别是在高湿度环境中。


<details>
  <summary>Details</summary>
Motivation: 城市绿化降温效果存在不平衡性，因为植被在降温的同时也可能增加空气湿度，但目前对这种权衡关系的理解仍不充分，缺乏对缓解政策的指导。

Method: 使用极端感知的1公里热指数重建数据和可解释机器学习框架（SHAP和ALE），分析植被结构与功能对热指数的影响。

Result: 当EVI≥0.4、LAI≥0.05时植被降温效果增强，但当EVI≥0.5、LAI≥0.2、fPAR≥0.5时开始转为增温效应，在潮湿密集城市核心区fPAR≥0.25时即出现逆转。

Conclusion: 研究确定了植被驱动降温的气候限制，为制定气候特定的绿化策略提供了量化阈值，有助于促进公平和热弹性城市发展。

Abstract: Efforts to green cities for cooling are succeeding unevenly because the same
vegetation that cools surfaces can also intensify how hot the air feels.
Previous studies have identified humid heat as a growing urban hazard, yet how
physiologically active vegetation governs this trade-off between cooling and
moisture accumulation remains poorly understood, leaving mitigation policy and
design largely unguided. Here we quantify how vegetation structure and function
influence the Heat Index (HI), a combined measure of temperature and humidity
in 138 Indian cities spanning tropical savanna, semi-arid steppe, and humid
subtropical climates, and across dense urban cores and semi-urban rings. Using
an extreme-aware, one kilometre reconstruction of HI and an interpretable
machine-learning framework that integrates SHapley Additive Explanations (SHAP)
and Accumulated Local Effects (ALE), we isolate vegetation-climate
interactions. Cooling generally strengthens for EVI >= 0.4 and LAI >= 0.05, but
joint-high regimes begin to reverse toward warming when EVI >= 0.5, LAI >= 0.2,
and fPAR >= 0.5,with an earlier onset for fPAR >= 0.25 in humid, dense cores.
In such environments, highly physiologically active vegetation elevates
near-surface humidity faster than it removes heat, reversing its cooling effect
and amplifying perceived heat stress. These findings establish the climatic
limits of vegetation-driven cooling and provide quantitative thresholds for
climate-specific greening strategies that promote equitable and heat-resilient
cities.

</details>


### [97] [A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://arxiv.org/abs/2511.00136)
*Qing Guo,Xinhang Li,Junyu Chen,Zheng Guo,Xiaocong Li,Lin Zhang,Lei Li*

Main category: cs.LG

TL;DR: HeraldLight是一个基于双LLM架构的交通信号控制系统，通过Herald引导提示增强，解决了传统LLM方法固定信号时长和幻觉问题，以及RL方法缺乏鲁棒性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的交通信号控制方法存在固定信号时长和幻觉错误的问题，而传统强化学习方法在信号时序决策上缺乏鲁棒性且泛化能力差。

Method: 提出HeraldLight双LLM架构：Herald模块提取上下文信息并预测各交通相位的排队长度；LLM-Agent基于预测进行细粒度交通信号控制；LLM-Critic精炼LLM-Agent的输出，纠正错误和幻觉；通过基于分数的微调提高准确性和鲁棒性。

Result: 在CityFlow模拟实验中，使用覆盖济南(12)、杭州(16)和纽约(196)共224个交叉口的真实数据集，HeraldLight优于最先进基线方法，在所有场景中平均旅行时间减少20.03%，在济南和杭州场景中平均排队长度减少10.74%。

Conclusion: HeraldLight通过双LLM架构和Herald引导提示，有效解决了现有方法的局限性，在交通信号控制中实现了更优的性能和鲁棒性。

Abstract: Leveraging large language models (LLMs) in traffic signal control (TSC)
improves optimization efficiency and interpretability compared to traditional
reinforcement learning (RL) methods. However, existing LLM-based approaches are
limited by fixed time signal durations and are prone to hallucination errors,
while RL methods lack robustness in signal timing decisions and suffer from
poor generalization. To address these challenges, this paper proposes
HeraldLight, a dual LLMs architecture enhanced by Herald guided prompts. The
Herald Module extracts contextual information and forecasts queue lengths for
each traffic phase based on real-time conditions. The first LLM, LLM-Agent,
uses these forecasts to make fine grained traffic signal control, while the
second LLM, LLM-Critic, refines LLM-Agent's outputs, correcting errors and
hallucinations. These refined outputs are used for score-based fine-tuning to
improve accuracy and robustness. Simulation experiments using CityFlow on real
world datasets covering 224 intersections in Jinan (12), Hangzhou (16), and New
York (196) demonstrate that HeraldLight outperforms state of the art baselines,
achieving a 20.03% reduction in average travel time across all scenarios and a
10.74% reduction in average queue length on the Jinan and Hangzhou scenarios.
The source code is available on GitHub:
https://github.com/BUPT-ANTlab/HeraldLight.

</details>


### [98] [Study on Supply Chain Finance Decision-Making Model and Enterprise Economic Performance Prediction Based on Deep Reinforcement Learning](https://arxiv.org/abs/2511.00166)
*Shiman Zhang,Jinghan Zhou,Zhoufan Yu,Ningai Leng*

Main category: cs.LG

TL;DR: 提出了一种融合深度学习与智能粒子群优化的决策模型，用于提高后端集中式冗余供应链的决策和规划效率。


<details>
  <summary>Details</summary>
Motivation: 提高后端集中式冗余供应链的决策和规划效率，解决动态环境下的实时决策调整和路径优化问题。

Method: 构建分布式节点部署模型和最优规划路径，使用卷积神经网络从历史数据中提取特征，线性规划捕获高阶统计特征，采用模糊关联规则调度和深度强化学习优化模型，神经网络拟合动态变化，建立"深度学习特征提取-智能粒子群优化"混合机制。

Result: 仿真结果显示资源消耗减少，空间规划增强，在动态环境中改善了实时决策调整、分布路径优化和鲁棒智能控制。

Conclusion: 该集成模型有效提升了供应链决策效率，在动态环境下表现出良好的适应性和优化性能。

Abstract: To improve decision-making and planning efficiency in back-end centralized
redundant supply chains, this paper proposes a decision model integrating deep
learning with intelligent particle swarm optimization. A distributed node
deployment model and optimal planning path are constructed for the supply chain
network. Deep learning such as convolutional neural networks extracts features
from historical data, and linear programming captures high-order statistical
features. The model is optimized using fuzzy association rule scheduling and
deep reinforcement learning, while neural networks fit dynamic changes. A
hybrid mechanism of "deep learning feature extraction - intelligent particle
swarm optimization" guides global optimization and selects optimal decisions
for adaptive control. Simulations show reduced resource consumption, enhanced
spatial planning, and in dynamic environments improved real-time decision
adjustment, distribution path optimization, and robust intelligent control.

</details>


### [99] [Can SAEs reveal and mitigate racial biases of LLMs in healthcare?](https://arxiv.org/abs/2511.00177)
*Hiba Ahsan,Byron C. Wallace*

Main category: cs.LG

TL;DR: 使用稀疏自编码器(SAEs)分析Gemma-2模型中与种族相关的潜在表征，发现这些表征能识别模型对种族信息的依赖，并可通过潜在空间操控影响模型输出，但在复杂临床任务中缓解偏见的效果有限。


<details>
  <summary>Details</summary>
Motivation: LLMs在医疗领域的应用存在加剧现有偏见的风险，需要开发方法来检测和控制模型对患者种族等敏感特征的依赖。

Method: 在Gemma-2模型中识别与黑人个体相关的SAE潜在表征，分析其激活模式，并通过潜在空间操控来影响模型输出。

Result: 发现特定潜在表征在合理输入(如"非裔美国人")和问题词汇(如"监禁")上均会激活；激活该表征会增加模型对黑人患者产生负面关联(如"好斗")的风险；在简单场景中缓解偏见有效，但在复杂临床任务中效果有限。

Conclusion: SAEs可作为识别LLMs在临床应用中依赖人口统计信息的工具，但通过SAE操控缓解偏见在现实任务中效果有限。

Abstract: LLMs are increasingly being used in healthcare. This promises to free
physicians from drudgery, enabling better care to be delivered at scale. But
the use of LLMs in this space also brings risks; for example, such models may
worsen existing biases. How can we spot when LLMs are (spuriously) relying on
patient race to inform predictions? In this work we assess the degree to which
Sparse Autoencoders (SAEs) can reveal (and control) associations the model has
made between race and stigmatizing concepts. We first identify SAE latents in
Gemma-2 models which appear to correlate with Black individuals. We find that
this latent activates on reasonable input sequences (e.g., "African American")
but also problematic words like "incarceration". We then show that we can use
this latent to steer models to generate outputs about Black patients, and
further that this can induce problematic associations in model outputs as a
result. For example, activating the Black latent increases the risk assigned to
the probability that a patient will become "belligerent". We evaluate the
degree to which such steering via latents might be useful for mitigating bias.
We find that this offers improvements in simple settings, but is less
successful for more realistic and complex clinical tasks. Overall, our results
suggest that: SAEs may offer a useful tool in clinical applications of LLMs to
identify problematic reliance on demographics but mitigating bias via SAE
steering appears to be of marginal utility for realistic tasks.

</details>


### [100] [PDE-SHARP: PDE Solver Hybrids Through Analysis & Refinement Passes](https://arxiv.org/abs/2511.00183)
*Shaghayegh Fazliani,Madeleine Udell*

Main category: cs.LG

TL;DR: PDE-SHARP是一个通过用更便宜的LLM推理替代昂贵的科学计算来降低计算成本的框架，能够在减少60-75%计算评估的情况下实现更高的求解器精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的方法在生成PDE求解器时需要执行大量求解器样本来识别高精度求解器，对于复杂PDE尤其计算成本高昂。

Method: 采用三阶段方法：(1)分析阶段：数学思维链分析，包括PDE分类、解类型检测和稳定性分析；(2)生成阶段：基于前一阶段数学洞察生成求解器；(3)合成阶段：通过LLM评委迭代精化实现的协作选择-混合锦标赛。

Result: 平均只需不到13次求解器评估（基线方法需要30+次），在所有测试PDE上平均精度提高4倍，并在不同LLM架构上表现出稳健性能。

Conclusion: PDE-SHARP框架能够显著降低计算成本，同时提高求解器精度，展示了在科学计算中有效利用LLM推理的潜力。

Abstract: Current LLM-driven approaches using test-time computing to generate PDE
solvers execute a large number of solver samples to identify high-accuracy
solvers. These paradigms are especially costly for complex PDEs requiring
substantial computational resources for numerical evaluation. We introduce
PDE-SHARP, a framework to reduce computational costs by replacing expensive
scientific computation by cheaper LLM inference that achieves superior solver
accuracy with 60-75% fewer computational evaluations. PDE-SHARP employs three
stages: (1) Analysis: mathematical chain-of-thought analysis including PDE
classification, solution type detection, and stability analysis; (2) Genesis:
solver generation based on mathematical insights from the previous stage; and
(3) Synthesis: collaborative selection-hybridization tournaments in which LLM
judges iteratively refine implementations through flexible performance
feedback. To generate high-quality solvers, PDE-SHARP requires fewer than 13
solver evaluations on average compared to 30+ for baseline methods, improving
accuracy uniformly across tested PDEs by $4\times$ on average, and demonstrates
robust performance across LLM architectures, from general-purpose to
specialized reasoning models.

</details>


### [101] [EL-MIA: Quantifying Membership Inference Risks of Sensitive Entities in LLMs](https://arxiv.org/abs/2511.00192)
*Ali Satvaty,Suzan Verberne,Fatih Turkmen*

Main category: cs.LG

TL;DR: 提出了EL-MIA框架，用于审计LLM中实体级别的成员推理风险，发现现有MIA方法在敏感属性实体级成员推理方面存在局限性。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法只能检测整个提示或文档是否在训练数据中，无法捕捉更细粒度的风险，特别是针对敏感信息（如PII、信用卡号等）的实体级成员风险。

Method: 提出了EL-MIA框架，构建了基准数据集，系统比较了现有MIA技术和两种新方法，分析了实体级MIA易感性与模型规模、训练轮数等因素的关系。

Result: 发现现有MIA方法在敏感属性实体级成员推理方面效果有限，但这种易感性可以通过相对简单的方法来识别，表明需要更强的对抗方法来测试威胁模型。

Conclusion: 强调了需要开发更强大的对抗方法来充分测试实体级成员推理威胁模型，以更好地保护LLM中的敏感信息隐私。

Abstract: Membership inference attacks (MIA) aim to infer whether a particular data
point is part of the training dataset of a model. In this paper, we propose a
new task in the context of LLM privacy: entity-level discovery of membership
risk focused on sensitive information (PII, credit card numbers, etc). Existing
methods for MIA can detect the presence of entire prompts or documents in the
LLM training data, but they fail to capture risks at a finer granularity. We
propose the ``EL-MIA'' framework for auditing entity-level membership risks in
LLMs. We construct a benchmark dataset for the evaluation of MIA methods on
this task. Using this benchmark, we conduct a systematic comparison of existing
MIA techniques as well as two newly proposed methods. We provide a
comprehensive analysis of the results, trying to explain the relation of the
entity level MIA susceptability with the model scale, training epochs, and
other surface level factors. Our findings reveal that existing MIA methods are
limited when it comes to entity-level membership inference of the sensitive
attributes, while this susceptibility can be outlined with relatively
straightforward methods, highlighting the need for stronger adversaries to
stress test the provided threat model.

</details>


### [102] [Diffusion Models at the Drug Discovery Frontier: A Review on Generating Small Molecules versus Therapeutic Peptides](https://arxiv.org/abs/2511.00209)
*Yiquan Wang,Yahui Ma,Yuhan Chang,Jiayao Yan,Jialin Zhang,Minnuo Cai,Kai Wei*

Main category: cs.LG

TL;DR: 扩散模型在药物发现中显示出巨大潜力，本文系统比较了其在设计小分子和肽类药物中的应用，分析了统一去噪框架如何适应不同分子表示和设计目标，并讨论了各自的挑战和共享问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为领先的生成建模框架，有望加速和改变传统缓慢且昂贵的药物发现过程，需要系统分析其在主要治疗模式中的应用。

Method: 采用系统比较方法，分析扩散模型在小分子和肽类药物设计中的统一去噪框架如何适应不同的分子表示、化学空间和设计目标。

Result: 对于小分子，扩散模型擅长基于结构的设计，生成新颖的配体，但面临化学可合成性挑战；对于肽类药物，重点在于生成功能序列和设计新结构，主要挑战是生物稳定性、正确折叠和免疫原性。

Conclusion: 扩散模型的全部潜力将通过弥合模式特定差距并整合到自动化DBTL平台中释放，从而将范式从化学探索转向靶向创建新型疗法。

Abstract: Diffusion models have emerged as a leading framework in generative modeling,
showing significant potential to accelerate and transform the traditionally
slow and costly process of drug discovery. This review provides a systematic
comparison of their application in designing two principal therapeutic
modalities: small molecules and therapeutic peptides. We analyze how a unified
framework of iterative denoising is adapted to the distinct molecular
representations, chemical spaces, and design objectives of each modality. For
small molecules, these models excel at structure-based design, generating
novel, pocket-fitting ligands with desired physicochemical properties, yet face
the critical hurdle of ensuring chemical synthesizability. Conversely, for
therapeutic peptides, the focus shifts to generating functional sequences and
designing de novo structures, where the primary challenges are achieving
biological stability against proteolysis, ensuring proper folding, and
minimizing immunogenicity. Despite these distinct challenges, both domains face
shared hurdles: the need for more accurate scoring functions, the scarcity of
high-quality experimental data, and the crucial requirement for experimental
validation. We conclude that the full potential of diffusion models will be
unlocked by bridging these modality-specific gaps and integrating them into
automated, closed-loop Design-Build-Test-Learn (DBTL) platforms, thereby
shifting the paradigm from chemical exploration to the targeted creation of
novel therapeutics.

</details>


### [103] [Iterative Foundation Model Fine-Tuning on Multiple Rewards](https://arxiv.org/abs/2511.00220)
*Pouya M. Ghari,Simone Sciabola,Ye Wang*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的多奖励信号微调基础模型的新方法，通过迭代优化多个奖励信号来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 在文本生成和药物发现等应用中，单一奖励信号优化往往不够理想，需要同时考虑多个评估标准。

Method: 采用基于强化学习的多奖励信号微调方法，通过迭代策略在多个奖励信号上进行优化。

Result: 在文本、生物序列和小分子生成等多个领域的实验结果表明，该方法优于现有的最先进基线方法。

Conclusion: 多奖励强化学习微调方法能够有效提升基础模型在复杂任务中的性能，理论分析为该方法提供了性能保证。

Abstract: Fine-tuning foundation models has emerged as a powerful approach for
generating objects with specific desired properties. Reinforcement learning
(RL) provides an effective framework for this purpose, enabling models to
generate outputs that maximize a given reward function. However, in many
applications such as text generation and drug discovery, it can be suboptimal
to optimize using a single reward signal, as multiple evaluation criteria are
often necessary. This paper proposes a novel reinforcement learning-based
method for fine-tuning foundation models using multiple reward signals. By
employing an iterative fine-tuning strategy across these rewards, our approach
generalizes state-of-the-art RL-based methods. We further provide a theoretical
analysis that offers insights into the performance of multi-reward RL
fine-tuning. Experimental results across diverse domains including text,
biological sequence, and small molecule generation, demonstrate the
effectiveness of the proposed algorithm compared to state-of-the-art baselines.

</details>


### [104] [Melanoma Classification Through Deep Ensemble Learning and Explainable AI](https://arxiv.org/abs/2511.00246)
*Wadduwage Shanika Perera,ABM Islam,Van Vung Pham,Min Kyung An*

Main category: cs.LG

TL;DR: 提出了一种结合集成学习和可解释人工智能（XAI）的机器学习模型，用于提高黑色素瘤检测的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤是致命性皮肤癌，早期检测至关重要。深度学习模型虽然准确率高，但缺乏可解释性，限制了其在医疗领域的应用。XAI技术可以解决这一信任问题。

Method: 使用三种最先进的深度迁移学习网络进行集成学习，并应用XAI技术来解释预测依据。

Result: 开发了一个能够可靠检测黑色素瘤的模型，通过XAI技术增强了预测结果的可解释性。

Conclusion: 集成学习结合XAI技术可以有效提高黑色素瘤检测系统的可靠性和信任度，为医疗诊断提供更可信的AI辅助工具。

Abstract: Melanoma is one of the most aggressive and deadliest skin cancers, leading to
mortality if not detected and treated in the early stages. Artificial
intelligence techniques have recently been developed to help dermatologists in
the early detection of melanoma, and systems based on deep learning (DL) have
been able to detect these lesions with high accuracy. However, the entire
community must overcome the explainability limit to get the maximum benefit
from DL for diagnostics in the healthcare domain. Because of the black box
operation's shortcomings in DL models' decisions, there is a lack of
reliability and trust in the outcomes. However, Explainable Artificial
Intelligence (XAI) can solve this problem by interpreting the predictions of AI
systems. This paper proposes a machine learning model using ensemble learning
of three state-of-the-art deep transfer Learning networks, along with an
approach to ensure the reliability of the predictions by utilizing XAI
techniques to explain the basis of the predictions.

</details>


### [105] [X-TRACK: Physics-Aware xLSTM for Realistic Vehicle Trajectory Prediction](https://arxiv.org/abs/2511.00266)
*Aanchal Rajesh Chugh,Marion Neumeier,Sebastian Dorn*

Main category: cs.LG

TL;DR: 本文提出了基于xLSTM的车辆轨迹预测框架X-TRAJ及其物理感知变体X-TRACK，通过整合车辆运动学约束来生成更真实可行的轨迹。


<details>
  <summary>Details</summary>
Motivation: 尽管xLSTM在时间序列预测中表现出色，但在车辆轨迹预测领域尚未得到充分探索。传统LSTM存在局限性，而xLSTM通过指数门控和增强内存结构能够更好地建模长期时间依赖性。

Method: 开发了X-TRAJ框架及其物理感知变体X-TRACK，将车辆运动学约束显式整合到模型学习过程中，确保生成的轨迹符合物理规律。

Result: 在highD和NGSIM数据集上的综合评估表明，X-TRACK优于现有的最先进基线方法。

Conclusion: 提出的xLSTM-based框架在车辆轨迹预测任务中表现出色，物理约束的引入显著提升了轨迹的真实性和可行性。

Abstract: Recent advancements in Recurrent Neural Network (RNN) architectures,
particularly the Extended Long Short Term Memory (xLSTM), have addressed the
limitations of traditional Long Short Term Memory (LSTM) networks by
introducing exponential gating and enhanced memory structures. These
improvements make xLSTM suitable for time-series prediction tasks as they
exhibit the ability to model long-term temporal dependencies better than LSTMs.
Despite their potential, these xLSTM-based models remain largely unexplored in
the context of vehicle trajectory prediction. Therefore, this paper introduces
a novel xLSTM-based vehicle trajectory prediction framework, X-TRAJ, and its
physics-aware variant, X-TRACK (eXtended LSTM for TRAjectory prediction
Constraint by Kinematics), which explicitly integrates vehicle motion
kinematics into the model learning process. By introducing physical
constraints, the proposed model generates realistic and feasible trajectories.
A comprehensive evaluation on the highD and NGSIM datasets demonstrates that
X-TRACK outperforms state-of-the-art baselines.

</details>


### [106] [Improving the Robustness of Control of Chaotic Convective Flows with Domain-Informed Reinforcement Learning](https://arxiv.org/abs/2511.00272)
*Michiel Straat,Thorben Markmann,Sebastian Peitz,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种基于领域知识的强化学习方法，用于控制混沌对流流动，在Rayleigh-Bénard对流系统中实现了对流热传输的显著降低。


<details>
  <summary>Details</summary>
Motivation: 混沌对流流动在微流体设备和化学反应器等实际系统中普遍存在，但传统控制方法在混沌状态下往往失效。虽然强化学习在层流控制中表现出潜力，但在混沌和湍流动力学下的泛化能力和鲁棒性尚未充分探索。

Method: 采用领域知识增强的强化学习代理，使用近端策略优化算法在不同初始条件和流动状态下进行训练。在奖励函数中引入鼓励Bénard单元合并的领域知识项，作为期望的宏观特性。

Result: 在层流状态下，领域知识增强的强化学习代理将对流热传输降低了33%；在混沌流动状态下，仍实现了10%的降低，显著优于传统控制器。与无领域知识的代理相比，该方法产生稳定流动、训练收敛更快，且无需重新训练即可跨流动状态泛化。

Conclusion: 优雅的领域知识先验可以显著增强强化学习控制混沌流动的鲁棒性，使实际部署更接近现实。

Abstract: Chaotic convective flows arise in many real-world systems, such as
microfluidic devices and chemical reactors. Stabilizing these flows is highly
desirable but remains challenging, particularly in chaotic regimes where
conventional control methods often fail. Reinforcement Learning (RL) has shown
promise for control in laminar flow settings, but its ability to generalize and
remain robust under chaotic and turbulent dynamics is not well explored,
despite being critical for real-world deployment. In this work, we improve the
practical feasibility of RL-based control of such flows focusing on
Rayleigh-B\'enard Convection (RBC), a canonical model for convective heat
transport. To enhance generalization and sample efficiency, we introduce
domain-informed RL agents that are trained using Proximal Policy Optimization
across diverse initial conditions and flow regimes. We incorporate domain
knowledge in the reward function via a term that encourages B\'enard cell
merging, as an example of a desirable macroscopic property. In laminar flow
regimes, the domain-informed RL agents reduce convective heat transport by up
to 33%, and in chaotic flow regimes, they still achieve a 10% reduction, which
is significantly better than the conventional controllers used in practice. We
compare the domain-informed to uninformed agents: Our results show that the
domain-informed reward design results in steady flows, faster convergence
during training, and generalization across flow regimes without retraining. Our
work demonstrates that elegant domain-informed priors can greatly enhance the
robustness of RL-based control of chaotic flows, bringing real-world deployment
closer.

</details>


### [107] [Calibration Across Layers: Understanding Calibration Evolution in LLMs](https://arxiv.org/abs/2511.00280)
*Abhinav Joshi,Areeb Ahmad,Ashutosh Modi*

Main category: cs.LG

TL;DR: 研究发现LLM的校准能力在网络上层存在一个置信度修正阶段，通过扰动残差流中的低维校准方向可以显著改善校准指标而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM中校准能力如何随网络深度演变，补充现有研究主要关注最终层组件（如熵神经元和未嵌入矩阵零空间）的视角。

Method: 分析多个开源权重模型在MMLU基准上的表现，研究校准在网络深度上的演化过程，识别残差流中的低维校准方向。

Result: 发现上层/后期层存在明显的置信度修正阶段，模型在达到决策确定性后会主动重新校准置信度；扰动低维校准方向可显著改善ECE和MCE指标。

Conclusion: 校准是分布式的现象，在整个网络前向传递过程中形成，而不仅仅在最终投影层，这为理解LLM内置信度调节机制提供了新见解。

Abstract: Large Language Models (LLMs) have demonstrated inherent calibration
capabilities, where predicted probabilities align well with correctness,
despite prior findings that deep neural networks are often overconfident.
Recent studies have linked this behavior to specific components in the final
layer, such as entropy neurons and the unembedding matrix null space. In this
work, we provide a complementary perspective by investigating how calibration
evolves throughout the network depth. Analyzing multiple open-weight models on
the MMLU benchmark, we uncover a distinct confidence correction phase in the
upper/later layers, where model confidence is actively recalibrated after
decision certainty has been reached. Furthermore, we identify a low-dimensional
calibration direction in the residual stream whose perturbation significantly
improves calibration metrics (ECE and MCE) without harming accuracy. Our
findings suggest that calibration is a distributed phenomenon, shaped
throughout the network forward pass, not just in its final projection,
providing new insights into how confidence-regulating mechanisms operate within
LLMs.

</details>


### [108] [A systematic evaluation of uncertainty quantification techniques in deep learning: a case study in photoplethysmography signal analysis](https://arxiv.org/abs/2511.00301)
*Ciaran Bench,Oskar Pfeffer,Vivek Desai,Mohammad Moulaeifard,Loïc Coquelin,Peter H. Charlton,Nils Strodthoff,Nando Hegemann,Philip J. Aston,Andrew Thompson*

Main category: cs.LG

TL;DR: 该研究比较了8种不确定性量化技术在医疗时间序列数据上的表现，重点关注心房颤动检测和血压回归任务，发现不同方法在不同评估指标下的可靠性表现复杂，强调应根据实际应用场景选择合适的不确定性评估标准。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医疗时间序列数据上部署时存在性能不佳的风险，可靠的不确定性估计可以为临床医生提供模型输出的可信度指导，因此需要比较不同不确定性量化方法的有效性。

Method: 实现了8种不确定性量化技术，应用于心房颤动检测（分类）和血压回归（两种变体）任务，制定了全面的评估程序来严格比较这些方法。

Result: 观察到不同技术的不确定性可靠性呈现复杂图景，最优方法取决于选择的不确定性表达方式、评估指标和可靠性评估尺度。发现局部校准和适应性评估能提供更实用的模型行为洞察。

Conclusion: 评估不确定性量化技术的标准应适应模型的实际使用场景，在每位患者测量数据较少的情况下，应优先考虑所选不确定性表达方式的小尺度可靠性，同时尽可能保持预测性能。

Abstract: In principle, deep learning models trained on medical time-series, including
wearable photoplethysmography (PPG) sensor data, can provide a means to
continuously monitor physiological parameters outside of clinical settings.
However, there is considerable risk of poor performance when deployed in
practical measurement scenarios leading to negative patient outcomes. Reliable
uncertainties accompanying predictions can provide guidance to clinicians in
their interpretation of the trustworthiness of model outputs. It is therefore
of interest to compare the effectiveness of different approaches. Here we
implement an unprecedented set of eight uncertainty quantification (UQ)
techniques to models trained on two clinically relevant prediction tasks:
Atrial Fibrillation (AF) detection (classification), and two variants of blood
pressure regression. We formulate a comprehensive evaluation procedure to
enable a rigorous comparison of these approaches. We observe a complex picture
of uncertainty reliability across the different techniques, where the most
optimal for a given task depends on the chosen expression of uncertainty,
evaluation metric, and scale of reliability assessed. We find that assessing
local calibration and adaptivity provides practically relevant insights about
model behaviour that otherwise cannot be acquired using more commonly
implemented global reliability metrics. We emphasise that criteria for
evaluating UQ techniques should cater to the model's practical use case, where
the use of a small number of measurements per patient places a premium on
achieving small-scale reliability for the chosen expression of uncertainty,
while preserving as much predictive performance as possible.

</details>


### [109] [Reject Only Critical Tokens: Pivot-Aware Speculative Decoding](https://arxiv.org/abs/2511.00351)
*Amir Ziashahabi,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Mostafa El-Khamy,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.LG

TL;DR: 本文提出了一种新的推测解码策略——Pivot-Aware Speculative Decoding，通过放宽传统推测解码的严格分布匹配要求，仅拒绝会导致最终输出效用下降的关键token，从而显著提高接受率并保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统推测解码要求输出分布与目标模型完全匹配，这导致接受率过低，限制了加速潜力。作者认为在实际应用中，任务特定性能（如代码正确性、事实准确性）比采样分布更重要。

Method: 提出Pivot-Aware Speculative Decoding策略，识别并仅拒绝会导致效用下降的关键token（称为pivot token）。训练轻量级分类器来检测这些关键token，实现标准推测解码的宽松版本。

Result: 在多个数据集上的评估显示，该方法能达到最高2.5倍的加速效果，同时保持可比较的效用性能。

Conclusion: 通过将解码目标重新定义为匹配预期效用而非严格分布匹配，可以显著提高推测解码的效率，在实际应用中实现更好的加速效果。

Abstract: Speculative Decoding (SD) ensures that the output matches the target model's
distribution exactly. However, we argue that this distribution matching
requirement is too stringent and results in unnecessarily low acceptance rates,
limiting potential speedups. Instead, we advocate a reformulation of the
decoding objective: the proposed decoding strategy should match the expected
utility, i.e., the task-specific performance, of the target model. This
perspective also aligns better with real-world use cases of LLMs, where utility
(e.g., code correctness, factual accuracy) is often more important than
sampling distribution. Based on this reformulation, we propose a novel decoding
strategy: Pivot-Aware Speculative Decoding, which rejects only those tokens
that would lead to a utility drop in the final output. We refer to these
critical tokens as pivot tokens. We propose a method for labeling tokens as
pivotal or non-pivotal and train a lightweight classifier to detect them. This
method can be viewed as a relaxed version of standard SD, which offers much
higher acceptance while preserving utility. We evaluate our method across
various datasets, demonstrating that we can achieve up to $2.5\times$ speedup
with comparable utility. Source code is available at
https://github.com/amir-zsh/PAD.

</details>


### [110] [Balancing Interpretability and Performance in Motor Imagery EEG Classification: A Comparative Study of ANFIS-FBCSP-PSO and EEGNet](https://arxiv.org/abs/2511.00369)
*Farjana Aktar,Mohd Ruhul Ameen,Akif Islam,Md Ekramul Hamid*

Main category: cs.LG

TL;DR: 比较了模糊推理方法(ANFIS-FBCSP-PSO)与深度学习基准(EEGNet)在运动想象EEG分类中的表现，发现模糊方法在个体内测试中表现更好，而深度方法在跨个体测试中泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 解决运动想象EEG分类中准确性和可解释性难以兼顾的关键挑战，为BCI系统设计提供实用指导。

Method: 使用ANFIS模糊推理方法结合滤波器组共空间模式特征提取和粒子群优化的模糊IF-THEN规则，与直接从原始EEG数据学习层次时空表示的EEGNet深度模型进行对比。

Result: 在个体内实验中，模糊神经模型表现更好(准确率68.58%±13.76%，kappa=58.04%±18.43)；在跨个体测试中，深度模型表现出更强的泛化能力(准确率68.20%±12.13%，kappa=57.33%±16.22)。

Conclusion: 根据设计目标(可解释性或跨用户鲁棒性)为MI-BCI系统选择提供实用指导，未来基于transformer和混合神经符号框架的研究有望推进透明EEG解码。

Abstract: Achieving both accurate and interpretable classification of motor imagery EEG
remains a key challenge in brain computer interface (BCI) research. This paper
compares a transparent fuzzy reasoning approach (ANFIS-FBCSP-PSO) with a deep
learning benchmark (EEGNet) using the BCI Competition IV-2a dataset. The ANFIS
pipeline combines filter bank common spatial pattern feature extraction with
fuzzy IF-THEN rules optimized via particle swarm optimization, while EEGNet
learns hierarchical spatial temporal representations directly from raw EEG
data. In within-subject experiments, the fuzzy neural model performed better
(68.58 percent +/- 13.76 percent accuracy, kappa = 58.04 percent +/- 18.43),
while in cross-subject (LOSO) tests, the deep model exhibited stronger
generalization (68.20 percent +/- 12.13 percent accuracy, kappa = 57.33 percent
+/- 16.22). The study provides practical guidance for selecting MI-BCI systems
according to design goals: interpretability or robustness across users. Future
investigations into transformer based and hybrid neuro symbolic frameworks are
expected to advance transparent EEG decoding.

</details>


### [111] [PolyRecommender: A Multimodal Recommendation System for Polymer Discovery](https://arxiv.org/abs/2511.00375)
*Xin Wang,Yunhao Xiao,Rui Qiao*

Main category: cs.LG

TL;DR: PolyRecommender是一个多模态聚合物发现框架，结合了化学语言表示和分子图表示，通过两阶段检索和排序实现高效聚合物发现。


<details>
  <summary>Details</summary>
Motivation: 为了解决聚合物发现中单一模态表示的局限性，利用语言和图表示中的互补知识来提高检索和排序的效率和鲁棒性。

Method: 首先使用基于语言的相似性检索候选聚合物，然后使用融合的多模态嵌入根据多个目标属性进行排序。

Result: 建立了可推广的多模态范式，能够跨相关聚合物属性实现高效检索和鲁棒排序。

Conclusion: 该工作推进了AI引导的下一代聚合物发现设计，为聚合物发现提供了有效的多模态方法。

Abstract: We introduce PolyRecommender, a multimodal discovery framework that
integrates chemical language representations from PolyBERT with molecular
graph-based representations from a graph encoder. The system first retrieves
candidate polymers using language-based similarity and then ranks them using
fused multimodal embeddings according to multiple target properties. By
leveraging the complementary knowledge encoded in both modalities,
PolyRecommender enables efficient retrieval and robust ranking across related
polymer properties. Our work establishes a generalizable multimodal paradigm,
advancing AI-guided design for the discovery of next-generation polymers.

</details>


### [112] [UME-R1: Exploring Reasoning-Driven Generative Multimodal Embeddings](https://arxiv.org/abs/2511.00405)
*Zhibin Lan,Liqiang Niu,Fandong Meng,Jie Zhou,Jinsong Su*

Main category: cs.LG

TL;DR: 本文提出了UME-R1，一种统一多模态嵌入的生成式框架，通过两阶段训练策略（监督微调+强化学习）实现判别式和生成式嵌入的统一，在MMEB-V2基准测试中显著优于传统判别式嵌入模型。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的嵌入方法主要是判别式的，限制了其从推理驱动的生成范式中获益的能力，需要探索生成式嵌入来统一嵌入任务。

Method: 提出UME-R1框架，采用两阶段训练：监督微调阶段赋予模型推理能力，使其能生成判别式和生成式嵌入；强化学习阶段增强推理并优化生成式嵌入质量。

Result: 在MMEB-V2基准的78个任务上显著优于传统判别式嵌入模型，生成式嵌入相比判别式嵌入带来显著性能提升，两者组合的oracle性能远超单独使用。

Conclusion: 生成式嵌入通过利用MLLMs的强大生成推理能力解锁了性能增益，RL能有效增强生成式嵌入，推理时重复采样可提升下游任务覆盖率，为更可解释、推理驱动的生成式多模态嵌入奠定了基础。

Abstract: The remarkable success of multimodal large language models (MLLMs) has driven
advances in multimodal embeddings, yet existing models remain inherently
discriminative, limiting their ability to benefit from reasoning-driven
generation paradigm. In this work, we pioneer the exploration of generative
embeddings, unifying embedding tasks within a generative paradigm. We propose
UME-R1, a universal multimodal embedding framework consisting of a two-stage
training strategy: a cold-start supervised fine-tuning equips the model with
reasoning capabilities and enables it to generate both discriminative and
generative embeddings; a subsequent reinforcement learning enhances reasoning
and further optimizes generative embedding quality. This pioneering work
reveals four key insights: 1) generative embeddings unlock substantial
performance gains over conventional discriminative embeddings by leveraging the
powerful generative reasoning capabilities of MLLMs; 2) discriminative and
generative embeddings are complementary, whose combined oracle performance far
exceeding that of either alone; 3) RL can effectively enhance generative
embeddings, establishing a scalable optimization paradigm.; 4) repeated
sampling at inference boosts downstream task coverage (pass@k), highlighting
the inference-time scalability potential of generative embeddings. Evaluated on
the MMEB-V2 benchmark across 78 tasks spanning video, image, and visual
documents, UME-R1 significantly outperforms conventional discriminative
embedding models and offers a foundation for more interpretable,
reasoning-driven generative multimodal embeddings. Our code, models, and
datasets will be publicly available at https://github.com/XMUDeepLIT/UME-R1.

</details>


### [113] [Enhancing Adversarial Transferability by Balancing Exploration and Exploitation with Gradient-Guided Sampling](https://arxiv.org/abs/2511.00411)
*Zenghao Niu,Weicheng Xie,Siyang Song,Zitong Yu,Feng Liu,Linlin Shen*

Main category: cs.LG

TL;DR: 提出Gradient-Guided Sampling (GGS)方法解决对抗攻击在迁移场景中的利用与探索困境，通过梯度引导采样平衡攻击强度与跨模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 对抗攻击在跨模型架构迁移时面临利用（最大化攻击强度）与探索（增强跨模型泛化）的根本困境，现有方法往往过度偏向其中一方。

Method: 基于MI-FGSM，引入内迭代随机采样，使用前一次内迭代的梯度指导采样方向，采样幅度由随机分布决定，实现采样效率与稳定性的平衡。

Result: 在多个DNN架构和多模态大语言模型上的综合实验表明，该方法优于最先进的迁移攻击方法。

Conclusion: GGS方法通过梯度引导采样机制，使对抗样本位于既平坦又具有较高局部最大值的平衡区域，有效解决了利用与探索的困境。

Abstract: Adversarial attacks present a critical challenge to deep neural networks'
robustness, particularly in transfer scenarios across different model
architectures. However, the transferability of adversarial attacks faces a
fundamental dilemma between Exploitation (maximizing attack potency) and
Exploration (enhancing cross-model generalization). Traditional momentum-based
methods over-prioritize Exploitation, i.e., higher loss maxima for attack
potency but weakened generalization (narrow loss surface). Conversely, recent
methods with inner-iteration sampling over-prioritize Exploration, i.e.,
flatter loss surfaces for cross-model generalization but weakened attack
potency (suboptimal local maxima). To resolve this dilemma, we propose a simple
yet effective Gradient-Guided Sampling (GGS), which harmonizes both objectives
through guiding sampling along the gradient ascent direction to improve both
sampling efficiency and stability. Specifically, based on MI-FGSM, GGS
introduces inner-iteration random sampling and guides the sampling direction
using the gradient from the previous inner-iteration (the sampling's magnitude
is determined by a random distribution). This mechanism encourages adversarial
examples to reside in balanced regions with both flatness for cross-model
generalization and higher local maxima for strong attack potency. Comprehensive
experiments across multiple DNN architectures and multimodal large language
models (MLLMs) demonstrate the superiority of our method over state-of-the-art
transfer attacks. Code is made available at https://github.com/anuin-cat/GGS.

</details>


### [114] [Tree Training: Accelerating Agentic LLMs Training via Shared Prefix Reuse](https://arxiv.org/abs/2511.00413)
*Shaojie Wang,Jinghui Wang,Yinghan Cui,Xuxing Chen,Chao Wang,Liang Huang,Xiaojiang Zhang,Junyi Peng,Li Wan,Haotian Zhang,Bin Chen*

Main category: cs.LG

TL;DR: 提出Tree Training方法，通过在训练过程中重用共享前缀的中间结果，显著提高智能体LLM训练的计算效率


<details>
  <summary>Details</summary>
Motivation: 当前训练流程将树状轨迹分解为独立线性段，导致共享前缀在前后向传播中重复计算，造成计算效率低下

Method: 采用Tree Packing重用轨迹间共享计算，结合Gradient Restoration确保梯度在重用前缀间的正确传播

Result: 在多个开源模型上实验显示，总训练时间最多减少3.9倍

Conclusion: Tree Training能够实现更高效的智能体LLM监督微调和强化学习训练

Abstract: In agentic LLM scenarios, an agent's interaction process during a single
rollout often exhibits branching behaviors. Due to memory retrieval and
concurrent tool executions at certain decision points, the token trajectory of
one task evolves into a tree-like structure rather than a linear sequence.
However, current training pipelines decompose such tree-structured trajectories
into separate linear segments, treating each branch as an independent sequence.
As a result, shared prefixes across these branches are repeatedly recomputed
during both forward and backward passes. To address this inefficiency, we
propose Tree Training, a paradigm that computes each shared prefix only once
and reuses its intermediate results across related branches during both forward
and backward passes, substantially improving computation efficiency in
large-scale agentic training. This is achieved via (i) Tree Packing, which
efficiently reuses shared computations across trajectories, and (ii) Gradient
Restoration, which ensures correct gradient propagation across reused prefixes.
Experiments on multiple open-source models demonstrate up to 3.9x reduction in
total training time, enabling more efficient agentic LLM SFT and RL training.

</details>


### [115] [Structure-Preserving Physics-Informed Neural Network for the Korteweg--de Vries (KdV) Equation](https://arxiv.org/abs/2511.00418)
*Victory Obieke,Emmanuel Oguadimma*

Main category: cs.LG

TL;DR: 该论文提出了结构保持的物理信息神经网络框架，用于求解非线性KdV方程，通过将质量守恒和哈密顿能量守恒嵌入损失函数，结合正弦激活函数，实现了物理一致且能量稳定的长期演化。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在长期积分中往往无法保持关键的物理不变量，特别是对于非线性KdV方程这样的哈密顿系统，需要确保物理守恒律的保持。

Method: 提出结构保持PINN框架，将质量守恒和哈密顿能量守恒直接嵌入损失函数，采用正弦激活函数增强谱表达能力，准确捕捉KdV孤子的振荡和色散特性。

Result: 通过单孤子传播、双孤子相互作用和余弦脉冲初始化等案例研究，成功再现了KdV动力学的典型行为，同时保持了守恒不变量。消融研究表明该方法加速收敛、提高长期稳定性并减轻漂移。

Conclusion: 计算高效的、不变量感知的正则化与正弦表示相结合，为KdV方程等哈密顿偏微分方程产生了鲁棒且能量一致的PINNs。

Abstract: Physics-Informed Neural Networks (PINNs) offer a flexible framework for
solving nonlinear partial differential equations (PDEs), yet conventional
implementations often fail to preserve key physical invariants during long-term
integration. This paper introduces a \emph{structure-preserving PINN} framework
for the nonlinear Korteweg--de Vries (KdV) equation, a prototypical model for
nonlinear and dispersive wave propagation. The proposed method embeds the
conservation of mass and Hamiltonian energy directly into the loss function,
ensuring physically consistent and energy-stable evolution throughout training
and prediction. Unlike standard \texttt{tanh}-based
PINNs~\cite{raissi2019pinn,wang2022modifiedpinn}, our approach employs
sinusoidal activation functions that enhance spectral expressiveness and
accurately capture the oscillatory and dispersive nature of KdV solitons.
Through representative case studies -- including single-soliton propagation
(shape-preserving translation), two-soliton interaction (elastic collision with
phase shift), and cosine-pulse initialization (nonlinear dispersive breakup) --
the model successfully reproduces hallmark behaviors of KdV dynamics while
maintaining conserved invariants. Ablation studies demonstrate that combining
invariant-constrained optimization with sinusoidal feature mappings accelerates
convergence, improves long-term stability, and mitigates drift without
multi-stage pretraining. These results highlight that computationally
efficient, invariant-aware regularization coupled with sinusoidal
representations yields robust, energy-consistent PINNs for Hamiltonian partial
differential equations such as the KdV equation.

</details>


### [116] [Bootstrap Off-policy with World Model](https://arxiv.org/abs/2511.00423)
*Guojian Zhan,Likun Wang,Xiangteng Zhang,Jiaxin Gao,Masayoshi Tomizuka,Shengbo Eben Li*

Main category: cs.LG

TL;DR: BOOM是一个将规划与离策略学习紧密结合的强化学习框架，通过引导循环和联合学习的世界模型解决规划带来的数据与策略行为不一致问题。


<details>
  <summary>Details</summary>
Motivation: 在线规划虽然能提高强化学习的样本效率和最终性能，但规划用于环境交互会导致收集的数据与策略实际行为不一致，从而降低模型学习和策略改进的效果。

Method: 提出BOOM框架，通过引导循环紧密集成规划和离策略学习：策略初始化规划器，规划器通过行为对齐精化动作来引导策略。使用联合学习的世界模型支持规划器模拟未来轨迹并提供价值目标。核心是使用规划器的非参数动作分布引导策略的无似然对齐损失，结合软价值加权机制优先高回报行为并减轻规划器动作质量在回放缓冲区中的变异性。

Result: 在高维DeepMind Control Suite和Humanoid-Bench上的实验表明，BOOM在训练稳定性和最终性能方面都达到了最先进的结果。

Conclusion: BOOM通过紧密集成规划和离策略学习，有效解决了规划带来的数据不一致问题，在复杂控制任务中实现了优异的性能表现。

Abstract: Online planning has proven effective in reinforcement learning (RL) for
improving sample efficiency and final performance. However, using planning for
environment interaction inevitably introduces a divergence between the
collected data and the policy's actual behaviors, degrading both model learning
and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy
with WOrld Model), a framework that tightly integrates planning and off-policy
learning through a bootstrap loop: the policy initializes the planner, and the
planner refines actions to bootstrap the policy through behavior alignment.
This loop is supported by a jointly learned world model, which enables the
planner to simulate future trajectories and provides value targets to
facilitate policy improvement. The core of BOOM is a likelihood-free alignment
loss that bootstraps the policy using the planner's non-parametric action
distribution, combined with a soft value-weighted mechanism that prioritizes
high-return behaviors and mitigates variability in the planner's action quality
within the replay buffer. Experiments on the high-dimensional DeepMind Control
Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in
both training stability and final performance. The code is accessible at
https://github.com/molumitu/BOOM_MBRL.

</details>


### [117] [Region-Aware Reconstruction Strategy for Pre-training fMRI Foundation Model](https://arxiv.org/abs/2511.00443)
*Ruthwik Reddy Doodipala,Pankaj Pandey,Carolina Torres Rojas,Manob Jyoti Saikia,Ranganatha Sitaram*

Main category: cs.LG

TL;DR: 提出了一种基于ROI引导掩码策略的fMRI基础模型预训练方法，相比传统随机掩码在ADHD分类任务上准确率提升4.23%，并增强了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 神经影像学中基础模型的发展需要处理大规模异构脑成像数据，现有基于随机区域掩码的自监督学习方法存在改进空间，需要更语义一致的区域掩码策略。

Method: 使用AAL3图谱进行ROI引导掩码策略，直接在4D fMRI体数据上选择性地掩码语义连贯的脑区，在自监督预训练中实现区域感知重建。

Result: 在ADHD-200数据集上，该方法比传统随机掩码在ADHD分类准确率上提升4.23%，区域级归因分析显示边缘系统和脑区对重建保真度和模型表示贡献最大。

Conclusion: 在模型预训练中掩码解剖区域不仅能增强可解释性，还能产生更鲁棒和判别性的表示，未来将扩展到更多神经影像数据集并开发新的区域感知重建损失函数。

Abstract: The emergence of foundation models in neuroimaging is driven by the
increasing availability of large-scale and heterogeneous brain imaging
datasets. Recent advances in self-supervised learning, particularly
reconstruction-based objectives, have demonstrated strong potential for
pretraining models that generalize effectively across diverse downstream
functional MRI (fMRI) tasks. In this study, we explore region-aware
reconstruction strategies for a foundation model in resting-state fMRI, moving
beyond approaches that rely on random region masking. Specifically, we
introduce an ROI-guided masking strategy using the Automated Anatomical
Labelling Atlas (AAL3), applied directly to full 4D fMRI volumes to selectively
mask semantically coherent brain regions during self-supervised pretraining.
Using the ADHD-200 dataset comprising 973 subjects with resting-state fMRI
scans, we show that our method achieves a 4.23% improvement in classification
accuracy for distinguishing healthy controls from individuals diagnosed with
ADHD, compared to conventional random masking. Region-level attribution
analysis reveals that brain volumes within the limbic region and cerebellum
contribute most significantly to reconstruction fidelity and model
representation. Our results demonstrate that masking anatomical regions during
model pretraining not only enhances interpretability but also yields more
robust and discriminative representations. In future work, we plan to extend
this approach by evaluating it on additional neuroimaging datasets, and
developing new loss functions explicitly derived from region-aware
reconstruction objectives. These directions aim to further improve the
robustness and interpretability of foundation models for functional
neuroimaging.

</details>


### [118] [Deep Learning Approach to Anomaly Detection in Enterprise ETL Processes with Autoencoders](https://arxiv.org/abs/2511.00462)
*Xin Chen,Saili Uday Gadgil,Kangning Gao,Yi Hu,Cong Nie*

Main category: cs.LG

TL;DR: 提出基于深度自编码器的异常检测方法，用于检测企业级ETL数据流中的多种异常类型，通过编码器-解码器结构和正则化约束实现高效异常检测。


<details>
  <summary>Details</summary>
Motivation: 解决企业级ETL数据流中常见的异常问题，包括延迟、缺失值、重复加载和突发异常变化等，确保数据处理的质量和稳定性。

Method: 使用深度自编码器结构，将高维输入压缩为潜在表示并重构，通过重构误差衡量异常程度，在潜在空间引入正则化约束增强特征稀疏性和分布学习。

Result: 在不同超参数设置、环境变化和数据特征下，该方法在AUC、ACC、Precision和Recall指标上均表现优异。

Conclusion: 基于深度自编码器的检测机制能有效捕捉企业级ETL数据流的潜在分布模式，准确识别多种异常，为企业数据处理和智能分析提供可靠支持。

Abstract: An anomaly detection method based on deep autoencoders is proposed to address
anomalies that often occur in enterprise-level ETL data streams. The study
first analyzes multiple types of anomalies in ETL processes, including delays,
missing values, duplicate loading, and sudden abnormal changes, and applies
data standardization and feature modeling to ensure stable and usable inputs.
In the method design, the encoder-decoder structure compresses high-dimensional
inputs into latent representations and reconstructs them, while reconstruction
error is used to measure anomaly levels. Regularization constraints are
introduced in the latent space to enhance feature sparsity and distribution
learning, thereby improving robustness in complex data streams. Systematic
analyses under different hyperparameter settings, environmental changes, and
data characteristics show that the proposed method achieves superior
performance in AUC, ACC, Precision, and Recall. The results demonstrate that
the deep autoencoder-based detection mechanism can effectively capture latent
distribution patterns in enterprise-level ETL data streams and accurately
identify diverse anomalies, providing reliable support for enterprise data
processing and intelligent analysis.

</details>


### [119] [Variational Autoencoder for Calibration: A New Approach](https://arxiv.org/abs/2511.00475)
*Travis Barrett,Amit Kumar Mishra,Joyce Mwangama*

Main category: cs.LG

TL;DR: 提出了一种基于变分自编码器（VAE）的传感器校准新方法，通过将潜在空间作为校准输出来校准传感器数据，并在多传感器气体数据集上进行了概念验证。


<details>
  <summary>Details</summary>
Motivation: 探索使用VAE进行传感器校准的可能性，利用其潜在空间作为校准输出，同时保持自编码器的重建功能。

Method: 使用变分自编码器（VAE）架构，将潜在空间训练为校准输出，并在多传感器气体数据集上进行训练和验证。

Result: 提出的校准VAE能够同时作为校准模型和自编码器工作，从校准输出和重建输出都能产生与真实数据统计相似的结果。

Conclusion: VAE可以有效地用于传感器校准，该方法展示了同时执行校准和重建任务的潜力，为未来测试和扩展奠定了基础。

Abstract: In this paper we present a new implementation of a Variational Autoencoder
(VAE) for the calibration of sensors. We propose that the VAE can be used to
calibrate sensor data by training the latent space as a calibration output. We
discuss this new approach and show a proof-of-concept using an existing
multi-sensor gas dataset. We show the performance of the proposed calibration
VAE and found that it was capable of performing as calibration model while
performing as an autoencoder simultaneously. Additionally, these models have
shown that they are capable of creating statistically similar outputs from both
the calibration output as well as the reconstruction output to their respective
truth data. We then discuss the methods of future testing and planned expansion
of this work.

</details>


### [120] [Reasoning Planning for Language Models](https://arxiv.org/abs/2511.00521)
*Bao Nguyen,Hieu Trung Nguyen,Ruifeng She,Xiaojin Fu,Viet Anh Nguyen*

Main category: cs.LG

TL;DR: EPIC框架通过对比学习构建共享表示空间，学习模型推理能力和查询-方法兼容性，在平衡准确性和计算成本的同时选择最优推理方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常生成多个候选响应并使用聚合策略选择输出答案，但假设更多候选答案能带来更高准确性。本文重新审视这一假设。

Method: 引入EPIC（Ensemble Planning with Contrastive learning）框架，通过对比学习构建共享表示空间，将概率边界作为正则化器纳入效用驱动的优化中。

Result: 在多样化数学推理任务上的实验表明，EPIC能持续选择最优推理方法，在提高准确性的同时减少计算开销。

Conclusion: EPIC框架通过理论分析和对比学习，有效解决了推理方法选择问题，实现了准确性和计算效率的平衡。

Abstract: Selecting an appropriate reasoning method for a given query remains a key
challenge in language model generation. Existing approaches typically generate
multiple candidate responses and use an aggregation strategy to select the
output answer, often assuming that more candidate answers yield higher
accuracy. We revisit this assumption through a rigorous theoretical analysis,
deriving accuracy bounds for standard aggregation methods under fixed
generation distributions and candidate sizes. Building on these insights, we
introduce EPIC, an Ensemble Planning with Contrastive learning framework to
learn a shared representation space that captures both model reasoning
abilities and query-method compatibility. EPIC incorporates our probability
bounds as a regularizer in a utility-driven optimization that balances accuracy
and computational cost. Experiments on diverse mathematical reasoning tasks
show that EPIC consistently selects optimal reasoning methods, improving
accuracy while reducing computational overhead. Our code can be found at
https://github.com/nguyenngocbaocmt02/EPIC.

</details>


### [121] [Air Pollution Forecasting in Bucharest](https://arxiv.org/abs/2511.00532)
*Dragoş-Andrei Şerban,Răzvan-Alexandru Smădu,Dumitru-Clementin Cercel*

Main category: cs.LG

TL;DR: 本文旨在设计和评估多种机器学习模型来预测PM2.5浓度，包括线性回归、集成方法、深度学习模型和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: PM2.5空气污染对健康造成严重威胁，预测未来PM2.5水平可以提供早期预警并帮助预防疾病。

Method: 设计、微调、测试和评估多种机器学习模型，包括线性回归算法、集成方法、深度学习模型（如循环神经网络和变换器）以及大型语言模型。

Result: 论文评估和比较了这些模型在PM2.5预测任务上的性能表现。

Conclusion: 通过系统比较多种机器学习方法，为PM2.5预测提供了有效的模型选择参考。

Abstract: Air pollution, especially the particulate matter 2.5 (PM2.5), has become a
growing concern in recent years, primarily in urban areas. Being exposed to air
pollution is linked to developing numerous health problems, like the
aggravation of respiratory diseases, cardiovascular disorders, lung function
impairment, and even cancer or early death. Forecasting future levels of PM2.5
has become increasingly important over the past few years, as it can provide
early warnings and help prevent diseases. This paper aims to design, fine-tune,
test, and evaluate machine learning models for predicting future levels of
PM2.5 over various time horizons. Our primary objective is to assess and
compare the performance of multiple models, ranging from linear regression
algorithms and ensemble-based methods to deep learning models, such as advanced
recurrent neural networks and transformers, as well as large language models,
on this forecasting task.

</details>


### [122] [Robust Single-Agent Reinforcement Learning for Regional Traffic Signal Control Under Demand Fluctuations](https://arxiv.org/abs/2511.00549)
*Qiang Li,Jin Niu,Lina Yu*

Main category: cs.LG

TL;DR: 提出基于单智能体强化学习的区域自适应交通信号控制框架，使用DreamerV3世界模型和邻接矩阵统一编码路网拓扑、实时队列状态和信号参数，有效缓解交通拥堵。


<details>
  <summary>Details</summary>
Motivation: 交通拥堵严重影响城市生活质量，传统交通信号控制模型难以捕捉真实交通复杂性，多智能体系统存在协调困难问题。

Method: 采用单智能体强化学习框架，通过邻接矩阵统一编码路网信息，利用DreamerV3世界模型学习控制策略，顺序选择交叉口并调整信号配时。

Result: 在SUMO仿真实验中，面对10%-30%的OD需求波动，该框架展现出强鲁棒性，显著减少了队列长度。

Conclusion: 建立了一种与探测车辆技术兼容的智能交通控制新范式，为未来交通管理提供了有效解决方案。

Abstract: Traffic congestion, primarily driven by intersection queuing, significantly
impacts urban living standards, safety, environmental quality, and economic
efficiency. While Traffic Signal Control (TSC) systems hold potential for
congestion mitigation, traditional optimization models often fail to capture
real-world traffic complexity and dynamics. This study introduces a novel
single-agent reinforcement learning (RL) framework for regional adaptive TSC,
circumventing the coordination complexities inherent in multi-agent systems
through a centralized decision-making paradigm. The model employs an adjacency
matrix to unify the encoding of road network topology, real-time queue states
derived from probe vehicle data, and current signal timing parameters.
Leveraging the efficient learning capabilities of the DreamerV3 world model,
the agent learns control policies where actions sequentially select
intersections and adjust their signal phase splits to regulate traffic
inflow/outflow, analogous to a feedback control system. Reward design
prioritizes queue dissipation, directly linking congestion metrics (queue
length) to control actions. Simulation experiments conducted in SUMO
demonstrate the model's effectiveness: under inference scenarios with
multi-level (10%, 20%, 30%) Origin-Destination (OD) demand fluctuations, the
framework exhibits robust anti-fluctuation capability and significantly reduces
queue lengths. This work establishes a new paradigm for intelligent traffic
control compatible with probe vehicle technology. Future research will focus on
enhancing practical applicability by incorporating stochastic OD demand
fluctuations during training and exploring regional optimization mechanisms for
contingency events.

</details>


### [123] [Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales](https://arxiv.org/abs/2511.00552)
*Santhi Bharath Punati,Sandeep Kanta,Udaya Bhasker Cheerala,Madhusudan G Lanjewar,Praveen Damacharla*

Main category: cs.LG

TL;DR: 使用时间融合变换器(TFT)对沃尔玛周度销售进行多步预测，融合静态商店标识符和动态外部变量，在2012年测试集上RMSE为$57.9k，R²为0.9875，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 准确的零售多步预测对库存和促销管理至关重要，需要融合多种静态和动态因素来提高预测精度。

Method: 采用时间融合变换器(TFT)，结合静态商店标识符和时间变化的外部信号(节假日、CPI、油价、温度)，通过分位数损失生成1-5周的概率预测，并提供可解释性分析。

Result: 在固定2012年测试集上，TFT的RMSE为$57.9k，R²为0.9875；在5折时序交叉验证中，平均RMSE为$64.6k，R²为0.9844，优于XGB、CNN、LSTM和CNN-LSTM基线模型。

Conclusion: TFT模型在保持透明度的同时，为库存规划和节假日优化提供了实用价值，证明了其在零售预测中的有效性。

Abstract: Accurate multi-horizon retail forecasts are critical for inventory and
promotions. We present a novel study of weekly Walmart sales (45 stores,
2010--2012) using a Temporal Fusion Transformer (TFT) that fuses static store
identifiers with time-varying exogenous signals (holidays, CPI, fuel price,
temperature). The pipeline produces 1--5-week-ahead probabilistic forecasts via
Quantile Loss, yielding calibrated 90\% prediction intervals and
interpretability through variable-selection networks, static enrichment, and
temporal attention. On a fixed 2012 hold-out dataset, TFT achieves an RMSE of
\$57.9k USD per store-week and an $R^2$ of 0.9875. Across a 5-fold
chronological cross-validation, the averages are RMSE = \$64.6k USD and $R^2$ =
0.9844, outperforming the XGB, CNN, LSTM, and CNN-LSTM baseline models. These
results demonstrate practical value for inventory planning and holiday-period
optimization, while maintaining model transparency.

</details>


### [124] [Red-teaming Activation Probes using Prompted LLMs](https://arxiv.org/abs/2511.00554)
*Phil Blandfort,Robert Graham*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级黑盒红队测试方法，使用现成LLM通过迭代反馈和上下文学习来测试激活探针的鲁棒性，无需微调、梯度或架构访问，发现了可解释的脆弱性模式。


<details>
  <summary>Details</summary>
Motivation: 激活探针作为AI系统监控工具具有低成本、低延迟的优点，但其在真实黑盒对抗压力下的鲁棒性尚未充分探索，需要发现故障模式并最小化测试成本。

Method: 采用轻量级黑盒红队测试流程，包装现成LLM并利用迭代反馈和上下文学习，无需微调、梯度或架构访问，在高风险交互场景下进行案例研究。

Result: 发现了可解释的脆弱性模式（如法律术语导致的误报、平淡程序性语调导致的漏报），在场景约束攻击下漏洞减少但仍持续存在。

Conclusion: 简单的提示式红队测试框架可以在部署前预测故障模式，为未来探针的加固提供有前景且可操作的见解。

Abstract: Activation probes are attractive monitors for AI systems due to low cost and
latency, but their real-world robustness remains underexplored. We ask: What
failure modes arise under realistic, black-box adversarial pressure, and how
can we surface them with minimal effort? We present a lightweight black-box
red-teaming procedure that wraps an off-the-shelf LLM with iterative feedback
and in-context learning (ICL), and requires no fine-tuning, gradients, or
architectural access. Running a case study with probes for high-stakes
interactions, we show that our approach can help discover valuable insights
about a SOTA probe. Our analysis uncovers interpretable brittleness patterns
(e.g., legalese-induced FPs; bland procedural tone FNs) and reduced but
persistent vulnerabilities under scenario-constraint attacks. These results
suggest that simple prompted red-teaming scaffolding can anticipate failure
patterns before deployment and might yield promising, actionable insights to
harden future probes.

</details>


### [125] [Bayesian Network Structure Discovery Using Large Language Models](https://arxiv.org/abs/2511.00574)
*Yinghuan Zhang,Yufei Zhang,Parisa Kordjamshidi,Zijun Cui*

Main category: cs.LG

TL;DR: 提出了一个以LLM为核心的贝叶斯网络结构发现统一框架，支持无数据和有数据两种场景，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统结构学习方法需要大量观测数据且计算成本高，现有LLM方法仅将其作为辅助工具，核心学习过程仍依赖数据驱动。

Method: 在无数据情况下使用PromptBN查询LLM获取概率关系；在有数据情况下使用ReActBN，将ReAct推理范式与结构评分（如BIC）结合进行迭代优化。

Result: 实验表明该方法显著优于现有LLM方法和传统数据驱动算法，尤其在低数据或无数据场景下表现突出。

Conclusion: 该框架将LLM置于结构发现过程的核心位置，支持端到端学习，在数据稀缺情况下具有重要应用价值。

Abstract: Understanding probabilistic relationships among variables is crucial for
analyzing complex systems. Traditional structure learning methods often require
extensive observational data and incur high computational costs. Recent studies
have explored using large language models (LLMs) for structure learning, but
most treat LLMs as auxiliary tools for pre-processing or post-processing,
leaving the core learning process data-driven. In this work, we propose a
unified framework for Bayesian network structure discovery that places LLMs at
the center, supporting both data-free and data-aware settings. In the data-free
case, we introduce \textbf{PromptBN} to query LLMs with metadata and
efficiently uncover valid probabilistic relationships. When observational data
are available, we introduce \textbf{ReActBN}, which integrates the ReAct
reasoning paradigm with structure scores such as the Bayesian Information
Criterion (BIC) for iterative refinement. Unlike prior methods that offload
refinement to external algorithms, our framework maintains the LLM actively in
the loop throughout the discovery process. Experiments demonstrate that our
method significantly outperforms both existing LLM-based approaches and
traditional data-driven algorithms, particularly in the low- or no-data
scenario. Code is publicly available at
{\texttt{\textcolor{magenta}{https://github.com/sherryzyh/prompt2bn}}}.

</details>


### [126] [Diagnosing Hallucination Risk in AI Surgical Decision-Support: A Sequential Framework for Sequential Validation](https://arxiv.org/abs/2511.00588)
*Dong Chen,Yanzhe Wei,Zonglin He,Guan-Ming Kuang,Canhua Ye,Meiru An,Huili Peng,Yong Hu,Huiren Tao,Kenneth MC Cheung*

Main category: cs.LG

TL;DR: 本研究提出了一个临床医生中心的框架来量化LLM在脊柱手术中的幻觉风险，评估了六个领先LLM在30个专家验证的脊柱病例上的表现。DeepSeek-R1表现最佳，但推理增强模型变体并未始终优于标准版本，多维度压力测试暴露了模型特定的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在脊柱手术临床决策支持中具有变革潜力，但存在幻觉风险，可能危及患者安全，需要量化评估这些风险。

Method: 采用临床医生中心框架，评估诊断精度、推荐质量、推理稳健性、输出一致性和知识对齐五个维度，在30个专家验证的脊柱病例上测试六个领先LLM。

Result: DeepSeek-R1总体表现最佳（总分：86.03±2.08），在创伤和感染等高风险领域表现尤为突出。推理增强模型变体并未始终优于标准版本，多维度压力测试显示推荐质量在复杂性增加时下降7.4%。

Conclusion: 研究主张将可解释性机制（如推理链可视化）整合到临床工作流程中，并为手术LLM部署建立了安全感知的验证框架。

Abstract: Large language models (LLMs) offer transformative potential for clinical
decision support in spine surgery but pose significant risks through
hallucinations, which are factually inconsistent or contextually misaligned
outputs that may compromise patient safety. This study introduces a
clinician-centered framework to quantify hallucination risks by evaluating
diagnostic precision, recommendation quality, reasoning robustness, output
coherence, and knowledge alignment. We assessed six leading LLMs across 30
expert-validated spinal cases. DeepSeek-R1 demonstrated superior overall
performance (total score: 86.03 $\pm$ 2.08), particularly in high-stakes
domains such as trauma and infection. A critical finding reveals that
reasoning-enhanced model variants did not uniformly outperform standard
counterparts: Claude-3.7-Sonnet's extended thinking mode underperformed
relative to its standard version (80.79 $\pm$ 1.83 vs. 81.56 $\pm$ 1.92),
indicating extended chain-of-thought reasoning alone is insufficient for
clinical reliability. Multidimensional stress-testing exposed model-specific
vulnerabilities, with recommendation quality degrading by 7.4% under amplified
complexity. This decline contrasted with marginal improvements in rationality
(+2.0%), readability (+1.7%) and diagnosis (+4.7%), highlighting a concerning
divergence between perceived coherence and actionable guidance. Our findings
advocate integrating interpretability mechanisms (e.g., reasoning chain
visualization) into clinical workflows and establish a safety-aware validation
framework for surgical LLM deployment.

</details>


### [127] [Gaining Momentum: Uncovering Hidden Scoring Dynamics in Hockey through Deep Neural Sequencing and Causal Modeling](https://arxiv.org/abs/2511.00615)
*Daniel Griffiths,Piper Moskow*

Main category: cs.LG

TL;DR: 提出了一个统一的数据驱动框架，通过五个阶段量化并提升冰球比赛中的进攻势头和得分可能性，结果显示采用最优事件序列和阵型能显著提高15%的得分潜力。


<details>
  <summary>Details</summary>
Motivation: 旨在为冰球分析提供基于因果推断的战术优化方法，为教练和分析师提供实时、可操作的洞察，推动冰球分析向有原则、因果基础的方向发展。

Method: 五阶段端到端流程：1) 逻辑回归进行可解释的势头权重分配；2) 梯度提升决策树进行非线性xG估计；3) LSTM网络进行时序建模；4) PCA和K-Means聚类发现空间阵型；5) X-Learner因果推断估计器量化最优策略的平均处理效应。

Result: 观察到平均处理效应为0.12（95%置信区间：0.05-0.17，p < 1e-50），相当于得分潜力相对提升15%，证明结构化序列和紧凑阵型能因果性地提升进攻表现。

Conclusion: 战略结构化的序列和紧凑阵型能够因果性地提升进攻表现，该框架为冰球战术优化提供了基于因果推断的实用工具。

Abstract: We present a unified, data-driven framework for quantifying and enhancing
offensive momentum and scoring likelihood (expected goals, xG) in professional
hockey. Leveraging a Sportlogiq dataset of 541,000 NHL event records, our
end-to-end pipeline comprises five stages: (1) interpretable momentum weighting
of micro-events via logistic regression; (2) nonlinear xG estimation using
gradient-boosted decision trees; (3) temporal sequence modeling with Long
Short-Term Memory (LSTM) networks; (4) spatial formation discovery through
principal component analysis (PCA) followed by K-Means clustering on
standardized player coordinates; and (5) use of an X-Learner causal inference
estimator to quantify the average treatment effect (ATE) of adopting the
identified "optimal" event sequences and formations. We observe an ATE of 0.12
(95% CI: 0.05-0.17, p < 1e-50), corresponding to a 15% relative gain in scoring
potential. These results demonstrate that strategically structured sequences
and compact formations causally elevate offensive performance. Our framework
delivers real-time, actionable insights for coaches and analysts, advancing
hockey analytics toward principled, causally grounded tactical optimization.

</details>


### [128] [Reviving Stale Updates: Data-Free Knowledge Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.00655)
*Baris Askin,Holger R. Roth,Zhenyu Sun,Carlee Joe-Wong,Gauri Joshi,Ziyue Xu*

Main category: cs.LG

TL;DR: FedRevive是一个异步联邦学习框架，通过数据无关知识蒸馏来缓解陈旧更新问题，在保持AFL可扩展性的同时显著提升训练速度和最终精度。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习(AFL)虽然解决了同步开销问题，但引入了陈旧更新（基于过时全局模型的客户端更新），这会破坏优化稳定性并阻碍收敛。

Method: FedRevive结合参数空间聚合与轻量级服务器端数据无关知识蒸馏(DFKD)，使用元学习生成器合成伪样本进行多教师蒸馏，采用混合聚合方案结合原始更新和DFKD更新。

Result: 在各种视觉和文本基准测试中，FedRevive相比异步基线实现了高达32.1%的更快训练速度和高达21.5%的更高最终精度。

Conclusion: FedRevive通过数据无关知识蒸馏有效缓解了异步联邦学习中的陈旧更新问题，在保持可扩展性的同时显著提升了训练效率和模型性能。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients without sharing raw data, yet its scalability is limited by
synchronization overhead. Asynchronous Federated Learning (AFL) alleviates this
issue by allowing clients to communicate independently, thereby improving
wall-clock efficiency in large-scale, heterogeneous environments. However, this
asynchrony introduces stale updates (client updates computed on outdated global
models) that can destabilize optimization and hinder convergence. We propose
FedRevive, an asynchronous FL framework that revives stale updates through
data-free knowledge distillation (DFKD). FedRevive integrates parameter-space
aggregation with a lightweight, server-side DFKD process that transfers
knowledge from stale client models to the current global model without access
to real or public data. A meta-learned generator synthesizes pseudo-samples,
which enables multi-teacher distillation. A hybrid aggregation scheme that
combines raw updates with DFKD updates effectively mitigates staleness while
retaining the scalability of AFL. Experiments on various vision and text
benchmarks show that FedRevive achieves faster training up to 32.1% and higher
final accuracy up to 21.5% compared to asynchronous baselines.

</details>


### [129] [Sensitivity Analysis for Climate Science with Generative Flow Models](https://arxiv.org/abs/2511.00663)
*Alex Dobra,Jakiw Pidstrigach,Tim Reichelt,Paolo Fraccaro,Johannes Jakubik,Anne Jones,Christian Schroeder de Witt,Philip Stier,Philip Torr*

Main category: cs.LG

TL;DR: 应用伴随状态方法计算生成流模型中的梯度，实现气候科学中敏感性分析的高效计算，将计算成本从超级计算机上的数周降低到GPU上的数小时。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型计算敏感性分析成本过高，而AI生成模型虽然评估速度快，但计算敏感性仍是瓶颈。

Method: 使用伴随状态方法计算生成流模型（特别是扩散模型）中的梯度，应用于cBottle生成模型进行海表温度敏感性分析，并提出梯度自一致性验证方法。

Result: 该方法能够产生可靠的梯度，显著降低敏感性分析的计算成本。

Conclusion: 该方法为气候科学中的关键工作流程提供了简化方案，证明了在生成模型中高效计算敏感性的可行性。

Abstract: Sensitivity analysis is a cornerstone of climate science, essential for
understanding phenomena ranging from storm intensity to long-term climate
feedbacks. However, computing these sensitivities using traditional physical
models is often prohibitively expensive in terms of both computation and
development time. While modern AI-based generative models are orders of
magnitude faster to evaluate, computing sensitivities with them remains a
significant bottleneck. This work addresses this challenge by applying the
adjoint state method for calculating gradients in generative flow models, with
diffusion models as a special case. We apply this method to the cBottle
generative model, an emulator of ERA5 data, to perform sensitivity analysis
with respect to sea surface temperatures. Furthermore, we propose a novel
gradient self-consistency check to quantitatively validate the computed
sensitivities against the model's own outputs. Our results provide initial
evidence that this approach can produce reliable gradients, reducing the
computational cost of sensitivity analysis from weeks on a supercomputer with a
physical model to hours on a GPU, thereby simplifying a critical workflow in
climate science.

</details>


### [130] [Inference-Time Chain-of-Thought Pruning with Latent Informativeness Signals](https://arxiv.org/abs/2511.00699)
*Sophie Li,Nicholas Huang,Nayan Saxena,Nina Luo,Vincent Lin,Kevin Zhu,Sunishchal Dev*

Main category: cs.LG

TL;DR: KAPPA是一种推理时方法，通过KL散度、置信度和熵的组合评分函数来指导渐进式剪枝，在保持准确性的同时大幅减少内存和token使用。


<details>
  <summary>Details</summary>
Motivation: 标准方法如Best-of-N在生成多个候选解时计算成本高，而现有的Self-Truncation Best-of-N依赖于一致性启发式方法，不能直接评估分支质量。

Method: 结合KL散度、置信度和熵构建原则性评分函数，在探索阶段促进多样性，并选择性消除低分分支。

Result: 在GSM8K和MATH500数据集上的实验显示，KAPPA在较小模型中稳定性能，相比BoN减少约60%峰值内存和约90%总token生成，对准确性影响最小。

Conclusion: KAPPA通过原则性分支评估和渐进式剪枝，有效平衡了推理准确性和计算效率。

Abstract: Large language models (LLMs) improve reasoning accuracy when generating
multiple candidate solutions at test time, but standard methods like Best-of-N
(BoN) incur high computational cost by fully generating all branches.
Self-Truncation Best-of-N (ST-BoN) mitigates this by truncating unpromising
paths early, but its reliance on consistency-based heuristics is a limitation
as it does not directly evaluate branch quality. We present KL-Adjusted Pruned
Path Algorithm (KAPPA), an inference-time method that combines Kullback-Leibler
divergence, confidence, and entropy into a principled scoring function to guide
progressive pruning. By promoting diversity during exploration and selectively
eliminating low-scoring branches, KAPPA maintains accuracy while substantially
reducing memory and token usage. Experiments on GSM8K and MATH500 with
DeepSeek-R1-Distill-Qwen-1.5B and Qwen2.5-7B-Instruct demonstrate that KAPPA
stabilizes performance in smaller models and achieves up to ~60% reduction in
peak memory and ~90% reduction in total token generation relative to BoN, with
minimal impact on accuracy.

</details>


### [131] [Privacy-Aware Time Series Synthesis via Public Knowledge Distillation](https://arxiv.org/abs/2511.00700)
*Penghang Liu,Haibei Zhu,Eleonora Kreacic,Svitlana Vyetrenko*

Main category: cs.LG

TL;DR: 提出Pub2Priv框架，利用公开知识生成隐私时间序列数据，通过自注意力机制编码公开数据，结合扩散模型生成合成隐私序列，在隐私-效用权衡方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 敏感时间序列数据（如金融、医疗记录）因隐私问题难以共享，现有隐私感知数据生成方法往往忽略与公开上下文元数据的相关性，导致隐私-效用权衡不理想。

Method: 使用自注意力机制将公开数据编码为时间和特征嵌入，作为扩散模型的条件输入来生成合成隐私序列，并引入可识别性评估指标来衡量隐私性。

Result: 在金融、能源和大宗商品交易领域的实验表明，Pub2Priv在改善隐私-效用权衡方面持续优于现有基准方法。

Conclusion: Pub2Priv通过有效利用异构公开知识，显著提升了隐私时间序列数据生成的性能，为敏感数据共享提供了更好的解决方案。

Abstract: Sharing sensitive time series data in domains such as finance, healthcare,
and energy consumption, such as patient records or investment accounts, is
often restricted due to privacy concerns. Privacy-aware synthetic time series
generation addresses this challenge by enforcing noise during training,
inherently introducing a trade-off between privacy and utility. In many cases,
sensitive sequences is correlated with publicly available, non-sensitive
contextual metadata (e.g., household electricity consumption may be influenced
by weather conditions and electricity prices). However, existing privacy-aware
data generation methods often overlook this opportunity, resulting in
suboptimal privacy-utility trade-offs. In this paper, we present Pub2Priv, a
novel framework for generating private time series data by leveraging
heterogeneous public knowledge. Our model employs a self-attention mechanism to
encode public data into temporal and feature embeddings, which serve as
conditional inputs for a diffusion model to generate synthetic private
sequences. Additionally, we introduce a practical metric to assess privacy by
evaluating the identifiability of the synthetic data. Experimental results show
that Pub2Priv consistently outperforms state-of-the-art benchmarks in improving
the privacy-utility trade-off across finance, energy, and commodity trading
domains.

</details>


### [132] [TRISKELION-1: Unified Descriptive-Predictive-Generative AI](https://arxiv.org/abs/2511.00711)
*Nardeep Kumar,Arun Kanwar*

Main category: cs.LG

TL;DR: TRISKELION-1是一个统一的描述-预测-生成架构，在单一编码器-解码器框架中集成了统计、机制和生成推理。


<details>
  <summary>Details</summary>
Motivation: 构建一个能够连接可解释性、准确性和创造性的通用智能架构蓝图。

Method: 使用变分目标联合优化描述性表示学习、预测推理和生成合成，在MNIST数据集上进行实验验证。

Result: 实验证明描述性重建、预测分类和生成采样可以在一个模型中稳定共存。

Conclusion: 该框架为实现连接可解释性、准确性和创造性的通用智能架构提供了蓝图。

Abstract: TRISKELION-1 is a unified descriptive-predictive-generative architecture that
integrates statistical, mechanistic, and generative reasoning within a single
encoder-decoder framework. The model demonstrates how descriptive
representation learning, predictive inference, and generative synthesis can be
jointly optimized using variational objectives. Experiments on MNIST validate
that descriptive reconstruction, predictive classification, and generative
sampling can coexist stably within one model. The framework provides a
blueprint toward universal intelligence architectures that connect
interpretability, accuracy, and creativity.

</details>


### [133] [Enhancing Heavy Rain Nowcasting with Multimodal Data: Integrating Radar and Satellite Observations](https://arxiv.org/abs/2511.00716)
*Rama Kassoumeh,David Rügamer,Henning Oppel*

Main category: cs.LG

TL;DR: 该论文提出了一种融合卫星和雷达数据的多模态临近预报模型，用于预测5、15和30分钟内的强降水事件。实验表明该方法显著优于仅使用雷达的方法，特别是在强降雨预测方面表现更佳。


<details>
  <summary>Details</summary>
Motivation: 传统地面传感器难以检测局部强降雨事件（德国2001-2018年间仅17.3%的强降雨被雨量计记录），雷达数据单独预测强降雨仍具挑战性，因此需要融合卫星数据来提高预报准确性。

Method: 开发了一个多模态临近预报模型，将雷达和卫星图像数据融合，用于预测5、15和30分钟内的降水量。

Result: 多模态方法显著优于仅使用雷达的方法，在5分钟预报时间内，强降雨的临界成功指数提高4%，剧烈降雨提高3%。在更长预报时间上保持更高的预测能力，对2021年德国北莱茵-威斯特法伦州洪水事件的定性分析也显示了其优越性能。

Conclusion: 融合卫星和雷达数据的多模态模型能够提供更详细准确的强降雨预报，实现及时可靠的救生预警。

Abstract: The increasing frequency of heavy rainfall events, which are a major cause of
urban flooding, underscores the urgent need for accurate precipitation
forecasting - particularly in urban areas where localized events often go
undetected by ground-based sensors. In Germany, only 17.3% of hourly heavy rain
events between 2001 and 2018 were recorded by rain gauges, highlighting the
limitations of traditional monitoring systems. Radar data are another source
that effectively tracks ongoing precipitation; however, forecasting the
development of heavy rain using radar alone remains challenging due to the
brief and unpredictable nature of such events. Our focus is on evaluating the
effectiveness of fusing satellite and radar data for nowcasting. We develop a
multimodal nowcasting model that combines both radar and satellite imagery for
predicting precipitation at lead times of 5, 15, and 30 minutes. We demonstrate
that this multimodal strategy significantly outperforms radar-only approaches.
Experimental results show that integrating satellite data improves prediction
accuracy, particularly for intense precipitation. The proposed model increases
the Critical Success Index for heavy rain by 4% and for violent rain by 3% at a
5-minute lead time. Moreover, it maintains higher predictive skill at longer
lead times, where radar-only performance declines. A qualitative analysis of
the severe flooding event in the state of North Rhine-Westphalia, Germany in
2021 further illustrates the superior performance of the multimodal model.
Unlike the radar-only model, which captures general precipitation patterns, the
multimodal model yields more detailed and accurate forecasts for regions
affected by heavy rain. This improved precision enables timely, reliable,
life-saving warnings. Implementation available at
https://github.com/RamaKassoumeh/Multimodal_heavy_rain

</details>


### [134] [Effective Series Decomposition and Components Learning for Time Series Generation](https://arxiv.org/abs/2511.00747)
*Zixuan Ma,Chenfeng Huang*

Main category: cs.LG

TL;DR: STDiffusion是一个新颖的多变量时间序列生成框架，结合扩散概率模型和可学习的序列分解技术，通过分别学习趋势和季节性成分来提高生成过程的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列生成方法缺乏可解释的分解方法，无法有效合成有意义的趋势和季节性模式，限制了生成数据的质量。

Method: 使用扩散概率模型，将趋势和季节性学习分离：MLP结构捕捉趋势，自适应小波蒸馏实现季节性成分的多分辨率学习，并设计了全面的校正机制确保生成成分的内部一致性。

Result: 在八个真实世界数据集上的实验表明，STDiffusion在时间序列生成任务中达到了最先进的性能，并在多窗口长序列生成中表现出鲁棒性和多功能性。

Conclusion: STDiffusion通过可解释的分解方法有效提升了时间序列生成的质量和可解释性，在多个应用场景中表现出色。

Abstract: Time series generation focuses on modeling the underlying data distribution
and resampling to produce authentic time series data. Key components, such as
trend and seasonality, drive temporal fluctuations, yet many existing
approaches fail to employ interpretative decomposition methods, limiting their
ability to synthesize meaningful trend and seasonal patterns. To address this
gap, we introduce Seasonal-Trend Diffusion (STDiffusion), a novel framework for
multivariate time series generation that integrates diffusion probabilistic
models with advanced learnable series decomposition techniques, enhancing the
interpretability of the generation process. Our approach separates the trend
and seasonal learning into distinct blocks: a Multi-Layer Perceptron (MLP)
structure captures the trend, while adaptive wavelet distillation facilitates
effective multi-resolution learning of seasonal components. This decomposition
improves the interpretability of the model on multiple scales. In addition, we
designed a comprehensive correction mechanism aimed at ensuring that the
generated components exhibit a high degree of internal consistency and preserve
meaningful interrelationships with one another. Our empirical studies on eight
real-world datasets demonstrate that STDiffusion achieves state-of-the-art
performance in time series generation tasks. Furthermore, we extend the model's
application to multi-window long-sequence time series generation, which
delivered reliable results and highlighted its robustness and versatility.

</details>


### [135] [Fast PINN Eigensolvers via Biconvex Reformulation](https://arxiv.org/abs/2511.00792)
*Akshay Sai Banderwaar,Abhishek Gupta*

Main category: cs.LG

TL;DR: 提出了一种基于双凸优化的PINN方法(PINN-ACS)，通过交替凸搜索快速求解特征值问题，比传统梯度训练快500倍


<details>
  <summary>Details</summary>
Motivation: 传统PINN方法求解特征值问题时速度比经典数值方法慢几个数量级，需要更高效的训练方法

Method: 将特征对搜索重新表述为双凸优化问题，使用解析最优更新的交替凸搜索(ACS)算法

Result: PINN-ACS实现了高精度，收敛速度比基于梯度的PINN训练快500倍

Conclusion: 该方法为特征值问题提供了快速、可证明收敛的PINN解决方案

Abstract: Eigenvalue problems have a distinctive forward-inverse structure and are
fundamental to characterizing a system's thermal response, stability, and
natural modes. Physics-Informed Neural Networks (PINNs) offer a mesh-free
alternative for solving such problems but are often orders of magnitude slower
than classical numerical schemes. In this paper, we introduce a reformulated
PINN approach that casts the search for eigenpairs as a biconvex optimization
problem, enabling fast and provably convergent alternating convex search (ACS)
over eigenvalues and eigenfunctions using analytically optimal updates.
Numerical experiments show that PINN-ACS attains high accuracy with convergence
speeds up to 500$\times$ faster than gradient-based PINN training. We release
our codes at https://github.com/NeurIPS-ML4PS-2025/PINN_ACS_CODES.

</details>


### [136] [Efficient Reinforcement Learning for Large Language Models with Intrinsic Exploration](https://arxiv.org/abs/2511.00794)
*Yan Sun,Jia Guo,Stanley Kok,Zihao Wang,Zujie Wen,Zhiqiang Zhang*

Main category: cs.LG

TL;DR: 提出PREPO方法，通过利用内在数据特性提高RLVR的数据效率，包含两个互补组件：使用提示困惑度作为模型适应性指标，以及通过相对熵放大rollout差异来优先探索性序列。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然提升了大型语言模型的推理能力，但训练成本高昂，因为许多rollout对优化的贡献很小。本研究探索如何利用几乎免费的内在数据特性来提高RLVR的数据效率。

Method: PREPO方法包含两个组件：1) 使用提示困惑度作为模型适应性指标，让模型从易到难学习；2) 通过相对熵差异放大rollout间的差异，优先选择探索性更高的序列。

Result: 在Qwen和Llama模型上，PREPO在数学推理基准测试中取得了有效结果，相比基线方法减少了高达3倍的rollout需求，同时保持竞争力性能。

Conclusion: PREPO通过内在数据特性显著提高了RLVR的数据效率，不仅获得了实证收益，还提供了理论分析来解释方法背后的基本原理。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has improved the
reasoning ability of large language models, yet training remains costly because
many rollouts contribute little to optimization, considering the amount of
computation required. This study investigates how simply leveraging intrinsic
data properties, almost free benefit during training, can improve data
efficiency for RLVR. We propose PREPO with two complementary components. First,
we adopt prompt perplexity as an indicator of model adaptability in learning,
enabling the model to progress from well-understood contexts to more
challenging ones. Second, we amplify the discrepancy among the rollouts by
differentiating their relative entropy, and prioritize sequences that exhibit a
higher degree of exploration. Together, these mechanisms reduce rollout demand
while preserving competitive performance. On the Qwen and Llama models, PREPO
achieves effective results on mathematical reasoning benchmarks with up to 3
times fewer rollouts than the baselines. Beyond empirical gains, we provide
theoretical and in-depth analyses explaining the underlying rationale of our
method to improve the data efficiency of RLVR.

</details>


### [137] [Attention Saturation and Gradient Suppression at Inflection Layers: Diagnosing and Mitigating Bottlenecks in Transformer Adaptation](https://arxiv.org/abs/2511.00797)
*Wang Zixian*

Main category: cs.LG

TL;DR: 该论文分析了预训练Transformer在微调时出现的输出饱和和梯度抑制问题，提出了诊断指标来识别拐点层，并通过选择性注入LoRA适配器来恢复被抑制的梯度信号。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在微调时经常表现出对源模式的过度自信，难以形成新的目标域模式，这源于输出饱和导致的梯度抑制问题。

Method: 提出了层间诊断指标（注意力熵、激活梯度范数、参数梯度范数和Delta-CKA），识别拐点层，并选择性注入LoRA适配器来恢复梯度信号。

Result: 实验表明，在过训练初始化下，拐点层LoRA注入能带来性能提升；而在欠训练初始化下，则需要全路径解阻塞来进行低层重构。

Conclusion: 当基础特征强时，解阻塞拐点层有助于高层组合适应；当基础特征弱时，需要全路径解阻塞来进行低层重构。

Abstract: Pre-trained Transformers often exhibit over-confidence in source patterns and
difficulty in forming new target-domain patterns during fine-tuning. We
formalize the mechanism of output saturation leading to gradient suppression
through standard cross-entropy and softmax analysis, showing that gradient
suppression at inflection layers confines adaptation to high-level
recombination of existing features while preventing low-level reconstruction.
We introduce a set of layer-wise diagnostic metrics -- attention entropy
(saturation proxy), activation gradient norm, parameter gradient norm, and
Delta-CKA under a shared PCA basis -- to identify inflection layers
characterized by both low attention entropy and steep gradient decay. Building
on these findings, we propose a diagnose-first, inject-light fine-tuning
strategy: selectively inserting LoRA adapters at inflection layers to restore
suppressed backward signals with minimal parameter overhead. Experiments on
BERT-base transfer from SST-2 to Rotten Tomatoes under under-trained and
over-trained source regimes reveal that over-trained initialization benefits
from inflection-layer LoRA injection, while under-trained initialization
suffers performance degradation. When base features are strong, unblocking
inflection layers facilitates high-level compositional adaptation; when base
features are weak, full-pathway unblocking is required for low-level
reconstruction, as supported by joint analysis of layer-wise activation
gradients and Delta-CKA dynamics.

</details>


### [138] [EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment](https://arxiv.org/abs/2511.00804)
*Abhiram Kusumba,Maitreya Patel,Kyle Min,Changhoon Kim,Chitta Baral,Yezhou Yang*

Main category: cs.LG

TL;DR: EraseFlow是一个基于GFlowNets的概念擦除框架，通过探索去噪轨迹空间来从文本到图像生成器中安全移除有害或专有概念，无需精心设计的奖励模型即可实现高效泛化。


<details>
  <summary>Details</summary>
Motivation: 当前的概念擦除技术存在图像质量下降、依赖脆弱的对抗损失或需要大量重新训练的问题，需要一种更有效的方法来安全移除文本到图像生成器中的有害或专有概念。

Method: 将概念遗忘重新定义为在去噪轨迹空间中的探索问题，使用配备轨迹平衡目标的GFlowNets优化，通过采样整个轨迹而非单一最终状态来学习随机策略。

Result: EraseFlow在广泛实证结果中优于现有基线方法，在性能和先验保持之间实现了最佳权衡，能够有效泛化到未见概念并避免可被攻击的奖励。

Conclusion: EraseFlow框架成功解决了概念擦除的关键挑战，无需精心设计的奖励模型即可实现高效的概念移除，同时保持图像生成质量。

Abstract: Erasing harmful or proprietary concepts from powerful text to image
generators is an emerging safety requirement, yet current "concept erasure"
techniques either collapse image quality, rely on brittle adversarial losses,
or demand prohibitive retraining cycles. We trace these limitations to a myopic
view of the denoising trajectories that govern diffusion based generation. We
introduce EraseFlow, the first framework that casts concept unlearning as
exploration in the space of denoising paths and optimizes it with GFlowNets
equipped with the trajectory balance objective. By sampling entire trajectories
rather than single end states, EraseFlow learns a stochastic policy that steers
generation away from target concepts while preserving the model's prior.
EraseFlow eliminates the need for carefully crafted reward models and by doing
this, it generalizes effectively to unseen concepts and avoids hackable rewards
while improving the performance. Extensive empirical results demonstrate that
EraseFlow outperforms existing baselines and achieves an optimal trade off
between performance and prior preservation.

</details>


### [139] [Logic-informed reinforcement learning for cross-domain optimization of large-scale cyber-physical systems](https://arxiv.org/abs/2511.00806)
*Guangxi Wan,Peng Zeng,Xiaoting Dong,Chunhe Song,Shijie Cui,Dong Li,Qingwei Dong,Yiyang Liu,Hongfei Bai*

Main category: cs.LG

TL;DR: 提出逻辑知情强化学习(LIRL)，通过逻辑投影确保混合动作空间的约束满足，在多个CPS场景中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有方法在CPS混合动作空间优化中存在全局最优性不足和约束保证困难的问题

Method: 在标准策略梯度算法中引入逻辑投影，将潜在动作映射到由一阶逻辑定义的可行混合流形

Result: 在工业制造、电动汽车充电站和交通信号控制等场景中均优于现有方法，在机器人减速器装配系统中使完工时间-能耗目标降低36.47%-44.33%

Conclusion: LIRL为大规模CPS提供了安全实时的优化框架，可无缝迁移到智能交通和智能电网等领域

Abstract: Cyber-physical systems (CPS) require the joint optimization of discrete cyber
actions and continuous physical parameters under stringent safety logic
constraints. However, existing hierarchical approaches often compromise global
optimality, whereas reinforcement learning (RL) in hybrid action spaces often
relies on brittle reward penalties, masking, or shielding and struggles to
guarantee constraint satisfaction. We present logic-informed reinforcement
learning (LIRL), which equips standard policy-gradient algorithms with
projection that maps a low-dimensional latent action onto the admissible hybrid
manifold defined on-the-fly by first-order logic. This guarantees feasibility
of every exploratory step without penalty tuning. Experimental evaluations have
been conducted across multiple scenarios, including industrial manufacturing,
electric vehicle charging stations, and traffic signal control, in all of which
the proposed method outperforms existing hierarchical optimization approaches.
Taking a robotic reducer assembly system in industrial manufacturing as an
example, LIRL achieves a 36.47\% to 44.33\% reduction at most in the combined
makespan-energy objective compared to conventional industrial hierarchical
scheduling methods. Meanwhile, it consistently maintains zero constraint
violations and significantly surpasses state-of-the-art hybrid-action
reinforcement learning baselines. Thanks to its declarative logic-based
constraint formulation, the framework can be seamlessly transferred to other
domains such as smart transportation and smart grid, thereby paving the way for
safe and real-time optimization in large-scale CPS.

</details>


### [140] [Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games](https://arxiv.org/abs/2511.00811)
*Runyu Lu,Peng Zhang,Ruochuan Shi,Yuanheng Zhu,Dongbin Zhao,Yang Liu,Dong Wang,Cesare Alippi*

Main category: cs.LG

TL;DR: 提出了一个均衡策略泛化（EPG）框架，用于在对抗性游戏中学习具有跨图零样本性能的通用策略，特别适用于追逃游戏（PEG）场景。


<details>
  <summary>Details</summary>
Motivation: 传统方法在追逃游戏中需要指数时间求解，且当图结构变化时需要重新计算或微调，限制了实时应用。需要一种能够跨图泛化的均衡策略学习方法。

Method: EPG框架在不同图结构上训练RL策略，针对每个单图的均衡策略。使用动态规划算法生成纯策略纳什均衡，并通过分组机制和序列模型实现策略分解以保证可扩展性。

Result: 实验结果表明，EPG框架在各种未见过的真实世界图中保证了理想的零样本性能，在带出口的图中，通用追逃策略甚至能与最先进方法的微调策略性能相当。

Conclusion: EPG框架成功实现了追逃游戏中跨图结构的策略泛化，为对抗性游戏中的均衡学习提供了有效的解决方案，具有重要的实际应用价值。

Abstract: Equilibrium learning in adversarial games is an important topic widely
examined in the fields of game theory and reinforcement learning (RL).
Pursuit-evasion game (PEG), as an important class of real-world games from the
fields of robotics and security, requires exponential time to be accurately
solved. When the underlying graph structure varies, even the state-of-the-art
RL methods require recomputation or at least fine-tuning, which can be
time-consuming and impair real-time applicability. This paper proposes an
Equilibrium Policy Generalization (EPG) framework to effectively learn a
generalized policy with robust cross-graph zero-shot performance. In the
context of PEGs, our framework is generally applicable to both pursuer and
evader sides in both no-exit and multi-exit scenarios. These two
generalizability properties, to our knowledge, are the first to appear in this
domain. The core idea of the EPG framework is to train an RL policy across
different graph structures against the equilibrium policy for each single
graph. To construct an equilibrium oracle for single-graph policies, we present
a dynamic programming (DP) algorithm that provably generates pure-strategy Nash
equilibrium with near-optimal time complexity. To guarantee scalability with
respect to pursuer number, we further extend DP and RL by designing a grouping
mechanism and a sequence model for joint policy decomposition, respectively.
Experimental results show that, using equilibrium guidance and a distance
feature proposed for cross-graph PEG training, the EPG framework guarantees
desirable zero-shot performance in various unseen real-world graphs. Besides,
when trained under an equilibrium heuristic proposed for the graphs with exits,
our generalized pursuer policy can even match the performance of the fine-tuned
policies from the state-of-the-art PEG methods.

</details>


### [141] [LL-ViT: Edge Deployable Vision Transformers with Look Up Table Neurons](https://arxiv.org/abs/2511.00812)
*Shashank Nag,Alan T. L. Bacellar,Zachary Susskind,Anshul Jha,Logan Liberty,Aishwarya Sivakumar,Eugene B. John,Krishnan Kailas,Priscila M. V. Lima,Neeraja J. Yadwadkar,Felipe M. G. Franca,Lizy K. John*

Main category: cs.LG

TL;DR: LL-ViT是一种面向边缘计算的视觉Transformer优化设计，通过集成LUT神经元层来减少模型大小和计算需求，在保持精度的同时显著提升能效。


<details>
  <summary>Details</summary>
Motivation: 传统视觉Transformer在边缘设备上部署时面临计算、内存和能耗的挑战，而现有的LUT网络在视觉任务上表现不佳，需要设计一种既能保持精度又能高效部署的边缘优化方案。

Method: 提出LL-ViT架构，基于分析发现大部分模型权重和计算来自通道混合器(MLP层)，设计了基于LUT的替代通道混合器，并开发了相应的FPGA加速器，采用神经学习方法原生学习LUT函数。

Result: 在CIFAR-10上达到95.5%准确率，CIFAR-100上78.8%，Tiny-ImageNet上60.9%，与基线Transformer相当；减少60%模型权重和50%乘法运算，能效提升1.9倍，延迟降低1.3倍。

Conclusion: LL-ViT为边缘设备上的视觉Transformer部署提供了高效解决方案，在保持精度的同时显著降低了计算和能耗需求。

Abstract: Vision Transformers have been tremendously successful in computer vision
tasks. However, their large computational, memory, and energy demands are a
challenge for edge inference on FPGAs -- a field that has seen a recent surge
in demand. We recognize the benefits of recent works on logic and Look Up Table
(LUT) based networks, such as LogicNets, NeuraLUT, DWN, among others, in
offering models that simultaneously reduce both the memory and compute
footprints. However, these models natively do not perform well on common vision
tasks, such as CIFAR-10/100. In this work, we propose LL-ViT, a novel edge
optimized vision transformer design that integrates layers of LUT neurons
within the transformer architecture. Based on our characterization that reveals
that a majority of model weights and computations are from the channel mixer
(MLP layer), we design an alternate LUT-based channel mixer, and simultaneously
develop an FPGA-based accelerator for LL-ViT. Contrary to some attempts to
replace each multiplication with a table lookup, our architecture utilizes a
neural learning approach which natively learns the LUT functions. This approach
allows for reduced model sizes, and a computational and energy-efficient
inference solution for vision transformer models. Evaluating on edge-suitable
workloads, we achieve accuracies of 95.5% on CIFAR-10, 78.8% on CIFAR-100, and
60.9% on Tiny-ImageNet datasets, comparable to the baseline transformer. LL-ViT
eliminates over 60% of the model weights and 50% of the multiplications in the
model, and achieves 1.9x energy efficiency and 1.3x lower latency over an
integer quantized ViT accelerator, while also offering superior throughput
against prior works at a 10.9W power budget.

</details>


### [142] [Identifying Slug Formation in Oil Well Pipelines: A Use Case from Industrial Analytics](https://arxiv.org/abs/2511.00851)
*Abhishek Patange,Sharat Chidambaran,Prabhat Shankar,Manjunath G. B.,Anindya Chatterjee*

Main category: cs.LG

TL;DR: 开发了一个用于油气管道段塞流检测的交互式应用，集成了数据探索、模型训练、可视化和实时推理功能，通过用户友好界面实现端到端的检测流程。


<details>
  <summary>Details</summary>
Motivation: 现有段塞流检测方法多为离线、需要专业知识且缺乏实时可解释性，难以满足工业实时监测需求。

Method: 构建包含数据标注、可配置模型训练、时间序列可视化分类结果和实时推理模块的完整系统，支持从CSV上传到实时推断的无缝工作流。

Result: 系统轻量、便携、易于部署，结合领域相关分析和创新的UI/UX功能（如快照持久化、可视化标注、实时警报），作为研究原型和工业应用都具有重要价值。

Conclusion: 该交互式人机协同ML系统弥合了数据科学方法与关键过程工业中实际决策之间的差距，在油气领域之外的时序故障诊断任务中具有更广泛的适用性。

Abstract: Slug formation in oil and gas pipelines poses significant challenges to
operational safety and efficiency, yet existing detection approaches are often
offline, require domain expertise, and lack real-time interpretability. We
present an interactive application that enables end-to-end data-driven slug
detection through a compact and user-friendly interface. The system integrates
data exploration and labeling, configurable model training and evaluation with
multiple classifiers, visualization of classification results with time-series
overlays, and a real-time inference module that generates persistence-based
alerts when slug events are detected. The demo supports seamless workflows from
labeled CSV uploads to live inference on unseen datasets, making it
lightweight, portable, and easily deployable. By combining domain-relevant
analytics with novel UI/UX features such as snapshot persistence, visual
labeling, and real-time alerting, our tool adds significant dissemination value
as both a research prototype and a practical industrial application. The demo
showcases how interactive human-in-the-loop ML systems can bridge the gap
between data science methods and real-world decision-making in critical process
industries, with broader applicability to time-series fault diagnosis tasks
beyond oil and gas.

</details>


### [143] [FlexiCache: Leveraging Temporal Stability of Attention Heads for Efficient KV Cache Management](https://arxiv.org/abs/2511.00868)
*Nazmul Takbir,Hamidreza Alikhani,Nikil Dutt,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: FlexiCache是一个基于KV头时间稳定性的分层KV缓存管理系统，通过将KV头分类为稳定和不稳定两类，显著减少GPU内存使用和计算开销，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型服务受限于KV缓存的增长，现有系统难以在不降低精度的情况下高效利用注意力集中在少数关键token的特性，特别是在长文本生成场景中。

Method: 基于KV头时间稳定性的观察，FlexiCache将KV头分类为稳定和不稳定：不稳定头的所有KV缓存页保留在GPU内存中，稳定头只保留前K页在GPU上，其余卸载到主机内存，并利用时间稳定性进行周期性重排来获取新提升的top页。

Result: 在vLLM上实现，FlexiCache将长上下文请求的GPU内存占用减少高达70%，离线服务吞吐量提升1.38-1.55倍，在线token延迟降低1.6-2.1倍，同时在长上下文、长生成场景中保持精度。

Conclusion: FlexiCache通过利用KV头的时间稳定性特性，有效解决了KV缓存的内存和计算瓶颈，为大语言模型的长文本服务提供了高效解决方案。

Abstract: Large Language Model (LLM) serving is increasingly constrained by the growing
size of the key-value (KV) cache, which scales with both context length and
generation length. Prior work shows that attention is dominated by a small
subset of critical tokens, yet existing systems struggle to exploit this
efficiently without degrading accuracy, especially in long generation. We make
a key observation: the temporal stability of these critical tokens varies
significantly across KV heads: some heads consistently focus on the same
tokens, while others shift frequently. Building on this insight, we introduce
FlexiCache, a hierarchical KV-cache management system that leverages the
temporal stability of KV heads to reduce GPU memory usage and computation
overhead, while preserving model accuracy. FlexiCache classifies KV heads as
stable or unstable: it retains all KV-cache pages from unstable heads in GPU
memory, whereas for stable heads, it keeps only the top-K pages on the GPU and
offloads the rest to host memory. By exploiting temporal stability, FlexiCache
performs periodic reranking for stable heads to fetch newly promoted top pages.
Implemented atop vLLM, FlexiCache reduces GPU memory footprint for long-context
requests by up to 70%, improves offline serving throughput by 1.38-1.55x, and
lowers online token latency by 1.6-2.1x, all while maintaining accuracy in
long-context, long-generation scenarios.

</details>


### [144] [Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding](https://arxiv.org/abs/2511.00874)
*Taowen Liu,Marta Andronic,Deniz Gündüz,George A. Constantinides*

Main category: cs.LG

TL;DR: 研究表明增加批量大小可以补偿反向传播中的精度损失，量化权重和激活对梯度方差有不同影响


<details>
  <summary>Details</summary>
Motivation: 量化训练虽然提高计算和内存效率，但引入量化噪声会阻碍收敛和降低模型精度，随机舍入与批量大小等训练因素的交互作用尚未充分探索

Method: 对带有随机舍入的mini-batch随机梯度下降进行理论和实证研究，分析量化权重和激活对梯度方差的影响

Result: 实验验证了理论见解，表明增加批量大小可以有效补偿反向传播中的精度损失

Conclusion: 通过适当调整批量大小，可以在量化训练中保持模型性能，为高效LLM训练提供了新思路

Abstract: LLM training is resource-intensive. Quantized training improves computational
and memory efficiency but introduces quantization noise, which can hinder
convergence and degrade model accuracy. Stochastic Rounding (SR) has emerged as
a theoretically attractive alternative to deterministic rounding, offering
unbiased gradient estimates. However, its interaction with other training
factors -- especially batch size -- remains under explored. In this paper, we
present a theoretical and empirical study of mini-batch stochastic gradient
descent (SGD) with SR, showing that increased batch sizes can compensate for
reduced precision during back-propagation. Furthermore, we show that quantizing
weights and activations impacts gradient variance in distinct ways. Our
experiments validate these theoretical insights.

</details>


### [145] [KFCPO: Kronecker-Factored Approximated Constrained Policy Optimization](https://arxiv.org/abs/2511.00880)
*Joonyoung Lim,Younghwan Yoo*

Main category: cs.LG

TL;DR: KFCPO是一种新颖的安全强化学习算法，结合了基于Kronecker分解近似曲率的二阶策略优化和安全感知的梯度操作，在保持安全约束的同时实现了更高的平均回报。


<details>
  <summary>Details</summary>
Motivation: 解决传统安全强化学习算法在奖励最大化和约束满足之间的权衡问题，避免固定硬阈值导致的性能下降和不稳定策略变化。

Method: 使用K-FAC高效近似Fisher信息矩阵进行自然梯度更新；引入边界感知梯度操作机制，根据智能体与安全边界的距离自适应调整奖励和成本梯度的影响；采用小批量级别的KL回滚策略确保信任区域合规。

Result: 在Safety Gymnasium环境中的实验表明，KFCPO相比最佳基线算法，平均回报提高了10.3%到50.2%，同时满足安全约束。

Conclusion: KFCPO通过二阶优化和自适应梯度操作，在安全强化学习中实现了安全性和性能的优越平衡。

Abstract: We propose KFCPO, a novel Safe Reinforcement Learning (Safe RL) algorithm
that combines scalable Kronecker-Factored Approximate Curvature (K-FAC) based
second-order policy optimization with safety-aware gradient manipulation. KFCPO
leverages K-FAC to perform efficient and stable natural gradient updates by
approximating the Fisher Information Matrix (FIM) in a layerwise, closed form
manner, avoiding iterative approximation overheads. To address the tradeoff
between reward maximization and constraint satisfaction, we introduce a margin
aware gradient manipulation mechanism that adaptively adjusts the influence of
reward and cost gradients based on the agent's proximity to safety boundaries.
This method blends gradients using a direction sensitive projection,
eliminating harmful interference and avoiding abrupt changes caused by fixed
hard thresholds. Additionally, a minibatch level KL rollback strategy is
adopted to ensure trust region compliance and to prevent destabilizing policy
shifts. Experiments on Safety Gymnasium using OmniSafe show that KFCPO achieves
10.3% to 50.2% higher average return across environments compared to the best
baseline that respected the safety constraint, demonstrating superior balance
of safety and performance.

</details>


### [146] [SpEx: A Spectral Approach to Explainable Clustering](https://arxiv.org/abs/2511.00885)
*Tal Argov,Tal Wagner*

Main category: cs.LG

TL;DR: 提出了一种基于谱图划分的可解释聚类新方法，能够为任何给定的非可解释聚类或数据集构建解释树。


<details>
  <summary>Details</summary>
Motivation: 现有可解释聚类方法局限于特定聚类目标，缺乏通用的方法来为任意聚类构建解释树。

Method: 基于谱图划分的可解释聚类算法，使用Trevisan(2013)的广义框架，同时在两个图上优化分割。

Result: 实验表明该方法在多个数据集上优于基线方法。

Conclusion: 谱图划分为可解释聚类提供了通用且有效的方法框架。

Abstract: Explainable clustering by axis-aligned decision trees was introduced by
Moshkovitz et al. (2020) and has gained considerable interest. Prior work has
focused on minimizing the price of explainability for specific clustering
objectives, lacking a general method to fit an explanation tree to any given
clustering, without restrictions. In this work, we propose a new and generic
approach to explainable clustering, based on spectral graph partitioning. With
it, we design an explainable clustering algorithm that can fit an explanation
tree to any given non-explainable clustering, or directly to the dataset
itself. Moreover, we show that prior algorithms can also be interpreted as
graph partitioning, through a generalized framework due to Trevisan (2013)
wherein cuts are optimized in two graphs simultaneously. Our experiments show
the favorable performance of our method compared to baselines on a range of
datasets.

</details>


### [147] [Learning with Category-Equivariant Representations for Human Activity Recognition](https://arxiv.org/abs/2511.00900)
*Yoshihiro Maruyama*

Main category: cs.LG

TL;DR: 提出了一种基于范畴对称性感知的学习框架，通过将时间、尺度和传感器层次结构的变化因素融入特征表示结构，使模型在现实扭曲下保持稳定，在UCI人类活动识别基准上显著提升了分布外准确率。


<details>
  <summary>Details</summary>
Motivation: 人类活动识别面临传感器信号随上下文、运动和环境变化的挑战，需要模型在世界变化时保持稳定性。

Method: 构建范畴对称性感知学习框架，将时间、尺度和传感器层次结构的变化因素融入特征表示的结构中，使模型自动保持传感器间关系并在时间偏移、幅度漂移和设备方向变化等现实扭曲下保持稳定。

Result: 在UCI人类活动识别基准上，该设计将分布外准确率提高了约46个百分点（约3.6倍于基线）。

Conclusion: 抽象对称性原理可以通过范畴等变表示理论转化为日常感知任务中的具体性能提升。

Abstract: Human activity recognition is challenging because sensor signals shift with
context, motion, and environment; effective models must therefore remain stable
as the world around them changes. We introduce a categorical symmetry-aware
learning framework that captures how signals vary over time, scale, and sensor
hierarchy. We build these factors into the structure of feature
representations, yielding models that automatically preserve the relationships
between sensors and remain stable under realistic distortions such as time
shifts, amplitude drift, and device orientation changes. On the UCI Human
Activity Recognition benchmark, this categorical symmetry-driven design
improves out-of-distribution accuracy by approx. 46 percentage points (approx.
3.6x over the baseline), demonstrating that abstract symmetry principles can
translate into concrete performance gains in everyday sensing tasks via
category-equivariant representation theory.

</details>


### [148] [Transformers as Intrinsic Optimizers: Forward Inference through the Energy Principle](https://arxiv.org/abs/2511.00907)
*Ruifeng Ren,Sheng Ouyang,Huayi Tang,Yong Liu*

Main category: cs.LG

TL;DR: 本文提出了一个基于能量的统一框架来理解Transformer注意力机制，将标准softmax注意力视为Helmholtz自由能最小化的特例，并基于经典梯度下降算法提出了新的注意力结构变体。


<details>
  <summary>Details</summary>
Motivation: Transformer模型虽然表现出强大的适应性，但其内在机制仍需深入探索。能量视角长期以来为理解神经计算提供了有价值的原则，本文旨在通过能量视角重新审视基于注意力的Transformer模型。

Method: 提出了一个统一的基于能量框架，包含三个关键组件：全局能量F*、能量函数Ei和梯度下降形式。在该框架下，标准softmax注意力可视为使用标准梯度下降最小化Helmholtz自由能的特例。还扩展了多头设置，并基于经典GD算法提出了动量GD、NAG和牛顿法变体的新注意力结构。

Result: 实验初步支持了基于能量框架设计注意力机制的潜力。

Conclusion: 能量视角为理解Transformer注意力机制提供了统一的理论框架，并能自然地扩展到线性注意力，同时启发了基于经典优化算法的新型注意力结构设计。

Abstract: Transformers have demonstrated strong adaptability across a wide range of
tasks and have become the backbone of modern Large Language Models (LLMs).
However, their underlying mechanisms remain open for further exploration. The
energy-based perspective has long provided a valuable principle for
understanding neural computation. In this paper, we revisit the principle of
energy as a lens to understand attention-based Transformer models. We present a
unified energy-based framework which is composed of three key components: the
global energy $F^*$, the energy function $E_i$ and the employed gradient
descent (GD) form. Within this framework, standard softmax attention can be
viewed as a special case of minimizing the Helmholtz free energy as $F^*$ using
standard GD when $E_i$ takes the form of elastic potential energy, with
residual connections ensuring that this optimization proceeds in an incremental
manner. In addition, linear attentions can also be naturally incorporated into
this framework by adjusting the corresponding energy forms. We also extend the
above analysis to the multi-head setting, where the energy is defined across
multiple low-dimensional subspaces. Building on this framework, we propose
energy-based modifications of attention structures. Inspired by classical GD
algorithms, we extend the original attention formulation based on standard GD
to the momentum-based GD, Nesterov Accelerated Gradient (NAG), and Newton's
method variants, each inducing a corresponding new attention structure. Our
experiments provide preliminary support for the potential of the energy-based
framework for designing attention mechanisms.

</details>


### [149] [Motion-Robust Multimodal Fusion of PPG and Accelerometer Signals for Three-Class Heart Rhythm Classification](https://arxiv.org/abs/2511.00949)
*Yangyang Zhao,Matti Kaisti,Olli Lahdenoja,Tero Koivisto*

Main category: cs.LG

TL;DR: RhythmiNet是一种结合PPG和加速度计信号的多模态神经网络，用于三分类心律检测（房颤、窦性心律、其他），在嘈杂的临床数据中表现优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 腕戴式PPG设备在心律监测中易受运动伪影和生理噪声影响，现有方法多局限于单通道PPG和二元房颤检测，无法捕捉临床中更广泛的心律失常类型。

Method: 提出RhythmiNet，一种带有时间和通道注意力模块的残差神经网络，联合利用PPG和加速度计信号进行三分类心律检测，并在不同运动强度条件下评估模型鲁棒性。

Result: RhythmiNet相比仅使用PPG的基线方法在宏观AUC上提升了4.3%，比基于手工HRV特征逻辑回归模型性能提升12%，显示了多模态融合和注意力学习的优势。

Conclusion: 多模态融合和注意力机制能有效提升在嘈杂真实临床数据中的心律分类性能，RhythmiNet为可穿戴设备的心律监测提供了更鲁棒的解决方案。

Abstract: Atrial fibrillation (AF) is a leading cause of stroke and mortality,
particularly in elderly patients. Wrist-worn photoplethysmography (PPG) enables
non-invasive, continuous rhythm monitoring, yet suffers from significant
vulnerability to motion artifacts and physiological noise. Many existing
approaches rely solely on single-channel PPG and are limited to binary AF
detection, often failing to capture the broader range of arrhythmias
encountered in clinical settings. We introduce RhythmiNet, a residual neural
network enhanced with temporal and channel attention modules that jointly
leverage PPG and accelerometer (ACC) signals. The model performs three-class
rhythm classification: AF, sinus rhythm (SR), and Other. To assess robustness
across varying movement conditions, test data are stratified by
accelerometer-based motion intensity percentiles without excluding any
segments. RhythmiNet achieved a 4.3% improvement in macro-AUC over the PPG-only
baseline. In addition, performance surpassed a logistic regression model based
on handcrafted HRV features by 12%, highlighting the benefit of multimodal
fusion and attention-based learning in noisy, real-world clinical data.

</details>


### [150] [Using Synthetic Data to estimate the True Error is theoretically and practically doable](https://arxiv.org/abs/2511.00964)
*Hai Hoang Thanh,Duy-Tung Nguyen,Hung The Tran,Khoat Than*

Main category: cs.LG

TL;DR: 本文提出了一种使用合成数据来估计训练模型测试误差的新方法，解决了在有限标记数据条件下模型评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，获取大量标记测试数据成本高昂且耗时，传统评估方法在有限标记数据条件下不可靠。

Method: 开发了考虑合成数据的泛化边界理论，提出了基于理论指导的优化合成数据生成方法。

Result: 在模拟和表格数据集上的实验表明，相比现有基线方法，该方法能够获得更准确可靠的测试误差估计。

Conclusion: 合成数据可以有效用于模型评估，生成器质量对评估准确性至关重要，理论指导的优化方法显著提升了评估可靠性。

Abstract: Accurately evaluating model performance is crucial for deploying machine
learning systems in real-world applications. Traditional methods often require
a sufficiently large labeled test set to ensure a reliable evaluation. However,
in many contexts, a large labeled dataset is costly and labor-intensive.
Therefore, we sometimes have to do evaluation by a few labeled samples, which
is theoretically challenging. Recent advances in generative models offer a
promising alternative by enabling the synthesis of high-quality data. In this
work, we make a systematic investigation about the use of synthetic data to
estimate the test error of a trained model under limited labeled data
conditions. To this end, we develop novel generalization bounds that take
synthetic data into account. Those bounds suggest novel ways to optimize
synthetic samples for evaluation and theoretically reveal the significant role
of the generator's quality. Inspired by those bounds, we propose a
theoretically grounded method to generate optimized synthetic data for model
evaluation. Experimental results on simulation and tabular datasets demonstrate
that, compared to existing baselines, our method achieves accurate and more
reliable estimates of the test error.

</details>


### [151] [Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow](https://arxiv.org/abs/2511.00977)
*Kristiyan Sakalyan,Alessandro Palma,Filippo Guerranti,Fabian J. Theis,Stephan Günnemann*

Main category: cs.LG

TL;DR: NicheFlow是一个基于流的生成模型，用于推断连续空间切片中细胞微环境的时序轨迹，通过将局部细胞邻域表示为点云，结合最优传输和变分流匹配来联合建模细胞状态和空间坐标的演化。


<details>
  <summary>Details</summary>
Motivation: 理解细胞微环境在时空数据中的演化对于解析组织发育和疾病进展至关重要。现有的单细胞水平建模方法忽视了组织中细胞状态的协调发育。

Method: 将局部细胞邻域表示为点云，使用最优传输和变分流匹配联合建模细胞状态和空间坐标的演化。

Result: 该方法成功恢复了从胚胎到大脑发育等不同时空数据集中的全局空间结构和局部微环境组成。

Conclusion: NicheFlow能够有效推断细胞微环境的时空演化轨迹，为理解组织发育提供了新工具。

Abstract: Understanding the evolution of cellular microenvironments in spatiotemporal
data is essential for deciphering tissue development and disease progression.
While experimental techniques like spatial transcriptomics now enable
high-resolution mapping of tissue organization across space and time, current
methods that model cellular evolution operate at the single-cell level,
overlooking the coordinated development of cellular states in a tissue. We
introduce NicheFlow, a flow-based generative model that infers the temporal
trajectory of cellular microenvironments across sequential spatial slides. By
representing local cell neighborhoods as point clouds, NicheFlow jointly models
the evolution of cell states and spatial coordinates using optimal transport
and Variational Flow Matching. Our approach successfully recovers both global
spatial architecture and local microenvironment composition across diverse
spatiotemporal datasets, from embryonic to brain development.

</details>


### [152] [Balanced Multimodal Learning via Mutual Information](https://arxiv.org/abs/2511.00987)
*Rongrong Xie,Guido Sanguinetti*

Main category: cs.LG

TL;DR: 提出了一种解决多模态学习中模态不平衡问题的新框架，通过互信息量化模态间交互，采用跨模态知识蒸馏和多任务式训练来平衡不同模态的贡献。


<details>
  <summary>Details</summary>
Motivation: 多模态学习面临模态不平衡问题，特别是在生物数据分析中，数据集有限、获取成本高且质量不均。传统方法难以同时利用模态间协同效应并有效解决模态冲突。

Method: 采用两阶段平衡多模态学习策略：1) 跨模态知识蒸馏预训练，用强模态增强弱模态预测能力；2) 多任务式训练，基于模态性能指标和互信息动态校准梯度贡献。

Result: 该方法有效缓解了模态不平衡问题，显著提升了多模态模型的整体性能。

Conclusion: 所提出的统一框架通过互信息量化和平衡学习策略，成功解决了多模态学习中的模态不平衡挑战，为生物数据等多模态应用提供了有效解决方案。

Abstract: Multimodal learning has increasingly become a focal point in research,
primarily due to its ability to integrate complementary information from
diverse modalities. Nevertheless, modality imbalance, stemming from factors
such as insufficient data acquisition and disparities in data quality, has
often been inadequately addressed. This issue is particularly prominent in
biological data analysis, where datasets are frequently limited, costly to
acquire, and inherently heterogeneous in quality. Conventional multimodal
methodologies typically fall short in concurrently harnessing intermodal
synergies and effectively resolving modality conflicts.
  In this study, we propose a novel unified framework explicitly designed to
address modality imbalance by utilizing mutual information to quantify
interactions between modalities. Our approach adopts a balanced multimodal
learning strategy comprising two key stages: cross-modal knowledge distillation
(KD) and a multitask-like training paradigm. During the cross-modal KD
pretraining phase, stronger modalities are leveraged to enhance the predictive
capabilities of weaker modalities. Subsequently, our primary training phase
employs a multitask-like learning mechanism, dynamically calibrating gradient
contributions based on modality-specific performance metrics and intermodal
mutual information. This approach effectively alleviates modality imbalance,
thereby significantly improving overall multimodal model performance.

</details>


### [153] [Hydra: Dual Exponentiated Memory for Multivariate Time Series Analysis](https://arxiv.org/abs/2511.00989)
*Asal Meskin,Alireza Mirrokni,Ali Najar,Ali Behrouz*

Main category: cs.LG

TL;DR: Hydra是一个双头元上下文记忆模块，通过二维循环在时间和变量维度上学习记忆模式，解决了现有时间序列模型缺乏时间归纳偏置、忽略变量间依赖关系以及长序列建模效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列模型（如Transformer、MLP、线性模型）存在三个主要问题：(1) 缺乏时间归纳偏置，难以捕捉时间动态；(2) 为单变量设计，忽略时间和变量维度的相互依赖；(3) 长序列建模效率低。线性RNN虽然解决了部分问题，但仅限于单序列且会传播误差。

Method: Hydra采用双头元上下文记忆模块，通过二维循环在时间和变量维度上学习如何记忆数据中更具信息量的模式。虽然二维循环训练不可并行，但提出了2D分块训练算法，在保持效果的同时提升10倍效率。

Result: 在时间序列预测、分类和异常检测等多个任务和数据集上的实验表明，Hydra相比最先进的基线方法表现出优越性能。

Conclusion: Hydra通过二维循环设计有效解决了现有时间序列模型的局限性，在保持高效性的同时显著提升了多变量时间序列建模的性能。

Abstract: In recent years, effectively modeling multivariate time series has gained
significant popularity, mainly due to its wide range of applications, ranging
from healthcare to financial markets and energy management. Transformers, MLPs,
and linear models as the de facto backbones of modern time series models have
shown promising results in single-variant and/or short-term forecasting. These
models, however: (1) are permutation equivariant and so lack temporal inductive
bias, being less expressive to capture the temporal dynamics; (2) are naturally
designed for univariate setup, missing the inter-dependencies of temporal and
variate dimensions; and/or (3) are inefficient for Long-term time series
modeling. To overcome training and inference efficiency as well as the lack of
temporal inductive bias, recently, linear Recurrent Neural Networks (RNNs) have
gained attention as an alternative to Transformer-based models. These models,
however, are inherently limited to a single sequence, missing inter-variate
dependencies, and can propagate errors due to their additive nature. In this
paper, we present Hydra, a by-design two-headed meta in-context memory module
that learns how to memorize patterns at test time by prioritizing time series
patterns that are more informative about the data. Hydra uses a 2-dimensional
recurrence across both time and variate at each step, which is more powerful
than mixing methods. Although the 2-dimensional nature of the model makes its
training recurrent and non-parallelizable, we present a new 2D-chunk-wise
training algorithm that approximates the actual recurrence with $\times 10$
efficiency improvement, while maintaining the effectiveness. Our experimental
results on a diverse set of tasks and datasets, including time series
forecasting, classification, and anomaly detection show the superior
performance of Hydra compared to state-of-the-art baselines.

</details>


### [154] [None To Optima in Few Shots: Bayesian Optimization with MDP Priors](https://arxiv.org/abs/2511.01006)
*Diantong Li,Kyunghyun Cho,Chong Liu*

Main category: cs.LG

TL;DR: 提出了ProfBO算法，通过MDP先验和元学习实现少量评估下的高效黑盒优化


<details>
  <summary>Details</summary>
Motivation: 贝叶斯优化在现实应用中评估成本高昂，需要减少评估次数

Method: 使用MDP先验建模优化轨迹，结合先验拟合神经网络和模型无关元学习

Result: 在Covid、癌症基准和超参数调优任务中显著优于现有方法

Conclusion: ProfBO能以更少评估获得高质量解，适合实际部署

Abstract: Bayesian Optimization (BO) is an efficient tool for optimizing black-box
functions, but its theoretical guarantees typically hold in the asymptotic
regime. In many critical real-world applications such as drug discovery or
materials design, where each evaluation can be very costly and time-consuming,
BO becomes impractical for many evaluations. In this paper, we introduce the
Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization
with remarkably few function evaluations. At the heart of our algorithmic
design are Markov Decision Process (MDP) priors that model optimization
trajectories from related source tasks, thereby capturing procedural knowledge
on efficient optimization. We embed these MDP priors into a prior-fitted neural
network and employ model-agnostic meta-learning for fast adaptation to new
target tasks. Experiments on real-world Covid and Cancer benchmarks and
hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms
state-of-the-art methods by achieving high-quality solutions with significantly
fewer evaluations, making it ready for practical deployment.

</details>


### [155] [Equality Graph Assisted Symbolic Regression](https://arxiv.org/abs/2511.01009)
*Fabricio Olivetti de Franca,Gabriel Kronberger*

Main category: cs.LG

TL;DR: 提出SymRegg算法，利用等式图(e-graph)结构避免符号回归中冗余计算，提高搜索效率


<details>
  <summary>Details</summary>
Motivation: 传统遗传编程在符号回归中存在大量冗余计算（可达总评估次数的60%），需要更高效的方法

Method: 基于e-graph结构，从图中采样表达式进行扰动，若生成新表达式则插入图中并生成等价形式

Result: SymRegg提高了搜索效率，在不同数据集上保持准确结果，且只需极简超参数设置

Conclusion: e-graph结构能有效避免符号回归中的冗余计算，SymRegg算法在效率和准确性上表现良好

Abstract: In Symbolic Regression (SR), Genetic Programming (GP) is a popular search
algorithm that delivers state-of-the-art results in term of accuracy. Its
success relies on the concept of neutrality, which induces large plateaus that
the search can safely navigate to more promising regions. Navigating these
plateaus, while necessary, requires the computation of redundant expressions,
up to 60% of the total number of evaluation, as noted in a recent study. The
equality graph (e-graph) structure can compactly store and group equivalent
expressions enabling us to verify if a given expression and their variations
were already visited by the search, thus enabling us to avoid unnecessary
computation. We propose a new search algorithm for symbolic regression called
SymRegg that revolves around the e-graph structure following simple steps:
perturb solutions sampled from a selection of expressions stored in the
e-graph, if it generates an unvisited expression, insert it into the e-graph
and generates its equivalent forms. We show that SymRegg is capable of
improving the efficiency of the search, maintaining consistently accurate
results across different datasets while requiring a choice of a minimalist set
of hyperparameters.

</details>


### [156] [What's the next frontier for Data-centric AI? Data Savvy Agents](https://arxiv.org/abs/2511.01015)
*Nabeel Seedat,Jiashuo Liu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文主张在AI智能体设计中应将数据处理能力作为首要考虑因素，提出了四个关键能力：主动数据获取、复杂数据处理、交互式测试数据合成和持续适应。


<details>
  <summary>Details</summary>
Motivation: 当前AI智能体在自主通信、协作和工具使用方面取得进展，但数据处理能力被忽视。可扩展的自主性需要智能体能够持续获取、处理和发展数据，以确保可靠的现实世界部署。

Method: 提出四个关键能力框架：(1)主动数据获取：自主收集任务关键知识或征求人类输入填补数据空白；(2)复杂数据处理：上下文感知和灵活处理多样化数据挑战；(3)交互式测试数据合成：从静态基准转向动态生成的交互式测试数据；(4)持续适应：迭代优化数据和背景知识以适应环境变化。

Result: 构建了一个数据感知智能体的理论框架，强调数据处理能力应成为智能体设计的核心要素。

Conclusion: 当前智能体研究主要关注推理能力，但数据感知智能体应成为以数据为中心AI的下一个前沿领域，需要重新思考智能体设计中数据能力的重要性。

Abstract: The recent surge in AI agents that autonomously communicate, collaborate with
humans and use diverse tools has unlocked promising opportunities in various
real-world settings. However, a vital aspect remains underexplored: how agents
handle data. Scalable autonomy demands agents that continuously acquire,
process, and evolve their data. In this paper, we argue that data-savvy
capabilities should be a top priority in the design of agentic systems to
ensure reliable real-world deployment. Specifically, we propose four key
capabilities to realize this vision: (1) Proactive data acquisition: enabling
agents to autonomously gather task-critical knowledge or solicit human input to
address data gaps; (2) Sophisticated data processing: requiring context-aware
and flexible handling of diverse data challenges and inputs; (3) Interactive
test data synthesis: shifting from static benchmarks to dynamically generated
interactive test data for agent evaluation; and (4) Continual adaptation:
empowering agents to iteratively refine their data and background knowledge to
adapt to shifting environments. While current agent research predominantly
emphasizes reasoning, we hope to inspire a reflection on the role of data-savvy
agents as the next frontier in data-centric AI.

</details>


### [157] [SARIMAX-Based Power Outage Prediction During Extreme Weather Events](https://arxiv.org/abs/2511.01017)
*Haoran Ye,Qiuzhuang Sun,Yang Yang*

Main category: cs.LG

TL;DR: 开发基于SARIMAX的短期停电预测系统，用于极端天气事件期间24-48小时的停电预测，通过特征工程和稳健优化策略，相比基线方法RMSE提升8.4%。


<details>
  <summary>Details</summary>
Motivation: 极端天气事件导致电力中断频发，需要准确的短期停电预测来支持应急响应和资源调配。

Method: 采用两阶段特征工程（数据清洗+相关性过滤），结合时间嵌入、多尺度滞后特征和天气变量作为外生输入，使用SARIMAX模型，并实施标准化、分层拟合策略和回退机制。

Result: 模型在24小时和48小时预测范围内达到RMSE 177.2，相比基线方法（RMSE 193.4）提升8.4%。

Conclusion: 特征工程和稳健优化策略能有效提升极端天气相关停电预测的准确性。

Abstract: This study develops a SARIMAX-based prediction system for short-term power
outage forecasting during extreme weather events. Using hourly data from
Michigan counties with outage counts and comprehensive weather features, we
implement a systematic two-stage feature engineering pipeline: data cleaning to
remove zero-variance and unknown features, followed by correlation-based
filtering to eliminate highly correlated predictors. The selected features are
augmented with temporal embeddings, multi-scale lag features, and weather
variables with their corresponding lags as exogenous inputs to the SARIMAX
model. To address data irregularity and numerical instability, we apply
standardization and implement a hierarchical fitting strategy with sequential
optimization methods, automatic downgrading to ARIMA when convergence fails,
and historical mean-based fallback predictions as a final safeguard. The model
is optimized separately for short-term (24 hours) and medium-term (48 hours)
forecast horizons using RMSE as the evaluation metric. Our approach achieves an
RMSE of 177.2, representing an 8.4\% improvement over the baseline method (RMSE
= 193.4), thereby validating the effectiveness of our feature engineering and
robust optimization strategy for extreme weather-related outage prediction.

</details>


### [158] [MedEqualizer: A Framework Investigating Bias in Synthetic Medical Data and Mitigation via Augmentation](https://arxiv.org/abs/2511.01054)
*Sama Salarian,Yue Zhang,Swati Padhee,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 评估GAN模型在MIMIC-III数据集上生成合成医疗数据的公平性，发现存在人口统计属性不平衡问题，并提出了MedEqualizer框架来改善代表性不足的子群体。


<details>
  <summary>Details</summary>
Motivation: 合成医疗数据生成可以增强数据可访问性，但需要确保在受保护属性上的公平性，避免在临床研究和决策中产生偏见结果。

Method: 使用对数差异度量评估子群体代表性，发现合成数据中存在显著不平衡，然后提出MedEqualizer模型无关的增强框架，在合成数据生成前丰富代表性不足的子群体。

Result: 观察到许多子群体在合成数据中代表性不足或过度代表，MedEqualizer显著改善了合成数据集中的人口统计平衡。

Conclusion: MedEqualizer为更公平和具有代表性的医疗数据合成提供了可行路径。

Abstract: Synthetic healthcare data generation presents a viable approach to enhance
data accessibility and support research by overcoming limitations associated
with real-world medical datasets. However, ensuring fairness across protected
attributes in synthetic data is critical to avoid biased or misleading results
in clinical research and decision-making. In this study, we assess the fairness
of synthetic data generated by multiple generative adversarial network
(GAN)-based models using the MIMIC-III dataset, with a focus on
representativeness across protected demographic attributes. We measure subgroup
representation using the logarithmic disparity metric and observe significant
imbalances, with many subgroups either underrepresented or overrepresented in
the synthetic data, compared to the real data. To mitigate these disparities,
we introduce MedEqualizer, a model-agnostic augmentation framework that
enriches the underrepresented subgroups prior to synthetic data generation. Our
results show that MedEqualizer significantly improves demographic balance in
the resulting synthetic datasets, offering a viable path towards more equitable
and representative healthcare data synthesis.

</details>


### [159] [Window-Based Feature Engineering for Cognitive Workload Detection](https://arxiv.org/abs/2511.01060)
*Andrew Hallam,R G Gayathri,Glory Lee,Atul Sajjanhar*

Main category: cs.LG

TL;DR: 使用COLET数据集进行认知负荷分类研究，采用基于窗口的特征生成方法和机器学习/深度学习技术，发现深度学习模型特别是表格架构在各项指标上优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 认知负荷在健康、心理学和国防应用等领域日益受到关注，需要开发有效的实时评估方法。

Method: 采用基于窗口的时间分割方法来增强特征，然后使用机器学习和深度学习模型对不同认知负荷水平进行分类。

Result: 深度学习模型，特别是表格架构，在精确度、F1分数、准确率和分类精度方面都优于传统机器学习方法。

Conclusion: 基于窗口的时间特征提取方法有效，深度学习技术在复杂动态任务的实时认知负荷评估中具有潜力。

Abstract: Cognitive workload is a topic of increasing interest across various fields
such as health, psychology, and defense applications. In this research, we
focus on classifying cognitive workload using the COLET dataset, employing a
window-based approach for feature generation and machine/deep learning
techniques for classification. We apply window-based temporal partitioning to
enhance features used in existing research, followed by machine learning and
deep learning models to classify different levels of cognitive workload. The
results demonstrate that deep learning models, particularly tabular
architectures, outperformed traditional machine learning methods in precision,
F1-score, accuracy, and classification precision. This study highlights the
effectiveness of window-based temporal feature extraction and the potential of
deep learning techniques for real-time cognitive workload assessment in complex
and dynamic tasks.

</details>


### [160] [Energy-Efficient Deep Learning Without Backpropagation: A Rigorous Evaluation of Forward-Only Algorithms](https://arxiv.org/abs/2511.01061)
*Przemysław Spyra,Witold Dzwinel*

Main category: cs.LG

TL;DR: 该论文挑战了反向传播(BP)对高性能至关重要的传统假设，证明无反向传播的Mono-Forward(MF)算法在MLP架构上不仅分类精度超过BP，还能节省41%能耗和34%训练时间。


<details>
  <summary>Details</summary>
Motivation: 质疑反向传播在深度学习中的必要性，探索更高效、可持续的替代算法。

Method: 从Hinton的Forward-Forward算法演进到Cascaded Forward，最终提出Mono-Forward算法，在相同架构和超参数优化框架下进行公平比较。

Result: MF算法在MLP上分类精度优于BP基线，能耗降低41%，训练速度提升34%。

Conclusion: MF算法是MLP中实用、高性能且可持续的BP替代方案。

Abstract: The long-held assumption that backpropagation (BP) is essential for
state-of-the-art performance is challenged by this work. We present rigorous,
hardware-validated evidence that the Mono-Forward (MF) algorithm, a
backpropagation-free method, consistently surpasses an optimally tuned BP
baseline in classification accuracy on its native Multi-Layer Perceptron (MLP)
architectures. This superior generalization is achieved with profound
efficiency gains, including up to 41% less energy consumption and up to 34%
faster training. Our analysis, which charts an evolutionary path from Geoffrey
Hinton's Forward-Forward (FF) to the Cascaded Forward (CaFo) and finally to MF,
is grounded in a fair comparative framework using identical architectures and
universal hyperparameter optimization. We further provide a critical
re-evaluation of memory efficiency in BP-free methods, empirically
demonstrating that practical overhead can offset theoretical gains. Ultimately,
this work establishes MF as a practical, high-performance, and sustainable
alternative to BP for MLPs.

</details>


### [161] [AI Progress Should Be Measured by Capability-Per-Resource, Not Scale Alone: A Framework for Gradient-Guided Resource Allocation in LLMs](https://arxiv.org/abs/2511.01077)
*David McCoy,Yulun Wu,Zachary Butzin-Dozier*

Main category: cs.LG

TL;DR: 本文挑战AI研究中的“规模至上主义”，主张以能力-资源比而非单纯能力为导向重新定位LLM发展，提出了基于梯度影响模式的资源分配框架，通过选择性更新高影响力参数来大幅提升效率。


<details>
  <summary>Details</summary>
Motivation: 当前AI研究中无限制的模型规模和计算增长导致了不可持续的环境影响和资源不平等，需要从根本上重新定位LLM发展方向，关注能力-资源比而非单纯追求能力提升。

Method: 提出了基于梯度影响模式的资源分配理论框架，通过识别遵循重尾分布的高影响力参数，仅更新这些参数，使用简单梯度范数作为计算高效的代理指标，并协调参数和数据选择实现乘法效率增益。

Result: 分析表明仅更新高影响力参数在性能-资源比上严格优于全参数调优，梯度范数能有效识别高影响力组件，协调参数和数据选择可减少数个数量级的资源需求。

Conclusion: 通过将资源意识嵌入模型开发、适应和评估过程，可以重塑AI进步方向，实现更可持续和公平的未来，将硬件变通方案转变为理论最优策略，民主化尖端AI能力访问并显著减少环境影响。

Abstract: This position paper challenges the "scaling fundamentalism" dominating AI
research, where unbounded growth in model size and computation has led to
unsustainable environmental impacts and widening resource inequality. We argue
that LLM development should be fundamentally reoriented toward
capability-per-resource rather than capability alone. We present a theoretical
framework demonstrating that resource-allocation decisions guided by gradient
influence patterns can dramatically improve efficiency throughout the AI
lifecycle. Our analysis shows that in transformer-based models, where a small
fraction of parameters exert outsized influence (following heavy-tailed
distributions), three critical insights emerge: (1) updating only
high-influence parameters strictly outperforms full-parameter tuning on a
performance-per-resource basis; (2) simple gradient norms provide
computationally efficient proxies for identifying these high-influence
components; and (3) coordinated parameter and data selection yields
multiplicative efficiency gains, potentially reducing resource requirements by
orders of magnitude. Building on these theoretical foundations, we propose a
two stage paradigm marginal-return pretraining for foundation developers and
influence guided adaptation for downstream users bridged by gradient
blueprints, metadata describing which parameters matter most for various tasks.
This capability-per-resource perspective transforms what were once considered
pragmatic hardware workarounds into theoretically optimal strategies,
democratizing access to cutting-edge AI capabilities while significantly
reducing environmental impact. By embedding resource consciousness into how we
develop, adapt, and evaluate models, we can reshape AI progress toward a more
sustainable and equitable future.

</details>


### [162] [Continual Learning, Not Training: Online Adaptation For Agents](https://arxiv.org/abs/2511.01093)
*Aman Jaglan,Jarrod Barnes*

Main category: cs.LG

TL;DR: ATLAS提出了一种无需梯度更新的持续学习系统，通过双智能体架构（教师-学生）和持久学习记忆，在推理时动态调整操作策略，实现系统级适应而非参数更新。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法依赖基于梯度的重新训练，不适合需要实时适应的部署智能体。ATLAS旨在实现无需梯度的持续学习，将适应重点从模型参数转移到系统级编排。

Method: 采用双智能体架构：教师负责推理，学生负责执行；包含持久学习记忆存储经验指导；通过编排层在推理时动态调整操作策略（如监督级别、初始计划选择）。

Result: 在微软ExCyTIn-Bench基准测试中，ATLAS使用GPT-5-mini达到54.1%成功率，比GPT-5（High）高13%，成本降低86%。跨事件验证显示零重新训练下准确率从28%提升至41%。

Conclusion: 梯度免费持续学习是实现自适应、可部署AI系统的可行路径，提供了可用于训练显式世界模型的有价值因果注释轨迹。

Abstract: Continual Learning (CL) methods have traditionally focused on mitigating
catastrophic forgetting through gradient-based retraining, an approach
ill-suited for deployed agents that must adapt in real time. We introduce our
Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that
decouples reasoning (Teacher) from execution (Student) and incorporates a
persistent learning memory that stores distilled guidance from experience. This
informs the orchestration layer, enabling the system to dynamically adjust its
operational strategies, such as supervision level or initial plan selection, at
inference time. In doing so, ATLAS achieves gradient-free continual learning,
shifting the locus of adaptation from model parameters to system-level
orchestration. We formulate this as a system-centric paradigm for continual
learning, where the objective is adaptive efficiency: maximizing task success
while minimizing computational cost through inference-time orchestration rather
than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source
benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1%
success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High)
by 13% while reducing cost by 86%. Cross-incident validation demonstrates
generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to
41% with zero retraining, while shifting output composition from verbose
exploration to structured reasoning. Together, these findings establish
gradient-free continual learning as a viable path toward adaptive, deployable
AI systems and provide causally annotated traces valuable for training explicit
world models.

</details>


### [163] [One model to solve them all: 2BSDE families via neural operators](https://arxiv.org/abs/2511.01125)
*Takashi Furuya,Anastasis Kratsios,Dylan Possamaï,Bogdan Raonić*

Main category: cs.LG

TL;DR: 提出了一种基于Kolmogorov-Arnold网络的温和生成变体神经算子模型，用于求解具有随机终止时间的二阶倒向随机微分方程族。


<details>
  <summary>Details</summary>
Motivation: 解决在规则有界欧几里得域上具有随机终止时间的二阶倒向随机微分方程族问题，探索神经算子模型在近似这类复杂随机系统方面的潜力。

Method: 使用Kolmogorov-Arnold网络构建温和生成变体的神经算子模型，特别识别了一个结构化子类的2BSDE族，其神经算子近似仅需要多项式数量的参数。

Result: 证明了对于广泛的2BSDE族，解算子可以通过适当的神经算子模型进行近似，并且对于结构化子类，参数需求从指数级降低到多项式级。

Conclusion: 该研究展示了神经算子在近似复杂随机系统方面的有效性，特别是对于结构化问题可以实现参数效率的显著提升，从指数需求降低到多项式需求。

Abstract: We introduce a mild generative variant of the classical neural operator
model, which leverages Kolmogorov--Arnold networks to solve infinite families
of second-order backward stochastic differential equations ($2$BSDEs) on
regular bounded Euclidean domains with random terminal time. Our first main
result shows that the solution operator associated with a broad range of
$2$BSDE families is approximable by appropriate neural operator models. We then
identify a structured subclass of (infinite) families of $2$BSDEs whose neural
operator approximation requires only a polynomial number of parameters in the
reciprocal approximation rate, as opposed to the exponential requirement in
general worst-case neural operator guarantees.

</details>


### [164] [Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization](https://arxiv.org/abs/2511.01126)
*Parvin Nazari,Bojian Hou,Davoud Ataee Tarzanagh,Li Shen,George Michailidis*

Main category: cs.LG

TL;DR: 提出了一种新的搜索方向，使随机在线双层优化算法无需窗口平滑即可实现次线性双层遗憾，提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前在线双层优化方法依赖确定性窗口平滑遗憾最小化，在函数快速变化时无法准确反映系统性能。

Method: 引入新的搜索方向，开发一阶和零阶随机算法，减少超梯度估计的oracle依赖，同时更新内外层变量和线性系统解，使用零阶方法估计Hessian、Jacobian和梯度。

Result: 实验验证了在在线参数损失调优和黑盒对抗攻击任务上的有效性。

Conclusion: 提出的框架在保证次线性随机双层遗憾的同时，显著提高了在线双层优化的计算效率。

Abstract: Online bilevel optimization (OBO) is a powerful framework for machine
learning problems where both outer and inner objectives evolve over time,
requiring dynamic updates. Current OBO approaches rely on deterministic
\textit{window-smoothed} regret minimization, which may not accurately reflect
system performance when functions change rapidly. In this work, we introduce a
novel search direction and show that both first- and zeroth-order (ZO)
stochastic OBO algorithms leveraging this direction achieve sublinear
{stochastic bilevel regret without window smoothing}. Beyond these guarantees,
our framework enhances efficiency by: (i) reducing oracle dependence in
hypergradient estimation, (ii) updating inner and outer variables alongside the
linear system solution, and (iii) employing ZO-based estimation of Hessians,
Jacobians, and gradients. Experiments on online parametric loss tuning and
black-box adversarial attacks validate our approach.

</details>


### [165] [A Comparative Study of Model Adaptation Strategies for Multi-Treatment Uplift Modeling](https://arxiv.org/abs/2511.01185)
*Ruyue Zhang,Xiaopeng Ke,Ming Liu,Fangzhou Shi,Chang Men,Zhengdan Zhu*

Main category: cs.LG

TL;DR: 本文研究了多治疗场景下的提升建模，提出了正交函数适应(OFA)方法，相比现有方法在效果和鲁棒性方面有显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前多治疗提升建模方法通常从二元治疗工作改编而来，但在各种数据特征下无法保持有效性，需要更鲁棒的解决方案。

Method: 提出基于函数逼近定理的正交函数适应(OFA)，将现有模型适应技术分为结构适应和特征适应两类。

Result: 实验结果表明OFA相比其他适应方法显著提升了提升模型性能，并展现出最高的鲁棒性。

Conclusion: OFA方法在多治疗提升建模中具有优越的性能和鲁棒性，能有效应对各种数据特征挑战。

Abstract: Uplift modeling has emerged as a crucial technique for individualized
treatment effect estimation, particularly in fields such as marketing and
healthcare. Modeling uplift effects in multi-treatment scenarios plays a key
role in real-world applications. Current techniques for modeling
multi-treatment uplift are typically adapted from binary-treatment works. In
this paper, we investigate and categorize all current model adaptations into
two types: Structure Adaptation and Feature Adaptation. Through our empirical
experiments, we find that these two adaptation types cannot maintain
effectiveness under various data characteristics (noisy data, mixed with
observational data, etc.). To enhance estimation ability and robustness, we
propose Orthogonal Function Adaptation (OFA) based on the function
approximation theorem. We conduct comprehensive experiments with multiple data
characteristics to study the effectiveness and robustness of all model
adaptation techniques. Our experimental results demonstrate that our proposed
OFA can significantly improve uplift model performance compared to other
vanilla adaptation methods and exhibits the highest robustness.

</details>


### [166] [Transmitter Identification and Protocol Categorization in Shared Spectrum via Multi-Task RF Classification at the Network Edge](https://arxiv.org/abs/2511.01198)
*Tariq Abdul-Quddoos,Tasnia Sharmin,Xiangfang Li,Lijun Qian*

Main category: cs.LG

TL;DR: 提出一个基于多任务RF信号分类的鲁棒框架，用于共享频谱环境中的发射机识别和协议分类，使用CNN处理重叠信号特性和环境变化，在POWDER平台数据上取得90%协议分类、100%基站分类和92%联合分类的准确率。


<details>
  <summary>Details</summary>
Motivation: 随着频谱共享日益重要，频谱监测和发射机识别对于执行频谱使用政策、提高频谱利用效率和保障网络安全至关重要。

Method: 设计卷积神经网络(CNN)，采用多通道输入策略提取有意义的信号特征，解决重叠信号特性和环境变化等关键挑战。

Result: 在POWDER平台RF数据上取得显著准确率：协议分类90%、发射基站分类100%、联合分类任务92%。

Conclusion: 该方法在增强现代无线网络频谱监测、管理和安全方面具有显著潜力。

Abstract: As spectrum sharing becomes increasingly vital to meet rising wireless
demands in the future, spectrum monitoring and transmitter identification are
indispensable for enforcing spectrum usage policy, efficient spectrum
utilization, and net- work security. This study proposed a robust framework for
transmitter identification and protocol categorization via multi- task RF
signal classification in shared spectrum environments, where the spectrum
monitor will classify transmission protocols (e.g., 4G LTE, 5G-NR, IEEE
802.11a) operating within the same frequency bands, and identify different
transmitting base stations, as well as their combinations. A Convolutional
Neural Network (CNN) is designed to tackle critical challenges such as
overlapping signal characteristics and environmental variability. The proposed
method employs a multi-channel input strategy to extract meaningful signal
features, achieving remarkable accuracy: 90% for protocol classification, 100%
for transmitting base station classification, and 92% for joint classification
tasks, utilizing RF data from the POWDER platform. These results highlight the
significant potential of the proposed method to enhance spectrum monitoring,
management, and security in modern wireless networks.

</details>


### [167] [FEval-TTC: Fair Evaluation Protocol for Test-Time Compute](https://arxiv.org/abs/2511.01203)
*Pavel Rumiantsev,Soumyasundar Pal,Yingxue Zhang,Mark Coates*

Main category: cs.LG

TL;DR: 提出了FEval-TTC协议，用于在LLM性能和API成本波动的情况下，确保测试时计算方法的公平评估。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能和API调用成本会随时间波动，这可能使先前研究的结论失效，因此需要一种公平的评估协议。

Method: 设计FEval-TTC协议，标准化少样本提示和答案提取过程，支持跨多个LLM在数学和常识推理数据集上的评估，并提供成本建模程序。

Result: 开发了开源评估框架，减少了研究者的时间和金钱开销，促进了不同TTC方法的公平比较。

Conclusion: FEval-TTC为测试时计算方法的评估提供了一个标准化、成本有效的框架，有助于在LLM性能波动的情况下保持评估的一致性。

Abstract: The performance of Large Language Models (LLMs) and the associated dollar
costs of API calls can fluctuate over time, potentially invalidating
conclusions drawn in prior research. To address this, we propose a Fair
Evaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure
consistent assessment of test-time compute (TTC) methods, regardless of such
fluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize
underlying Chains-of-Thought (CoT). It supports evaluations across multiple
LLMs on a diverse set of mathematical and commonsense reasoning datasets. The
few-shot prompting and answer extraction processes are standardized across
datasets, reducing both time and monetary overhead for researchers.
Furthermore, we provide a cost modelling procedure that estimates both the
token and dollar cost per query, facilitating equitable comparisons of
prevalent TTC methods. We open-source FEval-TTC for public use at
https://github.com/networkslab/feval_ttc .

</details>


### [168] [Optimizing Electric Vehicle Charging Station Placement Using Reinforcement Learning and Agent-Based Simulations](https://arxiv.org/abs/2511.01218)
*Minh-Duc Nguyen,Dung D. Le,Phi Long Nguyen*

Main category: cs.LG

TL;DR: 提出了一种结合深度强化学习和基于代理模拟的新框架，用于优化电动汽车充电站布局，在动态不确定环境中显著减少等待时间。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法采用确定性奖励系统，无法充分捕捉充电站布局的复杂性，导致评估成本高且难以反映真实世界场景。

Method: 使用深度强化学习与基于代理模拟相结合的方法，采用具有双Q网络的混合RL代理来选择最优位置和配置充电端口，通过结合确定性因素和模拟反馈的混合奖励函数进行指导。

Result: 在越南河内的案例研究中，该方法相比初始状态平均等待时间减少了53.28%，优于静态基线方法。

Conclusion: 该可扩展且自适应的解决方案增强了电动汽车基础设施规划，有效应对现实世界复杂性并改善用户体验。

Abstract: The rapid growth of electric vehicles (EVs) necessitates the strategic
placement of charging stations to optimize resource utilization and minimize
user inconvenience. Reinforcement learning (RL) offers an innovative approach
to identifying optimal charging station locations; however, existing methods
face challenges due to their deterministic reward systems, which limit
efficiency. Because real-world conditions are dynamic and uncertain, a
deterministic reward structure cannot fully capture the complexities of
charging station placement. As a result, evaluation becomes costly and
time-consuming, and less reflective of real-world scenarios. To address this
challenge, we propose a novel framework that integrates deep RL with
agent-based simulations to model EV movement and estimate charging demand in
real time. Our approach employs a hybrid RL agent with dual Q-networks to
select optimal locations and configure charging ports, guided by a hybrid
reward function that combines deterministic factors with simulation-derived
feedback. Case studies in Hanoi, Vietnam, show that our method reduces average
waiting times by 53.28% compared to the initial state, outperforming static
baseline methods. This scalable and adaptive solution enhances EV
infrastructure planning, effectively addressing real-world complexities and
improving user experience.

</details>


### [169] [WindMiL: Equivariant Graph Learning for Wind Loading Prediction](https://arxiv.org/abs/2511.01226)
*Themistoklis Vargiemezis,Charilaos Kanatsoulis,Catherine Gorlé*

Main category: cs.LG

TL;DR: WindMiL是一个结合系统化数据集生成和对称感知图神经网络的机器学习框架，用于高效预测建筑风荷载，相比传统方法大幅降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统风洞测试和大涡模拟计算成本过高（每个LES案例需24小时），难以进行大规模参数研究，需要开发高效的替代方法。

Method: 1）通过符号距离函数插值生成屋顶几何形状，模拟462个LES案例创建大规模数据集；2）开发反射等变图神经网络，确保在镜像几何下的物理一致性预测。

Result: 在插值和外推评估中，WindMiL对表面压力系数的均值和标准差均达到高精度（如平均Cp的RMSE≤0.02），在反射测试中保持96%以上的命中率，而非等变基线模型下降超过10%。

Conclusion: 通过将系统化数据集与等变代理模型结合，WindMiL实现了对建筑风荷载的高效、可扩展和准确预测。

Abstract: Accurate prediction of wind loading on buildings is crucial for structural
safety and sustainable design, yet conventional approaches such as wind tunnel
testing and large-eddy simulation (LES) are prohibitively expensive for
large-scale exploration. Each LES case typically requires at least 24 hours of
computation, making comprehensive parametric studies infeasible. We introduce
WindMiL, a new machine learning framework that combines systematic dataset
generation with symmetry-aware graph neural networks (GNNs). First, we
introduce a large-scale dataset of wind loads on low-rise buildings by applying
signed distance function interpolation to roof geometries and simulating 462
cases with LES across varying shapes and wind directions. Second, we develop a
reflection-equivariant GNN that guarantees physically consistent predictions
under mirrored geometries. Across interpolation and extrapolation evaluations,
WindMiL achieves high accuracy for both the mean and the standard deviation of
surface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) and
remains accurate under reflected-test evaluation, maintaining hit rates above
$96\%$ where the non-equivariant baseline model drops by more than $10\%$. By
pairing a systematic dataset with an equivariant surrogate, WindMiL enables
efficient, scalable, and accurate predictions of wind loads on buildings.

</details>


### [170] [KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records](https://arxiv.org/abs/2511.01249)
*Kun-Wei Lin,Yu-Chen Kuo,Hsin-Yao Wang,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: KAT-GNN是一个结合临床知识和时序动态的图神经网络框架，用于电子健康记录的风险预测，在冠状动脉疾病和院内死亡率预测任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录数据具有异构性和不规则时序特性，给临床风险预测带来挑战，需要有效建模这些复杂特征。

Method: 构建模态特定的患者图，通过SNOMED CT本体知识和EHR共现先验进行知识增强，使用时序感知transformer捕获纵向动态。

Result: 在冠状动脉疾病预测中AUROC达0.9269，在MIMIC-III和MIMIC-IV死亡率预测中分别达到0.9230和0.8849，均优于基线方法。

Conclusion: 将临床知识融入图表示并使用时序注意力机制，为跨临床任务的风险预测提供了有效且可推广的方法。

Abstract: Clinical risk prediction using electronic health records (EHRs) is vital to
facilitate timely interventions and clinical decision support. However,
modeling heterogeneous and irregular temporal EHR data presents significant
challenges. We propose \textbf{KAT-GNN} (Knowledge-Augmented Temporal Graph
Neural Network), a graph-based framework that integrates clinical knowledge and
temporal dynamics for risk prediction. KAT-GNN first constructs
modality-specific patient graphs from EHRs. These graphs are then augmented
using two knowledge sources: (1) ontology-driven edges derived from SNOMED CT
and (2) co-occurrence priors extracted from EHRs. Subsequently, a time-aware
transformer is employed to capture longitudinal dynamics from the graph-encoded
patient representations. KAT-GNN is evaluated on three distinct datasets and
tasks: coronary artery disease (CAD) prediction using the Chang Gung Research
Database (CGRD) and in-hospital mortality prediction using the MIMIC-III and
MIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CAD
prediction (AUROC: 0.9269 $\pm$ 0.0029) and demonstrated strong results in
mortality prediction in MIMIC-III (AUROC: 0.9230 $\pm$ 0.0070) and MIMIC-IV
(AUROC: 0.8849 $\pm$ 0.0089), consistently outperforming established baselines
such as GRASP and RETAIN. Ablation studies confirm that both knowledge-based
augmentation and the temporal modeling component are significant contributors
to performance gains. These findings demonstrate that the integration of
clinical knowledge into graph representations, coupled with a time-aware
attention mechanism, provides an effective and generalizable approach for risk
prediction across diverse clinical tasks and datasets.

</details>


### [171] [Adversarial Spatio-Temporal Attention Networks for Epileptic Seizure Forecasting](https://arxiv.org/abs/2511.01275)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: cs.LG

TL;DR: STAN是一个用于癫痫发作预测的对抗性时空注意力网络，通过级联注意力块联合建模大脑空间连接和时间神经动态，在无需个性化训练的情况下实现早期检测。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作预测需要高灵敏度、低误报率和患者特异性适应性，现有方法假设固定的发作前期持续时间或分别处理时空特征，无法有效捕捉双向依赖关系。

Method: 使用级联注意力架构交替处理空间和时间模块，通过对抗性训练和梯度惩罚来区分发作间期和发作前期状态，基于明确的15分钟发作前期窗口学习。

Result: 在两个EEG基准数据集上达到最先进性能：CHB-MIT头皮数据集96.6%灵敏度、0.011次/小时误报；MSSM颅内数据集94.2%灵敏度、0.063次/小时误报，计算效率高（230万参数，45ms延迟）。

Conclusion: STAN框架不仅为癫痫发作预测提供了有效解决方案，还为医疗健康和其他时间序列领域中的时空预测提供了通用范式，特别适合个体异质性和可解释性要求高的场景。

Abstract: Forecasting epileptic seizures from multivariate EEG signals represents a
critical challenge in healthcare time series prediction, requiring high
sensitivity, low false alarm rates, and subject-specific adaptability. We
present STAN, an Adversarial Spatio-Temporal Attention Network that jointly
models spatial brain connectivity and temporal neural dynamics through cascaded
attention blocks with alternating spatial and temporal modules. Unlike existing
approaches that assume fixed preictal durations or separately process spatial
and temporal features, STAN captures bidirectional dependencies between spatial
and temporal patterns through a unified cascaded architecture. Adversarial
training with gradient penalty enables robust discrimination between interictal
and preictal states learned from clearly defined 15-minute preictal windows.
Continuous 90-minute pre-seizure monitoring reveals that the learned
spatio-temporal attention patterns enable early detection: reliable alarms
trigger at subject-specific times (typically 15-45 minutes before onset),
reflecting the model's capacity to capture subtle preictal dynamics without
requiring individualized training. Experiments on two benchmark EEG datasets
(CHB-MIT scalp: 8 subjects, 46 events; MSSM intracranial: 4 subjects, 14
events) demonstrate state-of-the-art performance: 96.6% sensitivity with 0.011
false detections per hour and 94.2% sensitivity with 0.063 false detections per
hour, respectively, while maintaining computational efficiency (2.3M
parameters, 45 ms latency, 180 MB memory) for real-time edge deployment. Beyond
epilepsy, the proposed framework provides a general paradigm for
spatio-temporal forecasting in healthcare and other time series domains where
individual heterogeneity and interpretability are crucial.

</details>


### [172] [Identification of Capture Phases in Nanopore Protein Sequencing Data Using a Deep Learning Model](https://arxiv.org/abs/2511.01277)
*Annabelle Martin,Daphne Kontogiorgos-Heintz,Jeff Nivala*

Main category: cs.LG

TL;DR: 开发了一个轻量级的一维卷积神经网络（1D CNN）来检测纳米孔蛋白测序中的捕获阶段，将分析时间从几天缩短到30分钟以内，F1分数达到0.94。


<details>
  <summary>Details</summary>
Motivation: 纳米孔蛋白测序产生长而嘈杂的离子电流迹线，其中嵌入关键分子阶段如蛋白捕获。手动识别捕获阶段耗时数天，需要领域专业知识解释复杂信号模式。

Method: 使用轻量级一维卷积神经网络（1D CNN）检测降采样信号窗口中的捕获阶段，与CNN-LSTM混合模型、基于直方图的分类器和其他CNN变体进行比较。

Result: 最佳模型CaptureNet-Deep在保留测试数据上达到F1分数0.94和精度93.39%，支持低延迟推理，已集成到Oxford Nanopore实验仪表板中。

Conclusion: 使用简单、可解释的架构可以实现高效的实时捕获检测，轻量级机器学习模型在测序工作流程中具有更广泛的应用潜力。

Abstract: Nanopore protein sequencing produces long, noisy ionic current traces in
which key molecular phases, such as protein capture and translocation, are
embedded. Capture phases mark the successful entry of a protein into the pore
and serve as both a checkpoint and a signal that a channel merits further
analysis. However, manual identification of capture phases is time-intensive,
often requiring several days for expert reviewers to annotate the data due to
the need for domain-specific interpretation of complex signal patterns. To
address this, a lightweight one-dimensional convolutional neural network (1D
CNN) was developed and trained to detect capture phases in down-sampled signal
windows. Evaluated against CNN-LSTM (Long Short-Term Memory) hybrids,
histogram-based classifiers, and other CNN variants using run-level data
splits, our best model, CaptureNet-Deep, achieved an F1 score of 0.94 and
precision of 93.39% on held-out test data. The model supports low-latency
inference and is integrated into a dashboard for Oxford Nanopore experiments,
reducing the total analysis time from several days to under thirty minutes.
These results show that efficient, real-time capture detection is possible
using simple, interpretable architectures and suggest a broader role for
lightweight ML models in sequencing workflows.

</details>


### [173] [Lyapunov Stability Learning with Nonlinear Control via Inductive Biases](https://arxiv.org/abs/2511.01283)
*Yupu Lu,Shijie Lin,Hao Xu,Zeqing Zhang,Jia Pan*

Main category: cs.LG

TL;DR: 提出了一种基于归纳偏置的神经控制李雅普诺夫函数（CLF）和控制器设计方法，通过端到端学习实现更高的收敛率和更大的吸引区域。


<details>
  <summary>Details</summary>
Motivation: 现有方法将李雅普诺夫条件作为复杂约束进行优化，难以实现全局收敛，且验证过程过于复杂。需要改进学习-验证框架。

Method: 将李雅普诺夫条件视为归纳偏置，设计神经CLF和基于CLF的控制器，实现有限约束下的稳定优化过程和端到端学习。

Result: 在大量实验案例中，相比现有方法获得了更高的收敛率和更大的吸引区域（ROA），并深入揭示了先前方法学习过程中成功率下降的原因。

Conclusion: 基于归纳偏置的方法能够有效改进CLF学习框架，实现更好的稳定性和收敛性能。

Abstract: Finding a control Lyapunov function (CLF) in a dynamical system with a
controller is an effective way to guarantee stability, which is a crucial issue
in safety-concerned applications. Recently, deep learning models representing
CLFs have been applied into a learner-verifier framework to identify
satisfiable candidates. However, the learner treats Lyapunov conditions as
complex constraints for optimisation, which is hard to achieve global
convergence. It is also too complicated to implement these Lyapunov conditions
for verification. To improve this framework, we treat Lyapunov conditions as
inductive biases and design a neural CLF and a CLF-based controller guided by
this knowledge. This design enables a stable optimisation process with limited
constraints, and allows end-to-end learning of both the CLF and the controller.
Our approach achieves a higher convergence rate and larger region of attraction
(ROA) in learning the CLF compared to existing methods among abundant
experiment cases. We also thoroughly reveal why the success rate decreases with
previous methods during learning.

</details>


### [174] [LSHFed: Robust and Communication-Efficient Federated Learning with Locally-Sensitive Hashing Gradient Mapping](https://arxiv.org/abs/2511.01296)
*Guanjie Cheng,Mengzhen Yang,Xinkui Zhao,Shuyi Yu,Tianyu Du,Yangyang Wu,Mengying Zhu,Shuiguang Deng*

Main category: cs.LG

TL;DR: LSHFed是一个鲁棒且通信高效的联邦学习框架，通过局部敏感哈希将高维梯度压缩为二进制表示，在保护隐私的同时有效检测恶意梯度攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在信任缺失环境中易受推理攻击和投毒攻击，现有防御方法存在通信计算成本高、检测精度有限的问题。

Method: 提出LSHFed框架，核心是LSHGM梯度验证机制，使用多超平面局部敏感哈希将高维梯度投影为紧凑的二进制表示，仅通过不可逆哈希形式检测恶意梯度。

Result: 实验表明LSHFed在50%参与者为恶意攻击者时仍能保持高性能，梯度验证通信量比全梯度方法减少1000倍。

Conclusion: LSHFed在增强聚合鲁棒性和隐私保护的同时，显著降低了通信开销，为联邦学习提供了有效的安全防御方案。

Abstract: Federated learning (FL) enables collaborative model training across
distributed nodes without exposing raw data, but its decentralized nature makes
it vulnerable in trust-deficient environments. Inference attacks may recover
sensitive information from gradient updates, while poisoning attacks can
degrade model performance or induce malicious behaviors. Existing defenses
often suffer from high communication and computation costs, or limited
detection precision. To address these issues, we propose LSHFed, a robust and
communication-efficient FL framework that simultaneously enhances aggregation
robustness and privacy preservation. At its core, LSHFed incorporates LSHGM, a
novel gradient verification mechanism that projects high-dimensional gradients
into compact binary representations via multi-hyperplane locally-sensitive
hashing. This enables accurate detection and filtering of malicious gradients
using only their irreversible hash forms, thus mitigating privacy leakage risks
and substantially reducing transmission overhead. Extensive experiments
demonstrate that LSHFed maintains high model performance even when up to 50% of
participants are collusive adversaries while achieving up to a 1000x reduction
in gradient verification communication compared to full-gradient methods.

</details>


### [175] [Diffusion-Based Solver for CNF Placement on the Cloud-Continuum](https://arxiv.org/abs/2511.01343)
*Álvaro Vázquez Rodríguez,Manuel Fernández-Veiga,Carlos Giraldo-Rodríguez*

Main category: cs.LG

TL;DR: 本文提出了一种基于去噪扩散概率模型(DDPM)的新理论框架，用于解决云原生网络功能(CNF)在云连续体中的放置问题，将放置问题重新定义为生成图到分配任务，通过图神经网络去噪器迭代优化分配矩阵。


<details>
  <summary>Details</summary>
Motivation: 传统方法（混合整数非线性规划、启发式算法和强化学习）在可扩展性、约束处理和泛化能力方面存在局限性，无法有效解决5G/6G网络中CNF的分布式编排挑战。

Method: 将CNF放置问题编码为异构图，训练图神经网络去噪器迭代优化噪声CNF到云的分配矩阵，在损失函数中直接融入约束特定损失以学习可行解空间。

Result: 在多样化拓扑上的广泛评估表明，该模型始终产生可行解，推理速度比MINLP求解器快几个数量级。

Conclusion: 基于扩散的生成建模在约束网络嵌入问题中具有巨大潜力，为实现分布式云原生网络功能的实际可扩展编排做出了贡献。

Abstract: The placement of Cloud-Native Network Functions (CNFs) across the
Cloud-Continuum represents a core challenge in the orchestration of current 5G
and future 6G networks. The process involves the placement of interdependent
computing tasks, structured as Service Function Chains, over distributed cloud
infrastructures. This is achieved while satisfying strict resource, bandwidth
and latency constraints. It is acknowledged that classical approaches,
including mixed-integer nonlinear programming, heuristics and reinforcement
learning are limited in terms of scalability, constraint handling and
generalisation capacity. In the present study, a novel theoretical framework is
proposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) for
CNF placement. The present approach proposes a reconceptualisation of placement
as a generative graph to assignment task, where the placement problem is
encoded as a heterogeneous graph, and a Graph Neural Network denoiser is
trained to iteratively refine noisy CNF-to-cloud assignment matrices. The model
incorporates constraint-specific losses directly into the loss function,
thereby allowing it to learn feasible solution spaces. The integration of the
DDPM formulation with structured combinatorial constraints is achieved through
a rigorous and systematic approach. Extensive evaluations across diverse
topologies have been conducted, which have confirmed that the model
consistently produces feasible solutions with orders of magnitude faster
inference than MINLP solvers. The results obtained demonstrate the potential of
diffusion-based generative modelling for constrained network embedding
problems, making an impact towards the practical, scalable orchestration of
distributed Cloud-Native Network Functions.

</details>


### [176] [MiniFool - Physics-Constraint-Aware Minimizer-Based Adversarial Attacks in Deep Neural Networks](https://arxiv.org/abs/2511.01352)
*Lucie Flek,Oliver Janik,Philipp Alexander Jung,Akbar Karimi,Timo Saala,Alexander Schmidt,Matthias Schott,Philipp Soldin,Matthias Thiesmeyer,Christopher Wiebusch,Ulrich Willemsen*

Main category: cs.LG

TL;DR: 提出MiniFool算法，一种基于物理启发的对抗攻击方法，用于测试粒子物理和天体物理中的神经网络分类任务。该算法通过最小化结合χ²检验统计量和目标分数偏差的成本函数来生成对抗样本。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够考虑实验不确定性的对抗攻击方法，用于评估粒子物理和天体物理中神经网络分类器的鲁棒性，特别是在IceCube中微子天文台的天体τ中微子搜索等应用中。

Method: 基于最小化成本函数的方法，该函数结合了χ²检验统计量（基于实验不确定性量化扰动概率）与期望目标分数的偏差。算法应用于MNIST数据集和CMS实验的开放数据。

Result: 研究发现正确分类和错误分类事件的分类翻转可能性不同。通过缩放实验不确定性的攻击参数，可以量化网络决策的鲁棒性，并测试未标记实验数据的分类稳健性。

Conclusion: MiniFool算法具有通用适用性，能够有效评估神经网络分类器在考虑实验不确定性情况下的鲁棒性，为粒子物理和天体物理领域的机器学习应用提供重要的测试工具。

Abstract: In this paper, we present a new algorithm, MiniFool, that implements
physics-inspired adversarial attacks for testing neural network-based
classification tasks in particle and astroparticle physics. While we initially
developed the algorithm for the search for astrophysical tau neutrinos with the
IceCube Neutrino Observatory, we apply it to further data from other science
domains, thus demonstrating its general applicability. Here, we apply the
algorithm to the well-known MNIST data set and furthermore, to Open Data data
from the CMS experiment at the Large Hadron Collider. The algorithm is based on
minimizing a cost function that combines a $\chi^2$ based test-statistic with
the deviation from the desired target score. The test statistic quantifies the
probability of the perturbations applied to the data based on the experimental
uncertainties. For our studied use cases, we find that the likelihood of a
flipped classification differs for both the initially correctly and incorrectly
classified events. When testing changes of the classifications as a function of
an attack parameter that scales the experimental uncertainties, the robustness
of the network decision can be quantified. Furthermore, this allows testing the
robustness of the classification of unlabeled experimental data.

</details>


### [177] [Verifiable Split Learning via zk-SNARKs](https://arxiv.org/abs/2511.01356)
*Rana Alaa,Darío González-Ferreiro,Carlos Beis-Penedo,Manuel Fernández-Veiga,Rebeca P. Díaz-Redondo,Ana Fernández-Vilas*

Main category: cs.LG

TL;DR: 提出了一种可验证的分割学习框架，通过集成zk-SNARK证明来确保分割学习中客户端和服务器端计算的正确性和可验证性。


<details>
  <summary>Details</summary>
Motivation: 分割学习虽然能够实现数据或资源分离设备间的协同训练，但缺乏验证计算正确性和诚实性的能力。

Method: 在服务器端的前向传播和反向传播中为双方生成zk-SNARK证明和验证，确保双向可验证性。

Result: 与仅记录更新但不生成零知识证明的区块链系统相比，应用zk-SNARK测试实现了可验证性和正确性，而区块链虽然轻量但不可验证。

Conclusion: zk-SNARK证明能够有效解决分割学习中的可验证性问题，为协作学习提供可靠的计算验证机制。

Abstract: Split learning is an approach to collaborative learning in which a deep
neural network is divided into two parts: client-side and server-side at a cut
layer. The client side executes its model using its raw input data and sends
the intermediate activation to the server side. This configuration architecture
is very useful for enabling collaborative training when data or resources are
separated between devices. However, split learning lacks the ability to verify
the correctness and honesty of the computations that are performed and
exchanged between the parties. To this purpose, this paper proposes a
verifiable split learning framework that integrates a zk-SNARK proof to ensure
correctness and verifiability. The zk-SNARK proof and verification are
generated for both sides in forward propagation and backward propagation on the
server side, guaranteeing verifiability on both sides. The verifiable split
learning architecture is compared to a blockchain-enabled system for the same
deep learning network, one that records updates but without generating the
zero-knowledge proof. From the comparison, it can be deduced that applying the
zk-SNARK test achieves verifiability and correctness, while blockchains are
lightweight but unverifiable.

</details>


### [178] [Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization](https://arxiv.org/abs/2511.01374)
*Ziqi Wang,Jiashun Liu,Ling Pan*

Main category: cs.LG

TL;DR: 提出了一种新的多模态强化学习方法，通过重新参数化直接优化不可行的多模态策略，并引入基于距离的多样性正则化，在多样关键场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统连续深度强化学习算法使用确定性或单峰高斯策略，无法表达复杂的多模态决策分布，这在多样关键场景中会限制性能。现有基于扩散或摊销策略的多模态RL方法存在不可行性问题。

Method: 首先在统一框架内重新表述现有不可行多模态策略，证明可以通过重新参数化直接使用策略梯度优化。然后提出基于距离的多样性正则化，无需显式计算决策概率。

Result: 在多目标达成和生成RL等多样关键领域展示了多模态策略的优势，特别是在少样本鲁棒性方面。在传统MuJoCo基准测试中也表现出竞争力，实验表明摊销策略是具有强大多模态表达性和高性能的有前景策略模型。

Conclusion: 提出的方法成功平衡了性能、决策多样性和效率，解决了现有多模态RL方法的局限性，为复杂决策场景提供了有效的解决方案。

Abstract: Traditional continuous deep reinforcement learning (RL) algorithms employ
deterministic or unimodal Gaussian actors, which cannot express complex
multimodal decision distributions. This limitation can hinder their performance
in diversity-critical scenarios. There have been some attempts to design online
multimodal RL algorithms based on diffusion or amortized actors. However, these
actors are intractable, making existing methods struggle with balancing
performance, decision diversity, and efficiency simultaneously. To overcome
this challenge, we first reformulate existing intractable multimodal actors
within a unified framework, and prove that they can be directly optimized by
policy gradient via reparameterization. Then, we propose a distance-based
diversity regularization that does not explicitly require decision
probabilities. We identify two diversity-critical domains, namely multi-goal
achieving and generative RL, to demonstrate the advantages of multimodal
policies and our method, particularly in terms of few-shot robustness. In
conventional MuJoCo benchmarks, our algorithm also shows competitive
performance. Moreover, our experiments highlight that the amortized actor is a
promising policy model class with strong multimodal expressivity and high
performance. Our code is available at https://github.com/PneuC/DrAC

</details>


### [179] [Protecting the Neural Networks against FGSM Attack Using Machine Unlearning](https://arxiv.org/abs/2511.01377)
*Amir Hossein Khorasani,Ali Jahanian,Maryam Rastgarpour*

Main category: cs.LG

TL;DR: 本文研究了在LeNet神经网络上应用遗忘技术来防御FGSM对抗攻击，发现该方法能显著提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型容易受到对抗攻击，特别是FGSM攻击，因此需要开发有效的防御方法来提高模型安全性。

Method: 使用机器遗忘技术，在LeNet神经网络上对FGSM攻击进行遗忘处理，通过重新训练模型来移除对抗性扰动的影响。

Result: 实验表明，应用遗忘技术后，LeNet网络对FGSM攻击的鲁棒性得到了显著提升。

Conclusion: 机器遗忘技术是防御FGSM对抗攻击的有效方法，能够提高神经网络模型的安全性。

Abstract: Machine learning is a powerful tool for building predictive models. However,
it is vulnerable to adversarial attacks. Fast Gradient Sign Method (FGSM)
attacks are a common type of adversarial attack that adds small perturbations
to input data to trick a model into misclassifying it. In response to these
attacks, researchers have developed methods for "unlearning" these attacks,
which involves retraining a model on the original data without the added
perturbations. Machine unlearning is a technique that tries to "forget"
specific data points from the training dataset, to improve the robustness of a
machine learning model against adversarial attacks like FGSM. In this paper, we
focus on applying unlearning techniques to the LeNet neural network, a popular
architecture for image classification. We evaluate the efficacy of unlearning
FGSM attacks on the LeNet network and find that it can significantly improve
its robustness against these types of attacks.

</details>


### [180] [Memory-Efficient Training with In-Place FFT Implementation](https://arxiv.org/abs/2511.01385)
*Xinyu Ding,Bangtian Liu,Siyu Liao,Zhongfeng Wang*

Main category: cs.LG

TL;DR: 提出了首个实域完全原位FFT框架(rdFFT)，通过利用蝴蝶操作对称性和频域共轭特性，设计隐式复数编码方案，消除中间缓存使用，实现输入输出内存空间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的FFT实现（包括标准FFT和实FFT）无法实现真正的原位计算，特别是实FFT将大小为n的输入映射到大小为n/2+1的复数输出，导致维度不匹配并需要额外内存分配。

Method: 利用蝴蝶操作对称性和频域共轭特性，设计隐式复数编码方案，完全消除中间缓存使用，实现实域完全原位FFT计算。

Result: 在多个自然语言理解任务上的实验证明该方法能有效降低训练内存成本。

Conclusion: 该方法为频域轻量级适应提供了一个有前景的方向。

Abstract: Fast Fourier Transforms (FFT) are widely used to reduce memory and
computational costs in deep learning. However, existing implementations,
including standard FFT and real FFT (rFFT), cannot achieve true in-place
computation. In particular, rFFT maps an input of size n to a complex output of
size n/2+1, causing dimensional mismatch and requiring additional memory
allocation. We propose the first real-domain, fully in-place FFT framework
(rdFFT) that preserves input-output memory space consistency. By leveraging
butterfly operation symmetry and conjugate properties in the frequency domain,
we design an implicit complex encoding scheme that eliminates intermediate
cache usage entirely. Experiments on multiple natural language understanding
tasks demonstrate the method effectiveness in reducing training memory cost,
offering a promising direction for frequency-domain lightweight adaptation.

</details>


### [181] [Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping](https://arxiv.org/abs/2511.01408)
*Markus B. Pettersson,Adel Daoud*

Main category: cs.LG

TL;DR: 提出基于图的卫星嵌入方法，使用AlphaEarth低维嵌入预测撒哈拉以南非洲的财富指数，通过空间关系和模糊标签损失处理坐标位移问题。


<details>
  <summary>Details</summary>
Motivation: 全球南方地区缺乏精细的贫困地图，DHS调查数据空间覆盖有限且坐标因隐私保护而随机位移，降低了数据质量。

Method: 使用图结构建模调查点和未标记位置的空间关系，引入概率性"模糊标签"损失来处理坐标位移问题，基于AlphaEarth卫星嵌入进行预测。

Result: 在37个DHS数据集上的实验表明，相比仅使用图像的基线方法，加入图结构略微提高了预测准确性。

Conclusion: 紧凑的EO嵌入在大规模社会经济制图中具有潜力，图结构有助于提高财富预测的泛化能力。

Abstract: Accurate, fine-grained poverty maps remain scarce across much of the Global
South. While Demographic and Health Surveys (DHS) provide high-quality
socioeconomic data, their spatial coverage is limited and reported coordinates
are randomly displaced for privacy, further reducing their quality. We propose
a graph-based approach leveraging low-dimensional AlphaEarth satellite
embeddings to predict cluster-level wealth indices across Sub-Saharan Africa.
By modeling spatial relations between surveyed and unlabeled locations, and by
introducing a probabilistic "fuzzy label" loss to account for coordinate
displacement, we improve the generalization of wealth predictions beyond
existing surveys. Our experiments on 37 DHS datasets (2017-2023) show that
incorporating graph structure slightly improves accuracy compared to
"image-only" baselines, demonstrating the potential of compact EO embeddings
for large-scale socioeconomic mapping.

</details>


### [182] [CG-FKAN: Compressed-Grid Federated Kolmogorov-Arnold Networks for Communication Constrained Environment](https://arxiv.org/abs/2511.01433)
*Seunghun Yu,Youngjoon Lee,Jinu Gong,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出CG-FKAN方法，通过稀疏化和压缩扩展网格来减少联邦学习中KAN网络的通信开销，在通信受限环境下比固定网格KAN降低13.6%的RMSE。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私关键应用中广泛使用，但可解释性有限。KAN网络通过可学习的样条函数解决了这个问题，但现有研究忽略了网格扩展带来的通信开销问题。

Method: 提出CG-FKAN方法，在通信预算约束下，通过稀疏化处理并仅传输必要的系数来压缩扩展网格。

Result: 实验表明，在通信受限设置下，CG-FKAN比固定网格KAN实现了高达13.6%更低的RMSE。

Conclusion: CG-FKAN有效解决了联邦学习中KAN网络的通信开销问题，并推导了其近似误差的理论上界。

Abstract: Federated learning (FL), widely used in privacy-critical applications,
suffers from limited interpretability, whereas Kolmogorov-Arnold Networks (KAN)
address this limitation via learnable spline functions. However, existing FL
studies applying KAN overlook the communication overhead introduced by grid
extension, which is essential for modeling complex functions. In this letter,
we propose CG-FKAN, which compresses extended grids by sparsifying and
transmitting only essential coefficients under a communication budget.
Experiments show that CG-FKAN achieves up to 13.6% lower RMSE than fixed-grid
KAN in communication-constrained settings. In addition, we derive a theoretical
upper bound on its approximation error.

</details>


### [183] [The Curvature Rate λ: A Scalar Measure of Input-Space Sharpness in Neural Networks](https://arxiv.org/abs/2511.01438)
*Jacob Poschl*

Main category: cs.LG

TL;DR: 该论文提出了一种在输入空间中定义的曲率度量——曲率率λ，通过高阶输入导数的指数增长率来衡量神经网络的光滑性，并开发了相应的正则化方法CRR。


<details>
  <summary>Details</summary>
Motivation: 现有基于参数空间的尖锐度度量计算昂贵、对重参数化敏感且难以从功能角度解释，需要一种在输入空间中定义、更直观的曲率度量方法。

Method: 引入曲率率λ作为高阶输入导数范数的指数增长率，通过log ||D^n f||与n的斜率来估计；并提出了曲率率正则化(CRR)方法来直接塑造输入空间几何。

Result: 在解析函数和神经网络上的实验表明，λ在训练过程中可预测地演化，CRR能达到与SAM相似的准确率，同时产生更平坦的输入空间几何和更好的置信度校准。

Conclusion: 曲率率λ提供了一个紧凑、可解释且参数化不变的函数光滑性描述符，通过微分动力学将曲率概念基础化。

Abstract: Curvature influences generalization, robustness, and how reliably neural
networks respond to small input perturbations. Existing sharpness metrics are
typically defined in parameter space (e.g., Hessian eigenvalues) and can be
expensive, sensitive to reparameterization, and difficult to interpret in
functional terms. We introduce a scalar curvature measure defined directly in
input space: the curvature rate {\lambda}, given by the exponential growth rate
of higher-order input derivatives. Empirically, {\lambda} is estimated as the
slope of log ||D^n f|| versus n for small n. This growth-rate perspective
unifies classical analytic quantities: for analytic functions, {\lambda}
corresponds to the inverse radius of convergence, and for bandlimited signals,
it reflects the spectral cutoff. The same principle extends to neural networks,
where {\lambda} tracks the emergence of high-frequency structure in the
decision boundary. Experiments on analytic functions and neural networks (Two
Moons and MNIST) show that {\lambda} evolves predictably during training and
can be directly shaped using a simple derivative-based regularizer, Curvature
Rate Regularization (CRR). Compared to Sharpness-Aware Minimization (SAM), CRR
achieves similar accuracy while yielding flatter input-space geometry and
improved confidence calibration. By grounding curvature in differentiation
dynamics, {\lambda} provides a compact, interpretable, and
parameterization-invariant descriptor of functional smoothness in learned
models.

</details>


### [184] [Efficient Curvature-aware Graph Network](https://arxiv.org/abs/2511.01443)
*Chaoqun Fei,Tinglve Zhou,Tianyong Hao,Yangyang Li*

Main category: cs.LG

TL;DR: 提出了一种新的图曲率度量——有效电阻曲率，用节点间的有效电阻替代最优传输距离来量化消息传递的难易程度，在保持几何表达能力的同时显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决Ollivier-Ricci曲率计算复杂度过高的问题，使其能够适用于大规模图数据集，同时保持几何先验对图神经网络建模能力的增强作用。

Method: 基于节点间有效电阻来定义图曲率，替代传统的基于最优传输距离的Ollivier-Ricci曲率计算方法。

Result: 在多种图神经网络任务上的实验表明，该方法在保持与Ollivier-Ricci曲率相当性能的同时，大幅降低了计算开销。

Conclusion: 有效电阻曲率是Ollivier-Ricci曲率的高效替代方案，能够在保持几何表达能力的同时实现显著的计算效率提升。

Abstract: Graph curvature provides geometric priors for Graph Neural Networks (GNNs),
enhancing their ability to model complex graph structures, particularly in
terms of structural awareness, robustness, and theoretical interpretability.
Among existing methods, Ollivier-Ricci curvature has been extensively studied
due to its strong geometric interpretability, effectively characterizing the
local geometric distribution between nodes. However, its prohibitively high
computational complexity limits its applicability to large-scale graph
datasets. To address this challenge, we propose a novel graph curvature
measure--Effective Resistance Curvature--which quantifies the ease of message
passing along graph edges using the effective resistance between node pairs,
instead of the optimal transport distance. This method significantly
outperforms Ollivier-Ricci curvature in computational efficiency while
preserving comparable geometric expressiveness. Theoretically, we prove the low
computational complexity of effective resistance curvature and establish its
substitutability for Ollivier-Ricci curvature. Furthermore, extensive
experiments on diverse GNN tasks demonstrate that our method achieves
competitive performance with Ollivier-Ricci curvature while drastically
reducing computational overhead.

</details>


### [185] [DAMBench: A Multi-Modal Benchmark for Deep Learning-based Atmospheric Data Assimilation](https://arxiv.org/abs/2511.01468)
*Hao Wang,Zixuan Weng,Jindong Han,Wei Fan,Hao Liu*

Main category: cs.LG

TL;DR: DAMBench是首个大规模多模态数据同化基准测试，集成了高质量背景状态和真实世界多模态观测数据，为数据驱动DA模型在真实大气条件下的评估提供标准化框架。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习数据同化研究中两个关键局限：依赖过度简化的合成观测场景，以及缺乏标准化基准进行公平模型比较。

Method: 整合最先进预报系统的高质量背景状态和真实世界多模态观测（气象站和卫星图像），统一重采样到公共网格并进行时间对齐，提供统一评估协议。

Result: 建立了严谨的研究基础，促进可重现性、公平比较和扩展到真实世界多模态场景的能力，并展示了集成真实观测如何增强简单基线模型。

Conclusion: DAMBench为未来数据同化研究提供了标准化基准，支持系统化训练、验证和测试，推动数据驱动方法在复杂真实场景中的应用。

Abstract: Data Assimilation is a cornerstone of atmospheric system modeling, tasked
with reconstructing system states by integrating sparse, noisy observations
with prior estimation. While traditional approaches like variational and
ensemble Kalman filtering have proven effective, recent advances in deep
learning offer more scalable, efficient, and flexible alternatives better
suited for complex, real-world data assimilation involving large-scale and
multi-modal observations. However, existing deep learning-based DA research
suffers from two critical limitations: (1) reliance on oversimplified scenarios
with synthetically perturbed observations, and (2) the absence of standardized
benchmarks for fair model comparison. To address these gaps, in this work, we
introduce DAMBench, the first large-scale multi-modal benchmark designed to
evaluate data-driven DA models under realistic atmospheric conditions. DAMBench
integrates high-quality background states from state-of-the-art forecasting
systems and real-world multi-modal observations (i.e., real-world weather
stations and satellite imagery). All data are resampled to a common grid and
temporally aligned to support systematic training, validation, and testing. We
provide unified evaluation protocols and benchmark representative data
assimilation approaches, including latent generative models and neural process
frameworks. Additionally, we propose a lightweight multi-modal plugin to
demonstrate how integrating realistic observations can enhance even simple
baselines. Through comprehensive experiments, DAMBench establishes a rigorous
foundation for future research, promoting reproducibility, fair comparison, and
extensibility to real-world multi-modal scenarios. Our dataset and code are
publicly available at https://github.com/figerhaowang/DAMBench.

</details>


### [186] [Real-time Continual Learning on Intel Loihi 2](https://arxiv.org/abs/2511.01553)
*Elvin Hajizada,Danielle Rager,Timothy Shea,Leobardo Campos-Macias,Andreas Wild,Eyke Hüllermeier,Yulia Sandamirskaya,Mike Davies*

Main category: cs.LG

TL;DR: CLP-SNN是一种基于脉冲神经网络的在线持续学习架构，在Intel Loihi 2芯片上实现，通过事件驱动稀疏学习、自归一化学习规则和神经发生机制，在边缘设备上实现了高效且无回放的持续学习。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI设备在开放世界中面临的数据分布漂移和新类别出现的挑战，同时克服传统离线训练在功耗受限环境中的局限性。

Method: 提出三个创新：事件驱动时空稀疏局部学习、自归一化三因子学习规则维持权重归一化、集成神经发生和元可塑性实现容量扩展和遗忘缓解。

Result: 在OpenLORIS少样本学习实验中，CLP-SNN达到与回放方法相当的准确率，同时实现70倍速度提升和5600倍能效提升。

Conclusion: 脑启发算法与神经形态硬件的协同设计能够打破传统精度-效率权衡，为未来边缘AI系统提供突破性解决方案。

Abstract: AI systems on edge devices face a critical challenge in open-world
environments: adapting when data distributions shift and novel classes emerge.
While offline training dominates current paradigms, online continual learning
(OCL)--where models learn incrementally from non-stationary streams without
catastrophic forgetting--remains challenging in power-constrained settings. We
present a neuromorphic solution called CLP-SNN: a spiking neural network
architecture for Continually Learning Prototypes and its implementation on
Intel's Loihi 2 chip. Our approach introduces three innovations: (1)
event-driven and spatiotemporally sparse local learning, (2) a self-normalizing
three-factor learning rule maintaining weight normalization, and (3) integrated
neurogenesis and metaplasticity for capacity expansion and forgetting
mitigation. On OpenLORIS few-shot learning experiments, CLP-SNN achieves
accuracy competitive with replay methods while being rehearsal-free. CLP-SNN
delivers transformative efficiency gains: 70\times faster (0.33ms vs 23.2ms),
and 5,600\times more energy efficient (0.05mJ vs 281mJ) than the best
alternative OCL on edge GPU. This demonstrates that co-designed brain-inspired
algorithms and neuromorphic hardware can break traditional accuracy-efficiency
trade-offs for future edge AI systems.

</details>


### [187] [Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction](https://arxiv.org/abs/2511.01570)
*Xiaosha Xue,Peibo Duan,Zhipeng Liu,Qi Chu,Changsheng Zhang,Bin zhang*

Main category: cs.LG

TL;DR: MS-HGFN模型通过分层图神经网络和多尺度时空特征融合，有效解决了股票预测中忽略的股票内部属性模式和特征采样偏差问题，在美中股市数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度图神经网络在股票预测中经常忽略两个关键点：股票内部属性模式对股票间相关性的影响，以及在多尺度采样中对粗粒度和细粒度特征的注意力偏差。

Method: 提出MS-HGFN模型，包含分层GNN模块学习不同时间尺度下的内部属性模式和外部属性特征，形成动态图；采用自上而下的门控方法融合多尺度时空特征。

Result: 在美中股市真实数据集上的实验表明，MS-HGFN优于传统和先进模型，预测准确率提升达1.4%，且在收益模拟中表现出更强的稳定性。

Conclusion: MS-HGFN通过全面捕捉时空依赖关系并有效融合多尺度特征，显著提升了股票市场预测的准确性和稳定性。

Abstract: Accurately predicting stock market movements remains a formidable challenge
due to the inherent volatility and complex interdependencies among stocks.
Although multi-scale Graph Neural Networks (GNNs) hold potential for modeling
these relationships, they frequently neglect two key points: the subtle
intra-attribute patterns within each stock affecting inter-stock correlation,
and the biased attention to coarse- and fine-grained features during
multi-scale sampling. To overcome these challenges, we introduce MS-HGFN
(Multi-Scale Hierarchical Graph Fusion Network). The model features a
hierarchical GNN module that forms dynamic graphs by learning patterns from
intra-attributes and features from inter-attributes over different time scales,
thus comprehensively capturing spatio-temporal dependencies. Additionally, a
top-down gating approach facilitates the integration of multi-scale
spatio-temporal features, preserving critical coarse- and fine-grained features
without too much interference. Experiments utilizing real-world datasets from
U.S. and Chinese stock markets demonstrate that MS-HGFN outperforms both
traditional and advanced models, yielding up to a 1.4% improvement in
prediction accuracy and enhanced stability in return simulations. The code is
available at https://anonymous.4open.science/r/MS-HGFN.

</details>


### [188] [HIT-ROCKET: Hadamard-vector Inner-product Transformer for ROCKET](https://arxiv.org/abs/2511.01572)
*Wang Hao,Kuang Zhang,Hou Chengyu,Yuan Zhonghao,Tan Chenxing,Fu Weifeng,Zhu Yangying*

Main category: cs.LG

TL;DR: 提出基于Hadamard卷积变换的特征提取方法，使用Hadamard矩阵的列或行向量作为不同长度的卷积核，在保持与现有方法兼容的同时提升计算效率、鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类方法（如HIVE-COTE、Proximity Forest、TS-CHIEF）计算复杂度高，参数调优和训练周期长；轻量级方法如ROCKET在核选择和计算开销方面仍有改进空间。

Method: 使用Hadamard矩阵的列或行向量作为扩展长度的卷积核，利用核的正交性提升计算效率，与现有方法（如ROCKET）完全兼容。

Result: 在UCR时间序列数据集上的实验显示，F1分数比ROCKET至少提升5%，训练时间比最快的ROCKET变体miniROCKET缩短50%，可在超低功耗嵌入式设备上部署。

Conclusion: 提出的Hadamard卷积变换方法在保持高性能的同时显著提升了计算效率，为时间序列分类提供了更高效的解决方案。

Abstract: Time series classification holds broad application value in communications,
information countermeasures, finance, and medicine. However, state-of-the-art
(SOTA) methods-including HIVE-COTE, Proximity Forest, and TS-CHIEF-exhibit high
computational complexity, coupled with lengthy parameter tuning and training
cycles. In contrast, lightweight solutions like ROCKET (Random Convolutional
Kernel Transform) offer greater efficiency but leave substantial room for
improvement in kernel selection and computational overhead. To address these
challenges, we propose a feature extraction approach based on Hadamard
convolutional transform, utilizing column or row vectors of Hadamard matrices
as convolution kernels with extended lengths of varying sizes. This enhancement
maintains full compatibility with existing methods (e.g., ROCKET) while
leveraging kernel orthogonality to boost computational efficiency, robustness,
and adaptability. Comprehensive experiments on multi-domain datasets-focusing
on the UCR time series dataset-demonstrate SOTA performance: F1-score improved
by at least 5% vs. ROCKET, with 50% shorter training time than miniROCKET
(fastest ROCKET variant) under identical hyperparameters, enabling deployment
on ultra-low-power embedded devices. All code is available on GitHub.

</details>


### [189] [Explore More, Learn Better: Parallel MLLM Embeddings under Mutual Information Minimization](https://arxiv.org/abs/2511.01588)
*Zhicheng Wang,Chen Ju,Xu Chen,Shuai Xiao,Jinsong Lan,Xiaoyong Zhu,Ying Chen,Zhiguo Cao*

Main category: cs.LG

TL;DR: 提出并行解耦框架PDF，通过多模态大语言模型的指令可控性生成并行嵌入，突破传统SSC范式的限制，显著提升多模态嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 传统多模态嵌入方法采用SSC范式（单输入、单一嵌入、对比监督），将丰富的多模态输入压缩为单一嵌入，无法充分利用MLLM的能力。需要新的框架来生成多维度并行嵌入。

Method: 使用共享MLLM骨干网络，通过不同的可学习前缀条件化生成多个并行路径，为同一输入获得并行嵌入。采用互信息最小化约束确保并行多样性，结合每路径对比监督保持语义对齐。

Result: 在MMEB基准测试中显著提升性能：VLM2Vec-LLaVA-1.6-LR模型提升+8.9%（7B），VLM2Vec-Qwen2VL模型分别提升+4.2%（2B）和+3.1%（7B）。2B模型仅用一半计算预算就超越基线+2.6%。

Conclusion: PDF框架有效解决了传统SSC范式的局限性，通过并行嵌入生成实现了更全面的语义覆盖和更通用的嵌入空间，在推理时仅需单次前向传播，计算开销可忽略。

Abstract: Embedding models are a cornerstone of modern AI. Driven by Multimodal Large
Language Models (MLLMs), they have made great progress in architecture and data
curation, while the holistic paradigm is still limited to SSC, i.e., single
input, singular embedding, contrastive supervision, which collapses rich,
multifaceted inputs into monolithic embeddings and fails to fully exploit MLLM
capabilities. In this paper, we tailor one Parallel Decoupling Framework (PDF)
for multimodal embedding learning, by utilizing the proprietary steerability of
MLLMs, i.e., their ability to flexibly generate quite differentiated response
under explicit instructions. Concretely, PDF conditions a shared MLLM backbone
on distinct, learnable prefixes to roll out multiple parallel paths for one
input, then relies on these paths to obtain parallel embeddings. To promote
full parallel diversity, we employ Mutual Information Minimization (MIM) as an
explicit constraint, coupled with per-path contrastive supervision to maintain
semantic alignment. Such dual-objectives force PDF to yield robust semantic
coverage and a generalizable embedding space. Ultimately, the remarkable
embedding space are accessible at inference via one single forward pass,
incurring negligible computational overhead. We instantiate PDF on multiple
MLLM backbones and prove its effectiveness on MMEB benchmark. Significant gains
are consistently achieved across various resolutions and model sizes, e.g.,
boosting the VLM2Vec-LLaVA-1.6-LR model by a remarkable +8.9% (7B), while the
VLM2Vec-Qwen2VL models by +4.2% (2B) and +3.1% (7B). In terms of efficiency,
our 2B model surpasses its baseline by +2.6% using only half the computational
budget.

</details>


### [190] [Defining Energy Indicators for Impact Identification on Aerospace Composites: A Physics-Informed Machine Learning Perspective](https://arxiv.org/abs/2511.01592)
*Natália Ribeiro Marinho,Richard Loendersloot,Frank Grooteman,Jan Willem Wiegman,Uraz Odyurt,Tiedo Tinga*

Main category: cs.LG

TL;DR: 提出了一种物理信息驱动的机器学习框架，用于航空航天复合材料冲击能量估计，通过结合物理知识和特征选择，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 当前冲击能量预测方法受限于数据稀疏性、信号噪声、复杂特征依赖关系、非线性动力学、巨大设计空间和逆问题的不适定性等挑战。

Method: 构建物理信息输入空间，结合观测偏差和针对性特征选择，从时域、频域和时频域提取特征，采用统计显著性、相关性过滤、降维和噪声鲁棒性的结构化特征选择过程。

Result: 与传统的时序技术和纯数据驱动模型相比，冲击能量预测误差减少了三倍，预测精度显著提高。

Conclusion: 该方法产生了具有统计鲁棒性和物理意义的紧凑能量敏感指标集，使冲击能量预测保持可解释性，并能追溯到可测量的结构响应。

Abstract: Energy estimation is critical to impact identification on aerospace
composites, where low-velocity impacts can induce internal damage that is
undetectable at the surface. Current methodologies for energy prediction are
often constrained by data sparsity, signal noise, complex feature
interdependencies, non-linear dynamics, massive design spaces, and the
ill-posed nature of the inverse problem. This study introduces a
physics-informed framework that embeds domain knowledge into machine learning
through a dedicated input space. The approach combines observational biases,
which guide the design of physics-motivated features, with targeted feature
selection to retain only the most informative indicators. Features are
extracted from time, frequency, and time-frequency domains to capture
complementary aspects of the structural response. A structured feature
selection process integrating statistical significance, correlation filtering,
dimensionality reduction, and noise robustness ensures physical relevance and
interpretability. Exploratory data analysis further reveals domain-specific
trends, yielding a reduced feature set that captures essential dynamic
phenomena such as amplitude scaling, spectral redistribution, and transient
signal behaviour. Together, these steps produce a compact set of
energy-sensitive indicators with both statistical robustness and physical
significance, resulting in impact energy predictions that remain interpretable
and traceable to measurable structural responses. Using this optimised input
space, a fully-connected neural network is trained and validated with
experimental data from multiple impact scenarios, including pristine and
damaged states. The resulting model demonstrates significantly improved impact
energy prediction accuracy, reducing errors by a factor of three compared to
conventional time-series techniques and purely data-driven models.

</details>


### [191] [Scaling Graph Chain-of-Thought Reasoning: A Multi-Agent Framework with Efficient LLM Serving](https://arxiv.org/abs/2511.01633)
*Chengying Huan,Ziheng Meng,Yongchao Liu,Zhengyi Yang,Yun Zhu,Yue Yun,Shipeng Li,Rong Gu,Xiabao Wu,Haitao Zhang,Chuntao Hong,Shaonan Ma,Guihai Chen,Chen Tian*

Main category: cs.LG

TL;DR: GLM是一个多代理的图推理系统，通过分解推理任务为专门代理、优化KV缓存管理和流水线执行，显著提升了图推理的准确性、效率和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的图推理方法存在准确性低、令牌使用过多、延迟高和吞吐量低的问题，主要由于单代理设计、重复上下文编码和低效的服务执行。

Method: 将推理分解为分类、推理、动作生成和图检索等专门代理，采用分支和选择性上下文共享，并引入图感知的KV缓存管理、基于优先级的淘汰和流水线执行。

Result: GLM将答案准确性提升高达38%，令牌成本降低95.7%，推理延迟降低90.3%，吞吐量提高15.1倍。

Conclusion: GLM通过多代理设计和优化的服务架构，实现了高效、可扩展的复杂图推理，适用于现实世界的规模化应用。

Abstract: Graph Chain-of-Thought (Graph-CoT) enables large language models (LLMs) to
perform step-by-step reasoning over graph-structured knowledge, but existing
pipelines suffer from low accuracy, excessive token usage, high latency, and
low throughput due to single-agent monolithic prompts, repeated context
re-encoding, and inefficient serving execution. We present GLM, the first
multi-agent Graph-CoT system co-designed with an optimized LLM serving
architecture. GLM decomposes reasoning into specialized agents for
classification, reasoning, action generation, and graph retrieval, enabling
branching and selective context sharing to reduce prompt length and reasoning
iterations while preserving reasoning quality, thereby improving accuracy and
reducing overall token consumption. To scale inference, we introduce a
Graph-CoT-aware LLM inference mechanism with graph-specific KV-cache
management, priority-based eviction, and pipelined execution to improve serving
efficiency. Experiments demonstrate that GLM improves answer accuracy by up to
38%, reduces token cost by up to 95.7%, lowers inference latency by 90.3%, and
achieves up to 15.1x higher throughput compared to state-of-the-art Graph-CoT
baselines, enabling efficient adoption for complex real-world reasoning at
scale.

</details>


### [192] [Bayesian Natural Gradient Fine-Tuning of CLIP Models via Kalman Filtering](https://arxiv.org/abs/2511.01694)
*Hossein Abdi,Mingfei Sun,Wei Pan*

Main category: cs.LG

TL;DR: 提出基于卡尔曼滤波的贝叶斯自然梯度下降方法，用于CLIP模型的少样本微调，在保持ID性能的同时提升OOD鲁棒性


<details>
  <summary>Details</summary>
Motivation: 解决CLIP模型在少样本微调时面临的一阶优化方法收敛慢、对超参数敏感、OOD泛化差的问题

Method: 使用卡尔曼滤波器近似自然梯度下降，结合二阶优化的曲率信息和贝叶斯推断的不确定性量化

Result: 在多个图像分类数据集上实验表明，该方法在ID性能上达到或优于SOTA基线，同时显著提升OOD鲁棒性

Conclusion: 这是首次成功将卡尔曼滤波应用于CLIP模型微调，实现了更鲁棒高效的视觉语言任务学习

Abstract: Vision-language pre-trained models, such as CLIP, have established new
benchmarks in multimodal data mining. In such models, few-shot fine-tuning is a
major challenge to achieve optimal performance on both in-distribution (ID) and
out-of-distribution (OOD) datasets, especially when labeled data is scarce.
Most existing fine-tuning approaches rely on first-order gradient-based
optimizers, which typically suffer from slow convergence, sensitivity to
step-size hyperparameters, and poor generalization in OOD settings. In
contrast, second-order methods utilize local curvature information of the loss
landscape to adjust the update step size. This is particularly beneficial for
CLIP models, whose non-convex loss functions often contain sharp critical
points. In such cases, natural gradient direction can offer more substantial
and efficient per-iteration updates when fine-tuning with limited data. Natural
Gradient Descent (NGD) is obtained by preconditioning the standard gradient
with the inverse Fisher Information Matrix (FIM), which is computationally
expensive for large models. To address this, we propose a Bayesian
approximation of NGD using a Kalman filter for CLIP models. Our method combines
the benefits of second-order optimization with Bayesian inference, which
enhances generalization while providing uncertainty quantification. Extensive
experiments conducted on diverse image classification datasets demonstrate that
our algorithm consistently achieves superior--or comparable--ID performance and
improved OOD robustness compared to state-of-the-art baselines. To the best of
our knowledge, this work represents the first successful application of Kalman
filtering to fine-tuning CLIP-based models, which enables more robust and
efficient learning in vision-language tasks.

</details>


### [193] [Edge AI in Highly Volatile Environments: Is Fairness Worth the Accuracy Trade-off?](https://arxiv.org/abs/2511.01737)
*Obaidullah Zaland,Feras M. Awaysheh,Sawsan Al Zubi,Abdul Rahman Safi,Monowar Bhuyan*

Main category: cs.LG

TL;DR: 本文研究了在高度波动的边缘环境中联邦学习的模型准确性与公平性之间的权衡关系，通过实证评估比较了不同客户端选择算法在公平性、模型性能和时间方面的表现。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在边缘智能中具有重要价值，但边缘环境的波动性和客户端异构性给实现高准确性和公平参与带来了挑战，需要研究准确性与公平性之间的权衡关系。

Method: 在三个基准数据集（CIFAR10、FashionMNIST和EMNIST）上对基于公平性的客户端选择算法（如RBFF和RBCSF）与随机和贪婪选择算法进行了广泛的实证评估。

Result: 结果表明，更公平的客户端选择算法虽然能为客户端提供稍好的参与机会，但在波动环境中会导致全局训练速度变慢。

Conclusion: 在波动边缘环境中存在公平性与性能、公平性与速度之间的权衡关系，需要进一步研究来解决公平客户端选择策略中的现有缺陷。

Abstract: Federated learning (FL) has emerged as a transformative paradigm for edge
intelligence, enabling collaborative model training while preserving data
privacy across distributed personal devices. However, the inherent volatility
of edge environments, characterized by dynamic resource availability and
heterogeneous client capabilities, poses significant challenges for achieving
high accuracy and fairness in client participation. This paper investigates the
fundamental trade-off between model accuracy and fairness in highly volatile
edge environments. This paper provides an extensive empirical evaluation of
fairness-based client selection algorithms such as RBFF and RBCSF against
random and greedy client selection regarding fairness, model performance, and
time, in three benchmarking datasets (CIFAR10, FashionMNIST, and EMNIST). This
work aims to shed light on the fairness-performance and fairness-speed
trade-offs in a volatile edge environment and explore potential future research
opportunities to address existing pitfalls in \textit{fair client selection}
strategies in FL. Our results indicate that more equitable client selection
algorithms, while providing a marginally better opportunity among clients, can
result in slower global training in volatile environments\footnote{The code for
our experiments can be found at
https://github.com/obaidullahzaland/FairFL_FLTA.

</details>


### [194] [Game-theoretic distributed learning of generative models for heterogeneous data collections](https://arxiv.org/abs/2511.01740)
*Dmitrij Schlesinger,Boris Flach*

Main category: cs.LG

TL;DR: 提出一种分布式学习方法，通过交换合成数据而非模型参数来处理异构本地模型和数据，支持不同模态的异构数据，并基于博弈论证明存在唯一纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 解决分布式学习中处理异构本地模型和数据的挑战，利用生成模型交换合成数据而非共享参数，使本地模型可作为黑箱处理。

Method: 基于博弈论将本地模型学习建模为合作博弈，使用指数族本地模型，证明存在唯一纳什均衡，支持半监督学习和不同概率空间的本地模型。

Result: 在标准基准视觉数据集上的图像分类和条件生成任务中展示了方法的优势，证明学习过程收敛到纳什均衡。

Conclusion: 通过交换合成数据处理异构分布式学习是可行的，基于博弈论的方法能有效收敛到均衡点，支持不同模态的异构数据。

Abstract: One of the main challenges in distributed learning arises from the difficulty
of handling heterogeneous local models and data. In light of the recent success
of generative models, we propose to meet this challenge by building on the idea
of exchanging synthetic data instead of sharing model parameters. Local models
can then be treated as ``black boxes'' with the ability to learn their
parameters from data and to generate data according to these parameters.
Moreover, if the local models admit semi-supervised learning, we can extend the
approach by enabling local models on different probability spaces. This allows
to handle heterogeneous data with different modalities. We formulate the
learning of the local models as a cooperative game starting from the principles
of game theory. We prove the existence of a unique Nash equilibrium for
exponential family local models and show that the proposed learning approach
converges to this equilibrium. We demonstrate the advantages of our approach on
standard benchmark vision datasets for image classification and conditional
generation.

</details>


### [195] [HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes](https://arxiv.org/abs/2511.01741)
*Ameya S. Bhave,Navnil Choudhury,Kanad Basu*

Main category: cs.LG

TL;DR: HyperNQ是首个基于超图神经网络的QLDPC解码器，通过利用超边捕获高阶稳定子约束，在伪阈值区域下比BP和GNN解码器显著提升逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 传统BP解码在短循环存在时收敛性差，而GNN解码受限于Tanner图的成对交互，无法捕获高阶相关性。

Method: 提出HyperNQ解码器，使用超图神经网络和两阶段消息传递方案，利用超边捕获高阶稳定子约束。

Result: 在伪阈值标记以下，HyperNQ比BP提升逻辑错误率84%，比GNN策略提升50%。

Conclusion: HyperNQ通过捕获高阶相关性，为QLDPC解码提供了高度表达和紧凑的解决方案，性能优于现有最先进解码器。

Abstract: Quantum computing requires effective error correction strategies to mitigate
noise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes have
emerged as a promising solution for scalable Quantum Error Correction (QEC)
applications by supporting constant-rate encoding and a sparse parity-check
structure. However, decoding QLDPC codes via traditional approaches such as
Belief Propagation (BP) suffers from poor convergence in the presence of short
cycles. Machine learning techniques like Graph Neural Networks (GNNs) utilize
learned message passing over their node features; however, they are restricted
to pairwise interactions on Tanner graphs, which limits their ability to
capture higher-order correlations. In this work, we propose HyperNQ, the first
Hypergraph Neural Network (HGNN)- based QLDPC decoder that captures
higher-order stabilizer constraints by utilizing hyperedges-thus enabling
highly expressive and compact decoding. We use a two-stage message passing
scheme and evaluate the decoder over the pseudo-threshold region. Below the
pseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84%
over BP and 50% over GNN-based strategies, demonstrating enhanced performance
over the existing state-of-the-art decoders.

</details>


### [196] [Towards Efficient Federated Learning of Networked Mixture-of-Experts for Mobile Edge Computing](https://arxiv.org/abs/2511.01743)
*Song Gao,Shusen Jing,Shuai Zhang,Yue Wang,Xiangwei Zhou,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了网络化专家混合系统（NMoE），通过客户端协同推理和联邦学习框架解决边缘设备训练大型AI模型的资源限制问题。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型在移动边缘计算中的计算资源和训练数据需求与边缘设备有限的存储和计算能力之间存在冲突，这给在边缘训练和部署LAMs带来了重大挑战。

Method: 引入网络化专家混合系统（NMoE），客户端根据专家能力将任务分发给合适的邻居进行协同推理并聚合结果；提出结合监督学习和自监督学习的联邦学习框架，平衡个性化和泛化能力。

Result: 通过广泛实验证明了所提出的NMoE系统的有效性，为NMoE训练算法提供了见解和基准。

Conclusion: NMoE系统能够有效解决边缘设备训练大型AI模型的资源限制问题，同时保持通信效率和数据隐私。

Abstract: Recent advancements in large artificial intelligence models (LAMs) are
driving significant innovations in mobile edge computing within next-generation
wireless networks. However, the substantial demands for computational resources
and large-scale training data required to train LAMs conflict with the limited
storage and computational capacity of edge devices, posing significant
challenges to training and deploying LAMs at the edge. In this work, we
introduce the Networked Mixture-of-Experts (NMoE) system, in which clients
infer collaboratively by distributing tasks to suitable neighbors based on
their expertise and aggregate the returned results. For training the NMoE, we
propose a federated learning framework that integrates both supervised and
self-supervised learning to balance personalization and generalization, while
preserving communication efficiency and data privacy. We conduct extensive
experiments to demonstrate the efficacy of the proposed NMoE system, providing
insights and benchmarks for the NMoE training algorithms.

</details>


### [197] [An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications](https://arxiv.org/abs/2511.01745)
*Mei-Chin Pang,Suraj Adhikari,Takuma Kasahara,Nagihiro Haba,Saneyuki Ohno*

Main category: cs.LG

TL;DR: OSBAD是一个用于电池应用异常检测的开源基准测试框架，通过评估15种不同算法并引入物理统计特征工程和贝叶斯优化，提升异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 电池安全在消费电子、电动汽车和飞机等应用中至关重要，未检测到的异常可能引发安全隐患或昂贵的停机时间。

Method: 提出OSBAD基准测试框架，评估15种统计、距离和无监督机器学习算法；引入物理统计特征转换工作流将集体异常分解为点异常；使用贝叶斯优化管道进行超参数调优。

Result: 在涵盖液态和固态化学电池的数据集上验证，展示了OSBAD在不同电化学系统中的跨化学泛化能力。

Conclusion: OSBAD为开发安全、可扩展和可转移的电池分析异常检测工具建立了统一基础，强调了物理统计特征工程和概率超参数调优在安全关键能源系统中的重要性。

Abstract: Battery safety is critical in applications ranging from consumer electronics
to electric vehicles and aircraft, where undetected anomalies could trigger
safety hazards or costly downtime. In this study, we present OSBAD as an
open-source benchmark for anomaly detection frameworks in battery applications.
By benchmarking 15 diverse algorithms encompassing statistical, distance-based,
and unsupervised machine-learning methods, OSBAD enables a systematic
comparison of anomaly detection methods across heterogeneous datasets. In
addition, we demonstrate how a physics- and statistics-informed feature
transformation workflow enhances anomaly separability by decomposing collective
anomalies into point anomalies. To address a major bottleneck in unsupervised
anomaly detection due to incomplete labels, we propose a Bayesian optimization
pipeline that facilitates automated hyperparameter tuning based on
transfer-learning and regression proxies. Through validation on datasets
covering both liquid and solid-state chemistries, we further demonstrate the
cross-chemistry generalization capability of OSBAD to identify irregularities
across different electrochemical systems. By making benchmarking database with
open-source reproducible anomaly detection workflows available to the
community, OSBAD establishes a unified foundation for developing safe,
scalable, and transferable anomaly detection tools in battery analytics. This
research underscores the significance of physics- and statistics-informed
feature engineering as well as model selection with probabilistic
hyperparameter tuning, in advancing trustworthy, data-driven diagnostics for
safety-critical energy systems.

</details>


### [198] [RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks](https://arxiv.org/abs/2511.01758)
*Mian Wu,Gavin Zhang,Sewon Min,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 提出RLAC方法，通过动态批评家识别最可能的失败模式，减少验证成本，提升生成质量和错误检测能力


<details>
  <summary>Details</summary>
Motivation: 开放生成任务需要满足多样化的评估标准，但大量相关标准导致验证成本过高且评估不完整，使得基于标准的强化学习难以扩展

Method: 使用LLM作为动态批评家识别最可能的失败模式，通过外部验证器验证，联合优化生成器和批评家

Result: RLAC在文本生成中提高事实准确性，在代码生成中提高正确性，优于穷举验证和奖励模型方法

Conclusion: 动态批评家比固定批评家更有效，RLAC有潜力将RL后训练扩展到自由形式生成任务

Abstract: Open-ended generation tasks require outputs to satisfy diverse and often
implicit task-specific evaluation rubrics. The sheer number of relevant rubrics
leads to prohibitively high verification costs and incomplete assessments of a
response, making reinforcement learning (RL) post-training with rubric-based
rewards difficult to scale. This problem is exacerbated by the fact that often
the best way to combine these rubrics into one single reward is also highly
prompt-specific. We propose Reinforcement Learning with Adversarial Critic
(RLAC), a post-training approach that addresses these challenges via dynamic
rubric verification. Our approach employs a large language model (LLM) as a
critic that dynamically identifies only the most likely failure modes (e.g., a
factual error or unhandled edge case), which are then verified by an external
validator to optimize both generator and critic jointly. By training both the
generator and the critic, this game enhances the critic's error detection and
the generator's output quality while reducing required verifications. Our
experiments demonstrate that RLAC improves factual accuracy in text generation
and correctness in code generation, while also outperforming exhaustive
verification and reward model methods. We show that dynamic critics are more
effective than fixed critics, showcasing the potential of RLAC for scaling RL
post-training to free-form generation tasks.

</details>


### [199] [Random Initialization of Gated Sparse Adapters](https://arxiv.org/abs/2511.01794)
*Vi Retault,Yohaï-Eliel Berreby*

Main category: cs.LG

TL;DR: 提出了RIGSA方法，通过随机初始化全秩适配器、ReZero门控和迭代幅度剪枝来解决语言模型微调中的灾难性遗忘问题，相比QLoRA在GSM8k上表现出更少的遗忘。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在新任务微调时出现的灾难性遗忘问题，即先前学习任务性能下降的普遍问题。

Method: RIGSA方法：从随机初始化的全秩适配器开始，使用ReZero类似物进行门控，并通过迭代幅度剪枝进行稀疏化。

Result: 在SmolLM2-1.7B-Instruct模型上测试，RIGSA能够学习新的Textual MNIST任务，且相比QLoRA在GSM8k上表现出更少的遗忘，与随机掩码性能相当。

Conclusion: RIGSA提供了一种不施加秩约束的稀疏适应方法，在减少灾难性遗忘方面优于QLoRA，特别是对于GSM8k任务。

Abstract: When fine-tuning language models on new tasks, catastrophic forgetting --
performance degradation on previously-learned tasks -- is a ubiquitous problem.
While Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this
through low-rank adapters, sparse adaptation offers an alternative that doesn't
impose rank constraints. We introduce Random Initialization of Gated Sparse
Adapters (RIGSA), which starts from randomly-initialized full-rank adapters,
gates them with a ReZero analog, and sparsifies them with iterative magnitude
pruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel
vision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,
and GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on
Textual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA
and random masking. In spite of having more trainable parameters than QLoRA,
the RIGSA configurations that we studied displayed less forgetting than QLoRA,
particularly on GSM8k, though it performs comparably to random masking.

</details>


### [200] [Bayesian Coreset Optimization for Personalized Federated Learning](https://arxiv.org/abs/2511.01800)
*Prateek Chanda,Shrey Modi,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: 提出了一种个性化核心集加权联邦学习方法，通过仅使用客户端核心集代表性数据点而非完整数据来训练，在保持理论最优性的同时显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中客户端在完整数据集上训练负担过重的问题，通过核心集选择代表性数据点来降低计算和通信开销。

Method: 个性化核心集加权联邦学习，使用核心集代表性数据点而非完整客户端数据进行训练更新，并分析权重和样本大小对泛化误差的影响。

Result: 理论分析显示平均泛化误差达到极小极大最优，实验在多个基准数据集和医疗数据集上相比随机采样和其他子模优化方法有显著性能提升。

Conclusion: 智能选择训练样本能有效提升联邦学习性能，个性化核心集方法在保持理论最优性的同时降低了计算负担。

Abstract: In a distributed machine learning setting like Federated Learning where there
are multiple clients involved which update their individual weights to a single
central server, often training on the entire individual client's dataset for
each client becomes cumbersome. To address this issue we propose $\methodprop$:
a personalized coreset weighted federated learning setup where the training
updates for each individual clients are forwarded to the central server based
on only individual client coreset based representative data points instead of
the entire client data. Through theoretical analysis we present how the average
generalization error is minimax optimal up to logarithm bounds (upper bounded
by $\mathcal{O}(n_k^{-\frac{2 \beta}{2 \beta+\boldsymbol{\Lambda}}} \log ^{2
\delta^{\prime}}(n_k))$) and lower bounds of $\mathcal{O}(n_k^{-\frac{2
\beta}{2 \beta+\boldsymbol{\Lambda}}})$, and how the overall generalization
error on the data likelihood differs from a vanilla Federated Learning setup as
a closed form function ${\boldsymbol{\Im}}(\boldsymbol{w}, n_k)$ of the coreset
weights $\boldsymbol{w}$ and coreset sample size $n_k$. Our experiments on
different benchmark datasets based on a variety of recent personalized
federated learning architectures show significant gains as compared to random
sampling on the training data followed by federated learning, thereby
indicating how intelligently selecting such training samples can help in
performance. Additionally, through experiments on medical datasets our proposed
method showcases some gains as compared to other submodular optimization based
approaches used for subset selection on client's data.

</details>


### [201] [Dynamic Reconstruction of Ultrasound-Derived Flow Fields With Physics-Informed Neural Fields](https://arxiv.org/abs/2511.01804)
*Viraj Patel,Lisa Kreusser,Katharine Fraser*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的神经场模型，使用多尺度傅里叶特征编码从稀疏噪声超声数据中估计血流，无需真实监督即可实现去噪和修复。


<details>
  <summary>Details</summary>
Motivation: 超声血流分析对疾病诊断有价值，但超声存在深度衰减问题，传统EchoPIV技术测量血流速度面临挑战。物理信息机器学习可以增强准确性和鲁棒性，特别是在噪声或不完整数据场景下。

Method: 使用物理信息神经场模型结合多尺度傅里叶特征编码，从稀疏噪声超声数据中重建血流场，无需真实监督数据。

Result: 在合成和真实数据集上均实现了低均方误差的去噪和修复效果，与参考流场和真实流量测量结果一致。

Conclusion: 将其他成像模态中证明有效的方法应用于超声血流重建，物理信息神经场模型能够有效处理超声数据的噪声和稀疏性问题。

Abstract: Blood flow is sensitive to disease and provides insight into cardiac
function, making flow field analysis valuable for diagnosis. However, while
safer than radiation-based imaging and more suitable for patients with medical
implants, ultrasound suffers from attenuation with depth, limiting the quality
of the image. Despite advances in echocardiographic particle image velocimetry
(EchoPIV), accurately measuring blood velocity remains challenging due to the
technique's limitations and the complexity of blood flow dynamics.
Physics-informed machine learning can enhance accuracy and robustness,
particularly in scenarios where noisy or incomplete data challenge purely
data-driven approaches. We present a physics-informed neural field model with
multi-scale Fourier Feature encoding for estimating blood flow from sparse and
noisy ultrasound data without requiring ground truth supervision. We
demonstrate that this model achieves consistently low mean squared error in
denoising and inpainting both synthetic and real datasets, verified against
reference flow fields and ground truth flow rate measurements. While
physics-informed neural fields have been widely used to reconstruct medical
images, applications to medical flow reconstruction are mostly prominent in
Flow MRI. In this work, we adapt methods that have proven effective in other
imaging modalities to address the specific challenge of ultrasound-based flow
reconstruction.

</details>


### [202] [No-rank Tensor Decomposition Using Metric Learning](https://arxiv.org/abs/2511.01816)
*Maryam Bagherian*

Main category: cs.LG

TL;DR: 提出了一种基于度量学习的无秩张量分解框架，用判别性相似度优化替代传统重构目标，通过三元组损失和多样性、均匀性正则化学习数据驱动嵌入，在语义相似度与距离直接相关的特征空间中实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统张量分解方法基于重构和固定秩约束，难以捕捉高维数据中的语义结构，需要一种能直接优化语义相关性的新框架。

Method: 采用度量学习方法，通过优化三元组损失结合多样性和均匀性正则化，学习数据驱动的嵌入表示，构建距离反映语义相似度的特征空间。

Result: 在多个领域（人脸识别、脑连接分析、模拟数据）的评估中，该方法在聚类指标上显著优于PCA、t-SNE、UMAP和传统张量分解基线，且在数据稀缺场景下比基于Transformer的方法表现更好。

Conclusion: 度量学习为张量分析提供了新范式，优先考虑语义相关性而非像素级保真度，在数据稀缺场景下具有计算优势。

Abstract: Tensor decomposition faces fundamental challenges in analyzing
high-dimensional data, where traditional methods based on reconstruction and
fixed-rank constraints often fail to capture semantically meaningful
structures. This paper introduces a no-rank tensor decomposition framework
grounded in metric learning, which replaces reconstruction objectives with a
discriminative, similarity-based optimization. The proposed approach learns
data-driven embeddings by optimizing a triplet loss with diversity and
uniformity regularization, creating a feature space where distance directly
reflects semantic similarity. We provide theoretical guarantees for the
framework's convergence and establish bounds on its metric properties.
Evaluations across diverse domains --including face recognition (LFW,
Olivetti), brain connectivity analysis (ABIDE), and simulated data (galaxy
morphology, crystal structures)-- demonstrate that our method outperforms
baseline techniques, including PCA, t-SNE, UMAP, and tensor decomposition
baselines (CP and Tucker). Results show substantial improvements in clustering
metrics (Silhouette Score, Davies--Bouldin Index, Calinski--Harabasz Index,
Separation Ratio, Adjusted Rand Index, Normalized Mutual Information) and
reveal a fundamental trade-off: while metric learning optimizes global class
separation, it deliberately transforms local geometry to align with semantic
relationships. Crucially, our approach achieves superior performance with
smaller training datasets compared to transformer-based methods, offering an
efficient alternative for domains with limited labeled data. This work
establishes metric learning as a paradigm for tensor-based analysis,
prioritizing semantic relevance over pixel-level fidelity while providing
computational advantages in data-scarce scenarios.

</details>


### [203] [Machine and Deep Learning for Indoor UWB Jammer Localization](https://arxiv.org/abs/2511.01819)
*Hamed Fard,Mahsa Kholghi,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

TL;DR: 本文提出了一种基于域对抗ConvNeXt自编码器(A-CNT)的方法，用于在室内环境变化下实现鲁棒的恶意干扰源定位，解决了传统UWB定位系统在环境布局改变时性能严重下降的问题。


<details>
  <summary>Details</summary>
Motivation: UWB定位虽然能提供厘米级精度，但易受干扰攻击，且在室内环境布局变化时定位性能会严重下降。现有方法在跨房间布局的干扰源定位方面研究不足。

Method: 提出了域对抗ConvNeXt自编码器(A-CNT)，利用梯度反转层对齐不同域间的CIR特征，以减轻环境变化带来的域偏移问题。

Result: 在原始数据集上，Random Forest达到最高F1-macro分数0.95，XGBoost达到最低平均欧几里得误差20.16cm。但在修改的房间布局中，XGBoost误差增加十倍至207.99cm。A-CNT方法将误差降至34.67cm，比非对抗迁移学习提高77%，比最佳基线提高83%。

Conclusion: 域对抗特征对齐能够在环境变化的情况下实现鲁棒且可迁移的室内干扰源定位。

Abstract: Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but is
vulnerable to jamming attacks, creating security risks for asset tracking and
intrusion detection in smart buildings. Although machine learning (ML) and deep
learning (DL) methods have improved tag localization, localizing malicious
jammers within a single room and across changing indoor layouts remains largely
unexplored. Two novel UWB datasets, collected under original and modified room
configurations, are introduced to establish comprehensive ML/DL baselines.
Performance is rigorously evaluated using a variety of classification and
regression metrics. On the source dataset with the collected UWB features,
Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achieves
the lowest mean Euclidean error of 20.16 cm. However, deploying these
source-trained models in the modified room layout led to severe performance
degradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99
cm, demonstrating significant domain shift. To mitigate this degradation, a
domain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages a
gradient-reversal layer to align CIR-derived features across domains. The A-CNT
framework restores localization performance by reducing the mean Euclidean
error to 34.67 cm. This represents a 77 percent improvement over
non-adversarial transfer learning and an 83 percent improvement over the best
baseline, restoring the fraction of samples within 30 cm to 0.56. Overall, the
results demonstrate that adversarial feature alignment enables robust and
transferable indoor jammer localization despite environmental changes. Code and
dataset available at https://github.com/afbf4c8996f/Jammer-Loc

</details>


### [204] [Towards Multi-Fidelity Scaling Laws of Neural Surrogates in CFD](https://arxiv.org/abs/2511.01830)
*Paul Setinek,Gianluca Galletti,Johannes Brandstetter*

Main category: cs.LG

TL;DR: 本文研究了科学机器学习中多保真度数据集的经验缩放规律，通过调整建模假设和近似来平衡数据保真度与计算成本，揭示了计算预算与数据集组成之间的关系。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习通常受限于通过数值模拟生成训练数据的高昂成本，而通过调整建模假设可以在模拟保真度和计算成本之间进行权衡，这是其他领域所不具备的特点。

Method: 使用低保真度和高保真度的雷诺平均纳维-斯托克斯(RANS)模拟，重新构建经典缩放规律，将数据集轴分解为计算预算和数据集组成，研究计算-性能缩放行为。

Result: 实验揭示了计算-性能缩放行为，并显示了在给定数据集配置下预算依赖的最优保真度混合比例。

Conclusion: 这些发现为多保真度神经代理数据集提供了首个经验缩放规律研究，并为科学机器学习中计算高效的数据集生成提供了实际考虑。

Abstract: Scaling laws describe how model performance grows with data, parameters and
compute. While large datasets can usually be collected at relatively low cost
in domains such as language or vision, scientific machine learning is often
limited by the high expense of generating training data through numerical
simulations. However, by adjusting modeling assumptions and approximations,
simulation fidelity can be traded for computational cost, an aspect absent in
other domains. We investigate this trade-off between data fidelity and cost in
neural surrogates using low- and high-fidelity Reynolds-Averaged Navier-Stokes
(RANS) simulations. Reformulating classical scaling laws, we decompose the
dataset axis into compute budget and dataset composition. Our experiments
reveal compute-performance scaling behavior and exhibit budget-dependent
optimal fidelity mixes for the given dataset configuration. These findings
provide the first study of empirical scaling laws for multi-fidelity neural
surrogate datasets and offer practical considerations for compute-efficient
dataset generation in scientific machine learning.

</details>


### [205] [Dynamic Routing Between Experts: A Data-Efficient Approach to Continual Learning in Vision-Language Models](https://arxiv.org/abs/2511.01831)
*Jay Mohta,Kenan Emir Ak,Dimitrios Dimitriadis,Yan Xu,Mingwei Shen*

Main category: cs.LG

TL;DR: 提出基于路由的方法解决视觉语言模型在连续微调中的灾难性遗忘问题，无需同时访问所有任务数据，保持基础能力的同时提升专业任务性能。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在顺序微调新任务时会出现灾难性遗忘，而多任务学习需要同时访问所有数据集且计算开销随任务数量线性增长。

Method: 采用基于路由的方法，在InternVL-2模型（2B和8B参数）上实现新任务集成，同时保持预训练获得的基础知识。

Result: 路由方法在ChartQA、MMBench、DocVQA等通用基准上保持性能，同时提升专业任务准确率，无需所有任务数据并发访问。

Conclusion: 路由机制对任务数量增长具有鲁棒性，在语义相关任务上表现优异，并能实现跨模态知识迁移，优于现有持续学习方法。

Abstract: Vision-Language Models (VLMs) suffer from catastrophic forgetting when
sequentially fine-tuned on new tasks, degrading performance on previously
learned foundational and task-specific capabilities. While multi-task learning
can mitigate forgetting, it requires simultaneous access to all datasets and
imposes computational overhead that scales linearly with the number of tasks.
In this work, we introduce a routing-based approach that enables the
integration of new tasks while preserving the foundational knowledge acquired
during pretraining. We evaluate our method using InternVL-2 models (2B and 8B
parameters) and demonstrate that routing preserves the model's foundational
capabilities by maintaining performance on general-purpose benchmarks such as
ChartQA, MMBench, and DocVQA, while simultaneously improving accuracy on
specialized tasks. Importantly, our approach achieves this without requiring
concurrent access to data from all tasks, avoiding the significant
computational and data overhead associated with traditional multi-task
learning. We further conduct extensive ablation studies to evaluate the
scalability and robustness of routing-based learning, showing that the approach
is resilient to a growing number of tasks and performs particularly well when
new tasks are semantically related. Finally, we show that the routing mechanism
enables superior cross-modal transfer between language and vision capabilities,
allowing knowledge learned in one modality to enhance performance in another
capability not achieved by existing continual learning methods.

</details>


### [206] [Priors in Time: Missing Inductive Biases for Language Model Interpretability](https://arxiv.org/abs/2511.01836)
*Ekdeep Singh Lubana,Can Rager,Sai Sumedh R. Hindupur,Valerie Costa,Greta Tuckute,Oam Patel,Sonia Krishna Murthy,Thomas Fel,Daniel Wurgaft,Eric J. Bigelow,Johnny Lin,Demba Ba,Martin Wattenberg,Fernanda Viegas,Melanie Weber,Aaron Mueller*

Main category: cs.LG

TL;DR: 论文提出时间特征分析作为新的可解释性方法，挑战传统稀疏自编码器的独立性假设，通过分解语言模型激活为可预测和残差成分来捕捉时间动态。


<details>
  <summary>Details</summary>
Motivation: 现有特征提取方法假设概念是独立方向，但语言模型表示具有丰富的时间动态结构，包括概念维度增长、上下文相关相关性和非平稳性，这与稀疏自编码器的先验假设相冲突。

Method: 引入时间特征分析，将给定时间的表示分解为两部分：可从上下文推断的可预测成分，以及捕捉上下文无法解释的新信息的残差成分。

Result: 时间特征分析器能正确解析花园路径句子、识别事件边界，并更广泛地区分抽象慢变信息和新颖快变信息，而现有稀疏自编码器在上述任务中均表现出显著缺陷。

Conclusion: 研究结果强调了在设计稳健可解释性工具时需要与数据匹配的归纳偏置。

Abstract: Recovering meaningful concepts from language model activations is a central
aim of interpretability. While existing feature extraction methods aim to
identify concepts that are independent directions, it is unclear if this
assumption can capture the rich temporal structure of language. Specifically,
via a Bayesian lens, we demonstrate that Sparse Autoencoders (SAEs) impose
priors that assume independence of concepts across time, implying stationarity.
Meanwhile, language model representations exhibit rich temporal dynamics,
including systematic growth in conceptual dimensionality, context-dependent
correlations, and pronounced non-stationarity, in direct conflict with the
priors of SAEs. Taking inspiration from computational neuroscience, we
introduce a new interpretability objective -- Temporal Feature Analysis --
which possesses a temporal inductive bias to decompose representations at a
given time into two parts: a predictable component, which can be inferred from
the context, and a residual component, which captures novel information
unexplained by the context. Temporal Feature Analyzers correctly parse garden
path sentences, identify event boundaries, and more broadly delineate abstract,
slow-moving information from novel, fast-moving information, while existing
SAEs show significant pitfalls in all the above tasks. Overall, our results
underscore the need for inductive biases that match the data in designing
robust interpretability tools.

</details>


### [207] [Interpretable Machine Learning for Reservoir Water Temperatures in the U.S. Red River Basin of the South](https://arxiv.org/abs/2511.01837)
*Isabela Suaza-Sierra,Hernan A. Moreno,Luis A De la Fuente,Thomas M. Neeson*

Main category: cs.LG

TL;DR: 该研究结合可解释机器学习和符号建模，通过Kolmogorov Arnold Networks (KANs)将水库水温预测的黑盒模型转化为可解释的解析表达式，在保持高预测精度的同时揭示物理驱动因素。


<details>
  <summary>Details</summary>
Motivation: 准确预测水库水温对水资源管理、生态系统健康和气候韧性至关重要，但单纯预测无法揭示背后的物理过程，需要将数据驱动洞察转化为可解释的物理理解。

Method: 使用集成和神经网络模型（RF、XGBoost、MLP）进行预测，通过SHAP量化物理驱动因素贡献，开发KANs符号近似方法将数据驱动洞察转化为紧凑的解析表达式。

Result: 模型预测精度高（最佳RMSE=1.20°C，R²=0.97），KANs从单预测因子（7天前气温）R²=0.84提升到10个预测因子R²=0.92，深度是关键次要预测因子，降水影响有限。

Conclusion: 该框架通过KANs和可解释机器学习将黑盒模型转化为透明替代模型，在预测水库热动力学的同时增强了理解能力，平衡了简单性和准确性。

Abstract: Accurate prediction of Reservoir Water Temperature (RWT) is vital for
sustainable water management, ecosystem health, and climate resilience. Yet,
prediction alone offers limited insight into the governing physical processes.
To bridge this gap, we integrated explainable machine learning (ML) with
symbolic modeling to uncover the drivers of RWT dynamics across ten reservoirs
in the Red River Basin, USA, using over 10,000 depth-resolved temperature
profiles. We first employed ensemble and neural models, including Random Forest
(RF), Extreme Gradient Boosting (XGBoost), and Multilayer Perceptron (MLP),
achieving high predictive skill (best RMSE = 1.20 degree Celsius, R^2 = 0.97).
Using SHAP (SHapley Additive exPlanations), we quantified the contribution of
physical drivers such as air temperature, depth, wind, and lake volume,
revealing consistent patterns across reservoirs. To translate these data-driven
insights into compact analytical expressions, we developed Kolmogorov Arnold
Networks (KANs) to symbolically approximate RWT. Ten progressively complex KAN
equations were derived, improving from R^2 = 0.84 using a single predictor
(7-day antecedent air temperature) to R^2 = 0.92 with ten predictors, though
gains diminished beyond five, highlighting a balance between simplicity and
accuracy. The resulting equations, dominated by linear and rational forms,
incrementally captured nonlinear behavior while preserving interpretability.
Depth consistently emerged as a secondary but critical predictor, whereas
precipitation had limited effect. By coupling predictive accuracy with
explanatory power, this framework demonstrates how KANs and explainable ML can
transform black-box models into transparent surrogates that advance both
prediction and understanding of reservoir thermal dynamics.

</details>


### [208] [Coordinate ascent neural Kalman-MLE for state estimation](https://arxiv.org/abs/2511.01855)
*Bettina Hanlon,Angel Garcia Fernandez*

Main category: cs.LG

TL;DR: 提出一种坐标上升算法，通过最大似然估计以监督方式学习动态状态估计中的动态和测量模型，包括神经网络参数和噪声协方差矩阵，然后与非线性卡尔曼滤波器结合进行状态估计。


<details>
  <summary>Details</summary>
Motivation: 解决动态状态估计中动态和测量模型的联合学习问题，通过监督学习方式同时优化模型参数和噪声特性，提高状态估计的准确性。

Method: 使用坐标上升算法进行最大似然估计，学习高斯假设下的动态和测量模型的神经网络参数以及噪声协方差矩阵，训练后与非线性卡尔曼滤波器集成。

Result: 成功学习到动态和测量模型，能够与非线性卡尔曼滤波器有效结合，在测试阶段实现准确的状态估计。

Conclusion: 该方法能够有效联合学习动态状态估计中的关键组件，为复杂系统的状态估计提供了一种可行的监督学习框架。

Abstract: This paper presents a coordinate ascent algorithm to learn dynamic and
measurement models in dynamic state estimation using maximum likelihood
estimation in a supervised manner. In particular, the dynamic and measurement
models are assumed to be Gaussian and the algorithm learns the neural network
parameters that model the dynamic and measurement functions, and also the noise
covariance matrices. The trained dynamic and measurement models are then used
with a non-linear Kalman filter algorithm to estimate the state during the
testing phase.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [209] [Gradient Boosted Mixed Models: Flexible Joint Estimation of Mean and Variance Components for Clustered Data](https://arxiv.org/abs/2511.00217)
*Mitchell L. Prevett,Francis K. C. Hui,Zhi Yang Tho,A. H. Welsh,Anton H. Westveld*

Main category: stat.ML

TL;DR: GBMixed是一个将梯度提升扩展到混合模型的框架，通过基于似然的梯度联合估计均值和方差分量，支持非参数估计同时保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 线性混合模型依赖参数形式限制了灵活性，而梯度提升方法虽然预测准确但不支持聚类数据结构或不确定性量化。

Method: 使用基于似然的梯度联合估计均值和方差分量，通过回归树或样条等灵活基学习器建模随机效应和残差方差。

Result: 模拟和实际应用显示能准确恢复方差分量、校准预测区间，相比标准线性混合模型和非参数方法有更好的预测准确性。

Conclusion: GBMixed提供了异方差不确定性量化，支持异质性随机效应的提升，能够根据协变量调整聚类特定预测的收缩程度。

Abstract: Linear mixed models are widely used for clustered data, but their reliance on
parametric forms limits flexibility in complex and high-dimensional settings.
In contrast, gradient boosting methods achieve high predictive accuracy through
nonparametric estimation, but do not accommodate clustered data structures or
provide uncertainty quantification.
  We introduce Gradient Boosted Mixed Models (GBMixed), a framework and
algorithm that extends boosting to jointly estimate mean and variance
components via likelihood-based gradients. In addition to nonparametric mean
estimation, the method models both random effects and residual variances as
potentially covariate-dependent functions using flexible base learners such as
regression trees or splines, enabling nonparametric estimation while
maintaining interpretability.
  Simulations and real-world applications demonstrate accurate recovery of
variance components, calibrated prediction intervals, and improved predictive
accuracy relative to standard linear mixed models and nonparametric methods.
GBMixed provides heteroscedastic uncertainty quantification and introduces
boosting for heterogeneous random effects. This enables covariate-dependent
shrinkage for cluster-specific predictions to adapt between population and
cluster-level data. Under standard causal assumptions, the framework enables
estimation of heterogeneous treatment effects with reliable uncertainty
quantification.

</details>


### [210] [A Streaming Sparse Cholesky Method for Derivative-Informed Gaussian Process Surrogates Within Digital Twin Applications](https://arxiv.org/abs/2511.00366)
*Krishna Prasath Logakannan,Shridhar Vashishtha,Jacob Hochhalter,Shandian Zhe,Robert M. Kirby*

Main category: stat.ML

TL;DR: 提出了一种包含导数数据的稀疏高斯过程模型，用于数字孪生动态更新，提高预测精度并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 数字孪生需要高精度实时预测物理孪生的未来状态，传统多物理模型计算成本高，而包含导数数据的高斯过程模型能提高精度但计算复杂度剧增。

Method: 扩展高斯过程模型以包含导数数据，采用稀疏高斯过程近似来降低计算复杂度，开发了动态更新机制以在服务期间持续接收物理孪生数据。

Result: 数值实验表明，导数增强的稀疏高斯过程方法在动态数据添加后能产生改进的模型，预测精度显著提高。

Conclusion: 该方法成功应用于航空航天车辆疲劳裂纹增长的建模，证明了其在数字孪生框架中的有效性和实用性。

Abstract: Digital twins are developed to model the behavior of a specific physical
asset (or twin), and they can consist of high-fidelity physics-based models or
surrogates. A highly accurate surrogate is often preferred over multi-physics
models as they enable forecasting the physical twin future state in real-time.
To adapt to a specific physical twin, the digital twin model must be updated
using in-service data from that physical twin. Here, we extend Gaussian process
(GP) models to include derivative data, for improved accuracy, with dynamic
updating to ingest physical twin data during service. Including derivative
data, however, comes at a prohibitive cost of increased covariance matrix
dimension. We circumvent this issue by using a sparse GP approximation, for
which we develop extensions to incorporate derivatives. Numerical experiments
demonstrate that the prediction accuracy of the derivative-enhanced sparse GP
method produces improved models upon dynamic data additions. Lastly, we apply
the developed algorithm within a DT framework to model fatigue crack growth in
an aerospace vehicle.

</details>


### [211] [Accuracy estimation of neural networks by extreme value theory](https://arxiv.org/abs/2511.00490)
*Gero Junike,Marco Oesting*

Main category: stat.ML

TL;DR: 应用极值理论量化神经网络误差的大值分布，提出新的广义帕累托分布形状参数估计器


<details>
  <summary>Details</summary>
Motivation: 神经网络能够逼近紧集上的连续函数，但难以量化神经网络与目标函数之间的误差，特别是应用中相关的大误差值

Method: 应用极值理论分析误差分布，提出新的广义帕累托分布形状参数估计器来描述神经网络误差的尾部行为

Result: 数值实验表明，超过某个阈值的误差分布近似服从广义帕累托分布

Conclusion: 极值理论为量化神经网络误差提供了有效方法，特别是对于应用中重要的大误差值

Abstract: Neural networks are able to approximate any continuous function on a compact
set. However, it is not obvious how to quantify the error of the neural
network, i.e., the remaining bias between the function and the neural network.
Here, we propose the application of extreme value theory to quantify large
values of the error, which are typically relevant in applications. The
distribution of the error beyond some threshold is approximately generalized
Pareto distributed. We provide a new estimator of the shape parameter of the
Pareto distribution suitable to describe the error of neural networks.
Numerical experiments are provided.

</details>


### [212] [SOCRATES: Simulation Optimization with Correlated Replicas and Adaptive Trajectory Evaluations](https://arxiv.org/abs/2511.00685)
*Haoting Zhang,Haoxian Chen,Donglin Zhan,Hanyang Zhao,Henry Lam,Wenpin Tang,David Yao,Zeyu Zheng*

Main category: stat.ML

TL;DR: SOCRATES是一个新颖的两阶段仿真优化框架，利用大语言模型自动设计定制化优化算法。第一阶段构建真实系统的数字副本集合，第二阶段在副本上评估基线算法性能，由LLM作为元优化器分析轨迹并组合最终混合优化方案。


<details>
  <summary>Details</summary>
Motivation: 传统仿真优化方法需要人工选择算法组合，而大语言模型的出现提供了自动化利用系统结构和组合现有SO方法的新范式，以解决复杂随机系统的优化问题。

Method: 两阶段方法：第一阶段使用LLM从系统文本描述进行因果发现，构建结构骨架指导数字副本学习；第二阶段在副本集合上测试基线算法，LLM作为元优化器分析性能轨迹，迭代修订并组合自适应混合优化方案。

Result: 通过整合LLM驱动的推理和轨迹感知元优化，SOCRATES为复杂仿真优化问题创建了有效且样本高效的解决方案。

Conclusion: SOCRATES展示了LLM在自动化设计定制化仿真优化算法方面的潜力，能够生成自适应优化方案并在真实系统执行时动态更新。

Abstract: The field of simulation optimization (SO) encompasses various methods
developed to optimize complex, expensive-to-sample stochastic systems.
Established methods include, but are not limited to, ranking-and-selection for
finite alternatives and surrogate-based methods for continuous domains, with
broad applications in engineering and operations management. The recent advent
of large language models (LLMs) offers a new paradigm for exploiting system
structure and automating the strategic selection and composition of these
established SO methods into a tailored optimization procedure. This work
introduces SOCRATES (Simulation Optimization with Correlated Replicas and
Adaptive Trajectory Evaluations), a novel two-stage procedure that leverages
LLMs to automate the design of tailored SO algorithms. The first stage
constructs an ensemble of digital replicas of the real system. An LLM is
employed to implement causal discovery from a textual description of the
system, generating a structural `skeleton' that guides the sample-efficient
learning of the replicas. In the second stage, this replica ensemble is used as
an inexpensive testbed to evaluate a set of baseline SO algorithms. An LLM then
acts as a meta-optimizer, analyzing the performance trajectories of these
algorithms to iteratively revise and compose a final, hybrid optimization
schedule. This schedule is designed to be adaptive, with the ability to be
updated during the final execution on the real system when the optimization
performance deviates from expectations. By integrating LLM-driven reasoning
with LLM-assisted trajectory-aware meta-optimization, SOCRATES creates an
effective and sample-efficient solution for complex SO optimization problems.

</details>


### [213] [Perturbations in the Orthogonal Complement Subspace for Efficient Out-of-Distribution Detection](https://arxiv.org/abs/2511.00849)
*Zhexiao Huang,Weihao He,Shutao Deng,Junzhe Chen,Chao Yuan,Hongxin Wang,Changsheng Zhou*

Main category: stat.ML

TL;DR: P-OCS是一种轻量级的OOD检测方法，通过在ID特征主成分子空间的正交补空间中应用投影扰动来增强ID-OOD区分度，无需模型重训练或OOD数据。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法通常依赖高维表示来区分ID和OOD样本，但缺乏理论保证且计算成本较高。

Method: 在ID特征主成分子空间的正交补空间中应用单步投影扰动，增强ID-OOD差异同时保持ID表示几何结构。

Result: 在多种架构和数据集上的实验表明，P-OCS实现了最先进的OOD检测性能，计算成本可忽略不计。

Conclusion: P-OCS提供了一种理论可靠、计算高效的OOD检测解决方案，无需模型修改或额外数据。

Abstract: Out-of-distribution (OOD) detection is essential for deploying deep learning
models in open-world environments. Existing approaches, such as energy-based
scoring and gradient-projection methods, typically rely on high-dimensional
representations to separate in-distribution (ID) and OOD samples. We introduce
P-OCS (Perturbations in the Orthogonal Complement Subspace), a lightweight and
theoretically grounded method that operates in the orthogonal complement of the
principal subspace defined by ID features. P-OCS applies a single projected
perturbation restricted to this complementary subspace, enhancing subtle ID-OOD
distinctions while preserving the geometry of ID representations. We show that
a one-step update is sufficient in the small-perturbation regime and provide
convergence guarantees for the resulting detection score. Experiments across
multiple architectures and datasets demonstrate that P-OCS achieves
state-of-the-art OOD detection with negligible computational cost and without
requiring model retraining, access to OOD data, or changes to model
architecture.

</details>


### [214] [Binary perceptron computational gap -- a parametric fl RDT view](https://arxiv.org/abs/2511.01037)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 本文通过完全提升随机对偶理论（fl RDT）研究非对称二元感知器（ABP）的统计-计算间隙，发现随着提升层级增加，关键参数序列的现象学变化与算法阈值变化相关，最终在第五层级得到约束密度估计α≈0.7764，与聚类碎片化范围高度一致。


<details>
  <summary>Details</summary>
Motivation: 研究ABP中统计-计算间隙的特征，特别是满足性阈值αc和算法阈值αa之间的关系，探索fl RDT方法在理解ABP算法性能方面的潜力。

Method: 采用完全提升随机对偶理论（fl RDT）的参数化方法，分析不同提升层级下关键参数序列的变化模式，并将其与ABP的阈值变化联系起来。

Result: 在fl RDT的第二层级得到临界约束密度估计αc≈0.8331，随着层级提升该估计值下降，在第五层级收敛到α≈0.7764，与已知的聚类碎片化范围α∈(0.77,0.78)高度一致。

Conclusion: fl RDT方法揭示了ABP统计-计算间隙的结构特征，参数序列现象学变化与阈值变化密切相关，为理解ABP的算法性能提供了新的理论视角。

Abstract: Recent studies suggest that asymmetric binary perceptron (ABP) likely
exhibits the so-called statistical-computational gap characterized with the
appearance of two phase transitioning constraint density thresholds:
\textbf{\emph{(i)}} the \emph{satisfiability threshold} $\alpha_c$, below/above
which ABP succeeds/fails to operate as a storage memory; and
\textbf{\emph{(ii)}} \emph{algorithmic threshold} $\alpha_a$, below/above which
one can/cannot efficiently determine ABP's weight so that it operates as a
storage memory.
  We consider a particular parametric utilization of \emph{fully lifted random
duality theory} (fl RDT) [85] and study its potential ABP's algorithmic
implications. A remarkable structural parametric change is uncovered as one
progresses through fl RDT lifting levels. On the first two levels, the
so-called $\c$ sequence -- a key parametric fl RDT component -- is of the
(natural) decreasing type. A change of such phenomenology on higher levels is
then connected to the $\alpha_c$ -- $\alpha_a$ threshold change. Namely, on the
second level concrete numerical values give for the critical constraint density
$\alpha=\alpha_c\approx 0.8331$. While progressing through higher levels
decreases this estimate, already on the fifth level we observe a satisfactory
level of convergence and obtain $\alpha\approx 0.7764$. This allows to draw two
striking parallels: \textbf{\emph{(i)}} the obtained constraint density
estimate is in a remarkable agrement with range $\alpha\in (0.77,0.78)$ of
clustering defragmentation (believed to be responsible for failure of locally
improving algorithms) [17,88]; and \textbf{\emph{(ii)}} the observed change of
$\c$ sequence phenomenology closely matches the one of the negative Hopfield
model for which the existence of efficient algorithms that closely approach
similar type of threshold has been demonstrated recently [87].

</details>


### [215] [Generalized Guarantees for Variational Inference in the Presence of Even and Elliptical Symmetry](https://arxiv.org/abs/2511.01064)
*Charles C. Margossian,Lawrence K. Saul*

Main category: stat.ML

TL;DR: 本文扩展了变分推断在位置-尺度族中的对称性保证，研究了更广泛的散度族和部分对称性情况下的理论保证。


<details>
  <summary>Details</summary>
Motivation: 变分推断通常无法精确恢复目标分布，但在某些对称性条件下可以恢复关键统计量。本文旨在扩展这些对称性保证到更广泛的散度族和部分对称性场景。

Method: 通过理论分析，研究变分目标在更广泛散度族下的性质，并分析目标分布在部分坐标上具有偶对称和椭圆对称性的情况。

Result: 证明了在更广泛的散度族下，变分推断仍能恢复均值和相关矩阵；在部分对称性情况下也能获得相应的理论保证。

Conclusion: 扩展了变分推断的对称性理论保证，为贝叶斯分层模型等具有复杂几何但保留部分对称性的应用提供了理论支持。

Abstract: We extend several recent results providing symmetry-based guarantees for
variational inference (VI) with location-scale families. VI approximates a
target density~$p$ by the best match $q^*$ in a family $Q$ of tractable
distributions that in general does not contain $p$. It is known that VI can
recover key properties of $p$, such as its mean and correlation matrix, when
$p$ and $Q$ exhibit certain symmetries and $q^*$ is found by minimizing the
reverse Kullback-Leibler divergence. We extend these guarantees in two
important directions. First, we provide symmetry-based guarantees for a broader
family of divergences, highlighting the properties of variational objectives
under which VI provably recovers the mean and correlation matrix. Second, we
obtain further guarantees for VI when the target density $p$ exhibits even and
elliptical symmetries in some but not all of its coordinates. These partial
symmetries arise naturally in Bayesian hierarchical models, where the prior
induces a challenging geometry but still possesses axes of symmetry. We
illustrate these theoretical results in a number of experimental settings.

</details>


### [216] [Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes](https://arxiv.org/abs/2511.01096)
*Alex Boyd,Andrew Warrington,Taha Kass-Hout,Parminder Bhatia,Danica Xiao*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Foundational marked temporal point process (MTPP) models, such as the Hawkes
process, often use inexpressive model families in order to offer interpretable
parameterizations of event data. On the other hand, neural MTPPs models forego
this interpretability in favor of absolute predictive performance. In this
work, we present a new family MTPP models: the hyper Hawkes process (HHP),
which aims to be as flexible and performant as neural MTPPs, while retaining
interpretable aspects. To achieve this, the HHP extends the classical Hawkes
process to increase its expressivity by first expanding the dimension of the
process into a latent space, and then introducing a hypernetwork to allow time-
and data-dependent dynamics. These extensions define a highly performant MTPP
family, achieving state-of-the-art performance across a range of benchmark
tasks and metrics. Furthermore, by retaining the linearity of the recurrence,
albeit now piecewise and conditionally linear, the HHP also retains much of the
structure of the original Hawkes process, which we exploit to create direct
probes into how the model creates predictions. HHP models therefore offer both
state-of-the-art predictions, while also providing an opportunity to ``open the
box'' and inspect how predictions were generated.

</details>


### [217] [Few-Shot Multimodal Medical Imaging: A Theoretical Framework](https://arxiv.org/abs/2511.01140)
*Md Talha Mohsin,Ismail Abdulrashid*

Main category: stat.ML

TL;DR: 提出了一个统一的理论框架，用于解决医学影像在低资源条件下的学习和推理问题，通过样本复杂度分析、多模态融合和解释稳定性来构建可靠的数据高效诊断系统。


<details>
  <summary>Details</summary>
Motivation: 医学影像面临数据稀缺、系统分散、数据集不平衡等结构性障碍，导致诊断不确定性增加、模型鲁棒性降低和诊断决策偏差。现有方法缺乏在数据稀缺情况下成功或失败的理论依据。

Method: 基于PAC学习和PAC-Bayesian理论，形式化少样本条件下的学习目标，计算样本复杂度约束，分析多模态融合如何促进泛化，并提出解释稳定性度量。

Result: 建立了样本效率、不确定性量化和可解释性在统一理论框架下的联合表征，为构建可靠的数据高效诊断系统提供了理论基础。

Conclusion: 该框架为在低资源医学影像条件下构建可靠的诊断系统建立了原则性基础，通过统一理论设置共同表征样本效率、不确定性量化和可解释性。

Abstract: Medical imaging relies heavily on large, labeled datasets. But,
unfortunately, they are not always easily accessible in clinical settings.
Additionally, many practitioners often face various structural obstacles like
limited data availability, fragmented data systems, and unbalanced datasets.
These barriers often lead to the increased diagnostic uncertainty,
underrepresentation of certain conditions, reduced model robustness, and biased
diagnostic decisions. In response to these challenges, approaches such as
transfer learning, meta-learning, and multimodal fusion have made great
strides. However, they still need a solid theoretical justification for why
they succeed or fail in situations where data is scarce. To address this gap,
we propose a unified theoretical framework that characterizes learning and
inference under low-resource medical imaging conditions. We first formalize the
learning objective under few-shot conditions and compute sample complexity
constraints to estimate the smallest quantity of data needed to achieve
clinically reliable accuracy. Then based on ideas from PAC-learning and
PAC-Bayesian theory, we explain how multimodal integration encourages
generalization and quantifies uncertainty under sparse supervision. We further
propose a formal metric for explanation stability, offering interpretability
guarantees under low-data conditions. Taken together, the proposed framework
establishes a principled foundation for constructing dependable, data-efficient
diagnostic systems by jointly characterizing sample efficiency, uncertainty
quantification, and interpretability in a unified theoretical setting.

</details>


### [218] [An Interdisciplinary and Cross-Task Review on Missing Data Imputation](https://arxiv.org/abs/2511.01196)
*Jicong Fan*

Main category: stat.ML

TL;DR: 本文对缺失数据填补方法进行了系统性综述，涵盖了从经典统计方法到现代机器学习技术的各种填补方法，特别关注复杂数据类型和与下游任务的整合。


<details>
  <summary>Details</summary>
Motivation: 缺失数据是数据科学中的基础挑战，严重影响分析和决策。现有文献在不同领域间分散，需要将统计基础与现代机器学习进展进行综合连接。

Method: 系统回顾了核心概念（缺失机制、单/多重填补、不同填补目标），对填补方法进行全面分类，包括经典技术（回归、EM算法）和现代方法（低/高秩矩阵补全、深度学习模型、大语言模型），特别关注复杂数据类型的方法。

Result: 提供了填补方法的全面分类框架，分析了不同领域的问题特征，探讨了填补与下游任务的整合方式，评估了理论保证和评估指标。

Conclusion: 识别了关键挑战和未来方向，包括模型选择和超参数优化、隐私保护填补、可泛化模型的开发，为未来研究提供了路线图。

Abstract: Missing data is a fundamental challenge in data science, significantly
hindering analysis and decision-making across a wide range of disciplines,
including healthcare, bioinformatics, social science, e-commerce, and
industrial monitoring. Despite decades of research and numerous imputation
methods, the literature remains fragmented across fields, creating a critical
need for a comprehensive synthesis that connects statistical foundations with
modern machine learning advances. This work systematically reviews core
concepts-including missingness mechanisms, single versus multiple imputation,
and different imputation goals-and examines problem characteristics across
various domains. It provides a thorough categorization of imputation methods,
spanning classical techniques (e.g., regression, the EM algorithm) to modern
approaches like low-rank and high-rank matrix completion, deep learning models
(autoencoders, GANs, diffusion models, graph neural networks), and large
language models. Special attention is given to methods for complex data types,
such as tensors, time series, streaming data, graph-structured data,
categorical data, and multimodal data. Beyond methodology, we investigate the
crucial integration of imputation with downstream tasks like classification,
clustering, and anomaly detection, examining both sequential pipelines and
joint optimization frameworks. The review also assesses theoretical guarantees,
benchmarking resources, and evaluation metrics. Finally, we identify critical
challenges and future directions, emphasizing model selection and
hyperparameter optimization, the growing importance of privacy-preserving
imputation via federated learning, and the pursuit of generalizable models that
can adapt across domains and data types, thereby outlining a roadmap for future
research.

</details>


### [219] [Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift](https://arxiv.org/abs/2511.01292)
*Samet Demir,Zafer Dogan*

Main category: stat.ML

TL;DR: 本文首次从理论和实证角度研究了注意力温度在分布偏移下对上下文学习的影响，证明了最优注意力温度的存在可以最小化泛化误差，并通过实验验证了其作为提升Transformer鲁棒性的有效机制。


<details>
  <summary>Details</summary>
Motivation: 预训练Transformer在上下文学习中的性能会在预训练与测试数据分布偏移时急剧下降，而注意力温度的调整作用在这种情况下的研究尚属空白。

Method: 使用简化的线性化softmax框架推导闭式泛化误差表达式，通过线性回归任务模拟和大规模GPT-2、LLaMA2-7B在问答基准上的实验验证理论预测。

Result: 研究证明输入协方差或标签噪声的偏移会显著损害上下文学习性能，但存在最优注意力温度可以最小化这种误差。

Conclusion: 注意力温度是提升预训练Transformer上下文学习鲁棒性的原则性强大机制，为实践中选择注意力温度提供了可操作的指导。

Abstract: Pretrained Transformers excel at in-context learning (ICL), inferring new
tasks from only a handful of examples. Yet, their ICL performance can degrade
sharply under distribution shift between pretraining and test data, a regime
increasingly common in real-world deployments. While recent empirical work
hints that adjusting the attention temperature in the softmax can enhance
Transformer performance, the attention temperature's role in ICL under
distribution shift remains unexplored. This paper provides the first
theoretical and empirical study of attention temperature for ICL under
distribution shift. Using a simplified but expressive "linearized softmax"
framework, we derive closed-form generalization error expressions and prove
that shifts in input covariance or label noise substantially impair ICL, but
that an optimal attention temperature exists which minimizes this error. We
then validate our predictions through extensive simulations on linear
regression tasks and large-scale experiments with GPT-2 and LLaMA2-7B on
question-answering benchmarks. Our results establish attention temperature as a
principled and powerful mechanism for improving the robustness of ICL in
pretrained Transformers, advancing theoretical understanding and providing
actionable guidance for selecting attention temperature in practice.

</details>


### [220] [Partial Trace-Class Bayesian Neural Networks](https://arxiv.org/abs/2511.01628)
*Arran Carter,Torben Sell*

Main category: stat.ML

TL;DR: 提出了三种部分迹类贝叶斯神经网络架构，能够在保持不确定性量化能力的同时显著减少贝叶斯参数数量，提高计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯神经网络虽然能进行严格的不确定性量化，但计算成本过高，限制了其实际应用。

Method: 基于迹类神经网络先验构建三种部分迹类贝叶斯神经网络架构，通过对神经网络参数进行排序，显著减少贝叶斯参数数量。

Result: 在数值模拟研究中验证了所提方法的优势，并在真实数据集上展示了良好性能，实现了与标准BNNs相当的不确定性量化能力，但计算速度和内存需求显著改善。

Conclusion: 该方法为神经网络提供了可靠、鲁棒且可扩展的不确定性量化解决方案，解决了标准贝叶斯神经网络计算成本过高的问题。

Abstract: Bayesian neural networks (BNNs) allow rigorous uncertainty quantification in
deep learning, but often come at a prohibitive computational cost. We propose
three different innovative architectures of partial trace-class Bayesian neural
networks (PaTraC BNNs) that enable uncertainty quantification comparable to
standard BNNs but use significantly fewer Bayesian parameters. These PaTraC
BNNs have computational and statistical advantages over standard Bayesian
neural networks in terms of speed and memory requirements. Our proposed
methodology therefore facilitates reliable, robust, and scalable uncertainty
quantification in neural networks. The three architectures build on trace-class
neural network priors which induce an ordering of the neural network
parameters, and are thus a natural choice in our framework. In a numerical
simulation study, we verify the claimed benefits, and further illustrate the
performance of our proposed methodology on a real-world dataset.

</details>


### [221] [A Proof of Learning Rate Transfer under $μ$P](https://arxiv.org/abs/2511.01734)
*Soufiane Hayou*

Main category: stat.ML

TL;DR: 该论文首次证明了在μP参数化的线性多层感知机中，学习率随宽度变化的可转移性，显示最优学习率在无限宽度下收敛到非零常数，而标准参数化和神经正切参数化则不具备此特性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络参数化方法对学习率可转移性的影响，特别关注μP参数化在无限宽度极限下是否能保持有效的特征学习能力。

Method: 使用μP参数化的线性多层感知机模型，通过理论分析和数值实验验证学习率随宽度变化的收敛行为。

Result: 在μP参数化下，最优学习率随宽度增加收敛到非零常数，实现了学习率转移；而在SP和NTP参数化下，该特性不成立。

Conclusion: μP参数化是实现学习率转移的关键因素，为无限宽度神经网络的特征学习提供了理论保证。

Abstract: We provide the first proof of learning rate transfer with width in a linear
multi-layer perceptron (MLP) parametrized with $\mu$P, a neural network
parameterization designed to ``maximize'' feature learning in the
infinite-width limit. We show that under $\mu P$, the optimal learning rate
converges to a \emph{non-zero constant} as width goes to infinity, providing a
theoretical explanation to learning rate transfer. In contrast, we show that
this property fails to hold under alternative parametrizations such as Standard
Parametrization (SP) and Neural Tangent Parametrization (NTP). We provide
intuitive proofs and support the theoretical findings with extensive empirical
results.

</details>
