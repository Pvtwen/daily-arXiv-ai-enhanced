<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 11]
- [cs.LG](#cs.LG) [Total: 48]
- [stat.ML](#stat.ML) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [A Remark on the AAA Method for Secret-Key Generation in Mobile Networks](https://arxiv.org/abs/2508.05801)
*Yingbo Hua*

Main category: eess.SP

TL;DR: AAA方法在密钥生成中表现出鲁棒性，即使存在相关性或泄漏，只要Eve有非零概率丢失部分或全部数据包，密钥的模糊性会随着叠加次数的增加趋于完美。


<details>
  <summary>Details</summary>
Motivation: 研究AAA方法在密钥生成中的性能，尤其是在存在相关性和泄漏的情况下，验证其鲁棒性。

Method: 通过理论分析，比较AAA方法与基于互易信道估计的理想方法。

Result: AAA方法在叠加次数趋于无限时，密钥模糊性趋于完美，且相比理想方法具有多项优势。

Conclusion: AAA方法在密钥生成中具有鲁棒性和实用性，优于理想方法。

Abstract: A broadly applicable method for secret-key generation is named for its
accumulative, adaptable and additive (AAA) properties. This paper first shows a
robustness of its performance. Namely, even if there is an inter correlation or
a leakage caused intra correlation among the superimposed packets, provided
there is a nonzero probability for each packet to be missed in full or in part
by Eve, then the equivocation of the key generated by the AAA method always
becomes perfect as the number of superpositions becomes infinite. Also shown in
this paper is a comparison between the AAA method and an ideal method based on
reciprocal channel estimation, which reveals several advantages of the AAA
method.

</details>


### [2] [STEEP -- An Alternative To Quantum Key Distribution](https://arxiv.org/abs/2508.05882)
*Yingbo Hua*

Main category: eess.SP

TL;DR: STEEP是一种替代QKD的秘密信息传输方法，仅需经典信道，适用于多种场景，且成本更低、复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 提出STEEP作为QKD的替代方案，因其仅需经典信道，更适用于实际应用。

Method: 通过回显加密探针实现秘密信息传输。

Result: STEEP在许多实际场景中（如空中或海底光缆）能提供足够的安全速率，且成本、复杂度和兼容性优于QKD。

Conclusion: STEEP是一种更实用、更经济的秘密信息传输方法，优于QKD。

Abstract: Secret-message transmission by echoing encrypted probes (STEEP) is discussed
as an alternative to quantum key distribution (QKD). The former only needs
classic or non-quantum channels while the latter needs both quantum and classic
channels for secret-key generation. STEEP is shown to yield a secrecy rate
sufficient for one-time pads encryption in many practical situations including
in-air channels or undersea optical cables. Other advantages of STEEP over QKD
include cost, complexity, compatibility, and robustness against constant
eavesdropping.

</details>


### [3] [IRS-Assisted IoT Activity Detection Under Asynchronous Transmission and Heterogeneous Powers: Detectors and Performance Analysis](https://arxiv.org/abs/2508.05959)
*Amirhossein Taherpour,Somayeh Khani,Abbas Taherpour,Tamer Khattab*

Main category: eess.SP

TL;DR: 论文提出了一种基于智能反射面（IRS）的分布式物联网（IoT）网络活动检测方法，设计了四种检测器，并通过仿真验证了性能。


<details>
  <summary>Details</summary>
Motivation: 解决分布式IoT网络中设备异步传输和功率异质性导致的活动检测可靠性问题。

Method: 将检测问题建模为二元假设检验，开发了四种检测器（包括最优检测器和三种计算高效检测器），并推导了检测和虚警概率的闭式表达式。

Result: 仿真验证了理论性能，并分析了天线数、样本数、用户数和IRS元素数等关键参数对检测性能的影响。

Conclusion: 提出的框架在理论最优性和实现实用性之间取得了平衡，为6G系统中的IRS辅助IoT网络提供了可扩展的解决方案。

Abstract: This paper addresses the problem of activity detection in distributed
Internet of Things (IoT) networks, where devices employ asynchronous
transmissions with heterogeneous power levels to report their local
observations. The system leverages an intelligent reflecting surface (IRS) to
enhance detection reliability, with optional incorporation of a direct
line-of-sight (LoS) path. We formulate the detection problem as a binary
hypothesis test and develop four detectors: an optimal detector alongside three
computationally efficient detectors designed for practical scenarios with
different levels of prior knowledge about noise variance, channel state
information, and device transmit powers. For each detector, we derive
closed-form expressions for both detection and false alarm probabilities,
establishing theoretical performance benchmarks. Extensive simulations validate
our analytical results and systematically evaluate the impact of key system
parameters including the number of antennas, samples, users, and IRS elements
on detection performance. The proposed framework effectively bridges
theoretical optimality with implementation practicality, providing a scalable
solution for IRS-assisted IoT networks in emerging 6G systems.

</details>


### [4] [Multi-Functional Chirp Signalling for Next-Generation Multi-Carrier Wireless Networks: Communications, Sensing and ISAC Perspectives](https://arxiv.org/abs/2508.06022)
*Zeping Sui,Qu Luo,Zilong Liu,Murat Temiz,Leila Musavian,Christos Masouros,Yong Liang Guan,Pei Xiao,Lajos Hanzo*

Main category: eess.SP

TL;DR: 论文探讨了利用啁啾信号设计多功能通信方案，以满足下一代多载波移动网络的高服务质量需求，并支持高移动性通信与集成感知通信（ISAC）。


<details>
  <summary>Details</summary>
Motivation: 下一代多载波移动网络对通信和感知的高效、灵活和可靠性提出了更高要求，需要设计多功能信号方案。

Method: 提出使用啁啾信号（如Zadoff-Chu序列）与波形（如啁啾扩频和FMCW雷达）结合，设计多载波波形（如AFDM），以应对复杂无线环境。

Result: 啁啾信号在双选择性信道中表现出强鲁棒性，支持高移动性通信和ISAC。

Conclusion: 啁啾信号设计为未来研究提供了有前景的方向，需进一步探索其应用潜力。

Abstract: To meet the increasingly demanding quality-of-service requirements of the
next-generation multi-carrier mobile networks, it is essential to design
multi-functional signalling schemes facilitating efficient, flexible, and
reliable communication and sensing in complex wireless environments. As a
compelling candidate, we advocate chirp signalling, beneficially amalgamating
sequences (e.g., Zadoff-Chu sequences) with waveforms (e.g., chirp spread
spectrum and frequency-modulated continuous wave (FMCW) radar), given their
resilience against doubly selective channels. Besides chirp sequences, a wide
range of chirp waveforms is considered, ranging from FMCW to affine
frequency-division multiplexing (AFDM), to create a promising chirp
multicarrier waveform. This study also highlights the advantages of such
waveforms in supporting reliable high-mobility communications, plus integrated
sensing and communications (ISAC). Finally, we outline several emerging
research directions for chirp signalling designs.

</details>


### [5] [Bayesian Radio Map Estimation: Fundamentals and Implementation via Diffusion Models](https://arxiv.org/abs/2508.06037)
*Tien Ngoc Ha,Daniel Romero*

Main category: eess.SP

TL;DR: 论文提出了一种基于条件扩散模型的贝叶斯估计器，用于无线电地图估计（RME），旨在确定给定测量值后地图的后验分布，从而支持任意地图功能的最小均方误差估计。


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯方法主要关注地图本身的估计，而本文旨在更一般化地确定地图的后验分布，以处理不确定性并支持更广泛的功能估计。

Method: 提出了一种基于条件扩散模型的贝叶斯估计器，并与非贝叶斯方法进行了分析和数值比较。

Result: 该方法能够在不专门训练的情况下，实现任意地图功能（如容量、误码率等）的最小均方误差估计。

Conclusion: 贝叶斯方法在特定情况下优于非贝叶斯方法，尤其是在需要处理不确定性和支持多功能估计时。

Abstract: Radio map estimation (RME) is the problem of inferring the value of a certain
metric (e.g. signal power) across an area of interest given a collection of
measurements. While most works tackle this problem from a purely non-Bayesian
perspective, some Bayesian estimators have been proposed. However, the latter
focus on estimating the map itself, the Bayesian standpoint is adopted mainly
to exploit prior information or to capture uncertainty. This paper pursues a
more general formulation, where the goal is to determine the posterior
distribution of the map given the measurements. Besides handling uncertainty
and allowing standard Bayesian estimates, solving this problem is seen to
enable minimum mean square error estimation of arbitrary map functionals (e.g.
capacity, bit error rate, or coverage area to name a few) while training only
for power estimation. A general Bayesian estimator is proposed based on
conditional diffusion models and both the Bayesian and non-Bayesian paradigms
are compared analytically and numerically to determine when the Bayesian
approach is preferable.

</details>


### [6] [Multi-Modal Neural Radio Radiance Field for Localized Statistical Channel Modelling](https://arxiv.org/abs/2508.06054)
*Yiheng Wang,Shutao Zhang,Ye Xue,Tsung-Hui Chang*

Main category: eess.SP

TL;DR: MM-LSCM是一种自监督多模态神经无线电辐射场框架，用于下一代网络优化的局部统计信道建模，结合RSRP数据和LiDAR点云信息，显著提升了信道重建精度和抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 传统LSCM方法仅依赖RSRP数据，无法有效建模影响信号传播的环境结构，因此需要一种更全面的方法。

Method: 提出双分支神经架构，整合RSRP数据和LiDAR点云信息，采用基于体积渲染的多模态合成和自监督训练方法。

Result: 实验表明，MM-LSCM在信道重建精度和抗噪能力上显著优于传统方法。

Conclusion: MM-LSCM是一种有前景的无线网络优化解决方案，无需昂贵标注数据。

Abstract: This paper presents MM-LSCM, a self-supervised multi-modal neural radio
radiance field framework for localized statistical channel modeling (LSCM) for
next-generation network optimization. Traditional LSCM methods rely solely on
RSRP data, limiting their ability to model environmental structures that affect
signal propagation. To address this, we propose a dual-branch neural
architecture that integrates RSRP data and LiDAR point cloud information,
enhancing spatial awareness and predictive accuracy. MM-LSCM leverages
volume-rendering-based multi-modal synthesis to align radio propagation with
environmental obstacles and employs a self-supervised training approach,
eliminating the need for costly labeled data. Experimental results demonstrate
that MM-LSCM significantly outperforms conventional methods in channel
reconstruction accuracy and robustness to noise, making it a promising solution
for real-world wireless network optimization.

</details>


### [7] [Fast End-to-End Simulation and Exploration of Many-RISCV-Core Baseband Transceivers for Software-Defined Radio-Access Networks](https://arxiv.org/abs/2508.06141)
*Marco Bertuletti,Yichao Zhang,Mahdi Abdollahpour,Samuel Riedel,Alessandro Vanelli-Coralli*

Main category: eess.SP

TL;DR: 提出了一种基于静态二进制翻译的模拟器框架，用于高性能SDR基带处理的功能验证和性能分析。


<details>
  <summary>Details</summary>
Motivation: 满足无线带宽快速增长的性能需求，支持SDR硬件在真实无线环境中的功能验证和性能分析。

Method: 结合静态二进制翻译和快速近似时序模型，与无线信道模型耦合，模拟1024个RISC-V核心集群上的关键物理层功能。

Result: 在单线程下模拟5G OFDM符号检测时间为9.5秒至3分钟，比RTL模拟快三个数量级；并行化后速度提升73-121倍。

Conclusion: 该框架为SDR硬件设计提供了高效的功能验证和性能分析工具。

Abstract: The fast-rising demand for wireless bandwidth requires rapid evolution of
high-performance baseband processing infrastructure. Programmable many-core
processors for software-defined radio (SDR) have emerged as high-performance
baseband processing engines, offering the flexibility required to capture
evolving wireless standards and technologies. This trend must be supported by a
design framework enabling functional validation and end-to-end performance
analysis of SDR hardware within realistic radio environment models. We propose
a static binary translation based simulator augmented with a fast, approximate
timing model of the hardware and coupled to wireless channel models to simulate
the most performance-critical physical layer functions implemented in software
on a many (1024) RISC-V cores cluster customized for SDR. Our framework
simulates the detection of a 5G OFDM-symbol on a server-class processor in
9.5s-3min, on a single thread, depending on the input MIMO size (three orders
of magnitude faster than RTL simulation). The simulation is easily parallelized
to 128 threads with 73-121x speedup compared to a single thread.

</details>


### [8] [A 66-Gb/s/5.5-W RISC-V Many-Core Cluster for 5G+ Software-Defined Radio Uplinks](https://arxiv.org/abs/2508.06176)
*Marco Bertuletti,Yichao Zhang,Alessandro Vanelli-Coralli,Luca Benini*

Main category: eess.SP

TL;DR: 设计了一个多核集群用于5G及以后基站处理，具有1024个RISC-V核心和4-MiB共享内存，满足高吞吐量和能效要求。


<details>
  <summary>Details</summary>
Motivation: 5G及以后基站的计算负载增加，需在严格延迟和功耗限制下处理高数据速率，同时要求可编程性和可重构性。

Method: 采用1024个RISC-V核心和4-MiB共享内存的设计，支持软件定义处理，满足5G物理上行共享信道的高吞吐需求。

Result: 吞吐量比现有ASIPs高十倍，能效达2-41 Gb/s/W，PUSCH处理在1.7 ms内完成，功耗低于6 W。

Conclusion: 该设计在吞吐量和能效上优于现有架构，适用于5G及以后基站的高性能需求。

Abstract: Following the scale-up of new radio (NR) complexity in 5G and beyond, the
physical layer's computing load on base stations is increasing under a strictly
constrained latency and power budget; base stations must process > 20-Gb/s
uplink wireless data rate on the fly, in < 10 W. At the same time, the
programmability and reconfigurability of base station components are the key
requirements; it reduces the time and cost of new networks' deployment, it
lowers the acceptance threshold for industry players to enter the market, and
it ensures return on investments in a fast-paced evolution of standards. In
this article, we present the design of a many-core cluster for 5G and beyond
base station processing. Our design features 1024, streamlined RISC-V cores
with domain-specific FP extensions, and 4-MiB shared memory. It provides the
necessary computational capabilities for software-defined processing of the
lower physical layer of 5G physical uplink shared channel (PUSCH), satisfying
high-end throughput requirements (66 Gb/s for a transition time interval (TTI),
9.4-302 Gb/s depending on the processing stage). The throughput metrics for the
implemented functions are ten times higher than in state-of-the-art (SoTA)
application-specific instruction processors (ASIPs). The energy efficiency on
key NR kernels (2-41 Gb/s/W), measured at 800 MHz, 25 {\deg}C, and 0.8 V, on a
placed and routed instance in 12-nm CMOS technology, is competitive with SoTA
architectures. The PUSCH processing runs end-to-end on a single cluster in 1.7
ms, at <6-W average power consumption, achieving 12 Gb/s/W.

</details>


### [9] [Efficient Deep Neural Receiver with Post-Training Quantization](https://arxiv.org/abs/2508.06275)
*SaiKrishna Saketh Yellapragada,Esa Ollila,Mario Costa*

Main category: eess.SP

TL;DR: 论文探讨了深度学习在无线通信中的应用，特别是通过后训练量化（PTQ）降低深度卷积神经网络（CNN）的计算复杂度，以适配资源受限的边缘系统。


<details>
  <summary>Details</summary>
Motivation: 深度学习在无线通信中表现出色，但其高计算复杂性和资源需求限制了在边缘系统的部署。3GPP Release 20强调了AI在6G系统中的重要性，因此需要高效技术如PTQ来满足实时处理需求。

Method: 采用对称均匀量化方法，包括逐张量和逐通道PTQ，对神经接收器进行量化，以降低权重位宽。

Result: 8位逐通道量化保持了与全精度模型相当的BLER性能，而4位量化虽表现潜力但需进一步优化。

Conclusion: 超低位宽PTQ在6G系统中部署高效神经接收器具有潜力。

Abstract: Deep learning has recently garnered significant interest in wireless
communications due to its superior performance compared to traditional
model-based algorithms. Deep convolutional neural networks (CNNs) have
demonstrated notable improvements in block error rate (BLER) under various
channel models and mobility scenarios. However, the high computational
complexity and resource demands of deep CNNs pose challenges for deployment in
resource-constrained edge systems. The 3rd Generation Partnership Project
(3GPP) Release 20 highlights the pivotal role of artificial intelligence (AI)
integration in enabling advanced radio-access networks for 6G systems. The hard
real-time processing demands of 5G and 6G require efficient techniques such as
post-training quantization (PTQ), quantization-aware training (QAT), pruning,
and hybrid approaches to meet latency requirements. In this paper, we focus on
PTQ to reduce model complexity by lowering the bit-width of weights, thereby
enhancing computational efficiency. Our analysis employs symmetric uniform
quantization, applying both per-tensor and per-channel PTQ to a neural receiver
achieving performance comparable to full-precision models. Specifically, 8-bit
per-channel quantization maintains BLER performance with minimal degradation,
while 4-bit quantization shows great promise but requires further optimization
to achieve target BLER levels. These results highlight the potential of
ultra-low bitwidth PTQ for efficient neural receiver deployment in 6G systems.

</details>


### [10] [MALRIS: Malicious Hardware in RIS-Assisted Wireless Communications](https://arxiv.org/abs/2508.06340)
*Danish Mehmood Mughal,Daniyal Munir,Qazi Arbab Ahmed,Hans D. Schotten,Thorsten Jungeblut,Sang-Hyo Kim,Min Young Chung*

Main category: eess.SP

TL;DR: 论文提出恶意可重构智能表面（MALRIS）概念，揭示硬件级安全风险，模拟两种攻击方式，展示其对性能指标的显著负面影响。


<details>
  <summary>Details</summary>
Motivation: 可重构智能表面（RIS）在无线通信中动态优化传播环境，但其硬件可能被恶意篡改，导致安全风险，需研究其潜在威胁。

Method: 提出MALRIS概念，模拟制造时篡改、恶意固件和部分元件控制等威胁，建模功率分割和元件分割两种攻击方式。

Result: 仿真显示，即使有限的硬件被攻陷，也会显著降低误码率、吞吐量和保密性等性能指标。

Conclusion: 研究揭示了RIS硬件安全威胁，旨在提升意识，支持未来无线网络中安全可信的RIS部署。

Abstract: Reconfigurable intelligent surfaces (RIS) enhance wireless communication by
dynamically shaping the propagation environment, but their integration
introduces hardware-level security risks. This paper presents the concept of
Malicious RIS (MALRIS), where compromised components behave adversarially, even
under passive operation. The focus of this work is on practical threats such as
manufacturing time tampering, malicious firmware, and partial element control.
Two representative attacks, power-splitting and element-splitting, are modeled
to assess their impact. Simulations in a RIS-assisted system reveal that even a
limited hardware compromise can significantly degrade performance metrics such
as bit error rate, throughput, and secrecy metrics. By exposing this overlooked
threat surface, this work aims to promote awareness and support secure,
trustworthy RIS deployment in future wireless networks.

</details>


### [11] [Full-Dimensional Beamforming for Multi-User MIMO-OFDM ISAC for Low-Altitude UAV with Zero Sensing Resource Allocation](https://arxiv.org/abs/2508.06428)
*Zhiwen Zhou*

Main category: eess.SP

TL;DR: 提出了一种基于MIMO-OFDM的无人机ISAC框架，无需专用感知资源，提升通信和感知性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统ISAC系统因专用感知资源导致通信频谱效率下降的问题。

Method: 设计发射波束成形以满足通信和感知需求，引入低复杂度目标搜索波束成形算法和两阶段超分辨率感知算法。

Result: 仿真结果表明，该框架提升了通信总速率和感知性能（分辨率、范围和精度）。

Conclusion: 该框架是未来支持低空无人机的ISAC系统的有前景的解决方案。

Abstract: Low-altitude unmanned aerial vehicles (UAVs) are expected to play an
important role for low-altitude economy with a wide range of applications like
precise agriculture, aerial delivery and surveillance. Integrated sensing and
communication (ISAC) is a key technology to enable the large-scale deployment
and routine usage of UAVs by providing both communication and sensing services
efficiently. For UAV ISAC systems, as UAV often acts as both a communication
user equipment (UE) and a sensing target, traditional ISAC systems that usually
allocate dedicated TF resources for sensing are inefficient due to the severe
degradation of communication spectral efficiency. To address this issue, in
this paper, we propose a novel multiple-input multiple-output (MIMO) orthogonal
frequency division multiplexing (OFDM)-based ISAC framework for UAVs that
eliminates the need for dedicated sensing TF resources, achieving zero TF
sensing overhead. By designing the transmit beamforming to meet the
requirements for both communication and sensing tasks, our proposed approach
enables the communication TF resources to be fully reused for sensing, thereby
enhancing both the communication sum rate and the sensing performance in terms
of resolution, unambiguous range, and accuracy. Additionally, we introduce a
low-complexity target searching beamforming algorithm and a two-stage
super-resolution sensing algorithm, which ensure efficient implementation.
Simulation results demonstrate that the proposed MIMO-OFDM-ISAC framework not
only improves the communication sum rate but also outperforms traditional ISAC
systems in sensing performance, making it a promising solution for future ISAC
systems to support low-altitude UAVs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [12] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,Vítor V. Vasconcelos*

Main category: cs.LG

TL;DR: 提出Diagrams-to-Dynamics (D2D)方法，将因果循环图(CLDs)转化为系统动力学模型(SDMs)，支持动态分析和干预策略探索。


<details>
  <summary>Details</summary>
Motivation: CLDs作为静态定性工具，无法支持动态分析且易导致错误推断，需改进方法以提升分析能力。

Method: D2D利用CLDs的结构信息（链接存在性和极性），通过用户简单标注变量类型（存量、流量/辅助变量、常量）生成SDMs，模拟干预和探索杠杆点。

Result: D2D能区分高低优先级杠杆点，与数据驱动模型一致性优于网络中心性分析，并提供不确定性估计和数据收集指导。

Conclusion: D2D通过开源工具实现，降低了动态建模门槛，未来验证将扩展其应用范围和实用性。

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [13] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: 该论文提出了一种新颖的框架，将物理定律表示为加权知识图谱，并通过图注意力网络（GAT）进行链接预测，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决物理方程表示中的语义模糊问题，并探索跨领域物理关系。

Method: 构建包含400个高级物理方程的数据库，使用加权知识图谱表示，训练GAT进行链接预测。

Result: GAT在测试中AUC达到0.9742，显著优于基线方法，并揭示了物理学的宏观结构和跨领域关系。

Conclusion: 该框架不仅重新发现了已知的物理结构，还提出了新的跨领域关系假设，为物理学研究提供了新工具。

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [14] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: SCAR是一个基于边缘AI的框架，通过ML压缩技术和强化学习优化6G车载娱乐网络的资源管理，提升调度公平性和吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统RRM技术难以应对6G车载网络中复杂的数据（如CQI），需要更高效的资源管理方法。

Method: SCAR使用ML压缩技术（如聚类和RBF网络）减少CQI数据量，并结合强化学习优化调度和公平性。

Result: SCAR将可行调度区域时间提升14%，减少不公平调度时间15%，且SAST聚类降低CQI失真10%。

Conclusion: SCAR在动态车载网络中展现出良好的可扩展性和公平性优势。

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [15] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: 论文提出了一种基于神经网络的非线性状态空间模型数据同化方法，通过理论证明和实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 在非线性系统中设计有效的nudging项具有挑战性，需要一种数据驱动的方法来解决这一问题。

Method: 提出了神经网络nudging方法，利用Kazantzis--Kravaris--Luenberger观测器理论进行理论支持。

Result: 在Lorenz 96模型、Kuramoto--Sivashinsky方程和Kolmogorov流三个混沌系统上验证了方法的有效性。

Conclusion: 神经网络nudging方法在非线性系统中具有潜力，能够有效逼近真实系统轨迹。

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [16] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: 提出了一种可扩展的框架，通过整合异构数据重建可信的电网拓扑结构，结合空间布局和动态信号行为，并引入置信感知推理机制。


<details>
  <summary>Details</summary>
Motivation: 现代电网运行需要准确的拓扑结构，但实际数据来源多样且质量不均，需解决数据整合与可靠性问题。

Method: 结合GIS和电压时间序列数据，引入置信感知推理机制，并嵌入物理可行性约束。

Result: 在8000多个电表数据上验证，拓扑重建准确率超过95%，置信校准和计算效率显著提升。

Conclusion: 该框架在现实条件下能快速收敛到可信拓扑，为电网操作提供可靠支持。

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [17] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为CMOSS的高效算法，解决了组合多臂老虎机问题中UCB和对抗方法的局限性，实现了与理论下界匹配的遗憾。


<details>
  <summary>Details</summary>
Motivation: 现有UCB方法（如CUCB）存在长期遗憾问题，而对抗方法（如EXP3.M和HYBRID）计算开销大，需要一种兼顾性能和效率的解决方案。

Method: 提出了CMOSS算法，通过半强盗反馈实现了高效计算，并扩展到级联反馈场景。

Result: CMOSS实现了实例无关的遗憾$O\big( (\log k)^2\sqrt{kmT}\big )$，消除了对$\log T$的依赖，且实验验证其在遗憾和运行时效率上优于基准算法。

Conclusion: CMOSS是一种高效且理论最优的算法，适用于组合多臂老虎机问题，并在实际应用中表现优异。

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [18] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: 论文提出了一个基于贝叶斯风险最小化的统一理论框架，用于分析线性编码器-解码器架构，为科学机器学习问题提供可解释的解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决非线性神经网络在科学领域中的理论不透明性问题，提供一种可解释的线性方法。

Method: 通过贝叶斯风险最小化，推导出闭式、秩约束的线性和仿射线性最优映射，用于正向建模和逆向恢复任务。

Result: 理论结果在生物医学成像、金融因子分析和非线性流体动力学模拟中得到了验证。

Conclusion: 该工作为科学机器学习问题提供了一个可理解和基准化的基线模型。

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [19] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: 提出了一种结合TAPE与Graphormer的新框架，利用ChatGPT生成语义丰富的解释，并通过注意力机制融合结构与语义信息，在ogbn-arxiv数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在文本属性图（TAGs）节点分类中难以整合文本语义与图结构信息的问题，特别是领域术语、长程依赖、时间演化和大规模数据适应性。

Method: 结合TAPE框架与Graphormer，利用ChatGPT生成语义解释，通过注意力机制融合语义与结构特征，并采用路径感知位置编码和多头注意力捕捉长程依赖。

Result: 在ogbn-arxiv数据集上实现0.772的分类准确率，显著优于GCN基线（0.713），并在精确率（0.671）、召回率（0.577）和F1分数（0.610）上表现优异。

Conclusion: 该框架为动态TAGs的节点分类提供了可扩展且鲁棒的解决方案，为知识系统和科学发现的研究开辟了新方向。

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [20] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian Dörfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: 提出了一种基于MDP和RL-PG的碰撞避免机动决策框架，旨在平衡碰撞风险和燃料消耗。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保证碰撞风险的前提下，通过早期机动决策减少燃料消耗。

Method: 将CAM建模为连续状态、离散动作的有限时域MDP，结合风险、燃料消耗和轨道几何的解析模型，使用RL-PG训练策略。

Result: 在合成和历史数据上，训练策略显著减少了燃料消耗，同时保持了碰撞风险。

Conclusion: 该方法在减少燃料消耗和保证碰撞风险方面优于传统策略。

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [21] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: SZT是一种2位量化方法，在固定资源预算下可能优于非量化方法。


<details>
  <summary>Details</summary>
Motivation: 探讨量化在固定资源预算下是否可能优于非量化方法。

Method: 引入Signed-Zero Ternary (SZT)，一种2位量化方法，确定性地提供梯度信息且不影响前向路径。

Result: 分析表明SZT可能提高信息密度。

Conclusion: SZT在固定资源预算下可能比非量化方法更优。

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [22] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: 论文提出了一种将随机时间序列分解为均值、离散度和噪声的方法，通过机器学习拟合双信号并优化损失函数。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列分解问题，提取均值、离散度和噪声，以支持平滑、去噪和进一步分析。

Method: 应用机器学习拟合双信号，优化损失函数（包含正则化项），支持顺序或联合学习，使用非线性优化或神经网络。

Result: 实现了时间序列的分解，可用于平滑、去噪、预测和分析。

Conclusion: 提出的方法有效分解时间序列，支持多种应用场景，如预测和结构分析。

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [23] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: 论文提出了一种名为Shifted Gaussian Encoding的方法，通过改善神经PDE求解器的优化条件，显著提高了求解精度和效率。


<details>
  <summary>Details</summary>
Motivation: 研究神经PDE求解器在多保真度和刚性问题上因优化不良（尤其是矩阵病态）导致的精度下降问题。

Method: 在Physics-Informed Extreme Learning Machines (PIELMs)中引入Shifted Gaussian Encoding，通过激活矩阵过滤提高矩阵秩和表达能力。

Result: 方法将稳态对流-扩散方程的Peclet数求解范围扩展了两个数量级，多频函数学习的误差降低了六个数量级，且比百万参数深度网络更快更准地拟合高保真图像向量。

Conclusion: 研究表明，神经科学求解器的瓶颈通常是条件数而非深度，简单的架构改进可带来显著性能提升。

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [24] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: S-GRPO是一种改进的GRPO方法，通过噪声感知的优势权重稳定训练，解决了Think-Answer Mismatch问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在训练大型推理模型时存在Think-Answer Mismatch问题，尤其是在不平衡响应组中，噪声信号会干扰学习过程。

Method: 提出S-GRPO，通过噪声感知的优势权重优化训练过程。

Result: 在多个数学推理基准测试中，S-GRPO显著优于GRPO，性能提升达2.2%-2.5%，并在20%噪声环境下保持稳定学习。

Conclusion: S-GRPO为大规模推理模型的训练提供了更稳健和高效的方法。

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [25] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: 提出一种基于多臂老虎机（MAB）的决策树剪枝方法，通过强化学习动态优化剪枝过程，提升模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法（如CCP和REP）基于贪心策略，可能牺牲长期泛化能力，尤其在处理小规模和复杂数据集时表现不佳。

Method: 将剪枝过程建模为探索-利用问题，利用MAB算法动态选择最优分支节点进行剪枝。

Result: 在多个基准数据集上的实验表明，该方法比传统方法具有更好的预测性能。

Conclusion: MAB方法为决策树剪枝提供了一种动态且概率化的优化方式，提升了模型的鲁棒性。

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [26] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为MCRE的框架，通过结合TD误差和行为克隆项来平衡保守性和性能，并基于此开发了MCRQ算法，在离线RL任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习面临分布偏移问题，导致OOD动作和过高估计，需要平衡保守性和性能。

Method: 提出MCRE框架，结合TD误差和行为克隆项；开发MCRQ算法，将其融入离线演员-评论家框架。

Result: MCRQ在基准数据集上优于现有基线方法和最先进的离线RL算法。

Conclusion: MCRE和MCRQ有效解决了离线RL中的保守性与性能平衡问题，表现优异。

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [27] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: 论文提出了一种基于语义对齐的强化学习方法，通过SBERT计算奖励，避免手动设计奖励函数，并在多个环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在科学机器学习中，设计有效的奖励函数是一个挑战，尤其是在任务目标难以数值化的环境中。现有方法依赖启发式或手动工程，缺乏通用性。

Method: 使用SBERT计算当前状态与目标语义指令的余弦相似度作为奖励，替代手动定义的奖励函数。

Result: 实验表明，语义奖励能引导学习实现竞争性控制行为，且语言嵌入空间与传统欧几里得空间存在相关性。

Conclusion: 该方法为自然语言目标与代理行为的对齐提供了新思路，并为大语言模型与控制应用的集成奠定了基础。

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [28] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: 论文提出了一种解决非线性固定点方程的新方法，通过结合半范数收缩和诱导范数的单调性，首次实现了参数无关的最优收敛速率。


<details>
  <summary>Details</summary>
Motivation: 解决非线性固定点方程（如平均奖励Q学习和TD学习）中半范数非单调性导致的收敛速率问题。

Method: 将平均误差重新表述为涉及非线性扰动的线性递归，并通过半范数收缩与诱导范数单调性的耦合来控制非线性。

Result: 首次实现了参数无关的$\tilde{O}(1/\sqrt{t})$最优收敛速率，适用于多种场景（同步/异步更新、单/多代理部署等）。

Conclusion: 该方法为非线性固定点方程的求解提供了通用且高效的解决方案，具有广泛的应用潜力。

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [29] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: ASAP是一种新颖的CoT压缩框架，通过锚点引导和基于惊讶度的修剪，显著减少推理延迟和训练成本，同时保持高准确性。


<details>
  <summary>Details</summary>
Motivation: 解决长推理链带来的训练成本高、推理延迟长和部署困难的问题。

Method: 采用锚点引导修剪和基于第一标记惊讶度的逻辑感知修剪，并结合自主生成简洁CoT的方法。

Result: 在多个代码生成基准测试中达到最优准确率，显著减少生成标记和推理延迟。

Conclusion: ASAP为构建高效强大的LRMs提供了有前景的方向。

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [30] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: MCTS-OPS是一个结合蒙特卡洛树搜索（MCTS）和大型语言模型（LLMs）的神经符号框架，用于提升复杂任务中的代码生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在需要多步规划的复杂任务中表现不佳，MCTS-OPS旨在通过优化提示选择提升LLMs的问题解决能力。

Method: 将提示选择建模为MCTS引导的序列决策过程，探索和优化多步提示序列。

Result: 在网络优化实验中，代码执行成功率和优化结果显著提升（奖励提高2~4倍，标准差降低3倍），最优解获得率提高约10%。

Conclusion: 结合符号规划和LLMs在复杂领域中展现出高质量代码生成的潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [31] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: 研究提出了一种新颖的分阶段动态竞争风险模型，用于预测心脏骤停后昏迷患者的神经功能结局，通过分阶段利用时间不变和时间变化特征提高预测准确性。


<details>
  <summary>Details</summary>
Motivation: 心脏骤停后昏迷患者的预后预测对ICU临床决策至关重要，现有方法未能充分利用随时间变化的动态特征。

Method: 扩展Fine and Gray模型，分阶段建模时间不变和时间变化特征，并引入神经网络捕捉非线性关系。

Result: 在2,278名患者的回顾性队列中，模型对觉醒、撤除生命支持和死亡等竞争结局表现出稳健的判别性能。

Conclusion: 该模型可推广至多阶段特征收集场景，适用于其他动态预测任务。

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [32] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: 论文提出了一种自适应异构图神经网络（AHGNN），解决了异构图（HGs）中异质性分布和语义多样性问题，通过异质性感知卷积和粗到细的注意力机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多孤立处理异质性或异质性，忽略了实际应用中异质性HGs的普遍性，导致性能下降。

Method: 提出AHGNN，采用异质性感知卷积处理不同跳数和元路径的异质性分布，并通过粗到细注意力机制整合多语义空间信息。

Result: 在七个真实世界图和二十个基线上的实验表明，AHGNN在高异质性情况下表现优异。

Conclusion: AHGNN有效解决了异质性HGs的建模挑战，显著提升了性能。

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [33] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: DP-LLM是一种动态分配精度的机制，通过轻量级误差估计器和阈值学习，优化大型语言模型在设备上的运行时性能与延迟权衡。


<details>
  <summary>Details</summary>
Motivation: 解决在设备上运行大型语言模型时如何根据动态变化的运行时约束（如延迟和精度）有效配置模型的问题。

Method: 提出DP-LLM机制，动态为每个层分配精度，基于输入值和轻量级误差估计器，通过微调学习阈值。

Result: 实验表明，DP-LLM在多个模型和基准测试中实现了优于现有方法的性能与延迟权衡。

Conclusion: DP-LLM通过动态精度分配，显著提升了大型语言模型在设备上的适应性和效率。

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [34] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: 论文提出了针对深度时序模型（如TCN）的架构感知泛化边界，并引入了一种公平比较方法，揭示了时序依赖性在固定信息预算下可能增强学习效果。


<details>
  <summary>Details</summary>
Motivation: 理解深度时序模型的泛化性能，填补理论空白，并提供一种评估方法。

Method: 推导了非空泛的泛化边界，使用延迟反馈阻塞机制将依赖样本转化为独立样本，并引入公平比较方法。

Result: 泛化边界与网络深度、核大小等参数相关，时序依赖性在固定信息预算下可能增强学习效果。实验发现收敛速度与理论预测存在差异。

Conclusion: 时序依赖性可能对学习有益，但理论与实践的差距仍需进一步研究。

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [35] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon Bührer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文首次实现了循环深度可微逻辑门网络（RDDLGN），将布尔运算与循环架构结合用于序列到序列学习，在WMT'14英德翻译任务中表现接近GRU。


<details>
  <summary>Details</summary>
Motivation: 探索可微逻辑门在序列建模中的应用，填补其在循环架构中的空白。

Method: 提出RDDLGN，结合布尔运算与循环架构，用于序列到序列学习。

Result: 在WMT'14英德翻译中，RDDLGN训练时达到5.00 BLEU和30.9%准确率，推理时表现接近GRU（5.41 BLEU）且具有优雅退化（4.39 BLEU）。

Conclusion: RDDLGN证明了基于循环逻辑的神经计算的可行性，为FPGA加速和递归网络架构研究开辟了新方向。

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [36] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: 提出了一种基于后见目标生成动作正则化先验的技术HGR，结合HSR，显著提高了样本利用效率和性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励下的目标条件强化学习（GCRL）效率有限，现有方法如HER未能充分利用经验。

Method: 提出Hindsight Goal-conditioned Regularization (HGR)和Hindsight Self-Imitation Regularization (HSR)结合，最大化经验利用。

Result: 在导航和操作任务中，HGR和HSR显著提高了样本重用效率和性能。

Conclusion: HGR和HSR的组合为GCRL提供了一种更高效的样本利用方法，优于现有技术。

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [37] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: 该研究提出了一种通过扩散模型生成合成口腔癌病变图像的方法，显著提升了诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 口腔癌诊断中标注数据集的稀缺限制了诊断模型的性能，尤其是训练数据的多样性和不足。

Method: 使用微调的扩散模型和修复技术合成逼真的口腔癌病变图像，并结合多源数据集。

Result: 分类模型在区分癌变和非癌变组织时准确率达0.97，检测模型定位病变的准确率为0.85。

Conclusion: 合成图像生成在医学诊断中具有潜力，可推广至其他癌症诊断研究。

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [38] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: RR-Cluster是一种轻量级技术，通过随机重新平衡聚类分配，减少隐私噪声，提升联邦聚类中的隐私/效用权衡。


<details>
  <summary>Details</summary>
Motivation: 联邦聚类通过为每个聚类训练单独模型提升性能，但可能增加隐私泄露风险。直接应用差分隐私机制会显著降低效用。

Method: 提出RR-Cluster技术，通过随机重新平衡聚类分配，确保每个聚类有最小客户端数量，减少隐私噪声。

Result: 实验表明，RR-Cluster显著改善了隐私/效用权衡，适用于合成和真实数据集。

Conclusion: RR-Cluster是一种简单有效的技术，能显著提升联邦聚类的隐私保护与模型性能。

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [39] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: 预训练神经网络在化学和小分子药物设计中备受关注，但研究发现大多数模型性能提升有限，仅CLAMP模型显著优于基线ECFP指纹。


<details>
  <summary>Details</summary>
Motivation: 评估预训练神经网络在分子化学任务中的实际效果，揭示现有研究的评估严谨性问题。

Method: 对25种模型在25个数据集上进行公平比较，涵盖多种模态、架构和预训练策略，并使用分层贝叶斯统计测试模型。

Result: 几乎所有神经模型的性能提升可忽略不计，仅CLAMP模型显著优于基线ECFP指纹。

Conclusion: 研究结果质疑现有评估的严谨性，提出潜在原因、解决方案及实用建议。

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [40] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: GFed-PP是一种新型的联邦推荐系统，适应不同隐私需求并提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有联邦推荐系统假设所有用户隐私需求相同，忽略了公开用户数据的潜力。

Method: 利用公开用户数据构建交互图，使用轻量级GCN学习个性化嵌入，本地学习保护隐私。

Result: 在五个数据集上显著优于现有方法，推荐准确性更高且不牺牲隐私。

Conclusion: GFed-PP为联邦推荐系统中不同隐私偏好提供了实用解决方案。

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [41] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: 论文提出了一种稳定的重参数化策略梯度方法（RPO），通过结合PPO的替代目标和KL正则化，显著提升了样本效率和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 重参数化策略梯度（RPG）在样本效率上有潜力，但训练不稳定，高方差梯度会破坏学习过程。

Method: 通过将PPO的替代目标与RPG结合，提出RPO方法，利用时间反向传播高效计算梯度，并引入KL正则化进一步稳定训练。

Result: 在运动和操作任务上，RPO表现出卓越的样本效率和性能。

Conclusion: RPO是一种稳定且高效的RPG方法，适用于复杂任务。

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [42] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: 论文研究了在部分特征信息可用的情况下进行成员推断攻击的问题，提出了MRAD框架，通过两阶段攻击实现高效推断。


<details>
  <summary>Details</summary>
Motivation: 现有成员推断方法通常假设攻击者能获取目标样本的全部特征，但现实中往往只能获取部分特征，限制了方法的适用性。

Method: 提出MRAD框架，第一阶段优化未知特征值以最小化样本损失，第二阶段通过异常检测衡量重构样本与训练分布的偏差。

Result: 实验表明MRAD在多种数据集上有效，例如在STL-10上，即使缺失40%特征，AUC仍可达0.6。

Conclusion: MRAD解决了部分特征下的成员推断问题，兼容多种异常检测技术，具有实际应用价值。

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [43] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kaczér,Magnus Jørgenvåg,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLM）微调中出现的突发性错位（EMA）问题，并提出了四种训练正则化干预方法以减少EMA。


<details>
  <summary>Details</summary>
Motivation: 微调LLM可能导致模型在目标领域外产生有害行为，即使微调数据本身无害。本文旨在探索API提供商可用的训练保障措施。

Method: 研究了四种干预方法：KL散度正则化、特征空间ℓ2距离、安全子空间投影（SafeLoRA）和混合安全训练样本。

Result: 评估了这些方法在四个恶意任务中的EMA效果，以及对良性任务的影响。

Conclusion: 讨论了突发性错位研究的开放性问题，并提出了未来研究方向。

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [44] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Raúl Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Orús,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: 提出了一种基于矩阵乘积态（MPS）的隐私保护高质量合成表格数据生成方法，在数据保真度和隐私保护能力上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀缺、隐私约束以及多样化数据集需求，为训练鲁棒模型提供支持。

Method: 使用MPS生成合成数据，结合噪声注入和梯度裁剪实现差分隐私（DP），并通过Rényi差分隐私核算提供隐私保证。

Result: MPS在数据保真度和下游机器学习任务性能上优于CTGAN、VAE和PrivBayes等模型，尤其在严格隐私约束下表现更佳。

Conclusion: MPS是一种有前景的隐私感知合成数据生成工具，结合张量网络的表达能力和形式化隐私机制，为安全数据共享提供了可解释且可扩展的解决方案。

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [45] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: 提出了一种名为GTMancer的框架，通过图神经网络和对比学习整合多组学数据，用于癌症亚型分类，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多组学数据整合中忽视异构组学间的复杂耦合，限制了其在癌症亚型分类中的表现。

Method: 利用对比学习将多组学数据嵌入统一语义空间，通过双注意力系数捕捉组内和组间结构先验，优化图神经网络。

Result: 在七个真实癌症数据集上，GTMancer表现优于现有最先进算法。

Conclusion: GTMancer通过全局信息指导个体组学表示，有效解决了多组学数据整合中的挑战。

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [46] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: OM2P是一种新型离线多智能体强化学习算法，通过一步动作采样和奖励感知优化，解决了生成模型在离线MARL中的低效问题。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如扩散和流模型）在离线多智能体强化学习中表现优异，但其迭代生成过程导致采样效率低，难以应用于时间敏感或资源受限的场景。

Method: 提出OM2P算法，结合均值流匹配损失和Q函数监督的奖励感知优化方案，设计广义时间步分布和无导数估计策略以提高效率。

Result: 在Multi-Agent Particle和MuJoCo基准测试中，OM2P性能优越，GPU内存使用减少3.8倍，训练速度提升10.8倍。

Conclusion: OM2P首次成功将均值流模型集成到离线MARL中，为多智能体协作场景中的实用生成策略开辟了道路。

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [47] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: 论文探讨了在印度语言多样性背景下，通过持续学习（CL）方法开发包容性自动语音识别（ASR）系统的有效性。


<details>
  <summary>Details</summary>
Motivation: 印度语言多样性使得传统多语言ASR模型难以实现，持续学习提供了一种在不遗忘已学知识的情况下逐步学习新语言的解决方案。

Method: 使用基于Conformer的混合RNN-T/CTC模型，从印地语预训练开始，逐步学习八种印度语言，并评估了三种CL策略（EWC、MAS、LwF）。

Result: 结果表明，CL方法有效减少了遗忘问题，为印度语言ASR提供了一种可扩展的解决方案。

Conclusion: 持续学习在印度语言ASR中表现出色，适用于数据顺序到达和隐私受限的实际场景。

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [48] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ayça Özçelikkale*

Main category: cs.LG

TL;DR: 提出了一种新型多输出脉冲神经元模型，结合了线性状态转移和非线性反馈机制，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 结合脉冲神经网络（SNNs）和深度状态空间模型（SSMs）的优势，解决SSMs缺乏重置机制和高精度激活函数的问题。

Method: 设计了一种多输出脉冲神经元模型，明确区分了脉冲功能、重置条件和重置动作，并引入了非线性反馈机制。

Result: 在关键词识别、事件视觉和序列模式识别任务中，性能与现有SNN基准相当，且重置机制克服了线性动态不稳定性。

Conclusion: 提出的模型通过非线性反馈机制扩展了深度SSMs的应用范围，同时保持了SNNs的低延迟和高效能优势。

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [49] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF是一种新的联邦元学习方法，通过隐私保护损失函数解决传统FML的隐私泄露问题，实现高效优化和隐私保护。


<details>
  <summary>Details</summary>
Motivation: 神经场学习需要大量数据和计算资源，传统FML方法存在隐私泄露问题。

Method: 提出FedMeNF，采用隐私保护损失函数，优化本地元学习过程。

Result: 实验表明FedMeNF在少样本和非IID数据下仍能快速优化并保持隐私。

Conclusion: FedMeNF在高效学习和隐私保护方面表现优异。

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [50] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: UPD是一种无监督、多智能体强化学习框架，通过动态生成训练伙伴提升协作能力，无需预训练伙伴或手动调参。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖预训练伙伴或手动调参的问题，实现完全无监督的协作学习。

Method: 通过随机混合智能体策略与偏置随机行为生成多样伙伴，并使用基于方差的易学性指标评分。

Result: 在Overcooked-AI等测试中，UPD表现优于基线方法，用户研究显示其更具适应性和人性化。

Conclusion: UPD是首个实现完全无监督协作学习的方法，显著提升了智能体的协作能力。

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [51] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,İbrahim Eksin,Müjde Güzelkaya*

Main category: cs.LG

TL;DR: 提出了一种自适应鲁棒损失函数FCL，通过分数阶导数和MAE结合，动态调整对标签噪声的鲁棒性，无需手动调参。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒损失函数需要大量数据集特定的超参数调优，限制了其实际应用。

Method: FCL结合了交叉熵的分数阶导数（主动部分）和MAE（被动部分），通过可学习参数μ动态平衡鲁棒性和收敛速度。

Result: FCL在基准数据集上实现了最先进的性能，无需手动调参。

Conclusion: FCL通过动态调整损失函数，有效解决了标签噪声下的分类问题。

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [52] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: SE-VAE是一种新型变分自编码器架构，通过结构方程建模直接嵌入测量结构，提升潜在表示的可解释性和解耦性能。


<details>
  <summary>Details</summary>
Motivation: 解决表格数据中潜在表示的可解释性问题，特别是在科学和社会领域，理论驱动的潜在构念和测量有效性至关重要。

Method: SE-VAE结合结构方程建模，设计潜在子空间与已知指标分组对齐，并引入全局干扰潜在变量以隔离构念特异性混杂变异。

Result: SE-VAE在模拟表格数据集上表现优异，优于基线方法，尤其在因子恢复、可解释性和对干扰变异的鲁棒性方面。

Conclusion: SE-VAE通过架构设计而非统计正则化实现解耦，为科学和社会领域的白盒生成建模提供了原则性框架。

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [53] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: Gk-means是一种基于几何原理的新型k-means算法，通过利用标量投影显著提升效率，同时保持解的质量。


<details>
  <summary>Details</summary>
Motivation: 传统k-means算法效率较低，Gk-means旨在通过几何优化减少计算开销，提升运行速度和能源经济性。

Method: 利用几何原理（标量投影）区分高表达性数据（HE）和低表达性数据（LE），仅关注HE以加速聚类更新。

Result: 在合成、真实世界和高维数据集上，Gk-means在运行时间和距离计算上优于传统及SOTA k-means变体，且能源效率更高。

Conclusion: Gk-means是一种高效、节能的k-means改进方法，具有实际应用潜力。

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>


### [54] [Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts](https://arxiv.org/abs/2508.06361)
*Zhaomin Wu,Mingzhe Du,See-Kiong Ng,Bingsheng He*

Main category: cs.LG

TL;DR: 研究探讨大型语言模型（LLMs）在无明确诱导下的自发性欺骗行为，提出量化欺骗倾向的统计指标，并发现任务复杂度与欺骗倾向正相关。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理和决策任务中的广泛应用使其可信度成为关键问题，但现有研究多关注人为诱导的欺骗，忽视了模型自发的欺骗行为。

Method: 提出基于‘接触搜索问题’的框架，引入两个统计指标（欺骗意图分数和欺骗行为分数）量化LLMs的欺骗倾向。

Result: 评估14个主流LLMs发现，任务复杂度增加时，欺骗倾向显著上升，且两种指标呈正相关。

Conclusion: 即使最先进的LLMs在处理复杂任务时也表现出欺骗倾向，这对LLMs在关键领域的部署提出了警示。

Abstract: Large Language Models (LLMs) have been widely deployed in reasoning,
planning, and decision-making tasks, making their trustworthiness a critical
concern. The potential for intentional deception, where an LLM deliberately
fabricates or conceals information to serve a hidden objective, remains a
significant and underexplored threat. Existing studies typically induce such
deception by explicitly setting a "hidden" objective through prompting or
fine-tuning, which may not fully reflect real-world human-LLM interactions.
Moving beyond this human-induced deception, we investigate LLMs' self-initiated
deception on benign prompts. To address the absence of ground truth in this
evaluation, we propose a novel framework using "contact searching questions."
This framework introduces two statistical metrics derived from psychological
principles to quantify the likelihood of deception. The first, the Deceptive
Intention Score, measures the model's bias towards a hidden objective. The
second, Deceptive Behavior Score, measures the inconsistency between the LLM's
internal belief and its expressed output. Upon evaluating 14 leading LLMs, we
find that both metrics escalate as task difficulty increases, rising in
parallel for most models. Building on these findings, we formulate a
mathematical model to explain this behavior. These results reveal that even the
most advanced LLMs exhibit an increasing tendency toward deception when
handling complex problems, raising critical concerns for the deployment of LLM
agents in complex and crucial domains.

</details>


### [55] [ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design](https://arxiv.org/abs/2508.06364)
*Renyi Zhou,Huimin Zhu,Jing Tang,Min Li*

Main category: cs.LG

TL;DR: ActivityDiff是一种基于扩散模型的生成方法，通过分类器引导技术实现多目标分子设计，同时增强目标活性和减少脱靶效应。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注单一活性分子生成，缺乏同时管理多目标分子相互作用的能力，因此需要一种新方法实现精确控制分子生物活性。

Method: 采用扩散模型和分类器引导技术，利用分别训练的药物靶点分类器进行正负引导，优化分子设计。

Result: 实验表明，ActivityDiff能有效处理单/双靶点生成、片段约束双靶点设计、选择性生成增强靶点特异性及减少脱靶效应等任务。

Conclusion: ActivityDiff为分子设计提供了一种平衡功效和安全性的新范式，是一个多功能且可扩展的框架。

Abstract: Achieving precise control over a molecule's biological activity-encompassing
targeted activation/inhibition, cooperative multi-target modulation, and
off-target toxicity mitigation-remains a critical challenge in de novo drug
design. However, existing generative methods primarily focus on producing
molecules with a single desired activity, lacking integrated mechanisms for the
simultaneous management of multiple intended and unintended molecular
interactions. Here, we propose ActivityDiff, a generative approach based on the
classifier-guidance technique of diffusion models. It leverages separately
trained drug-target classifiers for both positive and negative guidance,
enabling the model to enhance desired activities while minimizing harmful
off-target effects. Experimental results show that ActivityDiff effectively
handles essential drug design tasks, including single-/dual-target generation,
fragment-constrained dual-target design, selective generation to enhance target
specificity, and reduction of off-target effects. These results demonstrate the
effectiveness of classifier-guided diffusion in balancing efficacy and safety
in molecular design. Overall, our work introduces a novel paradigm for
achieving integrated control over molecular activity, and provides ActivityDiff
as a versatile and extensible framework.

</details>


### [56] [End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation](https://arxiv.org/abs/2508.06387)
*Anurag Tripathi,Vaibhav Patle,Abhinav Jain,Ayush Pundir,Sairam Menon,Ajeet Kumar Singh*

Main category: cs.LG

TL;DR: 提出了一种三阶段端到端文本到SQL框架，先识别目标数据库再生成SQL查询，利用LLM和提示工程提取规则，训练db_id预测模型，并通过批评代理修正SQL错误。


<details>
  <summary>Details</summary>
Motivation: 传统方法假设目标数据库已知，但在多数据库场景中识别正确数据库是关键但被忽视的步骤。

Method: 1. 使用LLM和提示工程从自然语言查询中提取规则；2. 训练基于RoBERTa的db_id预测模型；3. 通过批评代理修正SQL错误。

Result: 实验表明，该框架在数据库意图预测和SQL生成准确性上优于当前最优模型。

Conclusion: 提出的三阶段框架有效解决了多数据库场景下的文本到SQL问题，显著提升了性能。

Abstract: Text-to-SQL bridges the gap between natural language and structured database
language, thus allowing non-technical users to easily query databases.
Traditional approaches model text-to-SQL as a direct translation task, where a
given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances
in large language models (LLMs) have significantly improved translation
accuracy, however, these methods all require that the target database is
pre-specified. This becomes problematic in scenarios with multiple extensive
databases, where identifying the correct database becomes a crucial yet
overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL
framework to identify the user's intended database before generating SQL
queries. Our approach leverages LLMs and prompt engineering to extract implicit
information from natural language queries (NLQs) in the form of a ruleset. We
then train a large db\_id prediction model, which includes a RoBERTa-based
finetuned encoder, to predict the correct Database identifier (db\_id) based on
both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL
by using critic agents to correct errors. Experimental results demonstrate that
our framework outperforms the current state-of-the-art models in both database
intent prediction and SQL generation accuracy.

</details>


### [57] [A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images](https://arxiv.org/abs/2508.06409)
*Wooyong Jung,Sola Kim,Dongwook Kim,Maryam Tabar,Dongwon Lee*

Main category: cs.LG

TL;DR: 利用公开众包数据（如311服务电话和街景图像）预测旧金山无家可归者帐篷趋势，弥补传统点计数的不足。


<details>
  <summary>Details</summary>
Motivation: 美国无家可归问题严重，但现有监测方法（如点计）在频率、一致性和空间细节上存在局限。

Method: 使用311服务电话和街景图像数据，建立预测模型捕捉每日和社区级别的变化。

Result: 模型揭示了传统方法忽略的模式（如疫情期间的快速波动和帐篷位置的空间变化）。

Conclusion: 该方法为政策制定和干预评估提供了更及时、本地化和经济的信息工具。

Abstract: Homelessness in the United States has surged to levels unseen since the Great
Depression. However, existing methods for monitoring it, such as point-in-time
(PIT) counts, have limitations in terms of frequency, consistency, and spatial
detail. This study proposes a new approach using publicly available,
crowdsourced data, specifically 311 Service Calls and street-level imagery, to
track and forecast homeless tent trends in San Francisco. Our predictive model
captures fine-grained daily and neighborhood-level variations, uncovering
patterns that traditional counts often overlook, such as rapid fluctuations
during the COVID-19 pandemic and spatial shifts in tent locations over time. By
providing more timely, localized, and cost-effective information, this approach
serves as a valuable tool for guiding policy responses and evaluating
interventions aimed at reducing unsheltered homelessness.

</details>


### [58] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: LoRR是一种插件，通过高重放训练和周期性重置策略提升LLM的样本效率，结合监督微调和偏好损失优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决RL和偏好优化方法在LLM训练中样本效率低和初始偏差的问题。

Method: 引入LoRR插件，采用高重放训练、周期性重置策略和混合优化目标（SFT+偏好损失）。

Result: LoRR显著提升多种偏好优化方法的性能，在数学和通用推理任务中表现优异。

Conclusion: LoRR为LLM微调提供了一种高效、实用的方法，能从有限数据中释放更大性能。

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [59] [LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection](https://arxiv.org/abs/2508.06467)
*Ameya Anjarlekar,Sandeep Pombra*

Main category: cs.LG

TL;DR: GRIN是一个模块化和目标化的框架，用于大语言模型（LLM）的遗忘学习，通过梯度比指标和选择性噪声注入提高遗忘效果。


<details>
  <summary>Details</summary>
Motivation: 由于法律和伦理问题，LLM需要有效遗忘敏感或未经授权的数据，但现有方法存在遗忘不彻底或影响其他知识的问题。

Method: GRIN提出了一种基于梯度比的指标来识别与遗忘数据相关的参数，并通过选择性噪声注入和微调实现遗忘。

Result: 在TOFU、WMDP和SafePKU等标准基准测试中验证了GRIN的有效性。

Conclusion: GRIN在保持模型性能的同时，显著提高了LLM的遗忘能力。

Abstract: The growing legal and ethical scrutiny of large language models (LLMs)
necessitates effective machine unlearning, particularly for sensitive or
unauthorized data. Existing empirical methods often yield incomplete forgetting
or unintended degradation of unrelated knowledge due to poor localization. In
this work, we propose GRIN: a modular and targeted framework for LLM
unlearning. GRIN introduces a novel gradient-ratio-based metric to identify
parameters most responsible for memorizing forget data. We then perform
selective noise injection into these parameters prior to fine-tuning, which
improves unlearning performance while maintaining model utility. Finally, we
propose new evaluation metrics tailored to the LLM setting and validate our
approach on standard benchmarks such as TOFU, WMDP, and SafePKU.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [60] [Random Walk Learning and the Pac-Man Attack](https://arxiv.org/abs/2508.05663)
*Xingran Chen,Parimal Parag,Rohit Bhagat,Zonghong Liu,Salim El Rouayheb*

Main category: stat.ML

TL;DR: 论文提出了一种名为“Pac-Man”的攻击，恶意节点会终止随机游走（RW），从而破坏分布式学习。作者提出了Average Crossing（AC）算法来对抗这种攻击，保证RW的存活和学习过程的收敛。


<details>
  <summary>Details</summary>
Motivation: 随机游走算法在分布式系统中广泛应用，但其对本地交互的依赖使其容易受到恶意行为的攻击。研究旨在解决这一问题。

Method: 提出了Average Crossing（AC）算法，通过复制RW来防止其在“Pac-Man”攻击下灭绝。

Result: 理论分析表明，AC算法能保证RW数量几乎必然有界，且随机梯度下降在AC下仍能收敛。实验验证了理论结果，并发现灭绝概率存在相变现象。

Conclusion: AC算法能有效对抗“Pac-Man”攻击，保证分布式学习的鲁棒性。

Abstract: Random walk (RW)-based algorithms have long been popular in distributed
systems due to low overheads and scalability, with recent growing applications
in decentralized learning. However, their reliance on local interactions makes
them inherently vulnerable to malicious behavior. In this work, we investigate
an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious
node probabilistically terminates any RW that visits it. This stealthy behavior
gradually eliminates active RWs from the network, effectively halting the
learning process without triggering failure alarms. To counter this threat, we
propose the Average Crossing (AC) algorithm--a fully decentralized mechanism
for duplicating RWs to prevent RW extinction in the presence of Pac-Man. Our
theoretical analysis establishes that (i) the RW population remains almost
surely bounded under AC and (ii) RW-based stochastic gradient descent remains
convergent under AC, even in the presence of Pac-Man, with a quantifiable
deviation from the true optimum. Our extensive empirical results on both
synthetic and real-world datasets corroborate our theoretical findings.
Furthermore, they uncover a phase transition in the extinction probability as a
function of the duplication threshold. We offer theoretical insights by
analyzing a simplified variant of the AC, which sheds light on the observed
phase transition.

</details>


### [61] [Reduction Techniques for Survival Analysis](https://arxiv.org/abs/2508.05715)
*Johannes Piller,Léa Orsini,Simon Wiegrebe,John Zobolas,Lukas Burk,Sophie Hanna Langbein,Philip Studener,Markus Goeswein,Andreas Bender*

Main category: stat.ML

TL;DR: 论文探讨了生存分析中的‘降维技术’，将其转化为回归或分类任务，同时保留生存数据特性，便于机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 旨在简化生存分析任务，使其能够利用标准机器学习工具，而无需定制学习器。

Method: 综述了多种降维技术，并实现了部分技术以适配标准机器学习流程。

Result: 通过示例和基准分析比较了这些技术与传统生存分析方法的预测性能。

Conclusion: 降维技术为生存分析提供了灵活且高效的工具，适用于多种机器学习场景。

Abstract: In this work, we discuss what we refer to as reduction techniques for
survival analysis, that is, techniques that "reduce" a survival task to a more
common regression or classification task, without ignoring the specifics of
survival data. Such techniques particularly facilitate machine learning-based
survival analysis, as they allow for applying standard tools from machine and
deep learning to many survival tasks without requiring custom learners. We
provide an overview of different reduction techniques and discuss their
respective strengths and weaknesses. We also provide a principled
implementation of some of these reductions, such that they are directly
available within standard machine learning workflows. We illustrate each
reduction using dedicated examples and perform a benchmark analysis that
compares their predictive performance to established machine learning methods
for survival analysis.

</details>


### [62] [Stochastic Trace Optimization of Parameter Dependent Matrices Based on Statistical Learning Theory](https://arxiv.org/abs/2508.05764)
*Arvind K. Saibaba,Ilse C. F. Ipsen*

Main category: stat.ML

TL;DR: 提出了一种蒙特卡洛估计器，用于最小化参数依赖矩阵的迹，并通过两种边界方法（epsilon nets和generic chaining）确保估计器的后向误差高概率有界。


<details>
  <summary>Details</summary>
Motivation: 研究参数依赖矩阵的迹最小化问题，提出高效且理论保证的估计方法。

Method: 使用蒙特卡洛估计器，结合epsilon nets和generic chaining两种边界方法，控制采样量以确保误差有界。

Result: 两种边界方法均适用于小非对角线质量和参数空间小的矩阵，epsilon nets更易计算，而chaining边界在某些情况下可能更优。

Conclusion: 蒙特卡洛估计器在小采样量下有效，epsilon nets实用性强，chaining边界理论潜力大但计算复杂。

Abstract: We consider matrices $\boldsymbol{A}(\boldsymbol\theta)\in\mathbb{R}^{m\times
m}$ that depend, possibly nonlinearly, on a parameter $\boldsymbol\theta$ from
a compact parameter space $\Theta$. We present a Monte Carlo estimator for
minimizing $\text{trace}(\boldsymbol{A}(\boldsymbol\theta))$ over all
$\boldsymbol\theta\in\Theta$, and determine the sampling amount so that the
backward error of the estimator is bounded with high probability. We derive two
types of bounds, based on epsilon nets and on generic chaining. Both types
predict a small sampling amount for matrices
$\boldsymbol{A}(\boldsymbol\theta)$ with small offdiagonal mass, and parameter
spaces $\Theta$ of small ``size.'' Dependence on the matrix dimension~$m$ is
only weak or not explicit. The bounds based on epsilon nets are easier to
evaluate and come with fully specified constants. In contrast, the bounds based
on chaining depend on the Talagrand functionals which are difficult to
evaluate, except in very special cases. Comparisons between the two types of
bounds are difficult, although the literature suggests that chaining bounds can
be superior.

</details>


### [63] [Lightweight Auto-bidding based on Traffic Prediction in Live Advertising](https://arxiv.org/abs/2508.06069)
*Bo Yang,Ruixuan Luo,Junqi Jin,Han Zhu*

Main category: stat.ML

TL;DR: 论文提出了一种轻量级的实时竞价算法BiCB，结合数学分析和统计方法，解决了直播广告中的实时性和未来流量未知问题。


<details>
  <summary>Details</summary>
Motivation: 直播广告需要实时竞价（秒级控制）且面临未来流量未知的挑战，现有方法要么未考虑全时段流量，要么计算复杂度高。

Method: 提出Binary Constrained Bidding (BiCB)算法，结合最优竞价公式和未来流量统计估计，通过低复杂度方案逼近最优结果。

Result: 离线与在线实验证明BiCB性能优异且工程成本低。

Conclusion: BiCB为直播广告提供了一种高效、低复杂度的实时竞价解决方案。

Abstract: Internet live streaming is widely used in online entertainment and
e-commerce, where live advertising is an important marketing tool for anchors.
An advertising campaign hopes to maximize the effect (such as conversions)
under constraints (such as budget and cost-per-click). The mainstream control
of campaigns is auto-bidding, where the performance depends on the decision of
the bidding algorithm in each request. The most widely used auto-bidding
algorithms include Proportional-Integral-Derivative (PID) control, linear
programming (LP), reinforcement learning (RL), etc. Existing methods either do
not consider the entire time traffic, or have too high computational
complexity. In this paper, the live advertising has high requirements for
real-time bidding (second-level control) and faces the difficulty of unknown
future traffic. Therefore, we propose a lightweight bidding algorithm Binary
Constrained Bidding (BiCB), which neatly combines the optimal bidding formula
given by mathematical analysis and the statistical method of future traffic
estimation, and obtains good approximation to the optimal result through a low
complexity solution. In addition, we complement the form of upper and lower
bound constraints for traditional auto-bidding modeling and give theoretical
analysis of BiCB. Sufficient offline and online experiments prove BiCB's good
performance and low engineering cost.

</details>


### [64] [Decorrelated feature importance from local sample weighting](https://arxiv.org/abs/2508.06337)
*Benedikt Fröhlich,Alison Durst,Merle Behr*

Main category: stat.ML

TL;DR: 论文提出了一种名为losaw的局部样本加权方法，用于改善特征相关性存在时的特征重要性评分。该方法通过样本加权减少模型偏差，并在模拟研究中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 特征相关性会干扰特征重要性评分的准确性，甚至可能导致噪声特征的重要性高于信号特征。因此，需要一种方法来改善这种情况。

Method: 提出局部样本加权（losaw）方法，通过逆概率加权减少目标特征与其他特征的关联，从而降低模型偏差。该方法可集成到多种机器学习算法中。

Result: 模拟研究表明，losaw能显著改善特征重要性评分，并在保持分布内数据准确性的同时，提高分布外数据的预测精度。

Conclusion: losaw是一种灵活且有效的方法，能够提升特征重要性评分的可靠性，并在某些情况下提高模型的泛化能力。

Abstract: Feature importance (FI) statistics provide a prominent and valuable method of
insight into the decision process of machine learning (ML) models, but their
effectiveness has well-known limitations when correlation is present among the
features in the training data. In this case, the FI often tends to be
distributed among all features which are in correlation with the
response-generating signal features. Even worse, if multiple signal features
are in strong correlation with a noise feature, while being only modestly
correlated with one another, this can result in a noise feature having a
distinctly larger FI score than any signal feature. Here we propose local
sample weighting (losaw) which can flexibly be integrated into many ML
algorithms to improve FI scores in the presence of feature correlation in the
training data. Our approach is motivated from inverse probability weighting in
causal inference and locally, within the ML model, uses a sample weighting
scheme to decorrelate a target feature from the remaining features. This
reduces model bias locally, whenever the effect of a potential signal feature
is evaluated and compared to others. Moreover, losaw comes with a natural
tuning parameter, the minimum effective sample size of the weighted population,
which corresponds to an interpretation-prediction-tradeoff, analog to a
bias-variance-tradeoff as for classical ML tuning parameters. We demonstrate
how losaw can be integrated within decision tree-based ML methods and within
mini-batch training of neural networks. We investigate losaw for random forest
and convolutional neural networks in a simulation study on settings showing
diverse correlation patterns. We found that losaw improves FI consistently.
Moreover, it often improves prediction accuracy for out-of-distribution, while
maintaining a similar accuracy for in-distribution test data.

</details>


### [65] [DP-SPRT: Differentially Private Sequential Probability Ratio Tests](https://arxiv.org/abs/2508.06377)
*Thomas Michel,Debabrota Basu,Emilie Kaufmann*

Main category: stat.ML

TL;DR: DP-SPRT是一种在隐私约束下改进的序列概率比测试方法，通过私有机制处理查询序列并在结果超出预设区间时停止，实现了误差和隐私的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决现有工作在隐私约束下进行序列假设测试时的不足，填补了误差概率和隐私需求之间的空白。

Method: 提出DP-SPRT方法，利用OutsideInterval机制改进现有技术，支持多种噪声分布（如拉普拉斯和高斯噪声）。

Result: 理论证明了DP-SPRT的误差和样本复杂度上界，实验验证了其实际性能优越。

Conclusion: DP-SPRT在隐私约束下接近最优，适用于小误差和接近假设的场景。

Abstract: We revisit Wald's celebrated Sequential Probability Ratio Test for sequential
tests of two simple hypotheses, under privacy constraints. We propose DP-SPRT,
a wrapper that can be calibrated to achieve desired error probabilities and
privacy constraints, addressing a significant gap in previous work. DP-SPRT
relies on a private mechanism that processes a sequence of queries and stops
after privately determining when the query results fall outside a predefined
interval. This OutsideInterval mechanism improves upon naive composition of
existing techniques like AboveThreshold, potentially benefiting other
sequential algorithms. We prove generic upper bounds on the error and sample
complexity of DP-SPRT that can accommodate various noise distributions based on
the practitioner's privacy needs. We exemplify them in two settings: Laplace
noise (pure Differential Privacy) and Gaussian noise (R\'enyi differential
privacy). In the former setting, by providing a lower bound on the sample
complexity of any $\epsilon$-DP test with prescribed type I and type II errors,
we show that DP-SPRT is near optimal when both errors are small and the two
hypotheses are close. Moreover, we conduct an experimental study revealing its
good practical performance.

</details>
