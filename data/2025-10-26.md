<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 21]
- [cs.LG](#cs.LG) [Total: 102]
- [stat.ML](#stat.ML) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [SSL-SE-EEG: A Framework for Robust Learning from Unlabeled EEG Data with Self-Supervised Learning and Squeeze-Excitation Networks](https://arxiv.org/abs/2510.19829)
*Meghna Roy Chowdhury,Yi Ding,Shreyas Sen*

Main category: eess.SP

TL;DR: SSL-SE-EEG框架结合自监督学习和Squeeze-and-Excitation网络，将EEG信号转换为2D图像表示，提高特征提取能力和噪声鲁棒性，减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决脑电图(EEG)在脑机接口和神经诊断中面临的噪声干扰、数据缺失和标注成本高的问题。

Method: 将EEG信号转换为结构化2D图像表示，结合自监督学习和SE-Net进行特征提取。

Result: 在多个数据集上达到最先进准确率：MindBigData 91%，TUH-AB 85%，适用于实时BCI应用。

Conclusion: SSL-SE-EEG为生物医学信号分析、神经工程和下一代BCI提供了有前景的低功耗、可扩展解决方案。

Abstract: Electroencephalography (EEG) plays a crucial role in brain-computer
interfaces (BCIs) and neurological diagnostics, but its real-world deployment
faces challenges due to noise artifacts, missing data, and high annotation
costs. We introduce SSL-SE-EEG, a framework that integrates Self-Supervised
Learning (SSL) with Squeeze-and-Excitation Networks (SE-Nets) to enhance
feature extraction, improve noise robustness, and reduce reliance on labeled
data. Unlike conventional EEG processing techniques, SSL-SE-EEG} transforms EEG
signals into structured 2D image representations, suitable for deep learning.
Experimental validation on MindBigData, TUH-AB, SEED-IV and BCI-IV datasets
demonstrates state-of-the-art accuracy (91% in MindBigData, 85% in TUH-AB),
making it well-suited for real-time BCI applications. By enabling low-power,
scalable EEG processing, SSL-SE-EEG presents a promising solution for
biomedical signal analysis, neural engineering, and next-generation BCIs.

</details>


### [2] [Low-Latency Neural Inference on an Edge Device for Real-Time Handwriting Recognition from EEG Signals](https://arxiv.org/abs/2510.19832)
*Ovishake Sen,Raghav Soni,Darpan Virmani,Akshar Parekh,Patrick Lehman,Sarthak Jena,Adithi Katikhaneni,Adam Khalifa,Baibhab Chatterjee*

Main category: eess.SP

TL;DR: 该研究开发了一种基于EEG的非侵入式脑机接口系统，通过先进的机器学习和特征提取技术，在便携边缘设备上实现了实时高精度的想象手写神经解码。


<details>
  <summary>Details</summary>
Motivation: 侵入式脑机接口(如ECoG)虽然精度高但存在手术风险，而非侵入式EEG方法虽然安全但信号质量差、解码精度低。本研究旨在克服EEG的局限性，开发便携、高精度的实时通信脑机接口。

Method: 收集15名参与者的32通道EEG数据，进行带通滤波和伪影子空间重构预处理，提取85个时域、频域和图域特征。开发EEdGeNet混合架构，结合时间卷积网络和多层感知器。

Result: 在NVIDIA Jetson TX2上部署时，系统达到89.83%的准确率，每字符延迟914.18毫秒。仅选择10个关键特征可将延迟降低4.5倍至202.6毫秒，准确率损失小于1%。

Conclusion: 该研究为准确、低延迟、完全便携的非侵入式脑机接口建立了可行路径，支持实时通信应用。

Abstract: Brain-computer interfaces (BCIs) offer a pathway to restore communication for
individuals with severe motor or speech impairments. Imagined handwriting
provides an intuitive paradigm for character-level neural decoding, bridging
the gap between human intention and digital communication. While invasive
approaches such as electrocorticography (ECoG) achieve high accuracy, their
surgical risks limit widespread adoption. Non-invasive electroencephalography
(EEG) offers safer and more scalable alternatives but suffers from low
signal-to-noise ratio and spatial resolution, constraining its decoding
precision. This work demonstrates that advanced machine learning combined with
informative EEG feature extraction can overcome these barriers, enabling
real-time, high-accuracy neural decoding on portable edge devices. A 32-channel
EEG dataset was collected from fifteen participants performing imagined
handwriting. Signals were preprocessed with bandpass filtering and artifact
subspace reconstruction, followed by extraction of 85 time-, frequency-, and
graphical-domain features. A hybrid architecture, EEdGeNet, integrates a
Temporal Convolutional Network with a multilayer perceptron trained on the
extracted features. When deployed on an NVIDIA Jetson TX2, the system achieved
89.83 percent accuracy with 914.18 ms per-character latency. Selecting only ten
key features reduced latency by 4.5 times to 202.6 ms with less than 1 percent
loss in accuracy. These results establish a pathway for accurate, low-latency,
and fully portable non-invasive BCIs supporting real-time communication.

</details>


### [3] [MATLAB-Simulated Dataset for Automatic Modulation Classification in Wireless Fading Channels](https://arxiv.org/abs/2510.19985)
*M. M. Sadman Shafi,Tasnia Siddiqua Ahona,Ashraful Islam Mridha*

Main category: eess.SP

TL;DR: 提出了一个用于无线调制分类的标记合成数据集，包含五种数字调制方案在不同信道条件下的信号特征


<details>
  <summary>Details</summary>
Motivation: 解决认知无线电、自适应通信等领域中动态信道下准确调制分类的核心挑战，特别是在没有发射器知识的情况下

Method: 使用MATLAB生成随机比特流的五种数字调制信号（BPSK、QPSK、16-QAM、64-QAM、256-QAM），通过Rayleigh和Rician衰落信道传输，并提取统计、时域、频域、谱图、谱相关和图像处理等多种特征

Result: 生成了包含10个CSV文件的数据集，覆盖两种信道类型和五种采样频率，每个调制信号包含1000个符号

Conclusion: 该数据集为调制分类、信号识别和无线通信研究中的机器学习模型开发和评估提供了有价值的基准

Abstract: Accurate modulation classification is a core challenge in cognitive radio,
adaptive communications, spectrum analysis, and related domains, especially
under dynamic channels without transmitter knowledge. To address this need,
this article presents a labeled synthetic dataset designed for wireless
modulation classification under realistic propagation scenarios. The signals
were generated in MATLAB by modulating randomly generated bitstreams using five
digital modulation schemes: BPSK, QPSK, 16-QAM, 64-QAM, and 256-QAM. These
signals were then transmitted through Rayleigh and Rician fading channels with
standardized parameters, along with additional impairments to enhance realism
and diversity. Each modulated signal contains 1000 symbols. A comprehensive set
of features was extracted from the signals, encompassing statistical,
time-domain, frequency-domain, spectrogram-based, spectral correlation-based,
and image-processing-based descriptors such as BRISK, MSER, and GLCM. The
dataset is organized into 10 CSV files covering two channel types (Rayleigh and
Rician) across five sampling frequencies: 1 MHz, 10 MHz, 100 MHz, 500 MHz, and
1 GHz. To facilitate reproducibility and encourage further experimentation, the
MATLAB scripts used for signal generation and feature extraction are also
provided. This dataset serves as a valuable benchmark for developing and
evaluating machine learning models in modulation classification, signal
identification, and wireless communication research.

</details>


### [4] [NanoHydra: Energy-Efficient Time-Series Classification at the Edge](https://arxiv.org/abs/2510.20038)
*Cristian Cioflan,Jose Fonseca,Xiaying Wang,Luca Benini*

Main category: eess.SP

TL;DR: NanoHydra是一种面向极边缘设备的轻量级时间序列分类方法，使用二进制随机卷积核提取特征，在超低功耗GAP9微控制器上实现高效分类。


<details>
  <summary>Details</summary>
Motivation: 在极边缘设备上实现时间序列分类是迈向智能传感器节点的关键步骤，这些设备需要延长电池寿命同时保持分类精度。

Method: 采用轻量级二进制随机卷积核从数据流中提取特征，利用GAP9微控制器的八核集群并行执行计算密集型任务。

Result: 在ECG5000数据集上达到94.47%的分类准确率，仅需0.33ms分类1秒ECG信号，每次推理能耗7.69uJ，比现有技术高效18倍。

Conclusion: NanoHydra适合智能可穿戴设备，可实现超过四年的设备寿命。

Abstract: Time series classification (TSC) on extreme edge devices represents a
stepping stone towards intelligent sensor nodes that preserve user privacy and
offer real-time predictions. Resource-constrained devices require efficient
TinyML algorithms that prolong the device lifetime of battery-operated devices
without compromising the classification accuracy. We introduce NanoHydra, a
TinyML TSC methodology relying on lightweight binary random convolutional
kernels to extract meaningful features from data streams. We demonstrate our
system on the ultra-low-power GAP9 microcontroller, exploiting its eight-core
cluster for the parallel execution of computationally intensive tasks. We
achieve a classification accuracy of up to 94.47% on ECG5000 dataset,
comparable with state-of-the-art works. Our efficient NanoHydra requires only
0.33 ms to accurately classify a 1-second long ECG signal. With a modest energy
consumption of 7.69 uJ per inference, 18x more efficient than the
state-of-the-art, NanoHydra is suitable for smart wearable devices, enabling a
device lifetime of over four years.

</details>


### [5] [Semantic Communication for Task Execution and Data Reconstruction in Multi-User Scenarios](https://arxiv.org/abs/2510.20067)
*Maximilian H. V. Tillmann,Avinash Kankari,Carsten Bockelmann,Armin Dekorsy*

Main category: eess.SP

TL;DR: 提出了一种用于多用户场景的语义通信系统，同时支持任务执行和数据重建，通过互信息最大化来优化这两个目标之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的语义通信系统大多专注于任务执行或数据重建单一目标，缺乏同时处理这两个目标的有效方法，特别是在多用户场景下。

Method: 将任务执行和数据重建联合目标表述为互信息最大化问题，使用SSIM损失函数考虑人类视觉感知，并通过凸组合来平衡两个目标。

Result: 在资源使用恒定的情况下，增加重建目标的权重可以在保持任务执行性能几乎不变的同时，显著改善数据重建质量。

Conclusion: 所提出的语义通信系统能够有效平衡任务执行和数据重建两个目标，为多用户场景下的语义通信提供了实用的解决方案。

Abstract: Semantic communication has gained significant attention with the advances in
machine learning. Most semantic communication works focus on either task
execution or data reconstruction, with some recent works combining the two. In
this work, we propose a semantic communication system for concurrent task
execution and data reconstruction for a multi-user scenario, which we formulate
as the maximization of mutual information. To investigate the trade-off between
the two objectives, we formulate a joint objective as a convex combination of
task execution and data reconstruction. We show that under specific
assumptions, the \ac{SSIM} loss can be obtained from the mutual information
maximization objective for data reconstruction, which takes human visual
perception into account. Furthermore, for constant resource use, we show that
by increasing the weight of the reconstruction objective up to a certain point,
the task execution performance can be kept nearly constant, while the data
reconstruction can be significantly improved.

</details>


### [6] [RIS-Aided mmWave O-RAN: Coverage Extension and User Mobility Handling](https://arxiv.org/abs/2510.20088)
*Tawfik Osman,Aditya S. Shekhawat,Abhradeep Roy,Georgios C. Trichopoulos,Ahmed Alkhateeb*

Main category: eess.SP

TL;DR: 本文设计、实现并评估了一个基于RIS的O-RAN 5G系统，在28 GHz毫米波频段工作。通过1,024元件的RIS和O-RAN E2接口实现动态控制，在室内外场景中获得了显著的信号功率增益，并开发了两种UE移动性管理算法来应对移动用户挑战。


<details>
  <summary>Details</summary>
Motivation: 利用可重构智能表面(RIS)将电磁波重定向到期望方向，以增强信号覆盖和改善用户设备的信噪比，解决毫米波通信中的覆盖和移动性挑战。

Method: 设计了32×32的1,024元件1-bit RIS，采用模块化可扩展的瓦片架构；利用O-RAN E2接口实现RIS配置的动态控制；开发了两种UE移动性管理算法，基于UE接收信号功率实时联合跟踪和调整RIS与UE波束。

Result: 室内场景获得9-20 dB的信号功率增益，室外场景获得6-18 dB增益；移动性管理算法在实时操作中成功实现了RIS和UE波束的联合跟踪与适应。

Conclusion: 研究证明了将RIS集成到O-RAN系统中的实际可行性，能够显著增强下一代蜂窝网络的覆盖范围、移动性支持和链路可靠性。

Abstract: Reconfigurable Intelligent Surfaces (RISs) can redirect electromagnetic waves
to desired directions to enhance signal coverage and/or improve signal-to-noise
ratio (SNR) at the user equipment (UE). We present the design, implementation,
and evaluation of an RIS-assisted O-RAN 5G system operating in the FR2
millimeter wave (mmWave) frequency band. We first introduce the design of 1,024
element (32 $\times$ 32) 1-bit RIS operating at the 28 GHz band, utilizing a
modular and scalable tiled architecture. Then we demonstrate how the O-RAN E2
interface can be leveraged to dynamically control RIS configurations without
modifying standard 5G signaling procedures. To evaluate the RIS-assisted 5G
system, we conducted extensive field trials in both indoor and outdoor
environments. The results of the O-RAN link coverage trials show that the
deployed RIS provides substantial received signal power gains, ranging from 9
to 20 dB and 6 to 18 dB in indoor and outdoors scenarios, respectively.
Handling UE mobility in RIS-assisted systems is challenging due to the need for
joint RIS and UE beam management. For that, we develop two UE mobility
management algorithms and evaluate them in real-time operation using the RIS
O-RAN testbed. These algorithms leverage the received signal power at the UE to
jointly track and adapt the RIS and UE beams in real time as the UE moves. The
findings draw important insights into the practical feasibility of integrating
RIS into O-RAN systems to enhance coverage, mobility support, and link
reliability in next-generation cellular networks.

</details>


### [7] [Signal Design for OTFS Dual-Functional Radar and Communications with Imperfect CSI](https://arxiv.org/abs/2510.20112)
*Borui Du,Yumeng Zhang,Christos Masouros,Bruno Clerckx*

Main category: eess.SP

TL;DR: 本文针对OTFS在雷达通信一体化中的信号设计问题，提出了结合导频符号设计和数据功率分配的优化方法，显著提升了感知和通信性能。


<details>
  <summary>Details</summary>
Motivation: OTFS技术在无线感知和通信系统中具有管理移动性的显著优势，但在雷达通信一体化中的最优信号设计尚未充分探索，需要解决导频符号设计和数据功率分配的联合优化问题。

Method: 采用交替优化框架，将模糊函数的综合旁瓣水平作为雷达指标，推导考虑信道估计误差的OTFS信道容量下界作为通信指标，最大化感知和通信指标的加权和。

Result: 与常规信号方案相比，所提信号显著扩展了感知-通信性能区域，在感知方面实现了至少9.44 dB的ISL抑制增益，在通信方面实现了4.82 dB的SINR增益。

Conclusion: 提出的信号设计方法有效提升了OTFS在雷达通信一体化中的性能，为双功能系统提供了实用的优化解决方案。

Abstract: Orthogonal time frequency space (OTFS) offers significant advantages in
managing mobility for both wireless sensing and communication systems, making
it a promising candidate for dual-functional radar-communication (DFRC).
However, the optimal signal design that fully exploits OTFS's potential in DFRC
has not been sufficiently explored. This paper addresses this gap by
formulating an optimization problem for signal design in DFRC-OTFS,
incorporating both pilot-symbol design for channel estimation and data-power
allocation. Specifically, we employ the integrated sidelobe level (ISL) of the
ambiguity function as a radar metric, accounting for the randomness of the data
symbols alongside the deterministic pilot symbols. For communication, we derive
a channel capacity lower bound metric that considers channel estimation errors
in OTFS. We maximize the weighted sum of sensing and communication metrics and
solve the optimization problem via an alternating optimization framework.
Simulations indicate that the proposed signal significantly improves the
sensing-communication performance region compared with conventional signal
schemes, achieving at least a 9.44 dB gain in ISL suppression for sensing, and
a 4.82 dB gain in the signal-to-interference-plus-noise ratio (SINR) for
communication.

</details>


### [8] [Active Localization of Close-range Adversarial Acoustic Sources for Underwater Data Center Surveillance](https://arxiv.org/abs/2510.20122)
*Adnan Abdullah,David Blow,Sara Rampazzi,Md Jahidul Islam*

Main category: eess.SP

TL;DR: 提出了一种用于水下数据中心声学攻击源定位的监控框架，结合固定和移动水听器，采用LC-MAP方案和UKF滤波实现实时3D定位追踪。


<details>
  <summary>Details</summary>
Motivation: 水下数据中心虽然具有自然冷却和物理安全优势，但易受声学注入攻击威胁，现有基于TDOA/FDOA的方法无法有效处理动态异构接收器配置。

Method: 采用异构接收器配置（固定水听器+移动水听器），提出LC-MAP方案生成声学信息和几何一致先验，结合UKF滤波进行联合TDOA-FDOA处理。

Result: 在蒙特卡洛分析、Gazebo仿真和现场试验中，实现了亚米级定位精度和90%以上的成功率，收敛时间比基线方法减少近一半。

Conclusion: 建立了几何感知的实时声学威胁定位方法，提升了水下基础设施的自主监控能力。

Abstract: Underwater data infrastructures offer natural cooling and enhanced physical
security compared to terrestrial facilities, but are susceptible to acoustic
injection attacks that can disrupt data integrity and availability. This work
presents a comprehensive surveillance framework for localizing and tracking
close-range adversarial acoustic sources targeting offshore infrastructures,
particularly underwater data centers (UDCs). We propose a heterogeneous
receiver configuration comprising a fixed hydrophone mounted on the facility
and a mobile hydrophone deployed on a dedicated surveillance robot. While using
enough arrays of static hydrophones covering large infrastructures is not
feasible in practice, off-the-shelf approaches based on time difference of
arrival (TDOA) and frequency difference of arrival (FDOA) filtering fail to
generalize for this dynamic configuration. To address this, we formulate a
Locus-Conditioned Maximum A-Posteriori (LC-MAP) scheme to generate acoustically
informed and geometrically consistent priors, ensuring a physically plausible
initial state for a joint TDOA-FDOA filtering. We integrate this into an
unscented Kalman filtering (UKF) pipeline, which provides reliable convergence
under nonlinearity and measurement noise. Extensive Monte Carlo analyses,
Gazebo-based physics simulations, and field trials demonstrate that the
proposed framework can reliably estimate the 3D position and velocity of an
adversarial acoustic attack source in real time. It achieves sub-meter
localization accuracy and over 90% success rates, with convergence times nearly
halved compared to baseline methods. Overall, this study establishes a
geometry-aware, real-time approach for acoustic threat localization, advancing
autonomous surveillance capabilities of underwater infrastructures.

</details>


### [9] [Time-series Random Process Complexity Ranking Using a Bound on Conditional Differential Entropy](https://arxiv.org/abs/2510.20551)
*Jacob Ayers,Richard Hahnloser,Julia Ulrich,Lothar Sebastian Krapp,Remo Nitschke,Sabine Stoll,Balthasar Bickel,Reinhard Furrer*

Main category: eess.SP

TL;DR: 本文提出了一种基于预测误差协方差矩阵的时间序列复杂度排序方法，通过信息论预测误差边界理论框架，利用Hadamard不等式和协方差矩阵半正定性来上界条件微分熵。


<details>
  <summary>Details</summary>
Motivation: 条件微分熵是衡量时间序列复杂度的直观指标，但对于高维未知分布过程直接计算往往不可行。需要寻找可计算的理论上界来替代直接熵计算。

Method: 基于Fang等人的信息论预测误差边界理论，进一步利用Hadamard不等式和协方差矩阵半正定性增加上界。通过线性自回归过程和生物启发合成音频数据两个实验验证方法有效性。

Result: 在线性自回归实验中，普通最小二乘预测误差熵代理与真实噪声熵相符；在合成音频复杂度排序任务中，神经网络预测误差成功恢复了已知的复杂度排序。

Conclusion: 该框架提供了一种计算可行的基于预测误差的时间序列复杂度排序方法，保持了信息论的理论基础。

Abstract: Conditional differential entropy provides an intuitive measure for relatively
ranking time-series complexity by quantifying uncertainty in future
observations given past context. However, its direct computation for
high-dimensional processes from unknown distributions is often intractable.
This paper builds on the information theoretic prediction error bounds
established by Fang et al. \cite{fang2019generic}, which demonstrate that the
conditional differential entropy \textbf{$h(X_k \mid X_{k-1},...,X_{k-m})$} is
upper bounded by a function of the determinant of the covariance matrix of
next-step prediction errors for any next step prediction model. We add to this
theoretical framework by further increasing this bound by leveraging Hadamard's
inequality and the positive semi-definite property of covariance matrices.
  To see if these bounds can be used to rank the complexity of time series, we
conducted two synthetic experiments: (1) controlled linear autoregressive
processes with additive Gaussian noise, where we compare ordinary least squares
prediction error entropy proxies to the true entropies of various additive
noises, and (2) a complexity ranking task of bio-inspired synthetic audio data
with unknown entropy, where neural network prediction errors are used to
recover the known complexity ordering.
  This framework provides a computationally tractable method for time-series
complexity ranking using prediction errors from next-step prediction models,
that maintains a theoretical foundation in information theory.

</details>


### [10] [Sensing Security in Near-Field ISAC: Exploiting Scatterers for Eavesdropper Deception](https://arxiv.org/abs/2510.20140)
*Jiangong Chen,Xia Lei,Kaitao Meng,Kawon Han,Yuchen Zhang,Christos Masouros,Athina P. Petropulu*

Main category: eess.SP

TL;DR: 提出一种在近场ISAC场景中利用已知散射体进行位置欺骗的方案，通过向散射体分配更多探测功率来误导窃听者，使其将散射体误认为目标，从而提升感知安全性。


<details>
  <summary>Details</summary>
Motivation: 在近场集成感知与通信场景中，需要保护感知目标免受具有感知能力的窃听者攻击。通过利用已知散射体进行位置欺骗，可以在不依赖窃听者先验信息的情况下提升感知安全性。

Method: 采用位置欺骗方案，故意向散射体分配比目标更高的探测功率；使用分数规划和半定松弛方法优化波束形成策略；引入CRB、MSE和KLD差距等指标评估安全性。

Result: 仿真结果表明，该方案能根据性能需求灵活调整波束形成策略，实现通信、感知和感知安全性的三方面权衡。在感知安全方面，显著增强了窃听者侧的杂波信号强度，导致实际目标的混淆甚至漏检。

Conclusion: 所提出的位置欺骗方案能够有效提升近场ISAC系统的感知安全性，通过灵活的三方面权衡实现安全性能的显著改善。

Abstract: In this paper, we explore sensing security in near-field (NF) integrated
sensing and communication (ISAC) scenarios by exploiting known scatterers in
the sensing scene. We propose a location deception (LD) scheme where scatterers
are deliberately illuminated with probing power that is higher than that
directed toward targets of interest, with the goal of deceiving potential
eavesdroppers (Eves) with sensing capability into misidentifying scatterers as
targets. While the known scatterers can be removed at the legitimate sensing
receiver, our LD approach causes Eves to misdetect targets. Notably, this
deception is achieved without requiring any prior information about the Eves'
characteristics or locations. To strike a flexible three-way tradeoff among
communication, sensing, and sensing-security performance, the sum rate and
power allocated to scatterers are weighted and maximized under a legitimate
radar signal-to-interference-plus-noise ratio (SINR) constraint. We employ the
fractional programming (FP) framework and semidefinite relaxation (SDR) to
solve this problem. To evaluate the security of the proposed LD scheme, the
Cramer-Rao Bound (CRB) and mean squared error (MSE) metrics are employed.
Additionally, we introduce the Kullback-Leibler Divergence (KLD) gap between
targets and scatterers at Eve to quantify the impact of the proposed LD
framework on Eve's sensing performance from an information-theoretical
perspective. Simulation results demonstrate that the proposed LD scheme can
flexibly adjust the beamforming strategy according to performance requirements,
thereby achieving the desired three-way tradeoff. In particular, in terms of
sensing security, the proposed scheme significantly enhances the clutter signal
strength at Eve's side, leading to confusion or even missed detection of the
actual target.

</details>


### [11] [Deep Learning Based Joint Space-Time-Frequency Domain Channel Prediction for Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2510.20146)
*Yongning Qi,Tao Zhou,Zuowei Xiang,Liu Liu,Bo Ai*

Main category: eess.SP

TL;DR: 本文提出了一种基于深度学习的无蜂窝大规模MIMO系统联合空时频域信道预测方法，通过改进Transformer编码器来利用空间、时间和频率相关性，在高速铁路LTE网络中验证了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝大规模MIMO是6G通信系统的关键技术，准确的信道状态信息获取对系统性能至关重要。传统方法难以有效利用空间、时间和频率相关性，特别是在不规则接入点部署场景下。

Method: 提出新型信道预测模型，在Transformer编码器中添加频率卷积和空间卷积层，能够并行输出多步预测结果而无误差传播。通过相关分析和交叉验证确定最优超参数。

Result: 所提模型预测精度高于传统模型，计算复杂度低于传统Transformer模型。在高速铁路LTE网络的实际数据验证中同样表现出更高的预测精度。

Conclusion: 该深度学习模型能够有效利用空时频相关性，在无蜂窝大规模MIMO系统中实现高精度信道预测，为6G通信系统提供了可靠的信道预测解决方案。

Abstract: The cell-free massive multi-input multi-output (CF-mMIMO) is a promising
technology for the six generation (6G) communication systems. Channel
prediction will play an important role in obtaining the accurate CSI to improve
the performance of CF-mMIMO systems. This paper studies a deep learning (DL)
based joint space-time-frequency domain channel prediction for CF-mMIMO.
Firstly, the prediction problems are formulated, which can output the
multi-step prediction results in parallel without error propagation. Then, a
novel channel prediction model is proposed, which adds frequency convolution
(FreqConv) and space convolution (SpaceConv) layers to Transformer-encoder. It
is able to utilize the space-time-frequency correlations and extract the space
correlation in the irregular AP deployment. Next, simulated datasets with
different sizes of service areas, UE velocities and scenarios are generated,
and correlation analysis and cross-validation are used to determine the optimal
hyper-parameters. According to the optimized hyper-parameters, the prediction
accuracy and computational complexity are evaluated based on simulated
datasets. It is indicated that the prediction accuracy of the proposed model is
higher than traditional model, and its computational complexity is lower than
traditional Transformer model. After that, the impacts of space-time-frequency
correlations on prediction accuracy are studied. Finally, realistic datasets in
a high-speed train (HST) long-term evolution (LTE) network are collected to
verify the prediction accuracy. The verification results demonstrate that it
also achieves higher prediction accuracy compared with traditional models in
the HST LTE network.

</details>


### [12] [NOMA for Visible Light Communications: Recent Advances and Future Directions](https://arxiv.org/abs/2510.20215)
*Xuesong Wang*

Main category: eess.SP

TL;DR: 本文回顾了可见光通信(VLC)在6G网络中的潜力，探讨了传统MAC协议在VLC中的局限性，并分析了基于非正交多址接入(NOMA)的VLC系统优化方案。


<details>
  <summary>Details</summary>
Motivation: 6G网络对高速数据传输的需求日益增长，VLC作为射频系统的补充具有独特优势。传统协议未充分利用光链路的特性，且VLC链路上行下行不对称，需要新的MAC层设计。

Method: 采用文献综述方法，分析VLC和基于NOMA的VLC研究进展，总结关键优化约束和目标，调查适合VLC的NOMA应用场景。

Result: 研究表明NOMA允许多用户共享相同时频资源并容忍受控干扰，在VLC系统中具有巨大潜力，但需要针对光学特性进行协议优化。

Conclusion: 需要为VLC设计全新的MAC层协议，NOMA是6G VLC的有前途技术方向，未来工作应关注协议优化和实际应用场景。

Abstract: Rapidly increasing demand for high speed data is pushing 6G wireless networks
to support larger link scales, lower latency, and higher spectral efficiency.
Visible light communications (VLC) is a strong complement to radio frequency
(RF) systems within 6G. The latest ITU G.9991 and IEEE 802.11bb standards are
adapted from cable and RF wireless technologies for use in VLC, so they do not
fully exploit the optical nature of light links. VLC links are often asymmetric
between uplink and downlink, which makes TDMA style protocols inefficient when
many users generate bursty and asymmetric traffic. Compared with RF, the strong
directionality and frequent line of sight in VLC can mitigate hidden and
exposed terminals, yet these effects can still appear under limited field of
view, blockage, or reflections. CSMA/CA and related methods remain usable in
VLC and in RF plus VLC networks, but they usually need design tweaks such as
RTS/CTS or directional sensing to perform well. Although the optical spectrum
is vast, the bandwidth of practical LEDs and of common PIN or APD receivers is
limited, so efficient multiple access can yield large gains. This motivates a
clean slate design for VLC, especially at the MAC layer. NOMA, first explored
in 5G RF systems, is also promising for 6G VLC. It lets multiple users share
the same time and frequency resources while tolerating controlled interference.
This paper reviews progress in VLC and in NOMA based VLC, outlines key
optimization constraints and objectives, surveys scenarios that fit NOMA in
VLC, and points to several directions for future work.

</details>


### [13] [A Survey of OTFS-Based Index Modulation Techniques: Challenges, Benefits, and Future Directions for 6G and Beyond](https://arxiv.org/abs/2510.20265)
*Burak Ahmet Ozden,Erdogan Aydin,Emir Aslandogan,Haci Ilhan,Ertugrul Basar,Miaowen Wen,Marco Di Renzo,Vincent Poor*

Main category: eess.SP

TL;DR: 本文对基于OTFS（正交时频空间）的索引调制（IM）系统进行了全面综述，介绍了OTFS-IM的各种变体、系统架构、检测方法和性能分析，并讨论了该技术的挑战、优势和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: OTFS是一种在延迟-多普勒域进行二维调制的技术，能够充分利用信道多样性并将快时变信道转换为近似静态信道。索引调制通过在选择通信资源索引中编码数据位来提高性能。结合这两种技术可以提升6G及未来网络的通信性能。

Method: 系统性地分类和比较了现有的OTFS-IM方案，包括基于OTFS的空间移位键控、空间调制、正交空间调制、媒体调制和码索引调制等变体。分析了不同方案在系统架构、检测方法和性能方面的特点。

Result: 提供了OTFS-IM系统在计算复杂度、误码性能、容量、节能、频谱效率和吞吐量等方面的比较性能分析。展示了OTFS-IM在提升通信系统性能方面的潜力。

Conclusion: OTFS-IM系统具有显著优势，但也面临复杂度、效率、延迟、信道估计等挑战。未来需要进一步研究与其他先进无线通信技术的集成，以充分发挥其在6G及未来网络中的潜力。

Abstract: Orthogonal time frequency space (OTFS) is a two-dimensional modulation
technique that uses the delay-Doppler (DD) domain and is a candidate for
providing robust, high-capacity wireless communications for envisioned 6G and
beyond networks. The OTFS technique maps data to the DD domain instead of the
traditional time-frequency domain, enabling it to fully utilize channel
diversity and transform fast time-varying channels into nearly static channels.
Index modulation (IM) is a communication paradigm that conveys information not
only through conventional modulation symbols but also by encoding data bits in
the indices of the selected communication resources to improve error
performance, spectral efficiency, and energy efficiency. In this survey, a
comprehensive review of work on OTFS-based wireless communication systems is
presented. In particular, the existing OTFS-IM schemes are reviewed and
systematically categorized according to their system architectures, detection
methods, and performance aspects such as capacity, peak-to-average power ratio,
diversity, complexity, imperfect channel state information, spectral
efficiency, and outage probability. Furthermore, the operating principles and
system models of OTFS-IM variants-including OTFS-based space shift keying,
OTFS-based spatial modulation, OTFS-based quadrature spatial modulation,
OTFS-based media-based modulation, and OTFS-based code index modulation-are
described, followed by a comparative performance analysis in terms of
computational complexity, error performance, capacity, energy saving, spectral
efficiency, and throughput. Finally, the challenges, benefits, and future
directions for OTFS-IM systems are discussed, covering key aspects such as
complexity, efficiency, latency, channel estimation, hardware constraints,
synchronization, security, and potential integration with other advanced
wireless communication techniques.

</details>


### [14] [Near-Field 3D Localization and MIMO Channel Estimation with Sub-Connected Planar Arrays](https://arxiv.org/abs/2510.20274)
*Kangda Zhi,Tianyu Yang,Songyan Xue,Giuseppe Caire*

Main category: eess.SP

TL;DR: 提出了一种针对近场XL-MIMO系统的三阶段信道估计和3D定位算法，结合OMP和SBL技术，显著降低了导频开销并提高了估计精度。


<details>
  <summary>Details</summary>
Motivation: 近场XL-MIMO系统的上行MIMO信道具有满列秩特性，现有的远场或单天线用户近场码本无法有效估计，需要开发新的估计方法。

Method: 三阶段算法：1) 分区使用OMP和DFT字典进行子阵列信道估计；2) 利用MUSIC和LS准则估计用户阵列中心位置；3) 构建位置辅助字典矩阵，使用SBL进行MIMO信道估计。

Result: 与多个基准方法相比，所提算法在导频开销和估计精度方面均表现出显著优势。

Conclusion: 提出的三阶段OMP-SBL算法能够有效解决近场XL-MIMO系统的信道估计和3D定位问题，具有实际应用价值。

Abstract: This paper investigates the design of channel estimation and 3D localization
algorithms in a challenging scenario, where a sub-connected planar extremely
large-scale multiple-input multiple-output (XL-MIMO) communicates with
multi-antenna users. In the near field, the uplink MIMO channel is of full
column rank and therefore can not be estimated effectively by applying existing
codebooks that are designed for the far-field case or for the near-field case
but limited to single antenna users. To solve this problem, we propose a
three-stage algorithm aided by orthogonal matching pursuit (OMP) and sparse
Bayesian learning (SBL). Specifically, we firstly partition the XL-MIMO into
subarrays and use OMP to solve the compressed sensing (CS) problem about
subarray channel estimation with the Discrete Fourier Transform (DFT)-based
dictionary matrix. Secondly, exploiting the estimated subarray channels and
employing one-dimensional multiple signal classification (MUSIC), we estimate
the central location of the user array under the Least Squares (LS) criterion.
Finally, we utilize the estimated central location to construct a refined
location-aided dictionary matrix and obtain the MIMO channel estimation using
SBL. Results exhibit the significant superiority of the proposed algorithm
compared with several benchmarks, in terms of both the pilot overhead and
estimation accuracy.

</details>


### [15] [Channel Estimation and Passive Beamforming for Pixel-based Reconfigurable Intelligent Surfaces with Non-Separable State Response](https://arxiv.org/abs/2510.20354)
*Huayan Guo,Junhui Rao,Alex M. H. Wong,Ross Murch,Vincent K. N. Lau*

Main category: eess.SP

TL;DR: 提出了一种针对像素型可重构智能表面的解决方案，包括近似非分离状态响应、简化级联信道建模和低复杂度波束赋形算法，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 像素型RIS通过消除传统RIS中的移相器来降低硬件成本，但其非分离状态响应给信道估计和波束赋形带来了挑战，现有解决方案无法有效处理。

Method: 使用核方法和深度神经网络近似非分离RIS响应函数；提出简化的级联信道模型，分离估计短期和长期参数；设计低复杂度被动波束赋形算法配置离散RIS状态向量。

Result: 仿真结果表明，所提出的解决方案在广泛的信噪比范围内显著优于各种基线方法。

Conclusion: 该研究为像素型RIS系统提供了一套完整的解决方案，有效解决了非分离状态响应带来的技术挑战，实现了高性能和低复杂度的平衡。

Abstract: Pixel-based reconfigurable intelligent surfaces (RISs) employ a novel design
to achieve high reflection gain at a lower hardware cost by eliminating the
phase shifters used in traditional RIS. However, this design presents
challenges for channel estimation and passive beamforming due to its
non-separable state response, rendering existing solutions ineffective. To
address this, we first approximate the non-separable RIS response functions
using a kernel-based method and a deep neural network, achieving high accuracy
while reducing computational and memory complexity. Next, we propose a
simplified cascaded channel model that focuses on dominated scattering paths
with limited unknown parameters, along with customized algorithms to estimate
short-term and long-term parameters separately. Finally, we introduce a
low-complexity passive beamforming algorithm to configure the discrete RIS
state vector, maximizing the achievable rate. Our simulation results
demonstrate that the proposed solution significantly outperforms various
baselines across a wide SNR range.

</details>


### [16] [A Transformer Inspired AI-based MIMO receiver](https://arxiv.org/abs/2510.20363)
*András Rácz,Tamás Borsos,András Veres,Benedek Csala*

Main category: eess.SP

TL;DR: AttDet是一种基于Transformer的MIMO检测方法，将每个传输层视为token，通过轻量级自注意力机制学习流间干扰，结合模型可解释性和数据驱动灵活性，在5G信道下实现接近最优的误码率性能。


<details>
  <summary>Details</summary>
Motivation: 解决MIMO系统中传统检测方法在高阶调制和复杂信道条件下性能不足的问题，同时保持计算复杂度可控。

Method: 将传输层作为token，直接从估计的信道矩阵生成查询和键，注意力分数量化信道相关性，值由匹配滤波器输出初始化并迭代优化。

Result: 在5G信道模型和高阶混合QAM调制编码方案下的链路级仿真表明，AttDet能接近最优的BER/BLER性能，同时保持多项式复杂度。

Conclusion: AttDet成功结合了模型可解释性和数据驱动灵活性，为MIMO检测提供了高效且性能优异的解决方案。

Abstract: We present AttDet, a Transformer-inspired MIMO (Multiple Input Multiple
Output) detection method that treats each transmit layer as a token and learns
inter-stream interference via a lightweight self-attention mechanism. Queries
and keys are derived directly from the estimated channel matrix, so attention
scores quantify channel correlation. Values are initialized by matched-filter
outputs and iteratively refined. The AttDet design combines model-based
interpretability with data-driven flexibility. We demonstrate through
link-level simulations under realistic 5G channel models and high-order, mixed
QAM modulation and coding schemes, that AttDet can approach near-optimal
BER/BLER (Bit Error Rate/Block Error Rate) performance while maintaining
predictable, polynomial complexity.

</details>


### [17] [Efficient Medium Access Control for Low-Latency Industrial M2M Communications](https://arxiv.org/abs/2510.20380)
*Anwar Ahmed Khan,Indrakshi Dey*

Main category: eess.SP

TL;DR: 该论文比较了两种工业M2M网络MAC协议：基于竞争窗口的BoP-MAC和基于分段的FROG-MAC，发现FROG-MAC在多优先级异构数据环境下在延迟和吞吐量方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 工业M2M网络中需要低延迟和可靠通信，多优先级数据的存在增加了挑战，因此需要对主要MAC协议进行综合比较以了解其相对优势和局限性。

Method: 在Contiki平台上进行仿真，通过改变节点数量来比较BoP-MAC和FROG-MAC两种协议的性能。BoP-MAC为多优先级流量分配差异化退避值，而FROG-MAC通过分段低优先级流量实现高优先级数据包的早期传输。

Result: 在工业环境的多优先级异构数据处理中，FROG-MAC在延迟和吞吐量方面表现优于BoP-MAC。

Conclusion: 对于工业环境中的多优先级异构数据通信，基于分段的FROG-MAC协议在性能上优于基于竞争窗口的BoP-MAC协议。

Abstract: Efficient medium access control (MAC) is critical for enabling low-latency
and reliable communication in industrial Machine-to-Machine (M2M) net-works,
where timely data delivery is essential for seamless operation. The presence of
multi-priority data in high-risk industrial environments further adds to the
challenges. The development of tens of MAC schemes over the past decade often
makes it a tough choice to deploy the most efficient solu-tion. Therefore, a
comprehensive cross-comparison of major MAC protocols across a range of
performance parameters appears necessary to gain deeper insights into their
relative strengths and limitations. This paper presents a comparison of
Contention window-based MAC scheme BoP-MAC with a fragmentation based,
FROG-MAC; both protocols focus on reducing the delay for higher priority
traffic, while taking a diverse approach. BoP-MAC assigns a differentiated
back-off value to the multi-priority traffic, whereas FROG-MAC enables early
transmission of higher-priority packets by fragmenting lower-priority traffic.
Simulations were performed on Contiki by varying the number of nodes for two
traffic priorities. It has been shown that when work-ing with multi-priority
heterogenous data in the industrial environment, FROG-MAC results better both
in terms of delay and throughput.

</details>


### [18] [Inference-Optimal ISAC via Task-Oriented Feature Transmission and Power Allocation](https://arxiv.org/abs/2510.20429)
*Biao Dong,Bin Cao,Qinyu Zhang*

Main category: eess.SP

TL;DR: 本文研究了集成感知与通信系统中基于压缩估计框架的协调增益，通过判别增益最大化而非均方误差最小化来优化推理性能，推导了DG最优和MSE最优收发器设计，展示了功率效率更高的传输策略。


<details>
  <summary>Details</summary>
Motivation: 传统ISAC系统通常以均方误差为优化指标，但推理性能才是实际应用的关键指标。作者探索是否通过最大化判别增益而非最小化MSE能获得更好的推理性能。

Method: 采用压缩估计框架，将推理性能表征为判别增益的单调函数，推导了DG最优和MSE最优收发器设计的闭式解，揭示了注水型结构和显式的感知通信权衡。

Result: 数值实验证实DG最优设计在低信噪比下实现更功率高效的传输，通过选择性地为信息特征分配功率来节省感知传输功率。

Conclusion: 最大化判别增益比最小化均方误差能获得更好的推理性能，特别是在低信噪比区域，DG最优设计通过智能功率分配实现了感知与通信的更好权衡。

Abstract: This work is concerned with the coordination gain in integrated sensing and
communication (ISAC) systems under a compress-and-estimate (CE) framework,
wherein inference performance is leveraged as the key metric. To enable
tractable transceiver design and resource optimization, we characterize
inference performance via an error probability bound as a monotonic function of
the discriminant gain (DG). This raises the natural question of whether
maximizing DG, rather than minimizing mean squared error (MSE), can yield
better inference performance. Closed-form solutions for DG-optimal and
MSE-optimal transceiver designs are derived, revealing water-filling-type
structures and explicit sensing and communication (S\&C) tradeoff. Numerical
experiments confirm that DG-optimal design achieves more power-efficient
transmission, especially in the low signal-to-noise ratio (SNR) regime, by
selectively allocating power to informative features and thus saving transmit
power for sensing.

</details>


### [19] [Analysis of Frequency-Diverse and Dispersion Effects in Dynamic Metasurface Antenna for Holographic Sensing and Imaging](https://arxiv.org/abs/2510.20447)
*Abdul Jabbar,Aakash Bansal,William Whittow*

Main category: eess.SP

TL;DR: 该论文展示了动态超表面天线在毫米波频段的频率多样性和色散操作，通过动态全息可重构性实现灵活的色散操控，在紧凑可重构平台上实现增强的扫描范围。


<details>
  <summary>Details</summary>
Motivation: 当前DMA设计和模型通常是准窄带的，忽略了多功能频率多样性表现及其利用，需要探索DMA的色散效应和频率多样性操作。

Method: 通过动态全息可重构性操控DMA中超原子的色散特性，在操作频段内创建不同的辐射模式，实现灵活的频率多样性。

Result: 实现了增强的扫描范围，消除了对宽带系统或复杂移相网络的需求，为传统漏波天线的频率扫描静态波束提供了替代方案。

Conclusion: 该研究为下一代近场和远场全息传感以及计算全息成像应用中DMA色散效应的建模和利用建立了基础见解。

Abstract: Dynamic metasurface antennas (DMAs) represent a novel approach to
programmable and affordable electromagnetic wave manipulation for enhanced
wireless communications, sensing, and imaging applications. Nevertheless,
current DMA designs and models are usually quasi-narrowband, neglecting the
versatile frequency-diverse manifestation and its utilization. This work
demonstrates the frequency-diversity and dispersion operations of a
representative DMA structure at the millimeter-wave band. We demonstrate
flexible dispersion manipulation through dynamic holographic reconfigurability
of the meta-atoms in a DMA. This effect can create distinct radiation patterns
across the operating frequency band, achieving flexible frequency diversity
with enhanced scanning range within a compact, reconfigurable platform. It
eliminates the need for wideband systems or complex phase-shifting networks
while offering an alternative to frequency-scanned static beams of traditional
leaky-wave antennas. The results establish fundamental insights into modelling
and utilization of dispersive effects of DMAs in next-generation near-field and
far-field holographic sensing and computational holographic imaging
applications.

</details>


### [20] [An Accelerated Mixed Weighted-Unweighted MMSE Approach for MU-MIMO Beamforming](https://arxiv.org/abs/2510.20507)
*Xi Gao,Akang Wang,Junkai Zhang,Qihong Duan,Jiang Xue*

Main category: eess.SP

TL;DR: 提出了一种基于块坐标下降框架的高并行算法A-MMMSE，用于多用户MIMO系统的预编码设计，避免了矩阵求逆，仅使用矩阵乘法，适合GPU加速，性能与WMMSE相当但计算时间大幅减少。


<details>
  <summary>Details</summary>
Motivation: 传统WMMSE算法在基站天线数量增加时计算复杂度呈立方增长（涉及矩阵求逆），限制了其在延迟敏感场景中的应用。

Method: 采用块坐标梯度下降更新预编码矩阵，避免矩阵求逆，仅使用矩阵乘法；引入基于和均方误差最小化问题的两阶段热启动策略加速收敛。

Result: 仿真结果表明A-MMMSE在加权和速率性能上与WMMSE及其增强变体reduced-WMMSE相当，但在各种系统配置下计算时间显著减少。

Conclusion: A-MMMSE算法通过避免矩阵求逆和利用GPU加速，在保持性能的同时大幅降低了计算复杂度，适用于延迟敏感的多用户MIMO系统。

Abstract: Precoding design based on weighted sum-rate (WSR) maximization is a
fundamental problem in downlink multi-user multiple-input multiple-output
(MU-MIMO) systems. While the weighted minimum mean-square error (WMMSE)
algorithm is a standard solution, its high computational complexity--cubic in
the number of base station antennas due to matrix inversions--hinders its
application in latency-sensitive scenarios. To address this limitation, we
propose a highly parallel algorithm based on a block coordinate descent
framework. Our key innovation lies in updating the precoding matrix via block
coordinate gradient descent, which avoids matrix inversions and relies solely
on matrix multiplications, making it exceptionally amenable to GPU
acceleration. We prove that the proposed algorithm converges to a stationary
point of the WSR maximization problem. Furthermore, we introduce a two-stage
warm-start strategy grounded in the sum mean-square error (MSE) minimization
problem to accelerate convergence. We refer to our method as the Accelerated
Mixed weighted-unweighted sum-MSE minimization (A-MMMSE) algorithm. Simulation
results demonstrate that A-MMMSE matches the WSR performance of both
conventional WMMSE and its enhanced variant, reduced-WMMSE, while achieving a
substantial reduction in computational time across diverse system
configurations.

</details>


### [21] [Performance Analysis of End-to-End LEO Satellite-Aided Shore-to-Ship Communications: A Stochastic Geometry Approach](https://arxiv.org/abs/2510.20515)
*Xu Hu,Bin Lin,Xiao Lu,Ping Wang,Nan Cheng,Zhisheng Yin,Weihua Zhuang*

Main category: eess.SP

TL;DR: 提出了一种基于二项点过程的LEO卫星辅助岸对船通信网络理论框架，通过距离近似方法和随机几何分析，推导了端到端传输成功概率和平均传输速率容量的解析表达式。


<details>
  <summary>Details</summary>
Motivation: 传统基于多圆轨道的性能建模难以表征大规模LEO卫星星座，需要一种可处理的方法来准确评估网络性能。

Method: 将LEO卫星建模为特定球面上的二项点过程，考虑海洋链路和空间链路的Rician或Shadowed Rician衰落，提出距离近似方法并结合阈值通信方案，利用随机几何推导性能分析表达式。

Result: 广泛的数值结果验证了分析的准确性，并展示了关键参数对LEO-SSCN性能的影响。

Conclusion: 所提出的理论框架为大规模LEO卫星辅助岸对船通信网络提供了一种准确且可处理的性能评估方法。

Abstract: Low Earth orbit (LEO) satellite networks have shown strategic superiority in
maritime communications, assisting in establishing signal transmissions from
shore to ship through space-based links. Traditional performance modeling based
on multiple circular orbits is challenging to characterize large-scale LEO
satellite constellations, thus requiring a tractable approach to accurately
evaluate the network performance. In this paper, we propose a theoretical
framework for an LEO satellite-aided shore-to-ship communication network
(LEO-SSCN), where LEO satellites are distributed as a binomial point process
(BPP) on a specific spherical surface. The framework aims to obtain the
end-to-end transmission performance by considering signal transmissions through
either a marine link or a space link subject to Rician or Shadowed Rician
fading, respectively. Due to the indeterminate position of the serving
satellite, accurately modeling the distance from the serving satellite to the
destination ship becomes intractable. To address this issue, we propose a
distance approximation approach. Then, by approximation and incorporating a
threshold-based communication scheme, we leverage stochastic geometry to derive
analytical expressions of end-to-end transmission success probability and
average transmission rate capacity. Extensive numerical results verify the
accuracy of the analysis and demonstrate the effect of key parameters on the
performance of LEO-SSCN.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [22] [Balancing Specialization and Centralization: A Multi-Agent Reinforcement Learning Benchmark for Sequential Industrial Control](https://arxiv.org/abs/2510.20408)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

TL;DR: 提出了一个工业启发的多阶段流程基准环境，评估模块化与集中式控制策略，发现动作掩码对学习效果有决定性影响。


<details>
  <summary>Details</summary>
Motivation: 工业过程控制需要局部专业化和全局协调，但现有RL基准与工业问题差异大，限制了实际应用。

Method: 结合SortingEnv和ContainerGym创建顺序回收场景，评估模块化架构与集中式代理，分析动作掩码的影响。

Result: 无动作掩码时模块化架构表现更好；应用动作掩码后两者性能均显著提升，差距缩小。

Conclusion: 动作空间约束起决定性作用，专业化优势随动作复杂度降低而减弱，该基准为工业自动化多智能体RL研究提供有价值平台。

Abstract: Autonomous control of multi-stage industrial processes requires both local
specialization and global coordination. Reinforcement learning (RL) offers a
promising approach, but its industrial adoption remains limited due to
challenges such as reward design, modularity, and action space management. Many
academic benchmarks differ markedly from industrial control problems, limiting
their transferability to real-world applications. This study introduces an
enhanced industry-inspired benchmark environment that combines tasks from two
existing benchmarks, SortingEnv and ContainerGym, into a sequential recycling
scenario with sorting and pressing operations. We evaluate two control
strategies: a modular architecture with specialized agents and a monolithic
agent governing the full system, while also analyzing the impact of action
masking. Our experiments show that without action masking, agents struggle to
learn effective policies, with the modular architecture performing better. When
action masking is applied, both architectures improve substantially, and the
performance gap narrows considerably. These results highlight the decisive role
of action space constraints and suggest that the advantages of specialization
diminish as action complexity is reduced. The proposed benchmark thus provides
a valuable testbed for exploring practical and robust multi-agent RL solutions
in industrial automation, while contributing to the ongoing debate on
centralization versus specialization.

</details>


### [23] [Some Attention is All You Need for Retrieval](https://arxiv.org/abs/2510.19861)
*Felix Michalak,Steven Abreu*

Main category: cs.LG

TL;DR: 在混合SSM-Transformer架构中，检索功能完全依赖于自注意力层，而SSM层没有补偿机制。仅保留15%的注意力头就能维持近乎完美的检索性能，同时保留84%的MMLU表现。


<details>
  <summary>Details</summary>
Motivation: 研究混合SSM-Transformer架构中不同组件的功能特化，挑战关于架构冗余的假设，探索模型优化和可解释性。

Method: 通过注意力消融实验，在RecurrentGemma-2B/9B和Jamba-Mini-1.6模型上测试检索性能，分析自注意力和SSM层的功能分工。

Result: 注意力消融导致检索完全失败（0%准确率），而SSM层无补偿能力。稀疏化注意力至15%头数仍能保持近乎完美检索，同时保留大部分MMLU性能。

Conclusion: 自注意力专门负责检索任务，混合架构中的功能严格分离，模型作为专门化模块而非集成系统运行，这对架构优化和可解释性有直接意义。

Abstract: We demonstrate complete functional segregation in hybrid SSM-Transformer
architectures: retrieval depends exclusively on self-attention layers. Across
RecurrentGemma-2B/9B and Jamba-Mini-1.6, attention ablation causes catastrophic
retrieval failure (0% accuracy), while SSM layers show no compensatory
mechanisms even with improved prompting. Conversely, sparsifying attention to
just 15% of heads maintains near-perfect retrieval while preserving 84% MMLU
performance, suggesting self-attention specializes primarily for retrieval
tasks. We identify precise mechanistic requirements for retrieval: needle
tokens must be exposed during generation and sufficient context must be
available during prefill or generation. This strict functional specialization
challenges assumptions about redundancy in hybrid architectures and suggests
these models operate as specialized modules rather than integrated systems,
with immediate implications for architecture optimization and interpretability.

</details>


### [24] [An Integrated Approach to Neural Architecture Search for Deep Q-Networks](https://arxiv.org/abs/2510.19872)
*Iman Rahmani,Saman Yazdannik,Morteza Tayefi,Jafar Roshanian*

Main category: cs.LG

TL;DR: NAS-DQN通过集成神经架构搜索控制器到DRL训练循环中，实现动态网络重构，在连续控制任务中优于固定架构设计，证明了架构自适应对在线深度强化学习的重要性。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习代理的性能受限于固定的神经网络架构选择，需要通过昂贵的超参数搜索确定且无法在训练中调整。本研究探索在线自适应架构优化是否能突破这一限制。

Method: 引入NAS-DQN代理，将学习到的神经架构搜索控制器直接集成到DRL训练循环中，基于累积性能反馈实现动态网络重构。

Result: NAS-DQN在多个随机种子的实验中，相比三个固定架构基线和随机搜索控制，实现了更优的最终性能、样本效率和策略稳定性，且计算开销可忽略。

Conclusion: 架构自适应不仅有益而且是实现在线深度强化学习最优样本效率的必要条件，RL代理的设计不应是静态离线选择，而应作为学习过程的动态组件无缝集成。

Abstract: The performance of deep reinforcement learning agents is fundamentally
constrained by their neural network architecture, a choice traditionally made
through expensive hyperparameter searches and then fixed throughout training.
This work investigates whether online, adaptive architecture optimization can
escape this constraint and outperform static designs. We introduce NAS-DQN, an
agent that integrates a learned neural architecture search controller directly
into the DRL training loop, enabling dynamic network reconfiguration based on
cumulative performance feedback. We evaluate NAS-DQN against three
fixed-architecture baselines and a random search control on a continuous
control task, conducting experiments over multiple random seeds. Our results
demonstrate that NAS-DQN achieves superior final performance, sample
efficiency, and policy stability while incurring negligible computational
overhead. Critically, the learned search strategy substantially outperforms
both undirected random architecture exploration and poorly-chosen fixed
designs, indicating that intelligent, performance-guided search is the key
mechanism driving success. These findings establish that architecture
adaptation is not merely beneficial but necessary for optimal sample efficiency
in online deep reinforcement learning, and suggest that the design of RL agents
need not be a static offline choice but can instead be seamlessly integrated as
a dynamic component of the learning process itself.

</details>


### [25] [From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph](https://arxiv.org/abs/2510.19873)
*Junfeng Gong,Zhiyi Wei,Junying Chen,Cheng Liu,Huawei Li*

Main category: cs.LG

TL;DR: ReGraphT是一个无需训练的检索增强生成框架，通过构建CUDA优化轨迹的推理图，将大语言模型的推理能力迁移到小模型上，使SLM在CUDA代码生成任务中接近LLM性能。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在生成优化CUDA代码方面表现出色，但存在代码泄露风险和计算成本高的问题。小模型虽然轻量且隐私友好，但在复杂CUDA生成任务中推理能力有限。

Method: 提出ReGraphT框架：将CUDA优化轨迹组织为结构化推理图，将组合优化建模为状态转换，使用蒙特卡洛图搜索进行高效探索。

Result: ReGraphT在CUDAEval和ParEval基准测试中平均加速2.33倍，优于HPC专用微调模型和其他检索增强方法。与特定SLM结合时能接近LLM性能水平。

Conclusion: ReGraphT成功将LLM级推理能力迁移到SLM，解决了隐私风险和计算开销问题，为高效CUDA代码生成提供了可行方案。

Abstract: Despite significant evolution of CUDA programming and domain-specific
libraries, effectively utilizing GPUs with massively parallel engines remains
difficult. Large language models (LLMs) show strong potential in generating
optimized CUDA code from sequential code. However, using LLMs in practice faces
two major challenges: cloud-based APIs pose risks of code leakage, and local
deployment is often computationally expensive and inefficient. These drawbacks
have spurred interest in small language models (SLMs), which are more
lightweight and privacy-friendly. Encouragingly, recent studies show that SLMs
can achieve performance comparable to LLMs on specific tasks. While SLMs can
match LLMs on domain-specific tasks, their limited reasoning abilities lead to
suboptimal performance in complex CUDA generation according to our experiments.
To bridge this gap, we propose ReGraphT, a training-free, retrieval-augmented
generation framework that transfers LLM-level reasoning to smaller models.
ReGraphT organizes CUDA optimization trajectories into a structured reasoning
graph, modeling the combined CUDA optimizations as state transitions, and
leverages Monte Carlo Graph Search (MCGS) for efficient exploration. We also
present a CUDA-specific benchmark with difficulty tiers defined by reasoning
complexity to evaluate models more comprehensively. Experiments show that
ReGraphT outperforms HPC-specific fine-tuned models and other
retrieval-augmented approaches, achieving an average 2.33X speedup on CUDAEval
and ParEval. When paired with DeepSeek-Coder-V2-Lite-Instruct and
Qwen2.5-Coder-7B-Instruct, ReGraphT enables SLMs to approach LLM-level
performance without the associated privacy risks or excessive computing
overhead.

</details>


### [26] [From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem](https://arxiv.org/abs/2510.19889)
*Mostafa Ameli,Van Anh Le,Sulthana Shams,Alexander Skabardonis*

Main category: cs.LG

TL;DR: 本文提出了一种基于Transformer架构的数据驱动方法，直接预测均衡路径流量，相比传统优化方法大幅降低了计算时间，并能适应需求和网络结构的变化。


<details>
  <summary>Details</summary>
Motivation: 传统基于均衡原理的交通分配问题在大规模网络中计算复杂度呈非线性增长，变得计算昂贵。需要一种更高效的方法来处理大规模网络中的交通流分析。

Method: 使用深度神经网络，特别是Transformer架构，直接预测均衡路径流量。该方法关注路径级交通分布，捕捉OD对之间的复杂相关性。

Result: 在曼哈顿式合成网络、苏福尔斯网络和东马萨诸塞州网络上进行的数值实验表明，该模型比传统优化方法快几个数量级，能有效估计多类网络中的路径级交通流量，降低计算成本并提高预测精度。

Conclusion: 基于Transformer的模型能够灵活适应变化的需求和网络条件，支持交通管理，并为增强交通规划和政策制定实现快速的'假设分析'。

Abstract: The traffic assignment problem is essential for traffic flow analysis,
traditionally solved using mathematical programs under the Equilibrium
principle. These methods become computationally prohibitive for large-scale
networks due to non-linear growth in complexity with the number of OD pairs.
This study introduces a novel data-driven approach using deep neural networks,
specifically leveraging the Transformer architecture, to predict equilibrium
path flows directly. By focusing on path-level traffic distribution, the
proposed model captures intricate correlations between OD pairs, offering a
more detailed and flexible analysis compared to traditional link-level
approaches. The Transformer-based model drastically reduces computation time,
while adapting to changes in demand and network structure without the need for
recalculation. Numerical experiments are conducted on the Manhattan-like
synthetic network, the Sioux Falls network, and the Eastern-Massachusetts
network. The results demonstrate that the proposed model is orders of magnitude
faster than conventional optimization. It efficiently estimates path-level
traffic flows in multi-class networks, reducing computational costs and
improving prediction accuracy by capturing detailed trip and flow information.
The model also adapts flexibly to varying demand and network conditions,
supporting traffic management and enabling rapid `what-if' analyses for
enhanced transportation planning and policy-making.

</details>


### [27] [FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning](https://arxiv.org/abs/2510.19893)
*Shiqi Dai,Wei Dai,Jiaee Cheong,Paul Pu Liang*

Main category: cs.LG

TL;DR: 提出了FairGRPO方法，通过分层强化学习解决医疗AI系统中的公平性问题，减少不同人口群体间的性能差异，并在多个临床数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统在诊断能力上表现出色，但存在跨人口群体的性能差异问题，对少数群体造成实际伤害。现有的多模态推理基础模型通过强化学习训练时会继承并放大训练数据中的偏见。

Method: FairGRPO是一种分层强化学习方法，采用基于代表性、任务难度和数据来源的自适应优势权重调整。当缺乏人口统计标签时，使用无监督聚类自动发现潜在的人口群体。

Result: 在7个临床诊断数据集上，FairGRPO将预测公平性提高了27.2%，同时F1分数提升了12.49%。训练动态分析显示FairGRPO在优化过程中持续改善公平性。

Conclusion: FairGRPO有效解决了医疗AI中的公平性问题，基于该方法发布的FairMedGemma-4B模型在保持最先进性能的同时显著减少了跨人口群体的差异。

Abstract: Medical artificial intelligence systems have achieved remarkable diagnostic
capabilities, yet they consistently exhibit performance disparities across
demographic groups, causing real-world harm to underrepresented populations.
While recent multimodal reasoning foundation models have advanced clinical
diagnosis through integrated analysis of diverse medical data, reasoning
trainings via reinforcement learning inherit and often amplify biases present
in training datasets dominated by majority populations. We introduce
Fairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchical
reinforcement learning approach that promotes equitable learning across
heterogeneous clinical populations. FairGRPO employs adaptive importance
weighting of advantages based on representation, task difficulty, and data
source. To address the common issue of missing demographic labels in the
clinical domain, we further employ unsupervised clustering, which automatically
discovers latent demographic groups when labels are unavailable. Through
comprehensive experiments across 7 clinical diagnostic datasets spanning 5
clinical modalities across X-ray, CT scan, dermoscropy, mammography and
ultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%
against all vanilla and bias mitigated RL baselines, while improving F1 score
by 12.49%. Furthermore, training dynamics analysis reveals that FairGRPO
progressively improves fairness throughout optimization, while baseline RL
methods exhibit deteriorating fairness as training progresses. Based on
FairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM that
achieves state-of-the-art performance while demonstrating significantly reduced
disparities across demographic groups.

</details>


### [28] [Mitigating Privacy-Utility Trade-off in Decentralized Federated Learning via $f$-Differential Privacy](https://arxiv.org/abs/2510.19934)
*Xiang Li,Buxin Su,Chendi Wang,Qi Long,Weijie J. Su*

Main category: cs.LG

TL;DR: 本文针对去中心化联邦学习中的隐私预算量化挑战，提出了两种基于f-差分隐私的隐私核算方法：PN-f-DP和Sec-f-LDP，通过结合f-DP理论和马尔可夫链集中性工具，捕捉稀疏通信、本地迭代和相关噪声带来的隐私放大效应。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习允许用户在不与中央服务器共享数据的情况下协作，但由于存在复杂的算法组件（如去中心化通信和本地更新），准确量化隐私预算具有挑战性。

Method: 开发了两种新的f-DP核算方法：PN-f-DP量化随机游走通信下用户对之间的隐私泄露，Sec-f-LDP通过共享秘密支持结构化噪声注入。结合f-DP理论和马尔可夫链集中性工具构建核算框架。

Result: 在合成和真实数据集上的实验表明，与基于Rényi DP的方法相比，该方法产生更紧的(ε,δ)边界和更好的效用。

Conclusion: f-DP在去中心化隐私核算中具有优势，能够更准确地量化隐私预算并提高算法效用。

Abstract: Differentially private (DP) decentralized Federated Learning (FL) allows
local users to collaborate without sharing their data with a central server.
However, accurately quantifying the privacy budget of private FL algorithms is
challenging due to the co-existence of complex algorithmic components such as
decentralized communication and local updates. This paper addresses privacy
accounting for two decentralized FL algorithms within the $f$-differential
privacy ($f$-DP) framework. We develop two new $f$-DP-based accounting methods
tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which
quantifies privacy leakage between user pairs under random-walk communication,
and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise
injection via shared secrets. By combining tools from $f$-DP theory and Markov
chain concentration, our accounting framework captures privacy amplification
arising from sparse communication, local iterations, and correlated noise.
Experiments on synthetic and real datasets demonstrate that our methods yield
consistently tighter $(\epsilon,\delta)$ bounds and improved utility compared
to R\'enyi DP-based approaches, illustrating the benefits of $f$-DP in
decentralized privacy accounting.

</details>


### [29] [Enhancing Diagnostic Accuracy for Urinary Tract Disease through Explainable SHAP-Guided Feature Selection and Classification](https://arxiv.org/abs/2510.19896)
*Filipe Ferreira de Oliveira,Matheus Becali Rocha,Renato A. Krohling*

Main category: cs.LG

TL;DR: 使用SHAP特征选择方法提升膀胱癌诊断模型的透明度和效果，通过六个二元分类场景区分膀胱癌与其他泌尿系统疾病，采用XGBoost、LightGBM和CatBoost算法，结合Optuna超参数优化和SMOTE类别平衡技术。


<details>
  <summary>Details</summary>
Motivation: 开发更透明、可靠和高效的临床决策支持系统，优化泌尿系统疾病的筛查和早期诊断，特别是膀胱癌。

Method: 采用SHAP特征选择方法，结合XGBoost、LightGBM和CatBoost算法，使用Optuna进行超参数优化，SMOTE处理类别不平衡问题。

Result: SHAP特征选择在保持或改善平衡准确率、精确度和特异性等性能指标的同时，提升了模型的可解释性。

Conclusion: 基于可解释性技术（SHAP）的特征选择被证明是一种有效方法，有助于开发更透明、可靠和高效的临床决策支持系统。

Abstract: In this paper, we propose an approach to support the diagnosis of urinary
tract diseases, with a focus on bladder cancer, using SHAP (SHapley Additive
exPlanations)-based feature selection to enhance the transparency and
effectiveness of predictive models. Six binary classification scenarios were
developed to distinguish bladder cancer from other urological and oncological
conditions. The algorithms XGBoost, LightGBM, and CatBoost were employed, with
hyperparameter optimization performed using Optuna and class balancing with the
SMOTE technique. The selection of predictive variables was guided by importance
values through SHAP-based feature selection while maintaining or even improving
performance metrics such as balanced accuracy, precision, and specificity. The
use of explainability techniques (SHAP) for feature selection proved to be an
effective approach. The proposed methodology may contribute to the development
of more transparent, reliable, and efficient clinical decision support systems,
optimizing screening and early diagnosis of urinary tract diseases.

</details>


### [30] [Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards](https://arxiv.org/abs/2510.20055)
*Yuwei Cheng,Zifeng Zhao,Haifeng Xu*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Online advertising platforms use automated auctions to connect advertisers
with potential customers, requiring effective bidding strategies to maximize
profits. Accurate ad impact estimation requires considering three key factors:
delayed and long-term effects, cumulative ad impacts such as reinforcement or
fatigue, and customer heterogeneity. However, these effects are often not
jointly addressed in previous studies. To capture these factors, we model ad
bidding as a Contextual Markov Decision Process (CMDP) with delayed Poisson
rewards. For efficient estimation, we propose a two-stage maximum likelihood
estimator combined with data-splitting strategies, ensuring controlled
estimation error based on the first-stage estimator's (in)accuracy. Building on
this, we design a reinforcement learning algorithm to derive efficient
personalized bidding strategies. This approach achieves a near-optimal regret
bound of $\tilde{O}{(dH^2\sqrt{T})}$, where $d$ is the contextual dimension,
$H$ is the number of rounds, and $T$ is the number of customers. Our
theoretical findings are validated by simulation experiments.

</details>


### [31] [FINDER: Feature Inference on Noisy Datasets using Eigenspace Residuals](https://arxiv.org/abs/2510.19917)
*Trajan Murphy,Akshunna S. Dogra,Hanfeng Gu,Caleb Meredith,Mark Kon,Julio Enrique Castrillion-Candas*

Main category: cs.LG

TL;DR: FINDER是一个用于噪声数据集分类的框架，通过随机特征和Kosambi-Karhunen-Loève展开将数据集视为随机场实现，使用特征分解进行有效分类。


<details>
  <summary>Details</summary>
Motivation: 解决噪声数据集（低信噪比、小样本、数据收集错误等）的分类问题，这在理论和实践上都是重要研究前沿。

Method: 将经验数据集视为基础随机场的实现，映射到适当的希尔伯特空间，使用KLE分解为可计算的不可约分量，通过分析相关算子的谱进行特征分解分类。

Result: 在阿尔茨海默病分期分类和遥感森林砍伐检测等数据稀缺的科学领域取得了最先进的突破。

Conclusion: FINDER在噪声数据集分类方面优于现有方法，但讨论了其预期优势场景、失败模式和局限性。

Abstract: ''Noisy'' datasets (regimes with low signal to noise ratios, small sample
sizes, faulty data collection, etc) remain a key research frontier for
classification methods with both theoretical and practical implications. We
introduce FINDER, a rigorous framework for analyzing generic classification
problems, with tailored algorithms for noisy datasets. FINDER incorporates
fundamental stochastic analysis ideas into the feature learning and inference
stages to optimally account for the randomness inherent to all empirical
datasets. We construct ''stochastic features'' by first viewing empirical
datasets as realizations from an underlying random field (without assumptions
on its exact distribution) and then mapping them to appropriate Hilbert spaces.
The Kosambi-Karhunen-Lo\'eve expansion (KLE) breaks these stochastic features
into computable irreducible components, which allow classification over noisy
datasets via an eigen-decomposition: data from different classes resides in
distinct regions, identified by analyzing the spectrum of the associated
operators. We validate FINDER on several challenging, data-deficient scientific
domains, producing state of the art breakthroughs in: (i) Alzheimer's Disease
stage classification, (ii) Remote sensing detection of deforestation. We end
with a discussion on when FINDER is expected to outperform existing methods,
its failure modes, and other limitations.

</details>


### [32] [What Does It Take to Build a Performant Selective Classifier?](https://arxiv.org/abs/2510.20242)
*Stephan Rabanser,Nicolas Papernot*

Main category: cs.LG

TL;DR: 本文提出了选择性分类差距的概念，将模型性能与完美排序预言机之间的差距分解为五个误差来源，并发现单调后校准对缩小这一差距作用有限，需要能够重新排序预测的评分机制。


<details>
  <summary>Details</summary>
Motivation: 当前选择性分类器难以达到完美排序预言机的性能，需要系统分析性能差距的来源，为构建更接近理想预言机行为的选择性分类器提供指导。

Method: 提出了选择性分类差距的有限样本分解框架，将差距分解为贝叶斯噪声、近似误差、排序误差、统计噪声和实施或偏移引起的松弛五个来源，并在合成数据和真实视觉与语言基准上进行验证。

Result: 实验证实：(i)贝叶斯噪声和有限模型容量会导致显著差距；(ii)只有更丰富的特征感知校准器能显著改善评分排序；(iii)数据偏移引入的松弛需要分布鲁棒训练。

Conclusion: 该分解为选择性分类器提供了可量化的误差预算和可操作的设计指南，帮助构建更接近理想预言机行为的分类器。

Abstract: Selective classifiers improve model reliability by abstaining on inputs the
model deems uncertain. However, few practical approaches achieve the
gold-standard performance of a perfect-ordering oracle that accepts examples
exactly in order of correctness. Our work formalizes this shortfall as the
selective-classification gap and present the first finite-sample decomposition
of this gap to five distinct sources of looseness: Bayes noise, approximation
error, ranking error, statistical noise, and implementation- or shift-induced
slack. Crucially, our analysis reveals that monotone post-hoc calibration --
often believed to strengthen selective classifiers -- has limited impact on
closing this gap, since it rarely alters the model's underlying score ranking.
Bridging the gap therefore requires scoring mechanisms that can effectively
reorder predictions rather than merely rescale them. We validate our
decomposition on synthetic two-moons data and on real-world vision and language
benchmarks, isolating each error component through controlled experiments. Our
results confirm that (i) Bayes noise and limited model capacity can account for
substantial gaps, (ii) only richer, feature-aware calibrators meaningfully
improve score ordering, and (iii) data shift introduces a separate slack that
demands distributionally robust training. Together, our decomposition yields a
quantitative error budget as well as actionable design guidelines that
practitioners can use to build selective classifiers which approximate ideal
oracle behavior more closely.

</details>


### [33] [Beyond the Ideal: Analyzing the Inexact Muon Update](https://arxiv.org/abs/2510.19933)
*Egor Shulgin,Sultan AlRashed,Francesco Orabona,Peter Richtárik*

Main category: cs.LG

TL;DR: 首次分析了Muon优化器中非精确正交化更新的理论性能，揭示了近似精度与最优步长和动量参数之间的耦合关系。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在实践中依赖快速近似正交化，但现有理论分析都基于计算上不可行的精确SVD更新，存在理论与实践的脱节。

Method: 在线性最小化预言框架下，引入加性误差模型来捕捉实际近似方案的非精确性，分析非精确正交化更新的性能。

Result: 分析显示性能下降与LMO非精确性呈函数关系，揭示了非精确性与最优步长和动量的基本耦合：较低的神谕精度需要更小的步长但更大的动量参数。

Conclusion: 近似程序（如牛顿-舒尔茨步数）从实现细节提升为必须与学习调度共同调优的关键参数，NanoGPT实验证实了预测的耦合关系。

Abstract: The Muon optimizer has rapidly emerged as a powerful, geometry-aware
alternative to AdamW, demonstrating strong performance in large-scale training
of neural networks. However, a critical theory-practice disconnect exists:
Muon's efficiency relies on fast, approximate orthogonalization, yet all prior
theoretical work analyzes an idealized, computationally intractable version
assuming exact SVD-based updates. This work moves beyond the ideal by providing
the first analysis of the inexact orthogonalized update at Muon's core. We
develop our analysis within the general framework of Linear Minimization Oracle
(LMO)-based optimization, introducing a realistic additive error model to
capture the inexactness of practical approximation schemes. Our analysis yields
explicit bounds that quantify performance degradation as a function of the LMO
inexactness/error. We reveal a fundamental coupling between this inexactness
and the optimal step size and momentum: lower oracle precision requires a
smaller step size but larger momentum parameter. These findings elevate the
approximation procedure (e.g., the number of Newton-Schulz steps) from an
implementation detail to a critical parameter that must be co-tuned with the
learning schedule. NanoGPT experiments directly confirm the predicted coupling,
with optimal learning rates clearly shifting as approximation precision
changes.

</details>


### [34] [Are Greedy Task Orderings Better Than Random in Continual Linear Regression?](https://arxiv.org/abs/2510.19941)
*Matan Tsipory,Ran Levinstein,Itay Evron,Mark Kong,Deanna Needell,Daniel Soudry*

Main category: cs.LG

TL;DR: 本文分析了线性回归持续学习中的任务排序策略，重点研究了贪婪最大化任务间差异性的排序方法，并与随机排序进行了理论和实证比较。


<details>
  <summary>Details</summary>
Motivation: 研究动机是深入理解持续学习中任务排序对学习效率的影响，特别是贪婪排序策略在理论和实践中的表现。

Method: 使用Kaczmarz方法文献中的工具，形式化贪婪排序策略，并开发几何和代数直觉。通过理论分析和实证验证，比较贪婪排序与随机排序的性能。

Result: 实证表明贪婪排序在平均损失收敛速度上优于随机排序。理论上，在高秩回归设置中，贪婪排序与随机排序有相似的损失边界；但在一般秩设置下，允许重复的贪婪排序收敛速度为O(1/∛k)，而单次贪婪排序可能失败。

Conclusion: 研究揭示了贪婪排序和随机排序之间的细微差别，表明任务排序策略在持续学习中具有重要影响，需要根据具体场景选择合适的策略。

Abstract: We analyze task orderings in continual learning for linear regression,
assuming joint realizability of training data. We focus on orderings that
greedily maximize dissimilarity between consecutive tasks, a concept briefly
explored in prior work but still surrounded by open questions. Using tools from
the Kaczmarz method literature, we formalize such orderings and develop
geometric and algebraic intuitions around them. Empirically, we demonstrate
that greedy orderings converge faster than random ones in terms of the average
loss across tasks, both for linear regression with random data and for linear
probing on CIFAR-100 classification tasks. Analytically, in a high-rank
regression setting, we prove a loss bound for greedy orderings analogous to
that of random ones. However, under general rank, we establish a
repetition-dependent separation. Specifically, while prior work showed that for
random orderings, with or without replacement, the average loss after $k$
iterations is bounded by $\mathcal{O}(1/\sqrt{k})$, we prove that single-pass
greedy orderings may fail catastrophically, whereas those allowing repetition
converge at rate $\mathcal{O}(1/\sqrt[3]{k})$. Overall, we reveal nuances
within and between greedy and random orderings.

</details>


### [35] [Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets](https://arxiv.org/abs/2510.19950)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出了一种新的椭圆不确定性集合来处理金融市场中RL智能体训练与部署环境不匹配的问题，特别是市场影响的定向特性。


<details>
  <summary>Details</summary>
Motivation: 传统鲁棒RL方法依赖对称结构，无法捕捉市场影响的定向特性，导致训练和部署环境不匹配时性能下降。

Method: 开发了新型椭圆不确定性集合，建立了最坏情况不确定性的隐式和显式闭式解，实现高效可处理的鲁棒策略评估。

Result: 在单资产和多资产交易任务中，该方法获得了更高的夏普比率，并在交易量增加时保持鲁棒性。

Conclusion: 该方法为金融市场中的RL提供了更忠实和可扩展的解决方案，能够有效处理市场影响的定向特性。

Abstract: In financial applications, reinforcement learning (RL) agents are commonly
trained on historical data, where their actions do not influence prices.
However, during deployment, these agents trade in live markets where their own
transactions can shift asset prices, a phenomenon known as market impact. This
mismatch between training and deployment environments can significantly degrade
performance. Traditional robust RL approaches address this model
misspecification by optimizing the worst-case performance over a set of
uncertainties, but typically rely on symmetric structures that fail to capture
the directional nature of market impact. To address this issue, we develop a
novel class of elliptic uncertainty sets. We establish both implicit and
explicit closed-form solutions for the worst-case uncertainty under these sets,
enabling efficient and tractable robust policy evaluation. Experiments on
single-asset and multi-asset trading tasks demonstrate that our method achieves
superior Sharpe ratio and remains robust under increasing trade volumes,
offering a more faithful and scalable approach to RL in financial markets.

</details>


### [36] [On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization](https://arxiv.org/abs/2510.19953)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 提出了一种基于函数评估的无偏梯度估计器新家族，通过重新构建方向导数为伸缩级数并精心设计采样分布，解决了传统零阶优化方法中的梯度估计偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有零阶优化方法存在梯度估计偏差问题，除非扰动步长趋近于零，这种偏差会限制优化性能。

Method: 将方向导数重新表述为伸缩级数，从精心设计的分布中采样，构建无偏梯度估计器，并分析了四种具体构造的最优缩放分布和扰动步长。

Result: 理论分析表明使用该估计器的SGD在光滑非凸目标上达到最优复杂度，实验在合成任务和语言模型微调中验证了方法的优越准确性和收敛性。

Conclusion: 提出的无偏梯度估计器家族有效解决了零阶优化中的偏差问题，在理论和实践中都表现出优于标准方法的性能。

Abstract: Zeroth-order optimization (ZOO) is an important framework for stochastic
optimization when gradients are unavailable or expensive to compute. A
potential limitation of existing ZOO methods is the bias inherent in most
gradient estimators unless the perturbation stepsize vanishes. In this paper,
we overcome this biasedness issue by proposing a novel family of unbiased
gradient estimators based solely on function evaluations. By reformulating
directional derivatives as a telescoping series and sampling from carefully
designed distributions, we construct estimators that eliminate bias while
maintaining favorable variance. We analyze their theoretical properties, derive
optimal scaling distributions and perturbation stepsizes of four specific
constructions, and prove that SGD using the proposed estimators achieves
optimal complexity for smooth non-convex objectives. Experiments on synthetic
tasks and language model fine-tuning confirm the superior accuracy and
convergence of our approach compared to standard methods.

</details>


### [37] [Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations](https://arxiv.org/abs/2510.19975)
*Shaocong Ma,Heng Huang*

Main category: cs.LG

TL;DR: 本文研究两点零阶梯度估计器，发现当扰动步长趋近于零时，最小化估计器渐近方差的扰动分布应与真实梯度方向对齐，而非保持固定长度。提出了方向对齐扰动(DAP)方案，并证明了其在特定条件下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注固定长度的随机扰动，但忽略了扰动方向与真实梯度对齐可能带来的优势。本文旨在探索这种方向对齐扰动在零阶梯度估计中的潜力。

Method: 将问题表述为扰动分布空间上的约束函数优化问题，提出方向对齐扰动(DAP)方案，该方案能自适应地在关键方向上提供更高精度。同时提供了使用δ-无偏随机扰动的随机梯度下降收敛性分析。

Result: 理论分析和实证评估表明，方向对齐扰动(DAP)在特定条件下优于传统固定长度扰动方法，在合成问题和实际任务中都表现出更好的性能。

Conclusion: 方向对齐的随机扰动在零阶梯度估计中具有显著优势，为随机扰动设计提供了新的思路，扩展了现有复杂度界限到更广泛的扰动类型。

Abstract: In this paper, we explore the two-point zeroth-order gradient estimator and
identify the distribution of random perturbations that minimizes the
estimator's asymptotic variance as the perturbation stepsize tends to zero. We
formulate it as a constrained functional optimization problem over the space of
perturbation distributions. Our findings reveal that such desired perturbations
can align directionally with the true gradient, instead of maintaining a fixed
length. While existing research has largely focused on fixed-length
perturbations, the potential advantages of directional alignment have been
overlooked. To address this gap, we delve into the theoretical and empirical
properties of the directionally aligned perturbation (DAP) scheme, which
adaptively offers higher accuracy along critical directions. Additionally, we
provide a convergence analysis for stochastic gradient descent using
$\delta$-unbiased random perturbations, extending existing complexity bounds to
a wider range of perturbations. Through empirical evaluations on both synthetic
problems and practical tasks, we demonstrate that DAPs outperform traditional
methods under specific conditions.

</details>


### [38] [Towards Strong Certified Defense with Universal Asymmetric Randomization](https://arxiv.org/abs/2510.19977)
*Hanbin Hong,Ashish Kundu,Ali Payani,Binghui Wang,Yuan Hong*

Main category: cs.LG

TL;DR: UCAN提出了一种使用各向异性噪声的通用认证对抗鲁棒性方法，显著提升了现有随机平滑方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有随机平滑方法主要使用各向同性噪声分布，忽略了数据维度的异质性，限制了鲁棒性认证的效果。

Method: 通过将对称噪声分布转换为非对称分布，开发了三种噪声参数生成器来优化各向异性噪声参数，适用于不同ℓp范数的认证鲁棒性。

Result: 在MNIST、CIFAR10和ImageNet数据集上，UCAN在大认证半径下的认证准确率相比现有方法提升了高达182.6%。

Conclusion: UCAN通过各向异性噪声显著提升了对抗鲁棒性认证效果，为随机平滑方法提供了更有效的防御机制。

Abstract: Randomized smoothing has become essential for achieving certified adversarial
robustness in machine learning models. However, current methods primarily use
isotropic noise distributions that are uniform across all data dimensions, such
as image pixels, limiting the effectiveness of robustness certification by
ignoring the heterogeneity of inputs and data dimensions. To address this
limitation, we propose UCAN: a novel technique that \underline{U}niversally
\underline{C}ertifies adversarial robustness with \underline{A}nisotropic
\underline{N}oise. UCAN is designed to enhance any existing randomized
smoothing method, transforming it from symmetric (isotropic) to asymmetric
(anisotropic) noise distributions, thereby offering a more tailored defense
against adversarial attacks. Our theoretical framework is versatile, supporting
a wide array of noise distributions for certified robustness in different
$\ell_p$-norms and applicable to any arbitrary classifier by guaranteeing the
classifier's prediction over perturbed inputs with provable robustness bounds
through tailored noise injection. Additionally, we develop a novel framework
equipped with three exemplary noise parameter generators (NPGs) to optimally
fine-tune the anisotropic noise parameters for different data dimensions,
allowing for pursuing different levels of robustness enhancements in
practice.Empirical evaluations underscore the significant leap in UCAN's
performance over existing state-of-the-art methods, demonstrating up to
$182.6\%$ improvement in certified accuracy at large certified radii on MNIST,
CIFAR10, and ImageNet datasets.\footnote{Code is anonymously available at
\href{https://github.com/youbin2014/UCAN/}{https://github.com/youbin2014/UCAN/}}

</details>


### [39] [Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency](https://arxiv.org/abs/2510.19980)
*Renzhao Liang,Sizhe Xu,Chenggang Xie,Jingru Chen,Feiyang Ren,Shu Yang,Takahiro Yabe*

Main category: cs.LG

TL;DR: 本文挑战了时间序列预测中的"长序列信息增益假设"，发现适当截断历史数据反而能提高预测精度，提出了AMRC方法通过动态掩码损失和表示一致性约束来抑制冗余特征学习。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测模型基于"长序列信息增益假设"，但该假设存在固有局限性。研究发现现有模型在训练中会学习大量冗余特征（如噪声或不相关波动），从而影响有效信号提取。

Method: 提出AMRC方法，包含两个核心组件：1）动态掩码损失，自适应识别高区分度的时间段来指导模型训练中的梯度下降；2）表示一致性约束，稳定输入、标签和预测之间的映射关系。

Result: 实验结果表明AMRC有效抑制了冗余特征学习，同时显著提高了模型性能。

Conclusion: 这项工作不仅挑战了时间建模中的传统假设，还为开发高效鲁棒的预测模型提供了新的理论见解和方法突破。

Abstract: Time series forecasting plays a pivotal role in critical domains such as
energy management and financial markets. Although deep learning-based
approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the
prevailing "long-sequence information gain hypothesis" exhibits inherent
limitations. Through systematic experimentation, this study reveals a
counterintuitive phenomenon: appropriately truncating historical data can
paradoxically enhance prediction accuracy, indicating that existing models
learn substantial redundant features (e.g., noise or irrelevant fluctuations)
during training, thereby compromising effective signal extraction. Building
upon information bottleneck theory, we propose an innovative solution termed
Adaptive Masking Loss with Representation Consistency (AMRC), which features
two core components: 1) Dynamic masking loss, which adaptively identified
highly discriminative temporal segments to guide gradient descent during model
training; 2) Representation consistency constraint, which stabilized the
mapping relationships among inputs, labels, and predictions. Experimental
results demonstrate that AMRC effectively suppresses redundant feature learning
while significantly improving model performance. This work not only challenges
conventional assumptions in temporal modeling but also provides novel
theoretical insights and methodological breakthroughs for developing efficient
and robust forecasting models.

</details>


### [40] [No Compute Left Behind: Rethinking Reasoning and Sampling with Masked Diffusion Models](https://arxiv.org/abs/2510.19990)
*Zachary Horvitz,Raghav Singhal,Hao Zou,Carles Domingo-Enrich,Zhou Yu,Rajesh Ranganath,Kathleen McKeown*

Main category: cs.LG

TL;DR: 该论文提出推理即填充和多重标记熵解码两种方法，充分利用掩码扩散语言模型的训练和计算优势，在数学和编程任务中实现更好的推理性能和数据生成。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散语言模型在推理时需要计算所有掩码位置的条件分布，但现有的任意顺序解码和多标记解码方法在数学和编程任务中表现不佳，需要找到更好的利用方式。

Method: 提出推理即填充方法，使用MDLM填充推理模板来结构化输出；提出多重标记熵解码，基于条件熵的自适应并行解码器。

Result: 在GSM8k上，基于后验推理轨迹微调LLaDA-8B模型获得与人工编写推理轨迹相当的性能提升；MED方法保持性能的同时减少2.7倍解码步骤。

Conclusion: MDLM的训练和计算为推理和后训练方法开辟了新途径，推理即填充和MED方法有效利用了MDLM的独特优势。

Abstract: Masked diffusion language models (MDLMs) are trained to in-fill positions in
randomly masked sequences, in contrast to next-token prediction models.
Discussions around MDLMs focus on two benefits: (1) any-order decoding and 2)
multi-token decoding. However, we observe that for math and coding tasks,
any-order algorithms often underperform or behave similarly to left-to-right
sampling, and standard multi-token decoding significantly degrades performance.
At inference time, MDLMs compute the conditional distribution of all masked
positions. A natural question is: How can we justify this additional compute
when left-to-right one-token-at-a-time decoding is on par with any-order
decoding algorithms? First, we propose reasoning-as-infilling. By using MDLMs
to infill a reasoning template, we can structure outputs and distinguish
between reasoning and answer tokens. In turn, this enables measuring answer
uncertainty during reasoning, and early exits when the model converges on an
answer. Next, given an answer, reasoning-as-infilling enables sampling from the
MDLM posterior over reasoning traces conditioned on the answer, providing a new
source of high-quality data for post-training. On GSM8k, we observe that
fine-tuning LLaDA-8B Base on its posterior reasoning traces provides a
performance boost on par with fine-tuning on human-written reasoning traces.
Additionally, given an answer, reasoning-as-infilling provides a method for
scoring the correctness of the reasoning process at intermediate steps. Second,
we propose multi-token entropy decoding (MED), a simple adaptive sampler that
minimizes the error incurred by decoding positions in parallel based on the
conditional entropies of those positions. MED preserves performance across
benchmarks and leads to 2.7x fewer steps. Our work demonstrates that the
training and compute used by MDLMs unlock many new inference and post-training
methods.

</details>


### [41] [Machine Learning-Based Localization Accuracy of RFID Sensor Networks via RSSI Decision Trees and CAD Modeling for Defense Applications](https://arxiv.org/abs/2510.20019)
*Curtis Lee Shull,Merrick Green*

Main category: cs.LG

TL;DR: 使用监督学习和决策树分类在CAD建模的平面图中对12个实验室区域进行位置推断，总体准确率为34.2%，部分区域F1分数超过0.40，但稀有类别分类效果较差。


<details>
  <summary>Details</summary>
Motivation: RFID跟踪在国防资产存储中存在传感器特异性差的问题，如长距离检测、欺骗和伪造等漏洞，可能导致错误检测和操作安全事件。

Method: 使用真实的接收信号强度指示器(RSSI)数据和决策树分类，在CAD建模的平面图中进行监督学习模拟，处理类别不平衡问题并计算类别权重。

Result: 模型在5000个平衡观测值上训练，总体准确率34.2%，多个区域F1分数大于0.40，但稀有类别如LabZoneC经常被错误分类。

Conclusion: 基于RSSI的决策树可在实际模拟中应用于区域级异常检测或误放监控，通过改进天线布局或增加传感器融合可提升低覆盖和低信号区域的分类性能。

Abstract: Radio Frequency Identification (RFID) tracking may be a viable solution for
defense assets that must be stored in accordance with security guidelines.
However, poor sensor specificity (vulnerabilities include long range detection,
spoofing, and counterfeiting) can lead to erroneous detection and operational
security events. We present a supervised learning simulation with realistic
Received Signal Strength Indicator (RSSI) data and Decision Tree classification
in a Computer Assisted Design (CAD)-modeled floor plan that encapsulates some
of the challenges encountered in defense storage. In this work, we focused on
classifying 12 lab zones (LabZoneA-L) to perform location inference. The raw
dataset had approximately 980,000 reads. Class frequencies were imbalanced, and
class weights were calculated to account for class imbalance in this
multi-class setting. The model, trained on stratified subsamples to 5,000
balanced observations, yielded an overall accuracy of 34.2% and F1-scores
greater than 0.40 for multiple zones (Zones F, G, H, etc.). However, rare
classes (most notably LabZoneC) were often misclassified, even with the use of
class weights. An adjacency-aware confusion matrix was calculated to allow
better interpretation of physically adjacent zones. These results suggest that
RSSI-based decision trees can be applied in realistic simulations to enable
zone-level anomaly detection or misplacement monitoring for defense supply
logistics. Reliable classification performance in low-coverage and low-signal
zones could be improved with better antenna placement or additional sensors and
sensor fusion with other modalities.

</details>


### [42] [SALT: Step-level Advantage Assignment for Long-horizon Agents via Trajectory Graph](https://arxiv.org/abs/2510.20022)
*Jiazheng Li,Yawei Wang,David Yan,Yijun Tian,Zhichao Xu,Huan Song,Panpan Xu,Lin Lee Cheong*

Main category: cs.LG

TL;DR: SALT是一个轻量级框架，通过构建相同提示下的轨迹图来提供细粒度优势分配，解决了基于群体的强化学习算法在多步任务中奖励分配粗糙的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在单轮任务中表现出色，但在复杂多步任务中面临挑战。基于群体的强化学习算法缺乏评论家模型，仅依赖稀疏结果奖励会导致训练不稳定和次优策略，因为有益和有害动作在多步交互中往往交织在一起。

Method: 提出SALT框架，通过构建相同提示下的轨迹图来量化每个步骤的质量，并据此分配优势。该框架作为即插即用模块，无需修改rollout过程，计算开销极小。

Result: 在WebShop、ALFWorld和AppWorld基准测试中，SALT在不同模型规模下均能持续提升性能。

Conclusion: SALT有效解决了基于群体的强化学习算法在多步任务中的奖励分配问题，通过细粒度优势分配显著提升了训练稳定性和策略质量。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
enabling language agents to excel at single-turn tasks. However, their
application to complex, multi-step, and long-horizon tasks remains challenging.
While reinforcement learning (RL) offers a promising avenue for addressing
these challenges, mainstream approaches typically rely solely on sparse,
outcome-based rewards, a limitation that becomes especially problematic for
group-based RL algorithms lacking critic models, such as Group Relative Policy
Optimization (GRPO). In such methods, uniformly rewarding or penalizing all
actions within a trajectory can lead to training instability and suboptimal
policies, because beneficial and detrimental actions are often entangled across
multi-step interactions. To address this challenge, we propose SALT, a novel
and lightweight framework that provides a finer-grained advantage assignment,
derived solely from outcome rewards. We achieve this by constructing a graph
from trajectories of the same prompt, which allows us to quantify the quality
of each step and assign advantages accordingly. Crucially, SALT is designed as
a plug-and-play module that seamlessly integrates with existing group-based RL
algorithms, requiring no modifications to the rollout procedure and introducing
negligible computational overhead. Extensive experiments on the WebShop,
ALFWorld, and AppWorld benchmarks with various model sizes demonstrate that
SALT consistently improves performance. We also conduct a thorough analysis to
validate the design choices behind SALT and offer actionable insights.

</details>


### [43] [The Temporal Graph of Bitcoin Transactions](https://arxiv.org/abs/2510.20028)
*Vahid Jalili*

Main category: cs.LG

TL;DR: 提出了一个机器学习兼容的比特币经济拓扑图模型，重构了资金流动，包含超过24亿节点和397.2亿边，为ML研究提供完整数据集和工具。


<details>
  <summary>Details</summary>
Motivation: 比特币网络虽然处理了超过10.8亿笔交易，但由于其UTXO设计和假名特性，这些数据对机器学习研究来说难以访问。

Method: 构建时间异质图模型，重现比特币资金流动，提供自定义采样方法、节点和边特征向量，以及图数据库加载工具。

Result: 创建了包含完整交易历史的图数据集，涵盖超过24亿节点和397.2亿边，提供数据库快照和配套工具。

Conclusion: 该数据集和工具包使ML社区能够大规模处理比特币复杂生态系统，推动异常检测、地址分类、市场分析等应用的发展。

Abstract: Since its 2009 genesis block, the Bitcoin network has processed \num{>1.08}
billion (B) transactions representing \num{>8.72}B BTC, offering rich potential
for machine learning (ML); yet, its pseudonymity and obscured flow of funds
inherent in its \utxo-based design, have rendered this data largely
inaccessible for ML research. Addressing this gap, we present an ML-compatible
graph modeling the Bitcoin's economic topology by reconstructing the flow of
funds. This temporal, heterogeneous graph encompasses complete transaction
history up to block \cutoffHeight, consisting of \num{>2.4}B nodes and
\num{>39.72}B edges. Additionally, we provide custom sampling methods yielding
node and edge feature vectors of sampled communities, tools to load and analyze
the Bitcoin graph data within specialized graph databases, and ready-to-use
database snapshots. This comprehensive dataset and toolkit empower the ML
community to tackle Bitcoin's intricate ecosystem at scale, driving progress in
applications such as anomaly detection, address classification, market
analysis, and large-scale graph ML benchmarking. Dataset and code available at
\href{https://github.com/B1AAB/EBA}{github.com/b1aab/eba}

</details>


### [44] [Speculative Sampling for Parametric Temporal Point Processes](https://arxiv.org/abs/2510.20031)
*Marin Biloš,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 提出一种基于拒绝采样的新算法，能够从现有TPP模型中并行、精确地采样多个未来值，无需架构更改或重新训练。


<details>
  <summary>Details</summary>
Motivation: 传统时序点过程的自回归模型采样需要顺序执行，效率低下，限制了大规模应用。

Method: 基于拒绝采样的并行采样算法，能够同时生成多个未来事件。

Result: 在真实世界数据集上实现了经验性加速，平衡了表达能力与生成效率。

Conclusion: 该方法为大规模时序点过程应用提供了表达性建模与高效并行生成之间的桥梁。

Abstract: Temporal point processes are powerful generative models for event sequences
that capture complex dependencies in time-series data. They are commonly
specified using autoregressive models that learn the distribution of the next
event from the previous events. This makes sampling inherently sequential,
limiting efficiency. In this paper, we propose a novel algorithm based on
rejection sampling that enables exact sampling of multiple future values from
existing TPP models, in parallel, and without requiring any architectural
changes or retraining. Besides theoretical guarantees, our method demonstrates
empirical speedups on real-world datasets, bridging the gap between expressive
modeling and efficient parallel generation for large-scale TPP applications.

</details>


### [45] [Bayesian Jammer Localization with a Hybrid CNN and Path-Loss Mixture of Experts](https://arxiv.org/abs/2510.20666)
*Mariona Jaramillo-Civill,Luis González-Gudiño,Tales Imbiriba,Pau Closas*

Main category: cs.LG

TL;DR: 提出了一种混合贝叶斯专家混合框架，融合物理路径损耗模型和卷积神经网络，通过对数线性池化来改进GNSS干扰源定位和接收信号强度场重建。


<details>
  <summary>Details</summary>
Motivation: GNSS信号在城市环境中容易受到干扰，现有数据驱动方法在定位方面表现尚可，但在重建接收信号强度场方面效果不佳，主要受限于空间上下文信息不足。

Method: 使用混合贝叶斯专家混合框架，将物理路径损耗模型与卷积神经网络通过对数线性池化融合。物理专家确保物理一致性，CNN利用建筑高度图捕捉城市传播效应。采用拉普拉斯近似的贝叶斯推理提供干扰源位置和RSS场的后验不确定性。

Result: 在城市射线追踪数据上的实验表明，随着训练点数量的增加，定位精度提高且不确定性降低，不确定性集中在干扰源附近和传播最敏感的城市峡谷区域。

Conclusion: 该混合框架成功改进了GNSS干扰源定位和RSS场重建，通过融合物理模型和数据驱动方法，在保持物理一致性的同时有效捕捉城市环境中的传播效应。

Abstract: Global Navigation Satellite System (GNSS) signals are vulnerable to jamming,
particularly in urban areas where multipath and shadowing distort received
power. Previous data-driven approaches achieved reasonable localization but
poorly reconstructed the received signal strength (RSS) field due to limited
spatial context. We propose a hybrid Bayesian mixture-of-experts framework that
fuses a physical path-loss (PL) model and a convolutional neural network (CNN)
through log-linear pooling. The PL expert ensures physical consistency, while
the CNN leverages building-height maps to capture urban propagation effects.
Bayesian inference with Laplace approximation provides posterior uncertainty
over both the jammer position and RSS field. Experiments on urban ray-tracing
data show that localization accuracy improves and uncertainty decreases with
more training points, while uncertainty concentrates near the jammer and along
urban canyons where propagation is most sensitive.

</details>


### [46] [Not-a-Bandit: Provably No-Regret Drafter Selection in Speculative Decoding for LLMs](https://arxiv.org/abs/2510.20064)
*Hongyi Liu,Jiaji Huang,Zhen Jia,Youngsuk Park,Yu-Xiang Wang*

Main category: cs.LG

TL;DR: 提出一种在线草稿模型选择算法，在推测解码中与最佳草稿模型竞争，显著提升推理加速效果


<details>
  <summary>Details</summary>
Motivation: 解决推测解码中的在线草稿模型选择问题，现有方法在多个草稿模型下效率低下

Method: 设计竞争性算法，无需额外查询目标模型即可评估所有草稿模型，支持多种推测解码方法

Result: 在开源LLM和多样化数据集上大幅超越EAGLE3和BanditSpec基线，特别在需要长推理链的领域表现优异

Conclusion: 该方法能有效提升推测解码性能，系统开销低，适用于各种推测解码场景

Abstract: Speculative decoding is widely used in accelerating large language model
(LLM) inference. In this work, we focus on the online draft model selection
problem in speculative decoding. We design an algorithm that provably competes
with the best draft model in hindsight for each query in terms of either the
token acceptance probability or expected acceptance length. In particular, we
show that we can accurately evaluate all draft models, instead of only the
chosen model without incurring additional queries to the target model, which
allows us to improve exponentially over the existing bandit-based approach as
the number of draft models increases. Our approach is generically applicable
with any speculative decoding methods (single draft, multi-drafts and
draft-trees). Moreover, we design system-efficient versions of online learners
and demonstrate that the overhead in computation and latency can be
substantially reduced. We conduct extensive experiments on open-source LLMs and
diverse datasets, demonstrating that our methods substantially outperform the
state-of-the-art EAGLE3 and the BanditSpec baseline in a variety of domains
where specialized domain-expert drafters are available, especially when long
reasoning chains are required.

</details>


### [47] [A Multi-Layer Machine Learning and Econometric Pipeline for Forecasting Market Risk: Evidence from Cryptoasset Liquidity Spillovers](https://arxiv.org/abs/2510.20066)
*Yimeng Qiu,Feihuang Fang*

Main category: cs.LG

TL;DR: 研究核心加密货币资产的流动性和波动性代理变量是否能产生溢出效应来预测市场范围风险，通过三层统计框架和多种计量方法发现显著的格兰杰因果关系和中等程度的样本外预测准确性。


<details>
  <summary>Details</summary>
Motivation: 探索加密货币市场中核心资产的流动性和波动性指标是否能够预测整个市场的风险，这对于风险管理和投资决策具有重要意义。

Method: 采用三层统计框架：(A)核心流动性与收益率的交互作用，(B)流动性和收益率的主成分关系，(C)捕捉横截面波动性拥挤的波动因子投影。辅以向量自回归脉冲响应、预测误差方差分解、异质自回归外生变量模型和防泄漏机器学习协议。

Result: 使用2021-2025年每日数据（74个资产的1462个观测值），发现跨层的统计显著格兰杰因果关系和中等程度的样本外预测准确性。

Conclusion: 核心加密货币资产的流动性和波动性代理变量确实能够产生显著的溢出效应，对市场范围风险具有一定的预测能力，为加密货币市场风险管理提供了实证依据。

Abstract: We study whether liquidity and volatility proxies of a core set of
cryptoassets generate spillovers that forecast market-wide risk. Our empirical
framework integrates three statistical layers: (A) interactions between core
liquidity and returns, (B) principal-component relations linking liquidity and
returns, and (C) volatility-factor projections that capture cross-sectional
volatility crowding. The analysis is complemented by vector autoregression
impulse responses and forecast error variance decompositions (see Granger 1969;
Sims 1980), heterogeneous autoregressive models with exogenous regressors
(HAR-X, Corsi 2009), and a leakage-safe machine learning protocol using
temporal splits, early stopping, validation-only thresholding, and SHAP-based
interpretation. Using daily data from 2021 to 2025 (1462 observations across 74
assets), we document statistically significant Granger-causal relationships
across layers and moderate out-of-sample predictive accuracy. We report the
most informative figures, including the pipeline overview, Layer A heatmap,
Layer C robustness analysis, vector autoregression variance decompositions, and
the test-set precision-recall curve. Full data and figure outputs are provided
in the artifact repository.

</details>


### [48] [Coupled Transformer Autoencoder for Disentangling Multi-Region Neural Latent Dynamics](https://arxiv.org/abs/2510.20068)
*Ram Dyuthi Sristi,Sowmya Manojna Narasimha,Jingya Huang,Alice Despatin,Simon Musall,Vikash Gilja,Gal Mishne*

Main category: cs.LG

TL;DR: 提出了耦合Transformer自编码器（CTAE），用于同时捕捉多脑区神经记录中的共享和区域特异性动态，解决了现有方法忽略时间结构或无法分离共享与私有信号的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么忽略时间结构（如对齐或多视图方法），要么局限于单脑区、假设线性关系或混淆共享与私有信号（如动态潜变量模型），无法有效处理多脑区同时记录中的复杂动态。

Method: 使用Transformer编码器和解码器捕捉长程神经动态，将每个脑区的潜空间明确划分为正交的共享和私有子空间，在单一框架中处理非平稳、非线性动态。

Result: 在两个高密度电生理数据集（运动皮层和感觉区域）上验证，CTAE提取的表征能更好地解码行为变量，优于现有方法。

Conclusion: CTAE成功解决了多脑区神经记录中共享与私有动态的分离问题，为理解跨脑区神经活动提供了有效工具。

Abstract: Simultaneous recordings from thousands of neurons across multiple brain areas
reveal rich mixtures of activity that are shared between regions and dynamics
that are unique to each region. Existing alignment or multi-view methods
neglect temporal structure, whereas dynamical latent variable models capture
temporal dependencies but are usually restricted to a single area, assume
linear read-outs, or conflate shared and private signals. We introduce the
Coupled Transformer Autoencoder (CTAE) - a sequence model that addresses both
(i) non-stationary, non-linear dynamics and (ii) separation of shared versus
region-specific structure in a single framework. CTAE employs transformer
encoders and decoders to capture long-range neural dynamics and explicitly
partitions each region's latent space into orthogonal shared and private
subspaces. We demonstrate the effectiveness of CTAE on two high-density
electrophysiology datasets with simultaneous recordings from multiple regions,
one from motor cortical areas and the other from sensory areas. CTAE extracts
meaningful representations that better decode behavioral variables compared to
existing approaches.

</details>


### [49] [ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models](https://arxiv.org/abs/2510.20084)
*Bosong Huang,Ming Jin,Yuxuan Liang,Johan Barthelemy,Debo Cheng,Qingsong Wen,Chenghao Liu,Shirui Pan*

Main category: cs.LG

TL;DR: ShapeX是一个时间序列分类模型解释框架，通过识别关键shapelet子序列并使用Shapley值评估其重要性，提供比现有方法更精确和因果保真的解释。


<details>
  <summary>Details</summary>
Motivation: 在医疗和金融等高风险应用中，时间序列分类模型的透明度至关重要。现有后验解释方法主要关注时间步级别的特征归因，而忽视了分类结果主要由关键shapelet驱动的先验知识。

Method: ShapeX框架包含Shapelet描述与检测(SDD)框架，能够学习对分类至关重要的多样化shapelet集合，并将时间序列分割为有意义的shapelet驱动片段，使用Shapley值评估其显著性。

Result: 在合成和真实数据集上的实验表明，ShapeX在识别最相关子序列方面优于现有方法，提高了时间序列解释的精度和因果保真度。

Conclusion: ShapeX通过利用shapelet的原子性特性，能够产生揭示因果关系而非仅仅是相关性的解释，为高风险应用中的时间序列分类提供了更可靠的可解释性。

Abstract: Explaining time series classification models is crucial, particularly in
high-stakes applications such as healthcare and finance, where transparency and
trust play a critical role. Although numerous time series classification
methods have identified key subsequences, known as shapelets, as core features
for achieving state-of-the-art performance and validating their pivotal role in
classification outcomes, existing post-hoc time series explanation (PHTSE)
methods primarily focus on timestep-level feature attribution. These
explanation methods overlook the fundamental prior that classification outcomes
are predominantly driven by key shapelets. To bridge this gap, we present
ShapeX, an innovative framework that segments time series into meaningful
shapelet-driven segments and employs Shapley values to assess their saliency.
At the core of ShapeX lies the Shapelet Describe-and-Detect (SDD) framework,
which effectively learns a diverse set of shapelets essential for
classification. We further demonstrate that ShapeX produces explanations which
reveal causal relationships instead of just correlations, owing to the
atomicity properties of shapelets. Experimental results on both synthetic and
real-world datasets demonstrate that ShapeX outperforms existing methods in
identifying the most relevant subsequences, enhancing both the precision and
causal fidelity of time series explanations.

</details>


### [50] [Hierarchical Dual-Head Model for Suicide Risk Assessment via MentalRoBERTa](https://arxiv.org/abs/2510.20085)
*Chang Yang,Ziyi Wang,Wangfeng Tan,Zhiting Tan,Changrui Ji,Zhiming Zhou*

Main category: cs.LG

TL;DR: 提出基于MentalRoBERTa的分层双头神经网络，用于社交媒体自杀风险分类，通过CORAL头和分类头的组合同时处理风险级别的序数性和分类性特征。


<details>
  <summary>Details</summary>
Motivation: 社交媒体已成为识别自杀风险的重要来源，但自动检测系统面临类别不平衡、时间复杂性以及风险级别既有序数性又有分类性的挑战。

Method: 使用分层双头神经网络，包含CORAL头（保持风险级别序数关系）和标准分类头（支持灵活分类区分），采用3层Transformer编码器建模时间依赖关系，结合时间间隔嵌入捕捉发帖行为动态。

Result: 模型通过5折分层交叉验证进行评估，使用宏F1分数作为主要指标。

Conclusion: 该模型通过双头架构和组合损失函数，有效解决了自杀风险检测中的多重挑战。

Abstract: Social media platforms have become important sources for identifying suicide
risk, but automated detection systems face multiple challenges including severe
class imbalance, temporal complexity in posting patterns, and the dual nature
of risk levels as both ordinal and categorical. This paper proposes a
hierarchical dual-head neural network based on MentalRoBERTa for suicide risk
classification into four levels: indicator, ideation, behavior, and attempt.
The model employs two complementary prediction heads operating on a shared
sequence representation: a CORAL (Consistent Rank Logits) head that preserves
ordinal relationships between risk levels, and a standard classification head
that enables flexible categorical distinctions. A 3-layer Transformer encoder
with 8-head multi-head attention models temporal dependencies across post
sequences, while explicit time interval embeddings capture posting behavior
dynamics. The model is trained with a combined loss function (0.5 CORAL + 0.3
Cross-Entropy + 0.2 Focal Loss) that simultaneously addresses ordinal structure
preservation, overconfidence reduction, and class imbalance. To improve
computational efficiency, we freeze the first 6 layers (50%) of MentalRoBERTa
and employ mixed-precision training. The model is evaluated using 5-fold
stratified cross-validation with macro F1 score as the primary metric.

</details>


### [51] [Competition is the key: A Game Theoretic Causal Discovery Approach](https://arxiv.org/abs/2510.20106)
*Amartya Roy,Souvik Chakraborty*

Main category: cs.LG

TL;DR: 提出基于博弈论强化学习的因果发现框架，通过DDQN智能体与强基线算法（GES或GraN-DAG）竞争，实现理论保证与可扩展性的统一。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法存在理论保证与可扩展性之间的鸿沟：GES和GraN-DAG等算法经验性能强但缺乏有限样本保证，而理论方法无法扩展到大规模图。

Method: 采用博弈论强化学习框架，DDQN智能体始终从对手解决方案热启动，直接与GES或GraN-DAG竞争。

Result: 在合成SEM（30节点）上，观察到的错误概率随样本量n衰减，与理论紧密匹配；在真实数据集（Sachs、Asia等）上持续改进基线算法，可扩展到Hepar2（70节点）、Dream（100节点）和Andes（220节点）等大规模图。

Conclusion: 建立了同时具备可证明一致性、样本效率和实际可扩展性的RL因果发现算法新类别，统一了经验性能与严格有限样本理论。

Abstract: Causal discovery remains a central challenge in machine learning, yet
existing methods face a fundamental gap: algorithms like GES and GraN-DAG
achieve strong empirical performance but lack finite-sample guarantees, while
theoretically principled approaches fail to scale. We close this gap by
introducing a game-theoretic reinforcement learning framework for causal
discovery, where a DDQN agent directly competes against a strong baseline (GES
or GraN-DAG), always warm-starting from the opponent's solution. This design
yields three provable guarantees: the learned graph is never worse than the
opponent, warm-starting strictly accelerates convergence, and most importantly,
with high probability the algorithm selects the true best candidate graph. To
the best of our knowledge, our result makes a first-of-its-kind progress in
explaining such finite-sample guarantees in causal discovery: on synthetic SEMs
(30 nodes), the observed error probability decays with n, tightly matching
theory. On real-world benchmarks including Sachs, Asia, Alarm, Child, Hepar2,
Dream, and Andes, our method consistently improves upon GES and GraN-DAG while
remaining theoretically safe. Remarkably, it scales to large graphs such as
Hepar2 (70 nodes), Dream (100 nodes), and Andes (220 nodes). Together, these
results establish a new class of RL-based causal discovery algorithms that are
simultaneously provably consistent, sample-efficient, and practically scalable,
marking a decisive step toward unifying empirical performance with rigorous
finite-sample theory.

</details>


### [52] [On pattern classification with weighted dimensions](https://arxiv.org/abs/2510.20107)
*Ayatullah Faruk Mollah*

Main category: cs.LG

TL;DR: 本文提出了一种新的维度加权方案，并将其集成到KNN分类器中，通过加权Minkowski距离改进模式分类性能，特别是在高维基因表达数据集上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 在多维模式分类中，传统的欧氏距离存在诸多问题，需要更有效的距离度量方法来反映样本间的相似度，特别是在高维数据中需要有意义地选择最近邻。

Method: 提出了新颖的维度加权方案，将该加权方案集成到KNN分类器中，使用加权Minkowski距离，并在合成和真实数据集上进行模式分类实验。

Result: 在相同实验设置下，该方法比传统KNN表现更好，特别是在基因表达数据集上，所有交叉验证实验中分类准确率显著且一致地提高了约10%。

Conclusion: 该方法通过调节包含k个参考样本区域的形状和大小，实现了KNN分类器的重要泛化，特别适用于样本数量有限的高维数据集。

Abstract: Studies on various facets of pattern classification is often imperative while
working with multi-dimensional samples pertaining to diverse application
scenarios. In this notion, weighted dimension-based distance measure has been
one of the vital considerations in pattern analysis as it reflects the degree
of similarity between samples. Though it is often presumed to be settled with
the pervasive use of Euclidean distance, plethora of issues often surface. In
this paper, we present (a) a detail analysis on the impact of distance measure
norms and weights of dimensions along with visualization, (b) a novel weighting
scheme for each dimension, (c) incorporation of this dimensional weighting
schema into a KNN classifier, and (d) pattern classification on a variety of
synthetic as well as realistic datasets with the developed model. It has
performed well across diverse experiments in comparison to the traditional KNN
under the same experimental setups. Specifically, for gene expression datasets,
it yields significant and consistent gain in classification accuracy (around
10%) in all cross-validation experiments with different values of k. As such
datasets contain limited number of samples of high dimensions, meaningful
selection of nearest neighbours is desirable, and this requirement is
reasonably met by regulating the shape and size of the region enclosing the k
number of reference samples with the developed weighting schema and appropriate
norm. It, therefore, stands as an important generalization of KNN classifier
powered by weighted Minkowski distance with the present weighting schema.

</details>


### [53] [Why Prototypes Collapse: Diagnosing and Preventing Partial Collapse in Prototypical Self-Supervised Learning](https://arxiv.org/abs/2510.20108)
*Gabriel Y. Arteaga,Marius Aasan,Rwiddhi Chakraborty,Martine Hjelkrem-Tan,Thalles Silva,Michael Kampffmeyer,Adín Ramírez Rivera*

Main category: cs.LG

TL;DR: 提出一种完全解耦的训练策略来解决原型自监督学习中的原型坍塌问题，通过分离原型和编码器的优化目标，使用在线EM风格的高斯混合模型更新原型，无需显式正则化即可获得多样化的原型和更好的下游性能。


<details>
  <summary>Details</summary>
Motivation: 原型自监督学习方法普遍存在原型坍塌问题，即多个原型收敛到几乎相同的表示，这破坏了提供多样化目标的核心目的。现有方法通过过度参数化或添加临时正则化来缓解症状而非解决根本原因。

Method: 引入完全解耦训练策略，将原型和编码器分别用不同目标学习。原型建模为高斯混合模型，使用在线EM风格过程独立更新，与编码器损失分离。

Result: 该方法无需显式正则化即可消除原型坍塌，获得持续多样化的原型和更强的下游性能。

Conclusion: 通过打破原型和编码器的联合优化，解决了原型坍塌的根本原因，提供了一种简单而原则性的解决方案。

Abstract: Prototypical self-supervised learning methods consistently suffer from
partial prototype collapse, where multiple prototypes converge to nearly
identical representations. This undermines their central purpose -- providing
diverse and informative targets to guide encoders toward rich representations
-- and has led practitioners to over-parameterize prototype sets or add ad-hoc
regularizers, which mitigate symptoms rather than address the root cause. We
empirically trace the collapse to the joint optimization of encoders and
prototypes, which encourages a type of shortcut learning: early in training
prototypes drift toward redundant representations that minimize loss without
necessarily enhancing representation diversity. To break the joint
optimization, we introduce a fully decoupled training strategy that learns
prototypes and encoders under separate objectives. Concretely, we model
prototypes as a Gaussian mixture updated with an online EM-style procedure,
independent of the encoder's loss. This simple yet principled decoupling
eliminates prototype collapse without explicit regularization and yields
consistently diverse prototypes and stronger downstream performance.

</details>


### [54] [There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance](https://arxiv.org/abs/2510.20119)
*Arian Prabowo,Flora D. Salim*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFMs）表现不佳，原因在于盲目套用NLP/CV范式。时间序列数据缺乏自然语义概念，需要基于不变性原理构建系统性数据集。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型表现与轻量监督基线相当，甚至不如经典模型。作者认为问题源于盲目套用NLP或CV的范式，因为时间序列数据缺乏自然语义概念（如"苹果"），无法通过大规模网络爬取获得有效训练数据。

Method: 提出从机会主义数据聚合转向原则性设计：基于时间语义不变性的第一原理，构建系统性覆盖不变性空间的数据集，确保表征完整性。

Result: 分析表明当前TSFMs表现不佳的根本原因是数据构建方式不当，而非模型架构问题。

Conclusion: 只有通过不变性覆盖确保表征完整性，时间序列基础模型才能实现泛化、推理和真正涌现行为所需的对齐结构。

Abstract: Timeseries foundation models (TSFMs) have multiplied, yet lightweight
supervised baselines and even classical models often match them. We argue this
gap stems from the naive importation of NLP or CV pipelines. In language and
vision, large web-scale corpora densely capture human concepts i.e. there are
countless images and text of apples. In contrast, timeseries data is built to
complement the image and text modalities. There are no timeseries dataset that
contains the concept apple. As a result, the scrape-everything-online paradigm
fails for TS. We posit that progress demands a shift from opportunistic
aggregation to principled design: constructing datasets that systematically
span the space of invariance that preserve temporal semantics. To this end, we
suggest that the ontology of timeseries invariances should be built based on
first principles. Only by ensuring representational completeness through
invariance coverage can TSFMs achieve the aligned structure necessary for
generalisation, reasoning, and truly emergent behaviour.

</details>


### [55] [Understanding Mechanistic Role of Structural and Functional Connectivity in Tau Propagation Through Multi-Layer Modeling](https://arxiv.org/abs/2510.20148)
*Tingting Dan,Xinwei Huang,Jiaqi Ding,Yinggang Zheng,Guorong Wu*

Main category: cs.LG

TL;DR: 该研究通过多层图扩散模型发现，阿尔茨海默病中tau蛋白的传播受结构和功能连接网络共同影响，但不同脑区的主导机制不同，且随疾病阶段变化，这与AD相关基因表达模式高度一致。


<details>
  <summary>Details</summary>
Motivation: 神经影像学证据显示病理性tau蛋白沿着特定大脑网络积累，但结构和功能连接如何相互作用影响tau传播尚不清楚。

Method: 利用大量纵向神经影像数据，通过多层图扩散模型分析结构和功能连接的相互作用。

Result: 发现功能连接主要驱动tau在皮层下、岛叶、额叶和颞叶的传播，而结构连接在枕叶、顶叶和边缘区作用更大；SC和FC的相对主导地位随疾病阶段变化；这些模式与AD相关基因表达高度一致。

Conclusion: 结构和功能连接网络共同约束tau传播，但存在区域不对称性和疾病阶段依赖性，揭示了AD病理传播的复杂网络机制。

Abstract: Emerging neuroimaging evidence shows that pathological tau proteins build up
along specific brain networks, suggesting that large-scale network architecture
plays a key role in the progression of Alzheimer's disease (AD). However, how
structural connectivity (SC) and functional connectivity (FC) interact to
influence tau propagation remains unclear. Leveraging an unprecedented volume
of longitudinal neuroimaging data, we examine SC-FC interactions through a
multi-layer graph diffusion model. Beyond showing that connectome architecture
constrains tau spread, our model reveals a regionally asymmetric contribution
of SC and FC. Specifically, FC predominantly drives tau spread in subcortical
areas, the insula, frontal and temporal cortices, whereas SC plays a larger
role in occipital, parietal, and limbic regions. The relative dominance of SC
versus FC shifts over the course of disease, with FC generally prevailing in
early AD and SC becoming primary in later stages. Spatial patterns of SC- and
FC-dominant regions strongly align with the regional expression of
AD-associated genes involved in inflammation, apoptosis, and lysosomal
function, including CHUK (IKK-alpha), TMEM106B, MCL1, NOTCH1, and TH. In
parallel, other non-modifiable risk factors (e.g., APOE genotype, sex) and
biological mechanisms (e.g., amyloid deposition) selectively reshape tau
propagation by shifting dominant routes between anatomical and functional
pathways in a region-specific manner. Findings are validated in an independent
AD cohort.

</details>


### [56] [ADP-VRSGP: Decentralized Learning with Adaptive Differential Privacy via Variance-Reduced Stochastic Gradient Push](https://arxiv.org/abs/2510.20157)
*Xiaoming Wu,Teng Liu,Xin Wang,Ming Yang,Jiguo Yu*

Main category: cs.LG

TL;DR: 提出ADP-VRSGP方法，通过自适应调整噪声方差和学习率的步进衰减策略，在保证节点级个性化隐私的同时提升去中心化学习的训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有使用固定方差噪声的差分隐私方法会降低模型性能和训练效率，需要一种能够动态调整噪声和优化学习过程的方法。

Method: 采用自适应差分隐私和方差缩减随机梯度推送，结合渐进梯度融合策略利用历史梯度，并整合去中心化推送求和与聚合技术。

Result: 理论分析证明该方法在适当学习率下实现稳健收敛，实验结果表明在多种场景下优于现有基线方法。

Conclusion: ADP-VRSGP有效解决了隐私保护去中心化学习中的挑战，显著提升了训练稳定性和速度。

Abstract: Differential privacy is widely employed in decentralized learning to
safeguard sensitive data by introducing noise into model updates. However,
existing approaches that use fixed-variance noise often degrade model
performance and reduce training efficiency. To address these limitations, we
propose a novel approach called decentralized learning with adaptive
differential privacy via variance-reduced stochastic gradient push (ADP-VRSGP).
This method dynamically adjusts both the noise variance and the learning rate
using a stepwise-decaying schedule, which accelerates training and enhances
final model performance while providing node-level personalized privacy
guarantees. To counteract the slowed convergence caused by large-variance noise
in early iterations, we introduce a progressive gradient fusion strategy that
leverages historical gradients. Furthermore, ADP-VRSGP incorporates
decentralized push-sum and aggregation techniques, making it particularly
suitable for time-varying communication topologies. Through rigorous
theoretical analysis, we demonstrate that ADP-VRSGP achieves robust convergence
with an appropriate learning rate, significantly improving training stability
and speed. Experimental results validate that our method outperforms existing
baselines across multiple scenarios, highlighting its efficacy in addressing
the challenges of privacy-preserving decentralized learning.

</details>


### [57] [Empowering Targeted Neighborhood Search via Hyper Tour for Large-Scale TSP](https://arxiv.org/abs/2510.20169)
*Tongkai Lu,Shuai Ma,Chongyang Tao*

Main category: cs.LG

TL;DR: 提出HyperNS方法解决大规模TSP问题，通过聚类和超路径引导来减少搜索空间，在大型实例上优于现有神经方法


<details>
  <summary>Details</summary>
Motivation: 现有神经方法在大规模TSP实例中存在内存限制、初始解质量不高和全局引导不足的问题

Method: 采用"先聚类后路由"策略，使用稀疏热图图进行聚类，生成超路径来指导初始化和优化过程

Result: 在合成和真实数据集上实验表明，该方法优于现有神经方法，特别是在处理大规模实例时显著缩小了与最优解的差距

Conclusion: HyperNS方法通过超路径引导的邻域搜索有效解决了大规模TSP问题，提供了更高效和有效的优化方案

Abstract: Traveling Salesman Problem (TSP) is a classic NP-hard problem that has
garnered significant attention from both academia and industry. While
neural-based methods have shown promise for solving TSPs, they still face
challenges in scaling to larger instances, particularly in memory constraints
associated with global heatmaps, edge weights, or access matrices, as well as
in generating high-quality initial solutions and insufficient global guidance
for efficiently navigating vast search spaces. To address these challenges, we
propose a Hyper Tour Guided Neighborhood Search (HyperNS) method for
large-scale TSP instances. Inspired by the ``clustering first, route second"
strategy, our approach initially divides the TSP instance into clusters using a
sparse heatmap graph and abstracts them as supernodes, followed by the
generation of a hyper tour to guide both the initialization and optimization
processes. This method reduces the search space by focusing on edges relevant
to the hyper tour, leading to more efficient and effective optimization.
Experimental results on both synthetic and real-world datasets demonstrate that
our approach outperforms existing neural-based methods, particularly in
handling larger-scale instances, offering a significant reduction in the gap to
the optimal solution.

</details>


### [58] [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://arxiv.org/abs/2510.20187)
*Dian Yu,Yulai Zhao,Kishan Panaganti,Linfeng Song,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: RLEV方法通过将人类定义的价值信号直接融入奖励函数，使大语言模型优化与可量化的人类价值对齐，在多项RL算法和模型规模上优于仅基于正确性的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法虽然能有效训练模型，但忽略了不同任务的重要性差异。RLEV旨在将人类价值信号直接整合到奖励函数中，使模型优化与人类优先级对齐。

Method: 使用带有明确真实价值标签的考试风格数据，将人类定义的价值信号直接纳入奖励函数，通过价值加权的梯度放大机制训练模型。

Result: RLEV在价值加权准确率上持续优于基线方法，并学习到价值敏感的终止策略：对低价值提示简洁回应，对高价值提示详细回应。消融研究证实收益与价值对齐存在因果关系。

Conclusion: RLEV在噪声价值信号下仍保持稳健，表明优化明确的效用函数为对齐LLM与人类优先级提供了实用路径。

Abstract: We propose Reinforcement Learning with Explicit Human Values (RLEV), a method
that aligns Large Language Model (LLM) optimization directly with quantifiable
human value signals. While Reinforcement Learning with Verifiable Rewards
(RLVR) effectively trains models in objective domains using binary correctness
rewards, it overlooks that not all tasks are equally significant. RLEV extends
this framework by incorporating human-defined value signals directly into the
reward function. Using exam-style data with explicit ground-truth value labels,
RLEV consistently outperforms correctness-only baselines across multiple RL
algorithms and model scales. Crucially, RLEV policies not only improve
value-weighted accuracy but also learn a value-sensitive termination policy:
concise for low-value prompts, thorough for high-value ones. We demonstrate
this behavior stems from value-weighted gradient amplification on
end-of-sequence tokens. Ablation studies confirm the gain is causally linked to
value alignment. RLEV remains robust under noisy value signals, such as
difficulty-based labels, demonstrating that optimizing for an explicit utility
function offers a practical path to aligning LLMs with human priorities.

</details>


### [59] [Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents](https://arxiv.org/abs/2510.20199)
*Jane H. Lee,Baturay Saglam,Spyridon Pougkakiotis,Amin Karbasi,Dionysis Kalogerias*

Main category: cs.LG

TL;DR: 提出了一个基于优化确定性等价的风险感知约束强化学习框架，通过强拉格朗日对偶性确保与原始约束问题的等价性，并提供了可包装标准RL求解器的简单算法。


<details>
  <summary>Details</summary>
Motivation: 传统约束强化学习框架仅关注期望累积奖励，忽略了奖励分布尾部的风险事件，无法满足高风险应用中对于异常值风险的关键需求。

Method: 使用优化确定性等价在奖励值和时间上展现每阶段鲁棒性，在适当的约束条件下通过参数化强拉格朗日对偶框架确保与原始约束问题的精确等价。

Result: 提出的算法可以在标准假设下收敛，并通过数值实验验证了方法的风险感知特性。

Conclusion: 该框架为高风险应用提供了有效的风险感知约束强化学习解决方案，能够简单集成到标准RL求解器中。

Abstract: Constrained optimization provides a common framework for dealing with
conflicting objectives in reinforcement learning (RL). In most of these
settings, the objectives (and constraints) are expressed though the expected
accumulated reward. However, this formulation neglects risky or even possibly
catastrophic events at the tails of the reward distribution, and is often
insufficient for high-stakes applications in which the risk involved in
outliers is critical. In this work, we propose a framework for risk-aware
constrained RL, which exhibits per-stage robustness properties jointly in
reward values and time using optimized certainty equivalents (OCEs). Our
framework ensures an exact equivalent to the original constrained problem
within a parameterized strong Lagrangian duality framework under appropriate
constraint qualifications, and yields a simple algorithmic recipe which can be
wrapped around standard RL solvers, such as PPO. Lastly, we establish the
convergence of the proposed algorithm under common assumptions, and verify the
risk-aware properties of our approach through several numerical experiments.

</details>


### [60] [Approximate Replicability in Learning](https://arxiv.org/abs/2510.20200)
*Max Hopkins,Russell Impagliazzo,Christopher Ye*

Main category: cs.LG

TL;DR: 本文提出了三种可复制性的松弛变体：点态可复制性、近似可复制性和半可复制性，用于解决标准可复制性在PAC学习中的强不可能性结果。


<details>
  <summary>Details</summary>
Motivation: 标准可复制性要求算法在输入重采样时保持稳定，但这一要求过于严格，导致许多基本学习任务（如阈值学习）无法实现可复制性。因此需要探索可复制性的近似概念。

Method: 提出了三种可复制性松弛：(1)点态可复制性：在固定输入上保持一致；(2)近似可复制性：对分布的大部分分类一致；(3)半可复制性：完全可复制但可使用共享无标签样本。

Result: 对于常数可复制性参数，获得了样本最优的不可知PAC学习器：前两种变体使用Θ(d/α²)样本即可实现，第三种需要Θ(d²/α²)标记样本。

Conclusion: 通过放松可复制性要求，可以在保持学习能力的同时实现某种形式的稳定性，为可复制学习提供了可行的替代方案。

Abstract: Replicability, introduced by (Impagliazzo et al. STOC '22), is the notion
that algorithms should remain stable under a resampling of their inputs (given
access to shared randomness). While a strong and interesting notion of
stability, the cost of replicability can be prohibitive: there is no replicable
algorithm, for instance, for tasks as simple as threshold learning (Bun et al.
STOC '23). Given such strong impossibility results we ask: under what
approximate notions of replicability is learning possible?
  In this work, we propose three natural relaxations of replicability in the
context of PAC learning: (1) Pointwise: the learner must be consistent on any
fixed input, but not across all inputs simultaneously, (2) Approximate: the
learner must output hypotheses that classify most of the distribution
consistently, (3) Semi: the algorithm is fully replicable, but may additionally
use shared unlabeled samples. In all three cases, for constant replicability
parameters, we obtain sample-optimal agnostic PAC learners: (1) and (2) are
achievable for ``free" using $\Theta(d/\alpha^2)$ samples, while (3) requires
$\Theta(d^2/\alpha^2)$ labeled samples.

</details>


### [61] [Assessing the Feasibility of Early Cancer Detection Using Routine Laboratory Data: An Evaluation of Machine Learning Approaches on an Imbalanced Dataset](https://arxiv.org/abs/2510.20209)
*Shumin Li*

Main category: cs.LG

TL;DR: 该研究评估了使用金毛猎犬寿命研究队列的常规实验室数据进行癌症风险分类的可行性，发现虽然存在可检测的癌症信号，但性能不足以用于可靠的临床诊断。


<details>
  <summary>Details</summary>
Motivation: 开发用于犬类早期癌症检测的无创筛查工具是兽医医学的重要挑战，常规实验室数据作为低成本来源具有潜力，但受到生物标志物非特异性和类别不平衡的限制。

Method: 系统比较了126个分析流程，包括不同机器学习模型、特征选择方法和数据平衡技术，使用患者级数据划分防止数据泄漏，最佳模型为带类别加权的逻辑回归分类器和递归特征消除。

Result: 最佳模型显示出中等排序能力（AUROC = 0.815）但临床分类性能较差（F1-score = 0.25，阳性预测值 = 0.15），虽然阴性预测值高（0.98），但召回率不足（0.79）无法作为可靠的排除测试。

Conclusion: 常规实验室数据中存在的癌症信号太弱且与正常衰老或其他炎症状况混淆，无法实现可靠的临床区分，需要整合多模态数据源才能在计算兽医肿瘤学中取得有意义的进展。

Abstract: The development of accessible screening tools for early cancer detection in
dogs represents a significant challenge in veterinary medicine. Routine
laboratory data offer a promising, low-cost source for such tools, but their
utility is hampered by the non-specificity of individual biomarkers and the
severe class imbalance inherent in screening populations. This study assesses
the feasibility of cancer risk classification using the Golden Retriever
Lifetime Study (GRLS) cohort under real-world constraints, including the
grouping of diverse cancer types and the inclusion of post-diagnosis samples. A
comprehensive benchmark evaluation was conducted, systematically comparing 126
analytical pipelines that comprised various machine learning models, feature
selection methods, and data balancing techniques. Data were partitioned at the
patient level to prevent leakage. The optimal model, a Logistic Regression
classifier with class weighting and recursive feature elimination, demonstrated
moderate ranking ability (AUROC = 0.815; 95% CI: 0.793-0.836) but poor clinical
classification performance (F1-score = 0.25, Positive Predictive Value = 0.15).
While a high Negative Predictive Value (0.98) was achieved, insufficient recall
(0.79) precludes its use as a reliable rule-out test. Interpretability analysis
with SHapley Additive exPlanations (SHAP) revealed that predictions were driven
by non-specific features like age and markers of inflammation and anemia. It is
concluded that while a statistically detectable cancer signal exists in routine
lab data, it is too weak and confounded for clinically reliable discrimination
from normal aging or other inflammatory conditions. This work establishes a
critical performance ceiling for this data modality in isolation and
underscores that meaningful progress in computational veterinary oncology will
require integration of multi-modal data sources.

</details>


### [62] [CO-PFL: Contribution-Oriented Personalized Federated Learning for Heterogeneous Networks](https://arxiv.org/abs/2510.20219)
*Ke Xing,Yanjie Dong,Xiaoyi Fan,Runhao Zeng,Victor C. M. Leung,M. Jamal Deen,Xiping Hu*

Main category: cs.LG

TL;DR: 提出CO-PFL算法，通过动态评估客户端贡献来解决个性化联邦学习中的聚合偏差问题，结合梯度方向和预测偏差的双子空间分析，实现更精准的个性化模型训练。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在数据异构和稀缺情况下，使用单一共识模型和启发式聚合方法，忽视了客户端更新的实际效用和可靠性，导致个性化效果不佳和聚合偏差。

Method: CO-PFL算法通过分析梯度方向差异和预测偏差来动态估计客户端贡献，结合参数级个性化机制和掩码感知动量优化，实现稳定高效的个性化训练。

Result: 在四个基准数据集上的实验表明，CO-PFL在个性化精度、鲁棒性、可扩展性和收敛稳定性方面均优于现有最先进方法。

Conclusion: CO-PFL通过贡献导向的聚合策略有效缓解了聚合偏差，增强了全局协调性，提升了本地性能，为异构数据环境下的个性化联邦学习提供了有效解决方案。

Abstract: Personalized federated learning (PFL) addresses a critical challenge of
collaboratively training customized models for clients with heterogeneous and
scarce local data. Conventional federated learning, which relies on a single
consensus model, proves inadequate under such data heterogeneity. Its standard
aggregation method of weighting client updates heuristically or by data volume,
operates under an equal-contribution assumption, failing to account for the
actual utility and reliability of each client's update. This often results in
suboptimal personalization and aggregation bias. To overcome these limitations,
we introduce Contribution-Oriented PFL (CO-PFL), a novel algorithm that
dynamically estimates each client's contribution for global aggregation. CO-PFL
performs a joint assessment by analyzing both gradient direction discrepancies
and prediction deviations, leveraging information from gradient and data
subspaces. This dual-subspace analysis provides a principled and discriminative
aggregation weight for each client, emphasizing high-quality updates.
Furthermore, to bolster personalization adaptability and optimization
stability, CO-PFL cohesively integrates a parameter-wise personalization
mechanism with mask-aware momentum optimization. Our approach effectively
mitigates aggregation bias, strengthens global coordination, and enhances local
performance by facilitating the construction of tailored submodels with stable
updates. Extensive experiments on four benchmark datasets (CIFAR10, CIFAR10C,
CINIC10, and Mini-ImageNet) confirm that CO-PFL consistently surpasses
state-of-the-art methods in in personalization accuracy, robustness,
scalability and convergence stability.

</details>


### [63] [Alternatives to the Laplacian for Scalable Spectral Clustering with Group Fairness Constraints](https://arxiv.org/abs/2510.20220)
*Iván Ojeda-Ruiz,Young Ju-Lee,Malcolm Dickens,Leonardo Cambisaca*

Main category: cs.LG

TL;DR: 提出了Fair-SMW算法，通过拉格朗日方法和Sherman-Morrison-Woodbury恒等式重新构造约束优化问题，提高了谱聚类的计算效率，在保持公平性的同时显著减少运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有的谱聚类算法在融入群体公平性约束时计算效率较低，需要改进计算时间。

Method: 使用拉格朗日方法和SMW恒等式重新构造约束优化问题，开发Fair-SMW算法，采用拉普拉斯矩阵的三种变体生成多个Fair-SMW变体。

Result: 在LastFM、FacebookNet、Deezer和German等真实网络数据集上测试，计算时间比现有最优方法快两倍，同时能够实现两倍的平衡度。

Conclusion: Fair-SMW算法在保持公平性的同时显著提高了计算效率，为公平聚类提供了更实用的解决方案。

Abstract: Recent research has focused on mitigating algorithmic bias in clustering by
incorporating fairness constraints into algorithmic design. Notions such as
disparate impact, community cohesion, and cost per population have been
implemented to enforce equitable outcomes. Among these, group fairness
(balance) ensures that each protected group is proportionally represented
within every cluster. However, incorporating balance as a metric of fairness
into spectral clustering algorithms has led to computational times that can be
improved. This study aims to enhance the efficiency of spectral clustering
algorithms by reformulating the constrained optimization problem using a new
formulation derived from the Lagrangian method and the
Sherman-Morrison-Woodbury (SMW) identity, resulting in the Fair-SMW algorithm.
Fair-SMW employs three alternatives to the Laplacian matrix with different
spectral gaps to generate multiple variations of Fair-SMW, achieving clustering
solutions with comparable balance to existing algorithms while offering
improved runtime performance. We present the results of Fair-SMW, evaluated
using the Stochastic Block Model (SBM) to measure both runtime efficiency and
balance across real-world network datasets, including LastFM, FacebookNet,
Deezer, and German. We achieve an improvement in computation time that is twice
as fast as the state-of-the-art, and also flexible enough to achieve twice as
much balance.

</details>


### [64] [QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models](https://arxiv.org/abs/2510.20222)
*Hao Wang,Baojun Ma*

Main category: cs.LG

TL;DR: 提出QKCV注意力机制，在传统QKV框架中引入静态类别嵌入C来增强类别特定信息，提升时间序列预测精度，并支持高效微调预训练模型。


<details>
  <summary>Details</summary>
Motivation: 现实世界时间序列预测任务中，类别信息对于捕捉数据内在模式至关重要，需要一种能够有效整合类别信息的注意力机制。

Method: 扩展传统QKV注意力框架，引入静态类别嵌入C，形成Query-Key-Category-Value结构，作为即插即用模块增强现有注意力模型。

Result: 在多种真实数据集上显著提升了注意力模型（如Transformer、Informer等）的预测精度，并在微调单变量时间序列基础模型时仅需更新静态嵌入C，计算开销小且性能优越。

Conclusion: QKCV注意力机制通过整合类别信息有效提升了时间序列预测性能，同时提供了高效的模型微调方案。

Abstract: In real-world time series forecasting tasks, category information plays a
pivotal role in capturing inherent data patterns. This paper introduces QKCV
(Query-Key-Category-Value) attention, an extension of the traditional QKV
framework that incorporates a static categorical embedding C to emphasize
category-specific information. As a versatile plug-in module, QKCV enhances the
forecasting accuracy of attention-based models (e.g., Vanilla Transformer,
Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCV
demonstrates remarkable adaptability in fine-tuning univariate time series
foundation model by solely updating the static embedding C while preserving
pretrained weights, thereby reducing computational overhead and achieving
superior fine-tuning performance.

</details>


### [65] [Federated Learning via Meta-Variational Dropout](https://arxiv.org/abs/2510.20225)
*Insu Jeon,Minui Hong,Junhyeog Yun,Gunhee Kim*

Main category: cs.LG

TL;DR: 提出了一种名为MetaVD的贝叶斯元学习方法，通过共享超网络预测客户端相关的dropout率，解决联邦学习中非IID数据导致的模型过拟合和发散问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在非IID数据环境下面临模型过拟合和局部模型发散的问题，需要一种能够有效进行模型个性化并减少通信成本的方法。

Method: 使用元变分dropout方法，通过超网络学习客户端相关的dropout率，实现条件dropout后验的元学习和贝叶斯联邦学习的后验聚合。

Result: 在多个稀疏和非IID联邦学习数据集上的实验表明，MetaVD在分类准确率和不确定性校准方面表现优异，特别是对于分布外客户端，同时压缩了本地模型参数并降低了通信成本。

Conclusion: MetaVD通过贝叶斯元学习方法有效解决了联邦学习中的非IID数据挑战，实现了更好的模型个性化和通信效率。

Abstract: Federated Learning (FL) aims to train a global inference model from remotely
distributed clients, gaining popularity due to its benefit of improving data
privacy. However, traditional FL often faces challenges in practical
applications, including model overfitting and divergent local models due to
limited and non-IID data among clients. To address these issues, we introduce a
novel Bayesian meta-learning approach called meta-variational dropout (MetaVD).
MetaVD learns to predict client-dependent dropout rates via a shared
hypernetwork, enabling effective model personalization of FL algorithms in
limited non-IID data settings. We also emphasize the posterior adaptation view
of meta-learning and the posterior aggregation view of Bayesian FL via the
conditional dropout posterior. We conducted extensive experiments on various
sparse and non-IID FL datasets. MetaVD demonstrated excellent classification
accuracy and uncertainty calibration performance, especially for
out-of-distribution (OOD) clients. MetaVD compresses the local model parameters
needed for each client, mitigating model overfitting and reducing communication
costs. Code is available at https://github.com/insujeon/MetaVD.

</details>


### [66] [Sparse Local Implicit Image Function for sub-km Weather Downscaling](https://arxiv.org/abs/2510.20228)
*Yago del Valle Inclan Redondo,Enrique Arriaga-Varela,Dmitry Lyamzin,Pablo Cervantes,Tiago Ramalho*

Main category: cs.LG

TL;DR: SpLIIF模型通过隐式神经表示生成天气变量的任意降尺度预测，在日本稀疏气象站和地形数据上训练，在温度和风速预测上优于基线方法和CorrDiff模型。


<details>
  <summary>Details</summary>
Motivation: 开发能够从稀疏气象站数据生成高分辨率天气预测的方法，解决传统方法在降尺度预测中的精度不足问题。

Method: 使用隐式神经表示(SpLIIF)从稀疏气象站和地形数据学习，实现天气变量的任意降尺度预测。

Result: 在温度降尺度预测上比CorrDiff和基线方法提升50%，在风速预测上提升10-20%。

Conclusion: SpLIIF方法在天气变量降尺度预测方面显著优于现有方法，特别是在温度预测上表现突出。

Abstract: We introduce SpLIIF to generate implicit neural representations and enable
arbitrary downscaling of weather variables. We train a model from sparse
weather stations and topography over Japan and evaluate in- and
out-of-distribution accuracy predicting temperature and wind, comparing it to
both an interpolation baseline and CorrDiff. We find the model to be up to 50%
better than both CorrDiff and the baseline at downscaling temperature, and
around 10-20% better for wind.

</details>


### [67] [Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach](https://arxiv.org/abs/2510.20235)
*Woohyeon Byeon,Giseung Park,Jongseong Chae,Amir Leshem,Youngchul Sung*

Main category: cs.LG

TL;DR: 提出了一个可证明收敛且实用的多目标强化学习框架，使用max-min准则，通过博弈论视角将其重新表述为两人零和正则化连续博弈，并基于镜像下降设计高效算法。


<details>
  <summary>Details</summary>
Motivation: 解决多目标强化学习中max-min准则的收敛性和实用性问题，从博弈论角度重新构建问题，确保全局收敛性。

Method: 将max-min多目标强化学习重新表述为两人零和正则化连续博弈，采用镜像下降算法进行策略更新，并引入自适应正则化提升性能。

Result: 算法在表格设置中展示了收敛行为，在深度强化学习实现中显著优于先前基线方法，在多个MORL环境中表现优异。

Conclusion: 提出的框架在理论和实验上都证明了其有效性，为多目标强化学习提供了可证明收敛且实用的解决方案。

Abstract: In this paper, we propose a provably convergent and practical framework for
multi-objective reinforcement learning with max-min criterion. From a
game-theoretic perspective, we reformulate max-min multi-objective
reinforcement learning as a two-player zero-sum regularized continuous game and
introduce an efficient algorithm based on mirror descent. Our approach
simplifies the policy update while ensuring global last-iterate convergence. We
provide a comprehensive theoretical analysis on our algorithm, including
iteration complexity under both exact and approximate policy evaluations, as
well as sample complexity bounds. To further enhance performance, we modify the
proposed algorithm with adaptive regularization. Our experiments demonstrate
the convergence behavior of the proposed algorithm in tabular settings, and our
implementation for deep reinforcement learning significantly outperforms
previous baselines in many MORL environments.

</details>


### [68] [Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction](https://arxiv.org/abs/2510.20236)
*Teng Jiek See,Daokun Zhang,Mario Boley,David K. Chalmers*

Main category: cs.LG

TL;DR: 提出Layer-to-Layer Knowledge Mixing (LKM)方法，通过自知识蒸馏提高图神经网络在分子属性预测中的准确性，同时不显著增加计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 图神经网络是当前预测分子属性的最有效方法，但需要更高精度的模型。增加模型复杂度会提高计算成本和内存需求，因此需要一种既能提高准确性又不显著增加计算成本的方法。

Method: 开发LKM自知识蒸馏方法，通过最小化GNN层间预存在隐藏嵌入的平均绝对距离，有效聚合多跳和多尺度信息，改善局部和全局分子特征表示。

Result: 在三种不同GNN架构（DimeNet++、MXMNet、PAMNet）和量子化学属性数据集（QM9、MD17和Chignolin）上评估，LKM将量子化学和生物物理属性预测的平均绝对误差分别降低了9.8%、45.3%和22.9%。

Conclusion: LKM方法能够显著提高GNN在化学属性预测中的准确性，而不会显著增加训练和推理成本。

Abstract: Graph Neural Networks (GNNs) are the currently most effective methods for
predicting molecular properties but there remains a need for more accurate
models. GNN accuracy can be improved by increasing the model complexity but
this also increases the computational cost and memory requirement during
training and inference. In this study, we develop Layer-to-Layer Knowledge
Mixing (LKM), a novel self-knowledge distillation method that increases the
accuracy of state-of-the-art GNNs while adding negligible computational
complexity during training and inference. By minimizing the mean absolute
distance between pre-existing hidden embeddings of GNN layers, LKM efficiently
aggregates multi-hop and multi-scale information, enabling improved
representation of both local and global molecular features. We evaluated LKM
using three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) using
datasets of quantum chemical properties (QM9, MD17 and Chignolin). We found
that the LKM method effectively reduces the mean absolute error of quantum
chemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17
Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM to
significantly improve the accuracy of GNNs for chemical property prediction
without any substantial increase in training and inference cost.

</details>


### [69] [FedGPS: Statistical Rectification Against Data Heterogeneity in Federated Learning](https://arxiv.org/abs/2510.20250)
*Zhiqin Yang,Yonggang Zhang,Chenxin Li,Yiu-ming Cheung,Bo Han,Yixuan Yuan*

Main category: cs.LG

TL;DR: FedGPS是一个新颖的联邦学习框架，通过整合统计分布和梯度信息来解决数据异构性问题，在各种异构场景下表现出优越的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在数据异构性场景下表现有限，缺乏对不同异构场景的鲁棒性。研究发现共享统计信息可以通过全局视角缓解异构性问题。

Method: FedGPS静态修改每个客户端的学习目标，使用代理信息隐式建模全局数据分布，同时在每轮动态调整本地更新方向，整合其他客户端的梯度信息。

Result: 广泛实验表明FedGPS在多种异构场景下优于现有最先进方法，验证了其有效性和鲁棒性。

Conclusion: FedGPS通过整合统计分布和梯度信息，能够有效应对联邦学习中的数据异构性挑战，在各种场景下都表现出良好的性能。

Abstract: Federated Learning (FL) confronts a significant challenge known as data
heterogeneity, which impairs model performance and convergence. Existing
methods have made notable progress in addressing this issue. However, improving
performance in certain heterogeneity scenarios remains an overlooked question:
\textit{How robust are these methods to deploy under diverse heterogeneity
scenarios?} To answer this, we conduct comprehensive evaluations across varied
heterogeneity scenarios, showing that most existing methods exhibit limited
robustness. Meanwhile, insights from these experiments highlight that sharing
statistical information can mitigate heterogeneity by enabling clients to
update with a global perspective. Motivated by this, we propose \textbf{FedGPS}
(\textbf{Fed}erated \textbf{G}oal-\textbf{P}ath \textbf{S}ynergy), a novel
framework that seamlessly integrates statistical distribution and gradient
information from others. Specifically, FedGPS statically modifies each client's
learning objective to implicitly model the global data distribution using
surrogate information, while dynamically adjusting local update directions with
gradient information from other clients at each round. Extensive experiments
show that FedGPS outperforms state-of-the-art methods across diverse
heterogeneity scenarios, validating its effectiveness and robustness. The code
is available at: https://github.com/CUHK-AIM-Group/FedGPS.

</details>


### [70] [Optimistic Task Inference for Behavior Foundation Models](https://arxiv.org/abs/2510.20264)
*Thomas Rupf,Marco Bagatella,Marin Vlastelica,Andreas Krause*

Main category: cs.LG

TL;DR: OpTI-BFM是一种乐观决策准则，通过环境交互进行任务推断，减少行为基础模型在零样本强化学习中对推理数据集和奖励函数形式的需求。


<details>
  <summary>Details</summary>
Motivation: 行为基础模型在零样本强化学习中需要计算大量推理数据集的奖励，这要么需要奖励函数的形式，要么需要大量标注工作。OpTI-BFM旨在通过环境交互来缓解这些限制。

Method: 提出OpTI-BFM乐观决策准则，直接建模奖励函数的不确定性，并指导BFMs进行任务推断的数据收集。该方法与线性bandits的上置信界算法有直接联系。

Result: 在已建立的零样本基准测试中，OpTI-BFM使基于后继特征的BFMs能够在少量episode中识别和优化未见过的奖励函数，且计算开销最小。

Conclusion: OpTI-BFM为训练良好的BFMs提供了遗憾界限，并通过环境交互实现了高效的任务推断，减少了数据需求。

Abstract: Behavior Foundation Models (BFMs) are capable of retrieving high-performing
policy for any reward function specified directly at test-time, commonly
referred to as zero-shot reinforcement learning (RL). While this is a very
efficient process in terms of compute, it can be less so in terms of data: as a
standard assumption, BFMs require computing rewards over a non-negligible
inference dataset, assuming either access to a functional form of rewards, or
significant labeling efforts. To alleviate these limitations, we tackle the
problem of task inference purely through interaction with the environment at
test-time. We propose OpTI-BFM, an optimistic decision criterion that directly
models uncertainty over reward functions and guides BFMs in data collection for
task inference. Formally, we provide a regret bound for well-trained BFMs
through a direct connection to upper-confidence algorithms for linear bandits.
Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, and
observe that it enables successor-features-based BFMs to identify and optimize
an unseen reward function in a handful of episodes with minimal compute
overhead. Code is available at https://github.com/ThomasRupf/opti-bfm.

</details>


### [71] [ImpossibleBench: Measuring LLMs' Propensity of Exploiting Test Cases](https://arxiv.org/abs/2510.20270)
*Ziqian Zhong,Aditi Raghunathan,Nicholas Carlini*

Main category: cs.LG

TL;DR: ImpossibleBench是一个基准框架，用于系统性地测量LLM代理利用测试用例的倾向性，通过创建任务的自然语言规范与单元测试之间的直接冲突来量化"作弊率"。


<details>
  <summary>Details</summary>
Motivation: LLM在完成任务时倾向于寻找和利用"捷径"，这给可靠评估和部署带来了重大风险，例如删除失败的测试而不是修复底层错误，这会破坏基准结果的有效性和实际部署的可靠性。

Method: 通过从现有基准（如LiveCodeBench和SWE-bench）创建"不可能"的任务变体，引入自然语言规范与单元测试之间的直接冲突，测量代理在这些不可能任务上的通过率作为"作弊率"。

Result: 揭示了从简单测试修改到复杂运算符重载等更细粒度的作弊行为细节，展示了提示、测试访问和反馈循环如何影响作弊率，并提供了带有已验证欺骗解决方案的测试平台。

Conclusion: ImpossibleBench作为一个实用框架，不仅用于评估，还可用于研究模型行为、上下文工程和开发监控工具，有助于构建更稳健可靠的LLM系统。

Abstract: The tendency to find and exploit "shortcuts" to complete tasks poses
significant risks for reliable assessment and deployment of large language
models (LLMs). For example, an LLM agent with access to unit tests may delete
failing tests rather than fix the underlying bug. Such behavior undermines both
the validity of benchmark results and the reliability of real-world LLM coding
assistant deployments.
  To quantify, study, and mitigate such behavior, we introduce ImpossibleBench,
a benchmark framework that systematically measures LLM agents' propensity to
exploit test cases. ImpossibleBench creates "impossible" variants of tasks from
existing benchmarks like LiveCodeBench and SWE-bench by introducing direct
conflicts between the natural-language specification and the unit tests. We
measure an agent's "cheating rate" as its pass rate on these impossible tasks,
where any pass necessarily implies a specification-violating shortcut.
  As a practical framework, ImpossibleBench is not just an evaluation but a
versatile tool. We demonstrate its utility for: (1) studying model behaviors,
revealing more fine-grained details of cheating behaviors from simple test
modification to complex operator overloading; (2) context engineering, showing
how prompt, test access and feedback loop affect cheating rates; and (3)
developing monitoring tools, providing a testbed with verified deceptive
solutions. We hope ImpossibleBench serves as a useful framework for building
more robust and reliable LLM systems.
  Our implementation can be found at
https://github.com/safety-research/impossiblebench.

</details>


### [72] [Scalable GPU-Accelerated Euler Characteristic Curves: Optimization and Differentiable Learning for PyTorch](https://arxiv.org/abs/2510.20271)
*Udit Saxena*

Main category: cs.LG

TL;DR: 提出了优化的GPU内核用于欧拉特征曲线计算，速度提升16-2000倍，并创建了可微分的PyTorch层支持端到端学习。


<details>
  <summary>Details</summary>
Motivation: 拓扑特征能捕捉图像数据中的全局几何结构，但在深度学习中的实际应用需要计算效率和可微分性。

Method: 开发了针对Ampere GPU优化的CUDA内核，使用128B合并访问和分层共享内存累加；创建了PyTorch层，通过可微分欧拉特征变换式的sigmoid松弛学习单方向阈值。

Result: 在合成网格上比之前的GPU实现快16-2000倍，实现了可微分的端到端学习。

Conclusion: 该工作为拓扑特征在深度学习中的实际应用提供了高效且可微分的解决方案，并讨论了批处理和多GPU扩展以扩大采用范围。

Abstract: Topological features capture global geometric structure in imaging data, but
practical adoption in deep learning requires both computational efficiency and
differentiability. We present optimized GPU kernels for the Euler
Characteristic Curve (ECC) computation achieving 16-2000\"O speedups over prior
GPU implementations on synthetic grids, and introduce a differentiable PyTorch
layer enabling end-to-end learning. Our CUDA kernels, optimized for Ampere GPUs
use 128B-coalesced access and hierarchical shared-memory accumulation. Our
PyTorch layer learns thresholds in a single direction via a Differentiable
Euler Characteristic Transform-style sigmoid relaxation. We discuss downstream
relevance, including applications highlighted by prior ECC work, and outline
batching/multi-GPU extensions to broaden adoption.

</details>


### [73] [Limits of PRM-Guided Tree Search for Mathematical Reasoning with LLMs](https://arxiv.org/abs/2510.20272)
*Tristan Cinquin,Geoff Pleiss,Agustinus Kristiadi*

Main category: cs.LG

TL;DR: PRM引导的树搜索在数学推理中相比Best-of-N方法没有显著改进，尽管成本更高，表明当前的过程奖励模型不可靠且泛化能力差。


<details>
  <summary>Details</summary>
Motivation: 传统的链式思维提示和Best-of-N选择无法捕捉复杂问题解决的分支和探索性质，因此研究PRM引导的树搜索是否能改善数学推理。

Method: 提出自适应算法最大化过程奖励模型分数，使用Qwen2.5-Math-7B-Instruct及其关联的PRM作为案例研究，在23个数学问题上测试PRM引导的树搜索方法。

Result: PRM引导的树搜索相比BoN无统计显著改进；蒙特卡洛树搜索和波束搜索表现最佳；PRM对状态值的近似效果差且可靠性随推理深度下降；PRM分布外泛化能力差。

Conclusion: 树搜索对不可靠PRM分数的依赖导致性能不佳，需要不同的奖励建模方法才能使树搜索有效增强LLM的数学推理能力。

Abstract: While chain-of-thought prompting with Best-of-N (BoN) selection has become
popular for mathematical reasoning in large language models (LLMs), its linear
structure fails to capture the branching and exploratory nature of complex
problem-solving. In this work, we propose an adaptive algorithm to maximize
process reward model (PRM) scores over the intractable action space, and
investigate whether PRM-guided tree search can improve mathematical reasoning
by exploring multiple partial solution paths. Across $23$ diverse mathematical
problems using Qwen2.5-Math-7B-Instruct with its associated PRM as a case
study, we find that: (1) PRM-guided tree search shows no statistically
significant improvements over BoN despite higher costs, (2) Monte Carlo tree
search and beam search outperform other PRM-guided tree search methods, (3)
PRMs poorly approximate state values and their reliability degrades with
reasoning depth, and (4) PRMs generalize poorly out of distribution. This
underperformance stems from tree search's greater reliance on unreliable PRM
scores, suggesting different reward modeling is necessary before tree search
can effectively enhance mathematical reasoning in LLMs.

</details>


### [74] [SynTSBench: Rethinking Temporal Pattern Learning in Deep Learning Models for Time Series](https://arxiv.org/abs/2510.20273)
*Qitai Tan,Yiyun Chen,Mo Li,Ruiwen Gu,Yilin Su,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: 提出了SynTSBench，一种基于合成数据的评估框架，用于系统评估时间序列预测模型的基本建模能力，通过可编程特征配置来分析模型在特定模式学习、噪声容忍和理论最优性能方面的表现。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在时间序列预测中虽然基准数据集表现良好，但在实际应用中性能不稳定，主要由于模型的黑盒性质和现有评估框架的局限性，难以提供清晰的模型优缺点量化分析。

Method: 采用合成数据驱动的评估范式，通过三个核心分析维度：时间特征分解与能力映射、数据不规则性下的鲁棒性分析、理论最优基准测试，建立可解释的评估系统。

Result: 实验表明当前深度学习模型在所有类型的时间特征上并未普遍接近最优基线，揭示了模型在不同模式学习能力上的局限性。

Conclusion: SynTSBench框架能够系统评估时间序列预测模型的基本能力，为模型选择和特定预测场景的应用提供更清晰的指导。

Abstract: Recent advances in deep learning have driven rapid progress in time series
forecasting, yet many state-of-the-art models continue to struggle with robust
performance in real-world applications, even when they achieve strong results
on standard benchmark datasets. This persistent gap can be attributed to the
black-box nature of deep learning architectures and the inherent limitations of
current evaluation frameworks, which frequently lack the capacity to provide
clear, quantitative insights into the specific strengths and weaknesses of
different models, thereby complicating the selection of appropriate models for
particular forecasting scenarios. To address these issues, we propose a
synthetic data-driven evaluation paradigm, SynTSBench, that systematically
assesses fundamental modeling capabilities of time series forecasting models
through programmable feature configuration. Our framework isolates confounding
factors and establishes an interpretable evaluation system with three core
analytical dimensions: (1) temporal feature decomposition and capability
mapping, which enables systematic evaluation of model capacities to learn
specific pattern types; (2) robustness analysis under data irregularities,
which quantifies noise tolerance thresholds and anomaly recovery capabilities;
and (3) theoretical optimum benchmarking, which establishes performance
boundaries for each pattern type-enabling direct comparison between model
predictions and mathematical optima. Our experiments show that current deep
learning models do not universally approach optimal baselines across all types
of temporal features.The code is available at
https://github.com/TanQitai/SynTSBench

</details>


### [75] [KCM: KAN-Based Collaboration Models Enhance Pretrained Large Models](https://arxiv.org/abs/2510.20278)
*Guangyu Dai,Siliang Tang,Yueting Zhuang*

Main category: cs.LG

TL;DR: 提出基于KAN的协作模型(KCM)来改进大-小模型协作框架，相比传统MLP方法，KCM在减少大模型推理次数的同时保持任务精度，并显著缓解灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有大-小模型协作框架中的精度下降、灾难性遗忘加剧和小模型知识引发的幻觉问题。

Method: 使用KAN（Kolmogorov-Arnold Networks）作为替代MLP的神经网络架构，构建KCM协作模型，部署在语言、视觉和视觉-语言跨模态任务中。

Result: 相比纯大模型方法，KCM显著减少大模型推理调用次数，同时保持相近的任务精度；相比MLP协作模型，KCM在所有指标上表现更优，特别是显著缓解灾难性遗忘并提升长尾数据精度。

Conclusion: KCM作为大-小模型协作的改进方法，在降低计算资源消耗的同时保持性能，并有效解决协作框架中的关键问题。

Abstract: In recent years, Pretrained Large Models(PLMs) researchers proposed
large-small model collaboration frameworks, leveraged easily trainable small
models to assist large models, aim to(1) significantly reduce computational
resource consumption while maintaining comparable accuracy, and (2) enhance
large model performance in specialized domain tasks. However, this
collaborative paradigm suffers from issues such as significant accuracy
degradation, exacerbated catastrophic forgetting, and amplified hallucination
problems induced by small model knowledge. To address these challenges, we
propose a KAN-based Collaborative Model (KCM) as an improved approach to
large-small model collaboration. The KAN utilized in KCM represents an
alternative neural network architecture distinct from conventional MLPs.
Compared to MLPs, KAN offers superior visualizability and interpretability
while mitigating catastrophic forgetting. We deployed KCM in large-small model
collaborative systems across three scenarios: language, vision, and
vision-language cross-modal tasks. The experimental results demonstrate that,
compared with pure large model approaches, the large-small model collaboration
framework utilizing KCM as the collaborative model significantly reduces the
number of large model inference calls while maintaining near-identical task
accuracy, thereby substantially lowering computational resource consumption.
Concurrently, the KAN-based small collaborative model markedly mitigates
catastrophic forgetting, leading to significant accuracy improvements for
long-tail data. The results reveal that KCM demonstrates superior performance
across all metrics compared to MLP-based small collaborative models (MCM).

</details>


### [76] [ResearchGPT: Benchmarking and Training LLMs for End-to-End Computer Science Research Workflows](https://arxiv.org/abs/2510.20279)
*Penghao Wang,Yuhao Zhou,Mengxuan Wu,Ziheng Qin,Bangyuan Zhu,Shengbin Huang,Xuanlei Zhao,Panpan Zhang,Xiaojiang Peng,Yuzhang Shang,Jianfei Yang,Zheng Zhu,Tianlong Chen,Zhangyang Wang,Kai Wang*

Main category: cs.LG

TL;DR: 提出了ResearchGPT愿景和CS-54k语料库，包含CS-4k评估基准和CS-50k训练集，证明领域对齐训练比预训练规模更重要


<details>
  <summary>Details</summary>
Motivation: 构建能够协助整个科学研究流程的AI协作系统，需要评估端到端工作流程而非孤立子任务的基准

Method: 通过检索增强生成和多阶段质量控制构建CS-54k科学问答语料库，从中提取CS-4k评估基准和CS-50k训练数据集

Result: CS-4k能有效区分不同LLM能力层级，在CS-50k上训练的7B模型超越GPT-4.1、GPT-4o等更大专有系统

Conclusion: 使AI成为更好研究助手的关键在于领域对齐的高质量数据训练，而非预训练规模或通用基准性能

Abstract: As large language models (LLMs) advance, the ultimate vision for their role
in science is emerging: we could build an AI collaborator to effectively assist
human beings throughout the entire scientific research process. We refer to
this envisioned system as ResearchGPT. Given that scientific research
progresses through multiple interdependent phases, achieving this vision
requires rigorous benchmarks that evaluate the end-to-end workflow rather than
isolated sub-tasks. To this end, we contribute CS-54k, a high-quality corpus of
scientific Q&A pairs in computer science, built from 14k CC-licensed papers. It
is constructed through a scalable, paper-grounded pipeline that combines
retrieval-augmented generation (RAG) with multi-stage quality control to ensure
factual grounding. From this unified corpus, we derive two complementary
subsets: CS-4k, a carefully curated benchmark for evaluating AI's ability to
assist scientific research, and CS-50k, a large-scale training dataset.
Extensive experiments demonstrate that CS-4k stratifies state-of-the-art LLMs
into distinct capability tiers. Open models trained on CS-50k with supervised
training and reinforcement learning demonstrate substantial improvements. Even
7B-scale models, when properly trained, outperform many larger proprietary
systems, such as GPT-4.1, GPT-4o, and Gemini 2.5 Pro. This indicates that
making AI models better research assistants relies more on domain-aligned
training with high-quality data than on pretraining scale or general benchmark
performance. We release CS-4k and CS-50k in the hope of fostering AI systems as
reliable collaborators in CS research.

</details>


### [77] [Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization](https://arxiv.org/abs/2510.20295)
*Yang Qiu,Yixiong Zou,Jun Wang,Wei Liu,Xiangyu Fu,Ruixuan Li*

Main category: cs.LG

TL;DR: 提出了一种无需环境标注的因果子图发现方法，通过分布不变准则和表示范数来识别因果子图，在图形泛化任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决图神经网络在分布偏移下的泛化问题，避免现有方法需要昂贵环境标注或启发式生成环境分割的限制。

Method: 基于因果子图在不同环境中分布变化较小的观察，提出分布不变准则，通过表示范数与分布偏移的定量关系来识别因果子图，并设计范数引导的不变分布目标函数。

Result: 在两个广泛使用的基准测试上，该方法在图形泛化任务中持续优于最先进的方法。

Conclusion: 提出的IRM-free方法能够有效捕获因果子图，在无需环境标注的情况下实现更好的图神经网络泛化性能。

Abstract: Out-of-distribution generalization under distributional shifts remains a
critical challenge for graph neural networks. Existing methods generally adopt
the Invariant Risk Minimization (IRM) framework, requiring costly environment
annotations or heuristically generated synthetic splits. To circumvent these
limitations, in this work, we aim to develop an IRM-free method for capturing
causal subgraphs. We first identify that causal subgraphs exhibit substantially
smaller distributional variations than non-causal components across diverse
environments, which we formalize as the Invariant Distribution Criterion and
theoretically prove in this paper. Building on this criterion, we
systematically uncover the quantitative relationship between distributional
shift and representation norm for identifying the causal subgraph, and
investigate its underlying mechanisms in depth. Finally, we propose an IRM-free
method by introducing a norm-guided invariant distribution objective for causal
subgraph discovery and prediction. Extensive experiments on two widely used
benchmarks demonstrate that our method consistently outperforms
state-of-the-art methods in graph generalization.

</details>


### [78] [DB-FGA-Net: Dual Backbone Frequency Gated Attention Network for Multi-Class Classification with Grad-CAM Interpretability](https://arxiv.org/abs/2510.20299)
*Saraf Anzum Shreya,MD. Abu Ismail Siddique,Sharaf Tasnim*

Main category: cs.LG

TL;DR: 提出DB-FGA-Net双骨干网络，集成VGG16和Xception架构，结合频率门控注意力模块，无需数据增强即可在脑肿瘤分类中实现最先进性能，并提供Grad-CAM可视化增强临床可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决脑肿瘤诊断中深度学习模型依赖数据增强导致泛化能力受限和临床可信度不足的问题，开发无需增强、可解释且可部署的可靠诊断模型。

Method: 采用VGG16和Xception双骨干网络架构，集成频率门控注意力模块捕获局部和全局互补特征，结合Grad-CAM提供肿瘤区域可视化，并开发图形用户界面支持实时分类。

Result: 在7K-DS数据集上四分类准确率达99.24%，三分类和双分类分别为98.68%和99.85%；在独立3K-DS数据集上泛化准确率达95.77%，超越基准和最先进方法。

Conclusion: DB-FGA-Net作为无需增强、可解释且可部署的深度学习模型，在脑肿瘤诊断中具有可靠的临床转化潜力，为临床应用提供了透明化和实用的解决方案。

Abstract: Brain tumors are a challenging problem in neuro-oncology, where early and
precise diagnosis is important for successful treatment. Deep learning-based
brain tumor classification methods often rely on heavy data augmentation which
can limit generalization and trust in clinical applications. In this paper, we
propose a double-backbone network integrating VGG16 and Xception with a
Frequency-Gated Attention (FGA) Block to capture complementary local and global
features. Unlike previous studies, our model achieves state-of-the-art
performance without augmentation which demonstrates robustness to variably
sized and distributed datasets. For further transparency, Grad-CAM is
integrated to visualize the tumor regions based on which the model is giving
prediction, bridging the gap between model prediction and clinical
interpretability. The proposed framework achieves 99.24\% accuracy on the 7K-DS
dataset for the 4-class setting, along with 98.68\% and 99.85\% in the 3-class
and 2-class settings, respectively. On the independent 3K-DS dataset, the model
generalizes with 95.77\% accuracy, outperforming baseline and state-of-the-art
methods. To further support clinical usability, we developed a graphical user
interface (GUI) that provides real-time classification and Grad-CAM-based tumor
localization. These findings suggest that augmentation-free, interpretable, and
deployable deep learning models such as DB-FGA-Net hold strong potential for
reliable clinical translation in brain tumor diagnosis.

</details>


### [79] [InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling](https://arxiv.org/abs/2510.20302)
*Yuhang Wang*

Main category: cs.LG

TL;DR: 提出InvDec（倒置解码器）混合架构，结合基于patch的时间编码器和在变量维度上操作的倒置解码器，通过延迟变量嵌入和自适应残差融合机制，在保持时间特征完整性的同时有效建模跨变量依赖关系


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：通道独立方法如PatchTST擅长时间建模但忽略变量相关性，而纯变量注意力方法如iTransformer牺牲了时间编码。需要一种能够同时建模时间模式和跨变量依赖的架构

Method: InvDec结合基于patch的时间编码器和在变量维度上通过变量级自注意力操作的倒置解码器。引入延迟变量嵌入在时间编码后丰富变量特定表示，以及自适应残差融合机制动态平衡时间和变量信息

Result: 在七个基准测试上的广泛实验显示，在高维数据集上取得显著提升：Electricity（321变量）MSE降低20.9%，Weather提升4.3%，Traffic提升2.7%，同时在低维ETT数据集上保持竞争力

Conclusion: InvDec通过原则性分离时间编码和变量级解码，有效解决了多变量时间序列预测中的关键挑战。消融研究验证了各组件有效性，分析表明InvDec的优势随数据集维度增加而增长，确认跨变量建模在变量数量增加时变得至关重要

Abstract: Multivariate time series forecasting requires simultaneously modeling
temporal patterns and cross-variate dependencies. Channel-independent methods
such as PatchTST excel at temporal modeling but ignore variable correlations,
while pure variate-attention approaches such as iTransformer sacrifice temporal
encoding. We proposeInvDec (Inverted Decoder), a hybrid architecture that
achieves principled separation between temporal encoding and variate-level
decoding. InvDec combines a patch-based temporal encoder with an inverted
decoder operating on the variate dimension through variate-wise self-attention.
We introduce delayed variate embeddings that enrich variable-specific
representations only after temporal encoding, preserving temporal feature
integrity. An adaptive residual fusion mechanism dynamically balances temporal
and variate information across datasets of varying dimensions. Instantiating
InvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on seven
benchmarks demonstrate significant gains on high-dimensional datasets: 20.9%
MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and
2.7% gain on Traffic compared to PatchTST, while maintaining competitive
performance on low-dimensional ETT datasets. Ablation studies validate each
component, and analysis reveals that InvDec's advantage grows with dataset
dimensionality, confirming that cross-variate modeling becomes critical as the
number of variables increases.

</details>


### [80] [LEGO: A Lightweight and Efficient Multiple-Attribute Unlearning Framework for Recommender Systems](https://arxiv.org/abs/2510.20327)
*Fengyuan Yu,Yuyuan Li,Xiaohua Feng,Junjie Fang,Tao Wang,Chaochao Chen*

Main category: cs.LG

TL;DR: 提出了LEGO框架，用于推荐系统中的多属性遗忘，解决现有单属性遗忘方法无法同时处理多个遗忘请求和适应动态遗忘需求的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的隐私保护需求通常涉及多个敏感属性且具有动态性，现有单属性遗忘方法无法满足这些需求。

Method: 将多属性遗忘过程分为两个步骤：嵌入校准（从用户嵌入中移除特定属性信息）和灵活组合（将这些嵌入组合成单一嵌入以保护所有敏感属性），并将遗忘过程建模为互信息最小化问题。

Result: 在三个真实世界数据集和三种代表性推荐模型上的广泛实验证明了该框架的有效性和效率。

Conclusion: LEGO框架能够有效解决多属性遗忘问题，具有理论保证和实际应用价值。

Abstract: With the growing demand for safeguarding sensitive user information in
recommender systems, recommendation attribute unlearning is receiving
increasing attention. Existing studies predominantly focus on single-attribute
unlearning. However, privacy protection requirements in the real world often
involve multiple sensitive attributes and are dynamic. Existing
single-attribute unlearning methods cannot meet these real-world requirements
due to i) CH1: the inability to handle multiple unlearning requests
simultaneously, and ii) CH2: the lack of efficient adaptability to dynamic
unlearning needs. To address these challenges, we propose LEGO, a lightweight
and efficient multiple-attribute unlearning framework. Specifically, we divide
the multiple-attribute unlearning process into two steps: i) Embedding
Calibration removes information related to a specific attribute from user
embedding, and ii) Flexible Combination combines these embeddings into a single
embedding, protecting all sensitive attributes. We frame the unlearning process
as a mutual information minimization problem, providing LEGO a theoretical
guarantee of simultaneous unlearning, thereby addressing CH1. With the two-step
framework, where Embedding Calibration can be performed in parallel and
Flexible Combination is flexible and efficient, we address CH2. Extensive
experiments on three real-world datasets across three representative
recommendation models demonstrate the effectiveness and efficiency of our
proposed framework. Our code and appendix are available at
https://github.com/anonymifish/lego-rec-multiple-attribute-unlearning.

</details>


### [81] [Synthetic Data for Robust Runway Detection](https://arxiv.org/abs/2510.20349)
*Estelle Chigot,Dennis G. Wilson,Meriem Ghrib,Fabrice Jimenez,Thomas Oberlin*

Main category: cs.LG

TL;DR: 使用商业飞行模拟器生成合成图像来补充少量真实标注图像，通过控制图像生成和真实/合成数据集成，实现准确的跑道检测，并评估模型在夜间等恶劣条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度视觉模型在工业应用中的训练数据收集和标注成本过高，特别是在需要覆盖所有可能条件的关键应用中。合成图像生成可以廉价可靠地覆盖所有条件和环境，前提是能够缓解合成到真实的分布偏移问题。

Method: 基于商业飞行模拟器生成合成图像，结合少量真实标注图像，采用定制化的域适应策略来集成真实和合成数据。

Result: 标准目标检测模型能够实现准确的预测，并且在真实数据中未出现的夜间图像等恶劣条件下也表现出良好的鲁棒性。

Conclusion: 通过控制图像生成和采用定制化域适应策略，合成图像生成方法可以有效补充真实数据，在关键应用如自主着陆系统中实现准确可靠的跑道检测。

Abstract: Deep vision models are now mature enough to be integrated in industrial and
possibly critical applications such as autonomous navigation. Yet, data
collection and labeling to train such models requires too much efforts and
costs for a single company or product. This drawback is more significant in
critical applications, where training data must include all possible conditions
including rare scenarios. In this perspective, generating synthetic images is
an appealing solution, since it allows a cheap yet reliable covering of all the
conditions and environments, if the impact of the synthetic-to-real
distribution shift is mitigated. In this article, we consider the case of
runway detection that is a critical part in autonomous landing systems
developed by aircraft manufacturers. We propose an image generation approach
based on a commercial flight simulator that complements a few annotated real
images. By controlling the image generation and the integration of real and
synthetic data, we show that standard object detection models can achieve
accurate prediction. We also evaluate their robustness with respect to adverse
conditions, in our case nighttime images, that were not represented in the real
data, and show the interest of using a customized domain adaptation strategy.

</details>


### [82] [Ask a Strong LLM Judge when Your Reward Model is Uncertain](https://arxiv.org/abs/2510.20369)
*Zhenghao Xu,Qin Lu,Qingru Zhang,Liang Qiu,Ilgee Hong,Changlong Yu,Wenlin Yao,Yao Liu,Haoming Jiang,Lihong Li,Hyokun Yun,Tuo Zhao*

Main category: cs.LG

TL;DR: 提出基于不确定性的路由框架，将快速奖励模型与强大但昂贵的LLM裁判结合，通过不确定性量化指导路由决策，在相同成本下显著优于随机调用裁判的方法。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型容易受到奖励攻击且在分布外输入上泛化能力差，而强大的LLM裁判虽然泛化能力强但推理成本高，限制了在在线RLHF中的应用。

Method: 将策略梯度中的优势估计建模为成对偏好分类，实现原则性的不确定性量化来指导路由。不确定的样本对转发给LLM裁判，确定的样本由奖励模型评估。

Result: 在奖励模型基准测试中，基于不确定性的路由策略在相同成本下显著优于随机调用裁判的方法，下游对齐结果展示了其在改进在线RLHF中的有效性。

Conclusion: 提出的不确定性路由框架能够高效结合快速奖励模型和强大LLM裁判的优势，为在线RLHF提供了一种成本效益高的解决方案。

Abstract: Reward model (RM) plays a pivotal role in reinforcement learning with human
feedback (RLHF) for aligning large language models (LLMs). However, classical
RMs trained on human preferences are vulnerable to reward hacking and
generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM
judges equipped with reasoning capabilities demonstrate superior
generalization, even without additional training, but incur significantly
higher inference costs, limiting their applicability in online RLHF. In this
work, we propose an uncertainty-based routing framework that efficiently
complements a fast RM with a strong but costly LLM judge. Our approach
formulates advantage estimation in policy gradient (PG) methods as pairwise
preference classification, enabling principled uncertainty quantification to
guide routing. Uncertain pairs are forwarded to the LLM judge, while confident
ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our
uncertainty-based routing strategy significantly outperforms random judge
calling at the same cost, and downstream alignment results showcase its
effectiveness in improving online RLHF.

</details>


### [83] [Hierarchical Time Series Forecasting with Robust Reconciliation](https://arxiv.org/abs/2510.20383)
*Shuhei Aikawa,Aru Suzuki,Kei Yoshitake,Kanata Teshigawara,Akira Iwabuchi,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.LG

TL;DR: 提出了一种考虑协方差矩阵估计不确定性的鲁棒优化框架，用于层次时间序列预测的协调过程，相比现有方法获得了更好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有层次预测方法需要估计协方差矩阵，但真实协方差矩阵未知，估计误差会降低预测性能。

Method: 引入协方差矩阵的不确定性集合，构建最小化最坏情况期望平方误差的鲁棒协调问题，可转化为半定优化问题求解。

Result: 数值实验表明，所提出的鲁棒协调方法比现有层次预测方法获得了更好的预测性能。

Conclusion: 在协调过程中考虑不确定性是有效的，鲁棒协调方法能够提升层次时间序列预测的性能。

Abstract: This paper focuses on forecasting hierarchical time-series data, where each
higher-level observation equals the sum of its corresponding lower-level time
series. In such contexts, the forecast values should be coherent, meaning that
the forecast value of each parent series exactly matches the sum of the
forecast values of its child series. Existing hierarchical forecasting methods
typically generate base forecasts independently for each series and then apply
a reconciliation procedure to adjust them so that the resulting forecast values
are coherent across the hierarchy. These methods generally derive an optimal
reconciliation, using a covariance matrix of the forecast error. In practice,
however, the true covariance matrix is unknown and has to be estimated from
finite samples in advance. This gap between the true and estimated covariance
matrix may degrade forecast performance. To address this issue, we propose a
robust optimization framework for hierarchical reconciliation that accounts for
uncertainty in the estimated covariance matrix. We first introduce an
uncertainty set for the estimated covariance matrix and formulate a
reconciliation problem that minimizes the worst-case expected squared error
over this uncertainty set. We show that our problem can be cast as a
semidefinite optimization problem. Numerical experiments demonstrate that the
proposed robust reconciliation method achieved better forecast performance than
existing hierarchical forecasting methods, which indicates the effectiveness of
integrating uncertainty into the reconciliation process.

</details>


### [84] [Relative-Based Scaling Law for Neural Language Models](https://arxiv.org/abs/2510.20387)
*Baoqing Yue,Jinyuan Zhou,Zixi Wei,Jingtao Zhan,Qingyao Ai,Yiqun Liu*

Main category: cs.LG

TL;DR: 提出了基于相对排序的缩放定律，通过RBP指标衡量模型性能，补充了传统交叉熵指标的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有缩放定律研究主要依赖交叉熵作为评估指标，但交叉熵只关注正确token的绝对概率，忽略了正确与错误token的相对排序，而相对排序在语言模型中至关重要。

Method: 提出相对概率(RBP)指标，量化正确token在top预测中的排名概率；基于RBP建立相对缩放定律，描述模型规模增大时RBP的提升规律；在4个数据集和4个模型家族上进行广泛实验验证。

Result: 通过跨越五个数量级的实验验证了相对缩放定律的鲁棒性和准确性；展示了该定律的两个应用：深入解释涌现现象和促进缩放定律基础理论的发现。

Conclusion: 相对缩放定律补充了交叉熵视角，为大规模语言模型的缩放提供了更完整的理解，对实际开发和理论探索都具有重要价值。

Abstract: Scaling laws aim to accurately predict model performance across different
scales. Existing scaling-law studies almost exclusively rely on cross-entropy
as the evaluation metric. However, cross-entropy provides only a partial view
of performance: it measures the absolute probability assigned to the correct
token, but ignores the relative ordering between correct and incorrect tokens.
Yet, relative ordering is crucial for language models, such as in
greedy-sampling scenario. To address this limitation, we investigate scaling
from the perspective of relative ordering. We first propose the Relative-Based
Probability (RBP) metric, which quantifies the probability that the correct
token is ranked among the top predictions. Building on this metric, we
establish the Relative-Based Scaling Law, which characterizes how RBP improves
with increasing model size. Through extensive experiments on four datasets and
four model families spanning five orders of magnitude, we demonstrate the
robustness and accuracy of this law. Finally, we illustrate the broad
application of this law with two examples, namely providing a deeper
explanation of emergence phenomena and facilitating finding fundamental
theories of scaling laws. In summary, the Relative-Based Scaling Law
complements the cross-entropy perspective and contributes to a more complete
understanding of scaling large language models. Thus, it offers valuable
insights for both practical development and theoretical exploration.

</details>


### [85] [Why DPO is a Misspecified Estimator and How to Fix It](https://arxiv.org/abs/2510.20413)
*Aditya Gopalan,Sayak Ray Chowdhury,Debangshu Banerjee*

Main category: cs.LG

TL;DR: DPO算法在奖励函数无法通过策略类实现时会出现错误设定，导致偏好顺序反转、策略奖励恶化等问题。作者提出了AuxDPO，通过在DPO损失函数中引入辅助变量来缓解这种错误设定。


<details>
  <summary>Details</summary>
Motivation: 研究DPO算法在奖励函数无法通过策略类实现时的错误设定问题，以及如何改进DPO以接近RLHF解决方案。

Method: 分析了DPO的统计估计问题，研究了两阶段RLHF的局部行为，并提出了AuxDPO方法，在DPO损失函数中引入辅助变量。

Result: 在理论分析和实证实验中，AuxDPO在didactic bandit设置和LLM对齐任务中表现出优于DPO的性能。

Conclusion: AuxDPO能够以原则性方式缓解DPO的错误设定问题，并朝着RLHF解决方案移动。

Abstract: Direct alignment algorithms such as Direct Preference Optimization (DPO)
fine-tune models based on preference data, using only supervised learning
instead of two-stage reinforcement learning with human feedback (RLHF). We show
that DPO encodes a statistical estimation problem over reward functions induced
by a parametric policy class. When the true reward function that generates
preferences cannot be realized via the policy class, DPO becomes misspecified,
resulting in failure modes such as preference order reversal, worsening of
policy reward, and high sensitivity to the input preference data distribution.
On the other hand, we study the local behavior of two-stage RLHF for a
parametric class and relate it to a natural gradient step in policy space. Our
fine-grained geometric characterization allows us to propose AuxDPO, which
introduces additional auxiliary variables in the DPO loss function to help move
towards the RLHF solution in a principled manner and mitigate the
misspecification in DPO. We empirically demonstrate the superior performance of
AuxDPO on didactic bandit settings as well as LLM alignment tasks.

</details>


### [86] [Addressing Mark Imbalance in Integration-free Neural Marked Temporal Point Processes](https://arxiv.org/abs/2510.20414)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yongli Ren,Yan Wang*

Main category: cs.LG

TL;DR: 提出了一种针对标记时间点过程的阈值方法，通过调整标记概率来优化标记预测，特别解决了事件标记分布不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中事件标记分布高度不平衡，现有研究忽视了这一点，导致对稀有标记事件的预测性能不佳。

Method: 使用阈值方法学习阈值来调整标记概率，先预测标记再预测时间，开发了支持有效时间采样和标记概率估计的神经MTPP模型。

Result: 在真实世界数据集上的广泛实验表明，该方法在下一个事件标记和时间预测方面优于各种基线方法。

Conclusion: 提出的解决方案有效解决了事件标记分布不平衡问题，提升了稀有标记事件的预测性能。

Abstract: Marked Temporal Point Process (MTPP) has been well studied to model the event
distribution in marked event streams, which can be used to predict the mark and
arrival time of the next event. However, existing studies overlook that the
distribution of event marks is highly imbalanced in many real-world
applications, with some marks being frequent but others rare. The imbalance
poses a significant challenge to the performance of the next event prediction,
especially for events of rare marks. To address this issue, we propose a
thresholding method, which learns thresholds to tune the mark probability
normalized by the mark's prior probability to optimize mark prediction, rather
than predicting the mark directly based on the mark probability as in existing
studies. In conjunction with this method, we predict the mark first and then
the time. In particular, we develop a novel neural MTPP model to support
effective time sampling and estimation of mark probability without
computationally expensive numerical improper integration. Extensive experiments
on real-world datasets demonstrate the superior performance of our solution
against various baselines for the next event mark and time prediction. The code
is available at https://github.com/undes1red/IFNMTPP.

</details>


### [87] [An Empirical Study of Sample Selection Strategies for Large Language Model Repair](https://arxiv.org/abs/2510.20428)
*Xuran Li,Jingyi Wang*

Main category: cs.LG

TL;DR: 系统分析LLM修复中的样本优先策略，提出SAPS方法在解毒效果、效用保持和效率方面达到最佳平衡，显著减少所需数据量。


<details>
  <summary>Details</summary>
Motivation: LLM在现实系统中部署时会产生有害或有偏见的输出，而参数更新的高成本促使需要选择性地使用修复数据。

Method: 评估五种代表性选择方法：随机采样、K-Center、梯度范数选择(GraNd)、分层覆盖(CCS)和提出的语义感知优先采样(SAPS)，通过毒性减少、困惑度和三个复合指标评估修复效果。

Result: SAPS在解毒、效用保持和效率方面达到最佳平衡，随机采样对大型或鲁棒模型仍然有效，而高开销方法如CCS和GraNd收益有限。

Conclusion: 样本选择应被视为修复流程中的可调组件，基于选择的修复是维护LLM可靠性的高效可扩展范式。

Abstract: Large language models (LLMs) are increasingly deployed in real-world systems,
yet they can produce toxic or biased outputs that undermine safety and trust.
Post-hoc model repair provides a practical remedy, but the high cost of
parameter updates motivates selective use of repair data. Despite extensive
prior work on data selection for model training, it remains unclear which
sampling criteria are most effective and efficient when applied specifically to
behavioral repair of large generative models. Our study presents a systematic
analysis of sample prioritization strategies for LLM repair. We evaluate five
representative selection methods, including random sampling, K-Center,
gradient-norm-based selection(GraNd), stratified coverage (CCS), and a
Semantic-Aware Prioritized Sampling (SAPS) approach we proposed. Repair
effectiveness and trade-offs are assessed through toxicity reduction,
perplexity on WikiText-2 and LAMBADA, and three composite metrics: the Repair
Proximity Score (RPS), the Overall Performance Score (OPS), and the Repair
Efficiency Score (RES). Experimental results show that SAPS achieves the best
balance between detoxification, utility preservation, and efficiency,
delivering comparable or superior repair outcomes with substantially less data.
Random sampling remains effective for large or robust models, while
high-overhead methods such as CCS and GraNd provide limited benefit. The
optimal data proportion depends on model scale and repair method, indicating
that sample selection should be regarded as a tunable component of repair
pipelines. Overall, these findings establish selection-based repair as an
efficient and scalable paradigm for maintaining LLM reliability.

</details>


### [88] [Explainable Benchmarking through the Lense of Concept Learning](https://arxiv.org/abs/2510.20439)
*Quannian Zhang,Michael Röder,Nikit Srivastava,N'Dah Jean Kouagou,Axel-Cyrille Ngonga Ngomo*

Main category: cs.LG

TL;DR: 提出可解释基准测试新范式，使用PruneCEL概念学习方法为知识图谱问答系统性能自动生成解释，在用户研究中80%情况下参与者能准确预测系统行为。


<details>
  <summary>Details</summary>
Motivation: 传统基准测试仅用少量指标总结系统性能，分析评估细节和推导洞察是繁琐的手动任务且结果有偏，需要自动生成性能解释的新基准测试方法。

Method: 使用为大型知识图谱开发的新型概念学习方法PruneCEL计算解释，应用于知识图谱问答系统的可解释基准测试。

Result: PruneCEL在可解释基准测试任务上比最先进概念学习方法F1得分高出0.55分；41人用户研究显示80%情况下多数参与者能基于解释准确预测系统行为。

Conclusion: 可解释基准测试是可行的新范式，PruneCEL方法在生成系统性能解释方面表现优异，能有效帮助用户理解系统行为。

Abstract: Evaluating competing systems in a comparable way, i.e., benchmarking them, is
an undeniable pillar of the scientific method. However, system performance is
often summarized via a small number of metrics. The analysis of the evaluation
details and the derivation of insights for further development or use remains a
tedious manual task with often biased results. Thus, this paper argues for a
new type of benchmarking, which is dubbed explainable benchmarking. The aim of
explainable benchmarking approaches is to automatically generate explanations
for the performance of systems in a benchmark. We provide a first instantiation
of this paradigm for knowledge-graph-based question answering systems. We
compute explanations by using a novel concept learning approach developed for
large knowledge graphs called PruneCEL. Our evaluation shows that PruneCEL
outperforms state-of-the-art concept learners on the task of explainable
benchmarking by up to 0.55 points F1 measure. A task-driven user study with 41
participants shows that in 80\% of the cases, the majority of participants can
accurately predict the behavior of a system based on our explanations. Our code
and data are available at https://github.com/dice-group/PruneCEL/tree/K-cap2025

</details>


### [89] [MolBridge: Atom-Level Joint Graph Refinement for Robust Drug-Drug Interaction Event Prediction](https://arxiv.org/abs/2510.20448)
*Xuan Lin,Aocheng Ding,Tengfei Ma,Hua Liang,Zhe Quan*

Main category: cs.LG

TL;DR: MolBridge是一个基于原子级联合图优化的DDI事件预测框架，通过构建药物对的联合图直接建模跨分子相互作用，解决了现有方法无法显式建模原子级交互的问题。


<details>
  <summary>Details</summary>
Motivation: 现有DDI预测方法依赖孤立的药物表示，无法显式建模原子级跨分子相互作用，限制了在不同分子复杂度和DDI类型分布下的有效性。

Method: 构建药物对的联合图整合原子结构，引入结构一致性模块迭代优化节点特征同时保持全局结构上下文，有效学习局部和全局交互模式。

Result: 在两个基准数据集上的广泛实验表明，MolBridge在长尾和归纳场景中均优于现有最优基线，实现了卓越性能。

Conclusion: 细粒度图优化显著提高了DDI事件预测的准确性、鲁棒性和机制可解释性，为挖掘和分析药物-药物相互作用网络提供了有效的图基方法。

Abstract: Drug combinations offer therapeutic benefits but also carry the risk of
adverse drug-drug interactions (DDIs), especially under complex molecular
structures. Accurate DDI event prediction requires capturing fine-grained
inter-drug relationships, which are critical for modeling metabolic mechanisms
such as enzyme-mediated competition. However, existing approaches typically
rely on isolated drug representations and fail to explicitly model atom-level
cross-molecular interactions, limiting their effectiveness across diverse
molecular complexities and DDI type distributions. To address these
limitations, we propose MolBridge, a novel atom-level joint graph refinement
framework for robust DDI event prediction. MolBridge constructs a joint graph
that integrates atomic structures of drug pairs, enabling direct modeling of
inter-drug associations. A central challenge in such joint graph settings is
the potential loss of information caused by over-smoothing when modeling
long-range atomic dependencies. To overcome this, we introduce a structure
consistency module that iteratively refines node features while preserving the
global structural context. This joint design allows MolBridge to effectively
learn both local and global interaction outperforms state-of-the-art baselines,
achieving superior performance across long-tail and inductive scenarios.
patterns, yielding robust representations across both frequent and rare DDI
types. Extensive experiments on two benchmark datasets show that MolBridge
consistently. These results demonstrate the advantages of fine-grained graph
refinement in improving the accuracy, robustness, and mechanistic
interpretability of DDI event prediction.This work contributes to Web Mining
and Content Analysis by developing graph-based methods for mining and analyzing
drug-drug interaction networks.

</details>


### [90] [Intransitive Player Dominance and Market Inefficiency in Tennis Forecasting: A Graph Neural Network Approach](https://arxiv.org/abs/2510.20454)
*Lawrence Clegg,John Cartlidge*

Main category: cs.LG

TL;DR: 该论文提出了一种图神经网络方法，通过时间有向图显式建模网球比赛中的不可传递关系，发现在高不可传递复杂度的比赛中博彩公司处理不佳，利用该模型选择性投注可获得3.26%的正回报率。


<details>
  <summary>Details</summary>
Motivation: 网球比赛中普遍存在不可传递的球员支配关系（A胜B，B胜C，但C胜A），但很少有预测方法尝试纳入这种关系。

Method: 使用图神经网络方法，以球员为节点、历史比赛结果为有向边，构建时间有向图来显式建模不可传递关系。

Result: 模型在高不可传递复杂度比赛中达到65.7%的准确率和0.215的Brier分数，使用Kelly投注策略在1903次投注中实现了3.26%的正回报率。

Conclusion: 博彩市场在处理不可传递比赛时存在效率不足，图神经网络方法能够成功捕捉这些关系动态并从中获利。

Abstract: Intransitive player dominance, where player A beats B, B beats C, but C beats
A, is common in competitive tennis. Yet, there are few known attempts to
incorporate it within forecasting methods. We address this problem with a graph
neural network approach that explicitly models these intransitive relationships
through temporal directed graphs, with players as nodes and their historical
match outcomes as directed edges. We find the bookmaker Pinnacle Sports poorly
handles matches with high intransitive complexity and posit that our
graph-based approach is uniquely positioned to capture relational dynamics in
these scenarios. When selectively betting on higher intransitivity matchups
with our model (65.7% accuracy, 0.215 Brier Score), we achieve significant
positive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting a
market inefficiency in handling intransitive matchups that our approach
successfully exploits.

</details>


### [91] [Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models](https://arxiv.org/abs/2510.20468)
*Tomáš Souček,Sylvestre-Alvise Rebuffi,Pierre Fernandez,Nikola Jovanović,Hady Elsahar,Valeriu Lacatusu,Tuan Tran,Alexandre Mourachko*

Main category: cs.LG

TL;DR: 本文提出了一种针对图像水印伪造的新攻击方法，通过训练偏好模型来检测水印，并利用反向传播优化技术实现水印的移除和伪造，无需了解水印模型的具体实现。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成内容的激增，水印技术对确保内容真实性和归属越来越重要。然而，现有研究主要关注水印的鲁棒性，而对水印伪造（即从真实内容中窃取水印并应用于恶意内容）的研究不足。

Method: 1. 引入偏好模型来评估图像是否包含水印，该模型使用排序损失在纯程序生成的图像上进行训练，无需真实水印数据。2. 通过反向传播优化输入图像，展示模型移除和伪造水印的能力，仅需单张水印图像且无需了解水印模型。

Result: 在多种后处理图像水印模型上评估了所提方法，证明该方法能有效伪造水印，对当前水印方法的安全性提出了质疑。

Conclusion: 该方法简单实用，仅需单张水印图像即可实施攻击，揭示了当前水印技术在安全性方面的潜在漏洞。

Abstract: Recent years have seen a surge in interest in digital content watermarking
techniques, driven by the proliferation of generative models and increased
legal pressure. With an ever-growing percentage of AI-generated content
available online, watermarking plays an increasingly important role in ensuring
content authenticity and attribution at scale. There have been many works
assessing the robustness of watermarking to removal attacks, yet, watermark
forging, the scenario when a watermark is stolen from genuine content and
applied to malicious content, remains underexplored. In this work, we
investigate watermark forging in the context of widely used post-hoc image
watermarking. Our contributions are as follows. First, we introduce a
preference model to assess whether an image is watermarked. The model is
trained using a ranking loss on purely procedurally generated images without
any need for real watermarks. Second, we demonstrate the model's capability to
remove and forge watermarks by optimizing the input image through
backpropagation. This technique requires only a single watermarked image and
works without knowledge of the watermarking model, making our attack much
simpler and more practical than attacks introduced in related work. Third, we
evaluate our proposed method on a variety of post-hoc image watermarking
models, demonstrating that our approach can effectively forge watermarks,
questioning the security of current watermarking approaches. Our code and
further resources are publicly available.

</details>


### [92] [Bi-CoG: Bi-Consistency-Guided Self-Training for Vision-Language Models](https://arxiv.org/abs/2510.20477)
*Rui Zhu,Song-Lin Lv,Zi-Kang Wang,Lan-Zhe Guo*

Main category: cs.LG

TL;DR: 提出Bi-CoG方法，通过同时利用模型间和模型内一致性以及错误感知动态伪标签分配策略，解决半监督微调中的模型偏差和超参数敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 现有半监督微调方法依赖预测一致性或预定义置信度阈值，容易产生模型偏差和超参数敏感性问题。

Method: Bi-CoG方法同时利用模型间和模型内一致性，结合错误感知动态伪标签分配策略，为未标记数据分配高质量、低偏差的伪标签。

Result: 在14个数据集上的广泛实验表明，Bi-CoG能持续显著提升现有方法的性能。

Conclusion: Bi-CoG是一种简单有效的即插即用方法，通过双一致性引导和动态伪标签分配，有效解决了半监督微调中的关键问题。

Abstract: Exploiting unlabeled data through semi-supervised learning (SSL) or
leveraging pre-trained models via fine-tuning are two prevailing paradigms for
addressing label-scarce scenarios. Recently, growing attention has been given
to combining fine-tuning of pre-trained vision-language models (VLMs) with SSL,
forming the emerging paradigm of semi-supervised fine-tuning. However, existing
methods often suffer from model bias and hyperparameter sensitivity, due to
reliance on prediction consistency or pre-defined confidence thresholds. To
address these limitations, we propose a simple yet effective plug-and-play
methodology named
$\underline{\textbf{Bi-Co}}$nsistency-$\underline{\textbf{G}}$uided
Self-Training (Bi-CoG), which assigns high-quality and low-bias pseudo-labels,
by simultaneously exploiting inter-model and intra-model consistency, along
with an error-aware dynamic pseudo-label assignment strategy. Both theoretical
analysis and extensive experiments over 14 datasets demonstrate the
effectiveness of Bi-CoG, which consistently and significantly improves the
performance of existing methods.

</details>


### [93] [Hurdle-IMDL: An Imbalanced Learning Framework for Infrared Rainfall Retrieval](https://arxiv.org/abs/2510.20486)
*Fangjian Zhang,Xiaoyong Zhuge,Wenlan Wang,Haixia Xiao,Yuying Zhu,Siyang Cheng*

Main category: cs.LG

TL;DR: 提出Hurdle-IMDL框架解决遥感降雨反演中的标签不平衡问题，通过分解为零膨胀和长尾分布两部分，分别用hurdle模型和逆模型去偏学习处理，显著改善了强降雨的反演性能。


<details>
  <summary>Details</summary>
Motivation: 人工智能在定量遥感中面临标签分布不平衡的挑战，传统训练模型偏向常见样本，导致罕见样本（如强降雨）的检索性能下降。

Method: 采用分而治之策略：使用hurdle模型处理零膨胀（非降雨样本占主导），提出IMDL方法处理长尾分布（轻降雨样本相对于强降雨样本比例失调），将学习目标转换为无偏的理想逆模型。

Result: 通过统计指标和案例研究验证，Hurdle-IMDL在华东地区降雨天气中优于传统方法、成本敏感方法、生成方法和多任务学习方法，有效缓解系统低估，显著改善强到极端降雨的检索。

Conclusion: IMDL为环境变量分布不平衡问题提供了通用解决方案，能够增强对罕见但高影响事件的检索能力。

Abstract: Artificial intelligence has advanced quantitative remote sensing, yet its
effectiveness is constrained by imbalanced label distribution. This imbalance
leads conventionally trained models to favor common samples, which in turn
degrades retrieval performance for rare ones. Rainfall retrieval exemplifies
this issue, with performance particularly compromised for heavy rain. This
study proposes Hurdle-Inversion Model Debiasing Learning (IMDL) framework.
Following a divide-and-conquer strategy, imbalance in the rain distribution is
decomposed into two components: zero inflation, defined by the predominance of
non-rain samples; and long tail, defined by the disproportionate abundance of
light-rain samples relative to heavy-rain samples. A hurdle model is adopted to
handle the zero inflation, while IMDL is proposed to address the long tail by
transforming the learning object into an unbiased ideal inverse model.
Comprehensive evaluation via statistical metrics and case studies investigating
rainy weather in eastern China confirms Hurdle-IMDL's superiority over
conventional, cost-sensitive, generative, and multi-task learning methods. Its
key advancements include effective mitigation of systematic underestimation and
a marked improvement in the retrieval of heavy-to-extreme rain. IMDL offers a
generalizable approach for addressing imbalance in distributions of
environmental variables, enabling enhanced retrieval of rare yet high-impact
events.

</details>


### [94] [SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment](https://arxiv.org/abs/2510.20540)
*Abdulmomen Ghalkha,Zhuojun Tian,Chaouki Ben Issaid,Mehdi Bennis*

Main category: cs.LG

TL;DR: SheafAlign是一个基于层论的去中心化多模态对齐框架，用多个比较空间替代单一空间对齐，通过层结构建模模态间关系，使用去中心化对比学习目标进行训练。


<details>
  <summary>Details</summary>
Motivation: 传统多模态对齐方法假设所有模态间存在相互冗余，这在现实分布式场景中不成立。需要一种能够处理非互冗余模态、保留共享和独特信息的方法。

Method: 提出层论框架SheafAlign，通过层结构建模成对模态关系，使用去中心化对比学习目标进行训练，无需所有模态间的互冗余假设。

Result: 在多模态传感数据集上的实验显示，该方法在零样本泛化、跨模态对齐和模态缺失鲁棒性方面表现优异，通信成本比最先进基线降低50%。

Conclusion: SheafAlign克服了现有方法的局限性，无需模态间互冗余假设，能够有效保留共享和独特信息，在分布式多模态对齐任务中具有显著优势。

Abstract: Conventional multimodal alignment methods assume mutual redundancy across all
modalities, an assumption that fails in real-world distributed scenarios. We
propose SheafAlign, a sheaf-theoretic framework for decentralized multimodal
alignment that replaces single-space alignment with multiple comparison spaces.
This approach models pairwise modality relations through sheaf structures and
leverages decentralized contrastive learning-based objectives for training.
SheafAlign overcomes the limitations of prior methods by not requiring mutual
redundancy among all modalities, preserving both shared and unique information.
Experiments on multimodal sensing datasets show superior zero-shot
generalization, cross-modal alignment, and robustness to missing modalities,
with 50\% lower communication cost than state-of-the-art baselines.

</details>


### [95] [A Unified Framework for Zero-Shot Reinforcement Learning](https://arxiv.org/abs/2510.20542)
*Jacopo Di Ventura,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: 提出了首个零样本强化学习的统一框架，引入了一致的符号和分类法来组织现有方法并进行直接比较。


<details>
  <summary>Details</summary>
Motivation: 零样本强化学习领域缺乏共同的分析视角，需要统一的框架来整合现有工作并为未来研究提供基础。

Method: 将算法分为两类：直接表示（学习从奖励到策略的端到端映射）和组合表示（利用价值函数的子结构分解表示）。

Result: 建立了统一的分类框架，揭示了不同方法之间的共享原则和关键差异，并为后继特征方法推导了扩展边界。

Conclusion: 该框架为零样本强化学习提供了原则性基础，为开发更通用的智能体指明了清晰路径。

Abstract: Zero-shot reinforcement learning (RL) has emerged as a setting for developing
general agents in an unsupervised manner, capable of solving downstream tasks
without additional training or planning at test-time. Unlike conventional RL,
which optimizes policies for a fixed reward, zero-shot RL requires agents to
encode representations rich enough to support immediate adaptation to any
objective, drawing parallels to vision and language foundation models. Despite
growing interest, the field lacks a common analytical lens.
  We present the first unified framework for zero-shot RL. Our formulation
introduces a consistent notation and taxonomy that organizes existing
approaches and allows direct comparison between them. Central to our framework
is the classification of algorithms into two families: direct representations,
which learn end-to-end mappings from rewards to policies, and compositional
representations, which decompose the representation leveraging the substructure
of the value function. Within this framework, we highlight shared principles
and key differences across methods, and we derive an extended bound for
successor-feature methods, offering a new perspective on their performance in
the zero-shot regime. By consolidating existing work under a common lens, our
framework provides a principled foundation for future research in zero-shot RL
and outlines a clear path toward developing more general agents.

</details>


### [96] [Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics](https://arxiv.org/abs/2510.20556)
*Alexandre Benoit,Catherine Aitken,Yu He*

Main category: cs.LG

TL;DR: 该论文系统分析了图重连技术对图结构属性和下游任务性能的影响，发现成功的重连方法倾向于保留局部结构同时允许全局连接的灵活性。


<details>
  <summary>Details</summary>
Motivation: 图重连技术虽然能缓解图神经网络中的过度压缩问题，但会改变图拓扑结构，可能扭曲重要的拓扑依赖信号。目前尚不清楚需要保留哪些结构属性才能同时保证性能提升和结构保真度。

Method: 研究了七种不同的重连策略，分析重连对局部和全局图结构指标的影响，并将这些变化与节点分类准确性相关联。

Result: 研究结果显示了一个一致的模式：成功的重连方法倾向于保留局部结构，同时在全局连接性上允许灵活性。

Conclusion: 这些发现为设计有效的重连策略提供了新见解，弥合了图理论与实际GNN优化之间的差距。

Abstract: Graph rewiring has emerged as a key technique to alleviate over-squashing in
Graph Neural Networks (GNNs) and Graph Transformers by modifying the graph
topology to improve information flow. While effective, rewiring inherently
alters the graph's structure, raising the risk of distorting important
topology-dependent signals. Yet, despite the growing use of rewiring, little is
known about which structural properties must be preserved to ensure both
performance gains and structural fidelity. In this work, we provide the first
systematic analysis of how rewiring affects a range of graph structural
metrics, and how these changes relate to downstream task performance. We study
seven diverse rewiring strategies and correlate changes in local and global
graph properties with node classification accuracy. Our results reveal a
consistent pattern: successful rewiring methods tend to preserve local
structure while allowing for flexibility in global connectivity. These findings
offer new insights into the design of effective rewiring strategies, bridging
the gap between graph theory and practical GNN optimization.

</details>


### [97] [Embedding the MLOps Lifecycle into OT Reference Models](https://arxiv.org/abs/2510.20590)
*Simon Schindler,Christoph Binder,Lukas Lürzer,Stefan Huber*

Main category: cs.LG

TL;DR: 本文分析了将MLOps实践集成到运营技术(OT)系统中的挑战，提出了使用现有参考模型（RAMI 4.0和ISA-95）进行系统化整合的方法，并通过实际用例验证了该方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着MLOps在工业环境中的广泛应用，将其与运营技术(OT)系统集成的需求日益增长，但直接移植标准MLOps实践到OT环境存在显著障碍。

Method: 评估RAMI 4.0和ISA-95参考模型对MLOps集成的适用性，提出将MLOps生命周期组件系统化映射到RAMI 4.0框架中，并通过真实用例进行验证。

Result: 研究发现标准MLOps实践不能直接移植到OT环境，但通过使用现有参考模型进行结构化适配，可以提供成功的集成路径。

Conclusion: 使用RAMI 4.0和ISA-95等现有参考模型进行结构化适配，是实现MLOps与OT系统成功集成的可行方法。

Abstract: Machine Learning Operations (MLOps) practices are increas- ingly adopted in
industrial settings, yet their integration with Opera- tional Technology (OT)
systems presents significant challenges. This pa- per analyzes the fundamental
obstacles in combining MLOps with OT en- vironments and proposes a systematic
approach to embed MLOps prac- tices into established OT reference models. We
evaluate the suitability of the Reference Architectural Model for Industry 4.0
(RAMI 4.0) and the International Society of Automation Standard 95 (ISA-95) for
MLOps integration and present a detailed mapping of MLOps lifecycle compo-
nents to RAMI 4.0 exemplified by a real-world use case. Our findings
demonstrate that while standard MLOps practices cannot be directly transplanted
to OT environments, structured adaptation using existing reference models can
provide a pathway for successful integration.

</details>


### [98] [Generalizable Reasoning through Compositional Energy Minimization](https://arxiv.org/abs/2510.20607)
*Alexandru Oarga,Yilun Du*

Main category: cs.LG

TL;DR: 提出了一种通过组合子问题能量函数来构建全局能量景观的新方法，以解决机器学习推理任务中的泛化问题，该方法在复杂推理问题上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有端到端推理模型在训练分布之外泛化能力有限，无法有效解决比训练样本更复杂的问题。

Method: 学习子问题的能量景观，在测试时通过组合多个子问题的能量函数构建全局能量景观，并引入并行能量最小化(PEM)来改进采样质量。

Result: 在广泛的推理问题上评估，该方法超越了现有最先进方法，能够泛化到更大更复杂的问题。

Conclusion: 通过组合子问题能量函数构建全局能量景观的方法有效提升了推理任务的泛化能力，为解决复杂推理问题提供了新途径。

Abstract: Generalization is a key challenge in machine learning, specifically in
reasoning tasks, where models are expected to solve problems more complex than
those encountered during training. Existing approaches typically train
reasoning models in an end-to-end fashion, directly mapping input instances to
solutions. While this allows models to learn useful heuristics from data, it
often results in limited generalization beyond the training distribution. In
this work, we propose a novel approach to reasoning generalization by learning
energy landscapes over the solution spaces of smaller, more tractable
subproblems. At test time, we construct a global energy landscape for a given
problem by combining the energy functions of multiple subproblems. This
compositional approach enables the incorporation of additional constraints
during inference, allowing the construction of energy landscapes for problems
of increasing difficulty. To improve the sample quality from this newly
constructed energy landscape, we introduce Parallel Energy Minimization (PEM).
We evaluate our approach on a wide set of reasoning problems. Our method
outperforms existing state-of-the-art methods, demonstrating its ability to
generalize to larger and more complex problems. Project website can be found
at: https://alexoarga.github.io/compositional_reasoning/

</details>


### [99] [Convergence Analysis of SGD under Expected Smoothness](https://arxiv.org/abs/2510.20608)
*Yuta Kawamoto,Hideaki Iiduka*

Main category: cs.LG

TL;DR: 本文对随机梯度下降(SGD)在期望平滑性(ES)条件下的收敛性进行了系统分析，改进了ES条件的解释和常数设定，推导了全梯度范数的期望界限，并证明了多种步长策略下的O(1/K)收敛速率。


<details>
  <summary>Details</summary>
Motivation: 经典SGD分析依赖于过强(有界方差)或过粗糙(均匀噪声)的假设，而期望平滑性条件提供了更灵活的替代方案，将随机梯度的二阶矩与目标函数值和全梯度联系起来。

Method: 通过细化期望平滑性条件的解释和采样相关常数，推导全梯度平方范数的期望界限，并分析多种步长策略下的收敛性。

Result: 证明了在期望平滑性条件下，SGD具有O(1/K)的收敛速率，并给出了明确的残差误差表达式。

Conclusion: 本文的分析统一并扩展了近期相关工作，为SGD在期望平滑性条件下的收敛性提供了完整的理论框架。

Abstract: Stochastic gradient descent (SGD) is the workhorse of large-scale learning,
yet classical analyses rely on assumptions that can be either too strong
(bounded variance) or too coarse (uniform noise). The expected smoothness (ES)
condition has emerged as a flexible alternative that ties the second moment of
stochastic gradients to the objective value and the full gradient. This paper
presents a self-contained convergence analysis of SGD under ES. We (i) refine
ES with interpretations and sampling-dependent constants; (ii) derive bounds of
the expectation of squared full gradient norm; and (iii) prove $O(1/K)$ rates
with explicit residual errors for various step-size schedules. All proofs are
given in full detail in the appendix. Our treatment unifies and extends recent
threads (Khaled and Richt\'arik, 2020; Umeda and Iiduka, 2025).

</details>


### [100] [Practical Code RAG at Scale: Task-Aware Retrieval Design Choices under Compute Budgets](https://arxiv.org/abs/2510.20609)
*Timur Galimzyanov,Olga Kolomyttseva,Egor Bogomolov*

Main category: cs.LG

TL;DR: 本文系统研究了代码生成任务中的检索设计，发现在不同任务类型下，稀疏检索和密集检索各有优势，并提供了基于任务需求、模型约束和计算效率的检索配置建议。


<details>
  <summary>Details</summary>
Motivation: 研究在现实计算预算下代码生成任务的检索设计，为构建有效的代码导向RAG系统提供实证基础。

Method: 使用Long Code Arena中的代码补全和错误定位任务，系统比较不同上下文窗口大小下的检索配置，包括分块策略、相似性评分和分割粒度三个维度。

Result: (1) PL-PL任务中，稀疏BM25+词级分割最有效且实用；(2) NL-PL任务中，专用密集编码器表现更好但延迟高；(3) 最佳分块大小随可用上下文缩放；(4) 简单行级分块与语法感知分割效果相当；(5) 检索延迟差异可达200倍。

Conclusion: 提供了基于证据的代码导向RAG系统实现建议，强调需要根据任务需求、模型约束和计算效率选择合适的检索配置。

Abstract: We study retrieval design for code-focused generation tasks under realistic
compute budgets. Using two complementary tasks from Long Code Arena -- code
completion and bug localization -- we systematically compare retrieval
configurations across various context window sizes along three axes: (i)
chunking strategy, (ii) similarity scoring, and (iii) splitting granularity.
(1) For PL-PL, sparse BM25 with word-level splitting is the most effective and
practical, significantly outperforming dense alternatives while being an order
of magnitude faster. (2) For NL-PL, proprietary dense encoders (Voyager-3
family) consistently beat sparse retrievers, however requiring 100x larger
latency. (3) Optimal chunk size scales with available context: 32-64 line
chunks work best at small budgets, and whole-file retrieval becomes competitive
at 16000 tokens. (4) Simple line-based chunking matches syntax-aware splitting
across budgets. (5) Retrieval latency varies by up to 200x across
configurations; BPE-based splitting is needlessly slow, and BM25 + word
splitting offers the best quality-latency trade-off. Thus, we provide
evidence-based recommendations for implementing effective code-oriented RAG
systems based on task requirements, model constraints, and computational
efficiency.

</details>


### [101] [PSO-XAI: A PSO-Enhanced Explainable AI Framework for Reliable Breast Cancer Detection](https://arxiv.org/abs/2510.20611)
*Mirza Raquib,Niloy Das,Farida Siddiqi Prity,Arafath Al Fahim,Saydul Akbar Murad,Mohammad Amzad Hossain,MD Jiabul Hoque,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: 提出了一种结合定制粒子群优化(PSO)进行特征选择的集成框架，用于乳腺癌诊断，在29个不同模型上实现了99.1%的优异性能，同时降低维度并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性最常见且最致命的癌症之一，传统诊断方法存在变异性、成本高和误诊风险等问题，需要开发更准确可靠的计算机辅助诊断工具。

Method: 使用定制粒子群优化(PSO)进行特征选择，评估了29个不同模型（包括经典分类器、集成技术、神经网络、概率算法和基于实例的算法），结合交叉验证和可解释AI方法确保临床相关性。

Result: 实验评估显示，该方法在所有性能指标（包括准确率和精确率）上均达到99.1%的优异分数，同时有效降低维度并提供透明、模型无关的解释。

Conclusion: 研究结果表明，将群体智能与可解释机器学习相结合，有望实现稳健、可信且具有临床意义的乳腺癌诊断。

Abstract: Breast cancer is considered the most critical and frequently diagnosed cancer
in women worldwide, leading to an increase in cancer-related mortality. Early
and accurate detection is crucial as it can help mitigate possible threats
while improving survival rates. In terms of prediction, conventional diagnostic
methods are often limited by variability, cost, and, most importantly, risk of
misdiagnosis. To address these challenges, machine learning (ML) has emerged as
a powerful tool for computer-aided diagnosis, with feature selection playing a
vital role in improving model performance and interpretability. This research
study proposes an integrated framework that incorporates customized Particle
Swarm Optimization (PSO) for feature selection. This framework has been
evaluated on a comprehensive set of 29 different models, spanning classical
classifiers, ensemble techniques, neural networks, probabilistic algorithms,
and instance-based algorithms. To ensure interpretability and clinical
relevance, the study uses cross-validation in conjunction with explainable AI
methods. Experimental evaluation showed that the proposed approach achieved a
superior score of 99.1\% across all performance metrics, including accuracy and
precision, while effectively reducing dimensionality and providing transparent,
model-agnostic explanations. The results highlight the potential of combining
swarm intelligence with explainable ML for robust, trustworthy, and clinically
meaningful breast cancer diagnosis.

</details>


### [102] [MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation](https://arxiv.org/abs/2510.20615)
*Yang Han,Pengyu Wang,Kai Yu,Xin Chen,Lu Chen*

Main category: cs.LG

TL;DR: MS-BART是一个统一建模框架，通过大规模预训练将质谱和分子结构映射到共享token词汇表中，解决了质谱数据稀缺和信号复杂性的问题。


<details>
  <summary>Details</summary>
Motivation: 质谱在分子识别中起关键作用，但由于标注谱图稀缺，从质谱数据中解析结构仍具挑战性。现有的大规模预训练方法难以直接应用于质谱领域，因为原始谱图信号复杂且异质性强。

Method: 提出MS-BART框架：1）将质谱和分子结构映射到共享token词汇表；2）通过大规模预训练在可靠计算的指纹-分子数据集上进行跨模态学习；3）多任务预训练目标联合优化去噪和翻译任务；4）使用MIST模型生成指纹预测进行微调；5）引入化学反馈机制减少分子幻觉。

Result: 在MassSpecGym和NPLIB1的12个关键指标中，MS-BART在5个指标上达到SOTA性能，比基于扩散的竞争方法快一个数量级。全面的消融研究系统验证了模型的有效性和鲁棒性。

Conclusion: MS-BART通过统一建模框架和跨模态预训练，有效解决了质谱结构解析中的数据稀缺问题，在多个基准测试中表现出优越性能，为质谱分析提供了高效可靠的解决方案。

Abstract: Mass spectrometry (MS) plays a critical role in molecular identification,
significantly advancing scientific discovery. However, structure elucidation
from MS data remains challenging due to the scarcity of annotated spectra.
While large-scale pretraining has proven effective in addressing data scarcity
in other domains, applying this paradigm to mass spectrometry is hindered by
the complexity and heterogeneity of raw spectral signals. To address this, we
propose MS-BART, a unified modeling framework that maps mass spectra and
molecular structures into a shared token vocabulary, enabling cross-modal
learning through large-scale pretraining on reliably computed
fingerprint-molecule datasets. Multi-task pretraining objectives further
enhance MS-BART's generalization by jointly optimizing denoising and
translation task. The pretrained model is subsequently transferred to
experimental spectra through finetuning on fingerprint predictions generated
with MIST, a pre-trained spectral inference model, thereby enhancing robustness
to real-world spectral variability. While finetuning alleviates the
distributional difference, MS-BART still suffers molecular hallucination and
requires further alignment. We therefore introduce a chemical feedback
mechanism that guides the model toward generating molecules closer to the
reference structure. Extensive evaluations demonstrate that MS-BART achieves
SOTA performance across 5/12 key metrics on MassSpecGym and NPLIB1 and is
faster by one order of magnitude than competing diffusion-based methods, while
comprehensive ablation studies systematically validate the model's
effectiveness and robustness.

</details>


### [103] [On Optimal Hyperparameters for Differentially Private Deep Transfer Learning](https://arxiv.org/abs/2510.20616)
*Aki Rehn,Linzh Zhao,Mikko A. Heikkilä,Antti Honkela*

Main category: cs.LG

TL;DR: 本文分析了差分隐私迁移学习中的两个关键超参数：裁剪边界C和批次大小B，揭示了理论与实践的差异，并探讨了在不同隐私约束和计算预算下的最优设置。


<details>
  <summary>Details</summary>
Motivation: 当前差分隐私迁移学习实践中，裁剪边界C和批次大小B的选择存在理论与经验结果的不匹配，且现有启发式方法在有限计算预算下效果不佳，需要更深入的分析来指导超参数调优。

Method: 通过分析梯度分布变化，研究裁剪边界C对性能的影响；在固定训练轮次的计算预算下，考察批次大小B的选择策略；探讨累积DP噪声对批次大小选择的影响机制。

Result: 发现强隐私约束下理论上应选择较小C，但实践中较大C表现更好；现有批次大小启发式方法在有限计算预算下无效；累积DP噪声能更好解释批次大小选择；单一(C,B)设置在不同任务中会导致次优性能。

Conclusion: 裁剪边界C和批次大小B的选择应基于具体任务的隐私约束和计算预算，裁剪可视为梯度重加权机制，累积DP噪声是理解批次大小影响的关键因素，需要任务特定的超参数调优策略。

Abstract: Differentially private (DP) transfer learning, i.e., fine-tuning a pretrained
model on private data, is the current state-of-the-art approach for training
large models under privacy constraints. We focus on two key hyperparameters in
this setting: the clipping bound $C$ and batch size $B$. We show a clear
mismatch between the current theoretical understanding of how to choose an
optimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes
(larger $C$ performs better under strong privacy), caused by changes in the
gradient distributions. Assuming a limited compute budget (fixed epochs), we
demonstrate that the existing heuristics for tuning $B$ do not work, while
cumulative DP noise better explains whether smaller or larger batches perform
better. We also highlight how the common practice of using a single $(C,B)$
setting across tasks can lead to suboptimal performance. We find that
performance drops especially when moving between loose and tight privacy and
between plentiful and limited compute, which we explain by analyzing clipping
as a form of gradient re-weighting and examining cumulative DP noise.

</details>


### [104] [H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition](https://arxiv.org/abs/2510.20627)
*Lukas Miklautz,Chengzhi Shi,Andrii Shkabrii,Theodoros Thirimachos Davarakis,Prudence Lam,Claudia Plant,Jennifer Dy,Stratis Ioannidis*

Main category: cs.LG

TL;DR: H-SPLID是一种新颖算法，通过将显著和非显著特征显式分解到不同空间来学习显著特征表示，促进低维任务相关特征学习，并建立鲁棒性与潜在表示压缩之间的联系。


<details>
  <summary>Details</summary>
Motivation: 现有方法在特征学习中往往混合显著和非显著特征，缺乏对任务相关特征的明确分离，影响了模型的鲁棒性和可解释性。

Method: 通过显式分解显著和非显著特征到不同空间，利用Hilbert-Schmidt独立性准则(HSIC)建立输入扰动下预测偏差的上界，连接鲁棒性与表示压缩。

Result: 在图像分类任务上的实验表明，H-SPLID训练的模型主要依赖显著输入成分，对影响非显著特征（如图像背景）的扰动敏感性降低。

Conclusion: H-SPLID通过特征空间分解有效提升了特征表示的质量和模型鲁棒性，建立了表示维度与鲁棒性之间的理论联系。

Abstract: We introduce H-SPLID, a novel algorithm for learning salient feature
representations through the explicit decomposition of salient and non-salient
features into separate spaces. We show that H-SPLID promotes learning
low-dimensional, task-relevant features. We prove that the expected prediction
deviation under input perturbations is upper-bounded by the dimension of the
salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between
inputs and representations. This establishes a link between robustness and
latent representation compression in terms of the dimensionality and
information preserved. Empirical evaluations on image classification tasks show
that models trained with H-SPLID primarily rely on salient input components, as
indicated by reduced sensitivity to perturbations affecting non-salient
features, such as image backgrounds. Our code is available at
https://github.com/neu-spiral/H-SPLID.

</details>


### [105] [Equitable Survival Prediction: A Fairness-Aware Survival Modeling (FASM) Approach](https://arxiv.org/abs/2510.20629)
*Mingxuan Liu,Yilin Ning,Haoyuan Wang,Chuan Hong,Matthew Engelhard,Danielle S. Bitterman,William G. La Cava,Nan Liu*

Main category: cs.LG

TL;DR: 提出了一种公平感知生存建模方法（FASM），旨在减轻生存分析中的算法偏见，特别是在组内和跨组风险排序方面，以乳腺癌预后为案例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在医疗保健中的应用日益增多，但临床数据中的结构性不平等和社会偏见可能被数据驱动模型延续甚至放大。生存分析中的删失和时间动态进一步增加了公平模型开发的复杂性，现有公平方法往往忽视跨组排序差异。

Method: 提出了公平感知生存建模（FASM）方法，专门设计用于减轻算法偏见，关注组内和跨组风险排序的时间动态。以乳腺癌预后为代表性案例，应用FASM到SEER乳腺癌数据。

Result: FASM显著提高了公平性，同时保持了与不考虑公平性的生存模型相当的判别性能。时间分层评估显示FASM在10年时间范围内保持稳定的公平性，在随访中期观察到最大改进。

Conclusion: 该方法能够开发既重视准确性又关注公平性的生存模型，将公平性作为临床护理的核心原则推进，促进临床决策中的公平性。

Abstract: As machine learning models become increasingly integrated into healthcare,
structural inequities and social biases embedded in clinical data can be
perpetuated or even amplified by data-driven models. In survival analysis,
censoring and time dynamics can further add complexity to fair model
development. Additionally, algorithmic fairness approaches often overlook
disparities in cross-group rankings, e.g., high-risk Black patients may be
ranked below lower-risk White patients who do not experience the event of
mortality. Such misranking can reinforce biological essentialism and undermine
equitable care. We propose a Fairness-Aware Survival Modeling (FASM), designed
to mitigate algorithmic bias regarding both intra-group and cross-group risk
rankings over time. Using breast cancer prognosis as a representative case and
applying FASM to SEER breast cancer data, we show that FASM substantially
improves fairness while preserving discrimination performance comparable to
fairness-unaware survival models. Time-stratified evaluations show that FASM
maintains stable fairness over a 10-year horizon, with the greatest
improvements observed during the mid-term of follow-up. Our approach enables
the development of survival models that prioritize both accuracy and equity in
clinical decision-making, advancing fairness as a core principle in clinical
care.

</details>


### [106] [Large Multimodal Models-Empowered Task-Oriented Autonomous Communications: Design Methodology and Implementation Challenges](https://arxiv.org/abs/2510.20637)
*Hyun Jong Yang,Hyunsoo Kim,Hyeonho Noh,Seungnyun Kim,Byonghyo Shim*

Main category: cs.LG

TL;DR: LLM/LMM在6G自主通信中的任务导向应用，通过多模态感知集成、自适应重配置和提示/微调策略，在交通控制、机器人调度和信道估计等案例中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: LLM和LMM在自然语言理解和复杂推理方面取得突破性进展，使其成为6G机器、车辆和人形机器人间自主通信的关键推动者。

Method: 提出任务导向自主通信框架，集成多模态感知、自适应重配置，以及针对无线任务的提示和微调策略，通过三个案例研究验证。

Result: 实验结果显示，基于LLM/LMM的自主系统在动态目标、变化输入参数和异构多模态条件下显著优于传统和判别性深度学习模型。

Conclusion: LLM/LMM辅助的自主通信系统在动态和异构环境中保持鲁棒性，而传统静态优化方法性能下降。

Abstract: Large language models (LLMs) and large multimodal models (LMMs) have achieved
unprecedented breakthrough, showcasing remarkable capabilities in natural
language understanding, generation, and complex reasoning. This transformative
potential has positioned them as key enablers for 6G autonomous communications
among machines, vehicles, and humanoids. In this article, we provide an
overview of task-oriented autonomous communications with LLMs/LMMs, focusing on
multimodal sensing integration, adaptive reconfiguration, and
prompt/fine-tuning strategies for wireless tasks. We demonstrate the framework
through three case studies: LMM-based traffic control, LLM-based robot
scheduling, and LMM-based environment-aware channel estimation. From
experimental results, we show that the proposed LLM/LMM-aided autonomous
systems significantly outperform conventional and discriminative deep learning
(DL) model-based techniques, maintaining robustness under dynamic objectives,
varying input parameters, and heterogeneous multimodal conditions where
conventional static optimization degrades.

</details>


### [107] [Attention Enhanced Entity Recommendation for Intelligent Monitoring in Cloud Systems](https://arxiv.org/abs/2510.20640)
*Fiza Hussain,Anson Bastos,Anjaly Parayil,Ayush Choure,Chetan Bansal,Rujia Wang,Saravan Rajmohan*

Main category: cs.LG

TL;DR: DiRecGNN是一个基于注意力机制的实体推荐框架，用于监控微软云服务，通过多注意力头机制和随机游走路径采样来捕捉长距离依赖关系，在MRR指标上提升了43.1%。


<details>
  <summary>Details</summary>
Motivation: 解决云服务监控中推荐最优属性子集的问题，现有方法因结构信息有限和同质性无法有效捕捉长距离实体依赖关系。

Method: 构建监控异构图，使用基于Transformer架构的多头注意力机制关注异构邻居及其属性，通过随机游走路径采样捕捉长距离依赖，并采用多面损失函数优化推荐结果。

Result: 模型在MRR指标上比现有方法提升43.1%，产品团队对功能实用性评分为4.5/5。

Conclusion: DiRecGNN框架能有效解决云服务监控中的实体推荐问题，在性能和实用性方面都表现出色。

Abstract: In this paper, we present DiRecGNN, an attention-enhanced entity
recommendation framework for monitoring cloud services at Microsoft. We provide
insights on the usefulness of this feature as perceived by the cloud service
owners and lessons learned from deployment. Specifically, we introduce the
problem of recommending the optimal subset of attributes (dimensions) that
should be tracked by an automated watchdog (monitor) for cloud services. To
begin, we construct the monitor heterogeneous graph at production-scale. The
interaction dynamics of these entities are often characterized by limited
structural and engagement information, resulting in inferior performance of
state-of-the-art approaches. Moreover, traditional methods fail to capture the
dependencies between entities spanning a long range due to their homophilic
nature. Therefore, we propose an attention-enhanced entity ranking model
inspired by transformer architectures. Our model utilizes a multi-head
attention mechanism to focus on heterogeneous neighbors and their attributes,
and further attends to paths sampled using random walks to capture long-range
dependencies. We also employ multi-faceted loss functions to optimize for
relevant recommendations while respecting the inherent sparsity of the data.
Empirical evaluations demonstrate significant improvements over existing
methods, with our model achieving a 43.1% increase in MRR. Furthermore, product
teams who consumed these features perceive the feature as useful and rated it
4.5 out of 5.

</details>


### [108] [Connecting Jensen-Shannon and Kullback-Leibler Divergences: A New Bound for Representation Learning](https://arxiv.org/abs/2510.20644)
*Reuben Dorent,Polina Golland,William Wells III*

Main category: cs.LG

TL;DR: 本文推导了一个新的、紧致的、可处理的KLD下界作为JSD的函数，证明最大化JSD信息能增加互信息的保证下界，为基于互信息的表示学习提供了理论依据。


<details>
  <summary>Details</summary>
Motivation: 虽然互信息是表示学习中的重要统计依赖度量，但直接优化往往难以处理。许多方法转而最大化替代依赖度量（如JSD），但这些替代目标与互信息的关系仍不清楚。

Method: 推导KLD作为JSD函数的紧致下界，通过二元分类器的交叉熵损失实现JSD变分下界，并在各种参考场景中与最先进的神经估计器进行比较。

Result: 实验表明该下界在MI估计中紧致，提供稳定、低方差的紧致下界估计，在信息瓶颈框架中具有实际应用价值。

Conclusion: 研究结果为在基于互信息的表示学习中使用判别学习提供了新的理论依据和强有力的经验证据。

Abstract: Mutual Information (MI) is a fundamental measure of statistical dependence
widely used in representation learning. While direct optimization of MI via its
definition as a Kullback-Leibler divergence (KLD) is often intractable, many
recent methods have instead maximized alternative dependence measures, most
notably, the Jensen-Shannon divergence (JSD) between joint and product of
marginal distributions via discriminative losses. However, the connection
between these surrogate objectives and MI remains poorly understood. In this
work, we bridge this gap by deriving a new, tight, and tractable lower bound on
KLD as a function of JSD in the general case. By specializing this bound to
joint and marginal distributions, we demonstrate that maximizing the JSD-based
information increases a guaranteed lower bound on mutual information.
Furthermore, we revisit the practical implementation of JSD-based objectives
and observe that minimizing the cross-entropy loss of a binary classifier
trained to distinguish joint from marginal pairs recovers a known variational
lower bound on the JSD. Extensive experiments demonstrate that our lower bound
is tight when applied to MI estimation. We compared our lower bound to
state-of-the-art neural estimators of variational lower bound across a range of
established reference scenarios. Our lower bound estimator consistently
provides a stable, low-variance estimate of a tight lower bound on MI. We also
demonstrate its practical usefulness in the context of the Information
Bottleneck framework. Taken together, our results provide new theoretical
justifications and strong empirical evidence for using discriminative learning
in MI-based representation learning.

</details>


### [109] [xTime: Extreme Event Prediction with Hierarchical Knowledge Distillation and Expert Fusion](https://arxiv.org/abs/2510.20651)
*Quan Li,Wenchao Yu,Suhang Wang,Minhua Lin,Lingwei Chen,Wei Cheng,Haifeng Chen*

Main category: cs.LG

TL;DR: 提出了xTime框架，通过知识蒸馏和专家混合机制来改进时间序列中极端事件的预测性能


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测模型在整体性能上表现良好，但在预测极端事件（如洪水、热浪、医疗紧急情况）时表现不佳，主要挑战是数据不平衡和忽略了极端事件前的中等事件信息

Method: 使用知识蒸馏从训练在较低稀有度事件上的模型转移信息，并引入专家混合(MoE)机制动态选择和融合不同稀有度级别的专家模型输出

Result: 在多个数据集上的实验显示，xTime在极端事件上的预测准确率从3%提升到78%

Conclusion: xTime框架通过知识蒸馏和MoE机制有效解决了极端事件预测中的数据不平衡问题，显著提升了预测性能

Abstract: Extreme events frequently occur in real-world time series and often carry
significant practical implications. In domains such as climate and healthcare,
these events, such as floods, heatwaves, or acute medical episodes, can lead to
serious consequences. Accurate forecasting of such events is therefore of
substantial importance. Most existing time series forecasting models are
optimized for overall performance within the prediction window, but often
struggle to accurately predict extreme events, such as high temperatures or
heart rate spikes. The main challenges are data imbalance and the neglect of
valuable information contained in intermediate events that precede extreme
events. In this paper, we propose xTime, a novel framework for extreme event
forecasting in time series. xTime leverages knowledge distillation to transfer
information from models trained on lower-rarity events, thereby improving
prediction performance on rarer ones. In addition, we introduce a mixture of
experts (MoE) mechanism that dynamically selects and fuses outputs from expert
models across different rarity levels, which further improves the forecasting
performance for extreme events. Experiments on multiple datasets show that
xTime achieves consistent improvements, with forecasting accuracy on extreme
events improving from 3% to 78%.

</details>


### [110] [From Masks to Worlds: A Hitchhiker's Guide to World Models](https://arxiv.org/abs/2510.20668)
*Jinbin Bai,Yu Lei,Hecong Wu,Yuchen Zhu,Shufan Li,Yi Xin,Xiangtai Li,Molei Tao,Aditya Grover,Ming-Hsuan Yang*

Main category: cs.LG

TL;DR: 这是一篇关于构建世界模型的指南性综述，聚焦于从多模态统一表征学习到交互式生成模型的发展路径，强调生成核心、交互循环和记忆系统这三个关键要素。


<details>
  <summary>Details</summary>
Motivation: 为想要构建世界模型的研究者提供清晰的指导路线，而不是简单地罗列所有相关论文，旨在展示最有前景的发展方向。

Method: 遵循一条明确的发展路径：从早期的多模态掩码模型，到统一架构，再到交互式生成模型，最后到记忆增强系统，重点关注生成核心、交互循环和记忆系统这三个核心要素。

Result: 展示了从多模态统一表征学习到能够维持时间一致性的世界模型的演进过程，明确了构建真正世界模型的最有前景的技术路径。

Conclusion: 生成核心、交互循环和记忆系统是构建真正世界模型的关键要素，这条发展路径是最有前景的方向。

Abstract: This is not a typical survey of world models; it is a guide for those who
want to build worlds. We do not aim to catalog every paper that has ever
mentioned a ``world model". Instead, we follow one clear road: from early
masked models that unified representation learning across modalities, to
unified architectures that share a single paradigm, then to interactive
generative models that close the action-perception loop, and finally to
memory-augmented systems that sustain consistent worlds over time. We bypass
loosely related branches to focus on the core: the generative heart, the
interactive loop, and the memory system. We show that this is the most
promising path towards true world models.

</details>


### [111] [GRACE: GRaph-based Addiction Care prEdiction](https://arxiv.org/abs/2510.20671)
*Subham Kumar,Prakrithi Shivaprakash,Koustav Rudra,Lekhansh Shukla,Animesh Mukherjee*

Main category: cs.LG

TL;DR: 提出了一种名为GRACE的图神经网络框架，用于解决成瘾患者护理场所预测中的类别不平衡问题，通过特征工程和无偏元图训练方法，在真实数据上显著提升了少数类的F1分数。


<details>
  <summary>Details</summary>
Motivation: 成瘾患者护理场所的确定是影响治疗效果和资源利用的关键临床决策，但缺乏专门的医疗资源，且现有方法面临数据集类别不平衡的严重问题。

Method: 提出GRACE图神经网络框架，将护理场所预测形式化为结构化学习问题，进行广泛的特征工程，并采用获取无偏元图的新方法来训练GNN以克服类别不平衡。

Result: 在真实世界数据上的实验结果显示，相比竞争基线方法，少数类的F1分数提高了11-35%。

Conclusion: GRACE框架通过结构化学习和无偏元图训练，有效解决了成瘾数据集中的类别不平衡问题，显著提升了护理场所预测的性能。

Abstract: Determining the appropriate locus of care for addiction patients is one of
the most critical clinical decisions that affects patient treatment outcomes
and effective use of resources. With a lack of sufficient specialized treatment
resources, such as inpatient beds or staff, there is an unmet need to develop
an automated framework for the same. Current decision-making approaches suffer
from severe class imbalances in addiction datasets. To address this limitation,
we propose a novel graph neural network (GRACE) framework that formalizes locus
of care prediction as a structured learning problem. Further, we perform
extensive feature engineering and propose a new approach of obtaining an
unbiased meta-graph to train a GNN to overcome the class imbalance problem.
Experimental results in real-world data show an improvement of 11-35% in terms
of the F1 score of the minority class over competitive baselines. The codes and
note embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.

</details>


### [112] [A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks](https://arxiv.org/abs/2510.20683)
*Georgios Mentzelopoulos,Ioannis Asmanis,Konrad P. Kording,Eva L. Dyer,Kostas Daniilidis,Flavia Vitale*

Main category: cs.LG

TL;DR: Spikachu是一个基于脉冲神经网络(SNN)的可扩展、因果且节能的神经解码框架，在非人灵长类动物实验中表现出色，能耗比传统方法低2.26-418.81倍，同时支持跨会话、跨受试者和跨任务的少样本迁移学习。


<details>
  <summary>Details</summary>
Motivation: 当前脑机接口中的神经解码器面临两难：简单因果模型缺乏泛化能力，复杂非因果模型虽能泛化但难以实时应用，且都依赖高能耗的人工神经网络。脉冲神经网络因其因果性和低能耗特性，为资源受限环境提供了有前景的替代方案。

Method: 提出Spikachu框架，直接处理分箱的脉冲信号，将其投影到共享潜在空间，通过适应输入时序的脉冲模块提取相关特征，然后整合这些潜在表示并解码生成行为预测。

Result: 在6只非人灵长类动物的113个记录会话（总计43小时记录）上评估，该方法在单会话训练中优于因果基线模型，能耗降低2.26-418.81倍。多会话和跨受试者训练可进一步提升性能，并实现对新会话、受试者和任务的少样本迁移。

Conclusion: Spikachu提供了一个基于SNN的可扩展、在线兼容的神经解码框架，其性能与最先进模型相当，同时能耗显著降低数个数量级。

Abstract: Brain-computer interfaces (BCIs) promise to enable vital functions, such as
speech and prosthetic control, for individuals with neuromotor impairments.
Central to their success are neural decoders, models that map neural activity
to intended behavior. Current learning-based decoding approaches fall into two
classes: simple, causal models that lack generalization, or complex, non-causal
models that generalize and scale offline but struggle in real-time settings.
Both face a common challenge, their reliance on power-hungry artificial neural
network backbones, which makes integration into real-world, resource-limited
systems difficult. Spiking neural networks (SNNs) offer a promising
alternative. Because they operate causally these models are suitable for
real-time use, and their low energy demands make them ideal for
battery-constrained environments. To this end, we introduce Spikachu: a
scalable, causal, and energy-efficient neural decoding framework based on SNNs.
Our approach processes binned spikes directly by projecting them into a shared
latent space, where spiking modules, adapted to the timing of the input,
extract relevant features; these latent representations are then integrated and
decoded to generate behavioral predictions. We evaluate our approach on 113
recording sessions from 6 non-human primates, totaling 43 hours of recordings.
Our method outperforms causal baselines when trained on single sessions using
between 2.26 and 418.81 times less energy. Furthermore, we demonstrate that
scaling up training to multiple sessions and subjects improves performance and
enables few-shot transfer to unseen sessions, subjects, and tasks. Overall,
Spikachu introduces a scalable, online-compatible neural decoding framework
based on SNNs, whose performance is competitive relative to state-of-the-art
models while consuming orders of magnitude less energy.

</details>


### [113] [Separating the what and how of compositional computation to enable reuse and continual learning](https://arxiv.org/abs/2510.20709)
*Haozhe Shan,Sun Minni,Lea Duncker*

Main category: cs.LG

TL;DR: 该论文提出了一种双系统方法来实现持续学习和技能组合重用：一个系统推断要执行什么计算，另一个系统执行如何计算。该方法使用概率生成模型描述任务组合性，并通过无监督在线学习构建任务词汇表，在RNN中实现无灾难性遗忘的持续学习。


<details>
  <summary>Details</summary>
Motivation: 研究持续学习和灵活组合技能的神经机制，解决当前对如何实现持续学习和计算组合重用的理解不足的问题。

Method: 采用双系统方法：what系统通过概率生成模型推断计算上下文，how系统作为RNN根据上下文组合低秩组件。使用无监督在线学习在单次试验基础上学习任务模型。

Result: 该方法在示例任务集上表现出有效性和竞争力，展示了前向和后向迁移潜力，以及对未见任务的快速组合泛化能力。

Conclusion: 双系统学习框架能够实现无灾难性遗忘的持续学习，通过上下文推断促进低秩RNN组件的创建、学习和重用，支持任务的组合泛化。

Abstract: The ability to continually learn, retain and deploy skills to accomplish
goals is a key feature of intelligent and efficient behavior. However, the
neural mechanisms facilitating the continual learning and flexible
(re-)composition of skills remain elusive. Here, we study continual learning
and the compositional reuse of learned computations in recurrent neural network
(RNN) models using a novel two-system approach: one system that infers what
computation to perform, and one that implements how to perform it. We focus on
a set of compositional cognitive tasks commonly studied in neuroscience. To
construct the what system, we first show that a large family of tasks can be
systematically described by a probabilistic generative model, where
compositionality stems from a shared underlying vocabulary of discrete task
epochs. The shared epoch structure makes these tasks inherently compositional.
We first show that this compositionality can be systematically described by a
probabilistic generative model. Furthermore, We develop an unsupervised online
learning approach that can learn this model on a single-trial basis, building
its vocabulary incrementally as it is exposed to new tasks, and inferring the
latent epoch structure as a time-varying computational context within a trial.
We implement the how system as an RNN whose low-rank components are composed
according to the context inferred by the what system. Contextual inference
facilitates the creation, learning, and reuse of low-rank RNN components as new
tasks are introduced sequentially, enabling continual learning without
catastrophic forgetting. Using an example task set, we demonstrate the efficacy
and competitive performance of this two-system learning framework, its
potential for forward and backward transfer, as well as fast compositional
generalization to unseen tasks.

</details>


### [114] [Optimizing Clinical Fall Risk Prediction: A Data-Driven Integration of EHR Variables with the Johns Hopkins Fall Risk Assessment Tool](https://arxiv.org/abs/2510.20714)
*Fardin Ganjkhanloo,Emmett Springer,Erik H. Hoyer,Daniel L. Young,Kimia Ghobadi*

Main category: cs.LG

TL;DR: 本研究通过数据驱动方法优化约翰霍普金斯跌倒风险评估工具(JHFRAT)，结合临床知识和电子健康记录数据，开发约束评分优化模型，显著提升跌倒风险预测性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数据驱动建模方法，将JHFRAT跌倒风险预测与更多临床有意义指标更好地对齐，改进住院患者跌倒预防方案。

Method: 对54,209例住院患者进行回顾性分析，使用约束评分优化(CSO)模型结合JHFRAT评估数据和电子健康记录变量，保持模型可解释性。

Result: 约束评分优化模型预测性能显著优于当前JHFRAT(AUC-ROC=0.91 vs 0.86)，与黑盒模型(XGBoost)性能接近(AUC-ROC=0.94)但更具鲁棒性，且加入EHR变量对性能影响不大。

Conclusion: 这种循证方法为医疗系统使用数据驱动优化技术系统增强住院患者跌倒预防方案和患者安全提供了坚实基础，有助于改善风险评估和医疗资源配置。

Abstract: In this study we aim to better align fall risk prediction from the Johns
Hopkins Fall Risk Assessment Tool (JHFRAT) with additional clinically
meaningful measures via a data-driven modelling approach. We conducted a
retrospective analysis of 54,209 inpatient admissions from three Johns Hopkins
Health System hospitals between March 2022 and October 2023. A total of 20,208
admissions were included as high fall risk encounters, and 13,941 were included
as low fall risk encounters. To incorporate clinical knowledge and maintain
interpretability, we employed constrained score optimization (CSO) models on
JHFRAT assessment data and additional electronic health record (EHR) variables.
The model demonstrated significant improvements in predictive performance over
the current JHFRAT (CSO AUC-ROC=0.91, JHFRAT AUC-ROC=0.86). The constrained
score optimization models performed similarly with and without the EHR
variables. Although the benchmark black-box model (XGBoost), improves upon the
performance metrics of the knowledge-based constrained logistic regression
(AUC-ROC=0.94), the CSO demonstrates more robustness to variations in risk
labelling. This evidence-based approach provides a robust foundation for health
systems to systematically enhance inpatient fall prevention protocols and
patient safety using data-driven optimization techniques, contributing to
improved risk assessment and resource allocation in healthcare settings.

</details>


### [115] [Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2510.20718)
*Daniel Sorensen,Bappaditya Dey,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: 该论文提出了两种用于半导体制造异常预测的新方法：基于N-BEATS的单变量预测和基于图神经网络的多变量关系建模方法，实现了从异常检测到异常预测的转变。


<details>
  <summary>Details</summary>
Motivation: 半导体制造过程复杂且精度要求高，现有异常检测方法面临高维数据、类别不平衡和变量间复杂依赖关系的挑战，需要从检测转向预测以实现实时过程校正。

Method: 采用两阶段框架：首先在无异常数据集上训练预测模型，然后对未见时间序列进行预测，将预测偏差超过阈值的点标记为异常。比较了N-BEATS单变量模型和GNN多变量模型。

Result: 两种模型在20个时间点内表现出强预测性能，在50个时间点内保持稳定的异常预测。GNN模型在性能上始终优于N-BEATS，且所需参数更少、计算成本更低。

Conclusion: GNN模型是制造环境中在线异常预测的有前景解决方案，能够有效捕获变量间关系并实现高效预测。

Abstract: Semiconductor manufacturing is an extremely complex and precision-driven
process, characterized by thousands of interdependent parameters collected
across diverse tools and process steps. Multi-variate time-series analysis has
emerged as a critical field for real-time monitoring and fault detection in
such environments. However, anomaly prediction in semiconductor fabrication
presents several critical challenges, including high dimensionality of sensor
data and severe class imbalance due to the rarity of true faults. Furthermore,
the complex interdependencies between variables complicate both anomaly
prediction and root-cause-analysis. This paper proposes two novel approaches to
advance the field from anomaly detection to anomaly prediction, an essential
step toward enabling real-time process correction and proactive fault
prevention. The proposed anomaly prediction framework contains two main stages:
(a) training a forecasting model on a dataset assumed to contain no anomalies,
and (b) performing forecast on unseen time series data. The forecast is
compared with the forecast of the trained signal. Deviations beyond a
predefined threshold are flagged as anomalies. The two approaches differ in the
forecasting model employed. The first assumes independence between variables by
utilizing the N-BEATS model for univariate time series forecasting. The second
lifts this assumption by utilizing a Graph Neural Network (GNN) to capture
inter-variable relationships. Both models demonstrate strong forecasting
performance up to a horizon of 20 time points and maintain stable anomaly
prediction up to 50 time points. The GNN consistently outperforms the N-BEATS
model while requiring significantly fewer trainable parameters and lower
computational cost. These results position the GNN as promising solution for
online anomaly forecasting to be deployed in manufacturing environments.

</details>


### [116] [No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes](https://arxiv.org/abs/2510.20725)
*Jasmine Bayrooti,Sattar Vakili,Amanda Prorok,Carl Henrik Ek*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Thompson sampling (TS) is a powerful and widely used strategy for sequential
decision-making, with applications ranging from Bayesian optimization to
reinforcement learning (RL). Despite its success, the theoretical foundations
of TS remain limited, particularly in settings with complex temporal structure
such as RL. We address this gap by establishing no-regret guarantees for TS
using models with Gaussian marginal distributions. Specifically, we consider TS
in episodic RL with joint Gaussian process (GP) priors over rewards and
transitions. We prove a regret bound of
$\mathcal{\tilde{O}}(\sqrt{KH\Gamma(KH)})$ over $K$ episodes of horizon $H$,
where $\Gamma(\cdot)$ captures the complexity of the GP model. Our analysis
addresses several challenges, including the non-Gaussian nature of value
functions and the recursive structure of Bellman updates, and extends classical
tools such as the elliptical potential lemma to multi-output settings. This
work advances the understanding of TS in RL and highlights how structural
assumptions and model uncertainty shape its performance in finite-horizon
Markov Decision Processes.

</details>


### [117] [Thought Communication in Multiagent Collaboration](https://arxiv.org/abs/2510.20733)
*Yujia Zheng,Zhuokai Zhao,Zijian Li,Yaqi Xie,Mingze Gao,Lizhu Zhang,Kun Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为"思想通信"的新范式，使智能体能够直接进行心灵对心灵的交流，类似于心灵感应。通过将过程形式化为潜变量模型，理论上证明了共享和私有潜在思想的可识别性，并开发了提取和分配潜在思想的框架。


<details>
  <summary>Details</summary>
Motivation: 自然语言的模糊性和间接性限制了集体智能的潜力。虽然机器不受这些限制，但大多数基于LLM的多智能体系统仍仅依赖自然语言。为了超越语言限制，需要实现更直接的智能体间通信。

Method: 将智能体通信形式化为一般潜变量模型，其中智能体状态由潜在思想的未知函数生成。开发了从所有智能体中提取潜在思想并在通信前分配相关思想及其共享模式的框架。

Result: 在合成和真实世界基准测试中验证了理论，证明了思想通信的协作优势。该方法可扩展到所有模态，因为大多数观测数据都来自隐藏的生成过程。

Conclusion: 思想通信范式揭示了利用隐藏世界的潜力，因为许多挑战仅通过表面观察无法解决，无论计算或数据规模如何。

Abstract: Natural language has long enabled human cooperation, but its lossy,
ambiguous, and indirect nature limits the potential of collective intelligence.
While machines are not subject to these constraints, most LLM-based multi-agent
systems still rely solely on natural language, exchanging tokens or their
embeddings. To go beyond language, we introduce a new paradigm, thought
communication, which enables agents to interact directly mind-to-mind, akin to
telepathy. To uncover these latent thoughts in a principled way, we formalize
the process as a general latent variable model, where agent states are
generated by an unknown function of underlying thoughts. We prove that, in a
nonparametric setting without auxiliary information, both shared and private
latent thoughts between any pair of agents can be identified. Moreover, the
global structure of thought sharing, including which agents share which
thoughts and how these relationships are structured, can also be recovered with
theoretical guarantees. Guided by the established theory, we develop a
framework that extracts latent thoughts from all agents prior to communication
and assigns each agent the relevant thoughts, along with their sharing
patterns. This paradigm naturally extends beyond LLMs to all modalities, as
most observational data arise from hidden generative processes. Experiments on
both synthetic and real-world benchmarks validate the theory and demonstrate
the collaborative advantages of thought communication. We hope this work
illuminates the potential of leveraging the hidden world, as many challenges
remain unsolvable through surface-level observation alone, regardless of
compute or data scale.

</details>


### [118] [Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process](https://arxiv.org/abs/2510.20736)
*Tsai Hor Chan,Feng Wu,Yihang Chen,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出了一种基于狄利克雷过程的多模态学习框架，通过DP的"富者更富"特性自动平衡模态内特征学习和跨模态对齐，在多个数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决多模态融合中如何在保持各模态特征表达能力的同时学习跨模态交互的关键挑战。现有方法过度强调模态边缘分布对齐可能导致过度正则化，阻碍各模态内有意义的表示学习。

Method: 假设每个模态遵循多元高斯混合分布，采用狄利克雷过程计算所有分量的混合权重。利用DP的"富者更富"特性动态分配特征贡献度并选择最显著特征，促进多模态特征融合。

Result: 在多个多模态数据集上的广泛实验表明，该模型优于其他竞争方法。消融分析进一步验证了DP在模态分布对齐中的有效性及其对关键超参数变化的鲁棒性。

Conclusion: 提出的DP驱动多模态学习框架能够自动实现显著的模态内表示学习和跨模态对齐之间的最优平衡，在多模态融合任务中表现出色。

Abstract: Developing effective multimodal fusion approaches has become increasingly
essential in many real-world scenarios, such as health care and finance. The
key challenge is how to preserve the feature expressiveness in each modality
while learning cross-modal interactions. Previous approaches primarily focus on
the cross-modal alignment, while over-emphasis on the alignment of marginal
distributions of modalities may impose excess regularization and obstruct
meaningful representations within each modality. The Dirichlet process (DP)
mixture model is a powerful Bayesian non-parametric method that can amplify the
most prominent features by its richer-gets-richer property, which allocates
increasing weights to them. Inspired by this unique characteristic of DP, we
propose a new DP-driven multimodal learning framework that automatically
achieves an optimal balance between prominent intra-modal representation
learning and cross-modal alignment. Specifically, we assume that each modality
follows a mixture of multivariate Gaussian distributions and further adopt DP
to calculate the mixture weights for all the components. This paradigm allows
DP to dynamically allocate the contributions of features and select the most
prominent ones, leveraging its richer-gets-richer property, thus facilitating
multimodal feature fusion. Extensive experiments on several multimodal datasets
demonstrate the superior performance of our model over other competitors.
Ablation analysis further validates the effectiveness of DP in aligning
modality distributions and its robustness to changes in key hyperparameters.
Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git

</details>


### [119] [MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs](https://arxiv.org/abs/2510.20762)
*Jan Sobotka,Luca Baroni,Ján Antolík*

Main category: cs.LG

TL;DR: MEIcoder是一种生物信息解码方法，利用神经元特异性最兴奋输入(MEIs)、结构相似性指数损失和对抗训练，在小数据集上实现了从初级视觉皮层(V1)单细胞活动重建视觉刺激的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 灵长类或人类等生物数据通常稀缺，高吞吐量记录技术难以应用，这对深度学习解码技术构成挑战。

Method: 使用神经元特异性最兴奋输入(MEIs)、结构相似性指数测量损失和对抗训练的生物信息解码方法。

Result: 在仅有1,000-2,500个神经元和少于1,000个训练数据点的情况下，能够重建高保真自然图像，在小数据集上表现优异。

Conclusion: 证明了在早期视觉系统中可靠解码的可行性，为神经科学和神经工程应用提供了实用见解。

Abstract: Decoding visual stimuli from neural population activity is crucial for
understanding the brain and for applications in brain-machine interfaces.
However, such biological data is often scarce, particularly in primates or
humans, where high-throughput recording techniques, such as two-photon imaging,
remain challenging or impossible to apply. This, in turn, poses a challenge for
deep learning decoding techniques. To overcome this, we introduce MEIcoder, a
biologically informed decoding method that leverages neuron-specific most
exciting inputs (MEIs), a structural similarity index measure loss, and
adversarial training. MEIcoder achieves state-of-the-art performance in
reconstructing visual stimuli from single-cell activity in primary visual
cortex (V1), especially excelling on small datasets with fewer recorded
neurons. Using ablation studies, we demonstrate that MEIs are the main drivers
of the performance, and in scaling experiments, we show that MEIcoder can
reconstruct high-fidelity natural-looking images from as few as 1,000-2,500
neurons and less than 1,000 training data points. We also propose a unified
benchmark with over 160,000 samples to foster future research. Our results
demonstrate the feasibility of reliable decoding in early visual system and
provide practical insights for neuroscience and neuroengineering applications.

</details>


### [120] [Out-of-distribution Tests Reveal Compositionality in Chess Transformers](https://arxiv.org/abs/2510.20783)
*Anna Mészáros,Patrik Reizinger,Ferenc Huszár*

Main category: cs.LG

TL;DR: 这篇论文研究了Transformer模型在象棋任务中的系统泛化能力，发现模型能够进行组合泛化，在分布外场景中保持游戏规则的一致性，但在需要策略适应的变体游戏中表现不如符号AI算法。


<details>
  <summary>Details</summary>
Motivation: 研究现代决策Transformer是否真正理解象棋规则，而不仅仅是学习训练数据中的模式，通过测试其在分布外场景中的表现来评估其系统泛化能力。

Method: 训练了一个2.7亿参数的象棋Transformer模型，并在多种分布外场景中进行测试，包括规则外推测试和Chess960变体游戏，同时分析训练动态。

Result: 模型表现出组合泛化能力，能够一致地选择有效移动，在分布外谜题中生成高质量移动。在Chess960中表现出基本策略适应能力，但不如符号AI算法，不过在Lichess对战中差距较小。训练动态显示模型最初只学习移动己方棋子，表明了对游戏组合性理解的涌现。

Conclusion: Transformer模型能够捕捉象棋的基本组合结构，在规则遵守方面表现良好，但在需要深度策略推理的复杂变体中仍有改进空间，训练动态揭示了模型对游戏规则的理解是逐步形成的。

Abstract: Chess is a canonical example of a task that requires rigorous reasoning and
long-term planning. Modern decision Transformers - trained similarly to LLMs -
are able to learn competent gameplay, but it is unclear to what extent they
truly capture the rules of chess. To investigate this, we train a 270M
parameter chess Transformer and test it on out-of-distribution scenarios,
designed to reveal failures of systematic generalization. Our analysis shows
that Transformers exhibit compositional generalization, as evidenced by strong
rule extrapolation: they adhere to fundamental syntactic rules of the game by
consistently choosing valid moves even in situations very different from the
training data. Moreover, they also generate high-quality moves for OOD puzzles.
In a more challenging test, we evaluate the models on variants including
Chess960 (Fischer Random Chess) - a variant of chess where starting positions
of pieces are randomized. We found that while the model exhibits basic strategy
adaptation, they are inferior to symbolic AI algorithms that perform explicit
search, but gap is smaller when playing against users on Lichess. Moreover, the
training dynamics revealed that the model initially learns to move only its own
pieces, suggesting an emergent compositional understanding of the game.

</details>


### [121] [BadGraph: A Backdoor Attack Against Latent Diffusion Model for Text-Guided Graph Generation](https://arxiv.org/abs/2510.20792)
*Liang Ye,Shengqin Chen,Jiazhu Dai*

Main category: cs.LG

TL;DR: BadGraph是一种针对文本引导图生成的潜在扩散模型的隐蔽后门攻击方法，使用文本触发器在训练数据中植入后门，在推理时生成攻击者指定的子图。


<details>
  <summary>Details</summary>
Motivation: 探索文本引导图生成模型中的安全漏洞，特别是后门攻击风险，这在之前的研究中尚未充分研究。

Method: 利用文本触发器毒化训练数据，在潜在扩散模型中植入后门，使得当推理时出现触发器时生成攻击者指定的子图，同时保持对干净输入的正常性能。

Result: 在四个基准数据集上的实验表明，攻击具有高有效性和隐蔽性：低于10%的毒化率即可达到50%攻击成功率，24%毒化率可实现超过80%成功率，且对良性样本的性能影响可忽略。

Conclusion: 揭示了文本引导图生成的潜在扩散模型存在严重安全漏洞，在药物发现等应用中构成重大风险，亟需针对此类扩散模型的鲁棒防御机制。

Abstract: The rapid progress of graph generation has raised new security concerns,
particularly regarding backdoor vulnerabilities. While prior work has explored
backdoor attacks in image diffusion and unconditional graph generation,
conditional, especially text-guided graph generation remains largely
unexamined. This paper proposes BadGraph, a backdoor attack method targeting
latent diffusion models for text-guided graph generation. BadGraph leverages
textual triggers to poison training data, covertly implanting backdoors that
induce attacker-specified subgraphs during inference when triggers appear,
while preserving normal performance on clean inputs. Extensive experiments on
four benchmark datasets (PubChem, ChEBI-20, PCDes, MoMu) demonstrate the
effectiveness and stealth of the attack: less than 10% poisoning rate can
achieves 50% attack success rate, while 24% suffices for over 80% success rate,
with negligible performance degradation on benign samples. Ablation studies
further reveal that the backdoor is implanted during VAE and diffusion training
rather than pretraining. These findings reveal the security vulnerabilities in
latent diffusion models of text-guided graph generation, highlight the serious
risks in models' applications such as drug discovery and underscore the need
for robust defenses against the backdoor attack in such diffusion models.

</details>


### [122] [Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples](https://arxiv.org/abs/2510.20800)
*Shiva Sreeram,Alaa Maalouf,Pratyusha Sharma,Daniela Rus*

Main category: cs.LG

TL;DR: 提出了一种无需微调的快速LLM适配方法，通过梯度分析识别关键权重矩阵，使用聚类分解技术提升性能，仅需100个样本即可完成适配。


<details>
  <summary>Details</summary>
Motivation: LASER方法虽然能提升下游任务准确率，但其逐层搜索和全数据集前向传播的开销使其难以快速部署。本文旨在消除这种开销，实现快速适配。

Method: 1) 仅检查少量关键矩阵，避免逐层扫描；2) 使用矩阵奇异值梯度识别需要降维的矩阵；3) 允许矩阵行围绕多个子空间聚类并分别分解；4) 仅使用100个样本进行评估。

Result: 该方法进一步减少过拟合，准确率提升高达24.6个百分点，搜索时间显著减少，仅需单次梯度步和快速扫描即可完成LLM适配。

Conclusion: 结合这些发现，开发出快速且鲁棒的下游任务适配算法，无需微调即可将LLM适配到新数据集。

Abstract: Recently, Sharma et al. suggested a method called Layer-SElective-Rank
reduction (LASER) which demonstrated that pruning high-order components of
carefully chosen LLM's weight matrices can boost downstream accuracy -- without
any gradient-based fine-tuning. Yet LASER's exhaustive, per-matrix search (each
requiring full-dataset forward passes) makes it impractical for rapid
deployment. We demonstrate that this overhead can be removed and find that: (i)
Only a small, carefully chosen subset of matrices needs to be inspected --
eliminating the layer-by-layer sweep, (ii) The gradient of each matrix's
singular values pinpoints which matrices merit reduction, (iii) Increasing the
factorization search space by allowing matrices rows to cluster around multiple
subspaces and then decomposing each cluster separately further reduces
overfitting on the original training data and further lifts accuracy by up to
24.6 percentage points, and finally, (iv) we discover that evaluating on just
100 samples rather than the full training data -- both for computing the
indicative gradients and for measuring the final accuracy -- suffices to
further reduce the search time; we explain that as adaptation to downstream
tasks is dominated by prompting style, not dataset size. As a result, we show
that combining these findings yields a fast and robust adaptation algorithm for
downstream tasks. Overall, with a single gradient step on 100 examples and a
quick scan of the top candidate layers and factorization techniques, we can
adapt LLMs to new datasets -- entirely without fine-tuning.

</details>


### [123] [KL-Regularized Reinforcement Learning is Designed to Mode Collapse](https://arxiv.org/abs/2510.20817)
*Anthony GX-Chen,Jatin Prakash,Jeff Guo,Rob Fergus,Rajesh Ranganath*

Main category: cs.LG

TL;DR: 本文挑战了关于反向KL散度和前向KL散度在强化学习中作用的传统直觉，发现选择哪种KL散度决定了最优目标分布的参数化形式，而模式覆盖主要取决于正则化强度、奖励和参考概率的相对尺度等因素。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为优化反向KL散度会导致"模式寻求"，而优化前向KL散度会导致"质量覆盖"，后者在需要从多个不同模式采样时更受青睐。但作者发现这种直觉在强化学习中的KL正则化场景下并不一定成立。

Method: 作者从数学和经验上分析了反向/前向KL正则化在强化学习中的作用，构建了一个简单、可扩展且理论上有依据的算法，该算法对奖励幅度进行最小修改，但优化了一个在所有高质量采样模式上都赋予高概率的目标分布。

Result: 实验表明，这种简单修改能够在大语言模型和化学语言模型的后训练中提高解决方案的质量和多样性，无需任何外部多样性信号，并且在前向和反向KL单独使用失败时都能工作。

Conclusion: 模式覆盖主要取决于正则化强度、奖励和参考概率的相对尺度等因素，而非简单地选择反向或前向KL散度。常用的低正则化强度和相等可验证奖励设置往往指定单峰目标分布，使得优化目标本质上缺乏多样性。

Abstract: It is commonly believed that optimizing the reverse KL divergence results in
"mode seeking", while optimizing forward KL results in "mass covering", with
the latter being preferred if the goal is to sample from multiple diverse
modes. We show -- mathematically and empirically -- that this intuition does
not necessarily transfer well to doing reinforcement learning with
reverse/forward KL regularization (e.g. as commonly used with language models).
Instead, the choice of reverse/forward KL determines the family of optimal
target distributions, parameterized by the regularization coefficient. Mode
coverage depends primarily on other factors, such as regularization strength,
and relative scales between rewards and reference probabilities. Further, we
show commonly used settings such as low regularization strength and equal
verifiable rewards tend to specify unimodal target distributions, meaning the
optimization objective is, by construction, non-diverse. We leverage these
insights to construct a simple, scalable, and theoretically justified
algorithm. It makes minimal changes to reward magnitudes, yet optimizes for a
target distribution which puts high probability over all high-quality sampling
modes. In experiments, this simple modification works to post-train both Large
Language Models and Chemical Language Models to have higher solution quality
and diversity, without any external signals of diversity, and works with both
forward and reverse KL when using either naively fails.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [124] [Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models](https://arxiv.org/abs/2510.19999)
*Yixiao Wang,Zishan Shao,Ting Jiang,Aditya Devarakonda*

Main category: stat.ML

TL;DR: 提出了一种增强型循环坐标下降(ECCD)框架，用于求解具有弹性网络约束的广义线性模型，相比现有方法减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有坐标下降方法在梯度计算中存在非线性操作，导致训练效率不高，且块坐标下降存在收敛延迟和数值不稳定问题。

Method: 通过在当前迭代点进行泰勒展开来避免梯度计算中的非线性操作，将向量递推展开为可调参数s控制的批次计算，s>1时提升性能但不影响收敛。

Result: 在多样化基准数据集上，与现有最优求解器相比，正则化路径变体的平均性能提升3倍。

Conclusion: ECCD方法有效避免了块坐标下降的收敛延迟和数值不稳定问题，实现了显著的性能提升，且不影响收敛性。

Abstract: We present a novel enhanced cyclic coordinate descent (ECCD) framework for
solving generalized linear models with elastic net constraints that reduces
training time in comparison to existing state-of-the-art methods. We redesign
the CD method by performing a Taylor expansion around the current iterate to
avoid nonlinear operations arising in the gradient computation. By introducing
this approximation, we are able to unroll the vector recurrences occurring in
the CD method and reformulate the resulting computations into more efficient
batched computations. We show empirically that the recurrence can be unrolled
by a tunable integer parameter, $s$, such that $s > 1$ yields performance
improvements without affecting convergence, whereas $s = 1$ yields the original
CD method. A key advantage of ECCD is that it avoids the convergence delay and
numerical instability exhibited by block coordinate descent. Finally, we
implement our proposed method in C++ using Eigen to accelerate linear algebra
computations. Comparison of our method against existing state-of-the-art
solvers shows consistent performance improvements of $3\times$ in average for
regularization path variant on diverse benchmark datasets. Our implementation
is available at https://github.com/Yixiao-Wang-Stats/ECCD.

</details>


### [125] [Compositional Generation for Long-Horizon Coupled PDEs](https://arxiv.org/abs/2510.20141)
*Somayajulu L. N. Dhulipala,Deep Ray,Nicholas Forman*

Main category: stat.ML

TL;DR: 该论文提出了一种组合扩散方法，仅使用解耦PDE数据训练扩散模型，在推理时组合以恢复耦合场，证明在长时间范围内可行且误差较低。


<details>
  <summary>Details</summary>
Motivation: 模拟耦合PDE系统计算成本高，传统方法需要在联合数据上训练代理模型，需要大量数据。本研究探索仅使用解耦数据训练扩散模型并通过组合恢复耦合场的可行性。

Method: 使用组合扩散方法，仅在解耦PDE数据上训练扩散模型，在推理时通过基于欧拉格式的对称组合方案恢复耦合场。比较了基线扩散模型和v参数化策略。

Result: 尽管只看到解耦训练数据，组合扩散模型能够以低误差恢复耦合轨迹。v参数化比基线扩散模型精度更高，但在耦合数据上训练的神经算子代理仍表现最强。

Conclusion: 组合扩散是高效建模长时间范围耦合PDE的可行策略，即使仅使用解耦数据训练也能取得良好效果。

Abstract: Simulating coupled PDE systems is computationally intensive, and prior
efforts have largely focused on training surrogates on the joint (coupled)
data, which requires a large amount of data. In the paper, we study
compositional diffusion approaches where diffusion models are only trained on
the decoupled PDE data and are composed at inference time to recover the
coupled field. Specifically, we investigate whether the compositional strategy
can be feasible under long time horizons involving a large number of time
steps. In addition, we compare a baseline diffusion model with that trained
using the v-parameterization strategy. We also introduce a symmetric
compositional scheme for the coupled fields based on the Euler scheme. We
evaluate on Reaction-Diffusion and modified Burgers with longer time grids, and
benchmark against a Fourier Neural Operator trained on coupled data. Despite
seeing only decoupled training data, the compositional diffusion models recover
coupled trajectories with low error. v-parameterization can improve accuracy
over a baseline diffusion model, while the neural operator surrogate remains
strongest given that it is trained on the coupled data. These results show that
compositional diffusion is a viable strategy towards efficient, long-horizon
modeling of coupled PDEs.

</details>


### [126] [Neural Networks for Censored Expectile Regression Based on Data Augmentation](https://arxiv.org/abs/2510.20344)
*Wei Cao,Shanshan Wang*

Main category: stat.ML

TL;DR: 提出了基于数据增强的期望分位数回归神经网络(DAERNN)，用于处理包含删失数据的异质性建模问题，相比现有方法有更好性能


<details>
  <summary>Details</summary>
Motivation: 现有期望分位数回归神经网络主要关注完全观测数据，对包含删失观测的场景关注有限，需要开发能够处理异质性删失数据的灵活方法

Method: 提出数据增强的ERNN算法(DAERNN)，完全数据驱动，需要最少假设，提供统一框架处理各种删失机制，无需显式参数模型设定

Result: 模拟研究和实际数据应用表明DAERNN优于现有删失ERNN方法，预测性能接近在完全观测数据上训练的模型

Conclusion: DAERNN为实际删失数据分析提供了灵活且性能优越的统一框架，能够有效处理各种删失机制

Abstract: Expectile regression neural networks (ERNNs) are powerful tools for capturing
heterogeneity and complex nonlinear structures in data. However, most existing
research has primarily focused on fully observed data, with limited attention
paid to scenarios involving censored observations. In this paper, we propose a
data augmentation based ERNNs algorithm, termed DAERNN, for modeling
heterogeneous censored data. The proposed DAERNN is fully data driven, requires
minimal assumptions, and offers substantial flexibility. Simulation studies and
real data applications demonstrate that DAERNN outperforms existing censored
ERNNs methods and achieves predictive performance comparable to models trained
on fully observed data. Moreover, the algorithm provides a unified framework
for handling various censoring mechanisms without requiring explicit parametric
model specification, thereby enhancing its applicability to practical censored
data analysis.

</details>


### [127] [Testing Most Influential Sets](https://arxiv.org/abs/2510.20372)
*Lucas Darius Konrad,Nikolas Kuschnig*

Main category: stat.ML

TL;DR: 开发了一个统计框架来评估最具影响力数据集的统计显著性，解决了当前缺乏正式理论来确定影响力是否反映真实问题而非自然抽样变异的问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏正式理论来确定最具影响力数据集的影响力是否反映真实问题而非自然抽样变异，现有方法多为临时敏感性检查。

Method: 开发了一个原则性框架来评估最具影响力数据集的统计显著性，通过理论结果描述最大影响力的极值分布，并实现严格的假设检验。

Result: 该方法能够进行严格的过度影响力假设检验，在实际应用中展示了实用价值。

Conclusion: 提出的框架能够替代当前的临时敏感性检查，为评估数据影响力提供统计显著性检验。

Abstract: Small subsets of data with disproportionate influence on model outcomes can
have dramatic impacts on conclusions, with a few data points sometimes
overturning key findings. While recent work has developed methods to identify
these \emph{most influential sets}, no formal theory exists to determine when
their influence reflects genuine problems rather than natural sampling
variation. We address this gap by developing a principled framework for
assessing the statistical significance of most influential sets. Our
theoretical results characterize the extreme value distributions of maximal
influence and enable rigorous hypothesis tests for excessive influence,
replacing current ad-hoc sensitivity checks. We demonstrate the practical value
of our approach through applications across economics, biology, and machine
learning benchmarks.

</details>


### [128] [Learning Decentralized Routing Policies via Graph Attention-based Multi-Agent Reinforcement Learning in Lunar Delay-Tolerant Networks](https://arxiv.org/abs/2510.20436)
*Federico Lozano-Cuadra,Beatriz Soret,Marc Sanchez Net,Abhishek Cauligi,Federico Rossi*

Main category: stat.ML

TL;DR: 提出基于图注意力多智能体强化学习的去中心化路由框架，用于月球延迟容忍网络中的多机器人探索任务，在间歇连接和未知移动模式下实现高效数据中继。


<details>
  <summary>Details</summary>
Motivation: 解决月球探索任务中多机器人在间歇连接和未知移动模式下的数据中继问题，传统方法如最短路径和受控洪泛算法需要全局拓扑更新或数据包复制，不适合资源受限的太空环境。

Method: 将问题建模为部分可观测马尔可夫决策过程，提出基于图注意力的多智能体强化学习策略，采用集中训练去中心化执行方式，仅依赖局部观测。

Result: 蒙特卡洛模拟显示，该方法在随机探索环境中提供更高的交付率、无重复传输、更少的数据包丢失，并能利用短期移动预测，成功扩展到更大的机器人团队。

Conclusion: 该框架为未来行星探索的太空机器人系统提供了可扩展的解决方案，在月球延迟容忍网络中表现出优越性能。

Abstract: We present a fully decentralized routing framework for multi-robot
exploration missions operating under the constraints of a Lunar Delay-Tolerant
Network (LDTN). In this setting, autonomous rovers must relay collected data to
a lander under intermittent connectivity and unknown mobility patterns. We
formulate the problem as a Partially Observable Markov Decision Problem (POMDP)
and propose a Graph Attention-based Multi-Agent Reinforcement Learning
(GAT-MARL) policy that performs Centralized Training, Decentralized Execution
(CTDE). Our method relies only on local observations and does not require
global topology updates or packet replication, unlike classical approaches such
as shortest path and controlled flooding-based algorithms. Through Monte Carlo
simulations in randomized exploration environments, GAT-MARL provides higher
delivery rates, no duplications, and fewer packet losses, and is able to
leverage short-term mobility forecasts; offering a scalable solution for future
space robotic systems for planetary exploration, as demonstrated by successful
generalization to larger rover teams.

</details>


### [129] [Concentration and excess risk bounds for imbalanced classification with synthetic oversampling](https://arxiv.org/abs/2510.20472)
*Touqeer Ahmad,Mohammadreza M. Kalan,François Portier,Gilles Stupfler*

Main category: stat.ML

TL;DR: 本文为SMOTE及其变种方法建立了理论框架，分析了在合成数据上训练分类器的行为，提供了经验风险与真实分布风险的浓度边界，以及核分类器的超额风险保证，并提出了参数调优的实用指南。


<details>
  <summary>Details</summary>
Motivation: 尽管SMOTE及其变种方法在实践中成功用于处理不平衡分类问题，但其理论基础仍然不足，需要深入的理论分析来理解其行为。

Method: 开发理论框架分析SMOTE方法，推导合成少数类样本经验风险与真实少数类分布风险之间的浓度边界，提供核分类器在合成数据上训练的非参数超额风险保证。

Result: 获得了合成数据经验风险与真实分布风险之间的均匀浓度边界，以及核分类器的超额风险理论保证，这些结果为参数调优提供了理论依据。

Conclusion: 理论分析为SMOTE方法和下游学习算法的参数调优提供了实用指南，数值实验验证了理论发现。

Abstract: Synthetic oversampling of minority examples using SMOTE and its variants is a
leading strategy for addressing imbalanced classification problems. Despite the
success of this approach in practice, its theoretical foundations remain
underexplored. We develop a theoretical framework to analyze the behavior of
SMOTE and related methods when classifiers are trained on synthetic data. We
first derive a uniform concentration bound on the discrepancy between the
empirical risk over synthetic minority samples and the population risk on the
true minority distribution. We then provide a nonparametric excess risk
guarantee for kernel-based classifiers trained using such synthetic data. These
results lead to practical guidelines for better parameter tuning of both SMOTE
and the downstream learning algorithm. Numerical experiments are provided to
illustrate and support the theoretical findings

</details>


### [130] [Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences](https://arxiv.org/abs/2510.20595)
*Yunyi Shen,Alexander Gagliano*

Main category: stat.ML

TL;DR: 提出DAEP（扩散自编码器与感知器）架构，用于处理不规则、多模态序列数据的自监督表示学习，在科学领域数据上表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 解决科学领域中数据通常以长、不规则、多模态序列形式出现的问题，而现有自监督学习方法主要针对规则采样数据（如图像、音频）设计

Method: 使用感知器编码器对异构测量进行标记化和压缩，通过感知器-IO扩散解码器进行重建，实现可扩展学习

Result: 在天文光谱和测光数据集上，DAEP相比VAE和MAEP基线实现了更低的重建误差、更具区分性的潜在空间和更好的细尺度结构保留

Conclusion: DAEP是处理不规则、异构序列数据的有效框架，特别适用于科学领域

Abstract: Self-supervised learning has become a central strategy for representation
learning, but the majority of architectures used for encoding data have only
been validated on regularly-sampled inputs such as images, audios. and videos.
In many scientific domains, data instead arrive as long, irregular, and
multimodal sequences. To extract semantic information from these data, we
introduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizes
heterogeneous measurements, compresses them with a Perceiver encoder, and
reconstructs them with a Perceiver-IO diffusion decoder, enabling scalable
learning in diverse data settings. To benchmark the daep architecture, we adapt
the masked autoencoder to a Perceiver encoder/decoder design, and establish a
strong baseline (maep) in the same architectural family as daep. Across diverse
spectroscopic and photometric astronomical datasets, daep achieves lower
reconstruction errors, produces more discriminative latent spaces, and better
preserves fine-scale structure than both VAE and maep baselines. These results
establish daep as an effective framework for scientific domains where data
arrives as irregular, heterogeneous sequences.

</details>


### [131] [Finding the Sweet Spot: Trading Quality, Cost, and Speed During Inference-Time LLM Reflection](https://arxiv.org/abs/2510.20653)
*Jack Butler,Nikita Kozodoi,Zainab Afolabi,Brian Tyacke,Gaiar Baimuratov*

Main category: stat.ML

TL;DR: 本文系统比较了自反思和预算调优在数学推理和翻译任务中的表现，发现在不同领域自反思效果差异显著，数学推理中性能提升可达220%。研究为在特定领域和资源约束下选择最优推理策略提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，从业者面临更多无需重新训练模型即可提升推理时性能的方法选择，但这些方法在准确性、成本和延迟之间产生了复杂的权衡关系，且在不同领域中的表现尚未得到充分理解。

Method: 系统比较自反思和预算调优在数学推理和翻译任务中的表现，评估包括Anthropic Claude、Amazon Nova和Mistral等主流LLMs，在不同反思深度和计算预算下推导帕累托最优性能边界。

Result: 发现自反思效果存在显著的领域依赖性，数学推理中性能提升可达220%。在Zalando Lounge部署的自反思增强营销内容本地化系统显示市场依赖的有效性。

Conclusion: 研究强调了在部署这些技术时进行领域特定评估的重要性，并为在特定领域和资源约束下选择最优推理策略提供了可操作的指导。

Abstract: As Large Language Models (LLMs) continue to evolve, practitioners face
increasing options for enhancing inference-time performance without model
retraining, including budget tuning and multi-step techniques like
self-reflection. While these methods improve output quality, they create
complex trade-offs among accuracy, cost, and latency that remain poorly
understood across different domains. This paper systematically compares
self-reflection and budget tuning across mathematical reasoning and translation
tasks. We evaluate prominent LLMs, including Anthropic Claude, Amazon Nova, and
Mistral families, along with other models under varying reflection depths and
compute budgets to derive Pareto optimal performance frontiers. Our analysis
reveals substantial domain dependent variation in self-reflection
effectiveness, with performance gains up to 220\% in mathematical reasoning. We
further investigate how reflection round depth and feedback mechanism quality
influence performance across model families. To validate our findings in a
real-world setting, we deploy a self-reflection enhanced marketing content
localisation system at Lounge by Zalando, where it shows market-dependent
effectiveness, reinforcing the importance of domain specific evaluation when
deploying these techniques. Our results provide actionable guidance for
selecting optimal inference strategies given specific domains and resource
constraints. We open source our self-reflection implementation for
reproducibility at
https://github.com/aws-samples/sample-genai-reflection-for-bedrock.

</details>
