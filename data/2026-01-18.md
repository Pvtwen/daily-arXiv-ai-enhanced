<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [cs.LG](#cs.LG) [Total: 68]
- [stat.ML](#stat.ML) [Total: 5]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Distributed Hypothesis Testing Under A Covertness Constraint](https://arxiv.org/abs/2601.09837)
*Ismaila Salihou Adamou,Michèle Wigger*

Main category: eess.SP

TL;DR: 研究分布式假设检验中的隐蔽性约束问题，在部分连接和全连接离散无记忆信道下分析Stein指数性能，提出无需共享密钥的编码方案


<details>
  <summary>Details</summary>
Motivation: 研究分布式假设检验中的隐蔽通信问题，特别是在非警觉情况下，要求外部监视者无法检测传感器与决策中心之间是否存在通信。传统隐蔽通信通常需要共享密钥，本文旨在探索无需共享密钥的方案。

Method: 针对部分连接离散无记忆信道，推导了Stein指数的闭式表达式；针对全连接信道，提出了可实现的Stein指数方案。所有编码方案都不需要传感器和决策中心共享密钥，且隐蔽性约束随观测长度n呈（几乎）指数衰减。

Result: 对于部分连接DMC，Stein指数与具体信道转移概率无关，等于Shalaby和Papamarcou的无监视者但传感器可发送k个无噪声比特时的指数，其中k是观测长度n的亚线性函数。对于全连接DMC，提出的Stein指数可以改善决策中心的局部指数。

Conclusion: 本文证明了在分布式假设检验中，无需共享密钥也能实现隐蔽通信，且隐蔽性约束可以指数衰减，这与传统隐蔽通信的典型行为不同。研究结果为隐蔽通信提供了新的理论框架和性能界限。

Abstract: We study distributed hypothesis testing under a covertness constraint in the non-alert situation, which requires that under the null-hypothesis an external warden be unable to detect whether communication between the sensor and the decision center is taking place. We characterize the achievable Stein exponent of this setup when the channel from the sensor to the decision center is a partially-connected discrete memoryless channel (DMC), i.e., when certain output symbols can only be induced by some of the inputs. The Stein-exponent in this case, does not depend on the specific transition law of the DMC and equals Shalaby and Papamarcou's exponent without a warden but where the sensor can send $k$ noise-free bits to the decision center, for $k$ a function that is sublinear in the observation length $n$. For fully-connected DMCs, we propose an achievable Stein-exponent and show that it can improve over the local exponent at the decision center. All our coding schemes do not require that the sensor and decision center share a common secret key, as commonly assumed in covert communication. Moreover, in our schemes the divergence covertness constraint vanishes (almost) exponentially fast in the obervation length $n$, again, an atypical behaviour for covert communication.

</details>


### [2] [Towards Native Intelligence: 6G-LLM Trained with Reinforcement Learning from NDT Feedback](https://arxiv.org/abs/2601.09992)
*Zhuoran Xiao,Tao Tao,Chenhui Ye,Yunbo Hu,Yijia Feng,Tianyu Jiao,Liyu Cai*

Main category: eess.SP

TL;DR: 提出RLDTF训练范式，通过数字孪生反馈强化学习训练6G-LLM，解决传统方法依赖人工标注数据和离线训练无法适应动态网络环境的问题。


<details>
  <summary>Details</summary>
Motivation: 现有6G-LLM构建依赖大规模人工标注数据，在现实场景中难以获取；纯离线训练模型缺乏持续自我改进能力，无法适应无线通信环境的高度动态需求。

Method: 提出RLDTF训练范式：利用网络数字孪生根据编排结果生成奖励信号，采用强化学习动态指导模型优化决策；引入加权令牌机制提高输出准确性。

Result: 综合实验结果表明，该框架在编排准确性和解决方案最优性方面显著优于现有最先进的基线方法。

Conclusion: RLDTF训练范式有效解决了6G-LLM构建中的数据依赖和自适应限制问题，为网络原生智能的实现提供了可行路径。

Abstract: Owing to its comprehensive understanding of upper-layer application requirements and the capabilities of practical communication systems, the 6G-LLM (6G domain large language model) offers a promising pathway toward realizing network native intelligence. Serving as the system orchestrator, the 6G-LLM drives a paradigm shift that fundamentally departs from existing rule-based approaches, which primarily rely on modular, experience-driven optimization. By contrast, the 6G-LLM substantially enhances network flexibility and adaptability. Nevertheless, current efforts to construct 6G-LLMs are constrained by their reliance on large-scale, meticulously curated, human-authored corpora, which are impractical to obtain in real-world scenarios. Moreover, purely offline-trained models lack the capacity for continual self-improvement, limiting their ability to adapt to the highly dynamic requirements of wireless communication environments. To overcome these limitations, we propose a novel training paradigm termed RLDTF (Reinforcement Learning from Digital Twin Feedback) for 6G-LLMs. This framework leverages network digital twins to generate reward signals based on orchestration outcomes, while employing reinforcement learning to guide the model toward optimal decision-making dynamically. Furthermore, we introduce a weighted token mechanism to improve output accuracy. Comprehensive experimental results demonstrate that our proposed framework significantly outperforms state-of-the-art baselines in orchestration accuracy and solution optimality.

</details>


### [3] [Clustering-Based User Selection in Federated Learning: Metadata Exploitation for 3GPP Networks](https://arxiv.org/abs/2601.10013)
*Ce Zheng,Shiyao Ma,Ke Zhang,Chen Sun,Wenqi Zhang*

Main category: eess.SP

TL;DR: 提出基于元数据的联邦学习框架，包含HPPP数据划分模型和聚类用户选择策略，在非IID场景下提升性能、稳定性和收敛速度


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习模拟使用不现实的数据划分，且现有用户选择方法忽略用户间数据相关性，需要更贴近实际部署的解决方案

Method: 1) 基于齐次泊松点过程(HPPP)的数据划分模型，捕捉数据量异质性和用户数据集自然重叠；2) 基于聚类的用户选择策略，利用位置等元数据减少数据相关性，增强标签多样性

Result: 在FMNIST和CIFAR-10数据集上，该框架在非IID场景下显著提升模型性能、稳定性和收敛速度，在IID设置下保持可比性能，尤其在每轮选择用户数较少时优势更明显

Conclusion: 该元数据驱动的联邦学习框架能有效提升实际部署中的性能，为未来标准化提供指导，特别适合用户数据存在相关性的现实场景

Abstract: Federated learning (FL) enables collaborative model training without sharing raw user data, but conventional simulations often rely on unrealistic data partitioning and current user selection methods ignore data correlation among users. To address these challenges, this paper proposes a metadatadriven FL framework. We first introduce a novel data partition model based on a homogeneous Poisson point process (HPPP), capturing both heterogeneity in data quantity and natural overlap among user datasets. Building on this model, we develop a clustering-based user selection strategy that leverages metadata, such as user location, to reduce data correlation and enhance label diversity across training rounds. Extensive experiments on FMNIST and CIFAR-10 demonstrate that the proposed framework improves model performance, stability, and convergence in non-IID scenarios, while maintaining comparable performance under IID settings. Furthermore, the method shows pronounced advantages when the number of selected users per round is small. These findings highlight the framework's potential for enhancing FL performance in realistic deployments and guiding future standardization.

</details>


### [4] [Microwave Linear Analog Computer (MiLAC)-aided Multiuser MISO: Fundamental Limits and Beamforming Design](https://arxiv.org/abs/2601.10060)
*Zheyu Wu,Matteo Nerini,Bruno Clerckx*

Main category: eess.SP

TL;DR: 微波线性模拟计算机（MiLAC）作为实现6G时代超大规模MIMO模拟波束成形的关键技术，在MU-MISO系统中展现出接近数字波束成形的性能，同时硬件复杂度更低。


<details>
  <summary>Details</summary>
Motivation: 随着无线通信系统向6G演进，超大规模MIMO成为关键技术，但面临可扩展性挑战。MiLAC作为一种在模拟域实现波束成形的新方法，有望解决传统数字和模拟波束成形的局限性。

Method: 首先严格表征MiLAC可实现的波束成形矩阵集合，证明其灵活性介于数字波束成形和传统相移器模拟波束成形之间。提出混合数字-MiLAC架构，当RF链数量等于数据流数量时实现数字波束成形灵活性。将MiLAC辅助的和速率最大化问题重新表述为凸线性矩阵不等式，并建立低维子空间特性以降低问题维度，基于WMMSE算法求解。

Result: MiLAC辅助的波束成形在超大规模MIMO系统中性能接近数字波束成形。与混合波束成形相比，通过避免符号级数字处理和支持低分辨率DAC，在硬件和计算复杂度更低的情况下实现相当或更优的性能。

Conclusion: MiLAC是超大规模MIMO系统中一种有前景的波束成形技术，在性能、灵活性和硬件复杂度之间取得了良好平衡，为6G通信系统提供了可行的解决方案。

Abstract: As wireless communication systems evolve toward the 6G era, ultra-massive/gigantic MIMO is envisioned as a key enabling technology. Recently, microwave linear analog computer (MiLAC) has emerged as a promising approach to realize beamforming entirely in the analog domain, thereby alleviating the scalability challenges associated with gigantic MIMO. In this paper, we investigate the fundamental beamforming flexibility and design of lossless and reciprocal MiLAC-aided beamforming for MU-MISO systems. We first provide a rigorous characterization of the set of beamforming matrices achievable by MiLAC. Based on this characterization, we prove that MiLAC-aided beamforming does not generally achieve the full flexibility of digital beamforming, while offering greater flexibility than conventional phase-shifter-based analog beamforming. Furthermore, we propose a hybrid digital-MiLAC architecture and show that it achieves digital beamforming flexibility when the number of radio frequency (RF) chains equals the number of data streams, halving that required by conventional hybrid beamforming. We then formulate the MiLAC-aided sum-rate maximization problem for MU-MISO systems. To solve the problem efficiently, we reformulate the MiLAC-related constraints as a convex linear matrix inequality and establish a low-dimensional subspace property that significantly reduces the problem dimension. Leveraging these results, we propose WMMSE-based algorithms for solving the resulting problem. Simulation results demonstrate that MiLAC-aided beamforming achieves performance close to that of digital beamforming in gigantic MIMO systems. Compared with hybrid beamforming, it achieves comparable or superior performance with lower hardware and computational complexity by avoiding symbol-level digital processing and enabling low-resolution digital-to-analog converters (DACs).

</details>


### [5] [P-norm based Fractional-Order Robust Subband Adaptive Filtering Algorithm for Impulsive Noise and Noisy Input](https://arxiv.org/abs/2601.10074)
*Jianhong Ye,Haiquan Zhao,Yi Peng*

Main category: eess.SP

TL;DR: 提出分数阶NSPN算法，通过分数阶随机梯度下降改进MPE框架，增强在α稳定噪声环境中的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统NSPN算法在α稳定噪声环境（1<α≤2）中表现良好，但在处理α≤1的噪声时性能显著下降，需要改进算法以应对更广泛的噪声环境

Method: 提出分数阶NSPN算法，将分数阶随机梯度下降方法融入MPE框架，分析步长收敛范围、分数阶β的理论值域，建立理论稳态均方偏差模型

Result: 在不同脉冲噪声环境下的仿真实验证实，FoNSPN算法优于现有最先进算法

Conclusion: FoNSPN算法通过分数阶优化有效扩展了NSPN算法的适用范围，在更广泛的α稳定噪声环境中表现出优越的鲁棒性

Abstract: Building upon the mean p-power error (MPE) criterion, the normalized subband p-norm (NSPN) algorithm demonstrates superior robustness in $α$-stable noise environments ($1 < α\leq 2$) through effective utilization of low-order moment hidden in robust loss functions. Nevertheless, its performance degrades significantly when processing noise input or additive noise characterized by $α$-stable processes ($0 < α\leq 1$). To overcome these limitations, we propose a novel fractional-order NSPN (FoNSPN) algorithm that incorporates the fractional-order stochastic gradient descent (FoSGD) method into the MPE framework. Additionally, this paper also analyzes the convergence range of its step-size, the theoretical domain of values for the fractional-order $β$, and establishes the theoretical steady-state mean square deviation (MSD) model. Simulations conducted in diverse impulsive noise environments confirm the superiority of the proposed FoNSPN algorithm against existing state-of-the-art algorithms.

</details>


### [6] [Service Provisioning and Path Planning with Obstacle Avoidance for Low-Altitude Wireless Networks](https://arxiv.org/abs/2601.10179)
*Senning Wan,Bin Li,Hongbin Chen,Lei Liu*

Main category: eess.SP

TL;DR: 该论文研究在异构通信网络中部署无人机作为空中基站，考虑地面障碍物约束，通过联合优化无人机3D轨迹、波束成形和用户关联来最大化用户满意度。


<details>
  <summary>Details</summary>
Motivation: 异构通信网络中，地面障碍物会影响无人机服务覆盖，用户设备有不同数据需求，需要提供个性化服务。同时需要考虑无人机电池耗尽、异构用户和障碍物等约束。

Method: 采用块坐标下降法将问题分解为两个子问题：波束成形子问题使用基于二分法的注水算法；轨迹和关联子问题使用基于近端策略优化的深度强化学习算法学习自适应控制策略。

Result: 仿真结果表明，所提方案在收敛速度和整体系统性能方面优于基线方案，实现了高效的用户关联和准确的障碍物避让。

Conclusion: 该研究提出了一种有效的无人机3D部署方案，能够在复杂障碍物环境下最大化用户满意度，为异构通信网络中的无人机部署提供了实用解决方案。

Abstract: This paper investigates the three-dimensional (3D) deployment of uncrewed aerial vehicles (UAVs) as aerial base stations in heterogeneous communication networks under constraints imposed by diverse ground obstacles. Given the diverse data demands of user equipments (UEs), a user satisfaction model is developed to provide personalized services. In particular, when a UE is located within a ground obstacle, the UAV must approach the obstacle boundary to ensure reliable service quality. Considering constraints such as UAV failures due to battery depletion, heterogeneous UEs, and obstacles, we aim to maximize overall user satisfaction by jointly optimizing the 3D trajectories of UAVs, transmit beamforming vectors, and binary association indicators between UAVs and UEs. To address the complexity and dynamics of the problem, a block coordinate descent method is adopted to decompose it into two subproblems. The beamforming subproblem is efficiently addressed via a bisection-based water-filling algorithm. For the trajectory and association subproblem, we design a deep reinforcement learning algorithm based on proximal policy optimization to learn an adaptive control policy. Simulation results demonstrate that the proposed scheme outperforms baseline schemes in terms of convergence speed and overall system performance. Moreover, it achieves efficient association and accurate obstacle avoidance.

</details>


### [7] [BeamCKMDiff: Beam-Aware Channel Knowledge Map Construction via Diffusion Transformer](https://arxiv.org/abs/2601.10207)
*Le Zhao,Yining Wang,Xinyi Wang,Zesong Fei,Yong Zeng*

Main category: eess.SP

TL;DR: BeamCKMDiff：基于扩散变换器的生成框架，无需站点特定采样即可构建高保真信道知识图，支持任意连续波束赋形向量


<details>
  <summary>Details</summary>
Motivation: 现有CKM构建方法依赖稀疏采样测量，且仅限于全向图或离散码本，限制了波束赋形增益的利用。需要一种能够处理任意连续波束向量且无需站点特定采样的解决方案。

Method: 提出BeamCKMDiff框架，在扩散变换器的噪声预测网络中引入自适应层归一化机制，将连续波束嵌入作为全局控制参数注入，引导生成过程捕捉波束模式与环境几何之间的复杂耦合关系。

Result: 仿真结果表明，BeamCKMDiff显著优于现有基准方法，在主瓣和旁瓣重建精度方面表现出色，实现了高保真CKM构建。

Conclusion: BeamCKMDiff为6G环境感知网络提供了一种无需站点特定采样、支持任意连续波束向量的高效CKM构建方法，能够更好地利用波束赋形增益。

Abstract: Channel knowledge map (CKM) is emerging as a critical enabler for environment-aware 6G networks, offering a site-specific database to significantly reduce pilot overhead. However, existing CKM construction methods typically rely on sparse sampling measurements and are restricted to either omnidirectional maps or discrete codebooks, hindering the exploitation of beamforming gain. To address these limitations, we propose BeamCKMDiff, a generative framework for constructing high-fidelity CKMs conditioned on arbitrary continuous beamforming vectors without site-specific sampling. Specifically, we incorporate a novel adaptive layer normalization (adaLN) mechanism into the noise prediction network of the Diffusion Transformer (DiT). This mechanism injects continuous beam embeddings as {global control parameters}, effectively steering the generative process to capture the complex coupling between beam patterns and environmental geometries. Simulation results demonstrate that BeamCKMDiff significantly outperforms state-of-the-art baselines, achieving superior reconstruction accuracy in capturing main lobes and side lobes.

</details>


### [8] [Sim2Real Deep Transfer for Per-Device CFO Calibration](https://arxiv.org/abs/2601.10264)
*Jingze Zheng,Zhiguo Shi,Shibo He,Chaojie Gu*

Main category: eess.SP

TL;DR: 提出Sim2Real迁移学习框架，通过仿真预训练和轻量级接收器适配，实现跨异构SDR平台的CFO校准，显著提升OFDM系统性能。


<details>
  <summary>Details</summary>
Motivation: OFDM系统中的载波频率偏移估计在异构软件定义无线电平台上因硬件损伤未校准而性能显著下降，现有基于深度神经网络的方法缺乏设备级适配能力，限制了实际部署。

Method: 提出Sim2Real迁移学习框架：1）在包含参数化硬件损伤的合成OFDM信号上预训练骨干DNN；2）仅对回归层使用每个目标设备的少量真实帧进行微调，保持硬件无关知识的同时适配设备特定损伤。

Result: 在三种SDR设备上的实验表明，相比传统CP方法，在室内多径条件下实现了30倍的误码率降低，有效弥合了仿真与现实差距。

Conclusion: 该框架为异构无线系统中稳健的CFO估计提供了成本有效的部署方案，通过仿真预训练和轻量适配实现了设备级校准。

Abstract: Carrier Frequency Offset (CFO) estimation in Orthogonal Frequency Division Multiplexing (OFDM) systems faces significant performance degradation across heterogeneous software-defined radio (SDR) platforms due to uncalibrated hardware impairments. Existing deep neural network (DNN)-based approaches lack device-level adaptation, limiting their practical deployment. This paper proposes a Sim2Real transfer learning framework for per-device CFO calibration, combining simulation-driven pretraining with lightweight receiver adaptation. A backbone DNN is pre-trained on synthetic OFDM signals incorporating parametric hardware distortions (e.g., phase noise, IQ imbalance), enabling generalized feature learning without costly cross-device data collection. Subsequently, only the regression layers are fine-tuned using $1,000$ real frames per target device, preserving hardware-agnostic knowledge while adapting to device-specific impairments. Experiments across three SDR families (USRP B210, USRP N210, HackRF One) achieve $30\times$ BER reduction compared to conventional CP-based methods under indoor multipath conditions. The framework bridges the simulation-to-reality gap for robust CFO estimation, enabling cost-effective deployment in heterogeneous wireless systems.

</details>


### [9] [Low-Complexity Blind Estimator of SNR and MSE for mmWave Multi-Antenna Communications](https://arxiv.org/abs/2601.10331)
*Hanyoung Park,Ji-Woong Choi*

Main category: eess.SP

TL;DR: 提出了一种用于毫米波通信的盲估计器，无需导频信号即可估计噪声功率、信号功率、SNR和MSE，计算复杂度低


<details>
  <summary>Details</summary>
Motivation: 毫米波信道快速变化导致传统基于导频的估计方法精度下降，现有盲估计算法计算复杂度高，难以用于实时服务

Method: 利用毫米波信道在波束空间的稀疏性，使信号和噪声功率分量更容易区分，设计无需真实信号先验知识的低复杂度盲估计算法

Result: 提出了无需导频信号即可估计平均噪声功率、信号功率、SNR和MSE的盲估计器，计算复杂度显著降低

Conclusion: 该方法解决了毫米波通信中信道估计的实时性和计算复杂度问题，提高了系统对动态环境变化的适应能力

Abstract: To enhance the robustness and resilience of wireless communication and meet performance requirements, various environment-reflecting metrics, such as the signal-to-noise ratio (SNR), are utilized as the system parameter. To obtain these metrics, training signals such as pilot sequences are generally employed. However, the rapid fluctuations of the millimeter-wave (mmWave) propagation channel often degrade the accuracy of such estimations. To address this challenge, various blind estimators that operate without pilot have been considered as potential solutions. However, these algorithms often involve a training phase for machine learning or a large number of iterations, which implies prohibitive computational complexity, making them difficult to employ for real-time services and the system less resilient to dynamic environment variation. In this paper, we propose blind estimators for average noise power, signal power, SNR, and mean-square error (MSE) that do not require knowledge of the ground-truth signal or involve high computational complexity. The proposed algorithm leverages the inherent sparsity of mmWave channel in beamspace domain, which makes the signal and noise power components more distinguishable.

</details>


### [10] [Achievable Degrees of Freedom Analysis and Optimization in Massive MIMO via Characteristic Mode Analysis](https://arxiv.org/abs/2601.10576)
*Shaohua Yue,Siyu Miao,Shuhao Zeng,Fenghan Lin,Boya Di*

Main category: eess.SP

TL;DR: 该论文将特征模分析(CMA)引入6G大规模MIMO系统的自由度分析中，同时考虑无线信道和天线激励辐射特性，建立了CMA自由度分析框架，并提出基于CMA的遗传算法优化可重构全息表面天线以提升自由度。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模MIMO自由度分析主要关注无线信道特性，忽略了天线激励和辐射特性的影响。为了更准确地分析MIMO系统可实现的自由度，需要同时考虑信道特性和天线特性。

Method: 1. 使用特征模分析(CMA)建模收发天线的激励和辐射特性；2. 建立基于CMA的自由度分析框架并推导可实现自由度；3. 提出天线特征模优化问题以最大化自由度；4. 针对可重构全息表面(RHS)天线案例，提出基于CMA的遗传算法优化RHS特性。

Result: 通过改变RHS天线的特征模电场和表面电流分布，系统可实现自由度得到提升。全波仿真验证了理论分析，表明基于所提算法的RHS重构能够有效改善系统自由度。

Conclusion: 该研究将CMA引入大规模MIMO自由度分析，建立了同时考虑信道和天线特性的分析框架，并通过优化RHS天线特征模成功提升了系统自由度，为6G通信系统设计提供了新思路。

Abstract: Massive multiple-input multiple-output (MIMO) is esteemed as a critical technology in 6G communications, providing large degrees of freedom (DoF) to improve multiplexing gain. This paper introduces characteristic mode analysis (CMA) to derive the achievable DoF. Unlike existing works primarily focusing on the DoF of the wireless channel,the excitation and radiation properties of antennas are also involved in our DoF analysis, which influences the number of independent data streams for communication of a MIMO system. Specifically, we model the excitation and radiation properties of transceiver antennas using CMA to analyze the excitation and radiation properties of antennas. The CMA-based DoF analysis framework is established and the achievable DoF is derived. A characteristic mode optimization problem of antennas is then formulated to maximize the achievable DoF. A case study where the reconfigurable holographic surface (RHS) antennas are deployed at the transceiver is investigated, and a CMA-based genetic algorithm is later proposed to solve the above problem. By changing the characteristic modes electric field and surface current distribution of RHS, the achievable DoF is enhanced. Full-wave simulation verifies the theoretical analysis on the the achievable DoF and shows that, via the reconfiguration of RHS based on the proposed algorithm, the achievable DoF is improved.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Social Determinants of Health Prediction for ICD-9 Code with Reasoning Models](https://arxiv.org/abs/2601.09709)
*Sharim Khan,Paul Landes,Adam Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: 该研究探索使用推理模型和传统大语言模型在MIMIC-III数据集上进行医院入院多标签社会健康决定因素ICD-9代码分类，利用现有ICD-9代码预测入院情况，达到89% F1分数。


<details>
  <summary>Details</summary>
Motivation: 社会健康决定因素与患者预后相关，但很少在结构化数据中捕获。虽然大语言模型在从句子中识别SDoH标签方面表现良好，但在长入院记录或纵向笔记中预测具有挑战性，因为存在长距离依赖关系。

Method: 在MIMIC-III数据集上使用推理模型和传统大语言模型进行医院入院多标签社会健康决定因素ICD-9代码分类，利用现有ICD-9代码进行预测。

Result: 达到了89%的F1分数，并在139个入院记录中发现了缺失的社会健康决定因素代码。

Conclusion: 该研究展示了利用现有ICD-9代码预测社会健康决定因素的有效性，并提供了可复现结果的代码，同时识别了数据集中缺失的SDoH标注。

Abstract: Social Determinants of Health correlate with patient outcomes but are rarely captured in structured data. Recent attention has been given to automatically extracting these markers from clinical text to supplement diagnostic systems with knowledge of patients' social circumstances. Large language models demonstrate strong performance in identifying Social Determinants of Health labels from sentences. However, prediction in large admissions or longitudinal notes is challenging given long distance dependencies. In this paper, we explore hospital admission multi-label Social Determinants of Health ICD-9 code classification on the MIMIC-III dataset using reasoning models and traditional large language models. We exploit existing ICD-9 codes for prediction on admissions, which achieved an 89% F1. Our contributions include our findings, missing SDoH codes in 139 admissions, and code to reproduce the results.

</details>


### [12] [The Geometry of Thought: Disclosing the Transformer as a Tropical Polynomial Circuit](https://arxiv.org/abs/2601.09775)
*Faruk Alpay,Bilge Senturk*

Main category: cs.LG

TL;DR: Transformer自注意力机制在高温限下等价于热带半环运算，揭示了其前向传播本质上是基于token相似度图的动态规划路径搜索


<details>
  <summary>Details</summary>
Motivation: 探索Transformer自注意力机制在数学上的本质，特别是在高温极限下的行为，以理解其计算过程的几何意义

Method: 通过分析Transformer自注意力在逆温度β→∞时的极限行为，证明softmax注意力转化为热带矩阵乘积，建立与动态规划算法的联系

Result: 证明了Transformer自注意力在高温极限下等价于热带半环运算，前向传播执行Bellman-Ford路径搜索算法，为思维链推理提供几何解释

Conclusion: Transformer的计算过程本质上是基于token相似度图的动态规划路径搜索，这为理解思维链推理提供了新的几何视角

Abstract: We prove that the Transformer self-attention mechanism in the high-confidence regime ($β\to \infty$, where $β$ is an inverse temperature) operates in the tropical semiring (max-plus algebra). In particular, we show that taking the tropical limit of the softmax attention converts it into a tropical matrix product. This reveals that the Transformer's forward pass is effectively executing a dynamic programming recurrence (specifically, a Bellman-Ford path-finding update) on a latent graph defined by token similarities. Our theoretical result provides a new geometric perspective for chain-of-thought reasoning: it emerges from an inherent shortest-path (or longest-path) algorithm being carried out within the network's computation.

</details>


### [13] [TimeSAE: Sparse Decoding for Faithful Explanations of Black-Box Time Series Models](https://arxiv.org/abs/2601.09776)
*Khalid Oublal,Quentin Bouniot,Qi Gan,Stephan Clémençon,Zeynep Akata*

Main category: cs.LG

TL;DR: TimeSAE：基于稀疏自编码器和因果关系的时序黑盒模型解释框架，在分布偏移下提供更忠实和鲁棒的解释


<details>
  <summary>Details</summary>
Motivation: 随着黑盒模型和预训练模型在时间序列应用中的普及，理解其预测变得至关重要，特别是在需要可解释性和信任的高风险领域。现有方法大多只关注分布内解释，无法泛化到训练支持之外，缺乏泛化能力。

Method: 提出TimeSAE框架，结合稀疏自编码器（SAEs）和因果关系双重视角来解释时间序列黑盒模型。通过稀疏自编码器学习时间序列的潜在表示，并利用因果分析提供更鲁棒的解释。

Result: 在合成和真实世界时间序列数据集上的广泛评估表明，TimeSAE相比现有基线方法提供了更忠实和鲁棒的解释，定量指标和定性分析都支持这一结论。

Conclusion: TimeSAE通过稀疏自编码器和因果分析相结合，解决了现有解释方法对分布偏移敏感的问题，为时间序列黑盒模型提供了更可靠和泛化能力更强的解释框架。

Abstract: As black box models and pretrained models gain traction in time series applications, understanding and explaining their predictions becomes increasingly vital, especially in high-stakes domains where interpretability and trust are essential. However, most of the existing methods involve only in-distribution explanation, and do not generalize outside the training support, which requires the learning capability of generalization. In this work, we aim to provide a framework to explain black-box models for time series data through the dual lenses of Sparse Autoencoders (SAEs) and causality. We show that many current explanation methods are sensitive to distributional shifts, limiting their effectiveness in real-world scenarios. Building on the concept of Sparse Autoencoder, we introduce TimeSAE, a framework for black-box model explanation. We conduct extensive evaluations of TimeSAE on both synthetic and real-world time series datasets, comparing it to leading baselines. The results, supported by both quantitative metrics and qualitative insights, show that TimeSAE provides more faithful and robust explanations. Our code is available in an easy-to-use library TimeSAE-Lib: https://anonymous.4open.science/w/TimeSAE-571D/.

</details>


### [14] [Step-by-Step Causality: Transparent Causal Discovery with Multi-Agent Tree-Query and Adversarial Confidence Estimation](https://arxiv.org/abs/2601.10137)
*Ziyi Ding,Chenfei Ye-Hao,Zheyuan Wang,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: Tree-Query：基于树状多专家LLM框架的因果发现方法，通过结构化查询减少错误传播，提供可解释的因果判断和置信度评分


<details>
  <summary>Details</summary>
Motivation: 传统基于约束的因果发现方法（如PC、FCI）存在错误传播问题，而最近的LLM因果预测器往往是不透明、无置信度的黑盒。需要一种既能减少错误传播又能提供可解释判断的方法

Method: 提出Tree-Query框架，将成对因果发现转化为关于后门路径、(不)独立性、潜在混杂和因果方向的短序列查询，采用树状多专家LLM结构，提供稳健感知的置信度评分

Result: 在基于Mooij等人和UCI因果图的数据无关基准测试中，Tree-Query在结构指标上优于直接LLM基线；饮食-体重案例研究展示了混杂因素筛选和稳定高置信度的因果结论

Conclusion: Tree-Query提供了一种原则性方法，从LLM获取数据无关的因果先验知识，可补充下游数据驱动的因果发现

Abstract: Causal discovery aims to recover ``what causes what'', but classical constraint-based methods (e.g., PC, FCI) suffer from error propagation, and recent LLM-based causal oracles often behave as opaque, confidence-free black boxes. This paper introduces Tree-Query, a tree-structured, multi-expert LLM framework that reduces pairwise causal discovery to a short sequence of queries about backdoor paths, (in)dependence, latent confounding, and causal direction, yielding interpretable judgments with robustness-aware confidence scores. Theoretical guarantees are provided for asymptotic identifiability of four pairwise relations. On data-free benchmarks derived from Mooij et al. and UCI causal graphs, Tree-Query improves structural metrics over direct LLM baselines, and a diet--weight case study illustrates confounder screening and stable, high-confidence causal conclusions. Tree-Query thus offers a principled way to obtain data-free causal priors from LLMs that can complement downstream data-driven causal discovery. Code is available at https://anonymous.4open.science/r/Repo-9B3E-4F96.

</details>


### [15] [QFed: Parameter-Compact Quantum-Classical Federated Learning](https://arxiv.org/abs/2601.09809)
*Samar Abdelghani,Soumaya Cherkaoui*

Main category: cs.LG

TL;DR: QFed量子联邦学习框架通过量子辅助减少模型参数77.6%，在保持准确率的同时降低边缘设备计算负担


<details>
  <summary>Details</summary>
Motivation: 医疗、金融、科研等领域需要在保护数据隐私和主权的前提下从分布式数据中提取集体智能。联邦学习虽能实现不共享原始数据的协作建模，但面临统计异构性、系统多样性和复杂模型计算负担等挑战。

Method: 提出QFed量子联邦学习框架，利用量子计算将经典模型参数数量减少多对数因子，从而降低训练开销。在边缘设备网络中实现量子辅助的联邦学习。

Result: 在FashionMNIST数据集上，QFed将VGG-like模型的参数数量减少了77.6%，同时在可扩展环境中保持了与经典方法相当的准确率。

Conclusion: 量子计算与联邦学习结合具有增强边缘设备联邦学习能力的潜力，能够显著降低计算负担同时保持模型性能。

Abstract: Organizations and enterprises across domains such as healthcare, finance, and scientific research are increasingly required to extract collective intelligence from distributed, siloed datasets while adhering to strict privacy, regulatory, and sovereignty requirements. Federated Learning (FL) enables collaborative model building without sharing sensitive raw data, but faces growing challenges posed by statistical heterogeneity, system diversity, and the computational burden from complex models. This study examines the potential of quantum-assisted federated learning, which could cut the number of parameters in classical models by polylogarithmic factors and thus lessen training overhead. Accordingly, we introduce QFed, a quantum-enabled federated learning framework aimed at boosting computational efficiency across edge device networks. We evaluate the proposed framework using the widely adopted FashionMNIST dataset. Experimental results show that QFed achieves a 77.6% reduction in the parameter count of a VGG-like model while maintaining an accuracy comparable to classical approaches in a scalable environment. These results point to the potential of leveraging quantum computing within a federated learning context to strengthen FL capabilities of edge devices.

</details>


### [16] [Reinforcement Learning with Multi-Step Lookahead Information Via Adaptive Batching](https://arxiv.org/abs/2601.10418)
*Nadav Merlis*

Main category: cs.LG

TL;DR: 研究具有多步前瞻信息的表格强化学习问题，提出自适应批处理策略（ABP）来优化利用前瞻信息，并设计了学习最优ABP的遗憾最小化算法。


<details>
  <summary>Details</summary>
Motivation: 在多步前瞻信息（可观察未来ℓ步的状态转移和奖励）的强化学习中，虽然前瞻信息能显著提升价值，但寻找最优策略是NP难的。现有两种启发式方法（固定批处理策略和模型预测控制）存在问题，需要更好的方法来有效利用前瞻信息。

Method: 提出自适应批处理策略（ABP），根据状态自适应地处理前瞻信息。推导了这些策略的最优贝尔曼方程，并设计了乐观的遗憾最小化算法，用于在未知环境中学习最优ABP。

Result: 提出的算法能够学习最优自适应批处理策略，遗憾界在阶数上是最优的，最多只差一个前瞻视野ℓ的因子（通常ℓ是较小的常数）。

Conclusion: 自适应批处理策略为解决多步前瞻强化学习问题提供了有效的解决方案，相比固定批处理和模型预测控制方法具有优势，且可以通过遗憾最小化算法在未知环境中学习到最优策略。

Abstract: We study tabular reinforcement learning problems with multiple steps of lookahead information. Before acting, the learner observes $\ell$ steps of future transition and reward realizations: the exact state the agent would reach and the rewards it would collect under any possible course of action. While it has been shown that such information can drastically boost the value, finding the optimal policy is NP-hard, and it is common to apply one of two tractable heuristics: processing the lookahead in chunks of predefined sizes ('fixed batching policies'), and model predictive control. We first illustrate the problems with these two approaches and propose utilizing the lookahead in adaptive (state-dependent) batches; we refer to such policies as adaptive batching policies (ABPs). We derive the optimal Bellman equations for these strategies and design an optimistic regret-minimizing algorithm that enables learning the optimal ABP when interacting with unknown environments. Our regret bounds are order-optimal up to a potential factor of the lookahead horizon $\ell$, which can usually be considered a small constant.

</details>


### [17] [Eluder dimension: localise it!](https://arxiv.org/abs/2601.09825)
*Alireza Bakhtiari,Alex Ayoub,Samuel Robertson,David Janz,Csaba Szepesvári*

Main category: cs.LG

TL;DR: 论文证明了广义线性模型类的eluder维度下界，表明基于标准eluder维度的分析无法获得一阶遗憾界。为此，作者引入了eluder维度的局部化方法，改进了伯努利多臂赌博机的结果，并首次为有限时域强化学习任务提供了真正的一阶遗憾界。


<details>
  <summary>Details</summary>
Motivation: 标准eluder维度分析无法获得一阶遗憾界，这限制了其在广义线性模型和强化学习中的应用。需要开发新的分析方法来突破这一限制。

Method: 引入了eluder维度的局部化方法，通过局部化处理来改进分析框架，使其能够处理更复杂的模型类别。

Result: 1) 建立了广义线性模型类eluder维度的下界；2) 恢复了伯努利多臂赌博机的经典结果并有所改进；3) 首次为有限时域强化学习任务提供了有界累积回报的一阶遗憾界。

Conclusion: 通过eluder维度的局部化方法，成功突破了标准分析的限制，实现了对广义线性模型和强化学习任务的一阶遗憾界分析，为相关领域提供了新的理论工具。

Abstract: We establish a lower bound on the eluder dimension of generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns.

</details>


### [18] [On the origin of neural scaling laws: from random graphs to natural language](https://arxiv.org/abs/2601.10684)
*Maissam Barkeshli,Alberto Alfarano,Andrey Gromov*

Main category: cs.LG

TL;DR: 该论文研究了神经缩放定律的起源，通过简化的随机游走和语言模型实验，发现缩放定律可以在没有数据幂律结构的情况下出现，并对传统语言建模缩放定律进行了批判性分析。


<details>
  <summary>Details</summary>
Motivation: 缩放定律在现代AI革命中发挥了重要作用，但对其起源仍存在争议。传统观点认为缩放定律源于数据中已有的幂律结构，本文旨在通过简化实验验证缩放定律是否可以在没有这种结构的情况下出现。

Method: 1) 在具有可调复杂度的图上训练transformer预测随机游走（二元语法）；2) 使用逐渐简化的生成语言模型（从4层到1层transformer语言模型，再到语言二元语法）采样序列进行训练；3) 在Erdös-Renyi和Barabási-Albert随机图上进行随机游走训练；4) 使用2层transformer和50个上下文长度重现传统语言建模缩放定律。

Result: 1) 即使在数据相关性中没有幂律结构的情况下，简化设置也能产生神经缩放定律；2) 缩放指数随着语言模型复杂度的简化而单调演化；3) 使用2层transformer和短上下文可以重现多个关键缩放结果；4) 最大更新参数化可能比标准参数化更具参数效率。

Conclusion: 缩放定律可以在没有数据幂律结构的情况下出现，挑战了传统解释。简化实验环境为研究缩放定律提供了可控框架，同时论文对现有拟合方法进行了批判性分析，并提出了替代的计算最优曲线获取方法。

Abstract: Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erdös-Renyi and scale-free Barabási-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.

</details>


### [19] [A New Convergence Analysis of Plug-and-Play Proximal Gradient Descent Under Prior Mismatch](https://arxiv.org/abs/2601.09831)
*Guixian Xu,Jinglai Li,Junqi Tang*

Main category: cs.LG

TL;DR: 本文为PnP-PGD算法在先验不匹配情况下提供了首个收敛性理论证明，移除了现有理论中多个限制性且不可验证的假设。


<details>
  <summary>Details</summary>
Motivation: 现有PnP算法的收敛性理论需要多个限制性且不可验证的假设，特别是在先验不匹配（去噪器训练数据分布与推理任务不同）的情况下缺乏理论保证。

Method: 提出了plug-and-play近端梯度下降（PnP-PGD）在数据分布不匹配情况下的新收敛理论，移除了传统理论中对去噪器性质的严格假设。

Result: 首次证明了PnP-PGD在先验不匹配情况下的收敛性，相比现有理论结果，移除了多个限制性且不可验证的假设条件。

Conclusion: 该工作为PnP算法在实际应用中的理论可靠性提供了重要支撑，特别是在训练数据与推理任务分布不一致的现实场景下。

Abstract: In this work, we provide a new convergence theory for plug-and-play proximal gradient descent (PnP-PGD) under prior mismatch where the denoiser is trained on a different data distribution to the inference task at hand. To the best of our knowledge, this is the first convergence proof of PnP-PGD under prior mismatch. Compared with the existing theoretical results for PnP algorithms, our new results removed the need for several restrictive and unverifiable assumptions.

</details>


### [20] [A pipeline for enabling path-specific causal fairness in observational health data](https://arxiv.org/abs/2601.09841)
*Aparajita Kashyap,Sara Matijevic,Noémie Elhadad,Steven A. Kushner,Shalmali Joshi*

Main category: cs.LG

TL;DR: 提出一个模型无关的管道，用于训练因果公平的机器学习模型，解决医疗保健中的直接和间接偏见问题


<details>
  <summary>Details</summary>
Motivation: 在医疗保健环境中部署机器学习模型时，需要确保模型不会复制或加剧现有的医疗偏见。虽然存在许多公平性定义，但本文关注路径特定的因果公平性，这能更好地考虑偏见发生的社会和医疗背景

Method: 将结构公平模型映射到观察性医疗保健设置中，创建一个通用的管道来训练因果公平模型。该管道明确考虑特定的医疗背景和差异来定义目标"公平"模型

Result: 扩展了"公平性-准确性"权衡的表征，解开了直接和间接偏见的来源，并展示了如何在具有已知社会和医疗差异的任务中利用未经公平约束训练的基础模型生成因果公平的下游预测

Conclusion: 提出了一个模型无关的管道，用于训练因果公平的机器学习模型，解决医疗保健中的直接和间接偏见形式，为医疗AI的公平部署提供了实用框架

Abstract: When training machine learning (ML) models for potential deployment in a healthcare setting, it is essential to ensure that they do not replicate or exacerbate existing healthcare biases. Although many definitions of fairness exist, we focus on path-specific causal fairness, which allows us to better consider the social and medical contexts in which biases occur (e.g., direct discrimination by a clinician or model versus bias due to differential access to the healthcare system) and to characterize how these biases may appear in learned models. In this work, we map the structural fairness model to the observational healthcare setting and create a generalizable pipeline for training causally fair models. The pipeline explicitly considers specific healthcare context and disparities to define a target "fair" model. Our work fills two major gaps: first, we expand on characterizations of the "fairness-accuracy" tradeoff by detangling direct and indirect sources of bias and jointly presenting these fairness considerations alongside considerations of accuracy in the context of broadly known biases. Second, we demonstrate how a foundation model trained without fairness constraints on observational health data can be leveraged to generate causally fair downstream predictions in tasks with known social and medical disparities. This work presents a model-agnostic pipeline for training causally fair machine learning models that address both direct and indirect forms of healthcare bias.

</details>


### [21] [Advancing Model Refinement: Muon-Optimized Distillation and Quantization for LLM Deployment](https://arxiv.org/abs/2601.09865)
*Jacob Sander,Brian Jalaian,Venkat R. Dasari*

Main category: cs.LG

TL;DR: 提出一个集成框架，结合GPTQ量化、LoRA和专门的数据蒸馏过程，显著减小LLM模型大小和复杂度，同时保持或提升任务特定性能，实现高达2倍内存压缩。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在资源受限的边缘设备上部署面临计算、内存和能耗挑战，需要解决三个关键问题：获取任务特定数据、微调性能、压缩模型以加速推理同时减少资源需求。

Method: 集成框架结合GPTQ量化、低秩适应（LoRA）和专门的数据蒸馏过程，利用数据蒸馏、KL散度知识蒸馏、贝叶斯超参数优化和Muon优化器。

Result: 实现高达2倍内存压缩（如6GB模型减至3GB），在标准LLM基准测试中表现优于单独GPTQ量化，Muon优化器显著增强微调模型在量化过程中的抗精度衰减能力。

Conclusion: 提出的集成框架能有效压缩LLM模型，实现高效推理，为资源受限环境下的LLM部署提供了可行解决方案。

Abstract: Large Language Models (LLMs) enable advanced natural language processing but face deployment challenges on resource-constrained edge devices due to high computational, memory, and energy demands. Optimizing these models requires addressing three key challenges: acquiring task-specific data, fine-tuning for performance, and compressing models to accelerate inference while reducing resource demands. We propose an integrated framework combining GPTQ-based quantization, low-rank adaptation (LoRA), and a specialized data distillation process to significantly reduce model size and complexity while preserving or enhancing task-specific performance. By leveraging data distillation, knowledge distillation via Kullback-Leibler divergence, Bayesian hyperparameter optimization, and the Muon optimizer, our pipeline achieves up to 2x memory compression (e.g., reducing a 6GB model to 3GB) and enables efficient inference for specialized tasks. Empirical results demonstrate superior performance on standard LLM benchmarks compared to GPTQ quantization alone, with the Muon optimizer notably enhancing fine-tuned models' resistance to accuracy decay during quantization.

</details>


### [22] [The PROPER Approach to Proactivity: Benchmarking and Advancing Knowledge Gap Navigation](https://arxiv.org/abs/2601.09926)
*Kirandeep Kaur,Vinayak Gupta,Aditya Gupta,Chirag Shah*

Main category: cs.LG

TL;DR: ProPer是一个两智能体架构，通过生成用户未表达的潜在需求维度，实现个性化、主动的AI助手响应，相比传统被动式助手显著提升服务质量。


<details>
  <summary>Details</summary>
Motivation: 现有语言助手主要采用被动问答模式，用户需要明确表达需求，导致许多相关但未表达的需求无法得到满足。现有主动式助手要么需要用户进一步澄清（增加负担），要么从上下文推断未来需求（常导致不必要或时机不当的干预）。

Method: 提出ProPer两智能体架构：1) 维度生成智能体(DGA)：基于微调的大语言模型，利用显式用户数据生成多个隐式维度（用户任务相关但未考虑的潜在方面）或知识缺口；2) 响应生成智能体(RGA)：平衡显式和隐式维度，生成个性化响应并进行及时主动干预。使用基于质量、多样性和任务相关性的重排序器筛选维度。

Result: 在多领域评估中，ProPer在所有领域都提高了质量分数和胜率，单轮评估中最高获得84%的提升，在多轮交互中持续占优。使用结构化、缺口感知的评估标准测量覆盖率、主动性适当性和意图对齐。

Conclusion: ProPer通过两智能体架构有效识别和响应用户未表达的潜在需求，实现了更个性化、主动的AI助手交互，显著优于传统被动式方法。

Abstract: Most language-based assistants follow a reactive ask-and-respond paradigm, requiring users to explicitly state their needs. As a result, relevant but unexpressed needs often go unmet. Existing proactive agents attempt to address this gap either by eliciting further clarification, preserving this burden, or by extrapolating future needs from context, often leading to unnecessary or mistimed interventions. We introduce ProPer, Proactivity-driven Personalized agents, a novel two-agent architecture consisting of a Dimension Generating Agent (DGA) and a Response Generating Agent (RGA). DGA, a fine-tuned LLM agent, leverages explicit user data to generate multiple implicit dimensions (latent aspects relevant to the user's task but not considered by the user) or knowledge gaps. These dimensions are selectively filtered using a reranker based on quality, diversity, and task relevance. RGA then balances explicit and implicit dimensions to tailor personalized responses with timely and proactive interventions. We evaluate ProPer across multiple domains using a structured, gap-aware rubric that measures coverage, initiative appropriateness, and intent alignment. Our results show that ProPer improves quality scores and win rates across all domains, achieving up to 84% gains in single-turn evaluation and consistent dominance in multi-turn interactions.

</details>


### [23] [Interpolation-Based Optimization for Enforcing lp-Norm Metric Differential Privacy in Continuous and Fine-Grained Domains](https://arxiv.org/abs/2601.09946)
*Chenxi Qiu*

Main category: cs.LG

TL;DR: 提出基于插值的框架优化lp范数度量差分隐私，通过锚点优化和log凸组合插值，在细粒度/连续域中实现高效隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有基于优化的方法在粗粒度域中能有效减少效用损失，但在细粒度或连续设置中优化度量差分隐私仍具挑战性，主要因为构建密集扰动矩阵的计算成本和满足逐点约束的困难。

Method: 提出插值框架：1) 在稀疏锚点集优化扰动分布；2) 通过log凸组合插值非锚点位置的分布；3) 针对高维空间中的隐私违规，将插值过程分解为一维步骤序列；4) 推导修正公式确保lp范数mDP；5) 探索扰动分布和隐私预算跨维度分配的联合优化。

Result: 在真实世界位置数据集上的实验表明，该方法在细粒度域中提供严格的隐私保证和具有竞争力的效用，优于基线机制。

Conclusion: 提出的插值框架成功解决了细粒度和连续域中优化度量差分隐私的挑战，通过锚点优化和log凸组合插值实现了高效且可证明的隐私保护，在高维空间中通过分解和修正公式确保了隐私约束。

Abstract: Metric Differential Privacy (mDP) generalizes Local Differential Privacy (LDP) by adapting privacy guarantees based on pairwise distances, enabling context-aware protection and improved utility. While existing optimization-based methods reduce utility loss effectively in coarse-grained domains, optimizing mDP in fine-grained or continuous settings remains challenging due to the computational cost of constructing dense perterubation matrices and satisfying pointwise constraints.
  In this paper, we propose an interpolation-based framework for optimizing lp-norm mDP in such domains. Our approach optimizes perturbation distributions at a sparse set of anchor points and interpolates distributions at non-anchor locations via log-convex combinations, which provably preserve mDP. To address privacy violations caused by naive interpolation in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms. in high-dimensional spaces, we decompose the interpolation process into a sequence of one-dimensional steps and derive a corrected formulation that enforces lp-norm mDP by design. We further explore joint optimization over perturbation distributions and privacy budget allocation across dimensions. Experiments on real-world location datasets demonstrate that our method offers rigorous privacy guarantees and competitive utility in fine-grained domains, outperforming baseline mechanisms.

</details>


### [24] [Kinematic Tokenization: Optimization-Based Continuous-Time Tokens for Learnable Decision Policies in Noisy Time Series](https://arxiv.org/abs/2601.09949)
*Griffin Kearney*

Main category: cs.LG

TL;DR: 提出Kinematic Tokenization方法，将连续时间信号转换为样条系数token，在金融时间序列中相比离散token化方法能维持更稳定的策略学习


<details>
  <summary>Details</summary>
Motivation: Transformer设计用于离散token，但现实世界信号是连续过程且存在噪声采样。在低信噪比环境下，离散token化方法（原始值、分块、有限差分）容易失效，特别是当下游目标施加非对称惩罚时

Method: 提出Kinematic Tokenization方法：基于优化的连续时间表示，从噪声测量中重建显式样条，并将局部样条系数（位置、速度、加速度、急动度）作为token。应用于金融时间序列数据（资产价格和交易量）

Result: 在多资产日频股票测试中，使用风险厌恶的非对称分类目标作为可学习性压力测试。在此目标下，多个离散基线方法崩溃为吸收性现金策略（清算均衡），而连续样条token能维持校准的非平凡动作分布和稳定策略

Conclusion: 显式连续时间token可以改善在噪声时间序列中，在诱导弃权的损失函数下选择性决策策略的可学习性和校准性

Abstract: Transformers are designed for discrete tokens, yet many real-world signals are continuous processes observed through noisy sampling. Discrete tokenizations (raw values, patches, finite differences) can be brittle in low signal-to-noise regimes, especially when downstream objectives impose asymmetric penalties that rationally encourage abstention. We introduce Kinematic Tokenization, an optimization-based continuous-time representation that reconstructs an explicit spline from noisy measurements and tokenizes local spline coefficients (position, velocity, acceleration, jerk). This is applied to financial time series data in the form of asset prices in conjunction with trading volume profiles. Across a multi-asset daily-equity testbed, we use a risk-averse asymmetric classification objective as a stress test for learnability. Under this objective, several discrete baselines collapse to an absorbing cash policy (the Liquidation Equilibrium), whereas the continuous spline tokens sustain calibrated, non-trivial action distributions and stable policies. These results suggest that explicit continuous-time tokens can improve the learnability and calibration of selective decision policies in noisy time series under abstention-inducing losses.

</details>


### [25] [A Sustainable AI Economy Needs Data Deals That Work for Generators](https://arxiv.org/abs/2601.09966)
*Ruoxi Jia,Luis Oala,Wenjie Xiong,Suqin Ge,Jiachen T. Wang,Feiyang Kang,Dawn Song*

Main category: cs.LG

TL;DR: 机器学习价值链存在结构性不平等：数据生成者获得的价值几乎为零，大部分价值流向聚合商，威胁到AI发展的可持续性。


<details>
  <summary>Details</summary>
Motivation: 论文指出机器学习价值链存在结构性不可持续问题，存在"经济数据处理不平等"现象：数据从输入到模型权重再到合成输出的每个阶段都增强了技术信号，但剥夺了数据生成者的经济权益。这不仅是一个经济福利问题，随着数据及其衍生物成为经济资产，维持当前学习算法的反馈循环面临风险。

Method: 分析了73个公开数据交易案例，发现大多数价值流向聚合商，创作者版税几乎为零，交易条款普遍不透明。识别了三个结构性缺陷：缺失溯源、不对称议价能力和非动态定价。提出了"公平数据价值交换（EDVEX）框架"来建立惠及所有参与者的最小市场。

Result: 研究显示数据交易中存在严重不平等：创作者版税四舍五入后为零，交易条款普遍不透明，大部分价值被聚合商获取。这威胁到机器学习生态系统的可持续性。

Conclusion: 需要建立更公平的数据价值交换机制来确保机器学习生态系统的可持续性。提出了EDVEX框架作为解决方案，并指出了社区可以做出具体贡献的研究方向，包括数据交易机制设计和相关法律政策研究。

Abstract: We argue that the machine learning value chain is structurally unsustainable due to an economic data processing inequality: each state in the data cycle from inputs to model weights to synthetic outputs refines technical signal but strips economic equity from data generators. We show, by analyzing seventy-three public data deals, that the majority of value accrues to aggregators, with documented creator royalties rounding to zero and widespread opacity of deal terms. This is not just an economic welfare concern: as data and its derivatives become economic assets, the feedback loop that sustains current learning algorithms is at risk. We identify three structural faults - missing provenance, asymmetric bargaining power, and non-dynamic pricing - as the operational machinery of this inequality. In our analysis, we trace these problems along the machine learning value chain and propose an Equitable Data-Value Exchange (EDVEX) Framework to enable a minimal market that benefits all participants. Finally, we outline research directions where our community can make concrete contributions to data deals and contextualize our position with related and orthogonal viewpoints.

</details>


### [26] [An Exploratory Study to Repurpose LLMs to a Unified Architecture for Time Series Classification](https://arxiv.org/abs/2601.09971)
*Hansen He,Shuheng Li*

Main category: cs.LG

TL;DR: 该研究探索了将专门的时间序列编码器与冻结的大型语言模型（LLM）主干结合的混合架构，发现Inception模型是唯一能持续带来性能提升的编码器架构。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类（TSC）是机器学习核心问题，现有研究主要关注将时间序列数据映射到文本域的校准策略，但时间序列编码器架构的选择尚未充分探索。研究者希望探索专门的时间序列编码器与LLM结合的混合架构效果。

Method: 采用探索性研究方法，评估多种时间序列编码器家族（包括Inception、卷积、残差、基于Transformer和多层感知机架构）与冻结LLM主干结合的混合架构。通过实验比较不同编码器在时间序列分类任务中的表现。

Result: 在所有评估的编码器架构中，只有Inception模型在与LLM主干集成时能持续产生正向性能提升。其他编码器架构（卷积、残差、Transformer等）未能表现出类似的稳定优势。

Conclusion: 时间序列编码器的选择对混合LLM架构有重要影响，基于Inception的模型是未来LLM驱动时间序列学习的有前景方向。这项研究强调了专门时间序列编码器在LLM集成中的关键作用。

Abstract: Time series classification (TSC) is a core machine learning problem with broad applications. Recently there has been growing interest in repurposing large language models (LLMs) for TSC, motivated by their strong reasoning and generalization ability. Prior work has primarily focused on alignment strategies that explicitly map time series data into the textual domain; however, the choice of time series encoder architecture remains underexplored. In this work, we conduct an exploratory study of hybrid architectures that combine specialized time series encoders with a frozen LLM backbone. We evaluate a diverse set of encoder families, including Inception, convolutional, residual, transformer-based, and multilayer perceptron architectures, among which the Inception model is the only encoder architecture that consistently yields positive performance gains when integrated with an LLM backbone. Overall, this study highlights the impact of time series encoder choice in hybrid LLM architectures and points to Inception-based models as a promising direction for future LLM-driven time series learning.

</details>


### [27] [In-Context Operator Learning on the Space of Probability Measures](https://arxiv.org/abs/2601.09979)
*Frank Cole,Dixi Wang,Yineng Chen,Yulong Lu,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出一种基于概率测度空间的上下文算子学习方法，用于最优传输问题，通过少量样本作为提示学习映射分布对到OT映射的算子，无需推理时的梯度更新。


<details>
  <summary>Details</summary>
Motivation: 传统最优传输方法通常需要为每个新的分布对重新计算，计算成本高。本文旨在学习一个通用的解决方案算子，能够通过少量上下文样本快速适应新任务，实现高效的最优传输计算。

Method: 提出上下文算子学习框架，将分布对映射到最优传输映射。在非参数设置下，当任务集中在低内在维度的源-目标对流形时，建立泛化界限；在参数设置下（如高斯族），给出显式架构恢复精确OT映射，并提供有限样本超额风险界限。

Result: 理论分析表明，在非参数设置下，上下文准确率随提示大小、内在任务维度和模型容量的扩展规律；在参数设置下，能够精确恢复OT映射。合成传输和生成建模基准实验验证了该框架的有效性。

Conclusion: 提出的上下文算子学习方法为最优传输问题提供了一种高效、灵活的解决方案，能够通过少量样本快速适应新任务，在理论和实验上都证明了其有效性，为概率测度空间上的算子学习开辟了新方向。

Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.

</details>


### [28] [FaTRQ: Tiered Residual Quantization for LLM Vector Search in Far-Memory-Aware ANNS Systems](https://arxiv.org/abs/2601.09985)
*Tianqi Zhang,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FaTRQ是一个针对近似最近邻搜索的远内存感知精炼系统，通过分层内存和渐进距离估计器消除从存储中获取完整向量的需求，显著提升存储效率和查询吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现代ANNS引擎虽然使用预建索引和压缩向量量化表示加速搜索，但仍依赖昂贵的二次精炼阶段从慢速存储（如SSD）读取完整精度向量。对于现代文本和多模态嵌入，这些读取操作现在主导了整个查询的延迟。

Method: 1) 提出渐进距离估计器，使用从远内存流式传输的紧凑残差来精炼粗略分数；2) 引入分层残差量化，将残差编码为三元值存储在远内存中；3) 在CXL Type-2设备中部署定制加速器执行低延迟本地精炼。

Result: FaTRQ将存储效率提高了2.4倍，相比最先进的GPU ANNS系统，吞吐量提升了高达9倍。

Conclusion: FaTRQ通过消除从存储中获取完整向量的需求，解决了ANNS中二次精炼阶段的瓶颈问题，显著提升了检索增强生成系统的性能和效率。

Abstract: Approximate Nearest-Neighbor Search (ANNS) is a key technique in retrieval-augmented generation (RAG), enabling rapid identification of the most relevant high-dimensional embeddings from massive vector databases. Modern ANNS engines accelerate this process using prebuilt indexes and store compressed vector-quantized representations in fast memory. However, they still rely on a costly second-pass refinement stage that reads full-precision vectors from slower storage like SSDs. For modern text and multimodal embeddings, these reads now dominate the latency of the entire query. We propose FaTRQ, a far-memory-aware refinement system using tiered memory that eliminates the need to fetch full vectors from storage. It introduces a progressive distance estimator that refines coarse scores using compact residuals streamed from far memory. Refinement stops early once a candidate is provably outside the top-k. To support this, we propose tiered residual quantization, which encodes residuals as ternary values stored efficiently in far memory. A custom accelerator is deployed in a CXL Type-2 device to perform low-latency refinement locally. Together, FaTRQ improves the storage efficiency by 2.4$\times$ and improves the throughput by up to 9$ \times$ than SOTA GPU ANNS system.

</details>


### [29] [Continuous-Depth Transformers with Learned Control Dynamics](https://arxiv.org/abs/2601.10007)
*Peter Jemley*

Main category: cs.LG

TL;DR: 提出一种混合Transformer架构，用连续深度神经ODE块替代离散中间层，通过学习的控制信号在推理时控制生成属性


<details>
  <summary>Details</summary>
Motivation: 标准Transformer通过固定离散层处理表示，缺乏推理时对生成属性的灵活控制能力。需要一种能够将深度作为连续变量处理，并通过控制信号实现可操控生成的架构

Method: 设计混合Transformer架构，用连续深度神经ODE块替换离散中间层。使用学习向量场F_θ(H, τ, u)，其中u是通过显式拼接注入的低维控制信号。采用伴随方法实现O(1)内存训练

Result: 梯度流稳定无爆炸/消失；情感控制准确率达98%/88%；连续插值轨迹差异仅0.068%；延迟与标准基线相当；自适应ODE求解器揭示学习动态的几何结构

Conclusion: 带有学习控制信号的连续深度动态为可操控语言生成提供了可行、高效的机制，实现了推理时对生成属性的灵活控制

Abstract: We present a hybrid transformer architecture that replaces discrete middle layers with a continuous-depth Neural Ordinary Differential Equation (ODE) block, enabling inference-time control over generation attributes via a learned steering signal. Unlike standard transformers that process representations through fixed discrete layers, our approach treats depth as a continuous variable governed by a learned vector field $F_θ(H, τ, u)$, where $u$ is a low-dimensional control signal injected via explicit concatenation. We validate the architecture through four experiments: (1) gradient flow stability with zero exploding/vanishing gradient events, (2) semantic steering achieving 98\%/88\% accuracy for positive/negative sentiment control, (3) continuous interpolation validated by a negligible 0.068\% trajectory divergence between fixed and adaptive solvers, and (4) efficiency benchmarking demonstrating latency parity with standard discrete baselines. Additionally, we show that adaptive ODE solvers reveal geometric structure in the learned dynamics: the control signal partitions the vector field into distinct dynamical regimes with different curvature characteristics. The adjoint method enables $O(1)$ memory training regardless of integration depth. Our results demonstrate that continuous-depth dynamics with learned control signals provide a viable, efficient mechanism for steerable language generation.

</details>


### [30] [PID-Guided Partial Alignment for Multimodal Decentralized Federated Learning](https://arxiv.org/abs/2601.10012)
*Yanhang Shi,Xiaoyu Wang,Houwei Cao,Jian Li,Yong Liu*

Main category: cs.LG

TL;DR: PARSE：一种基于部分信息分解的多模态去中心化联邦学习框架，通过特征分裂将表示分解为冗余、独特和协同切片，实现异构代理间的部分对齐知识共享，解决多模态DFL中的梯度冲突问题。


<details>
  <summary>Details</summary>
Motivation: 多模态去中心化联邦学习面临挑战：代理在可用模态和模型架构上存在差异，需要在无中心协调器的P2P网络中协作。传统的多模态方法学习所有模态的单一共享嵌入，在DFL中会导致单模态与多模态代理间的梯度错位，抑制异构共享和跨模态交互。

Method: PARSE框架基于部分信息分解（PID），每个代理执行特征分裂，将其潜在表示分解为冗余、独特和协同切片。通过切片级部分对齐实现异构代理间的P2P知识共享：仅交换具有对应模态的语义可共享分支。无需中央协调或梯度手术，解决单/多模态梯度冲突。

Result: 在多个基准测试和代理混合配置中，PARSE相比任务、模态和混合共享的DFL基线方法均取得一致性能提升。对融合操作和分割比例的消融研究，以及定性可视化结果，进一步证明了该设计的效率和鲁棒性。

Conclusion: PARSE通过部分信息分解和切片级部分对齐，有效解决了多模态去中心化联邦学习中的梯度冲突问题，在保持与标准DFL约束兼容的同时，克服了多模态DFL困境，实现了异构代理间的高效知识共享。

Abstract: Multimodal decentralized federated learning (DFL) is challenging because agents differ in available modalities and model architectures, yet must collaborate over peer-to-peer (P2P) networks without a central coordinator. Standard multimodal pipelines learn a single shared embedding across all modalities. In DFL, such a monolithic representation induces gradient misalignment between uni- and multimodal agents; as a result, it suppresses heterogeneous sharing and cross-modal interaction. We present PARSE, a multimodal DFL framework that operationalizes partial information decomposition (PID) in a server-free setting. Each agent performs feature fission to factorize its latent representation into redundant, unique, and synergistic slices. P2P knowledge sharing among heterogeneous agents is enabled by slice-level partial alignment: only semantically shareable branches are exchanged among agents that possess the corresponding modality. By removing the need for central coordination and gradient surgery, PARSE resolves uni-/multimodal gradient conflicts, thereby overcoming the multimodal DFL dilemma while remaining compatible with standard DFL constraints. Across benchmarks and agent mixes, PARSE yields consistent gains over task-, modality-, and hybrid-sharing DFL baselines. Ablations on fusion operators and split ratios, together with qualitative visualizations, further demonstrate the efficiency and robustness of the proposed design.

</details>


### [31] [CAFEDistill: Learning Personalized and Dynamic Models through Federated Early-Exit Network Distillation](https://arxiv.org/abs/2601.10015)
*Boyi Liu,Zimu Zhou,Yongxin Tong*

Main category: cs.LG

TL;DR: CAFEDistill是一个冲突感知的联邦退出蒸馏框架，将早期退出网络与个性化联邦学习结合，通过深度优先的学生协调机制解决客户端异构性和深度干扰问题，实现自适应推理。


<details>
  <summary>Details</summary>
Motivation: 现有PFL方法产生静态模型，在准确性和效率之间固定权衡，无法适应不同上下文和资源可用性的推理需求。早期退出网络提供自适应推理，但整合到PFL中面临客户端异构性和深度干扰的双重挑战。

Method: 提出CAFEDistill框架，采用渐进式深度优先的学生协调机制，缓解浅层和深层退出之间的干扰，同时实现跨客户端的个性化知识转移。通过客户端解耦公式减少通信开销。

Result: CAFEDistill在广泛评估中优于现有方法，实现更高准确性，并将推理成本降低30.79%-46.86%。

Conclusion: CAFEDistill成功解决了PFL与早期退出网络整合中的双重冲突问题，为异构环境下自适应推理提供了有效解决方案，显著提升了性能并降低了推理成本。

Abstract: Personalized Federated Learning (PFL) enables collaboratively model training on decentralized, heterogeneous data while tailoring them to each client's unique distribution. However, existing PFL methods produce static models with a fixed tradeoff between accuracy and efficiency, limiting their applicability in environments where inference requirements vary with contexts and resource availability. Early-exit networks (EENs) offer adaptive inference by attaching intermediate classifiers. Yet integrating them into PFL is challenging due to client-wise heterogeneity and depth-wise interference arising from conflicting exit objectives. Prior studies fail to resolve both conflicts simultaneously, leading to suboptimal performance. In this paper, we propose CAFEDistill, a Conflict-Aware Federated Exit Distillation framework that jointly addresses these conflicts and extends PFL to early-exit networks. Through a progressive, depth-prioritized student coordination mechanism, CAFEDistill mitigates interference among shallow and deep exits while allowing effective personalized knowledge transfer across clients. Furthermore, it reduces communication overhead via a client-decoupled formulation. Extensive evaluations show that CAFEDistill outperforms the state-of-the-arts, achieving higher accuracy and reducing inference costs by 30.79%-46.86%.

</details>


### [32] [Time Aggregation Features for XGBoost Models](https://arxiv.org/abs/2601.10019)
*Mykola Pinchuk*

Main category: cs.LG

TL;DR: 论文研究XGBoost模型在点击率预测中的时间聚合特征，发现简单的时间窗口特征比复杂的时间聚合设计效果更好


<details>
  <summary>Details</summary>
Motivation: 研究在严格时间分割和无前瞻特征约束下，不同时间聚合特征对点击率预测模型性能的影响，寻找最优的时间特征设计方法

Method: 使用Avazu点击率预测数据集，采用严格的时间外分割和无前瞻特征约束。比较了时间感知的目标编码基线模型与添加实体历史时间聚合特征的模型，测试了多种窗口设计（包括trailing windows、event count windows、gap windows和bucketized windows）

Result: 在确定性10%样本的两个滚动尾部折叠上，trailing window相比单纯的目标编码将ROC AUC提高了约0.0066-0.0082，PR AUC提高了约0.0084-0.0094。在时间聚合设计网格中，只有event count windows能持续改进trailing windows，但增益很小。Gap windows和bucketized windows表现不如简单的trailing windows

Conclusion: 建议将trailing windows作为实用默认选择，当边际ROC AUC增益重要时可选加event count window。复杂的时间聚合设计在此数据集和协议下表现不如简单方法

Abstract: This paper studies time aggregation features for XGBoost models in click-through rate prediction. The setting is the Avazu click-through rate prediction dataset with strict out-of-time splits and a no-lookahead feature constraint. Features for hour H use only impressions from hours strictly before H. This paper compares a strong time-aware target encoding baseline to models augmented with entity history time aggregation under several window designs. Across two rolling-tail folds on a deterministic ten percent sample, a trailing window specification improves ROC AUC by about 0.0066 to 0.0082 and PR AUC by about 0.0084 to 0.0094 relative to target encoding alone. Within the time aggregation design grid, event count windows provide the only consistent improvement over trailing windows, and the gain is small. Gap windows and bucketized windows underperform simple trailing windows in this dataset and protocol. These results support a practical default of trailing windows, with an optional event count window when marginal ROC AUC gains matter.

</details>


### [33] [BPE: Behavioral Profiling Ensemble](https://arxiv.org/abs/2601.10024)
*Yanxin Liu,Yunqi Zhang*

Main category: cs.LG

TL;DR: BPE框架通过为每个模型构建"行为档案"，基于测试实例响应与档案的偏差来分配集成权重，相比传统方法在预测精度、计算效率和存储资源方面都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统集成方法（静态和动态）主要依赖模型间的差异性进行集成，忽视了模型自身的内在特性，且严重依赖验证集进行能力估计。需要一种更本质、更高效的集成方法。

Method: 提出行为档案集成（BPE）框架：为每个模型构建内在的"行为档案"，然后根据模型对特定测试实例的响应与其已建立行为档案之间的偏差来推导集成权重。

Result: 在合成和真实数据集上的广泛实验表明，BPE框架算法相比最先进的集成基线方法，在预测精度、计算效率和存储资源利用方面都取得了显著改进。

Conclusion: BPE框架通过关注模型内在行为特性而非模型间差异，提供了一种新颖的集成范式，在多个维度上超越了传统集成方法。

Abstract: Ensemble learning is widely recognized as a pivotal strategy for pushing the boundaries of predictive performance. Traditional static ensemble methods, such as Stacking, typically assign weights by treating each base learner as a holistic entity, thereby overlooking the fact that individual models exhibit varying degrees of competence across different regions of the instance space. To address this limitation, Dynamic Ensemble Selection (DES) was introduced. However, both static and dynamic approaches predominantly rely on the divergence among different models as the basis for integration. This inter-model perspective neglects the intrinsic characteristics of the models themselves and necessitates a heavy reliance on validation sets for competence estimation. In this paper, we propose the Behavioral Profiling Ensemble (BPE) framework, which introduces a novel paradigm shift. Unlike traditional methods, BPE constructs a ``behavioral profile'' intrinsic to each model and derives integration weights based on the deviation between the model's response to a specific test instance and its established behavioral profile. Extensive experiments on both synthetic and real-world datasets demonstrate that the algorithm derived from the BPE framework achieves significant improvements over state-of-the-art ensemble baselines. These gains are evident not only in predictive accuracy but also in computational efficiency and storage resource utilization across various scenarios.

</details>


### [34] [Unlabeled Data Can Provably Enhance In-Context Learning of Transformers](https://arxiv.org/abs/2601.10058)
*Renpu Liu,Jing Yang*

Main category: cs.LG

TL;DR: 提出增强型上下文学习框架，将少量标注样本与大量未标注数据结合，通过CoT提示使Transformer隐式执行EM算法，提升分类性能


<details>
  <summary>Details</summary>
Motivation: 传统上下文学习受限于提示中能容纳的少量标注样本，而现实中存在大量未标注数据。如何利用这些未标注数据提升上下文学习性能成为一个重要问题

Method: 提出增强型上下文学习框架，提示中包含少量标注样本和大量未标注输入。在多类线性分类场景下，通过链式思维提示使多层Transformer隐式执行期望最大化算法，从标注和未标注数据中提取有用信息

Result: 理论分析表明该框架能提升上下文学习准确性，且Transformer可通过教师强制训练，参数以线性速率收敛到期望解。实验验证增强型框架持续优于传统少样本上下文学习

Conclusion: 这是首个关于未标注数据对Transformer上下文学习性能影响的理论研究，证明了结合未标注数据的增强型框架能显著提升性能，为利用大规模未标注数据提供了理论基础

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation-maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers.

</details>


### [35] [Efficient Content-based Recommendation Model Training via Noise-aware Coreset Selection](https://arxiv.org/abs/2601.10067)
*Hung Vinh Tran,Tong Chen,Hechuan Wen,Quoc Viet Hung Nguyen,Bin Cui,Hongzhi Yin*

Main category: cs.LG

TL;DR: 提出NaCS框架，通过基于梯度的子模优化构建核心集，同时校正噪声标签，并通过不确定性量化过滤低置信度样本，显著提升基于内容的推荐系统训练效率


<details>
  <summary>Details</summary>
Motivation: 基于内容的推荐系统需要大规模甚至连续训练以适应多样用户偏好，计算成本高。现有核心集选择方法在最小化数据集时容易受到用户-物品交互噪声的影响

Method: 提出噪声感知核心集选择框架：1）基于训练梯度的子模优化构建核心集；2）使用渐进训练模型校正噪声标签；3）通过不确定性量化过滤低置信度样本

Result: NaCS仅使用1%训练数据即可恢复93-95%的全数据集训练性能，优于现有核心集选择技术，同时实现更好的效率

Conclusion: NaCS为基于内容的推荐系统提供了高质量的核心集选择框架，有效应对噪声问题，显著降低训练开销同时保持模型性能

Abstract: Content-based recommendation systems (CRSs) utilize content features to predict user-item interactions, serving as essential tools for helping users navigate information-rich web services. However, ensuring the effectiveness of CRSs requires large-scale and even continuous model training to accommodate diverse user preferences, resulting in significant computational costs and resource demands. A promising approach to this challenge is coreset selection, which identifies a small but representative subset of data samples that preserves model quality while reducing training overhead. Yet, the selected coreset is vulnerable to the pervasive noise in user-item interactions, particularly when it is minimally sized. To this end, we propose Noise-aware Coreset Selection (NaCS), a specialized framework for CRSs. NaCS constructs coresets through submodular optimization based on training gradients, while simultaneously correcting noisy labels using a progressively trained model. Meanwhile, we refine the selected coreset by filtering out low-confidence samples through uncertainty quantification, thereby avoid training with unreliable interactions. Through extensive experiments, we show that NaCS produces higher-quality coresets for CRSs while achieving better efficiency than existing coreset selection techniques. Notably, NaCS recovers 93-95\% of full-dataset training performance using merely 1\% of the training data. The source code is available at \href{https://github.com/chenxing1999/nacs}{https://github.com/chenxing1999/nacs}.

</details>


### [36] [Comparative Evaluation of Deep Learning-Based and WHO-Informed Approaches for Sperm Morphology Assessment](https://arxiv.org/abs/2601.10070)
*Mohammad Abbadi*

Main category: cs.LG

TL;DR: 本研究比较了基于图像的深度学习模型(HuSHeM)与WHO(+SIRI)标准在精子形态评估中的表现，发现深度学习模型在判别性能、校准和临床效用方面均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 精子形态评估是男性生育力评估的关键但主观的组成部分，常受观察者间变异性和资源限制的影响。需要更客观、可重复的评估方法来提高评估质量。

Method: 开发了基于图像的深度学习模型HuSHeM，在高分辨率精子形态图像上训练，并与增强版WHO标准(WHO(+SIRI))进行比较。使用独立临床队列评估模型性能，包括判别分析、校准分析和临床效用分析。

Result: HuSHeM模型显示出更高的判别性能(更高的AUC值)，在类别不平衡情况下表现更好(更高的PR-AUC值)，校准更准确，且在临床相关阈值概率范围内提供更大的净临床效益。

Conclusion: 基于图像的深度学习相比传统规则基础和炎症增强标准，能提供更好的预测可靠性和临床效用。该框架支持精子形态的客观、可重复评估，可作为生育筛查和转诊工作流程中的决策支持工具，但并非旨在替代临床判断或实验室评估。

Abstract: Assessment of sperm morphological quality remains a critical yet subjective component of male fertility evaluation, often limited by inter-observer variability and resource constraints. This study presents a comparative biomedical artificial intelligence framework evaluating an image-based deep learning model (HuSHeM) alongside a clinically grounded baseline derived from World Health Organization criteria augmented with the Systemic Inflammation Response Index (WHO(+SIRI)).
  The HuSHeM model was trained on high-resolution sperm morphology images and evaluated using an independent clinical cohort. Model performance was assessed using discrimination, calibration, and clinical utility analyses. The HuSHeM model demonstrated higher discriminative performance, as reflected by an increased area under the receiver operating characteristic curve with relatively narrow confidence intervals compared to WHO(+SIRI). Precision-recall analysis further indicated improved performance under class imbalance, with higher precision-recall area values across evaluated thresholds. Calibration analysis indicated closer agreement between predicted probabilities and observed outcomes for HuSHeM, while decision curve analysis suggested greater net clinical benefit across clinically relevant threshold probabilities.
  These findings suggest that image-based deep learning may offer improved predictive reliability and clinical utility compared with traditional rule-based and inflammation-augmented criteria. The proposed framework supports objective and reproducible assessment of sperm morphology and may serve as a decision-support tool within fertility screening and referral workflows. The proposed models are intended as decision-support or referral tools and are not designed to replace clinical judgment or laboratory assessment.

</details>


### [37] [Sparse-RL: Breaking the Memory Wall in LLM Reinforcement Learning via Stable Sparse Rollouts](https://arxiv.org/abs/2601.10079)
*Sijia Luo,Xiaokang Zhang,Yuxuan Hu,Bohan Zhang,Ke Wang,Jinbo Su,Mengshu Sun,Lei Liang,Jing Zhang*

Main category: cs.LG

TL;DR: Sparse-RL：一种在稀疏rollouts下实现稳定RL训练的方法，通过稀疏感知拒绝采样和重要性重加权来纠正压缩引起的策略不匹配问题


<details>
  <summary>Details</summary>
Motivation: RL在激发LLMs复杂推理能力方面很重要，但长时程rollouts中KV缓存的巨大内存开销成为关键瓶颈。现有KV压缩技术虽然能用于推理，但直接应用于RL训练会导致严重的策略不匹配和性能崩溃。

Method: 提出Sparse-RL框架，包含：1) 稀疏感知拒绝采样，2) 基于重要性的重加权，用于纠正压缩引起的信息损失带来的离策略偏差。该方法解决了密集旧策略、稀疏采样器策略和学习器策略之间的策略不匹配问题。

Result: 实验结果表明，Sparse-RL相比密集基线减少了rollout开销，同时保持了性能。此外，Sparse-RL实现了稀疏感知训练，显著增强了模型在稀疏推理部署时的鲁棒性。

Conclusion: Sparse-RL成功解决了RL训练中KV压缩引起的策略不匹配问题，实现了在有限硬件上的高效训练，同时为稀疏推理部署提供了更好的模型鲁棒性。

Abstract: Reinforcement Learning (RL) has become essential for eliciting complex reasoning capabilities in Large Language Models (LLMs). However, the substantial memory overhead of storing Key-Value (KV) caches during long-horizon rollouts acts as a critical bottleneck, often prohibiting efficient training on limited hardware. While existing KV compression techniques offer a remedy for inference, directly applying them to RL training induces a severe policy mismatch, leading to catastrophic performance collapse. To address this, we introduce Sparse-RL empowers stable RL training under sparse rollouts. We show that instability arises from a fundamental policy mismatch among the dense old policy, the sparse sampler policy, and the learner policy. To mitigate this issue, Sparse-RL incorporates Sparsity-Aware Rejection Sampling and Importance-based Reweighting to correct the off-policy bias introduced by compression-induced information loss. Experimental results show that Sparse-RL reduces rollout overhead compared to dense baselines while preserving the performance. Furthermore, Sparse-RL inherently implements sparsity-aware training, significantly enhancing model robustness during sparse inference deployment.

</details>


### [38] [Adaptive Label Error Detection: A Bayesian Approach to Mislabeled Data Detection](https://arxiv.org/abs/2601.10084)
*Zan Chaudhry,Noam H. Rotenberg,Brian Caffo,Craig K. Jones,Haris I. Sair*

Main category: cs.LG

TL;DR: 提出ALED方法，通过深度卷积神经网络提取特征、去噪、建模类别分布，使用似然比检验检测错误标签，在医学影像数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习分类系统对错误标注标签敏感，即使专家标注的数据也可能存在错误。随着机器学习广泛应用，识别和纠正错误标注对开发更强大模型至关重要。

Method: ALED方法：1) 从深度卷积神经网络提取中间特征空间；2) 对特征去噪；3) 用多维高斯分布建模每个类别的降维流形；4) 使用似然比检验识别错误标注样本。

Result: ALED在多个医学影像数据集上相比现有标签错误检测方法显著提高了灵敏度，同时不损失精度。在纠正数据上微调神经网络可使测试集错误减少33.8%。

Conclusion: ALED是一种有效的标签错误检测方法，能显著提升模型性能，已部署在Python包statlab中，对终端用户有实际价值。

Abstract: Machine learning classification systems are susceptible to poor performance when trained with incorrect ground truth labels, even when data is well-curated by expert annotators. As machine learning becomes more widespread, it is increasingly imperative to identify and correct mislabeling to develop more powerful models. In this work, we motivate and describe Adaptive Label Error Detection (ALED), a novel method of detecting mislabeling. ALED extracts an intermediate feature space from a deep convolutional neural network, denoises the features, models the reduced manifold of each class with a multidimensional Gaussian distribution, and performs a simple likelihood ratio test to identify mislabeled samples. We show that ALED has markedly increased sensitivity, without compromising precision, compared to established label error detection methods, on multiple medical imaging datasets. We demonstrate an example where fine-tuning a neural network on corrected data results in a 33.8% decrease in test set errors, providing strong benefits to end users. The ALED detector is deployed in the Python package statlab.

</details>


### [39] [Bayesian Meta-Analyses Could Be More: A Case Study in Trial of Labor After a Cesarean-section Outcomes and Complications](https://arxiv.org/abs/2601.10089)
*Ashley Klein,Edward Raff,Marcia DesJardin*

Main category: cs.LG

TL;DR: 该研究针对医学元分析中关键决策变量未被记录的问题，提出贝叶斯方法评估阳性效应主张的有效性，并以剖宫产后试产(TOLAC)为例验证其临床实用性。


<details>
  <summary>Details</summary>
Motivation: 医学研究中元分析的可靠性依赖于先前研究是否准确记录了关键变量，但实际中影响医生决策的重要变量常未被记录，导致效应量未知和结论不可靠。

Method: 采用贝叶斯方法构建分析框架，处理医学研究中常见的关键变量缺失问题，并以剖宫产后试产(TOLAC)为具体应用场景进行验证。

Result: 开发了贝叶斯分析方法，能够评估在关键变量缺失情况下阳性效应主张是否仍然成立，为产科医生在TOLAC决策中提供了必要的支持依据。

Conclusion: 贝叶斯方法能够有效处理医学元分析中的关键变量缺失问题，为临床决策提供更可靠的证据支持，特别是在干预选择有限的TOLAC等场景中。

Abstract: The meta-analysis's utility is dependent on previous studies having accurately captured the variables of interest, but in medical studies, a key decision variable that impacts a physician's decisions was not captured. This results in an unknown effect size and unreliable conclusions. A Bayesian approach may allow analysis to determine if the claim of a positive effect is still warranted, and we build a Bayesian approach to this common medical scenario. To demonstrate its utility, we assist professional OBGYNs in evaluating Trial of Labor After a Cesarean-section (TOLAC) situations where few interventions are available for patients and find the support needed for physicians to advance patient care.

</details>


### [40] [LeMoF: Level-guided Multimodal Fusion for Heterogeneous Clinical Data](https://arxiv.org/abs/2601.10092)
*Jongseok Kim,Seongae Kang,Jonghwan Shin,Yuhan Lee,Ohyun Jo*

Main category: cs.LG

TL;DR: 提出LeMoF框架，通过层级引导的模态融合策略，在异构临床数据中实现预测稳定性与判别能力的平衡


<details>
  <summary>Details</summary>
Motivation: 现有多模态临床预测方法依赖静态模态集成方案和简单融合策略，未能充分利用模态特定表示，在异构临床环境中表现受限

Method: 提出层级引导模态融合(LeMoF)框架，选择性整合每个模态内的层级引导表示，明确分离并学习全局模态级预测和层级特定判别表示

Result: 在ICU住院时长预测实验中，LeMoF在不同编码器配置下均优于现有最先进的多模态融合技术，层级集成是实现稳健预测性能的关键因素

Conclusion: LeMoF通过层级引导的模态融合策略，在异构临床环境中实现了预测稳定性与判别能力的平衡，为多模态临床预测提供了有效解决方案

Abstract: Multimodal clinical prediction is widely used to integrate heterogeneous data such as Electronic Health Records (EHR) and biosignals. However, existing methods tend to rely on static modality integration schemes and simple fusion strategies. As a result, they fail to fully exploit modality-specific representations. In this paper, we propose Level-guided Modal Fusion (LeMoF), a novel framework that selectively integrates level-guided representations within each modality. Each level refers to a representation extracted from a different layer of the encoder. LeMoF explicitly separates and learns global modality-level predictions from level-specific discriminative representations. This design enables LeMoF to achieve a balanced performance between prediction stability and discriminative capability even in heterogeneous clinical environments. Experiments on length of stay prediction using Intensive Care Unit (ICU) data demonstrate that LeMoF consistently outperforms existing state-of-the-art multimodal fusion techniques across various encoder configurations. We also confirmed that level-wise integration is a key factor in achieving robust predictive performance across various clinical conditions.

</details>


### [41] [Multilingual-To-Multimodal (M2M): Unlocking New Languages with Monolingual Text](https://arxiv.org/abs/2601.10096)
*Piyush Singh Pasi*

Main category: cs.LG

TL;DR: METAL：一种轻量级对齐方法，仅使用英语文本学习少量线性层，将多语言文本嵌入映射到多模态空间，实现强大的零样本跨语言迁移。


<details>
  <summary>Details</summary>
Motivation: 多模态模型在英语上表现出色，但其他语言性能显著下降，主要原因是多语言多模态资源有限。现有解决方案过度依赖机器翻译，而多语言文本建模的进展未被充分利用。

Method: METAL（轻量级对齐方法）仅学习少量线性层，使用英语文本将多语言文本嵌入映射到多模态空间。该方法简单但有效，通过几何变换而非简单旋转来重塑嵌入空间。

Result: 在XTD文本到图像检索任务中，METAL在英语上达到94.9%的Recall@10，在11种语言（其中10种未见）上平均达到89.5%的Recall@10。定性分析显示多语言嵌入与多模态表示紧密对齐，权重分析表明变换重塑了嵌入几何结构。

Conclusion: METAL是一种简单有效的多语言多模态对齐方法，不仅适用于图像-文本检索，还能推广到音频-文本检索和跨语言文本到图像生成。作者发布了代码、检查点和多语言评估数据集以促进进一步研究。

Abstract: Multimodal models excel in English, supported by abundant image-text and audio-text data, but performance drops sharply for other languages due to limited multilingual multimodal resources. Existing solutions rely heavily on machine translation, while advances in multilingual text modeling remain underutilized. We introduce METAL, a lightweight alignment method that learns only a few linear layers using English text alone to map multilingual text embeddings into a multimodal space. Despite its simplicity, METAL matches baseline performance in English (94.9 percent Recall at 10) and achieves strong zero-shot transfer (89.5 percent Recall at 10 averaged across 11 languages, 10 unseen) on XTD text-to-image retrieval. Qualitative t-SNE visualizations show that multilingual embeddings align tightly with multimodal representations, while weight analysis reveals that the transformation reshapes embedding geometry rather than performing trivial rotations. Beyond image-text retrieval, METAL generalizes to audio-text retrieval and cross-lingual text-to-image generation. We release code and checkpoints at https://github.com/m2m-codebase/M2M , as well as multilingual evaluation datasets including MSCOCO Multilingual 30K (https://huggingface.co/datasets/piyushsinghpasi/mscoco-multilingual-30k ), AudioCaps Multilingual (https://huggingface.co/datasets/piyushsinghpasi/audiocaps-multilingual ), and Clotho Multilingual (https://huggingface.co/datasets/piyushsinghpasi/clotho-multilingual ), to facilitate further research.

</details>


### [42] [Understanding and Preserving Safety in Fine-Tuned LLMs](https://arxiv.org/abs/2601.10141)
*Jiawen Zhang,Yangfan Hu,Kejia Chen,Lipeng He,Jiachen Ma,Jian Lou,Dan Li,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: 提出SPF方法解决LLM微调中的安全-效用困境，通过几何分析安全与效用梯度，移除冲突方向以保持安全对齐


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调会严重破坏安全对齐，即使微调数据无害也会增加越狱攻击风险。现有方法面临安全-效用困境：强调安全会损害任务性能，而优先效用则需要深度微调导致安全大幅下降

Method: SPF（安全保持微调）：通过系统实证分析发现安全梯度位于低秩子空间，效用梯度跨越更广高维空间，两者方向冲突。从单个样本高效估计主导安全方向，在微调时显式移除与低秩安全子空间冲突的梯度分量

Result: 理论上证明SPF保证效用收敛同时限制安全漂移。实证显示SPF能保持下游任务性能，恢复几乎所有预训练安全对齐，即使在对抗性微调场景下。对深度微调和动态越狱攻击表现出强鲁棒性

Conclusion: 研究提供了对LLM微调中安全对齐机制的新理解，SPF方法为实现始终对齐的LLM微调提供了实用指导

Abstract: Fine-tuning is an essential and pervasive functionality for applying large language models (LLMs) to downstream tasks. However, it has the potential to substantially degrade safety alignment, e.g., by greatly increasing susceptibility to jailbreak attacks, even when the fine-tuning data is entirely harmless. Despite garnering growing attention in defense efforts during the fine-tuning stage, existing methods struggle with a persistent safety-utility dilemma: emphasizing safety compromises task performance, whereas prioritizing utility typically requires deep fine-tuning that inevitably leads to steep safety declination.
  In this work, we address this dilemma by shedding new light on the geometric interaction between safety- and utility-oriented gradients in safety-aligned LLMs. Through systematic empirical analysis, we uncover three key insights: (I) safety gradients lie in a low-rank subspace, while utility gradients span a broader high-dimensional space; (II) these subspaces are often negatively correlated, causing directional conflicts during fine-tuning; and (III) the dominant safety direction can be efficiently estimated from a single sample. Building upon these novel insights, we propose safety-preserving fine-tuning (SPF), a lightweight approach that explicitly removes gradient components conflicting with the low-rank safety subspace. Theoretically, we show that SPF guarantees utility convergence while bounding safety drift. Empirically, SPF consistently maintains downstream task performance and recovers nearly all pre-trained safety alignment, even under adversarial fine-tuning scenarios. Furthermore, SPF exhibits robust resistance to both deep fine-tuning and dynamic jailbreak attacks. Together, our findings provide new mechanistic understanding and practical guidance toward always-aligned LLM fine-tuning.

</details>


### [43] [Simple Network Graph Comparative Learning](https://arxiv.org/abs/2601.10150)
*Qiang Yu,Xinran Cheng,Shiqiang Xu,Chuanyi Liu*

Main category: cs.LG

TL;DR: 提出SNGCL方法，通过叠加多层拉普拉斯平滑滤波器处理图数据，使用改进的三元重组损失函数，在节点分类任务中实现强竞争力


<details>
  <summary>Details</summary>
Motivation: 当前图对比学习方法在节点分类任务中存在两个主要问题：1）现有数据增强技术可能导致新视图与原始视图差异过大，削弱视图相关性并影响训练效率；2）大多数现有算法依赖大量负样本

Method: 提出SNGCL方法：1）使用叠加多层拉普拉斯平滑滤波器处理数据，分别获得全局和局部特征平滑矩阵；2）将这些矩阵传入孪生网络的目标网络和在线网络；3）采用改进的三元重组损失函数，拉近类内距离并推远类间距离

Result: 在节点分类任务中与最先进模型比较，实验结果表明SNGCL在大多数任务中具有很强的竞争力

Conclusion: SNGCL方法有效解决了现有图对比学习在节点分类中的挑战，通过创新的数据处理和损失函数设计，在节点分类任务中表现出色

Abstract: The effectiveness of contrastive learning methods has been widely recognized in the field of graph learning, especially in contexts where graph data often lack labels or are difficult to label. However, the application of these methods to node classification tasks still faces a number of challenges. First, existing data enhancement techniques may lead to significant differences from the original view when generating new views, which may weaken the relevance of the view and affect the efficiency of model training. Second, the vast majority of existing graph comparison learning algorithms rely on the use of a large number of negative samples. To address the above challenges, this study proposes a novel node classification contrast learning method called Simple Network Graph Comparative Learning (SNGCL). Specifically, SNGCL employs a superimposed multilayer Laplace smoothing filter as a step in processing the data to obtain global and local feature smoothing matrices, respectively, which are thus passed into the target and online networks of the siamese network, and finally employs an improved triple recombination loss function to bring the intra-class distance closer and the inter-class distance farther. We have compared SNGCL with state-of-the-art models in node classification tasks, and the experimental results show that SNGCL is strongly competitive in most tasks.

</details>


### [44] [LOOKAT: Lookup-Optimized Key-Attention for Memory-Efficient Transformers](https://arxiv.org/abs/2601.10155)
*Aryan Karmore*

Main category: cs.LG

TL;DR: LOOKAT：一种基于乘积量化和非对称距离计算的KV缓存压缩方法，将注意力计算从内存受限转为计算受限，实现64倍压缩且保持95.7%输出保真度


<details>
  <summary>Details</summary>
Motivation: 当前量化方法虽然能压缩KV缓存存储，但无法减少带宽，因为注意力计算需要将INT4/INT8的键反量化为FP16。需要一种能同时减少存储和带宽的KV缓存压缩方法，以便在边缘设备上部署大语言模型。

Method: 将注意力评分视为内积相似度搜索，应用向量数据库的压缩技术。具体采用乘积量化和非对称距离计算，将键向量分解为子空间，学习码本，通过查找表计算注意力表，从而将注意力计算从内存受限转为计算受限。

Result: 在GPT-2上测试：64倍压缩时达到95.7%输出保真度，32倍压缩时达到95.0%保真度。无需架构修改或训练，保持秩相关ρ>0.95。理论分析表明秩相关退化与d_k/mK成正比，在序列长度达1024个token时验证了保证。

Conclusion: LOOKAT成功将向量数据库压缩技术应用于Transformer架构，有效压缩KV缓存，显著减少存储和带宽需求，使大语言模型能够在边缘设备上部署，同时保持高输出质量。

Abstract: Compressing the KV cache is a required step to deploy large language models on edge devices. Current quantization methods compress storage but fail to reduce bandwidth as attention calculation requires dequantizing keys from INT4/INT8 to FP16 before use. We observe that attention scoring is mathematically equivalent to the inner product similarity search and we can apply some compression techniques from vector databases to compress KV-cache better. We propose LOOKAT, which applies product quantization and asymmetric distance computation, to transformer architecture by decomposing key vectors into subspaces, learning codebooks and computing attention tables via lookup tables. This transforms attention from memory-bound to compute-bound. LOOKAT achieves 64 $\times$ compression at 95.7\% output fidelity and 32 $\times$ compression at 95.0\% fidelity when tested on GPT-2. LOOKAT requires no architecture changes or training while maintaining rank correlation $ρ> 0.95$. Theoretical analysis confirms that rank correlation degrades as $O(d_k/mK)$, with guarantees validated across sequence lengths up to 1024 tokens.

</details>


### [45] [CC-OR-Net: A Unified Framework for LTV Prediction through Structural Decoupling](https://arxiv.org/abs/2601.10176)
*Mingyu Zhao,Haoran Bai,Yu Tian,Bing Zhu,Hengliang Luo*

Main category: cs.LG

TL;DR: 提出CC-OR-Net框架，通过结构分解解决LTV预测中的零膨胀和长尾分布问题，在保证排序结构的同时实现细粒度回归。


<details>
  <summary>Details</summary>
Motivation: 客户生命周期价值预测面临零膨胀和长尾分布的独特挑战：大多数低中价值用户数量上压倒少数但关键的高价值"鲸鱼"用户，且低中价值用户内部也存在显著异质性。现有方法要么依赖刚性统计假设，要么试图通过有序分桶解耦排序和回归，但往往通过损失约束而非固有架构设计来强制有序性，无法平衡全局准确性和高价值精度。

Method: 提出条件级联有序残差网络(CC-OR-Net)，通过结构分解实现更稳健的解耦，其中排序在架构上得到保证。包含三个专门组件：结构有序分解模块用于稳健排序、桶内残差模块用于细粒度回归、定向高价值增强模块用于提升顶级用户精度。

Result: 在包含超过3亿用户的真实世界数据集上评估，CC-OR-Net在所有关键业务指标上实现了优越的权衡，超越了现有最先进方法，创造了全面且具有商业价值的LTV预测解决方案。

Conclusion: CC-OR-Net通过结构分解框架有效解决了LTV预测中的零膨胀和长尾分布问题，在架构上保证排序的同时实现细粒度回归，为实际商业应用提供了更优的解决方案。

Abstract: Customer Lifetime Value (LTV) prediction, a central problem in modern marketing, is characterized by a unique zero-inflated and long-tail data distribution. This distribution presents two fundamental challenges: (1) the vast majority of low-to-medium value users numerically overwhelm the small but critically important segment of high-value "whale" users, and (2) significant value heterogeneity exists even within the low-to-medium value user base. Common approaches either rely on rigid statistical assumptions or attempt to decouple ranking and regression using ordered buckets; however, they often enforce ordinality through loss-based constraints rather than inherent architectural design, failing to balance global accuracy with high-value precision. To address this gap, we propose \textbf{C}onditional \textbf{C}ascaded \textbf{O}rdinal-\textbf{R}esidual Networks \textbf{(CC-OR-Net)}, a novel unified framework that achieves a more robust decoupling through \textbf{structural decomposition}, where ranking is architecturally guaranteed. CC-OR-Net integrates three specialized components: a \textit{structural ordinal decomposition module} for robust ranking, an \textit{intra-bucket residual module} for fine-grained regression, and a \textit{targeted high-value augmentation module} for precision on top-tier users. Evaluated on real-world datasets with over 300M users, CC-OR-Net achieves a superior trade-off across all key business metrics, outperforming state-of-the-art methods in creating a holistic and commercially valuable LTV prediction solution.

</details>


### [46] [Bias in the Shadows: Explore Shortcuts in Encrypted Network Traffic Classification](https://arxiv.org/abs/2601.10180)
*Chuyi Wang,Xiaohui Xie,Tongze Wang,Yong Cui*

Main category: cs.LG

TL;DR: BiasSeeker：首个模型无关、数据驱动的半自动化框架，用于检测加密网络流量分类中的数据集特定捷径特征，通过统计相关分析识别虚假特征，提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在加密网络流量分类中常出现捷径学习问题，依赖虚假相关性而无法泛化到真实数据。现有解决方案过度依赖模型特定的解释技术，缺乏跨模型架构和部署场景的适应性和通用性。

Method: 提出BiasSeeker框架：1）直接在原始二进制流量上进行统计相关分析，识别可能损害泛化能力的虚假或环境纠缠特征；2）引入系统化的捷径特征分类，并应用类别特定的验证策略，在减少偏差的同时保留有意义信息；3）强调上下文感知的特征选择和数据集特定诊断。

Result: 在三个NTC任务的19个公共数据集上评估BiasSeeker，证明其有效性。该框架为理解和解决加密网络流量分类中的捷径学习问题提供了新视角。

Conclusion: BiasSeeker表明特征选择应该是模型训练前有意且场景敏感的步骤，提高了对加密网络流量分类中捷径学习问题的认识，为构建更鲁棒的分类系统提供了实用工具。

Abstract: Pre-trained models operating directly on raw bytes have achieved promising performance in encrypted network traffic classification (NTC), but often suffer from shortcut learning-relying on spurious correlations that fail to generalize to real-world data. Existing solutions heavily rely on model-specific interpretation techniques, which lack adaptability and generality across different model architectures and deployment scenarios.
  In this paper, we propose BiasSeeker, the first semi-automated framework that is both model-agnostic and data-driven for detecting dataset-specific shortcut features in encrypted traffic. By performing statistical correlation analysis directly on raw binary traffic, BiasSeeker identifies spurious or environment-entangled features that may compromise generalization, independent of any classifier. To address the diverse nature of shortcut features, we introduce a systematic categorization and apply category-specific validation strategies that reduce bias while preserving meaningful information.
  We evaluate BiasSeeker on 19 public datasets across three NTC tasks. By emphasizing context-aware feature selection and dataset-specific diagnosis, BiasSeeker offers a novel perspective for understanding and addressing shortcut learning in encrypted network traffic classification, raising awareness that feature selection should be an intentional and scenario-sensitive step prior to model training.

</details>


### [47] [Reinforcement Learning to Discover a NorthEast Monsoon Index for Monthly Rainfall Prediction in Thailand](https://arxiv.org/abs/2601.10181)
*Kiattikun Chobtham*

Main category: cs.LG

TL;DR: 提出基于强化学习优化的东北季风气候指数，结合LSTM模型显著提升泰国地区长期月降雨预测精度


<details>
  <summary>Details</summary>
Motivation: 现有全球气候指数（如ENSO）在泰国特定区域长期降雨预测中效果有限，缺乏针对本地尺度的气候指数来提高预测准确性

Method: 1) 提出新的东北季风气候指数，基于海表温度反映冬季季风气候；2) 使用深度Q网络强化学习代理优化计算区域，选择与季节性降雨相关性最高的矩形区域；3) 将降雨站分为12个聚类；4) 将优化后的指数输入LSTM模型进行长期月降雨预测

Result: 优化后的指数显著提高了大多数聚类区域的长期月降雨预测技能，有效降低了12个月提前预测的均方根误差

Conclusion: 通过强化学习优化的本地化气候指数能够有效提升特定区域长期降雨预测精度，为区域气候预测提供了新方法

Abstract: Climate prediction is a challenge due to the intricate spatiotemporal patterns within Earth systems. Global climate indices, such as the El Niño Southern Oscillation, are standard input features for long-term rainfall prediction. However, a significant gap persists regarding local-scale indices capable of improving predictive accuracy in specific regions of Thailand. This paper introduces a novel NorthEast monsoon climate index calculated from sea surface temperature to reflect the climatology of the boreal winter monsoon. To optimise the calculated areas used for this index, a Deep Q-Network reinforcement learning agent explores and selects the most effective rectangles based on their correlation with seasonal rainfall. Rainfall stations were classified into 12 distinct clusters to distinguish rainfall patterns between southern and upper Thailand. Experimental results show that incorporating the optimised index into Long Short-Term Memory models significantly improves long-term monthly rainfall prediction skill in most cluster areas. This approach effectively reduces the Root Mean Square Error for 12-month-ahead forecasts.

</details>


### [48] [Graph Regularized PCA](https://arxiv.org/abs/2601.10199)
*Antonio Briola,Marwin Schmidt,Fabio Caccioli,Carlos Ros Perez,James Singleton,Christian Michler,Tomaso Aste*

Main category: cs.LG

TL;DR: 提出图正则化PCA（GR-PCA），通过图拉普拉斯正则化将特征依赖结构融入PCA，抑制高频噪声同时保留低频信号，提升结构保真度


<details>
  <summary>Details</summary>
Motivation: 高维数据中变量间的依赖关系常违反PCA的各向同性噪声假设，当噪声在特征间非独立同分布时，传统PCA不再最优，需要能利用特征依赖结构的降维方法

Method: 通过图正则化扩展PCA：1）学习稀疏精度图捕捉特征依赖结构；2）使用图拉普拉斯正则化，使载荷偏向低频傅里叶模式；3）抑制高频信号，保留图相干低频信号

Result: 在多种图拓扑、信噪比和稀疏度合成数据上评估：1）将方差集中在目标支撑集上；2）产生更低图拉普拉斯能量的载荷；3）样本外重建保持竞争力；4）高频信号存在时防止过拟合

Conclusion: GR-PCA提供了一种简单、模块化、可扩展的结构感知降维方法，在高频信号图相关时显著优于PCA，在旋转不变时与PCA相当，改善结构保真度而不牺牲预测性能

Abstract: High-dimensional data often exhibit dependencies among variables that violate the isotropic-noise assumption under which principal component analysis (PCA) is optimal. For cases where the noise is not independent and identically distributed across features (i.e., the covariance is not spherical) we introduce Graph Regularized PCA (GR-PCA). It is a graph-based regularization of PCA that incorporates the dependency structure of the data features by learning a sparse precision graph and biasing loadings toward the low-frequency Fourier modes of the corresponding graph Laplacian. Consequently, high-frequency signals are suppressed, while graph-coherent low-frequency ones are preserved, yielding interpretable principal components aligned with conditional relationships. We evaluate GR-PCA on synthetic data spanning diverse graph topologies, signal-to-noise ratios, and sparsity levels. Compared to mainstream alternatives, it concentrates variance on the intended support, produces loadings with lower graph-Laplacian energy, and remains competitive in out-of-sample reconstruction. When high-frequency signals are present, the graph Laplacian penalty prevents overfitting, reducing the reconstruction accuracy but improving structural fidelity. The advantage over PCA is most pronounced when high-frequency signals are graph-correlated, whereas PCA remains competitive when such signals are nearly rotationally invariant. The procedure is simple to implement, modular with respect to the precision estimator, and scalable, providing a practical route to structure-aware dimensionality reduction that improves structural fidelity without sacrificing predictive performance.

</details>


### [49] [PRL: Process Reward Learning Improves LLMs' Reasoning Ability and Broadens the Reasoning Boundary](https://arxiv.org/abs/2601.10201)
*Jiarui Yao,Ruida Wang,Tong Zhang*

Main category: cs.LG

TL;DR: 本文提出Process Reward Learning (PRL)，一种将结果奖励分解为过程监督信号的强化学习方法，用于提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有工作大多基于轨迹级的结果奖励，缺乏推理过程中的细粒度监督；其他结合过程信号的训练框架依赖繁琐的额外步骤（如MCTS、训练单独奖励模型），效率低下；且过程信号设计缺乏严格理论支持，优化机制不透明。

Method: 提出Process Reward Learning (PRL)，将熵正则化强化学习目标分解为中间步骤，为模型分配严格的过程奖励。从理论动机出发，推导出PRL公式，本质上等价于奖励最大化加上策略模型与参考模型之间的KL散度惩罚项，但能将结果奖励转化为过程监督信号。

Result: 实验表明，PRL不仅提高了LLM推理能力的平均性能（average @ n），还通过改善pass @ n指标拓宽了推理边界。大量实验验证了PRL的有效性和泛化能力。

Conclusion: PRL提供了一种理论严谨、训练高效的过程奖励学习方法，能够有效提升大语言模型的推理能力，同时拓宽其推理边界。

Abstract: Improving the reasoning abilities of Large Language Models (LLMs) has been a continuous topic recently. But most relevant works are based on outcome rewards at the trajectory level, missing fine-grained supervision during the reasoning process. Other existing training frameworks that try to combine process signals together to optimize LLMs also rely heavily on tedious additional steps like MCTS, training a separate reward model, etc., doing harm to the training efficiency. Moreover, the intuition behind the process signals design lacks rigorous theoretical support, leaving the understanding of the optimization mechanism opaque. In this paper, we propose Process Reward Learning (PRL), which decomposes the entropy regularized reinforcement learning objective into intermediate steps, with rigorous process rewards that could be assigned to models accordingly. Starting from theoretical motivation, we derive the formulation of PRL that is essentially equivalent to the objective of reward maximization plus a KL-divergence penalty term between the policy model and a reference model. However, PRL could turn the outcome reward into process supervision signals, which helps better guide the exploration during RL optimization. From our experiment results, we demonstrate that PRL not only improves the average performance for LLMs' reasoning ability measured by average @ n, but also broadens the reasoning boundary by improving the pass @ n metric. Extensive experiments show the effectiveness of PRL could be verified and generalized.

</details>


### [50] [Fundamental Limitations of Favorable Privacy-Utility Guarantees for DP-SGD](https://arxiv.org/abs/2601.10237)
*Murat Bilgehan Ertan,Marten van Dijk*

Main category: cs.LG

TL;DR: DP-SGD在f-差分隐私框架下存在隐私-效用权衡的根本限制，即使使用洗牌采样，也无法同时实现强隐私和高效用


<details>
  <summary>Details</summary>
Motivation: 研究DP-SGD在最坏情况对抗性隐私定义下的根本限制，目前对这些限制的理解还不够深入

Method: 在f-差分隐私框架下分析DP-SGD，使用洗牌采样，推导出可实现的权衡曲线的显式次优上界

Result: 证明了强制小的分离度κ会强加高斯噪声乘子σ的严格下界，导致DP-SGD无法同时实现强隐私和高效用

Conclusion: DP-SGD在标准最坏情况对抗性假设下存在关键瓶颈，噪声水平要求导致实际训练中显著的准确率下降

Abstract: Differentially Private Stochastic Gradient Descent (DP-SGD) is the dominant paradigm for private training, but its fundamental limitations under worst-case adversarial privacy definitions remain poorly understood. We analyze DP-SGD in the $f$-differential privacy framework, which characterizes privacy via hypothesis-testing trade-off curves, and study shuffled sampling over a single epoch with $M$ gradient updates. We derive an explicit suboptimal upper bound on the achievable trade-off curve. This result induces a geometric lower bound on the separation $κ$ which is the maximum distance between the mechanism's trade-off curve and the ideal random-guessing line. Because a large separation implies significant adversarial advantage, meaningful privacy requires small $κ$. However, we prove that enforcing a small separation imposes a strict lower bound on the Gaussian noise multiplier $σ$, which directly limits the achievable utility. In particular, under the standard worst-case adversarial model, shuffled DP-SGD must satisfy
  $σ\ge \frac{1}{\sqrt{2\ln M}}$ $\quad\text{or}\quad$ $κ\ge\ \frac{1}{\sqrt{8}}\!\left(1-\frac{1}{\sqrt{4π\ln M}}\right)$,
  and thus cannot simultaneously achieve strong privacy and high utility. Although this bound vanishes asymptotically as $M \to \infty$, the convergence is extremely slow: even for practically relevant numbers of updates the required noise magnitude remains substantial. We further show that the same limitation extends to Poisson subsampling up to constant factors. Our experiments confirm that the noise levels implied by this bound leads to significant accuracy degradation at realistic training settings, thus showing a critical bottleneck in DP-SGD under standard worst-case adversarial assumptions.

</details>


### [51] [X-SAM: Boosting Sharpness-Aware Minimization with Dominant-Eigenvector Gradient Correction](https://arxiv.org/abs/2601.10251)
*Hongru Duan,Yongle Chen,Lei Guan*

Main category: cs.LG

TL;DR: 提出X-SAM方法，通过特征向量对齐修正SAM的梯度，更直接地正则化Hessian最大特征值，改善泛化性能


<details>
  <summary>Details</summary>
Motivation: SAM方法在训练中并不总是符合理论预期，因为尖锐和平坦区域都可能产生较小的扰动损失，导致梯度仍可能指向尖锐区域，无法达到SAM的预期效果

Method: 从谱和几何角度分析SAM，利用梯度与Hessian主特征向量夹角作为尖锐度度量；提出X-SAM方法，通过沿主特征向量的正交分解修正梯度

Result: 证明了X-SAM的收敛性和优越泛化能力，大量实验验证了理论和实践优势

Conclusion: X-SAM通过特征向量对齐更直接有效地正则化尖锐度，解决了SAM在特定情况下的局限性，提升了优化效果和泛化性能

Abstract: Sharpness-Aware Minimization (SAM) aims to improve generalization by minimizing a worst-case perturbed loss over a small neighborhood of model parameters. However, during training, its optimization behavior does not always align with theoretical expectations, since both sharp and flat regions may yield a small perturbed loss. In such cases, the gradient may still point toward sharp regions, failing to achieve the intended effect of SAM. To address this issue, we investigate SAM from a spectral and geometric perspective: specifically, we utilize the angle between the gradient and the leading eigenvector of the Hessian as a measure of sharpness. Our analysis illustrates that when this angle is less than or equal to ninety degrees, the effect of SAM's sharpness regularization can be weakened. Furthermore, we propose an explicit eigenvector-aligned SAM (X-SAM), which corrects the gradient via orthogonal decomposition along the top eigenvector, enabling more direct and efficient regularization of the Hessian's maximum eigenvalue. We prove X-SAM's convergence and superior generalization, with extensive experimental evaluations confirming both theoretical and practical advantages.

</details>


### [52] [In-Context Source and Channel Coding](https://arxiv.org/abs/2601.10267)
*Ziqiong Wang,Tianqi Ren,Rongpeng Li,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: 提出接收端上下文解码框架，通过可靠性引导的比特翻转和LLM算术解码增强分离信源信道编码的鲁棒性，避免低信噪比下的悬崖效应


<details>
  <summary>Details</summary>
Motivation: 分离信源信道编码在文本传输中具有模块化和兼容性优势，但在低信噪比下存在明显的悬崖效应，信道解码后的残留比特错误会破坏无损信源解码，特别是基于大语言模型的算术编码

Method: 提出接收端上下文解码框架：1) 使用错误校正码变换器获取比特级可靠性；2) 基于上下文一致性比特流，通过可靠性引导的比特翻转构建置信度排序的候选池；3) 采样紧凑多样的候选子集；4) 应用基于LLM的算术解码器获取重构和序列级对数似然；5) 通过可靠性-似然融合规则选择最终输出

Result: 在加性高斯白噪声和瑞利衰落信道上的大量实验表明，相比传统分离信源信道编码基线和代表性联合信源信道编码方案，该方法获得了持续的性能增益

Conclusion: 提出的接收端上下文解码框架在不修改发射端的情况下增强了分离信源信道编码的鲁棒性，有效缓解了低信噪比下的悬崖效应，为文本传输提供了更可靠的解决方案

Abstract: Separate Source-Channel Coding (SSCC) remains attractive for text transmission due to its modularity and compatibility with mature entropy coders and powerful channel codes. However, SSCC often suffers from a pronounced cliff effect in low Signal-to-Noise Ratio (SNR) regimes, where residual bit errors after channel decoding can catastrophically break lossless source decoding, especially for Arithmetic Coding (AC) driven by Large Language Models (LLMs). This paper proposes a receiver-side In-Context Decoding (ICD) framework that enhances SSCC robustness without modifying the transmitter. ICD leverages an Error Correction Code Transformer (ECCT) to obtain bit-wise reliability for the decoded information bits. Based on the context-consistent bitstream, ICD constructs a confidence-ranked candidate pool via reliability-guided bit flipping, samples a compact yet diverse subset of candidates, and applies an LLM-based arithmetic decoder to obtain both reconstructions and sequence-level log-likelihoods. A reliability-likelihood fusion rule then selects the final output. We further provide theoretical guarantees on the stability and convergence of the proposed sampling procedure. Extensive experiments over Additive White Gaussian Noise (AWGN) and Rayleigh fading channels demonstrate consistent gains compared with conventional SSCC baselines and representative Joint Source-Channel Coding (JSCC) schemes.

</details>


### [53] [Early Fault Detection on CMAPSS with Unsupervised LSTM Autoencoders](https://arxiv.org/abs/2601.10269)
*P. Sánchez,K. Reyes,B. Radu,E. Fernández*

Main category: cs.LG

TL;DR: 提出无需故障标签的无监督涡扇发动机健康监测框架，通过回归归一化消除工况影响，使用LSTM自编码器检测异常，实现高召回率和低误报率


<details>
  <summary>Details</summary>
Motivation: 现有健康监测方法通常需要运行至故障的标签数据，这在实践中难以获取。需要开发无需故障标签的无监督框架，能够快速部署并适应不同机队

Method: 1) 使用基于回归的归一化消除NASA CMAPSS传感器数据中的工况影响；2) 仅使用健康轨迹部分训练LSTM自编码器；3) 采用自适应数据驱动阈值估计持续重构误差，实时触发警报

Result: 基准测试显示该方法在多种运行状态下具有高召回率和低误报率，能够快速部署、扩展到不同机队，并可作为剩余使用寿命模型的补充预警层

Conclusion: 该无监督框架有效解决了传统方法对故障标签的依赖问题，实现了实用的涡扇发动机健康监测，具有快速部署和良好扩展性

Abstract: This paper introduces an unsupervised health-monitoring framework for turbofan engines that does not require run-to-failure labels. First, operating-condition effects in NASA CMAPSS sensor streams are removed via regression-based normalisation; then a Long Short-Term Memory (LSTM) autoencoder is trained only on the healthy portion of each trajectory. Persistent reconstruction error, estimated using an adaptive data-driven threshold, triggers real-time alerts without hand-tuned rules. Benchmark results show high recall and low false-alarm rates across multiple operating regimes, demonstrating that the method can be deployed quickly, scale to diverse fleets, and serve as a complementary early-warning layer to Remaining Useful Life models.

</details>


### [54] [Queueing-Aware Optimization of Reasoning Tokens for Accuracy-Latency Trade-offs in LLM Servers](https://arxiv.org/abs/2601.10274)
*Emre Ozbas,Melih Bastopcu*

Main category: cs.LG

TL;DR: 论文研究LLM服务器中为不同任务类型分配思考token的优化问题，在准确率-延迟权衡下最大化加权准确率并最小化系统时间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型服务器需要处理多种类型的查询任务，每个任务需要不同的计算资源（思考token）。如何为不同任务类型分配token以平衡准确率和延迟是一个关键问题，因为token分配会影响服务时间和响应准确性。

Method: 将系统建模为M/G/1队列，服务时间与分配的token数量呈近似仿射关系。建立约束优化问题，最大化加权平均准确率并惩罚平均系统时间，考虑token预算约束和队列稳定性条件。使用严格凹目标函数确保最优解存在唯一性，开发迭代求解方法和投影梯度法。

Result: 证明了目标函数在稳定区域内严格凹，确保最优token分配存在且唯一。推导出一阶最优条件，得到耦合投影定点特征，开发了可计算全局步长界的投影梯度法保证收敛。通过舍入连续解得到整数token分配，仿真评估性能损失。

Conclusion: 该研究为LLM服务器中多任务类型的资源分配提供了理论框架和实用算法，能够在准确率和延迟之间实现最优权衡，并通过整数化方法在实际系统中实现。

Abstract: We consider a single large language model (LLM) server that serves a heterogeneous stream of queries belonging to $N$ distinct task types. Queries arrive according to a Poisson process, and each type occurs with a known prior probability. For each task type, the server allocates a fixed number of internal thinking tokens, which determines the computational effort devoted to that query. The token allocation induces an accuracy-latency trade-off: the service time follows an approximately affine function of the allocated tokens, while the probability of a correct response exhibits diminishing returns. Under a first-in, first-out (FIFO) service discipline, the system operates as an $M/G/1$ queue, and the mean system time depends on the first and second moments of the resulting service-time distribution. We formulate a constrained optimization problem that maximizes a weighted average accuracy objective penalized by the mean system time, subject to architectural token-budget constraints and queue-stability conditions. The objective function is shown to be strictly concave over the stability region, which ensures existence and uniqueness of the optimal token allocation. The first-order optimality conditions yield a coupled projected fixed-point characterization of the optimum, together with an iterative solution and an explicit sufficient condition for contraction. Moreover, a projected gradient method with a computable global step-size bound is developed to guarantee convergence beyond the contractive regime. Finally, integer-valued token allocations are attained via rounding of the continuous solution, and the resulting performance loss is evaluated in simulation results.

</details>


### [55] [SPIKE: Sparse Koopman Regularization for Physics-Informed Neural Networks](https://arxiv.org/abs/2601.10282)
*Jose Marie Antonio Minoza*

Main category: cs.LG

TL;DR: SPIKE框架通过将连续时间Koopman算子与PINNs结合，利用稀疏线性动力学表示来提升PINNs的泛化能力和外推性能。


<details>
  <summary>Details</summary>
Motivation: PINNs在训练域内容易过拟合，在时空区域外推时泛化能力差，需要更好的正则化方法来学习简洁的动力学表示。

Method: 提出SPIKE框架，将连续时间Koopman算子与PINNs结合，在学习的可观测空间中强制线性动力学dz/dt=Az，并通过L1正则化实现稀疏性，利用矩阵指数积分确保刚性系统的无条件稳定性。

Result: 在抛物线、双曲线、色散和刚性PDE以及流体动力学和混沌ODE等多种问题上，SPIKE在时间外推、空间泛化和长期预测准确性方面均表现出持续改进。

Conclusion: SPIKE通过结合Koopman算子的稀疏线性表示，有效解决了PINNs的过拟合问题，提升了外推泛化能力，为复杂动力学的低维结构学习提供了有效框架。

Abstract: Physics-Informed Neural Networks (PINNs) provide a mesh-free approach for solving differential equations by embedding physical constraints into neural network training. However, PINNs tend to overfit within the training domain, leading to poor generalization when extrapolating beyond trained spatiotemporal regions. This work presents SPIKE (Sparse Physics-Informed Koopman-Enhanced), a framework that regularizes PINNs with continuous-time Koopman operators to learn parsimonious dynamics representations. By enforcing linear dynamics $dz/dt = Az$ in a learned observable space, both PIKE (without explicit sparsity) and SPIKE (with L1 regularization on $A$) learn sparse generator matrices, embodying the parsimony principle that complex dynamics admit low-dimensional structure. Experiments across parabolic, hyperbolic, dispersive, and stiff PDEs, including fluid dynamics (Navier-Stokes) and chaotic ODEs (Lorenz), demonstrate consistent improvements in temporal extrapolation, spatial generalization, and long-term prediction accuracy. The continuous-time formulation with matrix exponential integration provides unconditional stability for stiff systems while avoiding diagonal dominance issues inherent in discrete-time Koopman operators.

</details>


### [56] [We Need a More Robust Classifier: Dual Causal Learning Empowers Domain-Incremental Time Series Classification](https://arxiv.org/abs/2601.10312)
*Zhipeng Liu,Peibo Duan,Xuan Tang,Haodong Jing,Mingyang Geng,Yongsheng Huang,Jialu Xu,Bin Zhang,Binwu Wang*

Main category: cs.LG

TL;DR: 提出DualCD框架，通过双因果解耦增强时间序列分类模型在领域增量学习中的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有时间序列分类研究在领域增量学习方面面临挑战，需要提升模型在领域变化场景下的鲁棒性

Method: 提出轻量级双因果解耦框架DualCD：1) 时间特征解耦模块分离类因果特征和虚假特征；2) 双因果干预机制消除类内和类间混淆特征影响

Result: 在多个数据集和模型上的实验表明，DualCD能有效提升领域增量场景下的性能，并建立了全面的基准测试

Conclusion: DualCD框架为领域增量时间序列分类提供了有效的解决方案，通过因果解耦和干预机制增强了模型的鲁棒性

Abstract: The World Wide Web thrives on intelligent services that rely on accurate time series classification, which has recently witnessed significant progress driven by advances in deep learning. However, existing studies face challenges in domain incremental learning. In this paper, we propose a lightweight and robust dual-causal disentanglement framework (DualCD) to enhance the robustness of models under domain incremental scenarios, which can be seamlessly integrated into time series classification models. Specifically, DualCD first introduces a temporal feature disentanglement module to capture class-causal features and spurious features. The causal features can offer sufficient predictive power to support the classifier in domain incremental learning settings. To accurately capture these causal features, we further design a dual-causal intervention mechanism to eliminate the influence of both intra-class and inter-class confounding features. This mechanism constructs variant samples by combining the current class's causal features with intra-class spurious features and with causal features from other classes. The causal intervention loss encourages the model to accurately predict the labels of these variant samples based solely on the causal features. Extensive experiments on multiple datasets and models demonstrate that DualCD effectively improves performance in domain incremental scenarios. We summarize our rich experiments into a comprehensive benchmark to facilitate research in domain incremental time series classification.

</details>


### [57] [Meta Dynamic Graph for Traffic Flow Prediction](https://arxiv.org/abs/2601.10328)
*Yiqing Zou,Hanning Yuan,Qianyu Yang,Ziqiang Yuan,Shuliang Wang,Sijie Ruan*

Main category: cs.LG

TL;DR: 提出MetaDG框架，通过动态图结构建模时空动态性，统一处理时空异质性，提升交通流预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有交通流预测方法存在两个局限：1) 动态建模通常局限于空间拓扑变化；2) 时空异质性建模通常是分离的。需要更全面的动态建模方法来统一处理时空动态性和异质性。

Method: 提出MetaDG框架，利用节点表示的动态图结构显式建模时空动态性，生成动态邻接矩阵和元参数，将动态建模扩展到拓扑之外，并将时空异质性捕获统一到单一维度。

Result: 在四个真实世界数据集上的大量实验验证了MetaDG的有效性。

Conclusion: MetaDG通过动态图结构建模时空动态性，统一处理时空异质性，为交通流预测提供了有效的解决方案。

Abstract: Traffic flow prediction is a typical spatio-temporal prediction problem and has a wide range of applications. The core challenge lies in modeling the underlying complex spatio-temporal dependencies. Various methods have been proposed, and recent studies show that the modeling of dynamics is useful to meet the core challenge. While handling spatial dependencies and temporal dependencies using separate base model structures may hinder the modeling of spatio-temporal correlations, the modeling of dynamics can bridge this gap. Incorporating spatio-temporal heterogeneity also advances the main goal, since it can extend the parameter space and allow more flexibility. Despite these advances, two limitations persist: 1) the modeling of dynamics is often limited to the dynamics of spatial topology (e.g., adjacency matrix changes), which, however, can be extended to a broader scope; 2) the modeling of heterogeneity is often separated for spatial and temporal dimensions, but this gap can also be bridged by the modeling of dynamics. To address the above limitations, we propose a novel framework for traffic prediction, called Meta Dynamic Graph (MetaDG). MetaDG leverages dynamic graph structures of node representations to explicitly model spatio-temporal dynamics. This generates both dynamic adjacency matrices and meta-parameters, extending dynamic modeling beyond topology while unifying the capture of spatio-temporal heterogeneity into a single dimension. Extensive experiments on four real-world datasets validate the effectiveness of MetaDG.

</details>


### [58] [SuS: Strategy-aware Surprise for Intrinsic Exploration](https://arxiv.org/abs/2601.10349)
*Mark Kashirskiy,Ilya Makarov*

Main category: cs.LG

TL;DR: 提出Strategy-aware Surprise (SuS)框架，使用策略前后预测不匹配作为强化学习探索的新颖性信号，在数学推理任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统好奇心驱动方法仅依赖状态预测误差，缺乏对行为策略一致性和策略层面意外结果的考量，限制了探索效率和多样性。

Method: 引入两个互补组件：策略稳定性(SS)衡量时间步间行为策略的一致性，策略惊喜(SuS)捕捉相对于当前策略表示的意外结果，通过学习权重系数结合两种信号。

Result: 在数学推理任务上，SuS相比基线方法在Pass@1提升17.4%，Pass@5提升26.4%，同时保持更高的策略多样性；消融研究表明移除任一组件会导致至少10%的性能下降。

Conclusion: SuS框架通过策略层面的新颖性信号有效提升强化学习探索性能，两个组件的协同作用对性能提升至关重要，为基于大语言模型的推理任务提供了有效的内在动机机制。

Abstract: We propose Strategy-aware Surprise (SuS), a novel intrinsic motivation framework that uses pre-post prediction mismatch as a novelty signal for exploration in reinforcement learning. Unlike traditional curiosity-driven methods that rely solely on state prediction error, SuS introduces two complementary components: Strategy Stability (SS) and Strategy Surprise (SuS). SS measures consistency in behavioral strategy across temporal steps, while SuS captures unexpected outcomes relative to the agent's current strategy representation. Our combined reward formulation leverages both signals through learned weighting coefficients. We evaluate SuS on mathematical reasoning tasks using large language models, demonstrating significant improvements in both accuracy and solution diversity. Ablation studies confirm that removing either component results in at least 10% performance degradation, validating the synergistic nature of our approach. SuS achieves 17.4% improvement in Pass@1 and 26.4% improvement in Pass@5 compared to baseline methods, while maintaining higher strategy diversity throughout training.

</details>


### [59] [EvoMorph: Counterfactual Explanations for Continuous Time-Series Extrinsic Regression Applied to Photoplethysmography](https://arxiv.org/abs/2601.10356)
*Mesut Ceylan,Alexis Tabin,Patrick Langer,Elgar Fleisch,Filipe Barata*

Main category: cs.LG

TL;DR: 提出EvoMorph进化框架，为连续生物医学时间序列回归任务生成生理可信的形态感知反事实解释，支持临床不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备产生大量PPG信号用于临床评估，但现有反事实解释方法主要针对分类任务，忽略波形形态，且生成的信号生理不可信，限制了在连续生物医学时间序列中的应用。

Method: 提出EvoMorph多目标进化框架，优化基于可解释信号描述符的形态感知目标函数，应用变换保持波形结构，生成生理可信且多样的反事实解释。

Result: 在三个PPG数据集（心率、呼吸率、血氧饱和度）上评估，优于最近非相似邻居基线；案例研究表明EvoMorph可作为不确定性量化工具，将反事实敏感性与bootstrap集成不确定性和数据密度测量相关联。

Conclusion: EvoMorph能够为连续生物医学信号生成生理感知的反事实解释，支持不确定性感知的可解释性，推进临床时间序列应用的可信模型分析。

Abstract: Wearable devices enable continuous, population-scale monitoring of physiological signals, such as photoplethysmography (PPG), creating new opportunities for data-driven clinical assessment. Time-series extrinsic regression (TSER) models increasingly leverage PPG signals to estimate clinically relevant outcomes, including heart rate, respiratory rate, and oxygen saturation. For clinical reasoning and trust, however, single point estimates alone are insufficient: clinicians must also understand whether predictions are stable under physiologically plausible variations and to what extent realistic, attainable changes in physiological signals would meaningfully alter a model's prediction. Counterfactual explanations (CFE) address these "what-if" questions, yet existing time series CFE generation methods are largely restricted to classification, overlook waveform morphology, and often produce physiologically implausible signals, limiting their applicability to continuous biomedical time series. To address these limitations, we introduce EvoMorph, a multi-objective evolutionary framework for generating physiologically plausible and diverse CFE for TSER applications. EvoMorph optimizes morphology-aware objectives defined on interpretable signal descriptors and applies transformations to preserve the waveform structure. We evaluated EvoMorph on three PPG datasets (heart rate, respiratory rate, and oxygen saturation) against a nearest-unlike-neighbor baseline. In addition, in a case study, we evaluated EvoMorph as a tool for uncertainty quantification by relating counterfactual sensitivity to bootstrap-ensemble uncertainty and data-density measures. Overall, EvoMorph enables the generation of physiologically-aware counterfactuals for continuous biomedical signals and supports uncertainty-aware interpretability, advancing trustworthy model analysis for clinical time-series applications.

</details>


### [60] [PLGC: Pseudo-Labeled Graph Condensation](https://arxiv.org/abs/2601.10358)
*Jay Nandy,Arnab Kumar Mondal,Anuj Rathore,Mahesh Chandran*

Main category: cs.LG

TL;DR: PLGC是一种无需真实标签的自监督图压缩方法，通过构建潜在伪标签来匹配原图的结构和特征统计，在标签稀缺、噪声或不一致时表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有图压缩方法依赖干净、有监督的标签，但在标签稀缺、噪声或不一致时可靠性受限。需要一种无需真实标签的鲁棒图压缩方法。

Method: 提出伪标签图压缩(PLGC)框架：1) 从节点嵌入构建潜在伪标签；2) 联合学习潜在原型和节点分配；3) 优化压缩图以匹配原图的结构和特征统计，无需真实标签。

Result: 在节点分类和链接预测任务中，PLGC在干净数据集上与最先进的有监督压缩方法性能相当，在标签噪声下表现出显著鲁棒性，通常大幅超越所有基线方法。

Conclusion: 自监督图压缩在噪声或弱标签环境中具有实际和理论优势，PLGC为标签不可靠场景提供了有效的解决方案。

Abstract: Large graph datasets make training graph neural networks (GNNs) computationally costly. Graph condensation methods address this by generating small synthetic graphs that approximate the original data. However, existing approaches rely on clean, supervised labels, which limits their reliability when labels are scarce, noisy, or inconsistent. We propose Pseudo-Labeled Graph Condensation (PLGC), a self-supervised framework that constructs latent pseudo-labels from node embeddings and optimizes condensed graphs to match the original graph's structural and feature statistics -- without requiring ground-truth labels. PLGC offers three key contributions: (1) A diagnosis of why supervised condensation fails under label noise and distribution shift. (2) A label-free condensation method that jointly learns latent prototypes and node assignments. (3) Theoretical guarantees showing that pseudo-labels preserve latent structural statistics of the original graph and ensure accurate embedding alignment. Empirically, across node classification and link prediction tasks, PLGC achieves competitive performance with state-of-the-art supervised condensation methods on clean datasets and exhibits substantial robustness under label noise, often outperforming all baselines by a significant margin. Our findings highlight the practical and theoretical advantages of self-supervised graph condensation in noisy or weakly-labeled environments.

</details>


### [61] [Discrete Feynman-Kac Correctors](https://arxiv.org/abs/2601.10403)
*Mohsin Hasan,Viktor Ohanesian,Artem Gazizov,Yoshua Bengio,Alán Aspuru-Guzik,Roberto Bondesan,Marta Skreta,Kirill Neklyudov*

Main category: cs.LG

TL;DR: 提出Discrete Feynman-Kac Correctors框架，通过Sequential Monte Carlo算法在推理时控制离散扩散模型的生成分布，无需额外训练或微调。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型虽然能捕捉数据中的层次非序列依赖关系，但其自定义的去噪/去掩码过程缺乏对生成样本分布的灵活控制能力。

Method: 提出Discrete Feynman-Kac Correctors框架，基于Sequential Monte Carlo算法，在已训练的离散扩散模型基础上实现：1) 分布温度控制（退火）；2) 多个扩散过程边缘分布的乘积采样；3) 边缘分布与外部奖励函数的乘积采样。

Result: 框架在多个应用中展示效用：1) 伊辛模型退火玻尔兹曼分布的高效采样；2) 代码生成和摊销学习语言模型性能提升；3) 奖励倾斜的蛋白质序列生成。

Conclusion: 该框架为离散扩散模型提供了推理时的灵活分布控制能力，无需额外训练，在多种应用中展示了实用价值。

Abstract: Discrete diffusion models have recently emerged as a promising alternative to the autoregressive approach for generating discrete sequences. Sample generation via gradual denoising or demasking processes allows them to capture hierarchical non-sequential interdependencies in the data. These custom processes, however, do not assume a flexible control over the distribution of generated samples. We propose Discrete Feynman-Kac Correctors, a framework that allows for controlling the generated distribution of discrete masked diffusion models at inference time. We derive Sequential Monte Carlo (SMC) algorithms that, given a trained discrete diffusion model, control the temperature of the sampled distribution (i.e. perform annealing), sample from the product of marginals of several diffusion processes (e.g. differently conditioned processes), and sample from the product of the marginal with an external reward function, producing likely samples from the target distribution that also have high reward. Notably, our framework does not require any training of additional models or fine-tuning of the original model. We illustrate the utility of our framework in several applications including: efficient sampling from the annealed Boltzmann distribution of the Ising model, improving the performance of language models for code generation and amortized learning, as well as reward-tilted protein sequence generation.

</details>


### [62] [CS-GBA: A Critical Sample-based Gradient-guided Backdoor Attack for Offline Reinforcement Learning](https://arxiv.org/abs/2601.10407)
*Yuanjie Zhao,Junnan Qiu,Yue Ding,Jie Li*

Main category: cs.LG

TL;DR: 提出CS-GBA攻击框架，针对安全约束的离线RL算法，通过关键样本选择、相关性破坏触发器和梯度引导动作生成，在5%污染预算下实现高隐蔽性和破坏性攻击。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击策略在对抗安全约束算法（如CQL）时效果不佳，因为随机污染效率低且使用容易被检测的OOD触发器。离线RL虽然能从静态数据集优化策略，但天生易受后门攻击。

Method: 1. 自适应关键样本选择：基于TD误差理论洞察，集中攻击预算于对价值函数收敛最关键的状态转移；2. 相关性破坏触发器：利用状态特征的物理互斥性（如95%分位数边界）保持统计隐蔽性；3. 梯度引导动作生成：使用受害者Q网络的梯度在数据流形内搜索最坏动作，替代传统的标签反转。

Result: 在D4RL基准测试中，该方法显著优于现有最先进基线，在仅5%污染预算下对代表性安全约束算法实现高攻击成功率，同时在干净环境中保持智能体性能。

Conclusion: CS-GBA框架成功解决了现有后门攻击在对抗安全约束离线RL算法时的局限性，通过理论驱动的关键样本选择和统计隐蔽的触发器设计，实现了高效且隐蔽的后门攻击。

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to backdoor attacks. Existing attack strategies typically struggle against safety-constrained algorithms (e.g., CQL) due to inefficient random poisoning and the use of easily detectable Out-of-Distribution (OOD) triggers. In this paper, we propose CS-GBA (Critical Sample-based Gradient-guided Backdoor Attack), a novel framework designed to achieve high stealthiness and destructiveness under a strict budget. Leveraging the theoretical insight that samples with high Temporal Difference (TD) errors are pivotal for value function convergence, we introduce an adaptive Critical Sample Selection strategy that concentrates the attack budget on the most influential transitions. To evade OOD detection, we propose a Correlation-Breaking Trigger mechanism that exploits the physical mutual exclusivity of state features (e.g., 95th percentile boundaries) to remain statistically concealed. Furthermore, we replace the conventional label inversion with a Gradient-Guided Action Generation mechanism, which searches for worst-case actions within the data manifold using the victim Q-network's gradient. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms state-of-the-art baselines, achieving high attack success rates against representative safety-constrained algorithms with a minimal 5% poisoning budget, while maintaining the agent's performance in clean environments.

</details>


### [63] [DeFlow: Decoupling Manifold Modeling and Value Maximization for Offline Policy Extraction](https://arxiv.org/abs/2601.10471)
*Zhancun Mu*

Main category: cs.LG

TL;DR: DeFlow是一个解耦的离线强化学习框架，利用流匹配技术捕捉复杂行为流形，通过轻量级精炼模块在信任区域内优化，避免求解器微分和损失平衡问题。


<details>
  <summary>Details</summary>
Motivation: 优化生成策略通常需要通过ODE求解器进行反向传播，计算成本高昂。现有方法要么牺牲迭代生成能力进行单步蒸馏，要么面临求解器微分和损失平衡的稳定性问题。

Method: 提出解耦的离线RL框架，使用流匹配捕捉行为流形。在流形信任区域内学习轻量级精炼模块，而不是通过单步蒸馏牺牲迭代生成能力。这种方法避免了求解器微分，消除了损失平衡需求。

Result: 在具有挑战性的OGBench基准测试中取得优越性能，并展示了高效的离线到在线适应能力。

Conclusion: DeFlow通过解耦设计和信任区域内的轻量级精炼，在保持流迭代表达能力的同时实现了稳定改进，为离线RL提供了高效且性能优越的解决方案。

Abstract: We present DeFlow, a decoupled offline RL framework that leverages flow matching to faithfully capture complex behavior manifolds. Optimizing generative policies is computationally prohibitive, typically necessitating backpropagation through ODE solvers. We address this by learning a lightweight refinement module within an explicit, data-derived trust region of the flow manifold, rather than sacrificing the iterative generation capability via single-step distillation. This way, we bypass solver differentiation and eliminate the need for balancing loss terms, ensuring stable improvement while fully preserving the flow's iterative expressivity. Empirically, DeFlow achieves superior performance on the challenging OGBench benchmark and demonstrates efficient offline-to-online adaptation.

</details>


### [64] [Communication-Efficient Federated Learning by Exploiting Spatio-Temporal Correlations of Gradients](https://arxiv.org/abs/2601.10491)
*Shenlong Zheng,Zhen Zhang,Yuhui Deng,Geyong Min,Lin Cui*

Main category: cs.LG

TL;DR: GradESTC是一种联邦学习压缩技术，利用梯度的空间和时间相关性来显著减少通信开销，相比最强基线平均减少39.79%的上行通信。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在带宽受限网络中的通信开销是关键挑战。现有方法大多只关注压缩单个梯度，忽略了梯度之间的时间相关性。研究发现梯度不仅具有空间相关性（低秩结构），在相邻轮次间还存在强时间相关性。

Method: GradESTC利用空间相关性将完整梯度分解为紧凑的基础向量和组合系数，同时利用时间相关性，每轮只需动态更新少量基础向量。通过传输轻量级组合系数和有限数量的更新基础向量来替代完整梯度传输。

Result: 实验表明，在达到接近收敛的目标精度水平时，GradESTC相比最强基线平均减少39.79%的上行通信，同时保持与未压缩FedAvg相当的收敛速度和最终精度。

Conclusion: 通过有效利用梯度的时空结构，GradESTC为通信高效的联邦学习提供了实用且可扩展的解决方案。

Abstract: Communication overhead is a critical challenge in federated learning, particularly in bandwidth-constrained networks. Although many methods have been proposed to reduce communication overhead, most focus solely on compressing individual gradients, overlooking the temporal correlations among them. Prior studies have shown that gradients exhibit spatial correlations, typically reflected in low-rank structures. Through empirical analysis, we further observe a strong temporal correlation between client gradients across adjacent rounds. Based on these observations, we propose GradESTC, a compression technique that exploits both spatial and temporal gradient correlations. GradESTC exploits spatial correlations to decompose each full gradient into a compact set of basis vectors and corresponding combination coefficients. By exploiting temporal correlations, only a small portion of the basis vectors need to be dynamically updated in each round. GradESTC significantly reduces communication overhead by transmitting lightweight combination coefficients and a limited number of updated basis vectors instead of the full gradients. Extensive experiments show that, upon reaching a target accuracy level near convergence, GradESTC reduces uplink communication by an average of 39.79% compared to the strongest baseline, while maintaining comparable convergence speed and final accuracy to uncompressed FedAvg. By effectively leveraging spatio-temporal gradient structures, GradESTC offers a practical and scalable solution for communication-efficient federated learning.

</details>


### [65] [Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning](https://arxiv.org/abs/2601.10498)
*Nilin Abrahamsen*

Main category: cs.LG

TL;DR: PROMA是一种用于大语言模型微调的近端策略更新方法，通过投影去除序列梯度分量后进行微批次累积，实现更稳定的策略学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法如GRPO在控制局部KL散度方面不够严格，PPO和GRPO可能导致熵崩溃或依赖参考策略，需要一种更稳定且不依赖参考策略的近端更新方法。

Method: PROMA在反向传播过程中逐层投影去除序列梯度分量，然后在微批次间累积策略梯度，无需额外前向或反向传播，实现高效实现。

Result: PROMA比GRPO能更严格地控制局部KL散度，实现更稳定的策略学习，且不会像PPO和GRPO那样导致熵崩溃，也不依赖参考策略或似然比裁剪。

Conclusion: PROMA提供了一种有效的近端策略更新方法，通过梯度投影和微批次累积实现稳定的大语言模型微调，解决了现有方法的局限性。

Abstract: This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, resulting in more stable policy learning. Unlike PPO and GRPO, PROMA achieves proximal updates without inducing entropy collapse and does not rely on a reference policy or likelihood-ratio clipping.

</details>


### [66] [Transformer-Based Cognitive Radio: Adaptive Modulation Strategies Using Transformer Models](https://arxiv.org/abs/2601.10519)
*Andrea Melis,Andrea Piroddi,Roberto Girau*

Main category: cs.LG

TL;DR: 该研究探索使用GPT-2 Transformer模型生成新型调制方案，与传统方法相比，在SNR和PSD等性能指标上表现相当甚至更优，展示了Transformer模型在增强认知无线电系统方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 认知无线电系统需要动态适应频谱环境变化，机器学习技术特别是Transformer模型可以提升其频谱效率、鲁棒性和安全性。传统调制方案可能无法充分利用动态频谱环境，需要创新方法来生成更优的调制方案。

Method: 使用GPT-2 Transformer架构，在现有调制公式数据集上进行训练，生成新的调制方案。然后将生成的调制方案与传统方法在关键性能指标（如信噪比SNR和功率谱密度PSD）上进行比较评估。

Result: Transformer生成的调制方案在性能上与传统方法相当，在某些情况下甚至优于传统方法。这表明Transformer模型能够生成有效的调制方案，为认知无线电系统提供性能改进。

Conclusion: Transformer模型在生成新型调制方案方面具有显著潜力，能够增强认知无线电系统的效率、鲁棒性和安全性，为未来无线通信系统的发展提供了有前景的技术路径。

Abstract: Cognitive Radio (CR) systems, which dynamically adapt to changing spectrum environments, could benefit significantly from advancements in machine learning technologies. These systems can be enhanced in terms of spectral efficiency, robustness, and security through innovative approaches such as the use of Transformer models. This work investigates the application of Transformer models, specifically the GPT-2 architecture, to generate novel modulation schemes for wireless communications. By training a GPT-2 model on a dataset of existing modulation formulas, new modulation schemes has been created. These generated schemes are then compared to traditional methods using key performance metrics such as Signal-to-Noise Ratio (SNR) and Power Spectrum Density (PSD). The results show that Transformer-generated modulation schemes can achieve performance comparable to, and in some cases outperforming, traditional methods. This demonstrates that advanced CR systems could greatly benefit from the implementation of Transformer models, leading to more efficient, robust, and secure communication systems.

</details>


### [67] [Mixtures of Transparent Local Models](https://arxiv.org/abs/2601.10541)
*Niffa Cheick Oumar Diaby,Thierry Duchesne,Mario Marchand*

Main category: cs.LG

TL;DR: 该论文提出了一种混合局部透明模型的方法，通过结合多个简单透明的局部模型来提高整体模型的透明度和可解释性，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型在人类活动各领域的广泛应用，对模型透明度的需求日益增长。模型透明度有助于识别安全性和非歧视性等重要因素。然而，在某些情况下，简单的透明函数只能在输入空间的某些局部区域有效建模，而在不同区域之间可能发生突变。

Method: 提出了一种混合局部透明模型的方法，通过学习透明的标记函数以及该函数在其分配区域内风险较小的输入空间局部区域。使用新的多预测器（多局部）损失函数，为二元线性分类问题和线性回归问题建立了严格的PAC-Bayesian风险界限。

Result: 在合成数据集上展示了学习算法的工作原理，在真实数据集上的结果表明，该方法与其他现有方法以及某些不透明模型相比具有竞争力。

Conclusion: 混合局部透明模型提供了一种有效的可解释模型设计替代方案，能够在保持透明度的同时处理输入空间中不同区域可能存在的函数突变问题，并通过理论风险界限提供了理论保证。

Abstract: The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the label of instances in some localities/regions of the input space, but may change abruptly as we move from one locality to another. Consequently, the proposed algorithm is to learn both the transparent labeling function and the locality of the input space where the labeling function achieves a small risk in its assigned locality. By using a new multi-predictor (and multi-locality) loss function, we established rigorous PAC-Bayesian risk bounds for the case of binary linear classification problem and that of linear regression. In both cases, synthetic data sets were used to illustrate how the learning algorithms work. The results obtained from real data sets highlight the competitiveness of our approach compared to other existing methods as well as certain opaque models. Keywords: PAC-Bayes, risk bounds, local models, transparent models, mixtures of local transparent models.

</details>


### [68] [Process-Guided Concept Bottleneck Model](https://arxiv.org/abs/2601.10562)
*Reza M. Asiyabi,SEOSAW Partnership,Steven Hancock,Casey Ryan*

Main category: cs.LG

TL;DR: PG-CBM扩展概念瓶颈模型，通过领域定义的因果机制约束学习，利用生物物理意义中间概念，在科学应用中提高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准概念瓶颈模型(CBMs)存在三个主要问题：1) 忽略领域特定关系和因果机制；2) 依赖完整概念标签，在科学领域监督稀疏时适用性受限；3) 科学领域虽然监督稀疏但过程定义明确，需要利用这种领域知识。

Method: 提出过程引导概念瓶颈模型(PG-CBM)，通过领域定义的因果机制约束学习过程，利用生物物理意义的中间概念，能够处理多源异构训练数据，产生可解释的中间输出。

Result: 使用地球观测数据的地上生物量密度估计作为案例研究，PG-CBM相比多个基准方法减少了误差和偏差，同时利用多源异构数据并产生可解释的中间输出。

Conclusion: PG-CBM不仅提高了准确性，还增强了透明度，能够检测虚假学习，提供科学洞见，代表了科学应用中更可信AI系统的一步。

Abstract: Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.

</details>


### [69] [Kolmogorov Arnold Networks and Multi-Layer Perceptrons: A Paradigm Shift in Neural Modelling](https://arxiv.org/abs/2601.10563)
*Aradhya Gaonkar,Nihal Jain,Vignesh Chougule,Nikhil Deshpande,Sneha Varur,Channabasappa Muttal*

Main category: cs.LG

TL;DR: KANs在非线性函数逼近、时间序列预测和多变量分类任务中全面超越MLPs，具有更高的预测精度和显著降低的计算成本，特别适合资源受限和实时应用场景。


<details>
  <summary>Details</summary>
Motivation: 传统多层感知器（MLP）在计算效率和准确性之间存在权衡问题，特别是在资源受限环境中。研究旨在探索基于Kolmogorov表示定理的Kolmogorov-Arnold网络（KAN）是否能提供更好的性能平衡，为特定任务选择最合适的神经网络架构提供系统框架。

Method: 采用综合比较分析方法，使用多种数据集（包括二次和三次数学函数估计、日常温度预测、葡萄酒分类等），通过均方误差（MSE）评估模型精度，通过浮点运算（FLOPs）评估计算成本。KANs采用自适应样条基激活函数和网格结构，与传统的MLP架构进行对比。

Result: 在所有基准测试中，KANs都可靠地超越了MLPs，获得了更高的预测精度，同时显著降低了计算成本。KANs在计算效率和准确性之间保持了更好的平衡。

Conclusion: KANs在资源受限和实时操作环境中特别有益，为智能系统的发展提供了变革性能力。研究阐明了KANs和MLPs之间的架构和功能差异，为特定任务选择最合适的神经网络架构提供了系统框架，特别是在需要可解释性和计算效率的场景中。

Abstract: The research undertakes a comprehensive comparative analysis of Kolmogorov-Arnold Networks (KAN) and Multi-Layer Perceptrons (MLP), highlighting their effectiveness in solving essential computational challenges like nonlinear function approximation, time-series prediction, and multivariate classification. Rooted in Kolmogorov's representation theorem, KANs utilize adaptive spline-based activation functions and grid-based structures, providing a transformative approach compared to traditional neural network frameworks. Utilizing a variety of datasets spanning mathematical function estimation (quadratic and cubic) to practical uses like predicting daily temperatures and categorizing wines, the proposed research thoroughly assesses model performance via accuracy measures like Mean Squared Error (MSE) and computational expense assessed through Floating Point Operations (FLOPs). The results indicate that KANs reliably exceed MLPs in every benchmark, attaining higher predictive accuracy with significantly reduced computational costs. Such an outcome highlights their ability to maintain a balance between computational efficiency and accuracy, rendering them especially beneficial in resource-limited and real-time operational environments. By elucidating the architectural and functional distinctions between KANs and MLPs, the paper provides a systematic framework for selecting the most suitable neural architectures for specific tasks. Furthermore, the proposed study highlights the transformative capabilities of KANs in progressing intelligent systems, influencing their use in situations that require both interpretability and computational efficiency.

</details>


### [70] [Combinatorial Optimization Augmented Machine Learning](https://arxiv.org/abs/2601.10583)
*Maximilian Schiffer,Heiko Hoppe,Yue Su,Louis Bouvier,Axel Parmentier*

Main category: cs.LG

TL;DR: COAML综述：将组合优化与机器学习结合的新范式，通过嵌入优化器构建数据驱动且保持可行性的策略，涵盖静态/动态问题、模仿学习、强化学习等应用。


<details>
  <summary>Details</summary>
Motivation: 组合优化增强机器学习（COAML）作为新兴范式，旨在整合预测模型与组合决策，构建既数据驱动又保持可行性的策略，弥合机器学习、运筹学和随机优化之间的鸿沟。

Method: 提出统一的COAML框架，建立问题分类法（基于不确定性和决策结构），综述静态/动态问题的算法方法，涵盖调度、车辆路径、随机规划、强化学习等应用领域。

Result: 系统梳理了COAML领域的最新进展，建立了方法论框架，识别了关键研究前沿，为组合优化与机器学习交叉领域提供了全面的教程式介绍和研究路线图。

Conclusion: COAML是连接组合优化与机器学习的有力范式，本文综述旨在为该领域提供入门教程和未来研究路线图，推动两个领域的深度融合与创新。

Abstract: Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state of the art in COAML. We introduce a unifying framework for COAML pipelines, describe their methodological building blocks, and formalize their connection to empirical cost minimization. We then develop a taxonomy of problem settings based on the form of uncertainty and decision structure. Using this taxonomy, we review algorithmic approaches for static and dynamic problems, survey applications across domains such as scheduling, vehicle routing, stochastic programming, and reinforcement learning, and synthesize methodological contributions in terms of empirical cost minimization, imitation learning, and reinforcement learning. Finally, we identify key research frontiers. This survey aims to serve both as a tutorial introduction to the field and as a roadmap for future research at the interface of combinatorial optimization and machine learning.

</details>


### [71] [ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition](https://arxiv.org/abs/2601.10591)
*Arundeep Chinta,Lucas Vinh Tran,Jay Katukuri*

Main category: cs.LG

TL;DR: 本文提出了ProbFM，一种基于深度证据回归的Transformer概率框架，首次为时间序列基础模型提供了理论基础的预测不确定性分解，解决了金融应用中不确定性量化的核心挑战。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型在金融预测中存在不确定性量化的根本局限性：要么依赖限制性分布假设，要么混淆不同不确定性来源，或缺乏原则性校准机制。现有方法无法提供理论基础的预测不确定性分解，阻碍了其在金融应用中的采用。

Method: 提出了ProbFM（概率基础模型），这是一个基于Transformer的概率框架，利用深度证据回归提供原则性不确定性量化，具有明确的认知-偶然不确定性分解。该方法通过高阶证据学习最优不确定性表示，同时保持单次计算效率。

Result: 在加密货币回报预测评估中，深度证据回归保持了竞争力的预测准确性，同时提供了明确的认知-偶然不确定性分解。通过使用一致的LSTM架构对五种概率方法进行对比研究，验证了深度证据回归的有效性。

Conclusion: 这项工作为金融应用中的基础模型建立了一个可扩展的原则性不确定性量化框架，并为深度证据回归在金融应用中的有效性提供了实证证据，解决了时间序列基础模型在不确定性量化方面的核心挑战。

Abstract: Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student's t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student's-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER's effectiveness in financial applications.

</details>


### [72] [STEM: Scaling Transformers with Embedding Modules](https://arxiv.org/abs/2601.10639)
*Ranajoy Sadhukhan,Sheng Cao,Harry Dong,Changsheng Zhao,Attiano Purpura-Pontoniere,Yuandong Tian,Zechun Liu,Beidi Chen*

Main category: cs.LG

TL;DR: STEM是一种静态、基于token索引的稀疏Transformer架构，通过嵌入查找替换FFN上投影，减少计算和参数访问，同时提升性能、可解释性和长上下文处理能力。


<details>
  <summary>Details</summary>
Motivation: 细粒度稀疏性虽然能提高参数容量而不增加按token计算量，但存在训练不稳定、负载均衡和通信开销等问题。需要一种既能提高参数容量又能保持训练稳定性和效率的方法。

Method: STEM采用静态、基于token索引的方法，用层局部嵌入查找替换FFN的上投影，同时保持门控和下投影密集。这种方法消除了运行时路由，支持CPU异步预取，并将容量与按token FLOPs和跨设备通信解耦。

Result: STEM在极端稀疏情况下也能稳定训练，相比密集基线提升下游性能，同时减少约三分之一的FFN参数。在350M和1B模型规模上，整体准确率提升约3-4%，在知识和推理密集型基准测试（ARC-Challenge、OpenBookQA、GSM8K、MMLU）上表现突出。

Conclusion: STEM是扩展参数内存的有效方法，提供更好的可解释性、训练稳定性和效率。其token索引特性支持知识编辑和注入，无需输入文本干预或额外计算，同时增强长上下文性能。

Abstract: Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances its knowledge storage capacity. More interestingly, this enhanced knowledge capacity comes with better interpretability. The token-indexed nature of STEM embeddings allows simple ways to perform knowledge editing and knowledge injection in an interpretable manner without any intervention in the input text or additional computation. In addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to ~3--4% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while providing better interpretability, better training stability and improved efficiency.

</details>


### [73] [Single-Stage Huffman Encoder for ML Compression](https://arxiv.org/abs/2601.10673)
*Aditya Agrawal,Albert Magyar,Hiteshwar Eswaraiah,Patrick Sheridan,Pradeep Janedula,Ravi Krishnan Venkatesan,Krishna Nair,Ravi Iyer*

Main category: cs.LG

TL;DR: 提出一种单阶段哈夫曼编码器，使用基于历史数据平均概率分布的固定码本，消除传统三阶段哈夫曼编码的计算、延迟和数据开销，实现接近理想压缩率的实时压缩。


<details>
  <summary>Details</summary>
Motivation: 大语言模型训练和服务中，多加速器间的集体操作常受网络带宽限制。传统无损哈夫曼编码需要实时频率分析、码本生成和传输，引入的计算、延迟和数据开销在延迟敏感场景（如芯片间通信）中不可接受。

Method: 提出单阶段哈夫曼编码器，使用从先前数据批次平均概率分布推导出的固定码本。分析Gemma 2B模型发现张量在不同层和分片间具有高度统计相似性，利用这一特性实现高效压缩。

Result: 压缩率接近每分片哈夫曼编码的0.5%以内，接近理想香农可压缩性的1%以内，能够实现高效的实时压缩。

Conclusion: 通过利用张量统计相似性和固定码本，单阶段哈夫曼编码器消除了传统方法在延迟敏感场景中的开销，为大语言模型分布式训练和服务提供了高效的压缩解决方案。

Abstract: Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design requiring on-the-fly frequency analysis, codebook generation and transmission of codebook along with data introduces computational, latency and data overheads which are prohibitive for latency-sensitive scenarios such as die-to-die communication. This paper proposes a single-stage Huffman encoder that eliminates these overheads by using fixed codebooks derived from the average probability distribution of previous data batches. Through our analysis of the Gemma 2B model, we demonstrate that tensors exhibit high statistical similarity across layers and shards. Using this approach we achieve compression within 0.5% of per-shard Huffman coding and within 1% of the ideal Shannon compressibility, enabling efficient on-the-fly compression.

</details>


### [74] [Data-driven stochastic reduced-order modeling of parametrized dynamical systems](https://arxiv.org/abs/2601.10690)
*Andrew F. Ilersich,Kevin Course,Prasanth B. Nair*

Main category: cs.LG

TL;DR: 提出基于摊销随机变分推理的数据驱动框架，学习连续时间随机降阶模型，能够泛化到未见参数组合和强迫条件，计算成本与数据集大小和系统刚度无关。


<details>
  <summary>Details</summary>
Motivation: 复杂动力系统在变化条件下的建模计算量大，现有降阶模型方法难以处理随机动力学且无法量化预测不确定性，限制了在鲁棒决策中的应用。

Method: 基于摊销随机变分推理，利用马尔可夫高斯过程的重参数化技巧，联合学习概率自编码器和控制潜在动力学的随机微分方程，无需训练期间使用计算昂贵的正向求解器。

Result: 在三个具有挑战性的测试问题上展示了出色的泛化能力（未见参数组合和强迫条件），相比现有方法显著提高了效率。

Conclusion: 该框架能够学习连续时间随机降阶模型，有效处理随机动力学并量化不确定性，计算成本低且可融入物理先验知识，为复杂动力系统的鲁棒决策提供了实用工具。

Abstract: Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.

</details>


### [75] [Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis](https://arxiv.org/abs/2601.10701)
*Chun Hei Michael Shiu,Chih Wei Ling*

Main category: cs.LG

TL;DR: 本文对CEPAM（通信高效且隐私可调机制）进行了理论分析和实验评估，该机制通过RSUQ量化器实现联邦学习中通信效率和隐私保护的平衡。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据治理约束下提供了隐私保护的协作途径，但仍面临通信效率和参与方之间隐私保护的关键挑战。CEPAM作为一种新颖方法，旨在同时实现这两个目标。

Method: CEPAM利用拒绝采样通用量化器（RSUQ），这是一种随机向量量化器，其量化误差等同于预设噪声，可调节以定制参与方之间的隐私保护。本文对CEPAM进行理论分析，包括隐私保证和收敛特性。

Result: 通过实验评估CEPAM的效用性能，包括与其他基线的收敛曲线比较，以及不同参与方之间的准确率-隐私权衡分析。

Conclusion: CEPAM在联邦学习中能够有效平衡通信效率和隐私保护，通过理论分析和实验验证了其在实际应用中的可行性和优势。

Abstract: Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM's utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.

</details>


### [76] [Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication](https://arxiv.org/abs/2601.10705)
*Keval Jain,Anant Raj,Saurav Prakash,Girish Varma*

Main category: cs.LG

TL;DR: 该论文研究了一种半异步客户端-服务器感知机训练方法，通过迭代参数混合（IPM-style averaging）处理联邦学习中的系统效应：延迟更新、部分参与和不完美通信，并提出了一种确定性的聚合规则来保证收敛。


<details>
  <summary>Details</summary>
Motivation: 研究联邦学习和分布式部署中的三个关键系统效应：(1) 由于模型交付延迟和客户端计算延迟导致的陈旧更新（双向版本滞后），(2) 部分参与（客户端间歇性可用性），(3) 下行和上行链路的不完美通信（建模为有界二阶矩的零均值加性噪声）。

Method: 提出了一种服务器端聚合规则——带填充的陈旧性桶聚合，该规则确定性地强制执行预定的陈旧性配置文件，而不假设延迟或参与的随机模型。在边缘可分离性和有界数据半径条件下，分析半异步客户端-服务器感知机训练。

Result: 证明了在给定服务器轮数内，感知机错误累积数的有限时间期望界：延迟的影响仅通过平均强制陈旧性体现，而通信噪声贡献一个随噪声能量平方根增长的额外项。在无噪声情况下，展示了有限期望错误预算如何在温和的新参与条件下产生明确的有限轮稳定界。

Conclusion: 该研究为联邦学习中的异步感知机训练提供了理论保证，表明通过适当的聚合策略可以控制延迟和噪声的影响，并在无噪声情况下实现有限轮收敛。

Abstract: We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.

</details>


### [77] [High-accuracy and dimension-free sampling with diffusions](https://arxiv.org/abs/2601.10708)
*Khashayar Gatmiry,Sitan Chen,Adil Salim*

Main category: cs.LG

TL;DR: 提出一种新的扩散模型求解器，通过低阶近似和配置方法的巧妙结合，将迭代复杂度从多项式降低到多对数级别，实现首个仅使用数据分布分数近似访问的"高精度"扩散采样器保证。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型的数值求解方法需要大量小步长迭代才能生成高质量样本，迭代复杂度随环境维度和精度倒数多项式增长，计算效率低。

Method: 提出新求解器，结合低阶近似和配置方法（Lee, Song, Vempala 2018），利用数据分布分数的近似访问，实现高效求解。

Result: 证明新求解器的迭代复杂度在精度倒数上呈多对数增长，不显式依赖环境维度，仅通过目标分布支撑集的有效半径影响复杂度。

Conclusion: 首次为仅使用数据分布分数近似访问的扩散采样器提供高精度保证，显著降低迭代复杂度，为扩散模型的高效采样提供理论支持。

Abstract: Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy'' guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.

</details>


### [78] [DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids](https://arxiv.org/abs/2601.10715)
*Navami Kairanda,Shanthika Naik,Marc Habermann,Avinash Sharma,Christian Theobalt,Vladislav Golyanik*

Main category: cs.LG

TL;DR: 提出DInf-Grid：一种基于可微分网格的表示方法，结合特征网格效率和径向基函数插值，用于高效求解微分方程，比坐标MLP方法快5-20倍


<details>
  <summary>Details</summary>
Motivation: 现有神经求解器存在效率问题：坐标MLP计算密集训练慢，网格方法依赖线性插值无法计算高阶导数。需要一种既能高效训练又能计算高阶导数的微分方程求解方法

Method: 结合特征网格效率和径向基函数插值（无限可微），引入多分辨率分解和共位网格，通过微分方程作为损失函数进行隐式训练

Result: 在泊松方程图像重建、亥姆霍兹方程波场、基尔霍夫-洛夫边界值问题布料模拟等任务验证，比坐标MLP方法快5-20倍，可在秒或分钟级别求解微分方程

Conclusion: DInf-Grid成功结合网格效率和径向基函数可微性，为物理场建模提供高效准确的微分方程求解方案，在保持精度和紧凑性的同时大幅提升计算速度

Abstract: We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [79] [Accelerated Regularized Wasserstein Proximal Sampling Algorithms](https://arxiv.org/abs/2601.09848)
*Hong Ye Tan,Stanley Osher,Wuchen Li*

Main category: stat.ML

TL;DR: 提出ARWP方法：使用二阶得分ODE（类似Nesterov加速）和正则化Wasserstein近端方法进行Gibbs分布采样，相比传统方法有更快的收敛速度和更好的探索能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于布朗运动的粒子采样方法收敛速度有限，需要更高效的采样算法。特别是对于复杂分布（如多模态、非对数凹分布），现有方法在探索能力和收敛速度方面存在不足。

Method: 提出加速正则化Wasserstein近端方法（ARWP）：1）使用二阶得分ODE而非布朗运动来演化粒子；2）采用正则化Wasserstein近端方法进行得分估计；3）结合Nesterov加速思想；4）在连续时间和离散时间框架下分析混合速率。

Result: 1）相比动能Langevin采样算法，ARWP在渐近时间区域有更高的收缩率；2）在低维实验中（多模态高斯混合、病态Rosenbrock分布）表现出结构化收敛粒子、加速离散时间混合和更快的尾部探索；3）在某些非对数凹贝叶斯神经网络任务中具有更好的泛化性能。

Conclusion: ARWP方法通过结合二阶加速机制和正则化Wasserstein近端技术，显著提升了采样效率和探索能力，特别适用于复杂分布采样任务，为贝叶斯推理和机器学习应用提供了更有效的工具。

Abstract: We consider sampling from a Gibbs distribution by evolving a finite number of particles using a particular score estimator rather than Brownian motion. To accelerate the particles, we consider a second-order score-based ODE, similar to Nesterov acceleration. In contrast to traditional kernel density score estimation, we use the recently proposed regularized Wasserstein proximal method, yielding the Accelerated Regularized Wasserstein Proximal method (ARWP). We provide a detailed analysis of continuous- and discrete-time non-asymptotic and asymptotic mixing rates for Gaussian initial and target distributions, using techniques from Euclidean acceleration and accelerated information gradients. Compared with the kinetic Langevin sampling algorithm, the proposed algorithm exhibits a higher contraction rate in the asymptotic time regime. Numerical experiments are conducted across various low-dimensional experiments, including multi-modal Gaussian mixtures and ill-conditioned Rosenbrock distributions. ARWP exhibits structured and convergent particles, accelerated discrete-time mixing, and faster tail exploration than the non-accelerated regularized Wasserstein proximal method and kinetic Langevin methods. Additionally, ARWP particles exhibit better generalization properties for some non-log-concave Bayesian neural network tasks.

</details>


### [80] [CROCS: A Two-Stage Clustering Framework for Behaviour-Centric Consumer Segmentation with Smart Meter Data](https://arxiv.org/abs/2601.10494)
*Luke W. Yerbury,Ricardo J. G. B. Campello,G. C. Livingston,Mark Goldsworthy,Lachlan O'Neil*

Main category: stat.ML

TL;DR: CROCS提出一个两阶段聚类框架，通过代表性负荷集和加权最小距离和来改进消费者细分，能更好处理异常、缺失数据和大规模部署。


<details>
  <summary>Details</summary>
Motivation: 随着可再生能源集成和电气化带来的不确定性增加，需求侧管理（特别是需求响应）变得日益重要。现有基于聚类的消费者细分方法存在不足：无法充分反映消费者行为多样性，依赖刚性时间对齐，对异常、缺失数据或大规模部署处理不佳。

Method: 提出CROCS两阶段聚类框架：第一阶段为每个消费者的日负荷曲线独立聚类，形成代表性负荷集（RLS），总结其典型日消费行为；第二阶段使用加权最小距离和（WSMD）这一新颖的集合间度量来比较RLS，考虑行为的普遍性和相似性；最后在WSMD诱导图上进行社区检测，揭示定义消费者群体的共享日行为原型。

Result: 在合成和真实澳大利亚智能电表数据集上的实验表明，CROCS能捕捉消费者内部变异性，发现同步和异步行为相似性，对异常和缺失数据保持鲁棒性，并通过自然并行化实现高效扩展。

Conclusion: CROCS框架能有效改进消费者细分，为设计更有效的需求侧管理和需求响应项目提供更好的信息基础，解决了现有方法在行为多样性、时间对齐和可扩展性方面的局限性。

Abstract: With grid operators confronting rising uncertainty from renewable integration and a broader push toward electrification, Demand-Side Management (DSM) -- particularly Demand Response (DR) -- has attracted significant attention as a cost-effective mechanism for balancing modern electricity systems. Unprecedented volumes of consumption data from a continuing global deployment of smart meters enable consumer segmentation based on real usage behaviours, promising to inform the design of more effective DSM and DR programs. However, existing clustering-based segmentation methods insufficiently reflect the behavioural diversity of consumers, often relying on rigid temporal alignment, and faltering in the presence of anomalies, missing data, or large-scale deployments.
  To address these challenges, we propose a novel two-stage clustering framework -- Clustered Representations Optimising Consumer Segmentation (CROCS). In the first stage, each consumer's daily load profiles are clustered independently to form a Representative Load Set (RLS), providing a compact summary of their typical diurnal consumption behaviours. In the second stage, consumers are clustered using the Weighted Sum of Minimum Distances (WSMD), a novel set-to-set measure that compares RLSs by accounting for both the prevalence and similarity of those behaviours. Finally, community detection on the WSMD-induced graph reveals higher-order prototypes that embody the shared diurnal behaviours defining consumer groups, enhancing the interpretability of the resulting clusters.
  Extensive experiments on both synthetic and real Australian smart meter datasets demonstrate that CROCS captures intra-consumer variability, uncovers both synchronous and asynchronous behavioural similarities, and remains robust to anomalies and missing data, while scaling efficiently through natural parallelisation. These results...

</details>


### [81] [Coarsening Causal DAG Models](https://arxiv.org/abs/2601.10531)
*Francisco Madaleno,Pratik Misra,Alex Markham*

Main category: stat.ML

TL;DR: 提出了一种学习抽象因果图的新方法，包括图识别理论、高效算法和搜索空间结构分析，并在合成和真实数据集上验证


<details>
  <summary>Details</summary>
Motivation: 有向无环图模型能有效表示因果关系，但在实际应用中，在给定特征的粒度上估计因果模型并不总是可行或理想的。需要研究因果抽象方法来处理这类问题

Method: 1) 提出新的图识别理论结果；2) 开发高效、可证明一致的算法，直接从干预数据中学习抽象因果图（干预目标未知）；3) 揭示搜索空间的格结构理论洞察

Result: 在合成和真实数据集（包括已知地面真实值的受控物理系统测量数据）上验证了算法的有效性

Conclusion: 该研究为因果抽象领域做出了贡献，提供了实用的图识别理论、高效学习算法和理论洞察，有助于更广泛地理解因果发现

Abstract: Directed acyclic graphical (DAG) models are a powerful tool for representing causal relationships among jointly distributed random variables, especially concerning data from across different experimental settings. However, it is not always practical or desirable to estimate a causal model at the granularity of given features in a particular dataset. There is a growing body of research on causal abstraction to address such problems. We contribute to this line of research by (i) providing novel graphical identifiability results for practically-relevant interventional settings, (ii) proposing an efficient, provably consistent algorithm for directly learning abstract causal graphs from interventional data with unknown intervention targets, and (iii) uncovering theoretical insights about the lattice structure of the underlying search space, with connections to the field of causal discovery more generally. As proof of concept, we apply our algorithm on synthetic and real datasets with known ground truths, including measurements from a controlled physical system with interacting light intensity and polarization.

</details>


### [82] [Parametric RDT approach to computational gap of symmetric binary perceptron](https://arxiv.org/abs/2601.10628)
*Mihailo Stojnic*

Main category: stat.ML

TL;DR: 本文通过完全提升随机对偶理论(fl-RDT)研究对称二元感知机(SBP)中的统计-计算间隙(SCG)，发现第二提升层级的结构变化与满足性阈值α_c和算法阈值α_a相关，表明存在非零计算间隙SCG=α_c-α_a。


<details>
  <summary>Details</summary>
Motivation: 研究对称二元感知机中是否存在统计-计算间隙，即理论上的满足性阈值与算法可达到的阈值之间的差距。这种间隙在复杂优化问题中普遍存在，理解其机制对算法设计有重要意义。

Method: 采用完全提升随机对偶理论(fl-RDT)的参数化方法，分析c序列的结构变化。通过不同提升层级（第二层和第七层）的估计分别对应满足性阈值α_c和算法阈值α_a。

Result: 对于标准SBP(κ=1)，第二层估计得到α_c≈1.8159，第七层估计得到α_a≈1.6021（有向~1.59收敛的趋势）。结果与近期文献高度一致：与局部熵方法预测的α_LE≈1.58匹配；在α→0区域与重叠间隙性质(OGP)预测定性匹配；c序列排序变化现象与不对称二元感知机和负Hopfield模型相似。

Conclusion: fl-RDT方法成功揭示了对称二元感知机中的统计-计算间隙，第二提升层级的结构变化与阈值转变相关。提出的理论预测与多种独立方法结果一致，并设计了CLuP算法验证了理论预测的实用性。

Abstract: We study potential presence of statistical-computational gaps (SCG) in symmetric binary perceptrons (SBP) via a parametric utilization of \emph{fully lifted random duality theory} (fl-RDT) [96]. A structural change from decreasingly to arbitrarily ordered $c$-sequence (a key fl-RDT parametric component) is observed on the second lifting level and associated with \emph{satisfiability} ($α_c$) -- \emph{algorithmic} ($α_a$) constraints density threshold change thereby suggesting a potential existence of a nonzero computational gap $SCG=α_c-α_a$. The second level estimate is shown to match the theoretical $α_c$ whereas the $r\rightarrow \infty$ level one is proposed to correspond to $α_a$. For example, for the canonical SBP ($κ=1$ margin) we obtain $α_c\approx 1.8159$ on the second and $α_a\approx 1.6021$ (with converging tendency towards $\sim 1.59$ range) on the seventh level. Our propositions remarkably well concur with recent literature: (i) in [20] local entropy replica approach predicts $α_{LE}\approx 1.58$ as the onset of clustering defragmentation (presumed driving force behind locally improving algorithms failures); (ii) in $α\rightarrow 0$ regime we obtain on the third lifting level $κ\approx 1.2385\sqrt{\frac{α_a}{-\log\left ( α_a \right ) }}$ which qualitatively matches overlap gap property (OGP) based predictions of [43] and identically matches local entropy based predictions of [24]; (iii) $c$-sequence ordering change phenomenology mirrors the one observed in asymmetric binary perceptron (ABP) in [98] and the negative Hopfield model in [100]; and (iv) as in [98,100], we here design a CLuP based algorithm whose practical performance closely matches proposed theoretical predictions.

</details>


### [83] [Classification Imbalance as Transfer Learning](https://arxiv.org/abs/2601.10630)
*Eric Xia,Jason M. Klusowski*

Main category: stat.ML

TL;DR: 该论文将类别不平衡问题重新定义为标签偏移下的迁移学习问题，分析了过采样方法（特别是SMOTE）的理论性能，发现随机过采样在中等高维下通常优于SMOTE。


<details>
  <summary>Details</summary>
Motivation: 研究类别不平衡分类问题，特别是过采样方法（如SMOTE）的理论基础，为选择合适的数据增强策略提供理论指导。

Method: 将类别不平衡问题框架化为标签偏移下的迁移学习问题，分析过采样方法（包括SMOTE和随机过采样）的泛化误差分解，推导理论界。

Result: 超额风险可分解为平衡训练下的可达速率和迁移成本，其中SMOTE的迁移成本在中等高维下主导随机过采样，表明随机过采样通常性能更好。

Conclusion: 为不平衡分类选择数据增强策略提供了理论指导，随机过采样在中等高维情况下通常优于SMOTE，实验结果支持这一理论发现。

Abstract: Classification imbalance arises when one class is much rarer than the other. We frame this setting as transfer learning under label (prior) shift between an imbalanced source distribution induced by the observed data and a balanced target distribution under which performance is evaluated. Within this framework, we study a family of oversampling procedures that augment the training data by generating synthetic samples from an estimated minority-class distribution to roughly balance the classes, among which the celebrated SMOTE algorithm is a canonical example. We show that the excess risk decomposes into the rate achievable under balanced training (as if the data had been drawn from the balanced target distribution) and an additional term, the cost of transfer, which quantifies the discrepancy between the estimated and true minority-class distributions. In particular, we show that the cost of transfer for SMOTE dominates that of bootstrapping (random oversampling) in moderately high dimensions, suggesting that we should expect bootstrapping to have better performance than SMOTE in general. We corroborate these findings with experimental evidence. More broadly, our results provide guidance for choosing among augmentation strategies for imbalanced classification.

</details>
