<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [cs.LG](#cs.LG) [Total: 57]
- [stat.ML](#stat.ML) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Efficient handover based on Near-field and Far-field RIS for seamless connectivity](https://arxiv.org/abs/2507.22141)
*Atiquzzaman Mondal,Waheeb Tashan,Ayat Al-Olaimat,Hüseyin Arslan*

Main category: eess.SP

TL;DR: 提出了一种基于RIS的高效切换管理方案，用于6G网络，通过优化信号路径减少切换开销和提升移动管理。


<details>
  <summary>Details</summary>
Motivation: 利用RIS智能操控电磁波，增强6G网络的覆盖和连接性，解决频繁切换和信号开销问题。

Method: 分析RIS辅助网络中信号强度变化，推导近场区域距离概率密度函数，提出结合多种切换策略的新算法。

Result: 显著降低切换率和信令负载，提升频谱和能量效率，确保无缝连接和服务质量。

Conclusion: RIS辅助的切换管理方案在6G网络中具有潜力，能有效优化移动性和网络性能。

Abstract: Reconfigurable Intelligent Surfaces (RIS) is becoming a transformative
technology for the upcoming 6G communication networks, providing a way for
smartly maneuvering the electromagnetic waves to enhance coverage and
connectivity. This paper presents an efficient handover (HO) management scheme
leveraging RIS in the Fresnel region i.e., in both the near-field (NF) and
far-field (FF) regions to reduce signaling overhead and optimize mobility
management. For this, we analyzed the signal strength variations in the
considered RIS-aided networks, considering the radiative NF and FF regions, and
derive the probability density function (PDF) of the RIS-UE distance in the NF
region to quantify RIS reflection gains along the user equipment (UE)
trajectory. We propose a new HO algorithm incorporating several HO categories
like hard handover (HHO), soft handover (SHO), RIS-aided cell breathing
(RIS-CB), and RIS-aided ping-pong avoidance (RIS-PP) strategies. The proposed
algorithm uses bit error rate (BER) as a key parameter to predict the
minimization of unnecessary HOs by using RIS-aided pathways to retain
connectivity with the serving base station (BS), which minimizes the
requirement for frequent target BS searching and ultimately optimizes the HO.
By restricting measurement reports and HO requests, the suggested method
improves spectrum efficiency (SE) and energy efficiency (EE), especially in
crowded cellular networks. Numerical results highlight significant reductions
in HO rates and signaling load, ensuring seamless connectivity and improved
quality of service (QoS) in 6G systems.

</details>


### [2] [Deep Learning for Gradient and BCG Artifacts Removal in EEG During Simultaneous fMRI](https://arxiv.org/abs/2507.22263)
*K. A. Shahriar,E. H. Bhuiyan,Q. Luo,M. E. H. Chowdhury,X. J. Zhou*

Main category: eess.SP

TL;DR: 论文提出了一种基于去噪自编码器（DAR）的深度学习方法，用于去除EEG-fMRI同步记录中的MR相关伪影，相比传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: EEG-fMRI同步记录的高时空分辨率受到MR伪影（如梯度伪影和心电伪影）的干扰，限制了其应用。

Method: 使用1D卷积自编码器（DAR）从噪声信号中学习到清晰信号的直接映射，并通过CWL EEG-fMRI数据集进行验证。

Result: DAR在RMSE、SSIM和SNR增益上显著优于传统方法（如PCA、ICA等），且通过LOSO验证表明模型泛化能力强。

Conclusion: DAR是一种高效且可解释的实时EEG伪影去除方法，适用于EEG-fMRI同步记录。

Abstract: Simultaneous EEG-fMRI recording combines high temporal and spatial resolution
for tracking neural activity. However, its usefulness is greatly limited by
artifacts from magnetic resonance (MR), especially gradient artifacts (GA) and
ballistocardiogram (BCG) artifacts, which interfere with the EEG signal. To
address this issue, we used a denoising autoencoder (DAR), a deep learning
framework designed to reduce MR-related artifacts in EEG recordings. Using
paired data that includes both artifact-contaminated and MR-corrected EEG from
the CWL EEG-fMRI dataset, DAR uses a 1D convolutional autoencoder to learn a
direct mapping from noisy to clear signal segments. Compared to traditional
artifact removal methods like principal component analysis (PCA), independent
component analysis (ICA), average artifact subtraction (AAS), and wavelet
thresholding, DAR shows better performance. It achieves a root-mean-squared
error (RMSE) of 0.0218 $\pm$ 0.0152, a structural similarity index (SSIM) of
0.8885 $\pm$ 0.0913, and a signal-to-noise ratio (SNR) gain of 14.63 dB.
Statistical analysis with paired t-tests confirms that these improvements are
significant (p<0.001; Cohen's d>1.2). A leave-one-subject-out (LOSO)
cross-validation protocol shows that the model generalizes well, yielding an
average RMSE of 0.0635 $\pm$ 0.0110 and an SSIM of 0.6658 $\pm$ 0.0880 across
unseen subjects. Additionally, saliency-based visualizations demonstrate that
DAR highlights areas with dense artifacts, which makes its decisions easier to
interpret. Overall, these results position DAR as a potential and
understandable solution for real-time EEG artifact removal in simultaneous
EEG-fMRI applications.

</details>


### [3] [Robust Filtering and Learning in State-Space Models: Skewness and Heavy Tails Via Asymmetric Laplace Distribution](https://arxiv.org/abs/2507.22343)
*Yifan Yu,Shengjie Xiu,Daniel P. Palomar*

Main category: eess.SP

TL;DR: 论文提出了一种基于非对称拉普拉斯分布的鲁棒状态空间模型扩展，用于处理非高斯分布的异常数据，并通过高效的变分贝叶斯算法和单循环参数估计策略显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统状态空间模型在处理具有偏态和厚尾特性的异常数据时表现不佳，需要一种更鲁棒的方法来适应复杂的数据特征。

Method: 采用非对称拉普拉斯分布建模数据，提出高效的变分贝叶斯算法和单循环参数估计策略。

Result: 实验表明，该方法在多种噪声环境下表现稳健，无需手动调整超参数，且计算资源消耗更低。

Conclusion: 该方法在鲁棒控制和金融建模等领域具有实际应用潜力。

Abstract: State-space models are pivotal for dynamic system analysis but often struggle
with outlier data that deviates from Gaussian distributions, frequently
exhibiting skewness and heavy tails. This paper introduces a robust extension
utilizing the asymmetric Laplace distribution, specifically tailored to capture
these complex characteristics. We propose an efficient variational Bayes
algorithm and a novel single-loop parameter estimation strategy, significantly
enhancing the efficiency of the filtering, smoothing, and parameter estimation
processes. Our comprehensive experiments demonstrate that our methods provide
consistently robust performance across various noise settings without the need
for manual hyperparameter adjustments. In stark contrast, existing models
generally rely on specific noise conditions and necessitate extensive manual
tuning. Moreover, our approach uses far fewer computational resources, thereby
validating the model's effectiveness and underscoring its potential for
practical applications in fields such as robust control and financial modeling.

</details>


### [4] [Green One-Bit Quantized Precoding in Cell-Free Massive MIMO](https://arxiv.org/abs/2507.22400)
*Salih Gümüsbuğa,Ozan Alp Topal,Özlem Tuğfe Demir*

Main category: eess.SP

TL;DR: 提出了一种新型量化预编码算法，用于无小区大规模MIMO系统，动态关闭不必要的天线以提高能效。


<details>
  <summary>Details</summary>
Motivation: 解决无小区大规模MIMO系统中高分辨率RF链导致的高功耗问题。

Method: 动态关闭不必要的天线，基于符号向量的结构设计量化预编码算法。

Result: 仿真显示算法在性能和功耗上优于现有方法（如SQUID和RZF）。

Conclusion: 该算法在能效和性能上取得平衡，适用于6G及未来无线通信。

Abstract: Cell-free massive MIMO (multiple-input multiple-output) is expected to be one
of the key technologies in sixth-generation (6G) and beyond wireless
communications, offering enhanced spectral efficiency for cell-edge user
equipments by employing joint transmission and reception with a large number of
antennas distributed throughout the region. However, high-resolution RF chains
associated with these antennas significantly increase power consumption. To
address this issue, the use of low-resolution analog-to-digital and
digital-to-analog converters (ADCs/DACs) has emerged as a promising approach to
balance power efficiency and performance in massive MIMO networks. In this
work, we propose a novel quantized precoding algorithm tailored for cell-free
massive MIMO systems, where the proposed method dynamically deactivates
unnecessary antennas based on the structure of each symbol vector, thereby
enhancing energy efficiency. Simulation results demonstrate that our algorithm
outperforms existing methods such as squared-infinity norm Douglas-Rachford
splitting (SQUID) and regularized zero forcing (RZF), achieving superior
performance while effectively reducing power consumption.

</details>


### [5] [PINN and GNN-based RF Map Construction for Wireless Communication Systems](https://arxiv.org/abs/2507.22513)
*Lizhou Liu,Xiaohui Chen,Zihan Tang,Mengyao Ma,Wenyi Zhang*

Main category: eess.SP

TL;DR: 本文提出了一种基于物理信息神经网络（PINN）和图神经网络（GNN）的RF地图构建方法，结合物理约束和空间相关性，实现了多径参数的高精度预测。


<details>
  <summary>Details</summary>
Motivation: RF地图在无线通信网络中具有重要作用，但传统方法在稀疏采样条件下表现不佳。本文旨在通过结合物理约束和空间相关性，提升RF地图的构建精度和泛化能力。

Method: 结合PINN（引入电磁传播定律的物理约束）和GNN（建模接收器位置的空间相关性），将多径信号参数化为接收功率、延迟和到达角（AoA）。

Result: 实验表明，该方法在稀疏采样条件下能高精度构建RF地图，在室内和复杂室外环境中均表现优异，优于基线方法。

Conclusion: 该方法通过物理约束和空间相关性建模，显著提升了RF地图的构建精度和泛化能力，适用于多种环境。

Abstract: Radio frequency (RF) map is a promising technique for capturing the
characteristics of multipath signal propagation, offering critical support for
channel modeling, coverage analysis, and beamforming in wireless communication
networks. This paper proposes a novel RF map construction method based on a
combination of physics-informed neural network (PINN) and graph neural network
(GNN). The PINN incorporates physical constraints derived from electromagnetic
propagation laws to guide the learning process, while the GNN models spatial
correlations among receiver locations. By parameterizing multipath signals into
received power, delay, and angle of arrival (AoA), and integrating both
physical priors and spatial dependencies, the proposed method achieves accurate
prediction of multipath parameters. Experimental results demonstrate that the
method enables high-precision RF map construction under sparse sampling
conditions and delivers robust performance in both indoor and complex outdoor
environments, outperforming baseline methods in terms of generalization and
accuracy.

</details>


### [6] [Exploration of Low-Cost but Accurate Radar-Based Human Motion Direction Determination](https://arxiv.org/abs/2507.22567)
*Weicheng Gao*

Main category: eess.SP

TL;DR: 提出了一种低成本但准确的雷达基人类运动方向确定方法，结合特征增强和轻量级混合模型结构。


<details>
  <summary>Details</summary>
Motivation: 运动方向角影响微多普勒频谱宽度，确定人类运动方向可为步态识别等下游任务提供重要先验信息。

Method: 首先生成雷达基人类步态DTM，通过特征链接模型实现特征增强，再通过轻量级Vision Transformer-CNN混合模型实现运动方向确定。

Result: 通过开源数据集验证了方法的有效性。

Conclusion: 该方法在低成本下实现了准确的运动方向确定，并开源了代码。

Abstract: This work is completed on a whim after discussions with my junior colleague.
The motion direction angle affects the micro-Doppler spectrum width, thus
determining the human motion direction can provide important prior information
for downstream tasks such as gait recognition. However, Doppler-Time map
(DTM)-based methods still have room for improvement in achieving feature
augmentation and motion determination simultaneously. In response, a low-cost
but accurate radar-based human motion direction determination (HMDD) method is
explored in this paper. In detail, the radar-based human gait DTMs are first
generated, and then the feature augmentation is achieved using feature linking
model. Subsequently, the HMDD is implemented through a lightweight and fast
Vision Transformer-Convolutional Neural Network hybrid model structure. The
effectiveness of the proposed method is verified through open-source dataset.
The open-source code of this work is released at:
https://github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination.

</details>


### [7] [Fundamental Limits of Rigid Body Localization](https://arxiv.org/abs/2507.22573)
*Niclas Führling,Ivan Alexander Morales Sandoval,Giuseppe Thadeu Freitas de Abreu,Gonzalo Seco-Granados,David González G.,Osvaldo Gonsa*

Main category: eess.SP

TL;DR: 论文提出了一种新的方法来构建刚体定位问题的Cramér-Rao下界（CRLB），评估刚体平移和旋转估计的基本精度限制。


<details>
  <summary>Details</summary>
Motivation: 研究刚体定位问题中平移和旋转估计的精度限制，为现有算法提供理论支持。

Method: 采用信息中心的Fisher信息矩阵（FIM）构造方法，捕捉每种测量对FIM的贡献，推导适用于所有刚体定位场景的CRLB框架。

Result: 给出了所有CRLB的闭式表达式，包括旋转矩阵正交约束的边界，数值结果表明这些表达式正确下界了现有算法的误差。

Conclusion: 提出的CRLB框架揭示了现有刚体定位算法的精度限制，表明其仍有改进空间。

Abstract: We consider a novel approach to formulate the Cram\'er-Rao Lower Bound (CRLB)
for the rigid body localization (RBL) problem, which allows us to assess the
fundamental accuracy limits on the estimation of the translation and rotation
of a rigid body with respect to a known reference. To that end, we adopt an
information-centric construction of the Fisher information matrix (FIM), which
allows to capture the contribution of each measurement towards the FIM, both in
terms of input measurement types, as well as of their error distributions.
Taking advantage of this approach, we derive a generic framework for the CRLB
formulation, which is applicable to any type of rigid body localization
scenario, extending the conventional FIM formulation suitable for point targets
to the case of a rigid body whose location include both translation vector and
the rotation matrix (or alternative the rotation angles), with respect to a
reference. Closed-form expressions for all CRLBs are given, including the bound
incorporating an orthonormality constraint onto the rotation matrix. Numerical
results illustrate that the derived expression correctly lower-bounds the
errors of estimated localization parameters obtained via various related
state-of-the-art (SotA) estimators, revealing their accuracies and suggesting
that SotA RBL algorithms can still be improved.

</details>


### [8] [Measurement and Analysis of the Power Consumption of Hybrid-Amplified SCL-band Links](https://arxiv.org/abs/2507.22616)
*Ronit Sohanpal,Jiaqian Yang,Eric Sillekens,Henrique Buglia,Mingming Tan,Dini Pratiwi,Robert I. Killey,Polina Bayvel*

Main category: eess.SP

TL;DR: 研究了混合放大SCL波段链路的功耗，发现多跨段混合拉曼放大链路比集总放大节能26%。


<details>
  <summary>Details</summary>
Motivation: 探索混合放大技术在SCL波段链路中的节能潜力。

Method: 使用商用台式放大器和拉曼泵进行实验。

Result: 多跨段混合拉曼放大链路的每比特能耗降低26%。

Conclusion: 混合拉曼放大技术在节能方面优于集总放大。

Abstract: We studied the power consumption of hybrid-amplified SCL-band links using
commercial benchtop amplifiers and Raman pumps. We show a reduction in energy
per bit for multi-span hybrid Raman amplified links of up to 26% versus lumped
amplification.

</details>


### [9] [A Multi-Scale Spatial Attention Network for Near-field MIMO Channel Estimation](https://arxiv.org/abs/2507.22656)
*Zhiming Zhu,Shu Xu,Jiexin Zhang,Chunguo Li,Yongming Huang,Luxi Yang*

Main category: eess.SP

TL;DR: 论文提出了一种基于空间注意力的深度学习方法（MsSAN），用于解决极大规模MIMO（XL-MIMO）近场信道估计问题，通过多尺度架构和创新的空间注意力机制，显著提升了信道重建性能。


<details>
  <summary>Details</summary>
Motivation: 极大规模阵列（ELAA）部署带来了更高的频谱效率和空间自由度，但也引发了近场信道估计的问题。现有方法依赖于变换域的稀疏性，但对变换矩阵选择和停止准则敏感。

Method: 分析了近场信道的空间天线相关性，提出了一种多尺度空间注意力网络（MsSAN），通过子信道间相关性学习和创新的点积和空间注意力机制，优化信道估计。

Result: 仿真结果表明，MsSAN在近场信道重建中表现出色，显著优于其他方法。

Conclusion: MsSAN通过创新的空间注意力机制和多尺度架构，有效解决了近场信道估计问题，为极大规模MIMO系统提供了高效的信道重建方案。

Abstract: The deployment of extremely large-scale array (ELAA) brings higher spectral
efficiency and spatial degree of freedom, but triggers issues on near-field
channel estimation.
  Existing near-field channel estimation schemes primarily exploit sparsity in
the transform domain.
  However, these schemes are sensitive to the transform matrix selection and
the stopping criteria.
  Inspired by the success of deep learning (DL) in far-field channel
estimation, this paper proposes a novel spatial-attention-based method for
reconstructing extremely large-scale MIMO (XL-MIMO) channel.
  Initially, the spatial antenna correlations of near-field channels are
analyzed as an expectation over the angle-distance space, which demonstrate
correlation range of an antenna element varies with its position.
  Due to the strong correlation between adjacent antenna elements, interactions
of inter-subchannel are applied to describe inherent correlation of near-field
channels instead of inter-element.
  Subsequently, a multi-scale spatial attention network (MsSAN) with the
inter-subchannel correlation learning capabilities is proposed tailed to
near-field MIMO channel estimation.
  We employ the multi-scale architecture to refine the subchannel size in
MsSAN.
  Specially, we inventively introduce the sum of dot products as spatial
attention (SA) instead of cross-covariance to weight subchannel features at
different scales in the SA module.
  Simulation results are presented to validate the proposed MsSAN achieves
remarkable the inter-subchannel correlation learning capabilities and
outperforms others in terms of near-field channel reconstruction.

</details>


### [10] [Compressive Near-Field Wideband Channel Estimation for THz Extremely Large-scale MIMO Systems](https://arxiv.org/abs/2507.22727)
*Jionghui Wang,Hongwei Wang,Jun Fang,Lingxiang Li,Zhi Chen*

Main category: eess.SP

TL;DR: 论文提出了一种针对太赫兹通信系统的宽带近场信道估计方法，通过频率无关的正交字典和二维块稀疏结构，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 太赫兹通信系统中，由于路径衰减严重且存在近场球面波前和宽带波束分裂现象，需要高效的信道估计方法。

Method: 提出了一种频率无关的正交字典，扩展了标准DFT矩阵，以捕捉近场特性，并利用二维块稀疏结构在压缩感知框架下进行信道估计。

Result: 数值结果表明，所提出的二维块稀疏感知方法在近场宽带信道估计中优于传统基于极域的方法。

Conclusion: 该方法为太赫兹通信系统中的宽带近场信道估计提供了高效解决方案。

Abstract: We consider the channel acquisition problem for a wideband terahertz (THz)
communication system, where an extremely large-scale array is deployed to
mitigate severe path attenuation. In channel modeling, we account for both the
near-field spherical wavefront and the wideband beam-splitting phenomena,
resulting in a wideband near-field channel. We propose a frequency-independent
orthogonal dictionary that generalizes the standard discrete Fourier transform
(DFT) matrix by introducing an additional parameter to capture the near-field
property. This dictionary enables the wideband near-field channel to be
efficiently represented with a two-dimensional (2D) block-sparse structure.
Leveraging this specific sparse structure, the wideband near-field channel
estimation problem can be effectively addressed within a customized compressive
sensing framework. Numerical results demonstrate the significant advantages of
our proposed 2D block-sparsity-aware method over conventional
polar-domain-based approaches for near-field wideband channel estimation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models](https://arxiv.org/abs/2507.22766)
*Felix Kronenwett,Georg Maier,Thomas Laengle*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯优化的方法，用于优化、监控和调整传感器分选系统的工艺参数，以减少实验次数并考虑不确定性。


<details>
  <summary>Details</summary>
Motivation: 由于材料流组成和需求的变化，传感器分选系统的工艺参数需要持续验证和调整。

Method: 使用高斯过程回归模型作为代理模型，基于贝叶斯优化方法，同时考虑两个可能的优化目标和不确定性。

Result: 通过三个示例工艺参数验证了该方法的有效性。

Conclusion: 该方法能够高效优化工艺参数，同时减少实验需求并处理不确定性。

Abstract: Sensor-based sorting systems enable the physical separation of a material
stream into two fractions. The sorting decision is based on the image data
evaluation of the sensors used and is carried out using actuators. Various
process parameters must be set depending on the properties of the material
stream, the dimensioning of the system, and the required sorting accuracy.
However, continuous verification and re-adjustment are necessary due to
changing requirements and material stream compositions. In this paper, we
introduce an approach for optimizing, recurrently monitoring and adjusting the
process parameters of a sensor-based sorting system. Based on Bayesian
Optimization, Gaussian process regression models are used as surrogate models
to achieve specific requirements for system behavior with the uncertainties
contained therein. This method minimizes the number of necessary experiments
while simultaneously considering two possible optimization targets based on the
requirements for both material output streams. In addition, uncertainties are
considered during determining sorting accuracies in the model calculation. We
evaluated the method with three example process parameters.

</details>


### [12] [CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs](https://arxiv.org/abs/2507.22074)
*Yangshu Yuan,Heng Chen,Xinyi Jiang,Christian Ng,Kexin Qiu*

Main category: cs.LG

TL;DR: CIMR框架通过上下文感知的迭代推理和自我修正模块，显著提升了多模态复杂任务的性能，准确率达91.5%。


<details>
  <summary>Details</summary>
Motivation: 现有大模型在处理需要逻辑推理和动态反馈的多模态复杂指令时表现不佳。

Method: 提出CIMR框架，分两阶段：初始推理与响应生成，以及基于多模态反馈的迭代优化。

Result: 在MAP数据集上，CIMR准确率达91.5%，优于GPT-4V等模型。

Conclusion: CIMR的迭代推理和自我修正能力在复杂任务中表现出色。

Abstract: The rapid advancement of Large Language Models (LLMs) and Large
Vision-Language Models (LVLMs) has enhanced our ability to process and generate
human language and visual information. However, these models often struggle
with complex, multi-step multi-modal instructions that require logical
reasoning, dynamic feedback integration, and iterative self-correction. To
address this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a
novel framework that introduces a context-aware iterative reasoning and
self-correction module. CIMR operates in two stages: initial reasoning and
response generation, followed by iterative refinement using parsed multi-modal
feedback. A dynamic fusion module deeply integrates textual, visual, and
contextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual
Instruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced
Multi-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy,
outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5
(78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the
efficacy of its iterative reasoning and self-correction capabilities in complex
tasks.

</details>


### [13] [Prototype-Guided Pseudo-Labeling with Neighborhood-Aware Consistency for Unsupervised Adaptation](https://arxiv.org/abs/2507.22075)
*Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.LG

TL;DR: 提出了一种自适应伪标签框架，通过原型一致性和邻域一致性提升CLIP在无监督适应中的性能。


<details>
  <summary>Details</summary>
Motivation: 无监督适应中，CLIP的零样本预测伪标签噪声较大，传统固定阈值方法不可靠。

Method: 结合PICS（基于类内紧凑性和类间分离性评估伪标签准确性）和NALR（利用邻域语义相似性动态优化伪标签），并引入自适应权重机制。

Result: 在11个基准数据集上取得最优性能，提供更准确的伪标签且计算高效。

Conclusion: 该方法显著提升了无监督适应中伪标签的准确性，具有实际应用价值。

Abstract: In unsupervised adaptation for vision-language models such as CLIP,
pseudo-labels derived from zero-shot predictions often exhibit significant
noise, particularly under domain shifts or in visually complex scenarios.
Conventional pseudo-label filtering approaches, which rely on fixed confidence
thresholds, tend to be unreliable in fully unsupervised settings. In this work,
we propose a novel adaptive pseudo-labeling framework that enhances CLIP's
adaptation performance by integrating prototype consistency and
neighborhood-based consistency. The proposed method comprises two key
components: PICS, which assesses pseudo-label accuracy based on in-class
feature compactness and cross-class feature separation; and NALR, which
exploits semantic similarities among neighboring samples to refine
pseudo-labels dynamically. Additionally, we introduce an adaptive weighting
mechanism that adjusts the influence of pseudo-labeled samples during training
according to their estimated correctness. Extensive experiments on 11 benchmark
datasets demonstrate that our method achieves state-of-the-art performance in
unsupervised adaptation scenarios, delivering more accurate pseudo-labels while
maintaining computational efficiency.

</details>


### [14] [Test-time Prompt Refinement for Text-to-Image Models](https://arxiv.org/abs/2507.22076)
*Mohammad Abdul Hafeez Khan,Yash Jain,Siddhartha Bhattacharyya,Vibhav Vineet*

Main category: cs.LG

TL;DR: TIR是一种无需额外训练的闭环提示优化框架，通过多模态大语言模型迭代优化文本提示，提升文本到图像生成的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成模型对提示词微小变化的敏感性，导致输出不一致或不准确的问题。

Method: 采用闭环测试时提示优化框架（TIR），结合预训练多模态大语言模型（MLLM）分析图像与提示词，迭代优化提示词。

Result: TIR显著提升了生成图像与提示词的对齐性和视觉一致性，适用于多种基准数据集。

Conclusion: TIR通过闭环迭代优化提示词，有效解决了文本到图像生成中的敏感性问题，且无需额外训练，具有即插即用特性。

Abstract: Text-to-image (T2I) generation models have made significant strides but still
struggle with prompt sensitivity: even minor changes in prompt wording can
yield inconsistent or inaccurate outputs. To address this challenge, we
introduce a closed-loop, test-time prompt refinement framework that requires no
additional training of the underlying T2I model, termed TIR. In our approach,
each generation step is followed by a refinement step, where a pretrained
multimodal large language model (MLLM) analyzes the output image and the user's
prompt. The MLLM detects misalignments (e.g., missing objects, incorrect
attributes) and produces a refined and physically grounded prompt for the next
round of image generation. By iteratively refining the prompt and verifying
alignment between the prompt and the image, TIR corrects errors, mirroring the
iterative refinement process of human artists. We demonstrate that this
closed-loop strategy improves alignment and visual coherence across multiple
benchmark datasets, all while maintaining plug-and-play integration with
black-box T2I models.

</details>


### [15] [A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model](https://arxiv.org/abs/2507.22854)
*Andris Ambainis,Joao F. Doriguello,Debbie Lim*

Main category: cs.LG

TL;DR: 论文提出了一种基于混合探索-生成强化学习模型的经典和量子在线算法，用于学习有限和无限时间范围内的平均奖励马尔可夫决策过程（MDP）。通过直接计算和使用最优策略，避免了传统RL的范式，获得了更好的遗憾界限。量子算法在有限时间MDP中实现了对数依赖的遗憾界限，突破了经典算法的$O(\sqrt{T})$限制。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习（RL）方法如“乐观面对不确定性”和“后验采样”存在局限性，作者希望通过结合生成模型和量子算法，直接计算最优策略，从而改进遗憾界限。

Method: 采用混合探索-生成强化学习模型，结合经典和量子算法近似生成模型下的最优策略。

Result: 有限时间MDP的量子算法实现了对数依赖的遗憾界限（$\log{T}$），优于经典算法的$O(\sqrt{T})$。无限时间MDP的量子算法在特定度量下实现了$\operatorname{poly}\log{T}$的遗憾界限。

Conclusion: 论文展示了量子算法在RL中的潜力，尤其是在有限时间MDP中突破了经典算法的性能限制，为未来研究提供了新方向。

Abstract: We propose novel classical and quantum online algorithms for learning
finite-horizon and infinite-horizon average-reward Markov Decision Processes
(MDPs). Our algorithms are based on a hybrid exploration-generative
reinforcement learning (RL) model wherein the agent can, from time to time,
freely interact with the environment in a generative sampling fashion, i.e., by
having access to a "simulator". By employing known classical and new quantum
algorithms for approximating optimal policies under a generative model within
our learning algorithms, we show that it is possible to avoid several paradigms
from RL like "optimism in the face of uncertainty" and "posterior sampling" and
instead compute and use optimal policies directly, which yields better regret
bounds compared to previous works. For finite-horizon MDPs, our quantum
algorithms obtain regret bounds which only depend logarithmically on the number
of time steps $T$, thus breaking the $O(\sqrt{T})$ classical barrier. This
matches the time dependence of the prior quantum works of Ganguly et al.
(arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other
parameters like state space size $S$ and action space size $A$. For
infinite-horizon MDPs, our classical and quantum bounds still maintain the
$O(\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we
propose a novel measure of regret for infinite-horizon MDPs with respect to
which our quantum algorithms have $\operatorname{poly}\log{T}$ regret,
exponentially better compared to classical algorithms. Finally, we generalise
all of our results to compact state spaces.

</details>


### [16] [Multi-fidelity Bayesian Data-Driven Design of Energy Absorbing Spinodoid Cellular Structures](https://arxiv.org/abs/2507.22079)
*Leo Guo,Hirak Kansara,Siamak F. Khosroshahi,GuoQi Zhang,Wei Tan*

Main category: cs.LG

TL;DR: 该论文比较了贝叶斯优化（BO）和多保真贝叶斯优化（MFBO）在优化自旋体结构能量吸收（EA）问题中的性能，发现MFBO在多种超参数设置下比BO表现更优，提升达11%。


<details>
  <summary>Details</summary>
Motivation: 随着有限元（FE）模拟精度提高但计算成本增加，数据驱动设计需求增长，需要高效的优化方法。BO和MFBO虽在理论上有效，但缺乏在实际工程问题（如超材料设计）中的直接比较。此外，采样质量和设计参数敏感性分析在数据驱动设计中常被忽视。

Method: 论文采用Sobol'采样和基于方差的敏感性分析来降低设计问题复杂度，并实现、应用和比较BO与MFBO在优化自旋体结构能量吸收问题中的表现。

Result: MFBO在优化自旋体结构的能量吸收方面表现优于BO，提升幅度达11%。结果开源，支持多保真技术在昂贵数据驱动设计问题中的应用。

Conclusion: MFBO是一种有效优化自旋体结构能量吸收的方法，优于传统BO。研究结果支持多保真技术在高成本数据驱动设计中的实用性。

Abstract: Finite element (FE) simulations of structures and materials are getting
increasingly more accurate, but also more computationally expensive as a
collateral result. This development happens in parallel with a growing demand
of data-driven design. To reconcile the two, a robust and data-efficient
optimization method called Bayesian optimization (BO) has been previously
established as a technique to optimize expensive objective functions. In
parallel, the mesh width of an FE model can be exploited to evaluate an
objective at a lower or higher fidelity (cost & accuracy) level. The
multi-fidelity setting applied to BO, called multi-fidelity BO (MFBO), has also
seen previous success. However, BO and MFBO have not seen a direct comparison
with when faced with with a real-life engineering problem, such as metamaterial
design for deformation and absorption qualities. Moreover, sampling quality and
assessing design parameter sensitivity is often an underrepresented part of
data-driven design. This paper aims to address these shortcomings by employing
Sobol' samples with variance-based sensitivity analysis in order to reduce
design problem complexity. Furthermore, this work describes, implements,
applies and compares the performance BO with that MFBO when maximizing the
energy absorption (EA) problem of spinodoid cellular structures is concerned.
The findings show that MFBO is an effective way to maximize the EA of a
spinodoid structure and is able to outperform BO by up to 11% across various
hyperparameter settings. The results, which are made open-source, serve to
support the utility of multi-fidelity techniques across expensive data-driven
design problems.

</details>


### [17] [Shape Invariant 3D-Variational Autoencoder: Super Resolution in Turbulence flow](https://arxiv.org/abs/2507.22082)
*Anuraj Maurya*

Main category: cs.LG

TL;DR: 论文概述了深度学习和经典方法在湍流建模中的应用，并探讨了多尺度湍流模型与深度学习架构的整合以及深度生成模型在超分辨率重建中的应用。


<details>
  <summary>Details</summary>
Motivation: 利用深度学习从复杂数据中提取结构化信息，以更深入地理解流体动力学现象，特别是湍流建模领域。

Method: 结合经典方法和深度学习技术，研究多尺度湍流模型与深度学习架构的整合，以及深度生成模型的应用。

Result: 展示了深度学习在湍流建模中的潜力，特别是在多尺度建模和超分辨率重建方面的应用。

Conclusion: 深度学习为湍流建模提供了新的工具和方法，尤其是在处理高维数据和复杂现象时表现出色。

Abstract: Deep learning provides a versatile suite of methods for extracting structured
information from complex datasets, enabling deeper understanding of underlying
fluid dynamic phenomena. The field of turbulence modeling, in particular,
benefits from the growing availability of high-dimensional data obtained
through experiments, field observations, and large-scale simulations spanning
multiple spatio-temporal scales. This report presents a concise overview of
both classical and deep learningbased approaches to turbulence modeling. It
further investigates two specific challenges at the intersection of fluid
dynamics and machine learning: the integration of multiscale turbulence models
with deep learning architectures, and the application of deep generative models
for super-resolution reconstruction

</details>


### [18] [Principled Curriculum Learning using Parameter Continuation Methods](https://arxiv.org/abs/2507.22089)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: 提出了一种参数延续方法用于神经网络优化，结合同伦和课程学习，理论和实践均有效，优于ADAM等现有技术。


<details>
  <summary>Details</summary>
Motivation: 探索参数延续与同伦、课程学习之间的联系，提升神经网络优化的理论和实践效果。

Method: 提出参数延续方法，结合同伦和课程学习，优化神经网络。

Result: 在监督和无监督学习任务中，表现出比ADAM等现有技术更好的泛化性能。

Conclusion: 参数延续方法在神经网络优化中具有理论和实践优势，性能优于现有技术。

Abstract: In this work, we propose a parameter continuation method for the optimization
of neural networks. There is a close connection between parameter continuation,
homotopies, and curriculum learning. The methods we propose here are
theoretically justified and practically effective for several problems in deep
neural networks. In particular, we demonstrate better generalization
performance than state-of-the-art optimization techniques such as ADAM for
supervised and unsupervised learning tasks.

</details>


### [19] [Hybrid activation functions for deep neural networks: S3 and S4 -- a novel approach to gradient flow optimization](https://arxiv.org/abs/2507.22090)
*Sergii Kavun*

Main category: cs.LG

TL;DR: 论文提出两种新型混合激活函数S3和S4，S4通过平滑过渡机制在多项任务中表现优于基线函数。


<details>
  <summary>Details</summary>
Motivation: 传统激活函数如ReLU存在死神经元问题，sigmoid和tanh存在梯度消失问题，需要改进。

Method: S3结合sigmoid和softsign，S4引入平滑过渡参数k，实验覆盖分类和回归任务。

Result: S4在MNIST、Iris分类和Boston Housing回归中表现最佳，收敛更快且梯度稳定。

Conclusion: 混合激活函数是改进神经网络训练动态的有前景方向。

Abstract: Activation functions are critical components in deep neural networks,
directly influencing gradient flow, training stability, and model performance.
Traditional functions like ReLU suffer from dead neuron problems, while sigmoid
and tanh exhibit vanishing gradient issues. We introduce two novel hybrid
activation functions: S3 (Sigmoid-Softsign) and its improved version S4
(smoothed S3). S3 combines sigmoid for negative inputs with softsign for
positive inputs, while S4 employs a smooth transition mechanism controlled by a
steepness parameter k. We conducted comprehensive experiments across binary
classification, multi-class classification, and regression tasks using three
different neural network architectures. S4 demonstrated superior performance
compared to nine baseline activation functions, achieving 97.4% accuracy on
MNIST, 96.0% on Iris classification, and 18.7 MSE on Boston Housing regression.
The function exhibited faster convergence (-19 for ReLU) and maintained stable
gradient flow across network depths. Comparative analysis revealed S4's
gradient range of [0.24, 0.59] compared to ReLU's 18% dead neurons in deep
networks. The S4 activation function addresses key limitations of existing
functions through its hybrid design and smooth transition mechanism. The
tunable parameter k allows adaptation to different tasks and network depths,
making S4 a versatile choice for deep learning applications. These findings
suggest that hybrid activation functions represent a promising direction for
improving neural network training dynamics.

</details>


### [20] [Spatial-Temporal Reinforcement Learning for Network Routing with Non-Markovian Traffic](https://arxiv.org/abs/2507.22174)
*Molly Wang*

Main category: cs.LG

TL;DR: 提出了一种结合图神经网络和循环神经网络的时空强化学习方法，以优化通信网络中的分组路由。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习基于马尔可夫决策过程，假设环境状态完全决定系统演化，但实际场景中这一假设常不成立，且传统方法未能有效捕捉复杂网络拓扑的空间关系。

Method: 整合图神经网络（GNN）和循环神经网络（RNN），分别捕捉网络拓扑的空间动态和流量模式的时间动态。

Result: 评估表明，该方法优于传统强化学习技术，对网络拓扑变化的鲁棒性更强。

Conclusion: 提出的时空强化学习方法能更有效地优化通信网络中的路由决策。

Abstract: Reinforcement Learning (RL) has become a well-established approach for
optimizing packet routing in communication networks. Standard RL algorithms
typically are based on the Markov Decision Process (MDP), which assumes that
the current state of the environment provides all the necessary information for
system evolution and decision-making. However, this Markovian assumption is
invalid in many practical scenarios, making the MDP and RL frameworks
inadequate to produce the optimal solutions. Additionally, traditional RL
algorithms often employ function approximations (e.g., by neural networks) that
do not explicitly capture the spatial relationships inherent in environments
with complex network topologies. Communication networks are characterized by
dynamic traffic patterns and arbitrary numbers of nodes and links, which
further complicate the decision-making process. To address these challenges, we
propose a spatial-temporal RL approach that integrates Graph Neural Networks
(GNNs) and Recurrent Neural Networks (RNNs) to adequately capture the spatial
dynamics regarding network topology and temporal traffic patterns,
respectively, to enhance routing decisions. Our evaluation demonstrates that
the proposed method outperforms and is more robust to changes in the network
topology when compared with traditional RL techniques.

</details>


### [21] [SourceSplice: Source Selection for Machine Learning Tasks](https://arxiv.org/abs/2507.22186)
*Ambarish Singh,Romila Pradhan*

Main category: cs.LG

TL;DR: 论文提出SourceGrasp和SourceSplice框架，用于高效选择数据源子集以优化下游机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 数据质量对机器学习任务至关重要，但现有数据发现方法未考虑源质量对模型性能的影响。

Method: SourceGrasp基于贪婪准则和随机化，SourceSplice受基因剪接启发，两者旨在选择对任务效用贡献最大的数据源子集。

Result: 实验表明，SourceSplice在较少子集探索下能有效识别高任务效用的数据源子集。

Conclusion: SourceSplice在多种设置下表现出对决策选择的敏感性，为数据源选择提供了高效解决方案。

Abstract: Data quality plays a pivotal role in the predictive performance of machine
learning (ML) tasks - a challenge amplified by the deluge of data sources
available in modern organizations.Prior work in data discovery largely focus on
metadata matching, semantic similarity or identifying tables that should be
joined to answer a particular query, but do not consider source quality for
high performance of the downstream ML task.This paper addresses the problem of
determining the best subset of data sources that must be combined to construct
the underlying training dataset for a given ML task.We propose SourceGrasp and
SourceSplice, frameworks designed to efficiently select a suitable subset of
sources that maximizes the utility of the downstream ML model.Both the
algorithms rely on the core idea that sources (or their combinations)
contribute differently to the task utility, and must be judiciously
chosen.While SourceGrasp utilizes a metaheuristic based on a greediness
criterion and randomization, the SourceSplice framework presents a source
selection mechanism inspired from gene splicing - a core concept used in
protein synthesis.We empirically evaluate our algorithms on three real-world
datasets and synthetic datasets and show that, with significantly fewer subset
explorations, SourceSplice effectively identifies subsets of data sources
leading to high task utility.We also conduct studies reporting the sensitivity
of SourceSplice to the decision choices under several settings.

</details>


### [22] [Measuring Time-Series Dataset Similarity using Wasserstein Distance](https://arxiv.org/abs/2507.22189)
*Hongjie Chen,Akshay Mehra,Josh Kimball,Ryan A. Rossi*

Main category: cs.LG

TL;DR: 提出了一种基于Wasserstein距离的分布方法来衡量时间序列数据集的相似性，通过将数据集视为多元正态分布的实例，展示了该方法在模型选择和性能估计中的有效性。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型研究的兴起需要衡量数据集相似性，以支持模型选择、微调和可视化。

Method: 将时间序列数据集视为多元正态分布的实例，利用Wasserstein距离计算相似性。

Result: 实验表明，该方法能有效识别相似数据集，并与推理损失高度相关（>0.60）。

Conclusion: 提出的方法为时间序列数据集相似性提供了有效的衡量工具，支持基础模型的评估和选择。

Abstract: The emergence of time-series foundation model research elevates the growing
need to measure the (dis)similarity of time-series datasets. A time-series
dataset similarity measure aids research in multiple ways, including model
selection, finetuning, and visualization. In this paper, we propose a
distribution-based method to measure time-series dataset similarity by
leveraging the Wasserstein distance. We consider a time-series dataset an
empirical instantiation of an underlying multivariate normal distribution
(MVN). The similarity between two time-series datasets is thus computed as the
Wasserstein distance between their corresponding MVNs. Comprehensive
experiments and visualization show the effectiveness of our approach.
Specifically, we show how the Wasserstein distance helps identify similar
time-series datasets and facilitates inference performance estimation of
foundation models in both out-of-distribution and transfer learning evaluation,
with high correlations between our proposed measure and the inference loss
(>0.60).

</details>


### [23] [CTG-Insight: A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification](https://arxiv.org/abs/2507.22205)
*Black Sun,Die,Hu*

Main category: cs.LG

TL;DR: CTG-Insight是一个多智能体LLM系统，用于解析胎儿心率和宫缩信号，提供透明且可解释的输出，准确率高达96.4%。


<details>
  <summary>Details</summary>
Motivation: 现有远程胎儿监测系统缺乏可解释性，导致原始CTG数据难以理解。

Method: 利用多智能体LLM系统分解CTG信号为五个医学特征，并通过聚合智能体提供整体分类和自然语言解释。

Result: 在NeuroFetalNet数据集上，CTG-Insight的准确率为96.4%，F1分数为97.8%，优于深度学习和单智能体LLM基线。

Conclusion: 该研究提出了一个可解释且可扩展的CTG分析框架。

Abstract: Remote fetal monitoring technologies are becoming increasingly common. Yet,
most current systems offer limited interpretability, leaving expectant parents
with raw cardiotocography (CTG) data that is difficult to understand. In this
work, we present CTG-Insight, a multi-agent LLM system that provides structured
interpretations of fetal heart rate (FHR) and uterine contraction (UC) signals.
Drawing from established medical guidelines, CTG-Insight decomposes each CTG
trace into five medically defined features: baseline, variability,
accelerations, decelerations, and sinusoidal pattern, each analyzed by a
dedicated agent. A final aggregation agent synthesizes the outputs to deliver a
holistic classification of fetal health, accompanied by a natural language
explanation. We evaluate CTG-Insight on the NeuroFetalNet Dataset and compare
it against deep learning models and the single-agent LLM baseline. Results show
that CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score
(97.8%) while producing transparent and interpretable outputs. This work
contributes an interpretable and extensible CTG analysis framework.

</details>


### [24] [Explainability-Driven Feature Engineering for Mid-Term Electricity Load Forecasting in ERCOT's SCENT Region](https://arxiv.org/abs/2507.22220)
*Abhiram Bhupatiraju,Sung Bum Ahn*

Main category: cs.LG

TL;DR: 比较分析机器学习模型（线性回归、XGBoost、LightGBM、LSTM）用于电力负荷预测，并利用SHAP提升模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 电力负荷预测对电力系统运行至关重要，需捕捉非线性模式以支持长期规划。

Method: 使用线性回归、XGBoost、LightGBM和LSTM模型，结合SHAP方法量化特征贡献。

Result: 模型比较结果未明确提及，但SHAP提升了模型透明度和预测准确性。

Conclusion: SHAP方法有助于改进特征工程和模型解释，对电力负荷预测有重要意义。

Abstract: Accurate load forecasting is essential to the operation of modern electric
power systems. Given the sensitivity of electricity demand to weather
variability and temporal dynamics, capturing non-linear patterns is essential
for long-term planning. This paper presents a comparative analysis of machine
learning models, Linear Regression, XGBoost, LightGBM, and Long Short-Term
Memory (LSTM), for forecasting system-wide electricity load up to one year in
advance. Midterm forecasting has shown to be crucial for maintenance
scheduling, resource allocation, financial forecasting, and market
participation. The paper places a focus on the use of a method called "Shapley
Additive Explanations" (SHAP) to improve model explainability. SHAP enables the
quantification of feature contributions, guiding informed feature engineering
and improving both model transparency and forecasting accuracy.

</details>


### [25] [TRIBE: TRImodal Brain Encoder for whole-brain fMRI response prediction](https://arxiv.org/abs/2507.22229)
*Stéphane d'Ascoli,Jérémy Rapin,Yohann Benchetrit,Hubert Banville,Jean-Rémi King*

Main category: cs.LG

TL;DR: TRIBE是一个多模态深度神经网络，通过结合文本、音频和视频的预训练表示，成功预测了大脑对刺激的反应，并在Algonauts 2025竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 神经科学领域长期专注于单一模态或脑区研究，缺乏统一的认知模型。TRIBE旨在填补这一空白。

Method: 结合文本、音频和视频的预训练表示，使用transformer处理时间动态性，预测大脑的fMRI响应。

Result: 在Algonauts 2025竞赛中显著领先，多模态模型在高阶联合皮层表现优于单模态模型。

Conclusion: TRIBE为构建统一的大脑表征模型奠定了基础，未来可扩展至感知和理解领域。

Abstract: Historically, neuroscience has progressed by fragmenting into specialized
domains, each focusing on isolated modalities, tasks, or brain regions. While
fruitful, this approach hinders the development of a unified model of
cognition. Here, we introduce TRIBE, the first deep neural network trained to
predict brain responses to stimuli across multiple modalities, cortical areas
and individuals. By combining the pretrained representations of text, audio and
video foundational models and handling their time-evolving nature with a
transformer, our model can precisely model the spatial and temporal fMRI
responses to videos, achieving the first place in the Algonauts 2025 brain
encoding competition with a significant margin over competitors. Ablations show
that while unimodal models can reliably predict their corresponding cortical
networks (e.g. visual or auditory networks), they are systematically
outperformed by our multimodal model in high-level associative cortices.
Currently applied to perception and comprehension, our approach paves the way
towards building an integrative model of representations in the human brain.
Our code is available at https://github.com/facebookresearch/algonauts-2025.

</details>


### [26] [Using Scaling Laws for Data Source Utility Estimation in Domain-Specific Pre-Training](https://arxiv.org/abs/2507.22250)
*Oleksiy Ostapenko,Charles Guille-Escuret,Luke Kumar,Max Tian,Denis Kocetkov,Gopeshh Subbaraj,Raymond Li,Joel Lamy-Poirier,Sebastien Paquet,Torsten Scholak*

Main category: cs.LG

TL;DR: 提出了一种优化领域特定数据集构建的框架，通过多轮退火运行估计数据源的扩展规律，以支持资源分配决策。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖点估计可能导致误导的问题，特别是在不同计算规模下数据扩展规律不一致的情况下。

Method: 扩展了点估计方法，通过多轮退火运行分析数据源的扩展规律，结合性能和成本进行资源分配优化。

Result: 实验验证了该方法在医学和数学领域的有效性，表明多轮退火运行能更准确地估计数据源扩展行为。

Conclusion: 该方法支持数据驱动的数据源选择和优化决策，提升了领域特定模型训练的效率和效果。

Abstract: We introduce a framework for optimizing domain-specific dataset construction
in foundation model training. Specifically, we seek a cost-efficient way to
estimate the quality of data sources (e.g. synthetically generated or filtered
web data, etc.) in order to make optimal decisions about resource allocation
for data sourcing from these sources for the stage two pre-training phase, aka
annealing, with the goal of specializing a generalist pre-trained model to
specific domains. Our approach extends the usual point estimate approaches, aka
micro-annealing, to estimating scaling laws by performing multiple annealing
runs of varying compute spent on data curation and training. This addresses a
key limitation in prior work, where reliance on point estimates for data
scaling decisions can be misleading due to the lack of rank invariance across
compute scales -- a phenomenon we confirm in our experiments. By systematically
analyzing performance gains relative to acquisition costs, we find that scaling
curves can be estimated for different data sources. Such scaling laws can
inform cost effective resource allocation across different data acquisition
methods (e.g. synthetic data), data sources (e.g. user or web data) and
available compute resources. We validate our approach through experiments on a
pre-trained model with 7 billion parameters. We adapt it to: a domain
well-represented in the pre-training data -- the medical domain, and a domain
underrepresented in the pretraining corpora -- the math domain. We show that
one can efficiently estimate the scaling behaviors of a data source by running
multiple annealing runs, which can lead to different conclusions, had one used
point estimates using the usual micro-annealing technique instead. This enables
data-driven decision-making for selecting and optimizing data sources.

</details>


### [27] [Agent-centric learning: from external reward maximization to internal knowledge curation](https://arxiv.org/abs/2507.22255)
*Hanqi Zhou,Fryderyk Mantiuk,David G. Nagy,Charley M. Wu*

Main category: cs.LG

TL;DR: 论文提出了一种新的学习范式——表征赋权，强调通过内部知识结构的可控性和多样性来实现更通用的智能。


<details>
  <summary>Details</summary>
Motivation: 传统智能研究过于关注外部目标和任务，导致智能体缺乏适应性。作者希望通过内部表征赋权来提升智能体的通用性和准备性。

Method: 提出表征赋权，将控制焦点转向内部，衡量智能体对自身知识结构的可控性和多样化能力。

Result: 表征赋权为设计适应性强的智能系统提供了新视角。

Conclusion: 内部表征赋权是实现通用智能的关键要素，区别于传统的外部控制方法。

Abstract: The pursuit of general intelligence has traditionally centered on external
objectives: an agent's control over its environments or mastery of specific
tasks. This external focus, however, can produce specialized agents that lack
adaptability. We propose representational empowerment, a new perspective
towards a truly agent-centric learning paradigm by moving the locus of control
inward. This objective measures an agent's ability to controllably maintain and
diversify its own knowledge structures. We posit that the capacity -- to shape
one's own understanding -- is an element for achieving better ``preparedness''
distinct from direct environmental influence. Focusing on internal
representations as the main substrate for computing empowerment offers a new
lens through which to design adaptable intelligent systems.

</details>


### [28] [Weighted Conditional Flow Matching](https://arxiv.org/abs/2507.22270)
*Sergio Calvo-Ordonez,Matthieu Meunier,Alvaro Cartea,Christoph Reisinger,Yarin Gal,Jose Miguel Hernandez-Lobato*

Main category: cs.LG

TL;DR: W-CFM是一种改进的条件流匹配方法，通过加权训练对提升性能，避免了计算密集型的小批量最优传输。


<details>
  <summary>Details</summary>
Motivation: 标准CFM生成的路径偏离直线插值，导致生成效率低且不准确。现有改进方法依赖计算昂贵的最优传输，需要更高效的替代方案。

Method: 提出加权条件流匹配（W-CFM），通过Gibbs核加权训练对，恢复熵最优传输耦合，减少计算负担。

Result: W-CFM在合成和真实数据集上表现出与基线相当或更优的样本质量、保真度和多样性，同时保持计算效率。

Conclusion: W-CFM在性能和效率上优于标准CFM和其他基线，为条件流匹配提供了更优的解决方案。

Abstract: Conditional flow matching (CFM) has emerged as a powerful framework for
training continuous normalizing flows due to its computational efficiency and
effectiveness. However, standard CFM often produces paths that deviate
significantly from straight-line interpolations between prior and target
distributions, making generation slower and less accurate due to the need for
fine discretization at inference. Recent methods enhance CFM performance by
inducing shorter and straighter trajectories but typically rely on
computationally expensive mini-batch optimal transport (OT). Drawing insights
from entropic optimal transport (EOT), we propose Weighted Conditional Flow
Matching (W-CFM), a novel approach that modifies the classical CFM loss by
weighting each training pair $(x, y)$ with a Gibbs kernel. We show that this
weighting recovers the entropic OT coupling up to some bias in the marginals,
and we provide the conditions under which the marginals remain nearly
unchanged. Moreover, we establish an equivalence between W-CFM and the
minibatch OT method in the large-batch limit, showing how our method overcomes
computational and performance bottlenecks linked to batch size. Empirically, we
test our method on unconditional generation on various synthetic and real
datasets, confirming that W-CFM achieves comparable or superior sample quality,
fidelity, and diversity to other alternative baselines while maintaining the
computational efficiency of vanilla CFM.

</details>


### [29] [Comparing Cluster-Based Cross-Validation Strategies for Machine Learning Model Evaluation](https://arxiv.org/abs/2507.22299)
*Afonso Martini Spezia,Mariana Recamonde-Mendoza*

Main category: cs.LG

TL;DR: 论文研究了基于聚类的交叉验证策略，提出了一种结合Mini Batch K-Means和类分层的新技术，并在平衡和不平衡数据集上进行了实验比较。


<details>
  <summary>Details</summary>
Motivation: 交叉验证在机器学习中至关重要，但传统方法可能导致数据子集无法充分代表原始数据的多样性，从而产生偏差。本文旨在探索基于聚类的交叉验证策略，以提高模型评估的鲁棒性。

Method: 提出了一种结合Mini Batch K-Means和类分层的新交叉验证技术，并在20个数据集上进行了实验，比较了不同策略在偏差、方差和计算成本上的表现。

Result: 在平衡数据集上，新方法在偏差和方差上表现更优，但未显著降低计算成本；在不平衡数据集上，传统的分层交叉验证表现更稳定。

Conclusion: 研究为模型评估策略提供了新视角，验证了聚类技术的潜力，同时肯定了传统分层交叉验证的可靠性，尤其是在不平衡数据场景中。

Abstract: Cross-validation plays a fundamental role in Machine Learning, enabling
robust evaluation of model performance and preventing overestimation on
training and validation data. However, one of its drawbacks is the potential to
create data subsets (folds) that do not adequately represent the diversity of
the original dataset, which can lead to biased performance estimates. The
objective of this work is to deepen the investigation of cluster-based
cross-validation strategies by analyzing the performance of different
clustering algorithms through experimental comparison. Additionally, a new
cross-validation technique that combines Mini Batch K-Means with class
stratification is proposed. Experiments were conducted on 20 datasets (both
balanced and imbalanced) using four supervised learning algorithms, comparing
cross-validation strategies in terms of bias, variance, and computational cost.
The technique that uses Mini Batch K-Means with class stratification
outperformed others in terms of bias and variance on balanced datasets, though
it did not significantly reduce computational cost. On imbalanced datasets,
traditional stratified cross-validation consistently performed better, showing
lower bias, variance, and computational cost, making it a safe choice for
performance evaluation in scenarios with class imbalance. In the comparison of
different clustering algorithms, no single algorithm consistently stood out as
superior. Overall, this work contributes to improving predictive model
evaluation strategies by providing a deeper understanding of the potential of
cluster-based data splitting techniques and reaffirming the effectiveness of
well-established strategies like stratified cross-validation. Moreover, it
highlights perspectives for increasing the robustness and reliability of model
evaluations, especially in datasets with clustering characteristics.

</details>


### [30] [CS-SHRED: Enhancing SHRED for Robust Recovery of Spatiotemporal Dynamics](https://arxiv.org/abs/2507.22303)
*Romulo B. da Silva,Cássio M. Oishi,Diego Passos,J. Nathan Kutz*

Main category: cs.LG

TL;DR: CS-SHRED是一种新型深度学习架构，结合压缩感知（CS）和浅层循环解码器（SHRED），用于从不完整、压缩或损坏的数据中重建时空动态。其创新点包括CS技术的集成和自适应损失函数的设计，显著提高了重建精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏传感器布置、噪声测量和不完整数据采集等场景下时空动态重建的挑战。

Method: 将CS技术集成到SHRED架构中，采用基于批量的前向框架和ℓ1正则化；设计自适应损失函数，动态结合MSE、MAE和SNR正则化。

Result: 在多种复杂问题上验证，相比传统SHRED，CS-SHRED显著提高了重建保真度（SSIM、PSNR、LPIPS等指标改善），并增强了对噪声和异常值的鲁棒性。

Conclusion: CS-SHRED通过联合训练CS和SHRED架构，结合LSTM序列模型和浅层解码网络，成为一种在环境、气候和科学数据分析中具有广泛应用前景的工具。

Abstract: We present $\textbf{CS-SHRED}$, a novel deep learning architecture that
integrates Compressed Sensing (CS) into a Shallow Recurrent Decoder
($\textbf{SHRED}$) to reconstruct spatiotemporal dynamics from incomplete,
compressed, or corrupted data. Our approach introduces two key innovations.
First, by incorporating CS techniques into the $\textbf{SHRED}$ architecture,
our method leverages a batch-based forward framework with $\ell_1$
regularization to robustly recover signals even in scenarios with sparse sensor
placements, noisy measurements, and incomplete sensor acquisitions. Second, an
adaptive loss function dynamically combines Mean Squared Error (MSE) and Mean
Absolute Error (MAE) terms with a piecewise Signal-to-Noise Ratio (SNR)
regularization, which suppresses noise and outliers in low-SNR regions while
preserving fine-scale features in high-SNR regions.
  We validate $\textbf{CS-SHRED}$ on challenging problems including
viscoelastic fluid flows, maximum specific humidity fields, sea surface
temperature distributions, and rotating turbulent flows. Compared to the
traditional $\textbf{SHRED}$ approach, $\textbf{CS-SHRED}$ achieves
significantly higher reconstruction fidelity - as demonstrated by improved SSIM
and PSNR values, lower normalized errors, and enhanced LPIPS scores-thereby
providing superior preservation of small-scale structures and increased
robustness against noise and outliers.
  Our results underscore the advantages of the jointly trained CS and SHRED
design architecture which includes an LSTM sequence model for characterizing
the temporal evolution with a shallow decoder network (SDN) for modeling the
high-dimensional state space. The SNR-guided adaptive loss function for the
spatiotemporal data recovery establishes $\textbf{CS-SHRED}$ as a promising
tool for a wide range of applications in environmental, climatic, and
scientific data analyses.

</details>


### [31] [Hypernetworks for Model-Heterogeneous Personalized Federated Learning](https://arxiv.org/abs/2507.22330)
*Chen Zhang,Husheng Li,Xiang Liu,Linshan Jiang,Danxin Wang*

Main category: cs.LG

TL;DR: 提出了一种基于超网络的个性化联邦学习框架MH-pFedHN，通过多头部结构促进知识共享，并进一步提出MH-pFedHNGD集成轻量级全局模型以提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有个性化联邦学习方法依赖外部数据、模型解耦或部分学习策略的局限性，提升实用性和扩展性。

Method: 利用服务器端超网络生成个性化参数，引入多头部结构共享知识，并集成轻量级全局模型。

Result: 在多个基准测试和模型设置中表现出竞争性准确率和强泛化能力。

Conclusion: 该框架无需外部数据或披露模型架构，提供了隐私和灵活性，为未来研究奠定了坚实基础。

Abstract: Recent advances in personalized federated learning have focused on addressing
client model heterogeneity. However, most existing methods still require
external data, rely on model decoupling, or adopt partial learning strategies,
which can limit their practicality and scalability. In this paper, we revisit
hypernetwork-based methods and leverage their strong generalization
capabilities to design a simple yet effective framework for heterogeneous
personalized federated learning. Specifically, we propose MH-pFedHN, which
leverages a server-side hypernetwork that takes client-specific embedding
vectors as input and outputs personalized parameters tailored to each client's
heterogeneous model. To promote knowledge sharing and reduce computation, we
introduce a multi-head structure within the hypernetwork, allowing clients with
similar model sizes to share heads. Furthermore, we further propose
MH-pFedHNGD, which integrates an optional lightweight global model to improve
generalization. Our framework does not rely on external datasets and does not
require disclosure of client model architectures, thereby offering enhanced
privacy and flexibility. Extensive experiments on multiple benchmarks and model
settings demonstrate that our approach achieves competitive accuracy, strong
generalization, and serves as a robust baseline for future research in
model-heterogeneous personalized federated learning.

</details>


### [32] [Parametrized Multi-Agent Routing via Deep Attention Models](https://arxiv.org/abs/2507.22338)
*Salar Basiri,Dhananjay Tiwari,Srinivasa M. Salapaka*

Main category: cs.LG

TL;DR: 提出了一种可扩展的深度学习框架ParaSDM，用于参数化序列决策，结合离散动作策略和共享连续参数优化，特别适用于设施选址和路径优化（FLPO）问题。


<details>
  <summary>Details</summary>
Motivation: FLPO问题是NP难问题，具有混合离散-连续结构和非凸目标，传统方法难以高效求解。

Method: 结合最大熵原理（MEP）和最短路径网络（SPN），一种排列不变的编码器-解码器模型，实现高效梯度优化。

Result: SPN比MEP基线快100倍，最优性差距约6%；FLPO方法比元启发式基线成本低10倍，速度更快，与Gurobi最优解匹配但快1500倍。

Conclusion: 结构化深度模型在大规模混合整数优化任务中表现出强大能力，为ParaSDM问题设定了新标准。

Abstract: We propose a scalable deep learning framework for parametrized sequential
decision-making (ParaSDM), where multiple agents jointly optimize discrete
action policies and shared continuous parameters. A key subclass of this
setting arises in Facility-Location and Path Optimization (FLPO), where
multi-agent systems must simultaneously determine optimal routes and facility
locations, aiming to minimize the cumulative transportation cost within the
network. FLPO problems are NP-hard due to their mixed discrete-continuous
structure and highly non-convex objective. To address this, we integrate the
Maximum Entropy Principle (MEP) with a neural policy model called the Shortest
Path Network (SPN)-a permutation-invariant encoder-decoder that approximates
the MEP solution while enabling efficient gradient-based optimization over
shared parameters. The SPN achieves up to 100$\times$ speedup in policy
inference and gradient computation compared to MEP baselines, with an average
optimality gap of approximately 6% across a wide range of problem sizes. Our
FLPO approach yields over 10$\times$ lower cost than metaheuristic baselines
while running significantly faster, and matches Gurobi's optimal cost with
annealing at a 1500$\times$ speedup-establishing a new state of the art for
ParaSDM problems. These results highlight the power of structured deep models
for solving large-scale mixed-integer optimization tasks.

</details>


### [33] [MSQ: Memory-Efficient Bit Sparsification Quantization](https://arxiv.org/abs/2507.22349)
*Seokho Han,Seoyeon Yoon,Jinhee Kim,Dongwei Wang,Kang Eun Jeon,Huanrui Yang,Jong Hwan Ko*

Main category: cs.LG

TL;DR: 提出了一种名为MSQ的内存高效位稀疏量化方法，通过圆钳位量化器和Hessian信息，显著减少训练参数和时间，同时保持精度。


<details>
  <summary>Details</summary>
Motivation: 随着DNN在移动和边缘设备上的部署增加，优化模型效率变得至关重要。混合精度量化虽优于均匀量化，但确定每层最优精度仍具挑战性。

Method: MSQ采用圆钳位量化器实现权重的可微分计算，并通过正则化诱导稀疏性，同时利用Hessian信息修剪多个LSB，提升训练效率。

Result: 实验显示，MSQ可减少8倍训练参数和86%训练时间，同时保持竞争性精度和压缩率。

Conclusion: MSQ为资源受限设备上的高效DNN训练提供了实用解决方案。

Abstract: As deep neural networks (DNNs) see increased deployment on mobile and edge
devices, optimizing model efficiency has become crucial. Mixed-precision
quantization is widely favored, as it offers a superior balance between
efficiency and accuracy compared to uniform quantization. However, finding the
optimal precision for each layer is challenging. Recent studies utilizing
bit-level sparsity have shown promise, yet they often introduce substantial
training complexity and high GPU memory requirements. In this paper, we propose
Memory-Efficient Bit Sparsification Quantization (MSQ), a novel approach that
addresses these limitations. MSQ applies a round-clamp quantizer to enable
differentiable computation of the least significant bits (LSBs) from model
weights. It further employs regularization to induce sparsity in these LSBs,
enabling effective precision reduction without explicit bit-level parameter
splitting. Additionally, MSQ incorporates Hessian information, allowing the
simultaneous pruning of multiple LSBs to further enhance training efficiency.
Experimental results show that MSQ achieves up to 8.00x reduction in trainable
parameters and up to 86% reduction in training time compared to previous
bit-level quantization, while maintaining competitive accuracy and compression
rates. This makes it a practical solution for training efficient DNNs on
resource-constrained devices.

</details>


### [34] [Prediction of acoustic field in 1-D uniform duct with varying mean flow and temperature using neural networks](https://arxiv.org/abs/2507.22370)
*D. Veerababu,Prasanta K. Ghosh*

Main category: cs.LG

TL;DR: 本文提出了一种基于神经网络的物理约束方法，用于预测一维管道中非均匀介质声传播的声压和粒子速度，并通过传统数值方法验证。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络在物理约束下的应用，以解决声传播问题，并研究温度梯度对声场的影响。

Method: 将问题转化为无约束优化问题，利用神经网络求解，并结合迁移学习和自动微分技术。

Result: 成功预测了声压和粒子速度，并与Runge-Kutta方法结果一致。

Conclusion: 神经网络在声学应用中具有潜力，结合机器学习技术可提高效率。

Abstract: Neural networks constrained by the physical laws emerged as an alternate
numerical tool. In this paper, the governing equation that represents the
propagation of sound inside a one-dimensional duct carrying a heterogeneous
medium is derived. The problem is converted into an unconstrained optimization
problem and solved using neural networks. Both the acoustic state variables:
acoustic pressure and particle velocity are predicted and validated with the
traditional Runge-Kutta solver. The effect of the temperature gradient on the
acoustic field is studied. Utilization of machine learning techniques such as
transfer learning and automatic differentiation for acoustic applications is
demonstrated.

</details>


### [35] [Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance](https://arxiv.org/abs/2507.22424)
*Songsheng Wang,Rucheng Yu,Zhihang Yuan,Chao Yu,Feng Gao,Yu Wang,Derek F. Wong*

Main category: cs.LG

TL;DR: Spec-VLA框架通过改进的推测解码技术加速Vision-Language-Action模型，提升44%的接受长度和1.42倍速度，保持成功率。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型因参数规模和自回归解码导致计算需求高，推测解码在LLMs中有效但未应用于VLA模型。

Method: 提出Spec-VLA框架，利用动作令牌的相对距离放松接受机制，优化推测解码。

Result: 实验显示Spec-VLA提升44%接受长度，速度提升1.42倍，成功率不变。

Conclusion: Spec-VLA展示了推测执行在VLA预测中的潜力，为更广泛应用提供可能。

Abstract: Vision-Language-Action (VLA) models have made substantial progress by
leveraging the robust capabilities of Visual Language Models (VLMs). However,
VLMs' significant parameter size and autoregressive (AR) decoding nature impose
considerable computational demands on VLA models. While Speculative Decoding
(SD) has shown efficacy in accelerating Large Language Models (LLMs) by
incorporating efficient drafting and parallel verification, allowing multiple
tokens to be generated in one forward pass, its application to VLA models
remains unexplored. This work introduces Spec-VLA, an SD framework designed to
accelerate VLA models. Due to the difficulty of the action prediction task and
the greedy decoding mechanism of the VLA models, the direct application of the
advanced SD framework to the VLA prediction task yields a minor speed
improvement. To boost the generation speed, we propose an effective mechanism
to relax acceptance utilizing the relative distances represented by the action
tokens of the VLA model. Empirical results across diverse test scenarios affirm
the effectiveness of the Spec-VLA framework, and further analysis substantiates
the impact of our proposed strategies, which enhance the acceptance length by
44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without
compromising the success rate. The success of the Spec-VLA framework highlights
the potential for broader application of speculative execution in VLA
prediction scenarios.

</details>


### [36] [Multimodal Late Fusion Model for Problem-Solving Strategy Classification in a Machine Learning Game](https://arxiv.org/abs/2507.22426)
*Clemens Witt,Thiemo Leonhardt,Nadine Bergner,Mareen Grillenberger*

Main category: cs.LG

TL;DR: 提出了一种多模态融合模型，结合视觉数据和游戏行为序列，显著提升了学生问题解决策略的分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖抽象的游戏日志数据，可能忽略与学习者认知策略相关的细微行为线索。

Method: 采用多模态晚期融合模型，整合屏幕录像视觉数据和结构化游戏内行为序列。

Result: 在149名中学生的试点研究中，融合模型的分类准确率比单模态基线模型提高了15%以上。

Conclusion: 多模态机器学习在交互式学习环境中具有策略敏感评估和自适应支持的潜力。

Abstract: Machine learning models are widely used to support stealth assessment in
digital learning environments. Existing approaches typically rely on abstracted
gameplay log data, which may overlook subtle behavioral cues linked to
learners' cognitive strategies. This paper proposes a multimodal late fusion
model that integrates screencast-based visual data and structured in-game
action sequences to classify students' problem-solving strategies. In a pilot
study with secondary school students (N=149) playing a multitouch educational
game, the fusion model outperformed unimodal baseline models, increasing
classification accuracy by over 15%. Results highlight the potential of
multimodal ML for strategy-sensitive assessment and adaptive support in
interactive learning contexts.

</details>


### [37] [Theoretical Analysis of Relative Errors in Gradient Computations for Adversarial Attacks with CE Loss](https://arxiv.org/abs/2507.22428)
*Yunrui Yu,Hang Su,Cheng-zhong Xu,Zhizhong Su,Jun Zhu*

Main category: cs.LG

TL;DR: 论文分析了基于梯度的对抗攻击中交叉熵损失的浮点计算误差，提出了T-MIFPE损失函数以提升攻击效果。


<details>
  <summary>Details</summary>
Motivation: 研究浮点计算误差对梯度攻击的影响，解决现有方法中的过估计问题。

Method: 理论分析四种攻击场景下的浮点误差，提出T-MIFPE损失函数，引入最优缩放因子。

Result: T-MIFPE在MNIST、CIFAR-10和CIFAR-100数据集上表现优于现有损失函数。

Conclusion: T-MIFPE能有效减少浮点误差，提升对抗攻击的准确性和鲁棒性评估。

Abstract: Gradient-based adversarial attacks using the Cross-Entropy (CE) loss often
suffer from overestimation due to relative errors in gradient computation
induced by floating-point arithmetic. This paper provides a rigorous
theoretical analysis of these errors, conducting the first comprehensive study
of floating-point computation errors in gradient-based attacks across four
distinct scenarios: (i) unsuccessful untargeted attacks, (ii) successful
untargeted attacks, (iii) unsuccessful targeted attacks, and (iv) successful
targeted attacks. We establish theoretical foundations characterizing the
behavior of relative numerical errors under different attack conditions,
revealing previously unknown patterns in gradient computation instability, and
identify floating-point underflow and rounding as key contributors. Building on
this insight, we propose the Theoretical MIFPE (T-MIFPE) loss function, which
incorporates an optimal scaling factor $T = t^*$ to minimize the impact of
floating-point errors, thereby enhancing the accuracy of gradient computation
in adversarial attacks. Extensive experiments on the MNIST, CIFAR-10, and
CIFAR-100 datasets demonstrate that T-MIFPE outperforms existing loss
functions, including CE, C\&W, DLR, and MIFPE, in terms of attack potency and
robustness evaluation accuracy.

</details>


### [38] [RANA: Robust Active Learning for Noisy Network Alignment](https://arxiv.org/abs/2507.22434)
*Yixuan Nan,Xixun Lin,Yanmin Shang,Zhuofan Li,Can Zhao,Yanan Cao*

Main category: cs.LG

TL;DR: RANA是一个鲁棒的主动学习框架，用于解决网络对齐中的噪声问题，包括结构噪声和标签噪声，同时处理锚链接注释的稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现有网络对齐工作主要关注标签稀疏性问题，而忽略了噪声问题（如结构噪声和标签噪声），这会显著影响模型性能。

Method: RANA提出噪声感知选择模块和标签去噪模块，分别处理结构噪声和标签噪声。前者通过噪声感知最大化目标和清洁度评分选择节点对，后者采用多源融合去噪策略。

Result: 在三个真实数据集上，RANA在对齐准确性上优于现有的主动学习方法。

Conclusion: RANA通过噪声感知和去噪策略，显著提升了网络对齐模型的鲁棒性和准确性。

Abstract: Network alignment has attracted widespread attention in various fields.
However, most existing works mainly focus on the problem of label sparsity,
while overlooking the issue of noise in network alignment, which can
substantially undermine model performance. Such noise mainly includes
structural noise from noisy edges and labeling noise caused by human-induced
and process-driven errors. To address these problems, we propose RANA, a Robust
Active learning framework for noisy Network Alignment. RANA effectively tackles
both structure noise and label noise while addressing the sparsity of anchor
link annotations, which can improve the robustness of network alignment models.
Specifically, RANA introduces the proposed Noise-aware Selection Module and the
Label Denoising Module to address structural noise and labeling noise,
respectively. In the first module, we design a noise-aware maximization
objective to select node pairs, incorporating a cleanliness score to address
structural noise. In the second module, we propose a novel multi-source fusion
denoising strategy that leverages model and twin node pairs labeling to provide
more accurate labels for node pairs. Empirical results on three real-world
datasets demonstrate that RANA outperforms state-of-the-art active
learning-based methods in alignment accuracy. Our code is available at
https://github.com/YXNan0110/RANA.

</details>


### [39] [RCR-AF: Enhancing Model Generalization via Rademacher Complexity Reduction Activation Function](https://arxiv.org/abs/2507.22446)
*Yunrui Yu,Kafeng Wang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种新型激活函数RCR-AF，结合了GELU和ReLU的优点，并通过控制稀疏性和容量提升模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在对抗攻击下表现脆弱，尤其在安全敏感应用中风险显著。激活函数作为关键但未充分探索的组件，可能提升模型鲁棒性。

Method: 设计了RCR-AF激活函数，结合GELU的平滑性、梯度稳定性和负信息保留与ReLU的单调性，通过超参数α和γ控制稀疏性和容量。

Result: 理论分析表明RCR-AF能调节模型的Rademacher复杂度，实验证明其在标准训练和对抗训练中均优于ReLU、GELU和Swish。

Conclusion: RCR-AF为提升模型鲁棒性提供了一种理论支持且有效的方法。

Abstract: Despite their widespread success, deep neural networks remain critically
vulnerable to adversarial attacks, posing significant risks in safety-sensitive
applications. This paper investigates activation functions as a crucial yet
underexplored component for enhancing model robustness. We propose a Rademacher
Complexity Reduction Activation Function (RCR-AF), a novel activation function
designed to improve both generalization and adversarial resilience. RCR-AF
uniquely combines the advantages of GELU (including smoothness, gradient
stability, and negative information retention) with ReLU's desirable
monotonicity, while simultaneously controlling both model sparsity and capacity
through built-in clipping mechanisms governed by two hyperparameters, $\alpha$
and $\gamma$. Our theoretical analysis, grounded in Rademacher complexity,
demonstrates that these parameters directly modulate the model's Rademacher
complexity, offering a principled approach to enhance robustness. Comprehensive
empirical evaluations show that RCR-AF consistently outperforms widely-used
alternatives (ReLU, GELU, and Swish) in both clean accuracy under standard
training and in adversarial robustness within adversarial training paradigms.

</details>


### [40] [Towards Interpretable Renal Health Decline Forecasting via Multi-LMM Collaborative Reasoning Framework](https://arxiv.org/abs/2507.22464)
*Peng-Yi Wu,Pei-Cing Huang,Ting-Yu Chen,Chantung Ku,Ming-Yen Lin,Yihuang Kang*

Main category: cs.LG

TL;DR: 提出了一种协作框架，提升开源LMMs在eGFR预测中的性能，同时生成临床可解释的解释。


<details>
  <summary>Details</summary>
Motivation: 准确且可解释的eGFR预测对管理慢性肾病和临床决策至关重要，但现有LMMs在部署成本、数据隐私和可靠性方面存在挑战。

Method: 结合视觉知识迁移、溯因推理和短期记忆机制，提升预测准确性和可解释性。

Result: 实验表明，该框架在预测性能和可解释性上与专有模型相当，并提供临床推理过程。

Conclusion: 该方法为构建兼具预测准确性和临床可解释性的医疗AI系统提供了新思路。

Abstract: Accurate and interpretable prediction of estimated glomerular filtration rate
(eGFR) is essential for managing chronic kidney disease (CKD) and supporting
clinical decisions. Recent advances in Large Multimodal Models (LMMs) have
shown strong potential in clinical prediction tasks due to their ability to
process visual and textual information. However, challenges related to
deployment cost, data privacy, and model reliability hinder their adoption. In
this study, we propose a collaborative framework that enhances the performance
of open-source LMMs for eGFR forecasting while generating clinically meaningful
explanations. The framework incorporates visual knowledge transfer, abductive
reasoning, and a short-term memory mechanism to enhance prediction accuracy and
interpretability. Experimental results show that the proposed framework
achieves predictive performance and interpretability comparable to proprietary
models. It also provides plausible clinical reasoning processes behind each
prediction. Our method sheds new light on building AI systems for healthcare
that combine predictive accuracy with clinically grounded interpretability.

</details>


### [41] [Proto-EVFL: Enhanced Vertical Federated Learning via Dual Prototype with Extremely Unaligned Data](https://arxiv.org/abs/2507.22488)
*Wei Guo,Yiyang Duan,Zhaojun Hu,Yiqi Tong,Fuzhen Zhuang,Xiao Zhang,Jin Dong,Ruofan Wu,Tengfei Liu,Yifan Sun*

Main category: cs.LG

TL;DR: Proto-EVFL通过双原型增强垂直联邦学习，解决类别不平衡问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习中，未对齐样本的类别不平衡导致特征表示不足和模型预测空间受限。

Method: 引入类原型和概率双原型学习方案，动态选择样本，并采用自适应门控特征聚合策略。

Result: 在零样本场景下，Proto-EVFL性能优于基线至少6.97%。

Conclusion: Proto-EVFL是首个双层次优化框架，具有收敛性，能有效解决类别不平衡问题。

Abstract: In vertical federated learning (VFL), multiple enterprises address aligned
sample scarcity by leveraging massive locally unaligned samples to facilitate
collaborative learning. However, unaligned samples across different parties in
VFL can be extremely class-imbalanced, leading to insufficient feature
representation and limited model prediction space. Specifically,
class-imbalanced problems consist of intra-party class imbalance and
inter-party class imbalance, which can further cause local model bias and
feature contribution inconsistency issues, respectively. To address the above
challenges, we propose Proto-EVFL, an enhanced VFL framework via dual
prototypes. We first introduce class prototypes for each party to learn
relationships between classes in the latent space, allowing the active party to
predict unseen classes. We further design a probabilistic dual prototype
learning scheme to dynamically select unaligned samples by conditional optimal
transport cost with class prior probability. Moreover, a mixed prior guided
module guides this selection process by combining local and global class prior
probabilities. Finally, we adopt an \textit{adaptive gated feature aggregation
strategy} to mitigate feature contribution inconsistency by dynamically
weighting and aggregating local features across different parties. We proved
that Proto-EVFL, as the first bi-level optimization framework in VFL, has a
convergence rate of 1/\sqrt T. Extensive experiments on various datasets
validate the superiority of our Proto-EVFL. Even in a zero-shot scenario with
one unseen class, it outperforms baselines by at least 6.97%

</details>


### [42] [LoReUn: Data Itself Implicitly Provides Cues to Improve Machine Unlearning](https://arxiv.org/abs/2507.22499)
*Xiang Li,Qianli Shen,Haonan Wang,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: 论文提出了一种基于损失动态重加权的方法（LoReUn），用于改进机器遗忘（MU）技术，有效减少有害内容生成。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法对所有待遗忘数据赋予相同权重，难以有效处理难以遗忘的数据，导致有害内容生成风险。

Method: 提出LoReUn策略，利用数据损失动态调整权重，减少计算开销。

Result: 在图像分类和生成任务中显著缩小现有MU方法与精确遗忘的差距，有效减少有害内容生成。

Conclusion: LoReUn是一种简单高效的插件式方法，提升了机器遗忘的效果。

Abstract: Recent generative models face significant risks of producing harmful content,
which has underscored the importance of machine unlearning (MU) as a critical
technique for eliminating the influence of undesired data. However, existing MU
methods typically assign the same weight to all data to be forgotten, which
makes it difficult to effectively forget certain data that is harder to unlearn
than others. In this paper, we empirically demonstrate that the loss of data
itself can implicitly reflect its varying difficulty. Building on this insight,
we introduce Loss-based Reweighting Unlearning (LoReUn), a simple yet effective
plug-and-play strategy that dynamically reweights data during the unlearning
process with minimal additional computational overhead. Our approach
significantly reduces the gap between existing MU methods and exact unlearning
in both image classification and generation tasks, effectively enhancing the
prevention of harmful content generation in text-to-image diffusion models.

</details>


### [43] [Geometry of nonlinear forecast reconciliation](https://arxiv.org/abs/2507.22500)
*Lorenzo Nespoli,Anubhab Biswas,Vasco Medici*

Main category: cs.LG

TL;DR: 本文填补了非线性背景下预测协调的误差减少理论空白，提出了针对不同非线性超曲面和向量值函数的定理，并发布了相关Python工具包。


<details>
  <summary>Details</summary>
Motivation: 尽管预测协调在概率设置中已有扩展，但非线性背景下的误差减少理论仍缺乏，本文旨在填补这一空白。

Method: 针对具有恒定符号曲率的超曲面和更广泛的非恒定符号曲率超曲面及向量值函数，建立了误差减少的定理。

Result: 提出了与Panagiotelis等人(2021)类似的定理，并为更广泛的非线性情况提供了概率保证。

Conclusion: 本文的理论和工具包支持了非线性预测协调的实用性和可重复性。

Abstract: Forecast reconciliation, an ex-post technique applied to forecasts that must
satisfy constraints, has been a prominent topic in the forecasting literature
over the past two decades. Recently, several efforts have sought to extend
reconciliation methods to the probabilistic settings. Nevertheless, formal
theorems demonstrating error reduction in nonlinear contexts, analogous to
those presented in Panagiotelis et al.(2021), are still lacking. This paper
addresses that gap by establishing such theorems for various classes of
nonlinear hypersurfaces and vector-valued functions. Specifically, we derive an
exact analog of Theorem 3.1 from Panagiotelis et al.(2021) for hypersurfaces
with constant-sign curvature. Additionally, we provide probabilistic guarantees
for the broader case of hypersurfaces with non-constant-sign curvature and for
general vector-valued functions. To support reproducibility and practical
adoption, we release a JAX-based Python package, \emph{to be released upon
publication}, implementing the presented theorems and reconciliation
procedures.

</details>


### [44] [SmilesT5: Domain-specific pretraining for molecular language models](https://arxiv.org/abs/2507.22514)
*Philip Spence,Brooks Paige,Anne Osbourn*

Main category: cs.LG

TL;DR: 论文提出了一种基于领域特定文本到文本预训练任务的方法，用于分子属性预测，相比传统方法和现有微调任务，性能有所提升，并提高了数据和计算效率。


<details>
  <summary>Details</summary>
Motivation: 分子属性预测在药物发现和开发中至关重要，现有方法（如图形、语言或特征方法）仍有改进空间。受自然语言处理中掩码语言建模的启发，作者希望探索类似方法在分子语言（SMILES字符串）中的应用。

Method: 提出了一种新颖的领域特定文本到文本预训练任务，通过掩码语言建模训练大型Transformer模型，用于分子属性预测。

Result: 在六个分类基准测试中表现优于传统方法和现有微调任务，同时通过消融研究表明数据和计算效率得到提升。预训练嵌入可作为下游分类器的固定输入，性能与微调相当但计算开销更低。

Conclusion: 该方法在分子属性预测中表现出色，同时提高了效率和实用性，为药物发现提供了有力工具。

Abstract: Molecular property prediction is an increasingly critical task within drug
discovery and development. Typically, neural networks can learn molecular
properties using graph-based, language-based or feature-based methods. Recent
advances in natural language processing have highlighted the capabilities of
neural networks to learn complex human language using masked language
modelling. These approaches to training large transformer-based deep learning
models have also been used to learn the language of molecules, as represented
by simplified molecular-input line-entry system (SMILES) strings. Here, we
present novel domain-specific text-to-text pretraining tasks that yield
improved performance in six classification-based molecular property prediction
benchmarks, relative to both traditional likelihood-based training and
previously proposed fine-tuning tasks. Through ablation studies, we show that
data and computational efficiency can be improved by using these
domain-specific pretraining tasks. Finally, the pretrained embeddings from the
model can be used as fixed inputs into a downstream machine learning classifier
and yield comparable performance to finetuning but with much lower
computational overhead.

</details>


### [45] [HGCN(O): A Self-Tuning GCN HyperModel Toolkit for Outcome Prediction in Event-Sequence Data](https://arxiv.org/abs/2507.22524)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: HGCN(O)是一个基于GCN的自适应工具包，用于事件序列预测，包含四种GCN架构，优化预测准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在事件序列预测中表现不足，HGCN(O)旨在通过多图表示和自适应架构提升预测性能。

Method: 整合四种GCN架构（O-GCN、T-GCN、TP-GCN、TE-GCN），结合节点和图级属性及时间依赖的边权重。

Result: GCNConv模型在不平衡数据上表现优异，所有模型在平衡数据上表现一致，HGCN(O)优于传统方法。

Conclusion: HGCN(O)在事件序列预测中表现出色，尤其适用于PBPM等应用场景。

Abstract: We propose HGCN(O), a self-tuning toolkit using Graph Convolutional Network
(GCN) models for event sequence prediction. Featuring four GCN architectures
(O-GCN, T-GCN, TP-GCN, TE-GCN) across the GCNConv and GraphConv layers, our
toolkit integrates multiple graph representations of event sequences with
different choices of node- and graph-level attributes and in temporal
dependencies via edge weights, optimising prediction accuracy and stability for
balanced and unbalanced datasets. Extensive experiments show that GCNConv
models excel on unbalanced data, while all models perform consistently on
balanced data. Experiments also confirm the superior performance of HGCN(O)
over traditional approaches. Applications include Predictive Business Process
Monitoring (PBPM), which predicts future events or states of a business process
based on event logs.

</details>


### [46] [FGFP: A Fractional Gaussian Filter and Pruning for Deep Neural Networks Compression](https://arxiv.org/abs/2507.22527)
*Kuan-Ting Tu,Po-Hsien Yu,Yu-Syuan Tseng,Shao-Yi Chien*

Main category: cs.LG

TL;DR: 提出了一种结合分数阶高斯滤波器和自适应非结构化剪枝的框架（FGFP），用于深度神经网络的压缩，显著减小模型大小并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在边缘设备上的负载较重，现有压缩方法部署仍具挑战性。

Method: 结合分数阶微分和高斯函数构建分数阶高斯滤波器（FGF），并引入Grünwald-Letnikov分数阶导数降低计算复杂度；同时采用自适应非结构化剪枝（AUP）提高压缩率。

Result: 在CIFAR-10上，ResNet-20模型大小减少85.2%，精度仅下降1.52%；在ImageNet2012上，ResNet-50模型大小减少69.1%，精度仅下降1.63%。

Conclusion: FGFP框架在模型压缩和精度保持方面优于现有方法，适用于边缘设备部署。

Abstract: Network compression techniques have become increasingly important in recent
years because the loads of Deep Neural Networks (DNNs) are heavy for edge
devices in real-world applications. While many methods compress neural network
parameters, deploying these models on edge devices remains challenging. To
address this, we propose the fractional Gaussian filter and pruning (FGFP)
framework, which integrates fractional-order differential calculus and Gaussian
function to construct fractional Gaussian filters (FGFs). To reduce the
computational complexity of fractional-order differential operations, we
introduce Gr\"unwald-Letnikov fractional derivatives to approximate the
fractional-order differential equation. The number of parameters for each
kernel in FGF is minimized to only seven. Beyond the architecture of Fractional
Gaussian Filters, our FGFP framework also incorporates Adaptive Unstructured
Pruning (AUP) to achieve higher compression ratios. Experiments on various
architectures and benchmarks show that our FGFP framework outperforms recent
methods in accuracy and compression. On CIFAR-10, ResNet-20 achieves only a
1.52% drop in accuracy while reducing the model size by 85.2%. On ImageNet2012,
ResNet-50 achieves only a 1.63% drop in accuracy while reducing the model size
by 69.1%.

</details>


### [47] [Accident-Driven Congestion Prediction and Simulation: An Explainable Framework Using Advanced Clustering and Bayesian Networks](https://arxiv.org/abs/2507.22529)
*Kranthi Kumar Talluri,Galia Weidl,Vaishnavi Kasuluru*

Main category: cs.LG

TL;DR: 提出了一种基于AutoML增强的深度嵌入聚类（DEC）和贝叶斯网络（BN）的框架，用于预测事故对交通拥堵的影响，并在SUMO仿真中验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 城市交通事故导致的不确定性拥堵问题严重，影响交通效率、排放和安全。

Method: 结合AutoML增强的DEC进行拥堵标签分类，使用BN预测拥堵概率，并通过SUMO仿真验证。

Result: AutoML-DEC优于传统聚类方法，BN模型准确率达95.6%，SUMO验证显示预测可靠性高。

Conclusion: 所提框架能有效预测事故引发的拥堵，为城市交通管理提供可靠支持。

Abstract: Traffic congestion due to uncertainties, such as accidents, is a significant
issue in urban areas, as the ripple effect of accidents causes longer delays,
increased emissions, and safety concerns. To address this issue, we propose a
robust framework for predicting the impact of accidents on congestion. We
implement Automated Machine Learning (AutoML)-enhanced Deep Embedding
Clustering (DEC) to assign congestion labels to accident data and predict
congestion probability using a Bayesian Network (BN). The Simulation of Urban
Mobility (SUMO) simulation is utilized to evaluate the correctness of BN
predictions using evidence-based scenarios. Results demonstrate that the
AutoML-enhanced DEC has outperformed traditional clustering approaches. The
performance of the proposed BN model achieved an overall accuracy of 95.6%,
indicating its ability to understand the complex relationship of accidents
causing congestion. Validation in SUMO with evidence-based scenarios
demonstrated that the BN model's prediction of congestion states closely
matches those of SUMO, indicating the high reliability of the proposed BN model
in ensuring smooth urban mobility.

</details>


### [48] [Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law](https://arxiv.org/abs/2507.22543)
*Yanjin He,Qingkai Zeng,Meng Jiang*

Main category: cs.LG

TL;DR: 提出了一种基于Zipf定律的词汇量选择方法，通过分析词频分布优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 词汇量选择对NLP和序列建模至关重要，但目前缺乏系统方法，多依赖启发式或数据集特定选择。

Method: 通过分析词频分布的Zipf定律特性，提出词汇量选择的准则。

Result: 实验表明，词频分布符合Zipf定律时，模型性能最佳，适用于NLP、基因组学和化学领域。

Conclusion: Zipf定律对齐是词汇量选择的通用准则，能提升模型效率和效果。

Abstract: Tokenization is a fundamental step in natural language processing (NLP) and
other sequence modeling domains, where the choice of vocabulary size
significantly impacts model performance. Despite its importance, selecting an
optimal vocabulary size remains underexplored, typically relying on heuristics
or dataset-specific choices. In this work, we propose a principled method for
determining the vocabulary size by analyzing token frequency distributions
through Zipf's law. We show that downstream task performance correlates with
how closely token distributions follow power-law behavior, and that aligning
with Zipfian scaling improves both model efficiency and effectiveness.
Extensive experiments across NLP, genomics, and chemistry demonstrate that
models consistently achieve peak performance when the token distribution
closely adheres to Zipf's law, establishing Zipfian alignment as a robust and
generalizable criterion for vocabulary size selection.

</details>


### [49] [Thermodynamics-Inspired Computing with Oscillatory Neural Networks for Inverse Matrix Computation](https://arxiv.org/abs/2507.22544)
*George Tsormpatzoglou,Filip Sabo,Aida Todri-Sanial*

Main category: cs.LG

TL;DR: 提出了一种基于振荡神经网络（ONNs）的热力学启发的计算范式，用于解决线性代数问题，特别是逆矩阵问题。


<details>
  <summary>Details</summary>
Motivation: 探索ONNs在解决线性代数问题中的可行性，扩展其应用范围。

Method: 通过耦合Kuramoto振荡器模型的线性近似，理论推导出逆矩阵解，并通过数值模拟验证。

Result: 数值模拟验证了理论框架，并确定了计算精度最高的参数范围。

Conclusion: ONNs在解决线性代数问题中具有潜力，特别是在逆矩阵计算中表现良好。

Abstract: We describe a thermodynamic-inspired computing paradigm based on oscillatory
neural networks (ONNs). While ONNs have been widely studied as Ising machines
for tackling complex combinatorial optimization problems, this work
investigates their feasibility in solving linear algebra problems, specifically
the inverse matrix. Grounded in thermodynamic principles, we analytically
demonstrate that the linear approximation of the coupled Kuramoto oscillator
model leads to the inverse matrix solution. Numerical simulations validate the
theoretical framework, and we examine the parameter regimes that computation
has the highest accuracy.

</details>


### [50] [DeepC4: Deep Conditional Census-Constrained Clustering for Large-scale Multitask Spatial Disaggregation of Urban Morphology](https://arxiv.org/abs/2507.22554)
*Joshua Dimasaka,Christian Geiß,Emily So*

Main category: cs.LG

TL;DR: 论文提出了一种名为DeepC4的深度学习方法，用于改进基于卫星影像和人口普查数据的城市形态空间分解，解决了现有方法在局部不一致性和模型不确定性上的问题。


<details>
  <summary>Details</summary>
Motivation: 为了支持可持续发展和灾害风险减少的全球目标，需要更精确的大规模城市形态地图，但现有方法在局部验证和模型不确定性上存在挑战。

Method: 提出DeepC4方法，结合局部人口普查统计数据作为聚类约束，并通过多任务学习处理卫星影像模式。

Result: 在卢旺达的实验中，DeepC4显著提升了城市形态地图的质量，特别是在建筑暴露和物理脆弱性方面。

Conclusion: DeepC4为大规模空间信息审计提供了一种新的深度学习技术，有助于实现2030年全球框架目标。

Abstract: To understand our global progress for sustainable development and disaster
risk reduction in many developing economies, two recent major initiatives - the
Uniform African Exposure Dataset of the Global Earthquake Model (GEM)
Foundation and the Modelling Exposure through Earth Observation Routines
(METEOR) Project - implemented classical spatial disaggregation techniques to
generate large-scale mapping of urban morphology using the information from
various satellite imagery and its derivatives, geospatial datasets of the built
environment, and subnational census statistics. However, the local discrepancy
with well-validated census statistics and the propagated model uncertainties
remain a challenge in such coarse-to-fine-grained mapping problems,
specifically constrained by weak and conditional label supervision. Therefore,
we present Deep Conditional Census-Constrained Clustering (DeepC4), a novel
deep learning-based spatial disaggregation approach that incorporates local
census statistics as cluster-level constraints while considering multiple
conditional label relationships in a joint multitask learning of the patterns
of satellite imagery. To demonstrate, compared to GEM and METEOR, we enhanced
the quality of Rwandan maps of urban morphology, specifically building exposure
and physical vulnerability, at the third-level administrative unit from the
2022 census. As the world approaches the conclusion of our global frameworks in
2030, our work has offered a new deep learning-based mapping technique towards
a spatial auditing of our existing coarse-grained derived information at large
scales.

</details>


### [51] [VAR: Visual Analysis for Rashomon Set of Machine Learning Models' Performance](https://arxiv.org/abs/2507.22556)
*Yuanzhe Jin*

Main category: cs.LG

TL;DR: 提出了一种名为VAR的可视化解决方案，用于横向比较Rashomon集合中的机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 传统方法侧重于垂直结构分析，缺乏有效的横向比较可视化工具。

Method: 结合热图和散点图进行模型比较。

Result: 帮助开发者找到特定条件下的最优模型，并理解Rashomon集合的整体特性。

Conclusion: VAR为机器学习模型的横向比较提供了有效的可视化工具。

Abstract: Evaluating the performance of closely matched machine learning(ML) models
under specific conditions has long been a focus of researchers in the field of
machine learning. The Rashomon set is a collection of closely matched ML
models, encompassing a wide range of models with similar accuracies but
different structures. Traditionally, the analysis of these sets has focused on
vertical structural analysis, which involves comparing the corresponding
features at various levels within the ML models. However, there has been a lack
of effective visualization methods for horizontally comparing multiple models
with specific features. We propose the VAR visualization solution. VAR uses
visualization to perform comparisons of ML models within the Rashomon set. This
solution combines heatmaps and scatter plots to facilitate the comparison. With
the help of VAR, ML model developers can identify the optimal model under
specific conditions and better understand the Rashomon set's overall
characteristics.

</details>


### [52] [Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning](https://arxiv.org/abs/2507.22565)
*Afshin Khadangi,Amir Sartipi,Igor Tchappi,Ramin Bahmani,Gilbert Fridgen*

Main category: cs.LG

TL;DR: RLDP框架通过强化学习动态调整差分隐私优化中的梯度裁剪和噪声注入，显著提升语言模型的隐私保护与性能平衡。


<details>
  <summary>Details</summary>
Motivation: 解决差分隐私优化中全局固定参数导致的隐私预算浪费或模型性能下降问题。

Method: 将差分隐私优化建模为闭环控制问题，使用深度强化学习动态调整梯度裁剪和噪声注入。

Result: 在多个语言模型上，RLDP显著降低困惑度（1.3-30.5%）并提升下游任务性能（平均5.6%），同时加速训练（平均71%）。

Conclusion: RLDP在保持相同隐私保护水平下，显著提升了模型性能和训练效率。

Abstract: The tension between data privacy and model utility has become the defining
bottleneck for the practical deployment of large language models (LLMs) trained
on sensitive corpora including healthcare. Differentially private stochastic
gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a
pronounced cost: gradients are forcibly clipped and perturbed with noise,
degrading sample efficiency and final accuracy. Numerous variants have been
proposed to soften this trade-off, but they all share a handicap: their control
knobs are hard-coded, global, and oblivious to the evolving optimization
landscape. Consequently, practitioners are forced either to over-spend privacy
budget in pursuit of utility, or to accept mediocre models in order to stay
within privacy constraints. We present RLDP, the first framework to cast DP
optimization itself as a closed-loop control problem amenable to modern deep
reinforcement learning (RL). RLDP continuously senses rich statistics of the
learning dynamics and acts by selecting fine-grained per parameter
gradient-clipping thresholds as well as the magnitude of injected Gaussian
noise. A soft actor-critic (SAC) hyper-policy is trained online during language
model fine-tuning; it learns, from scratch, how to allocate the privacy budget
where it matters and when it matters. Across more than 1,600 ablation
experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers
perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream
utility gain. RLDP reaches each baseline's final utility after only 13-43% of
the gradient-update budget (mean speed-up 71%), all while honoring the same
($\epsilon$, $\delta$)-DP contract and exhibiting equal or lower susceptibility
to membership-inference and canary-extraction attacks.

</details>


### [53] [Explaining Deep Network Classification of Matrices: A Case Study on Monotonicity](https://arxiv.org/abs/2507.22570)
*Leandro Farina,Sergey Korotov*

Main category: cs.LG

TL;DR: 利用深度学习和可解释AI技术，提出了一种方法，通过神经网络和XAI技术，从矩阵的抽象代数特性中提取可解释的分类规则，应用于单调矩阵的分类。


<details>
  <summary>Details</summary>
Motivation: 单调矩阵的定义简单，但其元素或派生参数的明确特征尚不明确，因此需要一种系统化的方法进行分类。

Method: 通过随机生成单调和非单调矩阵建立数据集，使用深度神经网络和矩阵特征进行分类，并通过显著性方法（如积分梯度）提取关键特征。

Result: 发现矩阵特征多项式的两个最低阶系数c0和c1足以分类单调矩阵，准确率达95%。数据研究表明，单调矩阵满足|c0/c1|≤0.18的概率>99.98%。

Conclusion: 该方法成功提取了单调矩阵的简单分类规则，为类似问题提供了可解释的解决方案。

Abstract: This work demonstrates a methodology for using deep learning to discover
simple, practical criteria for classifying matrices based on abstract algebraic
properties. By combining a high-performance neural network with explainable AI
(XAI) techniques, we can distill a model's learned strategy into
human-interpretable rules. We apply this approach to the challenging case of
monotone matrices, defined by the condition that their inverses are entrywise
nonnegative. Despite their simple definition, an easy characterization in terms
of the matrix elements or the derived parameters is not known. Here, we
present, to the best of our knowledge, the first systematic machine-learning
approach for deriving a practical criterion that distinguishes monotone from
non-monotone matrices. After establishing a labelled dataset by randomly
generated monotone and non-monotone matrices uniformly on $(-1,1)$, we employ
deep neural network algorithms for classifying the matrices as monotone or
non-monotone, using both their entries and a comprehensive set of matrix
features. By saliency methods, such as integrated gradients, we identify among
all features, two matrix parameters which alone provide sufficient information
for the matrix classification, with $95\%$ accuracy, namely the absolute values
of the two lowest-order coefficients, $c_0$ and $c_1$ of the matrix's
characteristic polynomial. A data-driven study of 18,000 random $7\times7$
matrices shows that the monotone class obeys $\lvert c_{0}/c_{1}\rvert\le0.18$
with probability $>99.98\%$; because $\lvert c_{0}/c_{1}\rvert =
1/\mathrm{tr}(A^{-1})$ for monotone $A$, this is equivalent to the simple bound
$\mathrm{tr}(A^{-1})\ge5.7$.

</details>


### [54] [Deep learning of geometrical cell division rules](https://arxiv.org/abs/2507.22587)
*Alexandre Durrmeyer,Jean-Christophe Palauqui,Philippe Andrey*

Main category: cs.LG

TL;DR: 论文提出了一种基于数据的方法，利用深度神经网络研究细胞几何形状与分裂平面定位的关系，解决了传统几何规则的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖先验几何规则，限制了细胞分裂平面定位的研究，因此需要一种数据驱动的方法来探索更复杂的关系。

Method: 采用改进的UNet架构，基于细胞掩模图像，从母细胞几何形状中学习和预测分裂模式。

Result: 模型在合成数据和A. thaliana胚胎细胞上表现优异，能够解释现有几何规则无法涵盖的分裂模式。

Conclusion: 深度网络在理解细胞分裂模式和生成新假设方面具有潜力。

Abstract: The positioning of new cellular walls during cell division plays a key role
in shaping plant tissue organization. The influence of cell geometry on the
positioning of division planes has been previously captured into various
geometrical rules. Accordingly, linking cell shape to division orientation has
relied on the comparison between observed division patterns and predictions
under specific rules. The need to define a priori the tested rules is a
fundamental limitation of this hypothesis-driven approach. As an alternative,
we introduce a data-based approach to investigate the relation between cell
geometry and division plane positioning, exploiting the ability of deep neural
network to learn complex relationships across multidimensional spaces. Adopting
an image-based cell representation, we show how division patterns can be
learned and predicted from mother cell geometry using a UNet architecture
modified to operate on cell masks. Using synthetic data and A. thaliana embryo
cells, we evaluate the model performances on a wide range of diverse cell
shapes and division patterns. We find that the trained model accounted for
embryo division patterns that were previously irreconcilable under existing
geometrical rules. Our work shows the potential of deep networks to understand
cell division patterns and to generate new hypotheses on the control of cell
division positioning.

</details>


### [55] [H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity](https://arxiv.org/abs/2507.22633)
*Wei Guo,Siyuan Lu,Yiqi Tong,Zhaojun Hu,Fuzhen Zhuang,Xiao Zhang,Tao Fan,Jin Dong*

Main category: cs.LG

TL;DR: HHFFT是一种混合异构联邦微调方法，解决了模型架构和下游任务的双重异构性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有联邦微调方法无法处理模型架构和任务的双重异构性，导致参数聚合和知识共享困难。

Method: 提出H2Tune框架，包括稀疏三重矩阵分解、关系引导的矩阵层对齐和交替任务知识解耦机制。

Result: 实验表明H2Tune在准确率上比现有方法提升15.4%，理论分析证明收敛速率为O(1/√T)。

Conclusion: H2Tune有效解决了混合异构联邦微调中的关键挑战，显著提升了性能。

Abstract: Different from existing federated fine-tuning (FFT) methods for foundation
models, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-explored
scenario where clients exhibit double heterogeneity in model architectures and
downstream tasks. This hybrid heterogeneity introduces two significant
challenges: 1) heterogeneous matrix aggregation, where clients adopt different
large-scale foundation models based on their task requirements and resource
limitations, leading to dimensional mismatches during LoRA parameter
aggregation; and 2) multi-task knowledge interference, where local shared
parameters, trained with both task-shared and task-specific knowledge, cannot
ensure only task-shared knowledge is transferred between clients. To address
these challenges, we propose H2Tune, a federated foundation model fine-tuning
with hybrid heterogeneity. Our framework H2Tune consists of three key
components: (i) sparsified triple matrix decomposition to align hidden
dimensions across clients through constructing rank-consistent middle matrices,
with adaptive sparsification based on client resources; (ii) relation-guided
matrix layer alignment to handle heterogeneous layer structures and
representation capabilities; and (iii) alternating task-knowledge
disentanglement mechanism to decouple shared and specific knowledge of local
model parameters through alternating optimization. Theoretical analysis proves
a convergence rate of O(1/\sqrt{T}). Extensive experiments show our method
achieves up to 15.4% accuracy improvement compared to state-of-the-art
baselines. Our code is available at
https://anonymous.4open.science/r/H2Tune-1407.

</details>


### [56] [Transductive Model Selection under Prior Probability Shift](https://arxiv.org/abs/2507.22647)
*Lorenzo Volpi,Alejandro Moreo,Fabrizio Sebastiani*

Main category: cs.LG

TL;DR: 提出了一种针对转导分类场景的方法，用于在数据存在先验概率偏移时进行模型选择（超参数优化），直接在未标记数据上优化超参数，而非传统基于标记数据的交叉验证方法。


<details>
  <summary>Details</summary>
Motivation: 传统转导学习在数据偏移（如先验概率偏移）时，模型选择方法可能不适用。本文旨在解决这一问题。

Method: 提出一种直接在未标记数据上优化超参数的方法，适用于先验概率偏移的转导分类场景。

Result: 实验结果表明，该方法优于传统的基于标记数据的交叉验证方法。

Conclusion: 该方法为转导学习中的数据偏移问题提供了一种有效的解决方案。

Abstract: Transductive learning is a supervised machine learning task in which, unlike
in traditional inductive learning, the unlabelled data that require labelling
are a finite set and are available at training time. Similarly to inductive
learning contexts, transductive learning contexts may be affected by dataset
shift, i.e., may be such that the IID assumption does not hold. We here propose
a method, tailored to transductive classification contexts, for performing
model selection (i.e., hyperparameter optimisation) when the data exhibit prior
probability shift, an important type of dataset shift typical of anti-causal
learning problems. In our proposed method the hyperparameters can be optimised
directly on the unlabelled data to which the trained classifier must be
applied; this is unlike traditional model selection methods, that are based on
performing cross-validation on the labelled training data. We provide
experimental results that show the benefits brought about by our method.

</details>


### [57] [Cluster-Based Random Forest Visualization and Interpretation](https://arxiv.org/abs/2507.22665)
*Max Sondag,Christofer Meinecke,Dennis Collaris,Tatiana von Landesberger,Stef van den Elzen*

Main category: cs.LG

TL;DR: 提出了一种可视化方法和系统，通过聚类相似树来提高随机森林的可解释性，并引入新的距离度量和两种可视化方法。


<details>
  <summary>Details</summary>
Motivation: 随机森林性能高但难以解释，需要一种方法让用户无需分析每棵树即可理解模型行为。

Method: 聚类相似树，引入新的距离度量（考虑决策规则和预测），提出两种可视化方法：特征图和规则图。

Result: 在“Glass”数据集和用户研究中验证了方法的有效性。

Conclusion: 该方法显著提高了随机森林的可解释性，适用于复杂数据集。

Abstract: Random forests are a machine learning method used to automatically classify
datasets and consist of a multitude of decision trees. While these random
forests often have higher performance and generalize better than a single
decision tree, they are also harder to interpret. This paper presents a
visualization method and system to increase interpretability of random forests.
We cluster similar trees which enables users to interpret how the model
performs in general without needing to analyze each individual decision tree in
detail, or interpret an oversimplified summary of the full forest. To
meaningfully cluster the decision trees, we introduce a new distance metric
that takes into account both the decision rules as well as the predictions of a
pair of decision trees. We also propose two new visualization methods that
visualize both clustered and individual decision trees: (1) The Feature Plot,
which visualizes the topological position of features in the decision trees,
and (2) the Rule Plot, which visualizes the decision rules of the decision
trees. We demonstrate the efficacy of our approach through a case study on the
"Glass" dataset, which is a relatively complex standard machine learning
dataset, as well as a small user study.

</details>


### [58] [Enhanced Prediction of CAR T-Cell Cytotoxicity with Quantum-Kernel Methods](https://arxiv.org/abs/2507.22710)
*Filippo Utro,Meltem Tolunay,Kahn Rhrissorrakrai,Tanvi P. Gujarati,Jie Shi,Sara Capponi,Mirko Amico,Nate Earnest-Noble,Laxmi Parida*

Main category: cs.LG

TL;DR: 论文提出了一种基于量子计算的方法（PQK）来解决CAR T细胞工程中组合空间探索不足的问题，并展示了其在分类性能上的优势。


<details>
  <summary>Details</summary>
Motivation: CAR T细胞工程中，由于组合空间巨大且实验数据有限，传统方法难以有效探索和优化。

Method: 使用投影量子核（PQK）将经典数据嵌入高维希尔伯特空间，并通过核方法测量样本相似性。

Result: 在61量子位的门控量子计算机上，PQK在CAR T细胞毒性预测中表现优于经典机器学习方法，尤其在信息较少的信号域和域位置上。

Conclusion: 量子计算在数据受限问题中具有潜力，尤其是在CAR T细胞工程等复杂组合优化任务中。

Abstract: Chimeric antigen receptor (CAR) T-cells are T-cells engineered to recognize
and kill specific tumor cells. Through their extracellular domains, CAR T-cells
bind tumor cell antigens which triggers CAR T activation and proliferation.
These processes are regulated by co-stimulatory domains present in the
intracellular region of the CAR T-cell. Through integrating novel signaling
components into the co-stimulatory domains, it is possible to modify CAR T-cell
phenotype. Identifying and experimentally testing new CAR constructs based on
libraries of co-stimulatory domains is nontrivial given the vast combinatorial
space defined by such libraries. This leads to a highly data constrained,
poorly explored combinatorial problem, where the experiments undersample all
possible combinations. We propose a quantum approach using a Projected Quantum
Kernel (PQK) to address this challenge. PQK operates by embedding classical
data into a high dimensional Hilbert space and employs a kernel method to
measure sample similarity. Using 61 qubits on a gate-based quantum computer, we
demonstrate the largest PQK application to date and an enhancement in the
classification performance over purely classical machine learning methods for
CAR T cytotoxicity prediction. Importantly, we show improved learning for
specific signaling domains and domain positions, particularly where there was
lower information highlighting the potential for quantum computing in
data-constrained problems.

</details>


### [59] [Teaching the Teacher: Improving Neural Network Distillability for Symbolic Regression via Jacobian Regularization](https://arxiv.org/abs/2507.22767)
*Soumyadeep Dhar,Kei Sen Fong,Mehul Motani*

Main category: cs.LG

TL;DR: 本文提出了一种新的训练范式，通过Jacobian-based regularizer主动优化教师网络，使其学习更平滑且易于蒸馏的函数，显著提高了符号模型蒸馏的保真度。


<details>
  <summary>Details</summary>
Motivation: 将大型神经网络蒸馏为简单、可读的符号公式是实现可信赖和可解释AI的重要途径，但传统方法因目标函数复杂而效果不佳。

Method: 引入Jacobian-based regularizer，主动优化教师网络，使其学习更平滑且易于蒸馏的函数。

Result: 在真实回归基准测试中，符号模型的R2分数平均提高了120%（相对值），同时保持教师网络的预测准确性。

Conclusion: 该方法为从复杂神经网络中提取高保真可解释模型提供了实用且原则性的解决方案。

Abstract: Distilling large neural networks into simple, human-readable symbolic
formulas is a promising path toward trustworthy and interpretable AI. However,
this process is often brittle, as the complex functions learned by standard
networks are poor targets for symbolic discovery, resulting in low-fidelity
student models. In this work, we propose a novel training paradigm to address
this challenge. Instead of passively distilling a pre-trained network, we
introduce a \textbf{Jacobian-based regularizer} that actively encourages the
``teacher'' network to learn functions that are not only accurate but also
inherently smoother and more amenable to distillation. We demonstrate through
extensive experiments on a suite of real-world regression benchmarks that our
method is highly effective. By optimizing the regularization strength for each
problem, we improve the $R^2$ score of the final distilled symbolic model by an
average of \textbf{120\% (relative)} compared to the standard distillation
pipeline, all while maintaining the teacher's predictive accuracy. Our work
presents a practical and principled method for significantly improving the
fidelity of interpretable models extracted from complex neural networks.

</details>


### [60] [Label-free estimation of clinically relevant performance metrics under distribution shifts](https://arxiv.org/abs/2507.22776)
*Tim Flühmann,Alceu Bissoto,Trung-Dung Hoang,Lisa M. Koch*

Main category: cs.LG

TL;DR: 论文提出了在临床部署中估计图像分类模型性能的新方法，通过扩展现有技术直接估计混淆矩阵，并在真实和模拟数据偏移下验证其效果。


<details>
  <summary>Details</summary>
Motivation: 由于目标数据集通常缺乏真实标签，直接评估模型性能不可行，现有方法主要依赖置信度分数估计准确性，但未充分验证在临床领域的适用性。

Method: 扩展现有性能预测方法，直接估计完整的混淆矩阵，并在真实和模拟数据偏移（如协变量和流行度偏移）下进行基准测试。

Result: 提出的混淆矩阵估计方法在医学图像分布偏移下可靠预测了临床相关计数指标，但模拟偏移场景揭示了现有技术的局限性。

Conclusion: 研究强调在医学AI模型后市场监测中，需更深入理解实际部署环境以优化性能监控技术。

Abstract: Performance monitoring is essential for safe clinical deployment of image
classification models. However, because ground-truth labels are typically
unavailable in the target dataset, direct assessment of real-world model
performance is infeasible. State-of-the-art performance estimation methods
address this by leveraging confidence scores to estimate the target accuracy.
Despite being a promising direction, the established methods mainly estimate
the model's accuracy and are rarely evaluated in a clinical domain, where
strong class imbalances and dataset shifts are common. Our contributions are
twofold: First, we introduce generalisations of existing performance prediction
methods that directly estimate the full confusion matrix. Then, we benchmark
their performance on chest x-ray data in real-world distribution shifts as well
as simulated covariate and prevalence shifts. The proposed confusion matrix
estimation methods reliably predicted clinically relevant counting metrics on
medical images under distribution shifts. However, our simulated shift
scenarios exposed important failure modes of current performance estimation
techniques, calling for a better understanding of real-world deployment
contexts when implementing these performance monitoring techniques for
postmarket surveillance of medical AI models.

</details>


### [61] [DO-EM: Density Operator Expectation Maximization](https://arxiv.org/abs/2507.22786)
*Adit Vishnu,Abhay Shastry,Dhruva Kashyap,Chiranjib Bhattacharyya*

Main category: cs.LG

TL;DR: 本文提出了一种基于密度算子的期望最大化框架（DO-EM），用于在经典硬件上训练量子生成模型，解决了现有算法无法扩展到真实数据的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的量子生成模型训练算法（如量子玻尔兹曼机）无法扩展到真实数据（如MNIST数据集），需要一种类似经典概率模型的期望最大化框架。

Method: 通过将期望步骤重新定义为量子信息投影（QIP）问题，并利用Petz恢复映射解决条件概率缺失的问题，提出了DO-EM算法。

Result: DO-EM算法在MNIST数据集上训练量子交错深度玻尔兹曼机（QiDBM），其图像生成性能优于经典DBM，Fréchet Inception Distance降低了40-60%。

Conclusion: DO-EM算法为量子生成模型提供了一种可扩展的训练框架，并在真实数据上展示了优越性能。

Abstract: Density operators, quantum generalizations of probability distributions, are
gaining prominence in machine learning due to their foundational role in
quantum computing. Generative modeling based on density operator models
(\textbf{DOMs}) is an emerging field, but existing training algorithms -- such
as those for the Quantum Boltzmann Machine -- do not scale to real-world data,
such as the MNIST dataset. The Expectation-Maximization algorithm has played a
fundamental role in enabling scalable training of probabilistic latent variable
models on real-world datasets. \textit{In this paper, we develop an
Expectation-Maximization framework to learn latent variable models defined
through \textbf{DOMs} on classical hardware, with resources comparable to those
used for probabilistic models, while scaling to real-world data.} However,
designing such an algorithm is nontrivial due to the absence of a well-defined
quantum analogue to conditional probability, which complicates the Expectation
step. To overcome this, we reformulate the Expectation step as a quantum
information projection (QIP) problem and show that the Petz Recovery Map
provides a solution under sufficient conditions. Using this formulation, we
introduce the Density Operator Expectation Maximization (DO-EM) algorithm -- an
iterative Minorant-Maximization procedure that optimizes a quantum evidence
lower bound. We show that the \textbf{DO-EM} algorithm ensures non-decreasing
log-likelihood across iterations for a broad class of models. Finally, we
present Quantum Interleaved Deep Boltzmann Machines (\textbf{QiDBMs}), a
\textbf{DOM} that can be trained with the same resources as a DBM. When trained
with \textbf{DO-EM} under Contrastive Divergence, a \textbf{QiDBM} outperforms
larger classical DBMs in image generation on the MNIST dataset, achieving a
40--60\% reduction in the Fr\'echet Inception Distance.

</details>


### [62] [G-Core: A Simple, Scalable and Balanced RLHF Trainer](https://arxiv.org/abs/2507.22789)
*Junyu Wu,Weiming Chang,Xiaotao Liu,Guanyou He,Haoqiang Hong,Boqi Liu,Hongtao Tian,Tao Yang,Yunsheng Shi,Feng Lin,Ting Yao*

Main category: cs.LG

TL;DR: G-Core是一个简单、可扩展且平衡的RLHF训练框架，解决了现有系统在多模态和动态工作负载中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF训练系统在多模态和扩散工作流中面临扩展性和动态工作负载适应性的问题。

Method: G-Core引入并行控制器编程模型和动态资源分配方案，优化复杂RLHF工作流的编排和资源利用率。

Result: G-Core成功支持微信产品功能的大规模模型训练，显著提升硬件利用率和训练效率。

Conclusion: G-Core在RLHF训练中取得先进成果，为大规模人类对齐模型的未来研究和部署奠定基础。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become an increasingly
popular paradigm for training large language models (LLMs) and diffusion
models. While existing RLHF training systems have enabled significant progress,
they often face challenges in scaling to multi-modal and diffusion workflows
and adapting to dynamic workloads. In particular, current approaches may
encounter limitations in controller scalability, flexible resource placement,
and efficient orchestration when handling complex RLHF pipelines, especially in
scenarios involving dynamic sampling or generative reward modeling. In this
paper, we present \textbf{G-Core}, a simple, scalable, and balanced RLHF
training framework designed to address these challenges. G-Core introduces a
parallel controller programming model, enabling flexible and efficient
orchestration of complex RLHF workflows without the bottlenecks of a single
centralized controller. Furthermore, we propose a dynamic placement schema that
adaptively partitions resources and schedules workloads, significantly reducing
hardware idle time and improving utilization, even under highly variable
training conditions. G-Core has successfully trained models that support WeChat
product features serving a large-scale user base, demonstrating its
effectiveness and robustness in real-world scenarios. Our results show that
G-Core advances the state of the art in RLHF training, providing a solid
foundation for future research and deployment of large-scale, human-aligned
models.

</details>


### [63] [Quantifying surprise in clinical care: Detecting highly informative events in electronic health records with foundation models](https://arxiv.org/abs/2507.22798)
*Michael C. Burkhart,Bashar Ramadan,Luke Solo,William F. Parker,Brett K. Beaulieu-Jones*

Main category: cs.LG

TL;DR: 提出了一种基于基础模型的方法，用于识别电子健康记录中的高信息量标记和事件，能够发现规则方法忽略的异常事件，并证明这些事件对预测患者结果具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 传统规则方法可能忽略看似正常但实际异常的事件，需要一种更全面的方法来识别这些事件。

Method: 利用基础模型分析患者住院期间的完整上下文数据，识别高信息量事件和标记。

Result: 模型识别的事件对预测患者结果具有显著意义，部分低信息量事件可安全忽略。

Conclusion: 该方法不仅提高了事件识别的准确性，还为预后模型的预测提供了可解释性。

Abstract: We present a foundation model-derived method to identify highly informative
tokens and events in electronic health records. Our approach considers incoming
data in the entire context of a patient's hospitalization and so can flag
anomalous events that rule-based approaches would consider within a normal
range. We demonstrate that the events our model flags are significant for
predicting downstream patient outcomes and that a fraction of events identified
as carrying little information can safely be dropped. Additionally, we show how
informativeness can help interpret the predictions of prognostic models trained
on foundation model-derived representations.

</details>


### [64] [Tapping into the Black Box: Uncovering Aligned Representations in Pretrained Neural Networks](https://arxiv.org/abs/2507.22832)
*Maciej Satkiewicz*

Main category: cs.LG

TL;DR: ReLU网络学习了一种隐式线性模型，通过简单的反向传播修改可以提取其决策边界，揭示高分辨率特征。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络是否依赖可解释的学习模式，并验证其可恢复性。

Method: 通过修改反向传播过程，提取隐式线性模型的决策边界（称为excitation pullbacks）。

Result: 在多个ImageNet预训练架构中，提取的特征具有高分辨率和感知对齐性。

Conclusion: 神经网络确实依赖可解释的学习模式，这对知识发现和可靠AI系统开发有深远意义。

Abstract: In this paper we argue that ReLU networks learn an implicit linear model we
can actually tap into. We describe that alleged model formally and show that we
can approximately pull its decision boundary back to the input space with
certain simple modification to the backward pass. The resulting gradients
(called excitation pullbacks) reveal high-resolution input- and target-specific
features of remarkable perceptual alignment on a number of popular
ImageNet-pretrained deep architectures. This strongly suggests that neural
networks do, in fact, rely on learned interpretable patterns that can be
recovered after training. Thus, our findings may have profound implications for
knowledge discovery and the development of dependable artificial systems.

</details>


### [65] [PAF-Net: Phase-Aligned Frequency Decoupling Network for Multi-Process Manufacturing Quality Prediction](https://arxiv.org/abs/2507.22840)
*Yang Luo,Haoyang Luan,Haoyun Pan,Yongquan Jia,Xiaofeng Gao,Guihai Chen*

Main category: cs.LG

TL;DR: PAF-Net是一个频率解耦的时间序列预测框架，解决了多工艺制造中的时间滞后、操作重叠和频率依赖性问题。


<details>
  <summary>Details</summary>
Motivation: 多工艺制造中的质量预测面临时间滞后、操作重叠和频率依赖性等挑战，需要一种新方法来解决这些问题。

Method: PAF-Net采用相位相关对齐、频率独立补丁注意力和频率解耦交叉注意力模块，结合DCT分解。

Result: 在4个真实数据集上，PAF-Net的MSE和MAE分别比基线方法低7.06%和3.88%。

Conclusion: PAF-Net在多工艺制造质量预测中表现出色，解决了核心挑战并显著提升了预测精度。

Abstract: Accurate quality prediction in multi-process manufacturing is critical for
industrial efficiency but hindered by three core challenges: time-lagged
process interactions, overlapping operations with mixed periodicity, and
inter-process dependencies in shared frequency bands. To address these, we
propose PAF-Net, a frequency decoupled time series prediction framework with
three key innovations: (1) A phase-correlation alignment method guided by
frequency domain energy to synchronize time-lagged quality series, resolving
temporal misalignment. (2) A frequency independent patch attention mechanism
paired with Discrete Cosine Transform (DCT) decomposition to capture
heterogeneous operational features within individual series. (3) A frequency
decoupled cross attention module that suppresses noise from irrelevant
frequencies, focusing exclusively on meaningful dependencies within shared
bands. Experiments on 4 real-world datasets demonstrate PAF-Net's superiority.
It outperforms 10 well-acknowledged baselines by 7.06% lower MSE and 3.88%
lower MAE. Our code is available at
https://github.com/StevenLuan904/PAF-Net-Official.

</details>


### [66] [RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents](https://arxiv.org/abs/2507.22844)
*Zijing Zhang,Ziyang Chen,Mingxiao Li,Zhaopeng Tu,Xiaolong Li*

Main category: cs.LG

TL;DR: RLVMR框架通过引入过程级监督和元推理行为奖励，解决了传统强化学习中低效探索的问题，显著提升了智能体的推理能力和任务成功率。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法仅优化最终任务成功率，导致智能体学习低效或不合理的推理路径，缺乏鲁棒性和泛化能力。

Method: RLVMR通过程序化规则奖励元推理行为（如规划、探索和反思），并结合最终任务信号，使用无评论家策略梯度方法进行优化。

Result: 在ALFWorld和ScienceWorld基准测试中，RLVMR取得了83.6%的成功率，显著减少了冗余动作并提升了错误恢复能力。

Conclusion: RLVMR通过过程级监督提升了智能体的推理质量和效率，实现了更鲁棒、高效和可解释的智能体。

Abstract: The development of autonomous agents for complex, long-horizon tasks is a
central goal in AI. However, dominant training paradigms face a critical
limitation: reinforcement learning (RL) methods that optimize solely for final
task success often reinforce flawed or inefficient reasoning paths, a problem
we term inefficient exploration. This leads to agents that are brittle and fail
to generalize, as they learn to find solutions without learning how to reason
coherently. To address this, we introduce RLVMR, a novel framework that
integrates dense, process-level supervision into end-to-end RL by rewarding
verifiable, meta-reasoning behaviors. RLVMR equips an agent to explicitly tag
its cognitive steps, such as planning, exploration, and reflection, and
provides programmatic, rule-based rewards for actions that contribute to
effective problem-solving. These process-centric rewards are combined with the
final outcome signal and optimized using a critic-free policy gradient method.
On the challenging ALFWorld and ScienceWorld benchmarks, RLVMR achieves new
state-of-the-art results, with our 7B model reaching an 83.6% success rate on
the most difficult unseen task split. Our analysis confirms these gains stem
from improved reasoning quality, including significant reductions in redundant
actions and enhanced error recovery, leading to more robust, efficient, and
interpretable agents.

</details>


### [67] [Decentralized Differentially Private Power Method](https://arxiv.org/abs/2507.22849)
*Andrew Campbell,Anna Scaglione,Sean Peisert*

Main category: cs.LG

TL;DR: 提出了一种去中心化差分隐私幂方法（D-DP-PM），用于多智能体网络中的主成分分析（PCA），确保差分隐私并实现全局特征向量的协作估计。


<details>
  <summary>Details</summary>
Motivation: 解决传统去中心化PCA方法中每个智能体需访问完整样本空间的限制，处理智能体仅观测部分维度的挑战性场景。

Method: 通过共享局部嵌入和添加高斯噪声，结合随机初始化的隐私保护，实现差分隐私和协作估计。

Result: 算法满足(ε,δ)-差分隐私，收敛速度受网络拓扑影响，实验显示在中等隐私区间（ε∈[2,5]）表现优异。

Conclusion: D-DP-PM在隐私与效用间取得良好平衡，适用于实际应用。

Abstract: We propose a novel Decentralized Differentially Private Power Method
(D-DP-PM) for performing Principal Component Analysis (PCA) in networked
multi-agent settings. Unlike conventional decentralized PCA approaches where
each agent accesses the full n-dimensional sample space, we address the
challenging scenario where each agent observes only a subset of dimensions
through row-wise data partitioning. Our method ensures
$(\epsilon,\delta)$-Differential Privacy (DP) while enabling collaborative
estimation of global eigenvectors across the network without requiring a
central aggregator. We achieve this by having agents share only local
embeddings of the current eigenvector iterate, leveraging both the inherent
privacy from random initialization and carefully calibrated Gaussian noise
additions. We prove that our algorithm satisfies the prescribed
$(\epsilon,\delta)$-DP guarantee and establish convergence rates that
explicitly characterize the impact of the network topology. Our theoretical
analysis, based on linear dynamics and high-dimensional probability theory,
provides tight bounds on both privacy and utility. Experiments on real-world
datasets demonstrate that D-DP-PM achieves superior privacy-utility tradeoffs
compared to naive local DP approaches, with particularly strong performance in
moderate privacy regimes ($\epsilon\in[2, 5]$). The method converges rapidly,
allowing practitioners to trade iterations for enhanced privacy while
maintaining competitive utility.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [68] [Simulating Posterior Bayesian Neural Networks with Dependent Weights](https://arxiv.org/abs/2507.22095)
*Nicola Apollonio,Giovanni Franzina,Giovanni Luca Torrisi*

Main category: stat.ML

TL;DR: 论文研究了具有依赖权重的贝叶斯全连接前馈深度神经网络的后验分布，特别在高斯似然情况下，确定了宽宽度极限的分布并提供了采样算法。在浅层网络中，明确计算了输出的分布，证明其为高斯混合分布，并通过数值验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 研究具有依赖权重的贝叶斯神经网络的后验分布及其性质，为理解深度神经网络的极限行为提供理论支持。

Method: 采用贝叶斯方法，分析全连接前馈深度神经网络的后验分布，特别关注高斯似然情况下的宽宽度极限分布，并设计采样算法。在浅层网络中，通过数学推导计算输出分布。

Result: 确定了宽宽度极限的分布，并提供了采样算法；在浅层网络中，输出分布为高斯混合分布。所有结果均通过数值实验验证。

Conclusion: 论文为贝叶斯神经网络的后验分布提供了理论分析和数值验证，证明了在特定条件下输出分布的性质，为后续研究奠定了基础。

Abstract: In this paper we consider posterior Bayesian fully connected and feedforward
deep neural networks with dependent weights. Particularly, if the likelihood is
Gaussian, we identify the distribution of the wide width limit and provide an
algorithm to sample from the network. In the shallow case we explicitly compute
the distribution of the output, proving that it is a Gaussian mixture. All the
theoretical results are numerically validated.

</details>


### [69] [Stacked SVD or SVD stacked? A Random Matrix Theory perspective on data integration](https://arxiv.org/abs/2507.22170)
*Tavor Z. Baharav,Phillip B. Nicol,Rafael A. Irizarry,Rong Ma*

Main category: stat.ML

TL;DR: 论文分析了两种共享潜在结构估计方法（Stack-SVD和SVD-Stack）的渐近性能和相变，提出最优加权方案，并证明加权Stack-SVD优于加权SVD-Stack。


<details>
  <summary>Details</summary>
Motivation: 现代数据分析需要跨多个高维数据集识别共享潜在结构，但现有方法缺乏理论支持，导致选择困难。

Method: 推导两种方法的渐近性能和相变表达式，开发最优加权方案，并扩展到多共享组件。

Result: 加权Stack-SVD优于加权SVD-Stack，数值模拟和基因组数据实验验证了理论结果。

Conclusion: 提供了方法选择的理论指导，优化了数据集成问题的解决方案。

Abstract: Modern data analysis increasingly requires identifying shared latent
structure across multiple high-dimensional datasets. A commonly used model
assumes that the data matrices are noisy observations of low-rank matrices with
a shared singular subspace. In this case, two primary methods have emerged for
estimating this shared structure, which vary in how they integrate information
across datasets. The first approach, termed Stack-SVD, concatenates all the
datasets, and then performs a singular value decomposition (SVD). The second
approach, termed SVD-Stack, first performs an SVD separately for each dataset,
then aggregates the top singular vectors across these datasets, and finally
computes a consensus amongst them. While these methods are widely used, they
have not been rigorously studied in the proportional asymptotic regime, which
is of great practical relevance in today's world of increasing data size and
dimensionality. This lack of theoretical understanding has led to uncertainty
about which method to choose and limited the ability to fully exploit their
potential. To address these challenges, we derive exact expressions for the
asymptotic performance and phase transitions of these two methods and develop
optimal weighting schemes to further improve both methods. Our analysis reveals
that while neither method uniformly dominates the other in the unweighted case,
optimally weighted Stack-SVD dominates optimally weighted SVD-Stack. We extend
our analysis to accommodate multiple shared components, and provide practical
algorithms for estimating optimal weights from data, offering theoretical
guidance for method selection in practical data integration problems. Extensive
numerical simulations and semi-synthetic experiments on genomic data
corroborate our theoretical findings.

</details>


### [70] [LVM-GP: Uncertainty-Aware PDE Solver via coupling latent variable model and Gaussian process](https://arxiv.org/abs/2507.22493)
*Xiaodong Feng,Ling Guo,Xiaoliang Wan,Hao Wu,Tao Zhou,Wenwen Zhou*

Main category: stat.ML

TL;DR: 提出了一种名为LVM-GP的概率框架，用于在噪声数据下求解偏微分方程（PDE）的正反问题时进行不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如B-PINNs和深度集成）在功能依赖性和不确定性量化方面存在不足，需要更高效的解决方案。

Method: 结合了置信感知编码器和概率解码器，通过高斯过程和神经算子的融合构建随机映射。

Result: 在数值实验中表现出较高的预测准确性和鲁棒的不确定性量化能力。

Conclusion: LVM-GP框架在解决PDE问题时具有高效性和可靠性。

Abstract: We propose a novel probabilistic framework, termed LVM-GP, for uncertainty
quantification in solving forward and inverse partial differential equations
(PDEs) with noisy data. The core idea is to construct a stochastic mapping from
the input to a high-dimensional latent representation, enabling
uncertainty-aware prediction of the solution. Specifically, the architecture
consists of a confidence-aware encoder and a probabilistic decoder. The encoder
implements a high-dimensional latent variable model based on a Gaussian process
(LVM-GP), where the latent representation is constructed by interpolating
between a learnable deterministic feature and a Gaussian process prior, with
the interpolation strength adaptively controlled by a confidence function
learned from data. The decoder defines a conditional Gaussian distribution over
the solution field, where the mean is predicted by a neural operator applied to
the latent representation, allowing the model to learn flexible
function-to-function mapping. Moreover, physical laws are enforced as soft
constraints in the loss function to ensure consistency with the underlying PDE
structure. Compared to existing approaches such as Bayesian physics-informed
neural networks (B-PINNs) and deep ensembles, the proposed framework can
efficiently capture functional dependencies via merging a latent Gaussian
process and neural operator, resulting in competitive predictive accuracy and
robust uncertainty quantification. Numerical experiments demonstrate the
effectiveness and reliability of the method.

</details>


### [71] [A Unified Analysis of Generalization and Sample Complexity for Semi-Supervised Domain Adaptation](https://arxiv.org/abs/2507.22632)
*Elif Vural,Huseyin Karaca*

Main category: stat.ML

TL;DR: 论文提出了基于域对齐的领域自适应算法的理论分析，推导了泛化边界和样本复杂度，并通过实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 领域自适应的理论分析多集中在简化场景，未能充分反映现代方法通过特征变换对齐域的行为，因此需要更全面的理论研究。

Method: 研究基于域对齐的特征变换和共享分类器的联合学习，推导泛化边界，并分析使用MMD或对抗目标的神经网络的样本复杂度。

Result: 对于基于MMD和对抗的模型，样本复杂度随网络深度和宽度呈二次增长；半监督设置中，目标损失的缩放与标记目标样本数的平方根成比例。

Conclusion: 理论分析为领域自适应算法提供了更全面的理解，并通过实验验证了其有效性。

Abstract: Domain adaptation seeks to leverage the abundant label information in a
source domain to improve classification performance in a target domain with
limited labels. While the field has seen extensive methodological development,
its theoretical foundations remain relatively underexplored. Most existing
theoretical analyses focus on simplified settings where the source and target
domains share the same input space and relate target-domain performance to
measures of domain discrepancy. Although insightful, these analyses may not
fully capture the behavior of modern approaches that align domains into a
shared space via feature transformations. In this paper, we present a
comprehensive theoretical study of domain adaptation algorithms based on domain
alignment. We consider the joint learning of domain-aligning feature
transformations and a shared classifier in a semi-supervised setting. We first
derive generalization bounds in a broad setting, in terms of covering numbers
of the relevant function classes. We then extend our analysis to characterize
the sample complexity of domain-adaptive neural networks employing maximum mean
discrepancy (MMD) or adversarial objectives. Our results rely on a rigorous
analysis of the covering numbers of these architectures. We show that, for both
MMD-based and adversarial models, the sample complexity admits an upper bound
that scales quadratically with network depth and width. Furthermore, our
analysis suggests that in semi-supervised settings, robustness to limited
labeled target data can be achieved by scaling the target loss proportionally
to the square root of the number of labeled target samples. Experimental
evaluation in both shallow and deep settings lends support to our theoretical
findings.

</details>


### [72] [Subgrid BoostCNN: Efficient Boosting of Convolutional Networks via Gradient-Guided Feature Selection](https://arxiv.org/abs/2507.22842)
*Biyi Fang,Jean Utke,Truong Vo,Diego Klabjan*

Main category: stat.ML

TL;DR: 提出了一种结合动态特征选择和BoostCNN原则的新框架，通过子网格选择和重要性采样提升CNN性能，实验证明其优于传统CNN。


<details>
  <summary>Details</summary>
Motivation: 传统CNN因层数和参数多导致训练成本高且需手动调优，需改进其效率和性能。

Method: 采用子网格选择和重要性采样策略，将Boosting权重嵌入网络训练过程，使用最小二乘损失公式。

Result: 在多个细粒度分类基准测试中，性能优于传统CNN，且训练速度更快。

Conclusion: 新框架不仅减轻了手动设计架构的负担，还提高了准确性和效率。

Abstract: Convolutional Neural Networks (CNNs) have achieved remarkable success across
a wide range of machine learning tasks by leveraging hierarchical feature
learning through deep architectures. However, the large number of layers and
millions of parameters often make CNNs computationally expensive to train,
requiring extensive time and manual tuning to discover optimal architectures.
In this paper, we introduce a novel framework for boosting CNN performance that
integrates dynamic feature selection with the principles of BoostCNN. Our
approach incorporates two key strategies: subgrid selection and importance
sampling, to guide training toward informative regions of the feature space. We
further develop a family of algorithms that embed boosting weights directly
into the network training process using a least squares loss formulation. This
integration not only alleviates the burden of manual architecture design but
also enhances accuracy and efficiency. Experimental results across several
fine-grained classification benchmarks demonstrate that our boosted CNN
variants consistently outperform conventional CNNs in both predictive
performance and training speed.

</details>


### [73] [Consistency of Feature Attribution in Deep Learning Architectures for Multi-Omics](https://arxiv.org/abs/2507.22877)
*Daniel Claborne,Javier Flores,Samantha Erwin,Luke Durell,Rachel Richardson,Ruby Fore,Lisa Bramer*

Main category: stat.ML

TL;DR: 论文探讨了在多组学数据中应用SHAP方法评估多视图深度学习模型的特征重要性，发现SHAP的排名对模型架构和权重初始化敏感，并提出了一种替代方法来评估生物分子识别的稳健性。


<details>
  <summary>Details</summary>
Motivation: 机器学习在生物学研究中应用广泛，但模型可解释性仍存在挑战。研究旨在通过SHAP方法提升多视图深度学习模型的可解释性，并评估其稳健性。

Method: 使用SHAP方法对多视图深度学习模型的特征重要性进行排名，并通过随机森林模型和聚类质量评估SHAP的稳健性。

Result: 研究发现SHAP的排名对模型架构和权重初始化敏感，提出了替代方法来提高生物分子识别的可靠性。

Conclusion: 建议在多组学数据的多视图深度学习模型中谨慎使用SHAP方法，并推荐使用替代方法评估特征重要性。

Abstract: Machine and deep learning have grown in popularity and use in biological
research over the last decade but still present challenges in interpretability
of the fitted model. The development and use of metrics to determine features
driving predictions and increase model interpretability continues to be an open
area of research. We investigate the use of Shapley Additive Explanations
(SHAP) on a multi-view deep learning model applied to multi-omics data for the
purposes of identifying biomolecules of interest. Rankings of features via
these attribution methods are compared across various architectures to evaluate
consistency of the method. We perform multiple computational experiments to
assess the robustness of SHAP and investigate modeling approaches and
diagnostics to increase and measure the reliability of the identification of
important features. Accuracy of a random-forest model fit on subsets of
features selected as being most influential as well as clustering quality using
only these features are used as a measure of effectiveness of the attribution
method. Our findings indicate that the rankings of features resulting from SHAP
are sensitive to the choice of architecture as well as different random
initializations of weights, suggesting caution when using attribution methods
on multi-view deep learning models applied to multi-omics data. We present an
alternative, simple method to assess the robustness of identification of
important biomolecules.

</details>
