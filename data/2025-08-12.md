<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 26]
- [cs.LG](#cs.LG) [Total: 144]
- [stat.ML](#stat.ML) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [A Completely Blind Channel Estimation Technique for OFDM Using Generalized Constellation Splitting and Modified Phase-Directed Algorithm](https://arxiv.org/abs/2508.06508)
*Sameera Bharadwaja H.,D. K. Mehra*

Main category: eess.SP

TL;DR: 论文提出了一种解决SISO-OFDM系统中盲信道估计的算法，通过广义星座分裂和改进的相位导向算法解决了传统方法中的复数标量估计模糊问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于二阶统计量的盲信道估计方法存在复数标量估计模糊问题，需依赖导频或参考符号解决，这与盲技术的初衷相矛盾。

Method: 提出了一种结合广义星座分裂和改进相位导向算法的盲方法，无需导频即可解决估计模糊问题。

Result: 通过MATLAB数值模拟验证了所提算法的性能。

Conclusion: 所提算法有效解决了盲信道估计中的复数标量模糊问题，避免了导频的使用，符合盲技术的初衷。

Abstract: The problem of blind channel estimation for SISO-OFDM systems using
second-order statistics (SOS) is addressed. A comparison of two prominent
SOS-based techniques: subspace-decomposition and precoding-induced
correlation-averaging techniques in terms of MSE performance is presented. The
drawback of these methods is that they suffer from a complex-scalar estimation
ambiguity which is resolved by using pilots or reference symbols. By using
pilots the whole purpose of blind techniques is contradicted. We propose an
algorithm to resolve this ambiguity in blind manner using generalized
constellation-splitting and modified phase-directed algorithm. The performance
of the proposed scheme is evaluated via numerical simulations in MATLAB
environment.

</details>


### [2] [GPU-accelerated Direct Geolocation of GNSS Interference](https://arxiv.org/abs/2508.06672)
*Jacob S. Clements,Zachary L. Clements*

Main category: eess.SP

TL;DR: 论文提出了一种利用GPU加速的直接地理定位方法，以解决低地球轨道卫星接收器在GNSS干扰检测中的计算负担问题。


<details>
  <summary>Details</summary>
Motivation: 近年来，GNSS干扰激增，现有接收器缺乏有效防御措施，低地球轨道卫星为干扰检测提供了新机会，但传统直接地理定位方法计算量大。

Method: 通过利用位置域相关性在候选点和时间步上的独立性，将计算任务并行化到GPU上，实现加速。

Result: GPU加速的直接地理定位显著提升了计算效率，相比传统CPU处理具有明显优势。

Conclusion: GPU加速方法有效解决了直接地理定位的计算瓶颈，为GNSS干扰检测提供了实用解决方案。

Abstract: In recent years, there has been a sharp increase in Global Navigation
Satellite Systems (GNSS) interference, which has proven to be problematic in
GNSS-dependent civilian applications. Many currently deployed GNSS receivers
lack the proper countermeasures to defend themselves against interference,
prompting the need for alternative defenses. Satellites in Low Earth Orbit
(LEO) provide an opportunity for GNSS interference detection, classification,
and localization. The direct geolocation approach has been shown to be
well-suited for low SNR regimes and in cases limited to short captures --
exactly what is expected for receivers in LEO. Direct geolocation is a
single-step search over a geographical grid that enables estimation of the
transmitter location directly from correlating raw observed signals. However, a
key limitation to this approach is the computational requirements. This
computational burden is compounded for LEO-based receivers as the geographic
search space is extensive. This paper alleviates the computational burden of
direct geolocation by exploiting the independence of position-domain
correlation across candidate points and time steps: nearly all computation can
be accomplished in parallel on a graphics processing unit (GPU). This paper
presents and evaluates the performance of GPU-accelerated direct geolocation
compared to traditional CPU processing.

</details>


### [3] [Applying the Spectral Method for Modeling Linear Filters: Butterworth, Linkwitz-Riley, and Chebyshev filters](https://arxiv.org/abs/2508.07206)
*Konstantin A. Rybakov,Egor D. Shermatov*

Main category: eess.SP

TL;DR: 提出了一种基于线性系统数学描述谱形式的新型计算机建模线性滤波器技术。


<details>
  <summary>Details</summary>
Motivation: 传统滤波器建模方法可能无法高效处理连续时间信号，需要更灵活的技术。

Method: 输入和输出信号表示为正交展开，滤波器用二维非平稳传递函数描述。

Result: 成功测试了Butterworth、Linkwitz-Riley和Chebyshev滤波器不同阶数。

Conclusion: 该技术能有效建模连续时间输出信号，适用于多种滤波器类型。

Abstract: This paper proposes a new technique for computer modeling linear filters
based on the spectral form of mathematical description of linear systems. It
assumes that input and output signals of the filter are represented as
orthogonal expansions, while filters themselves are described by
two-dimensional non-stationary transfer functions. This technique allows one to
model the output signal in continuous time, and it is successfully tested on
the Butterworth, Linkwitz-Riley, and Chebyshev filters with different orders.

</details>


### [4] [Physical Layer Authentication Based on Hierarchical Variational Auto-Encoder for Industrial Internet of Things](https://arxiv.org/abs/2508.06794)
*Rui Meng,Xiaodong Xu,Bizhu Wang,Hao Sun,Shida Xia,Shujun Han,Ping Zhang*

Main category: eess.SP

TL;DR: 本文提出了一种基于信道冲激响应（CIR）的物理层认证方案HVAE，用于工业物联网（IIoT），无需攻击者先验信道信息即可实现高性能认证。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中，如IIoT，现有物理层认证（PLA）方案需要攻击者的先验信道信息，导致性能下降。本文旨在解决这一问题。

Method: HVAE结合了自动编码器（AE）和变分自动编码器（VAE）模块，提取CIR特征并输出认证结果，同时考虑了单峰和双峰高斯分布的新目标函数。

Result: 仿真结果表明，HVAE在静态和移动IIoT场景下优于其他三种PLA方案，即使训练数据较少。

Conclusion: HVAE是一种高效的PLA方案，适用于复杂环境中的IIoT认证。

Abstract: Recently, Physical Layer Authentication (PLA) has attracted much attention
since it takes advantage of the channel randomness nature of transmission media
to achieve communication confidentiality and authentication. In the complex
environment, such as the Industrial Internet of Things (IIoT), machine learning
(ML) is widely employed with PLA to extract and analyze complex channel
characteristics for identity authentication. However, most PLA schemes for IIoT
require attackers' prior channel information, leading to severe performance
degradation when the source of the received signals is unknown in the training
stage. Thus, a channel impulse response (CIR)-based PLA scheme named
"Hierarchical Variational Auto-Encoder (HVAE)" for IIoT is proposed in this
article, aiming at achieving high authentication performance without knowing
attackers' prior channel information even when trained on a few data in the
complex environment. HVAE consists of an Auto-Encoder (AE) module for CIR
characteristics extraction and a Variational Auto-Encoder (VAE) module for
improving the representation ability of the CIR characteristic and outputting
the authentication results. Besides, a new objective function is constructed in
which both the single-peak and the double-peak Gaussian distribution are taken
into consideration in the VAE module. Moreover, the simulations are conducted
under the static and mobile IIoT scenario, which verify the superiority of the
proposed HVAE over three comparison PLA schemes even with a few training data.

</details>


### [5] [Deep Domain-Adversarial Adaptation for Automatic Modulation Classification under Channel Variability](https://arxiv.org/abs/2508.06829)
*K. A. Shahriar*

Main category: eess.SP

TL;DR: 提出了一种基于DANN的深度学习框架，用于解决AMC任务中信道引起的分布偏移问题，显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 在认知无线电系统中，AMC的准确性对自适应通信至关重要，但传统模型在异构信道条件下泛化能力不足。

Method: 采用域对抗神经网络（DANN）框架，显式缓解源域与目标域之间的信道诱导分布偏移。

Result: 在模拟数据集上，DANN模型相比基线模型在某些调制情况下绝对准确率提升14.93%。

Conclusion: DANN在真实信道变异性下具有工程可行性，为自适应频谱智能研究提供了稳健方向。

Abstract: Automatic Modulation Classification (AMC) plays a significant role in modern
cognitive and intelligent radio systems, where accurate identification of
modulation is crucial for adaptive communication. The presence of heterogeneous
wireless channel conditions, such as Rayleigh and Rician fading, poses
significant challenges to the generalization ability of conventional AMC
models. In this work, a domain-adversarial neural network (DANN) based deep
learning framework is proposed that explicitly mitigates channel-induced
distribution shifts between source and target domains. The approach is
evaluated using a comprehensive simulated dataset containing five modulation
schemes (BPSK, QPSK, 16QAM, 64QAM, 256QAM) across Rayleigh and Rician fading
channels at five frequency bands. Comparative experiments demonstrate that the
DANN-based model achieves up to 14.93% absolute accuracy improvement in certain
modulation cases compared to a baseline supervised model trained solely on the
source domain. The findings establish the engineering feasibility of domain
adversarial learning in AMC tasks under real-world channel variability and
offer a robust direction for future research in adaptive spectrum intelligence

</details>


### [6] [Secure Transmission for Cell-Free Symbiotic Radio Communications with Movable Antenna: Continuous and Discrete Positioning Designs](https://arxiv.org/abs/2508.06868)
*Bin Lyu,Jiayu Guan,Meng Hua,Changsheng You,Tianqi Mao,Abbas Jamalipour*

Main category: eess.SP

TL;DR: 论文研究了基于可移动天线（MA）的安全传输方案，用于可重构智能表面（RIS）辅助的无蜂窝共生无线电（SR）系统，通过优化MA位置提升主次传输的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 提升无蜂窝共生无线电系统的安全性和传输效率，同时抑制窃听风险。

Method: 针对连续和离散MA位置情况，分别提出基于差分进化（DEO）的两层迭代优化方法和交替优化（AO）框架。

Result: 数值结果验证了所提方案及其优化算法的有效性。

Conclusion: MA赋能的安全传输方案及其优化算法在提升系统性能方面具有显著效果。

Abstract: In this paper, we study a movable antenna (MA) empowered secure transmission
scheme for reconfigurable intelligent surface (RIS) aided cell-free symbiotic
radio (SR) system. Specifically, the MAs deployed at distributed access points
(APs) work collaboratively with the RIS to establish high-quality propagation
links for both primary and secondary transmissions, as well as suppressing the
risk of eavesdropping on confidential primary information. We consider both
continuous and discrete MA position cases and maximize the secrecy rate of
primary transmission under the secondary transmission constraints,
respectively. For the continuous position case, we propose a two-layer
iterative optimization method based on differential evolution with one-in-one
representation (DEO), to find a high-quality solution with relatively moderate
computational complexity. For the discrete position case, we first extend the
DEO based iterative framework by introducing the mapping and determination
operations to handle the characteristic of discrete MA positions. To further
reduce the computational complexity, we then design an alternating optimization
(AO) iterative framework to solve all variables within a single layer. In
particular, we develop an efficient strategy to derive the sub-optimal solution
for the discrete MA positions, superseding the DEO-based method. Numerical
results validate the effectiveness of the proposed MA empowered secure
transmission scheme along with its optimization algorithms.

</details>


### [7] [Extremely Large-Scale Dynamic Metasurface Antennas for 6G Near-Field Networks: Opportunities and Challenges](https://arxiv.org/abs/2508.06952)
*Haiyang Zhang,Nir Shlezinger,Giulia Torcolacci,Francesco Guidi,Anna Guerra,Qianyu Yang,Mohammadreza F. Imani,Davide Dardari,Yonina C. Eldar*

Main category: eess.SP

TL;DR: 论文探讨了6G网络中XL-DMAs（超大规模动态超表面天线）的机遇与挑战，包括其基本原理、近场模型、应用场景及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要支持更高数据速率、高精度定位和成像能力，传统架构存在高功耗和高成本问题，XL-DMAs提供了一种低功耗、低成本的解决方案。

Method: 介绍了XL-DMAs的基本原理和近场模型，并探讨了其在近场通信、定位和成像中的应用。

Result: XL-DMAs在6G近场网络中展现出巨大潜力，能够满足高数据速率和精准定位等需求。

Conclusion: 未来需解决XL-DMAs的开放性问题，以充分发挥其在6G近场网络中的潜力。

Abstract: 6G networks will need to support higher data rates, high-precision
localization, and imaging capabilities. Near-field technologies, enabled by
extremely large-scale (XL)-arrays, are expected to be essential physical-layer
solutions to meet these ambitious requirements. However, implementing XL-array
systems using traditional fully-digital or hybrid analog/digital architectures
poses significant challenges due to high power consumption and implementation
costs. Emerging XL-dynamic metasurface antennas (XL-DMAs) provide a promising
alternative, enabling ultra-low power and cost-efficient solutions, making them
ideal candidates for 6G near-field networks. In this article, we discuss the
opportunities and challenges of XL-DMAs employed in 6G near-field networks. We
first outline the fundamental principles of XL-DMAs and present the specifics
of the near-field model of XL-DMAs. We then highlight several promising
applications that might benefit from XL-DMAs, including near-field
communication, localization, and imaging. Finally, we discuss several open
problems and potential future directions that should be addressed to fully
exploit the capabilities of XL-DMAs in the next 6G near-field networks.

</details>


### [8] [Millimeter-Wave Position Sensing Using Reconfigurable Intelligent Surfaces: Positioning Error Bound and Phase Shift Configuration](https://arxiv.org/abs/2508.06958)
*Xin Cheng,Guangjie Han,Menglu Li,Ruoguang Li,Feng Shu*

Main category: eess.SP

TL;DR: 该论文研究了基于多RIS辅助的3D MISO毫米波定位系统，提出了两种优化方法以最小化定位误差，并通过仿真验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 毫米波定位是下一代智能系统的关键技术，而RIS的动态操控能力为高精度定位提供了新机遇。

Method: 提出了一种测量框架，结合顺序RIS激活和定向波束成形，并针对连续和离散相位配置提出了两种优化算法。

Result: 数值仿真表明，所提算法能有效降低定位误差，多RIS显著提升了定位精度。

Conclusion: 多RIS辅助的毫米波定位系统具有显著潜力，所提优化方法为实际应用提供了有效解决方案。

Abstract: Millimeter-wave (mmWave) positioning has emerged as a promising technology
for next-generation intelligent systems. The advent of reconfigurable
intelligent surfaces (RISs) has revolutionized high-precision mmWave
localization by enabling dynamic manipulation of wireless propagation
environments. This paper investigates a three-dimensional (3D) multi-input
single-output (MISO) mmWave positioning system assisted by multiple RISs. We
introduce a measurement framework incorporating sequential RIS activation and
directional beamforming to fully exploit virtual line-of-sight (VLoS) paths.
The theoretical performance limits are rigorously analyzed through derivation
of the Fisher information and subsequent positioning error bound (PEB). To
minimize the PEB, two distinct optimization approaches are proposed for
continuous and discrete phase shift configurations of RISs. For continuous
phase shifts, a Riemannian manifold-based optimization algorithm is proposed.
For discrete phase shifts, a heuristic algorithm incorporating the grey wolf
optimizer is proposed. Extensive numerical simulations demonstrate the
effectiveness of the proposed algorithms in reducing the PEB and validate the
improvement in positioning accuracy achieved by multiple RISs.

</details>


### [9] [Joint Beamforming Optimization for Pinching-Antenna Systems (PASS)-assisted Symbiotic Radio](https://arxiv.org/abs/2508.07002)
*Ze Wang,Guoping Zhang,Hongbo Xu*

Main category: eess.SP

TL;DR: 本文提出了一种基于夹持天线系统（PASS）的新型下行链路共生无线电（SR）框架，通过可重构天线位置优化主次传输性能。


<details>
  <summary>Details</summary>
Motivation: 旨在通过灵活调整天线位置，同时操控大尺度路径损耗和信号相位，提升主次传输性能。

Method: 提出了两种解决方案：1）基于学习的梯度下降（LGD）算法，将问题转化为可微分形式并通过端到端学习求解；2）两阶段优化方法（SCA-PSO），先优化发射波束成形，再通过粒子群优化（PSO）搜索夹持波束成形。

Result: SCA-PSO算法在降低计算复杂度的同时接近逐元素方法的性能，并优于LGD方法，避免了局部最优。

Conclusion: PASS框架和提出的优化方法有效提升了SR系统的性能，为未来无线通信提供了新思路。

Abstract: This paper investigates a novel downlink symbiotic radio (SR) framework
empowered by the pinching antenna system (PASS), aiming to enhance both primary
and secondary transmissions through reconfigurable antenna positioning. PASS
consists of multiple waveguides equipped with numerous low-cost pinching
antennas (PAs), whose positions can be flexibly adjusted to simultaneously
manipulate large-scale path loss and signal phases.We formulate a joint
transmit and pinching beamforming optimization problem to maximize the
achievable sum rate while satisfying the detection error probability constraint
for the IR and the feasible deployment region constraints for the PAs. This
problem is inherently nonconvex and highly coupled. To address it, two solution
strategies are developed. 1) A learning-aided gradient descent (LGD) algorithm
is proposed, where the constrained problem is reformulated into a
differentiable form and solved through end-to-end learning based on the
principle of gradient descent. The PA position matrix is reparameterized to
inherently satisfy minimum spacing constraints, while transmit power and
waveguide length limits are enforced via projection and normalization. 2) A
two-stage optimization-based approach is designed, in which the transmit
beamforming is first optimized via successive convex approximation (SCA),
followed by pinching beamforming optimization using a particle swarm
optimization (PSO) search over candidate PA placements. The SCA-PSO algorithm
achieves performance close to that of the element-wise method while
significantly reducing computational complexity by exploring a randomly
generated effective solution subspace, while further improving upon the LGD
method by avoiding undesirable local optima.

</details>


### [10] [Robust Super-Resolution Compressive Sensing: A Two-timescale Alternating MAP Approach](https://arxiv.org/abs/2508.07013)
*Yufan Zhou,Jingyi Li,Wenkang Xu,An Liu*

Main category: eess.SP

TL;DR: 本文提出了一种基于双时间尺度交替最大后验（MAP）的鲁棒超分辨率压缩感知算法框架，解决了现有方法在网格参数不精确和密集情况下的分辨率限制和超参数敏感性问题。


<details>
  <summary>Details</summary>
Motivation: 现有超分辨率压缩感知方法在网格参数不精确或密集时分辨率有限且对超参数敏感，影响了稀疏信号的准确恢复。

Method: 采用双时间尺度交替MAP框架，结合稀疏信号估计模块（基于tanh-VBI算法）和网格更新模块（基于BFGS算法），实现超分辨率估计。

Result: 仿真结果表明，该算法在信道外推问题中优于基线方案。

Conclusion: 提出的算法框架能有效提升超分辨率压缩感知的性能，具有低计算成本和强稀疏性促进能力。

Abstract: The problem of super-resolution compressive sensing (SR-CS) is crucial for
various wireless sensing and communication applications. Existing methods often
suffer from limited resolution capabilities and sensitivity to
hyper-parameters, hindering their ability to accurately recover sparse signals
when the grid parameters do not lie precisely on a fixed grid and are close to
each other. To overcome these limitations, this paper introduces a novel robust
super-resolution compressive sensing algorithmic framework using a
two-timescale alternating maximum a posteriori (MAP) approach. At the slow
timescale, the proposed framework iterates between a sparse signal estimation
module and a grid update module. In the sparse signal estimation module, a
hyperbolic-tangent prior distribution based variational Bayesian inference
(tanh-VBI) algorithm with a strong sparsity promotion capability is adopted to
estimate the posterior probability of the sparse vector and accurately identify
active grid components carrying primary energy under a dense grid.
Subsequently, the grid update module utilizes the BFGS algorithm to refine
these low-dimensional active grid components at a faster timescale to achieve
super-resolution estimation of the grid parameters with a low computational
cost. The proposed scheme is applied to the channel extrapolation problem, and
simulation results demonstrate the superiority of the proposed scheme compared
to baseline schemes.

</details>


### [11] [Pinching-Antenna System Design with LoS Blockage: Does In-Waveguide Attenuation Matter?](https://arxiv.org/abs/2508.07131)
*Yanqing Xu,Zhiguo Ding,Octavia A. Dobre,Tsung-Hui Chang*

Main category: eess.SP

TL;DR: 本文研究了波导内衰减对夹持天线系统性能的影响，发现在典型视距（LoS）阻塞条件下，忽略波导内衰减对数据速率的损失可忽略。通过动态样本平均逼近算法优化多用户场景下的天线位置和波束成形，显著提升了系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有文献中波导内衰减常被忽略，但其对系统性能的影响尚未充分研究，尤其是在非理想视距条件下。

Method: 从单用户场景推导波导内衰减对数据速率损失的影响，扩展到多用户场景，提出动态样本平均逼近算法优化天线位置和波束成形。

Result: 仿真结果表明，在典型视距阻塞条件下，波导内衰减的影响可忽略，且优化算法显著提升了系统性能。

Conclusion: 波导内衰减在典型条件下影响有限，动态优化算法在多用户场景中表现出色。

Abstract: In the literature of pinching-antenna systems, in-waveguide attenuation is
often neglected to simplify system design and enable more tractable analysis.
However, its effect on overall system performance has received limited
attention in the existing literature. While a recent study has shown that, in
line-of-sight (LoS)-dominated environments, the data rate loss incurred by
omitting in-waveguide attenuation is negligible when the communication area is
not excessively large, its effect under more general conditions remains
unclear. This work extends the analysis to more realistic scenarios involving
arbitrary levels of LoS blockage. We begin by examining a single-user case and
derive an explicit expression for the average data rate loss caused by
neglecting in-waveguide attenuation. The results demonstrate that, even for
large service areas, the rate loss remains negligible under typical LoS
blockage conditions. We then consider a more general multi-user scenario, where
multiple pinching antennas, each deployed on a separate waveguide, jointly
serve multiple users. The objective is to maximize the average sum rate by
jointly optimize antenna positions and transmit beamformers to maximize the
average sum rate under probabilistic LoS blockage. To solve the resulting
stochastic and nonconvex optimization problem, we propose a dynamic sample
average approximation (SAA) algorithm. At each iteration, this method replaces
the expected objective with an empirical average computed from dynamically
regenerated random channel realizations, ensuring that the optimization
accurately reflects the current antenna configuration. Extensive simulation
results are provided to the proposed algorithm and demonstrate the substantial
performance gains of pinching-antenna systems, particularly in environments
with significant LoS blockage.

</details>


### [12] [Low-Complexity Equalization of Zak-OTFS in the Frequency Domain](https://arxiv.org/abs/2508.07148)
*Sandesh Rao Mattu,Nishant Mehrotra,Saif Khan Mohammed,Venkatesh Khammammetti,Robert Calderbank*

Main category: eess.SP

TL;DR: 论文提出了一种低复杂度的频域（FD）均衡方法，用于Zak-OTFS调制，解决了传统OFDM在高移动性下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: OFDM在高移动性下因多普勒扩展导致子载波间干扰（ICI），性能下降。Zak-OTFS对双选择性信道具有鲁棒性，但均衡复杂度高。

Method: 提出频域均衡方法，推导FD系统模型，证明其与DD模型等价，并利用共轭梯度法降低复杂度。

Result: FD均衡复杂度与Zak-OTFS帧维度呈线性关系，性能与DD域均衡相当。

Conclusion: 频域均衡为Zak-OTFS提供了低复杂度且高效的解决方案。

Abstract: 4G/5G wireless standards use orthogonal frequency division multiplexing
(OFDM) which is robust to frequency selectivity. Equalization is possible with
a single tap filter, and low-complexity equalization makes OFDM an attractive
physical layer. However the performance of OFDM degrades with mobility, since
Doppler spreads introduce inter-carrier interference (ICI) between subcarriers
and they are no longer orthogonal. Zak-transform based orthogonal time
frequency space (Zak-OTFS) modulation has been shown to be robust to doubly
selective channels. Zak-OTFS signals are formed in the delay-Doppler (DD)
domain, converted to time domain (TD) for transmission and reception, then
returned to the DD domain for processing. The received signal is a
superposition of many attenuated copies since the doubly selective channel
introduces delay and Doppler shifts. The received symbols are more difficult to
equalize since they are subject to interference along both delay and Doppler
axes. In this paper, we propose a new low-complexity method of equalizing
Zak-OTFS in the frequency domain (FD). We derive the FD system model and show
that it is unitarily equivalent to the DD system model. We show that the
channel matrix in the FD is banded, making it possible to apply conjugate
gradient methods to reduce the complexity of equalization. We show that
complexity of FD equalization is linear in the dimension of a Zak-OTFS frame.
For comparison the complexity of naive MMSE equalization is cubic in the frame
dimension. Through numerical simulations we show that FD equalization of
Zak-OTFS achieves similar performance as equalization in DD domain.

</details>


### [13] [Vector Orthogonal Chirp Division Multiplexing Over Doubly Selective Channels](https://arxiv.org/abs/2508.07160)
*Deyu Lu,Xiaoli Ma,Yiyin Wang*

Main category: eess.SP

TL;DR: 论文将正交啁啾分复用（OCDM）扩展为向量OCDM（VOCDM），以提供更多设计自由度处理双选择性信道。通过并行IDFnT实现VOCDM调制，并基于CE-BEM模型分析其性能。


<details>
  <summary>Details</summary>
Motivation: 扩展OCDM以应对双选择性信道，提供更多设计自由度。

Method: 通过M个并行N点逆离散菲涅尔变换（IDFnT）实现VOCDM调制，基于CE-BEM模型推导输入输出关系。

Result: VOCDM在双选择性信道下表现出优越的多样性性能，且PAPR随N减小而降低。

Conclusion: VOCDM在满足特定约束条件下性能优越，理论结果通过数值模拟验证。

Abstract: In this letter, we extend orthogonal chirp division multiplexing (OCDM) to
vector OCDM (VOCDM) to provide more design freedom to deal with doubly
selective channels. The VOCDM modulation is implemented by performing M
parallel N-size inverse discrete Fresnel transforms (IDFnT). Based on the
complex exponential basis expansion model (CE-BEM) for doubly selective
channels, we derive the VOCDM input-output relationship, and show performance
tradeoffs of VOCDM with respect to (w.r.t.) its modulation parameters M and N.
Specifically, we investigate the diversity and peak-to-average power ratio
(PAPR) of VOCDM w.r.t. M and N. Under doubly selective channels, VOCDM exhibits
superior diversity performance as long as the parameters M and N are configured
to satisfy some constraints from the delay and the Doppler spreads of the
channel, respectively. Furthermore, the PAPR of VOCDM signals decreases with a
decreasing N. These theoretical findings are verified through numerical
simulations.

</details>


### [14] [Multi-RIS Deployment Optimization for mmWave ISAC Systems in Real-World Environments](https://arxiv.org/abs/2508.07226)
*Yueheng Li,Xueyun Long,Mario Pauli,Suheng Tian,Xiang Wan,Benjamin Nuss,Tiejun Cui,Haixia Zhang,Thomas Zwick*

Main category: eess.SP

TL;DR: 论文提出了一种多RIS-ISAC系统的能效优化方法，通过优化RIS部署提升毫米波频段的通信覆盖和感知精度。


<details>
  <summary>Details</summary>
Motivation: 为未来空地一体化网络应用增强ISAC功能，需精心设计和评估RIS部署。

Method: 建立多RIS-ISAC系统信号模型，提出基于等效增益缩放的简化重构方法，设计两步迭代算法优化RIS部署。

Result: 仿真结果显示优化后的RIS部署显著提升了通信覆盖和感知精度，且所需RIS尺寸最小。

Conclusion: 优化RIS部署能有效提升系统性能，优于现有方法。

Abstract: Reconfigurable intelligent surface-assisted integrated sensing and
communication (RIS-ISAC) presents a promising system architecture to leverage
the wide bandwidth available at millimeter-wave (mmWave) frequencies, while
mitigating severe signal propagation losses and reducing infrastructure costs.
To enhance ISAC functionalities in the future air-ground integrated network
applications, RIS deployment must be carefully designed and evaluated, which
forms the core motivation of this paper. To ensure practical relevance, a
multi-RIS-ISAC system is established, with its signal model at mmWave
frequencies demonstrated using ray-launching calibrated to real-world
environments. On this basis, an energy-efficiency-driven optimization problem
is formulated to minimize the multi-RIS size-to-coverage sum ratio,
comprehensively considering real-world RIS deployment constraints, positions,
orientations, as well as ISAC beamforming strategies at both the base station
and the RISs. To solve the resulting non-convex mixed-integer problem, a
simplified reformulation based on equivalent gain scaling method is introduced.
A two-step iterative algorithm is then proposed, in which the deployment
parameters are determined under fixed RIS positions in the first step, and the
RIS position set is updated in the second step to progressively approach the
optimum solution. Simulation results based on realistic parameter benchmarks
present that the optimized RISs deployment significantly enhances communication
coverage and sensing accuracy with the minimum RIS sizes, outperforming
existing approaches.

</details>


### [15] [A Scalable Machine Learning Approach Enabled RIS Optimization with Implicit Channel Estimation](https://arxiv.org/abs/2508.07265)
*Bile Peng,Vahid Jamali,Eduard Jorswieck*

Main category: eess.SP

TL;DR: 论文提出了一种基于无监督机器学习的RIS优化方法，通过RISnet神经网络架构直接映射接收信号到RIS配置，无需显式信道估计，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决RIS元素可扩展性和信道状态信息需求两大难题，推动RIS在下一代移动无线电系统中的实际应用。

Method: 结合RISnet神经网络架构和隐式信道估计方法，直接从接收的导频信号映射到RIS配置。

Result: 仿真结果表明，所提算法显著优于基线方法。

Conclusion: 无监督机器学习方法为RIS配置提供了高效解决方案，克服了传统方法的局限性。

Abstract: The reconfigurable intelligent surface (RIS) is considered as a key enabler
of the next-generation mobile radio systems. While attracting extensive
interest from academia and industry due to its passive nature and low cost,
scalability of RIS elements and requirement for channel state information (CSI)
are two major difficulties for the RIS to become a reality. In this work, we
introduce an unsupervised machine learning (ML) enabled optimization approach
to configure the RIS. The dedicated neural network (NN) architecture RISnet is
combined with an implicit channel estimation method. The RISnet learns to map
from received pilot signals to RIS configuration directly without explicit
channel estimation. Simulation results show that the proposed algorithm
outperforms baselines significantly.

</details>


### [16] [Channel Charting in Smart Radio Environments](https://arxiv.org/abs/2508.07305)
*Mahdi Maleki,Reza Agahzadeh Ayoubi,Marouan Mizmizi,Umberto Spagnolini*

Main category: eess.SP

TL;DR: 本文提出利用静态电磁表面（EMS）通过信道制图（CC）在复杂城市环境中实现鲁棒的设备定位。通过优化EMS配置，显著降低了定位误差并提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 在非视距（NLoS）条件下，传统定位方法性能受限。本文旨在利用静态EMS增强信道差异性和空间指纹，以提升定位精度。

Method: 提出了一种基于码本的优化框架，设计EMS相位剖面以最大化关键嵌入指标、定位误差、可信度和连续性。通过3D射线追踪仿真验证。

Result: 优化后的EMS配置将90%分位定位误差从60米降至25米以下，同时显著提升可信度和连续性。

Conclusion: 本文首次利用静态EMS增强CC，在NLoS条件下实现了显著的定位性能提升。

Abstract: This paper introduces the use of static electromagnetic skins (EMSs) to
enable robust device localization via channel charting (CC) in realistic urban
environments. We develop a rigorous optimization framework that leverages EMS
to enhance channel dissimilarity and spatial fingerprinting, formulating EMS
phase profile design as a codebook-based problem targeting the upper quantiles
of key embedding metric, localization error, trustworthiness, and continuity.
Through 3D ray-traced simulations of a representative city scenario, we
demonstrate that optimized EMS configurations, in addition to significant
improvement of the average positioning error, reduce the 90th-percentile
localization error from over 60 m (no EMS) to less than 25 m, while drastically
improving trustworthiness and continuity. To the best of our knowledge, this is
the first work to exploit Smart Radio Environment (SRE) with static EMS for
enhancing CC, achieving substantial gains in localization performance under
challenging None-Line-of-Sight (NLoS) conditions.

</details>


### [17] [Detection and Classification of Internal Leakage in Hydraulic Cylinders](https://arxiv.org/abs/2508.07436)
*Mehrbod Zarifi,Mohamad Amin Jamshidi,Zolfa Anvari,Hamed Ghafarirad,Mohammad Zareinejad*

Main category: eess.SP

TL;DR: 提出了一种基于LSTM的液压系统泄漏检测算法，准确率达96%，可实时在线诊断故障，降低维护成本并延长系统寿命。


<details>
  <summary>Details</summary>
Motivation: 液压系统泄漏问题（尤其是内部泄漏）难以检测，可能导致系统效率下降或完全故障，亟需高效检测方法。

Method: 利用压力传感器采集数据，采用LSTM循环神经网络进行复杂数据分析，开发泄漏检测算法。

Result: 算法在泄漏分类中达到96%的准确率，支持实时在线故障诊断。

Conclusion: 该方法有效提升了液压系统泄漏检测的效率和准确性，具有实际应用价值。

Abstract: Hydraulic systems have been one of the most used technologies in many
industries due to their reliance on incompressible fluids that facilitate
energy and power transfer. Within such systems, hydraulic cylinders are prime
devices that convert hydraulic energy into mechanical energy. Some of the
genuine and very common problems related to hydraulic cylinders are leakages.
Leakage in hydraulic systems can cause a drop in pressure, general
inefficiency, and even complete failure of such systems. The various ways
leakage can occur define the major categorization of leakage: internal and
external leakage. External leakage is easily noticeable, while internal
leakage, which involves fluid movement between pressure chambers, can be harder
to detect and may gradually impact system performance without obvious signs.
When leakage surpasses acceptable limits, it is classified as a fault or
failure. In such cases, leakage is divided into three categories: no leakage,
low leakage, and high leakage. It suggests a fault detection algorithm with the
basic responsibility of detecting minimum leakage within the Hydraulic system,
and minimizing detection time is the core idea of this paper. In order to fully
develop this idea, experimental data collection of Hydraulic systems is
required. The collected data uses pressure sensors and other signals that are
single-related. Due to the utilization of Long Short-Term Memory (LSTM)
recurrent neural networks, more complex data analysis was enabled, which the
LSTM-based leakage detection algorithm successfully achieved, providing almost
96% accuracy in classifying leakage types. Results demonstrate that the
proposed method can perform real-time and online fault diagnosis for each
cycle, reducing maintenance costs and prolonging the hydraulic system's
lifespan.

</details>


### [18] [Direction of Arrival Estimation with Virtual Antenna Array Using FMCW Radar Simulated Data](https://arxiv.org/abs/2508.07513)
*Emre Kurtoglu,Mohammad Mahbubur Rahman*

Main category: eess.SP

TL;DR: 本文研究了77GHz FMCW汽车雷达的角度估计问题，比较了FFT、MUSIC和压缩感知算法的性能，发现FFT速度最快但角度分辨率较差。


<details>
  <summary>Details</summary>
Motivation: 随着安全功能需求的增加，需要解决两个近距离目标的方向到达（DOA）估计问题。

Method: 研究了FFT、MUSIC和压缩感知算法在角度估计任务中的表现。

Result: FFT速度最快但角度分辨率较差，而MUSIC和压缩感知是超分辨率算法，性能更优。

Conclusion: MUSIC和压缩感知在角度估计任务中表现优于FFT，适合高分辨率需求的应用。

Abstract: The FMCW radars are widely used for automotive radar systems. The basic idea
for FMCW radars is to generate a linear frequency ramp as transmit signal. The
difference frequency, (i.e., beat frequency) between the transmitted and
received signal is determined after down conversion. The FFT operation on beat
frequency signal can recognize targets at different range and velocity.
Increasing demand on safety functionality leads to the Direction of Arrival
(DOA) estimation to resolve two closely located targets. Consequently, the
problem of angle estimation for 77GHz FMCW automotive radar simulated data has
been investigated in this term project. In particular, we examined the
performances of FFT, MUSIC and compressed sensing in angle estimation task, and
it was found that although FFT is the fastest algorithm, it has very poor
angular resolution when compared with others which are both super resolution
algorithms. The code for this project report is available at
https://github.com/ekurtgl/FMCW-MIMO-Radar-Simulation.

</details>


### [19] [Pinching-Antenna Systems (PASS): A Tutorial](https://arxiv.org/abs/2508.07572)
*Yuanwei Liu,Hao Jiang,Xiaoxia Xu,Zhaolin Wang,Jia Guo,Chongjun Ouyang,Xidong Mu,Zhiguo Ding,Arumugam Nallanathan,George K. Karagiannidis,Robert Schober*

Main category: eess.SP

TL;DR: PASS技术通过大规模天线重构、视线创建和近场优势，将无线通信从最后一公里扩展到最后一米。本文全面介绍了PASS的基础、性能优势、波束成形设计、多用户通信协议、宽带实现、信道状态信息获取及机器学习应用。


<details>
  <summary>Details</summary>
Motivation: PASS技术突破了传统天线技术的限制，提供了更高的灵活性和性能，为下一代无线网络提供了新的可能性。

Method: 讨论了PASS的基础模型、信息论容量限制、波束成形设计、多用户通信协议、宽带实现、信道状态信息获取方法，并探索了基于机器学习的优化方法。

Result: PASS在单用户和多用户通信中表现出优于传统天线技术的性能，并通过机器学习方法进一步优化了复杂度问题。

Conclusion: PASS技术具有广泛的应用前景，尤其在下一代无线网络中，其灵活性和高性能将推动无线通信的进一步发展。

Abstract: Pinching antenna systems (PASS) present a breakthrough among the
flexible-antenna technologies, and distinguish themselves by facilitating
large-scale antenna reconfiguration, line-of-sight creation, scalable
implementation, and near-field benefits, thus bringing wireless communications
from the last mile to the last meter. A comprehensive tutorial is presented in
this paper. First, the fundamentals of PASS are discussed, including PASS
signal models, hardware models, power radiation models, and pinching antenna
activation methods. Building upon this, the information-theoretic capacity
limits achieved by PASS are characterized, and several typical performance
metrics of PASS-based communications are analyzed to demonstrate its
superiority over conventional antenna technologies. Next, the pinching
beamforming design is investigated. The corresponding power scaling law is
first characterized. For the joint transmit and pinching design in the general
multiple-waveguide case, 1) a pair of transmission strategies is proposed for
PASS-based single-user communications to validate the superiority of PASS,
namely sub-connected and fully connected structures; and 2) three practical
protocols are proposed for facilitating PASS-based multi-user communications,
namely waveguide switching, waveguide division, and waveguide multiplexing. A
possible implementation of PASS in wideband communications is further
highlighted. Moreover, the channel state information acquisition in PASS is
elaborated with a pair of promising solutions. To overcome the high complexity
and suboptimality inherent in conventional convex-optimization-based
approaches, machine-learning-based methods for operating PASS are also
explored, focusing on selected deep neural network architectures and training
algorithms. Finally, several promising applications of PASS in next-generation
wireless networks are highlighted.

</details>


### [20] [Remote ID Based UAV Collision Avoidance Optimization for Low-Altitude Airspace Safety](https://arxiv.org/abs/2508.07651)
*Ziye Jia,Yian Zhu,Qihui Wu,Lei Zhang,Sen Yang,Zhu Han*

Main category: eess.SP

TL;DR: 本文提出了一种基于Remote ID的分布式多无人机碰撞避免框架（DMUCA），通过优化通信延迟提升碰撞避免性能。


<details>
  <summary>Details</summary>
Motivation: 随着无人机的快速发展，确保其在开放空域的安全高效运行至关重要。Remote ID作为一种实时监控系统，具有支持无人机间通信的潜力。

Method: 提出DMUCA框架，分析Remote ID消息的平均传输延迟，并设计基于多智能体深度Q网络的自适应通信配置算法。

Result: 数值结果表明，DMUCA框架可行，且提出的机制能将平均延迟降低32%。

Conclusion: Remote ID可用于无人机碰撞避免，且自适应协议配置能显著提升性能。

Abstract: With the rapid development of unmanned aerial vehicles (UAVs), it is
paramount to ensure safe and efficient operations in open airspaces. The remote
identification (Remote ID) is deemed an effective real-time UAV monitoring
system by the federal aviation administration, which holds potentials for
enabling inter-UAV communications. This paper deeply investigates the
application of Remote ID for UAV collision avoidance while minimizing
communication delays. First, we propose a Remote ID based distributed multi-UAV
collision avoidance (DMUCA) framework to support the collision detection,
avoidance decision-making, and trajectory recovery. Next, the average
transmission delays for Remote ID messages are analyzed, incorporating the
packet reception mechanisms and packet loss due to interference. The
optimization problem is formulated to minimize the long-term average
communication delay, where UAVs can flexibly select the Remote ID protocol to
enhance the collision avoidance performance. To tackle the problem, we design a
multi-agent deep Q-network based adaptive communication configuration
algorithm, allowing UAVs to autonomously learn the optimal protocol
configurations in dynamic environments. Finally, numerical results verify the
feasibility of the proposed DMUCA framework, and the proposed mechanism can
reduce the average delay by 32% compared to the fixed protocol configuration.

</details>


### [21] [Importance-Aware Semantic Communication in MIMO-OFDM Systems Using Vision Transformer](https://arxiv.org/abs/2508.07696)
*Joohyuk Park,Yongjeong Oh,Jihun Park,Yo-Seb Jeon*

Main category: eess.SP

TL;DR: 提出了一种基于预训练ViT的IA-QSMPA框架，用于MIMO-OFDM系统中的语义通信，通过联合优化量化、子载波映射和功率分配，显著提升任务性能和通信效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在语义通信中资源分配不合理的问题，利用ViT的注意力机制提取重要性信息，优化通信性能。

Method: 基于预训练ViT提取重要性，联合优化量化、子载波映射和功率分配，采用块坐标下降算法解决非凸优化问题，并扩展到有限块长传输场景。

Result: 在MVP-N数据集上的多视图图像分类任务中，IA-QSMPA在理想和有限块长传输场景下均显著优于传统方法。

Conclusion: IA-QSMPA框架通过重要性感知的资源分配，显著提升了语义通信的任务性能和效率。

Abstract: This paper presents a novel importance-aware quantization, subcarrier
mapping, and power allocation (IA-QSMPA) framework for semantic communication
in multiple-input multiple-output orthogonal frequency division multiplexing
(MIMO-OFDM) systems, empowered by a pretrained Vision Transformer (ViT). The
proposed framework exploits attention-based importance extracted from a
pretrained ViT to jointly optimize quantization levels, subcarrier mapping, and
power allocation. Specifically, IA-QSMPA maps semantically important features
to high-quality subchannels and allocates resources in accordance with their
contribution to task performance and communication latency. To efficiently
solve the resulting nonconvex optimization problem, a block coordinate descent
algorithm is employed. The framework is further extended to operate under
finite blocklength transmission, where communication errors may occur. In this
setting, a segment-wise linear approximation of the channel dispersion penalty
is introduced to enable efficient joint optimization under practical
constraints. Simulation results on a multi-view image classification task using
the MVP-N dataset demonstrate that IA-QSMPA significantly outperforms
conventional methods in both ideal and finite blocklength transmission
scenarios, achieving superior task performance and communication efficiency.

</details>


### [22] [Touch-Augmented Gaussian Splatting for Enhanced 3D Scene Reconstruction](https://arxiv.org/abs/2508.07717)
*Yuchen Gao,Xiao Xu,Eckehard Steinbach,Daniel E. Lucani,Qi Zhang*

Main category: eess.SP

TL;DR: 提出了一种多模态框架，将触觉信号（接触点和表面法线）集成到3D高斯泼溅（3DGS）中，提升场景重建质量，尤其在低光照、有限视角和遮挡等挑战性条件下。


<details>
  <summary>Details</summary>
Motivation: 传统视觉方法在低光照、有限视角和遮挡等条件下表现不佳，因此需要引入触觉信号以增强重建效果。

Method: 采用两阶段采样方案：先探测稀疏区域，再聚焦于高不确定边界；提出几何损失以确保表面平滑。

Result: 在多种场景下几何精度显著提升，严重遮挡情况下Chamfer Distance降低超过15倍。

Conclusion: 触觉信号的集成显著提升了3D高斯泼溅的重建效果，且完全在线流程适用于视觉退化环境。

Abstract: This paper presents a multimodal framework that integrates touch signals
(contact points and surface normals) into 3D Gaussian Splatting (3DGS). Our
approach enhances scene reconstruction, particularly under challenging
conditions like low lighting, limited camera viewpoints, and occlusions.
Different from the visual-only method, the proposed approach incorporates
spatially selective touch measurements to refine both the geometry and
appearance of the 3D Gaussian representation. To guide the touch exploration,
we introduce a two-stage sampling scheme that initially probes sparse regions
and then concentrates on high-uncertainty boundaries identified from the
reconstructed mesh. A geometric loss is proposed to ensure surface smoothness,
resulting in improved geometry. Experimental results across diverse scenarios
show consistent improvements in geometric accuracy. In the most challenging
case with severe occlusion, the Chamfer Distance is reduced by over 15x,
demonstrating the effectiveness of integrating touch cues into 3D Gaussian
Splatting. Furthermore, our approach maintains a fully online pipeline,
underscoring its feasibility in visually degraded environments.

</details>


### [23] [RIS-Assisted NOMA with Partial CSI and Mutual Coupling: A Machine Learning Approach](https://arxiv.org/abs/2508.07909)
*Bile Peng,Karl-Ludwig Besser,Shanpu Shen,Finn Siegismund-Poschmann,Ramprasad Raghunath,Daniel M. Mittleman,Vahid Jamali,Eduard A. Jorswieck*

Main category: eess.SP

TL;DR: 论文提出了一种基于无监督机器学习的联合优化方法，用于基站预编码和RIS配置，以提高NOMA性能。


<details>
  <summary>Details</summary>
Motivation: NOMA的性能受无线信道特性影响，而RIS可以增强信道特性，因此需要一种高效的方法来联合优化BS预编码和RIS配置。

Method: 提出了一种名为RISnet的专用神经网络架构，结合了通信领域的专业知识，实现自主寻找最优解。

Result: 该方法具有高扩展性（可控制超过1000个RIS元素）、低CSI输入需求，并能解决RIS元素间的互耦问题。

Conclusion: 该研究是领域知识驱动ML的早期贡献，展示了结合通信专业知识设计优于通用ML方法的潜力。

Abstract: Non-orthogonal multiple access (NOMA) is a promising multiple access
technique. Its performance depends strongly on the wireless channel property,
which can be enhanced by reconfigurable intelligent surfaces (RISs). In this
paper, we jointly optimize base station (BS) precoding and RIS configuration
with unsupervised machine learning (ML), which looks for the optimal solution
autonomously. In particular, we propose a dedicated neural network (NN)
architecture RISnet inspired by domain knowledge in communication. Compared to
state-of-the-art, the proposed approach combines analytical optimal BS
precoding and ML-enabled RIS, has a high scalability to control more than 1000
RIS elements, has a low requirement for channel state information (CSI) in
input, and addresses the mutual coupling between RIS elements. Beyond the
considered problem, this work is an early contribution to domain knowledge
enabled ML, which exploit the domain expertise of communication systems to
design better approaches than general ML methods.

</details>


### [24] [Advancing the Control of Low-Altitude Wireless Networks: Architecture, Design Principles, and Future Directions](https://arxiv.org/abs/2508.07967)
*Haijia Jin,Weijie Yuan,Jun Wu,Jiacheng Wang,Dusit Niyato,Xianbin Wang,George K. Karagiannidis,Zhiyun Lin,Yi Gong,Dong In Kim,Athina Petropulu,Maria Sabrina Greco,Abbas Jamalipour,Sumei Sun*

Main category: eess.SP

TL;DR: 本文介绍了一种面向控制的低空无线网络（LAWN），集成了近地通信和内部系统状态的远程估计，以支持动态空-地环境中的可靠网络控制。


<details>
  <summary>Details</summary>
Motivation: 在动态空-地环境中实现可靠的网络控制，需要整合通信和状态估计技术。

Method: 提出了模块化架构和关键性能指标，讨论了控制、通信和估计层的核心设计权衡，并通过案例研究展示了无线约束下的闭环协调。

Result: 展示了LAWN在动态环境中的可行性和潜力。

Conclusion: 未来研究方向包括在实时和资源受限场景下实现可扩展、弹性的LAWN部署。

Abstract: This article introduces a control-oriented low-altitude wireless network
(LAWN) that integrates near-ground communications and remote estimation of the
internal system state. This integration supports reliable networked control in
dynamic aerial-ground environments. First, we introduce the network's modular
architecture and key performance metrics. Then, we discuss core design
trade-offs across the control, communication, and estimation layers. A case
study illustrates closed-loop coordination under wireless constraints. Finally,
we outline future directions for scalable, resilient LAWN deployments in
real-time and resource-constrained scenarios.

</details>


### [25] [Robust Design of Beyond-Diagonal Reconfigurable Intelligent Surface Empowered RSMA-SWIPT System Under Channel Estimation Errors](https://arxiv.org/abs/2508.08097)
*Muhammad Asif,Zain Ali,Asim Ihsan,Ali Ranjha,Zhu Shoujin,Manzoor Ahmed,Xingwang Li,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 论文提出了一种结合RSMA、SWIPT和BD-RIS的优化框架，用于提升6G网络的性能，并通过交替优化方法解决多变量问题。


<details>
  <summary>Details</summary>
Motivation: 提升6G网络的频谱效率、能量效率、覆盖范围和连接性，同时考虑实际硬件限制和信道不确定性。

Method: 采用交替优化框架，将问题分解为多个块，分别优化预编码向量、功率分配比和BD-RIS散射矩阵，并结合SCA和流形优化策略。

Result: 数值模拟显示，所提方案在性能和收敛速度上显著优于现有基准。

Conclusion: 该方案为6G网络提供了一种高效的优化方法，具有实际应用潜力。

Abstract: This work explores the integration of rate-splitting multiple access (RSMA),
simultaneous wireless information and power transfer (SWIPT), and
beyond-diagonal reconfigurable intelligent surface (BD-RIS) to enhance the
spectral-efficiency, energy-efficiency, coverage, and connectivity of future
sixth-generation (6G) communication networks. Specifically, with a multiuser
BD-RIS-empowered RSMA-SWIPT system, we jointly optimize the transmit precoding
vectors, the common rate proportion of users, the power-splitting ratios, and
scattering matrix of BD-RIS node, under the assumption of imperfect channel
state information (CSI). Additionally, to better capture practical hardware
behavior, we incorporate a nonlinear energy harvesting model under energy
harvesting constraints. We design a robust optimization framework to maximize
the system sum-rate, while explicitly accounting for the worst-case impact of
CSI uncertainties. Further, we introduce an alternating optimization framework
that partitions the problem into several blocks, which are optimized
iteratively. More specifically, the transmit precoding vectors are optimized by
reformulating the problem as a convex semidefinite programming through
successive-convex approximation (SCA), whereas the power-splitting problem is
solved using the MOSEK-enabled CVX toolbox. Subsequently, to optimize the
scattering matrix of the BD-RIS, we first employ SCA to reformulate the problem
into a convex form, and then design a manifold optimization strategy based on
the Conjugate-Gradient method. Finally, numerical simulation results reveal
that the proposed scheme provides significant performance improvements over
existing benchmarks and demonstrates rapid convergence within a reasonable
number of iterations.

</details>


### [26] [Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers](https://arxiv.org/abs/2508.08206)
*Amirhossein Taherpour,Abbas Taherpour,Tamer Khattab*

Main category: eess.SP

TL;DR: 提出了一种联合学习框架，用于在信道状态信息（CSI）不确定的情况下实现拜占庭弹性的频谱感知和安全智能反射面（IRS）辅助的机会接入。


<details>
  <summary>Details</summary>
Motivation: 解决在存在拜占庭用户和CSI不确定性的情况下，如何实现高效的频谱感知和安全通信的问题。

Method: 采用对数域贝叶斯更新与修剪聚合和注意力加权共识的感知阶段，基站通过保守最小规则融合网络信念。下行链路设计基于和均方误差（MSE）最小化，优化基站预编码器、IRS相位偏移和用户均衡器。针对部分或未知CSI，分别开发了增强拉格朗日交替算法和约束贝叶斯优化方法。

Result: 在多种网络条件下，仿真显示在固定虚警率下更高的检测概率，诚实用户的和MSE显著降低，窃听者信号功率被有效抑制，且算法快速收敛。

Conclusion: 该框架为安全机会通信提供了实用路径，能够适应CSI可用性，并通过联合学习协调感知和传输。

Abstract: We propose a joint learning framework for Byzantine-resilient spectrum
sensing and secure intelligent reflecting surface (IRS)--assisted opportunistic
access under channel state information (CSI) uncertainty. The sensing stage
performs logit-domain Bayesian updates with trimmed aggregation and
attention-weighted consensus, and the base station (BS) fuses network beliefs
with a conservative minimum rule, preserving detection accuracy under a bounded
number of Byzantine users. Conditioned on the sensing outcome, we pose downlink
design as sum mean-squared error (MSE) minimization under transmit-power and
signal-leakage constraints and jointly optimize the BS precoder, IRS phase
shifts, and user equalizers. With partial (or known) CSI, we develop an
augmented-Lagrangian alternating algorithm with projected updates and provide
provable sublinear convergence, with accelerated rates under mild local
curvature. With unknown CSI, we perform constrained Bayesian optimization (BO)
in a geometry-aware low-dimensional latent space using Gaussian process (GP)
surrogates; we prove regret bounds for a constrained upper confidence bound
(UCB) variant of the BO module, and demonstrate strong empirical performance of
the implemented procedure. Simulations across diverse network conditions show
higher detection probability at fixed false-alarm rate under adversarial
attacks, large reductions in sum MSE for honest users, strong suppression of
eavesdropper signal power, and fast convergence. The framework offers a
practical path to secure opportunistic communication that adapts to CSI
availability while coherently coordinating sensing and transmission through
joint learning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [27] [Discovery Learning accelerates battery design evaluation](https://arxiv.org/abs/2508.06985)
*Jiawei Zhang,Yifei Zhang,Baozhao Yi,Yao Ren,Qi Jiao,Hanyu Bai,Weiran Jiang,Ziyou Song*

Main category: cs.LG

TL;DR: 论文提出了一种名为Discovery Learning（DL）的科学机器学习范式，通过结合主动学习、物理引导学习和零样本学习，显著减少电池设计验证的时间和能源成本。


<details>
  <summary>Details</summary>
Motivation: 电池研发因高昂的时间和能源成本而受限，现有方法依赖目标设计的标记数据且效率不足，无法满足快速反馈需求。

Method: DL整合主动学习、物理引导学习和零样本学习，利用历史电池设计数据，减少原型测试需求。

Result: 在123个工业级锂离子电池上测试，DL仅用公开数据集训练，预测误差为7.2%，节省98%时间和95%能源。

Conclusion: DL展示了从历史设计中提取信息以加速下一代电池技术开发的潜力，是数据驱动建模的重要进展。

Abstract: Fast and reliable validation of novel designs in complex physical systems
such as batteries is critical to accelerating technological innovation.
However, battery research and development remain bottlenecked by the
prohibitively high time and energy costs required to evaluate numerous new
design candidates, particularly in battery prototyping and life testing.
Despite recent progress in data-driven battery lifetime prediction, existing
methods require labeled data of target designs to improve accuracy and cannot
make reliable predictions until after prototyping, thus falling far short of
the efficiency needed to enable rapid feedback for battery design. Here, we
introduce Discovery Learning (DL), a scientific machine-learning paradigm that
integrates active learning, physics-guided learning, and zero-shot learning
into a human-like reasoning loop, drawing inspiration from learning theories in
educational psychology. DL can learn from historical battery designs and
actively reduce the need for prototyping, thus enabling rapid lifetime
evaluation for unobserved material-design combinations without requiring
additional data labeling. To test DL, we present 123 industrial-grade
large-format lithium-ion pouch cells, spanning eight material-design
combinations and diverse cycling protocols. Trained solely on public datasets
of small-capacity cylindrical cells, DL achieves 7.2% test error in predicting
the average cycle life under unknown device variability. This results in
savings of 98% in time and 95% in energy compared to industrial practices. This
work highlights the potential of uncovering insights from historical designs to
inform and accelerate the development of next-generation battery technologies.
DL represents a key advance toward efficient data-driven modeling and helps
realize the promise of machine learning for accelerating scientific discovery
and engineering innovation.

</details>


### [28] [From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving](https://arxiv.org/abs/2508.07029)
*Antonio Guillen-Perez*

Main category: cs.LG

TL;DR: 论文提出了一种结合行为克隆（BC）和离线强化学习（CQL）的方法，显著提升了自动驾驶策略的鲁棒性和成功率。


<details>
  <summary>Details</summary>
Motivation: 解决行为克隆策略在闭环执行中易出错的问题，探索从静态专家数据中学习鲁棒驾驶策略的方法。

Method: 开发了基于Transformer的行为克隆基线模型，并应用离线强化学习算法CQL，结合精心设计的奖励函数。

Result: 在Waymo数据集上，CQL策略的成功率比最强BC基线高3.2倍，碰撞率低7.4倍。

Conclusion: 离线强化学习是从静态数据中学习鲁棒驾驶策略的关键方法。

Abstract: Learning robust driving policies from large-scale, real-world datasets is a
central challenge in autonomous driving, as online data collection is often
unsafe and impractical. While Behavioral Cloning (BC) offers a straightforward
approach to imitation learning, policies trained with BC are notoriously
brittle and suffer from compounding errors in closed-loop execution. This work
presents a comprehensive pipeline and a comparative study to address this
limitation. We first develop a series of increasingly sophisticated BC
baselines, culminating in a Transformer-based model that operates on a
structured, entity-centric state representation. While this model achieves low
imitation loss, we show that it still fails in long-horizon simulations. We
then demonstrate that by applying a state-of-the-art Offline Reinforcement
Learning algorithm, Conservative Q-Learning (CQL), to the same data and
architecture, we can learn a significantly more robust policy. Using a
carefully engineered reward function, the CQL agent learns a conservative value
function that enables it to recover from minor errors and avoid
out-of-distribution states. In a large-scale evaluation on 1,000 unseen
scenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a
3.2x higher success rate and a 7.4x lower collision rate than the strongest BC
baseline, proving that an offline RL approach is critical for learning robust,
long-horizon driving policies from static expert data.

</details>


### [29] [Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems](https://arxiv.org/abs/2508.06539)
*Atahan Karagoz*

Main category: cs.LG

TL;DR: 论文提出了一种新的生存模型理论，将生存视为生物状态空间中几何特性的结果，而非传统的有监督学习任务。


<details>
  <summary>Details</summary>
Motivation: 传统生存模型依赖标注数据和固定协变量，而该研究认为生存是生物状态空间中几何流动和曲率的自然结果。

Method: 提出了自组织生存流形（SOSM）理论，基于低曲率测地流和生物约束构建生存能量泛函。

Result: 理论证明了在生物合理条件下，生存对齐轨迹的出现和收敛，并将健康、疾病、衰老和死亡视为流形结构的几何相变。

Conclusion: 该理论为生存建模提供了一个无需标注的通用框架，连接了机器学习、生物物理和生命几何学。

Abstract: Survival is traditionally modeled as a supervised learning task, reliant on
curated outcome labels and fixed covariates. This work rejects that premise. It
proposes that survival is not an externally annotated target but a geometric
consequence: an emergent property of the curvature and flow inherent in
biological state space. We develop a theory of Self-Organizing Survival
Manifolds (SOSM), in which survival-relevant dynamics arise from low-curvature
geodesic flows on latent manifolds shaped by internal biological constraints. A
survival energy functional based on geodesic curvature minimization is
introduced and shown to induce structures where prognosis aligns with geometric
flow stability. We derive discrete and continuous formulations of the objective
and prove theoretical results demonstrating the emergence and convergence of
survival-aligned trajectories under biologically plausible conditions. The
framework draws connections to thermodynamic efficiency, entropy flow, Ricci
curvature, and optimal transport, grounding survival modeling in physical law.
Health, disease, aging, and death are reframed as geometric phase transitions
in the manifold's structure. This theory offers a universal, label-free
foundation for modeling survival as a property of form, not annotation-bridging
machine learning, biophysics, and the geometry of life itself.

</details>


### [30] [Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering](https://arxiv.org/abs/2508.06574)
*Fatemeh Moradi,Mehran Tarif,Mohammadhossein Homaei*

Main category: cs.LG

TL;DR: 提出了一种两阶段学习框架，结合无监督异常检测和半监督学习，用于供应链欺诈检测，在真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现代供应链欺诈检测面临复杂性和标记数据稀缺的挑战，传统方法因类别不平衡和有限监督而效果不佳。

Method: 第一阶段使用Isolation Forest进行无监督异常检测，第二阶段通过自训练SVM结合标记和高置信度伪标记样本进行半监督学习。

Result: 在DataCo数据集上F1得分为0.817，假阳性率低于3.0%。

Conclusion: 该方法在真实约束下有效，但需注意概念漂移问题，并与深度学习方法进一步比较。

Abstract: Detecting fraud in modern supply chains is a growing challenge, driven by the
complexity of global networks and the scarcity of labeled data. Traditional
detection methods often struggle with class imbalance and limited supervision,
reducing their effectiveness in real-world applications. This paper proposes a
novel two-phase learning framework to address these challenges. In the first
phase, the Isolation Forest algorithm performs unsupervised anomaly detection
to identify potential fraud cases and reduce the volume of data requiring
further analysis. In the second phase, a self-training Support Vector Machine
(SVM) refines the predictions using both labeled and high-confidence
pseudo-labeled samples, enabling robust semi-supervised learning. The proposed
method is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive
real-world supply chain dataset with fraud indicators. It achieves an F1-score
of 0.817 while maintaining a false positive rate below 3.0%. These results
demonstrate the effectiveness and efficiency of combining unsupervised
pre-filtering with semi-supervised refinement for supply chain fraud detection
under real-world constraints, though we acknowledge limitations regarding
concept drift and the need for comparison with deep learning approaches.

</details>


### [31] [Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow](https://arxiv.org/abs/2508.07841)
*Carlo Cena,Mauro Martini,Marcello Chiaberge*

Main category: cs.LG

TL;DR: 论文研究了将物理信息神经网络（PINNs）引入航天器姿态动力学学习的优势，相比纯数据驱动方法，PINNs显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在物理模型不完整或计算成本高的情况下，纯数据驱动方法泛化性和稳定性不足，因此探索PINNs的潜力。

Method: 使用Real NVP神经网络架构和自注意力机制，基于Basilisk模拟器生成的数据，比较纯数据驱动和物理信息变体的训练策略。

Result: 物理信息模型的平均相对误差降低了27.08%，在MPC框架中控制精度和鲁棒性提升高达42.86%。

Conclusion: PINNs显著提升了航天器姿态控制的性能，尤其在MPC框架中表现优于纯数据驱动方法。

Abstract: Attitude control is a fundamental aspect of spacecraft operations. Model
Predictive Control (MPC) has emerged as a powerful strategy for these tasks,
relying on accurate models of the system dynamics to optimize control actions
over a prediction horizon. In scenarios where physics models are incomplete,
difficult to derive, or computationally expensive, machine learning offers a
flexible alternative by learning the system behavior directly from data.
However, purely data-driven models often struggle with generalization and
stability, especially when applied to inputs outside their training domain. To
address these limitations, we investigate the benefits of incorporating
Physics-Informed Neural Networks (PINNs) into the learning of spacecraft
attitude dynamics, comparing their performance with that of purely data-driven
approaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network
architecture with a self-attention mechanism, we trained several models on
simulated data generated with the Basilisk simulator. Two training strategies
were considered: a purely data-driven baseline and a physics-informed variant
to improve robustness and stability. Our results demonstrate that the inclusion
of physics-based information significantly enhances the performance in terms of
the mean relative error of the best architectures found by 27.08%. These
advantages are particularly evident when the learned models are integrated into
an MPC framework, where PINN-based models consistently outperform their purely
data-driven counterparts in terms of control accuracy and robustness, yielding
improvements of up to 42.86% in performance stability error and increased
robustness-to-noise.

</details>


### [32] [GFlowNets for Learning Better Drug-Drug Interaction Representations](https://arxiv.org/abs/2508.06576)
*Azmine Toushik Wasi*

Main category: cs.LG

TL;DR: 提出了一种结合生成流网络（GFlowNet）和变分图自编码器（VGAE）的框架，用于生成罕见药物相互作用的合成样本，以解决数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用（DDI）预测中，罕见但关键的相互作用在数据集中代表性不足，导致模型性能不佳。现有方法通常将DDI预测视为二分类问题，忽视了类别特异性，加剧了对常见相互作用的偏向。

Method: 结合GFlowNet和VGAE生成罕见类别的合成样本，以平衡数据集并生成新颖有效的DDI对。

Result: 该方法提高了对不同类型药物相互作用的预测性能，增强了临床可靠性。

Conclusion: 提出的框架有效解决了DDI预测中的数据不平衡问题，提升了模型在罕见相互作用上的表现。

Abstract: Drug-drug interactions pose a significant challenge in clinical pharmacology,
with severe class imbalance among interaction types limiting the effectiveness
of predictive models. Common interactions dominate datasets, while rare but
critical interactions remain underrepresented, leading to poor model
performance on infrequent cases. Existing methods often treat DDI prediction as
a binary problem, ignoring class-specific nuances and exacerbating bias toward
frequent interactions. To address this, we propose a framework combining
Generative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)
to generate synthetic samples for rare classes, improving model balance and
generate effective and novel DDI pairs. Our approach enhances predictive
performance across interaction types, ensuring better clinical reliability.

</details>


### [33] [MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation](https://arxiv.org/abs/2508.08137)
*Pravallika Abbineni,Saoud Aldowaish,Colin Liechty,Soroosh Noorzad,Ali Ghazizadeh,Morteza Fayazi*

Main category: cs.LG

TL;DR: MuaLLM是一种开源多模态大型语言模型（LLM）代理，用于电路设计辅助，结合混合检索增强生成（RAG）框架和自适应电路设计研究论文向量数据库，显著提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 电路设计文献综述面临快速更新的研究、数据表示不一致和优化目标复杂等挑战，需要高效工具辅助。

Method: MuaLLM采用Reason + Act（ReAct）工作流，支持多模态数据处理，动态检索和实时数据库更新，分离检索与推理以扩展处理能力。

Result: MuaLLM在RAG-250和Reas-100数据集上分别达到90.1%召回率和86.8%准确率，成本降低10倍，速度提升1.6倍。

Conclusion: MuaLLM为电路设计提供高效、低成本的自动化辅助工具，克服传统方法的局限性。

Abstract: Conducting a comprehensive literature review is crucial for advancing circuit
design methodologies. However, the rapid influx of state-of-the-art research,
inconsistent data representation, and the complexity of optimizing circuit
design objectives make this task significantly challenging. In this paper, we
propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for
circuit design assistance that integrates a hybrid Retrieval-Augmented
Generation (RAG) framework with an adaptive vector database of circuit design
research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +
Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step
information retrieval. It functions as a question-answering design assistant,
capable of interpreting complex queries and providing reasoned responses
grounded in circuit literature. Its multimodal capabilities enable processing
of both textual and visual data, facilitating more efficient and comprehensive
analysis. The system dynamically adapts using intelligent search tools,
automated document retrieval from the internet, and real-time database updates.
Unlike conventional approaches constrained by model context limits, MuaLLM
decouples retrieval from inference, enabling scalable reasoning over
arbitrarily large corpora. At the maximum context length supported by standard
LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining
the same accuracy. This allows rapid, no-human-in-the-loop database generation,
overcoming the bottleneck of simulation-based dataset creation for circuits. To
evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval
and citation performance, and Reasoning-100 (Reas-100), focused on multistep
reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%
accuracy on Reas-100.

</details>


### [34] [Hypergraph Neural Network with State Space Models for Node Classification](https://arxiv.org/abs/2508.06587)
*A. Quadir,M. Tanveer*

Main category: cs.LG

TL;DR: 提出了一种新型超图神经网络HGMN，结合角色感知表示和状态空间模型，显著提升了节点分类任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统GNN主要关注节点间的邻接关系，忽略了角色特征对节点表示的重要性，现有方法多为无监督且性能不足。

Method: HGMN通过超图构建技术建模高阶关系，结合角色和邻接表示，使用可学习的mamba transformer机制，并引入残差网络防止过平滑。

Result: 在多个数据集上的实验显示，HGMN在节点分类任务中优于现有GNN方法。

Conclusion: HGMN通过有效融合角色特征和邻接信息，提供了更丰富的节点表示，适用于多种图学习任务。

Abstract: In recent years, graph neural networks (GNNs) have gained significant
attention for node classification tasks on graph-structured data. However,
traditional GNNs primarily focus on adjacency relationships between nodes,
often overlooking the rich role-based characteristics that are crucial for
learning more expressive node representations. Existing methods for capturing
role-based features are largely unsupervised and fail to achieve optimal
performance in downstream tasks. To address these limitations, we propose a
novel hypergraph neural network with state space model (HGMN) that effectively
integrates role-aware representations into GNNs and the state space model. HGMN
utilizes hypergraph construction techniques to model higher-order relationships
and combines role-based and adjacency-based representations through a learnable
mamba transformer mechanism. By leveraging two distinct hypergraph construction
methods-based on node degree and neighborhood levels, it strengthens the
connections among nodes with similar roles, enhancing the model's
representational power. Additionally, the inclusion of hypergraph convolution
layers enables the model to capture complex dependencies within hypergraph
structures. To mitigate the over-smoothing problem inherent in deep GNNs, we
incorporate a residual network, ensuring improved stability and better feature
propagation across layers. Extensive experiments conducted on one newly
introduced dataset and four benchmark datasets demonstrate the superiority of
HGMN. The model achieves significant performance improvements on node
classification tasks compared to state-of-the-art GNN methods. These results
highlight HGMN's ability to provide enriched node representations by
effectively embedding role-based features alongside adjacency information,
making it a versatile and powerful tool for a variety of graph-based learning
applications.

</details>


### [35] [Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning](https://arxiv.org/abs/2508.06588)
*Zian Zhai,Fan Li,Xingyu Tan,Xiaoyang Wang,Wenjie Zhang*

Main category: cs.LG

TL;DR: 论文提出了RGVQ框架，通过引入图拓扑和特征相似性作为正则化信号，解决图数据中VQ的码本崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 图数据的向量量化（VQ）中码本崩溃问题未被充分研究，限制了图令牌的表达能力和泛化性。

Method: 提出RGVQ框架，结合Gumbel-Softmax重参数化和结构感知对比正则化，增强码本利用率和令牌多样性。

Result: 实验表明RGVQ显著提升码本利用率，并在多个下游任务中提升性能。

Conclusion: RGVQ为图数据VQ提供了更表达和可迁移的令牌表示。

Abstract: Vector Quantization (VQ) has recently emerged as a promising approach for
learning discrete representations of graph-structured data. However, a
fundamental challenge, i.e., codebook collapse, remains underexplored in the
graph domain, significantly limiting the expressiveness and generalization of
graph tokens.In this paper, we present the first empirical study showing that
codebook collapse consistently occurs when applying VQ to graph data, even with
mitigation strategies proposed in vision or language domains. To understand why
graph VQ is particularly vulnerable to collapse, we provide a theoretical
analysis and identify two key factors: early assignment imbalances caused by
redundancy in graph features and structural patterns, and self-reinforcing
optimization loops in deterministic VQ. To address these issues, we propose
RGVQ, a novel framework that integrates graph topology and feature similarity
as explicit regularization signals to enhance codebook utilization and promote
token diversity. RGVQ introduces soft assignments via Gumbel-Softmax
reparameterization, ensuring that all codewords receive gradient updates. In
addition, RGVQ incorporates a structure-aware contrastive regularization to
penalize the token co-assignments among similar node pairs. Extensive
experiments demonstrate that RGVQ substantially improves codebook utilization
and consistently boosts the performance of state-of-the-art graph VQ backbones
across multiple downstream tasks, enabling more expressive and transferable
graph token representations.

</details>


### [36] [Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels](https://arxiv.org/abs/2508.06622)
*Jeremiah Birrell,Reza Ebrahimi*

Main category: cs.LG

TL;DR: ANTIDOTE是一种针对噪声标签学习的新目标函数，通过信息散度邻域的松弛定义，采用对抗训练方法，计算成本接近标准交叉熵损失。


<details>
  <summary>Details</summary>
Motivation: 解决训练数据中标签噪声或对抗性标签篡改的问题。

Method: 利用凸对偶性将目标函数转化为对抗训练方法，自适应减少噪声标签样本的影响。

Result: 在不同类型的标签噪声（对称、非对称、人工标注和真实噪声）下表现优异，计算效率接近标准交叉熵损失。

Conclusion: ANTIDOTE在噪声标签学习中具有高效性和鲁棒性，优于现有方法。

Abstract: We introduce ANTIDOTE, a new class of objectives for learning under noisy
labels which are defined in terms of a relaxation over an
information-divergence neighborhood. Using convex duality, we provide a
reformulation as an adversarial training method that has similar computational
cost to training with standard cross-entropy loss. We show that our approach
adaptively reduces the influence of the samples with noisy labels during
learning, exhibiting a behavior that is analogous to forgetting those samples.
ANTIDOTE is effective in practical environments where label noise is inherent
in the training data or where an adversary can alter the training labels.
Extensive empirical evaluations on different levels of symmetric, asymmetric,
human annotation, and real-world label noise show that ANTIDOTE outperforms
leading comparable losses in the field and enjoys a time complexity that is
very close to that of the standard cross entropy loss.

</details>


### [37] [A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis](https://arxiv.org/abs/2508.06589)
*Xinglin Zhao,Yanwen Wang,Xiaobo Liu,Yanrong Hao,Rui Cao,Xin Wen*

Main category: cs.LG

TL;DR: 提出了一种针对神经影像CAD系统的联邦学习框架，通过动态导航和元整合模块解决小样本和大规模数据中的异质性问题，显著提高了诊断准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决神经影像CAD系统中因小样本研究可重复性低和大规模数据中疾病亚型混杂导致的异质性问题。

Method: 提出包含动态导航模块（根据潜在亚型表示将样本路由到最合适的本地模型）和元整合模块（将异质本地模型的预测整合为统一诊断输出）的联邦学习框架。

Result: 在包含1300多名MDD患者和1100名健康对照的fMRI数据集上，平均准确率达到74.06%，显著优于传统方法。

Conclusion: 该框架通过处理数据异质性和亚型混杂，提升了神经影像CAD系统的可靠性和可重复性，对个性化医疗和临床决策具有重要潜力。

Abstract: Computer-aided diagnosis (CAD) systems play a crucial role in analyzing
neuroimaging data for neurological and psychiatric disorders. However,
small-sample studies suffer from low reproducibility, while large-scale
datasets introduce confounding heterogeneity due to multiple disease subtypes
being labeled under a single category. To address these challenges, we propose
a novel federated learning framework tailored for neuroimaging CAD systems. Our
approach includes a dynamic navigation module that routes samples to the most
suitable local models based on latent subtype representations, and a
meta-integration module that combines predictions from heterogeneous local
models into a unified diagnostic output. We evaluated our framework using a
comprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100
healthy controls across multiple study cohorts. Experimental results
demonstrate significant improvements in diagnostic accuracy and robustness
compared to traditional methods. Specifically, our framework achieved an
average accuracy of 74.06\% across all tested sites, showcasing its
effectiveness in handling subtype heterogeneity and enhancing model
generalizability. Ablation studies further confirmed the importance of both the
dynamic navigation and meta-integration modules in improving performance. By
addressing data heterogeneity and subtype confounding, our framework advances
reliable and reproducible neuroimaging CAD systems, offering significant
potential for personalized medicine and clinical decision-making in neurology
and psychiatry.

</details>


### [38] [Using Imperfect Synthetic Data in Downstream Inference Tasks](https://arxiv.org/abs/2508.06635)
*Yewon Byun,Shantanu Gupta,Zachary C. Lipton,Rachel Leah Childers,Bryan Wilder*

Main category: cs.LG

TL;DR: 提出了一种基于广义矩估计的新方法，用于结合合成数据与真实数据，以在计算社会科学中实现统计有效的结论。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用大语言模型生成的合成数据与真实数据结合，以解决有限数据条件下的统计问题。

Method: 引入基于广义矩估计的新估计器，无需超参数调整，具有强理论保证。

Result: 发现合成数据与真实数据的矩残差交互可以改善目标参数估计，实证验证了其在小样本中的优越性能。

Conclusion: 新方法在计算社会科学应用中表现出显著的实证优势，为合成数据的使用提供了统计有效性保障。

Abstract: Predictions and generations from large language models are increasingly being
explored as an aid to computational social science and human subject research
in limited data regimes. While previous technical work has explored the
potential to use model-predicted labels for unlabeled data in a principled
manner, there is increasing interest in using large language models to generate
entirely new synthetic samples (also termed as synthetic simulations), such as
in responses to surveys. However, it is not immediately clear by what means
practitioners can combine such data with real data and yet produce
statistically valid conclusions upon them. In this work, we introduce a new
estimator based on generalized method of moments, providing a
hyperparameter-free solution with strong theoretical guarantees to address the
challenge at hand. Surprisingly, we find that interactions between the moment
residuals of synthetic data and those of real data can improve estimates of the
target parameter. We empirically validate the finite-sample performance of our
estimator across different regression tasks in computational social science
applications, demonstrating large empirical gains.

</details>


### [39] [Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials](https://arxiv.org/abs/2508.06591)
*Rachel K. Luu,Jingyu Deng,Mohammed Shahrudin Ibrahim,Nam-Joon Cho,Ming Dao,Subra Suresh,Markus J. Buehler*

Main category: cs.LG

TL;DR: 论文提出了一种结合生成式AI和多学科文献的框架，用于设计新型仿生材料，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在多学科实验科学（如材料科学）中的应用，填补其在特定领域的研究空白。

Method: 结合BioinspiredLLM、检索增强生成（RAG）、代理系统和分层采样策略，从植物科学、仿生学和材料工程中提取结构-性能关系。

Result: 成功设计并实验验证了一种新型花粉基粘合剂，具有可调形态和实测剪切强度。

Conclusion: AI辅助创意能够推动现实世界的材料设计，并实现有效的人机协作。

Abstract: Large language models (LLMs) have reshaped the research landscape by enabling
new approaches to knowledge retrieval and creative ideation. Yet their
application in discipline-specific experimental science, particularly in highly
multi-disciplinary domains like materials science, remains limited. We present
a first-of-its-kind framework that integrates generative AI with literature
from hitherto-unconnected fields such as plant science, biomimetics, and
materials engineering to extract insights and design experiments for materials.
We focus on humidity-responsive systems such as pollen-based materials and
Rhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and
adaptive performance. Using a suite of AI tools, including a fine-tuned model
(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a
Hierarchical Sampling strategy, we extract structure-property relationships and
translate them into new classes of bioinspired materials. Structured inference
protocols generate and evaluate hundreds of hypotheses from a single query,
surfacing novel and experimentally tractable ideas. We validate our approach
through real-world implementation: LLM-generated procedures, materials designs,
and mechanical predictions were tested in the laboratory, culminating in the
fabrication of a novel pollen-based adhesive with tunable morphology and
measured shear strength, establishing a foundation for future plant-derived
adhesive design. This work demonstrates how AI-assisted ideation can drive
real-world materials design and enable effective human-AI collaboration.

</details>


### [40] [Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift](https://arxiv.org/abs/2508.06776)
*Amit Pandey*

Main category: cs.LG

TL;DR: Zero-Direction Probing (ZDP) 是一种无需任务标签或输出评估的理论框架，通过检测 Transformer 激活的零方向来发现模型漂移。


<details>
  <summary>Details</summary>
Motivation: 研究如何在无需任务标签或输出评估的情况下，通过理论方法检测模型漂移。

Method: 提出 ZDP 框架，基于假设 A1-A6，证明了一系列理论结果（如方差泄漏定理、Fisher 零守恒等），并推导了 Spectral Null-Leakage (SNL) 度量。

Result: 证明了监控层激活的左右零空间及其 Fisher 几何可以提供关于表示变化的具体、可测试的保证。

Conclusion: ZDP 框架为模型漂移检测提供了理论支持，并通过 SNL 度量实现了可量化的监控。

Abstract: We present Zero-Direction Probing (ZDP), a theory-only framework for
detecting model drift from null directions of transformer activations without
task labels or output evaluations. Under assumptions A1--A6, we prove: (i) the
Variance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound
for low-rank updates, and (iv) a logarithmic-regret guarantee for online
null-space trackers. We derive a Spectral Null-Leakage (SNL) metric with
non-asymptotic tail bounds and a concentration inequality, yielding a-priori
thresholds for drift under a Gaussian null model. These results show that
monitoring right/left null spaces of layer activations and their Fisher
geometry provides concrete, testable guarantees on representational change.

</details>


### [41] [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/abs/2508.06601)
*Kyle O'Brien,Stephen Casper,Quentin Anthony,Tomek Korbak,Robert Kirk,Xander Davies,Ishan Mishra,Geoffrey Irving,Yarin Gal,Stella Biderman*

Main category: cs.LG

TL;DR: 研究探讨通过过滤训练数据中的双用途主题文本，防止开放权重AI系统被篡改攻击，提出一种多阶段数据过滤方法，显著提升模型对抗攻击的能力。


<details>
  <summary>Details</summary>
Motivation: 开放权重AI系统虽具透明性和开放性，但易受篡改攻击，现有安全微调方法效果有限，需探索更有效的防护手段。

Method: 引入多阶段数据过滤管道，预训练多个6.9B参数模型，测试其对生物威胁相关文本的抵抗能力。

Result: 过滤后的模型在对抗10,000步微调和3亿标记攻击时表现优异，优于现有基线，且不影响其他能力。

Conclusion: 预训练数据过滤是开放权重AI系统的有效防护层，但需结合其他防御措施以全面应对风险。

Abstract: Open-weight AI systems offer unique benefits, including enhanced
transparency, open research, and decentralized access. However, they are
vulnerable to tampering attacks which can efficiently elicit harmful behaviors
by modifying weights or activations. Currently, there is not yet a robust
science of open-weight model risk management. Existing safety fine-tuning
methods and other post-training techniques have struggled to make LLMs
resistant to more than a few dozen steps of adversarial fine-tuning. In this
paper, we investigate whether filtering text about dual-use topics from
training data can prevent unwanted capabilities and serve as a more
tamper-resistant safeguard. We introduce a multi-stage pipeline for scalable
data filtering and show that it offers a tractable and effective method for
minimizing biothreat proxy knowledge in LLMs. We pretrain multiple
6.9B-parameter models from scratch and find that they exhibit substantial
resistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M
tokens of biothreat-related text -- outperforming existing post-training
baselines by over an order of magnitude -- with no observed degradation to
unrelated capabilities. However, while filtered models lack internalized
dangerous knowledge, we find that they can still leverage such information when
it is provided in context (e.g., via search tool augmentation), demonstrating a
need for a defense-in-depth approach. Overall, these findings help to establish
pretraining data curation as a promising layer of defense for open-weight AI
systems.

</details>


### [42] [Tight Bounds for Schrödinger Potential Estimation in Unpaired Image-to-Image Translation Problems](https://arxiv.org/abs/2508.07392)
*Nikita Puchkin,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Main category: cs.LG

TL;DR: 论文研究了基于Schrödinger桥和随机最优控制理论的生成建模和无配对图像转换方法，通过Ornstein-Uhlenbeck过程估计Schrödinger势，并推导了经验风险最小化器的泛化能力界限。


<details>
  <summary>Details</summary>
Motivation: 解决在仅能获取初始和最终分布的独立同分布样本的情况下，如何通过随机最优控制理论实现生成建模和无配对图像转换的问题。

Method: 采用Ornstein-Uhlenbeck过程作为参考过程，估计Schrödinger势，并通过Kullback-Leibler散度定义风险函数，推导经验风险最小化器的泛化界限。

Result: 在有利场景下，几乎实现了快速收敛速率（除对数因子外），并通过数值实验验证了方法的性能。

Conclusion: 提出的方法在生成建模和无配对图像转换中表现出良好的泛化能力和收敛性能。

Abstract: Modern methods of generative modelling and unpaired image-to-image
translation based on Schr\"odinger bridges and stochastic optimal control
theory aim to transform an initial density to a target one in an optimal way.
In the present paper, we assume that we only have access to i.i.d. samples from
initial and final distributions. This makes our setup suitable for both
generative modelling and unpaired image-to-image translation. Relying on the
stochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as
the reference one and estimate the corresponding Schr\"odinger potential.
Introducing a risk function as the Kullback-Leibler divergence between
couplings, we derive tight bounds on generalization ability of an empirical
risk minimizer in a class of Schr\"odinger potentials including Gaussian
mixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we
almost achieve fast rates of convergence up to some logarithmic factors in
favourable scenarios. We also illustrate performance of the suggested approach
with numerical experiments.

</details>


### [43] [Local Diffusion Models and Phases of Data Distributions](https://arxiv.org/abs/2508.06614)
*Fangjun Hu,Guangkuo Liu,Yifan Zhang,Xun Gao*

Main category: cs.LG

TL;DR: 扩散模型通过局部去噪器降低计算成本，提出数据分布相变理论，并设计更高效的模型架构。


<details>
  <summary>Details</summary>
Motivation: 现实数据（如图像）通常具有低维空间结构，但传统扩散模型忽略局部结构，导致计算成本高。

Method: 定义数据分布相，证明去噪过程包含早期平凡相和晚期数据相，相变时局部去噪器失效。提出基于条件互信息的信息论界限诊断相变。

Result: 实验验证局部去噪器在远离相变点时高效，全局网络仅在相变区间必要。

Conclusion: 该研究为扩散模型设计提供新思路，并拓展了数据分布相变的研究方向。

Abstract: As a class of generative artificial intelligence frameworks inspired by
statistical physics, diffusion models have shown extraordinary performance in
synthesizing complicated data distributions through a denoising process
gradually guided by score functions. Real-life data, like images, is often
spatially structured in low-dimensional spaces. However, ordinary diffusion
models ignore this local structure and learn spatially global score functions,
which are often computationally expensive. In this work, we introduce a new
perspective on the phases of data distributions, which provides insight into
constructing local denoisers with reduced computational costs. We define two
distributions as belonging to the same data distribution phase if they can be
mutually connected via spatially local operations such as local denoisers.
Then, we show that the reverse denoising process consists of an early trivial
phase and a late data phase, sandwiching a rapid phase transition where local
denoisers must fail. To diagnose such phase transitions, we prove an
information-theoretic bound on the fidelity of local denoisers based on
conditional mutual information, and conduct numerical experiments in a
real-world dataset. This work suggests simpler and more efficient architectures
of diffusion models: far from the phase transition point, we can use small
local neural networks to compute the score function; global neural networks are
only necessary around the narrow time interval of phase transitions. This
result also opens up new directions for studying phases of data distributions,
the broader science of generative artificial intelligence, and guiding the
design of neural networks inspired by physics concepts.

</details>


### [44] [MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification](https://arxiv.org/abs/2508.07465)
*Tiantian Yang,Zhiqian Chen*

Main category: cs.LG

TL;DR: MOTGNN是一种新颖的多组学数据整合框架，通过XGBoost构建监督图，结合GNN和深度前馈网络，显著提升了疾病分类的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多组学数据的高维性和复杂交互为疾病建模带来挑战，需要一种既能整合数据又能保持可解释性的方法。

Method: 使用XGBoost进行组学特异性监督图构建，结合模态特异性GNN进行分层表示学习，最后通过深度前馈网络整合跨组学数据。

Result: 在三个真实疾病数据集上，MOTGNN在准确性、ROC-AUC和F1分数上优于现有方法5-10%，且在类别不平衡时表现稳健。

Conclusion: MOTGNN在多组学疾病建模中显著提升了预测准确性和可解释性，具有广泛应用潜力。

Abstract: Integrating multi-omics data, such as DNA methylation, mRNA expression, and
microRNA (miRNA) expression, offers a comprehensive view of the biological
mechanisms underlying disease. However, the high dimensionality and complex
interactions among omics layers present major challenges for predictive
modeling. We propose Multi-Omics integration with Tree-generated Graph Neural
Network (MOTGNN), a novel and interpretable framework for binary disease
classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform
omics-specific supervised graph construction, followed by modality-specific
Graph Neural Networks (GNNs) for hierarchical representation learning, and a
deep feedforward network for cross-omics integration. On three real-world
disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in
accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance
(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains
computational efficiency through sparse graphs (2.1-2.8 edges per node) and
provides built-in interpretability, revealing both top-ranked biomarkers and
the relative contributions of each omics modality. These results highlight
MOTGNN's potential to improve both predictive accuracy and interpretability in
multi-omics disease modeling.

</details>


### [45] [Generalizing Scaling Laws for Dense and Sparse Large Language Models](https://arxiv.org/abs/2508.06617)
*Md Arafat Hossain,Xingfu Wu,Valerie Taylor,Ali Jannesari*

Main category: cs.LG

TL;DR: 本文提出了一种通用的扩展法则，适用于密集和稀疏大型语言模型，以优化训练效率和资源分配。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型规模和计算成本的快速增长，需要新的技术来提高训练效率，但现有扩展法则多为特定架构设计，缺乏通用性。

Method: 重新审视现有扩展法则，提出一种通用的扩展法则框架，适用于密集和稀疏模型，并进行评估比较。

Result: 提出的通用扩展法则在密集和稀疏模型中均表现出有效性。

Conclusion: 通用扩展法则为大型语言模型的训练提供了统一的优化框架，适用于多种架构。

Abstract: Over the past few years, the size of language models has grown exponentially,
as has the computational cost to train these large models. This rapid growth
has motivated researchers to develop new techniques aimed at enhancing the
efficiency of the training process. Despite these advancements, optimally
predicting the model size or allocating optimal resources remains a challenge.
Several efforts have addressed the challenge by proposing different scaling
laws, but almost all of them are architecture-specific (dense or sparse). In
this work we revisit existing scaling laws and propose a generalized scaling
law to provide a unified framework that is applicable to both dense and sparse
large language models. We evaluate and compare our proposed scaling law with
existing scaling laws to demonstrate its effectiveness.

</details>


### [46] [Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications](https://arxiv.org/abs/2508.07473)
*Zijian Liu*

Main category: cs.LG

TL;DR: 该论文研究了在线凸优化（OCO）中梯度估计具有重尾分布时的性能，证明了经典算法（如在线梯度下降）无需修改即可实现最优遗憾界，并扩展了应用场景。


<details>
  <summary>Details</summary>
Motivation: 研究在梯度估计具有重尾分布（即仅有限p阶中心矩）时，经典OCO算法的性能，填补现有研究的空白。

Method: 分析了经典OCO算法（如在线梯度下降）在重尾分布下的表现，无需额外操作（如梯度裁剪）。

Result: 证明了这些算法在重尾分布下仍能实现完全最优的遗憾界，且适用于更广泛的场景（如非光滑非凸优化）。

Conclusion: OCO在重尾分布下无需额外操作即可有效解决，扩展了算法的应用范围，特别是在非光滑非凸优化中。

Abstract: In Online Convex Optimization (OCO), when the stochastic gradient has a
finite variance, many algorithms provably work and guarantee a sublinear
regret. However, limited results are known if the gradient estimate has a heavy
tail, i.e., the stochastic gradient only admits a finite $\mathsf{p}$-th
central moment for some $\mathsf{p}\in\left(1,2\right]$. Motivated by it, this
work examines different old algorithms for OCO (e.g., Online Gradient Descent)
in the more challenging heavy-tailed setting. Under the standard bounded domain
assumption, we establish new regrets for these classical methods without any
algorithmic modification. Remarkably, these regret bounds are fully optimal in
all parameters (can be achieved even without knowing $\mathsf{p}$), suggesting
that OCO with heavy tails can be solved effectively without any extra operation
(e.g., gradient clipping). Our new results have several applications. A
particularly interesting one is the first provable convergence result for
nonsmooth nonconvex optimization under heavy-tailed noise without gradient
clipping. Furthermore, we explore broader settings (e.g., smooth OCO) and
extend our ideas to optimistic algorithms to handle different cases
simultaneously.

</details>


### [47] [N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting](https://arxiv.org/abs/2508.07490)
*Ricardo Matos,Luis Roque,Vitor Cerqueira*

Main category: cs.LG

TL;DR: N-BEATS-MOE是N-BEATS的扩展，通过引入混合专家（MoE）层和动态块加权策略，提升了时间序列预测的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 改进N-BEATS模型，使其能更好地适应不同时间序列的特性，并通过门控机制增强模型的可解释性。

Method: 在N-BEATS基础上引入MoE层和动态块加权策略，利用门控网络选择最相关的专家块。

Result: 在12个基准数据集上测试，N-BEATS-MOE表现优于其他方法，尤其在异质性时间序列上效果显著。

Conclusion: N-BEATS-MOE通过MoE层和动态加权策略，提升了预测性能和可解释性，适用于复杂时间序列任务。

Abstract: Deep learning approaches are increasingly relevant for time series
forecasting tasks. Methods such as N-BEATS, which is built on stacks of
multilayer perceptrons (MLPs) blocks, have achieved state-of-the-art results on
benchmark datasets and competitions. N-BEATS is also more interpretable
relative to other deep learning approaches, as it decomposes forecasts into
different time series components, such as trend and seasonality. In this work,
we present N-BEATS-MOE, an extension of N-BEATS based on a Mixture-of-Experts
(MoE) layer. N-BEATS-MOE employs a dynamic block weighting strategy based on a
gating network which allows the model to better adapt to the characteristics of
each time series. We also hypothesize that the gating mechanism provides
additional interpretability by identifying which expert is most relevant for
each series. We evaluate our method across 12 benchmark datasets against
several approaches, achieving consistent improvements on several datasets,
especially those composed of heterogeneous time series.

</details>


### [48] [Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record](https://arxiv.org/abs/2508.06627)
*Mosbah Aouad,Anirudh Choudhary,Awais Farooq,Steven Nevers,Lusine Demirkhanyan,Bhrandon Harris,Suguna Pappu,Christopher Gondi,Ravishankar Iyer*

Main category: cs.LG

TL;DR: 提出一种多模态方法，结合电子健康记录中的诊断代码和实验室数据，提前一年检测胰腺导管腺癌（PDAC），性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: PDAC早期检测困难，缺乏特异性症状和可靠生物标志物。

Method: 结合神经控制微分方程、预训练语言模型、循环网络和交叉注意力机制，整合诊断代码和实验室数据。

Result: 在4700名患者数据上，AUC提升6.5%至15.5%，并识别出新的高风险生物标志物。

Conclusion: 该方法显著提升PDAC早期检测能力，并发现新的潜在生物标志物。

Abstract: Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and
early detection remains a major clinical challenge due to the absence of
specific symptoms and reliable biomarkers. In this work, we propose a new
multimodal approach that integrates longitudinal diagnosis code histories and
routinely collected laboratory measurements from electronic health records to
detect PDAC up to one year prior to clinical diagnosis. Our method combines
neural controlled differential equations to model irregular lab time series,
pretrained language models and recurrent networks to learn diagnosis code
trajectory representations, and cross-attention mechanisms to capture
interactions between the two modalities. We develop and evaluate our approach
on a real-world dataset of nearly 4,700 patients and achieve significant
improvements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.
Furthermore, our model identifies diagnosis codes and laboratory panels
associated with elevated PDAC risk, including both established and new
biomarkers. Our code is available at
https://github.com/MosbahAouad/EarlyPDAC-MML.

</details>


### [49] [FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction](https://arxiv.org/abs/2508.07518)
*Sichen Zhao,Wei Shao,Jeffrey Chan,Ziqi Xu,Flora Salim*

Main category: cs.LG

TL;DR: 论文提出了一种基于解耦表示学习的新框架FairDRL-ST，用于解决时空预测中的公平性问题，特别关注移动需求预测。


<details>
  <summary>Details</summary>
Motivation: 由于时空神经网络在城市计算中的广泛应用可能对关键城市基础设施的用户产生直接影响，而现有方法多关注准确性，公平性问题日益受到关注。

Method: 通过对抗学习和解耦表示学习，框架学习分离包含敏感信息的属性，以无监督方式实现公平性。

Result: 在真实城市移动数据集上验证，框架能缩小公平性差距，同时保持与最先进公平感知方法相当的预测性能。

Conclusion: FairDRL-ST框架在无监督条件下有效解决了时空预测中的公平性问题，性能损失最小。

Abstract: As deep spatio-temporal neural networks are increasingly utilised in urban
computing contexts, the deployment of such methods can have a direct impact on
users of critical urban infrastructure, such as public transport, emergency
services, and traffic management systems. While many spatio-temporal methods
focus on improving accuracy, fairness has recently gained attention due to
growing evidence that biased predictions in spatio-temporal applications can
disproportionately disadvantage certain demographic or geographic groups,
thereby reinforcing existing socioeconomic inequalities and undermining the
ethical deployment of AI in public services. In this paper, we propose a novel
framework, FairDRL-ST, based on disentangled representation learning, to
address fairness concerns in spatio-temporal prediction, with a particular
focus on mobility demand forecasting. By leveraging adversarial learning and
disentangled representation learning, our framework learns to separate
attributes that contain sensitive information. Unlike existing methods that
enforce fairness through supervised learning, which may lead to
overcompensation and degraded performance, our framework achieves fairness in
an unsupervised manner with minimal performance loss. We apply our framework to
real-world urban mobility datasets and demonstrate its ability to close
fairness gaps while delivering competitive predictive performance compared to
state-of-the-art fairness-aware methods.

</details>


### [50] [Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning](https://arxiv.org/abs/2508.07556)
*Stephan Rabanser*

Main category: cs.LG

TL;DR: 该论文研究了如何通过不确定性估计提升机器学习系统的安全性和可信度，提出了一种轻量级的选择性预测方法，并探讨了隐私保护与不确定性质量的关系。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域部署机器学习系统时，可靠性至关重要。论文旨在通过不确定性估计增强模型的安全性和可信度，特别是在选择性预测中。

Method: 通过利用模型训练轨迹中的不确定性信号，提出了一种无需改变架构或损失函数的轻量级后处理选择性预测方法，并研究了隐私保护对不确定性的影响。

Result: 该方法在选择性预测任务中表现优异，且在隐私保护下仍保持鲁棒性。论文还分解了选择性分类差距，并提出了防御对抗性攻击的策略。

Conclusion: 论文通过改进、评估和保护不确定性估计，推动了可靠机器学习的发展，使模型不仅能准确预测，还能知道何时应放弃预测。

Abstract: Machine learning (ML) systems are increasingly deployed in high-stakes
domains where reliability is paramount. This thesis investigates how
uncertainty estimation can enhance the safety and trustworthiness of ML,
focusing on selective prediction -- where models abstain when confidence is
low.
  We first show that a model's training trajectory contains rich uncertainty
signals that can be exploited without altering its architecture or loss. By
ensembling predictions from intermediate checkpoints, we propose a lightweight,
post-hoc abstention method that works across tasks, avoids the cost of deep
ensembles, and achieves state-of-the-art selective prediction performance.
Crucially, this approach is fully compatible with differential privacy (DP),
allowing us to study how privacy noise affects uncertainty quality. We find
that while many methods degrade under DP, our trajectory-based approach remains
robust, and we introduce a framework for isolating the privacy-uncertainty
trade-off. Next, we then develop a finite-sample decomposition of the selective
classification gap -- the deviation from the oracle accuracy-coverage curve --
identifying five interpretable error sources and clarifying which interventions
can close the gap. This explains why calibration alone cannot fix ranking
errors, motivating methods that improve uncertainty ordering. Finally, we show
that uncertainty signals can be adversarially manipulated to hide errors or
deny service while maintaining high accuracy, and we design defenses combining
calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating,
and safeguarding uncertainty estimation, enabling models that not only make
accurate predictions -- but also know when to say "I do not know".

</details>


### [51] [Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series](https://arxiv.org/abs/2508.06638)
*Muyan Anna Li,Aditi Gautam*

Main category: cs.LG

TL;DR: 论文提出了两种自适应阈值框架（SCS和MACS），用于非平稳环境中的时间序列异常检测，显著提升了F1分数。


<details>
  <summary>Details</summary>
Motivation: 传统静态阈值在非平稳环境中容易失效，需要适应统计特性随时间变化的场景。

Method: 提出SCS和MACS框架，结合统计在线学习和分割原理，实现局部自适应阈值。

Result: 在Wafer Manufacturing数据集上，F1分数显著优于传统方法。

Conclusion: 自适应阈值框架能实现可靠、可解释且及时的异常检测。

Abstract: As time series data become increasingly prevalent in domains such as
manufacturing, IT, and infrastructure monitoring, anomaly detection must adapt
to nonstationary environments where statistical properties shift over time.
Traditional static thresholds are easily rendered obsolete by regime shifts,
concept drift, or multi-scale changes. To address these challenges, we
introduce and empirically evaluate two novel adaptive thresholding frameworks:
Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence
Segments (MACS). Both leverage statistical online learning and segmentation
principles for local, contextually sensitive adaptation, maintaining guarantees
on false alarm rates even under evolving distributions. Our experiments across
Wafer Manufacturing benchmark datasets show significant F1-score improvement
compared to traditional percentile and rolling quantile approaches. This work
demonstrates that robust, statistically principled adaptive thresholds enable
reliable, interpretable, and timely detection of diverse real-world anomalies.

</details>


### [52] [Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information](https://arxiv.org/abs/2508.07713)
*Jinghan Yang,Jiayu Weng*

Main category: cs.LG

TL;DR: 提出了一种基于互信息的数据选择框架，用于混合噪声场景下的高质量数据筛选，显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现实数据集常受标签噪声和输入噪声影响，而深度神经网络会记忆错误标签，因此数据质量对模型性能至关重要。

Method: 通过计算样本对整体互信息的点贡献，识别低贡献样本为噪声或错误标签实例。

Result: 在MNIST数据集上验证，该方法能有效过滤低质量样本，标签噪声下分类准确率提升高达15%。

Conclusion: 该方法对良性输入修改具有鲁棒性，能保留语义有效数据并过滤真正损坏的样本。

Abstract: Deep neural networks can memorize corrupted labels, making data quality
critical for model performance, yet real-world datasets are frequently
compromised by both label noise and input noise. This paper proposes a mutual
information-based framework for data selection under hybrid noise scenarios
that quantifies statistical dependencies between inputs and labels. We compute
each sample's pointwise contribution to the overall mutual information and find
that lower contributions indicate noisy or mislabeled instances. Empirical
validation on MNIST with different synthetic noise settings demonstrates that
the method effectively filters low-quality samples. Under label corruption,
training on high-MI samples improves classification accuracy by up to 15\%
compared to random sampling. Furthermore, the method exhibits robustness to
benign input modifications, preserving semantically valid data while filtering
truly corrupted samples.

</details>


### [53] [Fractal Language Modelling by Universal Sequence Maps (USM)](https://arxiv.org/abs/2508.06641)
*Jonas S Almeida,Daniel E Russ,Susana Vinga,Ines Duarte,Lee Mason,Praphulla Bhawsar,Aaron Ge,Arlindo Oliveira,Jeya Balaji Balasubramanian*

Main category: cs.LG

TL;DR: 论文提出了一种改进的通用序列映射（USM）方法，解决了迭代过程中的种子偏差问题，实现了序列身份与数值定位的完全一致，并揭示了USM作为高效数值过程的本质。


<details>
  <summary>Details</summary>
Motivation: 随着基于Transformer的语言模型（如ChatGPT）的兴起，研究者对多尺度和多维度符号序列编码方法重新产生兴趣，以保留符号序列的上下文信息。

Method: USM由两个混沌游戏表示（CGR）组成，分别向前和向后迭代，可投影到频域（FCGR），用于计算Chebyshev距离和k-mer频率。

Result: 改进的USM解决了种子偏差问题，实现了序列身份与数值定位的一致，并发现USM是一种高效收敛的数值过程。

Conclusion: USM适用于任意基数的字母表，尤其在基因组序列中表现出色。

Abstract: Motivation: With the advent of Language Models using Transformers,
popularized by ChatGPT, there is a renewed interest in exploring encoding
procedures that numerically represent symbolic sequences at multiple scales and
embedding dimensions. The challenge that encoding addresses is the need for
mechanisms that uniquely retain contextual information about the succession of
individual symbols, which can then be modeled by nonlinear formulations such as
neural networks.
  Context: Universal Sequence Maps(USM) are iterated functions that bijectively
encode symbolic sequences onto embedded numerical spaces. USM is composed of
two Chaos Game Representations (CGR), iterated forwardly and backwardly, that
can be projected into the frequency domain (FCGR). The corresponding USM
coordinates can be used to compute a Chebyshev distance metric as well as k-mer
frequencies, without having to recompute the embedded numeric coordinates, and,
paradoxically, allowing for non-integers values of k.
  Results: This report advances the bijective fractal encoding by Universal
Sequence Maps (USM) by resolving seeding biases affecting the iterated process.
The resolution had two results, the first expected, the second an intriguing
outcome: 1) full reconciliation of numeric positioning with sequence identity;
and 2) uncovering the nature of USM as an efficient numeric process converging
towards a steady state sequence embedding solution. We illustrate these results
for genomic sequences because of the convenience of a planar representation
defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,
the application to alphabet of arbitrary cardinality was found to be
straightforward.

</details>


### [54] [A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory](https://arxiv.org/abs/2508.07746)
*Fengdi Che*

Main category: cs.LG

TL;DR: 该论文探讨了离线强化学习（RL）的理论基础与实践算法设计之间的桥梁，分析了理论条件、数据覆盖假设以及离线RL的固有挑战。


<details>
  <summary>Details</summary>
Motivation: 离线RL旨在通过固定数据集优化回报，但理论洞察与算法设计之间存在脱节，本文试图弥合这一差距。

Method: 通过列举理论证明所需的条件（如函数表示和数据覆盖假设），分析反例，并讨论缓解挑战的技术。

Result: 揭示了离线RL的固有困难，并提出了满足条件的解决方案，同时指出算法的局限性。

Conclusion: 理论条件不仅是证明的基础，也揭示了算法的局限性，提醒我们在条件不满足时寻找新解决方案。

Abstract: Offline reinforcement learning (RL) aims to optimize the return given a fixed
dataset of agent trajectories without additional interactions with the
environment. While algorithm development has progressed rapidly, significant
theoretical advances have also been made in understanding the fundamental
challenges of offline RL. However, bridging these theoretical insights with
practical algorithm design remains an ongoing challenge. In this survey, we
explore key intuitions derived from theoretical work and their implications for
offline RL algorithms.
  We begin by listing the conditions needed for the proofs, including function
representation and data coverage assumptions. Function representation
conditions tell us what to expect for generalization, and data coverage
assumptions describe the quality requirement of the data. We then examine
counterexamples, where offline RL is not solvable without an impractically
large amount of data. These cases highlight what cannot be achieved for all
algorithms and the inherent hardness of offline RL. Building on techniques to
mitigate these challenges, we discuss the conditions that are sufficient for
offline RL. These conditions are not merely assumptions for theoretical proofs,
but they also reveal the limitations of these algorithms and remind us to
search for novel solutions when the conditions cannot be satisfied.

</details>


### [55] [Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN](https://arxiv.org/abs/2508.06647)
*Andrey Sidorenko,Paul Tiwald*

Main category: cs.LG

TL;DR: TabularARGN是一种新型神经网络架构，用于生成高质量合成表格数据，平衡隐私与实用性。


<details>
  <summary>Details</summary>
Motivation: 传统匿名化技术无法充分保护隐私，需要更高效的方法生成合成数据。

Method: 采用基于离散化的自回归方法，设计TabularARGN神经网络架构。

Result: 在统计相似性、机器学习实用性和检测鲁棒性上表现优异，隐私评估显示其具有稳健性。

Conclusion: TabularARGN在隐私保护和数据实用性方面取得了有效平衡。

Abstract: Synthetic data generation has become essential for securely sharing and
analyzing sensitive data sets. Traditional anonymization techniques, however,
often fail to adequately preserve privacy. We introduce the Tabular
Auto-Regressive Generative Network (TabularARGN), a neural network architecture
specifically designed for generating high-quality synthetic tabular data. Using
a discretization-based auto-regressive approach, TabularARGN achieves high data
fidelity while remaining computationally efficient. We evaluate TabularARGN
against existing synthetic data generation methods, showing competitive results
in statistical similarity, machine learning utility, and detection robustness.
We further perform an in-depth privacy evaluation using systematic
membership-inference attacks, highlighting the robustness and effective
privacy-utility balance of our approach.

</details>


### [56] [Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent](https://arxiv.org/abs/2508.08222)
*Tong Yang,Yu Huang,Yingbin Liang,Yuejie Chi*

Main category: cs.LG

TL;DR: 该论文研究了Transformer如何通过训练学习解决符号多步推理问题，特别是树中的路径查找任务。通过理论分析，证明了单层Transformer可以解决前向和后向推理任务，并具有泛化能力。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer如何通过训练获得多步推理能力，尤其是从理论角度分析其机制。

Method: 分析了后向推理和前向推理任务，通过梯度下降动态理论，研究了单层Transformer的训练过程。

Result: 研究表明，训练后的单层Transformer可以解决任务，并具有泛化能力；多阶段训练动态揭示了注意力头的分工与协作。

Conclusion: 论文揭示了Transformer如何实现序列化算法过程，表明结构化任务和中间思维链步骤可以使浅层Transformer解决复杂问题。

Abstract: Transformers have demonstrated remarkable capabilities in multi-step
reasoning tasks. However, understandings of the underlying mechanisms by which
they acquire these abilities through training remain limited, particularly from
a theoretical standpoint. This work investigates how transformers learn to
solve symbolic multi-step reasoning problems through chain-of-thought
processes, focusing on path-finding in trees. We analyze two intertwined tasks:
a backward reasoning task, where the model outputs a path from a goal node to
the root, and a more complex forward reasoning task, where the model implements
two-stage reasoning by first identifying the goal-to-root path and then
reversing it to produce the root-to-goal path. Our theoretical analysis,
grounded in the dynamics of gradient descent, shows that trained one-layer
transformers can provably solve both tasks with generalization guarantees to
unseen trees. In particular, our multi-phase training dynamics for forward
reasoning elucidate how different attention heads learn to specialize and
coordinate autonomously to solve the two subtasks in a single autoregressive
path. These results provide a mechanistic explanation of how trained
transformers can implement sequential algorithmic procedures. Moreover, they
offer insights into the emergence of reasoning abilities, suggesting that when
tasks are structured to take intermediate chain-of-thought steps, even shallow
multi-head transformers can effectively solve problems that would otherwise
require deeper architectures.

</details>


### [57] [In-Context Reinforcement Learning via Communicative World Models](https://arxiv.org/abs/2508.06659)
*Fernando Martinez-Lopez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: 本文提出CORAL框架，通过将表示学习与控制解耦，提升强化学习代理的上下文适应能力。


<details>
  <summary>Details</summary>
Motivation: 强化学习代理在新任务和环境中泛化能力不足，因其表示和策略过度拟合训练环境。

Method: CORAL框架将上下文强化学习视为双代理通信问题，通过预训练信息代理（IA）生成可迁移的通信上下文，控制代理（CA）利用该上下文学习任务。

Result: 实验表明，CORAL显著提升了样本效率，并在未见过的稀疏奖励环境中实现零样本适应。

Conclusion: 学习可迁移的通信表示是提升强化学习代理泛化能力的有效方法。

Abstract: Reinforcement learning (RL) agents often struggle to generalize to new tasks
and contexts without updating their parameters, mainly because their learned
representations and policies are overfit to the specifics of their training
environments. To boost agents' in-context RL (ICRL) ability, this work
formulates ICRL as a two-agent emergent communication problem and introduces
CORAL (Communicative Representation for Adaptive RL), a framework that learns a
transferable communicative context by decoupling latent representation learning
from control. In CORAL, an Information Agent (IA) is pre-trained as a world
model on a diverse distribution of tasks. Its objective is not to maximize task
reward, but to build a world model and distill its understanding into concise
messages. The emergent communication protocol is shaped by a novel Causal
Influence Loss, which measures the effect that the message has on the next
action. During deployment, the previously trained IA serves as a fixed
contextualizer for a new Control Agent (CA), which learns to solve tasks by
interpreting the provided communicative context. Our experiments demonstrate
that this approach enables the CA to achieve significant gains in sample
efficiency and successfully perform zero-shot adaptation with the help of
pre-trained IA in entirely unseen sparse-reward environments, validating the
efficacy of learning a transferable communicative representation.

</details>


### [58] [Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.06663)
*Yuan-Hung Chao,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 将KANs（Kolmogorov-Arnold Networks）集成到三种流行的GNN架构中，提出新模型KGAT、KSGC和KAPPNP，并通过多教师知识融合框架提升性能。


<details>
  <summary>Details</summary>
Motivation: GNNs在图结构数据上表现优异，但对图连接的依赖限制了其可扩展性和效率。KANs具有强大的非线性表达能力和高效推理能力，可弥补GNNs的不足。

Method: 将KANs集成到GAT、SGC和APPNP三种GNN架构中，形成新模型；采用多教师知识融合框架，将多个KAN-based GNNs的知识蒸馏到一个图无关的KAN学生模型中。

Result: 实验表明，新模型提高了节点分类准确性，知识融合方法显著提升了学生模型的性能。

Conclusion: KANs能增强GNN的表达能力，并实现高效的图无关推理，具有广阔的应用潜力。

Abstract: Graph Neural Networks (GNNs) have shown strong performance on
graph-structured data, but their reliance on graph connectivity often limits
scalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent
architecture with learnable univariate functions, offer strong nonlinear
expressiveness and efficient inference. In this work, we integrate KANs into
three popular GNN architectures-GAT, SGC, and APPNP-resulting in three new
models: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge
amalgamation framework, where knowledge from multiple KAN-based GNNs is
distilled into a graph-independent KAN student model. Experiments on benchmark
datasets show that the proposed models improve node classification accuracy,
and the knowledge amalgamation approach significantly boosts student model
performance. Our findings highlight the potential of KANs for enhancing GNN
expressiveness and for enabling efficient, graph-free inference.

</details>


### [59] [Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation](https://arxiv.org/abs/2508.06676)
*Chia-Hsun Lu,Guan-Jhih Wu,Ya-Chi Ho,Chih-Ya Shen*

Main category: cs.LG

TL;DR: 提出了一种针对Kolmogorov-Arnold Networks (KAN)的新型水印方法DCT-AW，通过离散余弦变换扰动激活输出，实现了任务无关性和高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习中知识产权保护的重要性增加，现有水印方法难以适应KAN的新型架构，因此需要一种专门的水印方法。

Method: 利用KAN的可学习激活函数，通过离散余弦变换（DCT）扰动激活输出嵌入水印。

Result: 实验表明DCT-AW对模型性能影响小，且能有效抵抗微调、剪枝等攻击。

Conclusion: DCT-AW是一种适用于KAN的高效水印方法，兼具任务无关性和鲁棒性。

Abstract: With the increasing importance of protecting intellectual property in machine
learning, watermarking techniques have gained significant attention. As
advanced models are increasingly deployed in domains such as social network
analysis, the need for robust model protection becomes even more critical.
While existing watermarking methods have demonstrated effectiveness for
conventional deep neural networks, they often fail to adapt to the novel
architecture, Kolmogorov-Arnold Networks (KAN), which feature learnable
activation functions. KAN holds strong potential for modeling complex
relationships in network-structured data. However, their unique design also
introduces new challenges for watermarking. Therefore, we propose a novel
watermarking method, Discrete Cosine Transform-based Activation Watermarking
(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of
KAN, our method embeds watermarks by perturbing activation outputs using
discrete cosine transform, ensuring compatibility with diverse tasks and
achieving task independence. Experimental results demonstrate that DCT-AW has a
small impact on model performance and provides superior robustness against
various watermark removal attacks, including fine-tuning, pruning, and
retraining after pruning.

</details>


### [60] [Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select](https://arxiv.org/abs/2508.06692)
*Md. Akmol Masud,Md Abrar Jahin,Mahmud Hasan*

Main category: cs.LG

TL;DR: HeteRo-Select框架通过智能选择客户端子集，解决了联邦学习中数据异构性导致的训练不稳定问题，显著提升了准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）因客户端数据多样性常导致训练不稳定，现有方法（如Oort）在后期训练中准确性下降明显。

Method: 提出HeteRo-Select框架，基于客户端有用性、公平性、更新速度和数据多样性进行逐步评分，并理论分析其收敛性。

Result: 在CIFAR-10数据集上，HeteRo-Select的峰值准确率为74.75%，最终准确率为72.76%，稳定性下降仅1.99%，优于Oort。

Conclusion: HeteRo-Select为异构FL问题提供了理论和实证支持，是可靠的解决方案。

Abstract: Federated Learning (FL) is a machine learning technique that often suffers
from training instability due to the diverse nature of client data. Although
utility-based client selection methods like Oort are used to converge by
prioritizing high-loss clients, they frequently experience significant drops in
accuracy during later stages of training. We propose a theoretical
HeteRo-Select framework designed to maintain high performance and ensure
long-term training stability. We provide a theoretical analysis showing that
when client data is very different (high heterogeneity), choosing a smart
subset of client participation can reduce communication more effectively
compared to full participation. Our HeteRo-Select method uses a clear,
step-by-step scoring system that considers client usefulness, fairness, update
speed, and data variety. It also shows convergence guarantees under strong
regularization. Our experimental results on the CIFAR-10 dataset under
significant label skew ($\alpha=0.1$) support the theoretical findings. The
HeteRo-Select method performs better than existing approaches in terms of peak
accuracy, final accuracy, and training stability. Specifically, HeteRo-Select
achieves a peak accuracy of $74.75\%$, a final accuracy of $72.76\%$, and a
minimal stability drop of $1.99\%$. In contrast, Oort records a lower peak
accuracy of $73.98\%$, a final accuracy of $71.25\%$, and a larger stability
drop of $2.73\%$. The theoretical foundations and empirical performance in our
study make HeteRo-Select a reliable solution for real-world heterogeneous FL
problems.

</details>


### [61] [CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations](https://arxiv.org/abs/2508.06704)
*Hager Radi Abdelwahed,Mélisande Teng,Robin Zbinden,Laura Pollock,Hugo Larochelle,Devis Tuia,David Rolnick*

Main category: cs.LG

TL;DR: CISO是一种基于深度学习的物种分布建模方法，能够结合不完整的物种观测数据与环境变量，提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统物种分布模型（SDMs）通常忽略物种间的生物相互作用，且现有方法对数据要求严格。CISO旨在解决这些问题。

Method: CISO利用深度学习，灵活结合环境变量和不完整的物种观测数据，适用于不同物种组的数据集（如植物、鸟类和蝴蝶）。

Result: 实验表明，CISO在预测性能上优于其他方法，尤其是在结合部分生物信息或多数据集时表现更佳。

Conclusion: CISO是一种有前景的生态工具，能够整合不完整生物数据并识别跨类群物种间的潜在相互作用。

Abstract: Species distribution models (SDMs) are widely used to predict species'
geographic distributions, serving as critical tools for ecological research and
conservation planning. Typically, SDMs relate species occurrences to
environmental variables representing abiotic factors, such as temperature,
precipitation, and soil properties. However, species distributions are also
strongly influenced by biotic interactions with other species, which are often
overlooked. While some methods partially address this limitation by
incorporating biotic interactions, they often assume symmetrical pairwise
relationships between species and require consistent co-occurrence data. In
practice, species observations are sparse, and the availability of information
about the presence or absence of other species varies significantly across
locations. To address these challenges, we propose CISO, a deep learning-based
method for species distribution modeling Conditioned on Incomplete Species
Observations. CISO enables predictions to be conditioned on a flexible number
of species observations alongside environmental variables, accommodating the
variability and incompleteness of available biotic data. We demonstrate our
approach using three datasets representing different species groups: sPlotOpen
for plants, SatBird for birds, and a new dataset, SatButterfly, for
butterflies. Our results show that including partial biotic information
improves predictive performance on spatially separate test sets. When
conditioned on a subset of species within the same dataset, CISO outperforms
alternative methods in predicting the distribution of the remaining species.
Furthermore, we show that combining observations from multiple datasets can
improve performance. CISO is a promising ecological tool, capable of
incorporating incomplete biotic information and identifying potential
interactions between species from disparate taxa.

</details>


### [62] [Analysis of Schedule-Free Nonconvex Optimization](https://arxiv.org/abs/2508.06743)
*Connor Brown*

Main category: cs.LG

TL;DR: 论文提出了一种鲁棒的Lyapunov框架，用于分析非凸优化中的Schedule-Free方法，证明了其无需依赖总步数T的收敛性。


<details>
  <summary>Details</summary>
Motivation: 解决传统一阶方法需要依赖总步数T的步长调度问题，扩展Schedule-Free方法在非凸优化中的应用。

Method: 引入Lyapunov框架，结合Polyak-Ruppert平均和动量方法，分析非凸优化中的收敛性。

Result: 证明了在非凸优化中，Schedule-Free方法具有与T无关的收敛率，如O(1/log T)和O(log T/T)。

Conclusion: 该研究扩展了Schedule-Free方法的适用范围，为非凸优化提供了新的收敛性分析工具。

Abstract: First-order methods underpin most large-scale learning algorithms, yet their
classical convergence guarantees hinge on carefully scheduled step-sizes that
depend on the total horizon $T$, which is rarely known in advance. The
Schedule-Free (SF) method promises optimal performance with hyperparameters
that are independent of $T$ by interpolating between Polyak--Ruppert averaging
and momentum, but nonconvex analysis of SF has been limited or reliant on
strong global assumptions. We introduce a robust Lyapunov framework that, under
only $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step
descent inequality. This yields horizon-agnostic bounds in the nonconvex
setting: $O(1/\log T)$ for constant step + PR averaging, $O(\log T/T)$ for a
linearly growing step-size, and a continuum of $O(T^{-(1-\alpha)})$ rates for
polynomial averaging. We complement these proofs with Performance Estimation
Problem (PEP) experiments that numerically validate our rates and suggest that
our $O(1/\log T)$ bound on the original nonconvex SF algorithm may tighten to
$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex
optimization and charts future directions for optimal nonconvex rates.

</details>


### [63] [Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning](https://arxiv.org/abs/2508.06765)
*Xingke Yang,Liang Li,Sicong Li,Liwei Guan,Hao Wang,Xiaoqi Qi,Jiang Liu,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: Fed MobiLLM提出了一种高效的联邦学习方法，用于在异构移动设备上微调大型语言模型，显著降低了计算和通信开销。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法在移动设备上计算和内存负担过重，且同步协议导致效率低下。Fed MobiLLM旨在解决这些问题。

Method: 采用服务器辅助的联邦侧调优范式，设备仅执行轻量级前向传播，服务器独立训练共享侧网络，并引入自适应层特征对齐方法。

Result: 实验显示，Fed MobiLLM在保持性能的同时，显著降低了计算（95.2%）、通信（93.2%）开销，并加速收敛（5.1倍）。

Conclusion: Fed MobiLLM为异构移动设备上的LLM微调提供了高效实用的解决方案。

Abstract: Collaboratively fine-tuning (FT) large language models (LLMs) over
heterogeneous mobile devices fosters immense potential applications of
personalized intelligence. However, such a vision faces critical system
challenges. Conventional federated LLM FT approaches place prohibitive
computational and memory burdens on mobile hardware, and their synchronous
model aggregation protocols stall for slower devices. In this paper, we propose
Fed MobiLLM, a novel design to facilitate efficient federated LLM FT across
mobile devices with diverse computing/communication speeds and local model
architectures. In particular, Fed MobiLLM implements a pioneering
server-assisted federated side-tuning paradigm. Briefly, mobile devices perform
lightweight forward propagation computations on local data using their frozen
pre-scaled backbone LLMs, and then upload selected intermediate activations.
The server trains a shared side-network independently, eliminating client-side
backpropagation and enabling asynchronous updates. To bridge model
heterogeneity across different devices, we introduce an adaptive layer-wise
feature alignment method, which ensures consistent representations for
collaboratively tuning a shared side network. Extensive experimental results
demonstrate that Fed MobiLLM can maintain robust fine-tuning performance while
achieving extremely low on-device memory, with at least 95.2% reduction in
computation overhead, 93.2% reduction in communication costs and 5.1x faster
convergence compared to existing methods, validating its efficacy for practical
LLM adaptation over heterogeneous mobile devices.

</details>


### [64] [PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems](https://arxiv.org/abs/2508.06767)
*Arman Dogru,R. Irem Bor-Yaliniz,Nimal Gamini Senarath*

Main category: cs.LG

TL;DR: PANAMA算法通过优先不对称性和网络感知的MARL框架，提升了多智能体路径规划的性能，优化了数据共享策略，适用于复杂环境。


<details>
  <summary>Details</summary>
Motivation: 随着数字孪生和自动化系统的发展，高效的数据共享和算法成为关键，研究旨在解决网络与应用提供商在数字孪生生态系统中的动态交互问题。

Method: 提出PANAMA算法，采用集中训练分散执行（CTDE）框架和异步执行者-学习者架构，结合网络感知的MARL进行多智能体路径规划。

Result: PANAMA在路径规划的准确性、速度和可扩展性上优于现有基准，并通过仿真验证了优化的数据共享策略。

Conclusion: PANAMA在网络感知决策与多智能体协调之间架起桥梁，推动了数字孪生、无线网络和AI自动化之间的协同发展。

Abstract: Digital Twins (DTs) are transforming industries through advanced data
processing and analysis, positioning the world of DTs, Digital World, as a
cornerstone of nextgeneration technologies including embodied AI. As robotics
and automated systems scale, efficient data-sharing frameworks and robust
algorithms become critical. We explore the pivotal role of data handling in
next-gen networks, focusing on dynamics between application and network
providers (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with
Priority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)
based multi-agent path finding (MAPF). By adopting a Centralized Training with
Decentralized Execution (CTDE) framework and asynchronous actor-learner
architectures, PANAMA accelerates training while enabling autonomous task
execution by embodied AI. Our approach demonstrates superior pathfinding
performance in accuracy, speed, and scalability compared to existing
benchmarks. Through simulations, we highlight optimized data-sharing strategies
for scalable, automated systems, ensuring resilience in complex, real-world
environments. PANAMA bridges the gap between network-aware decision-making and
robust multi-agent coordination, advancing the synergy between DTs, wireless
networks, and AI-driven automation.

</details>


### [65] [Differentiable Adaptive Kalman Filtering via Optimal Transport](https://arxiv.org/abs/2508.07037)
*Yangguang He,Wenhao Li,Minzhe Li,Juan Zhang,Xiangfeng Wang,Bo Jin*

Main category: cs.LG

TL;DR: OTAKNet是一种在线解决方案，用于解决学习型自适应卡尔曼滤波中的噪声统计漂移问题，通过最优传输实现无需地面真实标签或重新训练的在线适应。


<details>
  <summary>Details</summary>
Motivation: 现实环境中，环境因素（如风况变化或电磁干扰）会导致未观测到的噪声统计漂移，显著降低学习型方法的性能。

Method: OTAKNet通过一步预测测量似然建立状态估计与漂移之间的联系，并利用最优传输的几何感知成本和稳定梯度实现在线适应。

Result: 在合成和真实NCLT数据集上，OTAKNet在有限训练数据下表现优于传统模型自适应卡尔曼滤波和离线学习型滤波。

Conclusion: OTAKNet为解决噪声统计漂移问题提供了一种高效且无需重新训练的在线解决方案。

Abstract: Learning-based filtering has demonstrated strong performance in non-linear
dynamical systems, particularly when the statistics of noise are unknown.
However, in real-world deployments, environmental factors, such as changing
wind conditions or electromagnetic interference, can induce unobserved
noise-statistics drift, leading to substantial degradation of learning-based
methods. To address this challenge, we propose OTAKNet, the first online
solution to noise-statistics drift within learning-based adaptive Kalman
filtering. Unlike existing learning-based methods that perform offline
fine-tuning using batch pointwise matching over entire trajectories, OTAKNet
establishes a connection between the state estimate and the drift via one-step
predictive measurement likelihood, and addresses it using optimal transport.
This leverages OT's geometry - aware cost and stable gradients to enable fully
online adaptation without ground truth labels or retraining. We compare OTAKNet
against classical model-based adaptive Kalman filtering and offline
learning-based filtering. The performance is demonstrated on both synthetic and
real-world NCLT datasets, particularly under limited training data.

</details>


### [66] [PROPS: Progressively Private Self-alignment of Large Language Models](https://arxiv.org/abs/2508.06783)
*Noel Teku,Fengwei Tian,Payel Bhattacharjee,Souradip Chakraborty,Amrit Singh Bedi,Ravi Tandon*

Main category: cs.LG

TL;DR: PROPS框架通过多阶段隐私保护对齐LLM，在相同隐私预算下比DP-SGD和RR方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决依赖人类反馈的隐私问题，同时避免过度隐私化对模型效用的影响。

Method: 提出PROPS框架，利用前一阶段私有对齐模型为后续阶段提供标签数据。

Result: PROPS在相同隐私预算下，胜率比DP-SGD高3倍，比RR高2.5倍。

Conclusion: PROPS在隐私保护和模型效用之间取得了更好的平衡。

Abstract: Alignment is a key step in developing Large Language Models (LLMs) using
human feedback to ensure adherence to human values and societal norms.
Dependence on human feedback raises privacy concerns about how much a labeler's
preferences may reveal about their personal values, beliefs, and personality
traits. Existing approaches, such as Differentially Private SGD (DP-SGD),
provide rigorous privacy guarantees by privatizing gradients during fine-tuning
and alignment but can provide more privacy than necessary as human preferences
are tied only to labels of (prompt, response) pairs and can degrade model
utility. This work focuses on LLM alignment with preference-level privacy,
which preserves the privacy of preference labels provided by humans. We propose
PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving
alignment framework where privately aligned models in previous stages can serve
as labelers for supplementing training data in the subsequent stages of
alignment. We present theoretical guarantees for PROPS as well as comprehensive
validation using multiple models (Pythia and GPT) and datasets (AlpacaEval,
Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over
existing methods while still providing high privacy. For the same privacy
budget, alignment via PROPS can achieve up to 3x higher win-rates compared to
DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based
alignment.

</details>


### [67] [PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets](https://arxiv.org/abs/2508.07253)
*Bartlomiej Chybowski,Shima Abdullateef,Hollan Haule,Alfredo Gonzalez-Sulser,Javier Escudero*

Main category: cs.LG

TL;DR: 提出了一种开源机器学习框架，用于跨不同临床数据集的可靠癫痫发作检测，具有高泛化能力和可重复性。


<details>
  <summary>Details</summary>
Motivation: 解决现有癫痫发作检测方法依赖特定数据集优化、泛化能力不足的问题，推动临床应用的普及。

Method: 开发了一个包含自动预处理管道和多数投票机制的框架，评估了模型在数据集内和跨数据集的性能。

Result: 模型在数据集内表现优异（AUC 0.904和0.864），跨数据集泛化能力显著（AUC 0.615和0.762），后处理进一步提升了性能。

Conclusion: 该框架为临床可行的、数据集无关的癫痫发作检测系统奠定了基础，具有广泛应用的潜力。

Abstract: Reliable seizure detection is critical for diagnosing and managing epilepsy,
yet clinical workflows remain dependent on time-consuming manual EEG
interpretation. While machine learning has shown promise, existing approaches
often rely on dataset-specific optimisations, limiting their real-world
applicability and reproducibility. Here, we introduce an innovative,
open-source machine-learning framework that enables robust and generalisable
seizure detection across varied clinical datasets. We evaluate our approach on
two publicly available EEG datasets that differ in patient populations and
electrode configurations. To enhance robustness, the framework incorporates an
automated pre-processing pipeline to standardise data and a majority voting
mechanism, in which multiple models independently assess each second of EEG
before reaching a final decision. We train, tune, and evaluate models within
each dataset, assessing their cross-dataset transferability. Our models achieve
high within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and
0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets
despite differences in EEG setups and populations (AUC 0.615+/-0.039 for models
trained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case)
without any post-processing. Furthermore, a mild post-processing improved the
within-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset
results to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the
potential of, and essential considerations for, deploying our framework in
diverse clinical settings. By making our methodology fully reproducible, we
provide a foundation for advancing clinically viable, dataset-agnostic seizure
detection systems. This approach has the potential for widespread adoption,
complementing rather than replacing expert interpretation, and accelerating
clinical integration.

</details>


### [68] [Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning](https://arxiv.org/abs/2508.06784)
*Junjing Zheng,Chengliang Song,Weidong Jiang,Xinyu Zhang*

Main category: cs.LG

TL;DR: MA-NTAE是一种新型自监督学习模型，通过非线性Tucker分解和Pick-and-Unfold策略，高效处理高维张量数据，解决了传统方法的计算和优化问题。


<details>
  <summary>Details</summary>
Motivation: 高维张量数据在自监督学习中面临维度灾难和计算复杂度高的挑战，传统方法如MLP自编码器依赖展平操作，导致模型过大且难以优化。

Method: 提出MA-NTAE，将经典Tucker分解推广到非线性框架，采用Pick-and-Unfold策略，通过递归展开-编码-折叠操作灵活处理高维张量。

Result: MA-NTAE的计算复杂度随张量阶数线性增长，实验显示其在压缩和聚类任务中优于标准自编码器和现有张量网络。

Conclusion: MA-NTAE为高维张量数据提供了一种高效的非线性建模方法，尤其适用于高阶高维场景。

Abstract: High-dimensional data, particularly in the form of high-order tensors,
presents a major challenge in self-supervised learning. While MLP-based
autoencoders (AE) are commonly employed, their dependence on flattening
operations exacerbates the curse of dimensionality, leading to excessively
large model sizes, high computational overhead, and challenging optimization
for deep structural feature capture. Although existing tensor networks
alleviate computational burdens through tensor decomposition techniques, most
exhibit limited capability in learning non-linear relationships. To overcome
these limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder
(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear
framework and employs a Pick-and-Unfold strategy, facilitating flexible
per-mode encoding of high-order tensors via recursive unfold-encode-fold
operations, effectively integrating tensor structural priors. Notably, MA-NTAE
exhibits linear growth in computational complexity with tensor order and
proportional growth with mode dimensions. Extensive experiments demonstrate
MA-NTAE's performance advantages over standard AE and current tensor networks
in compression and clustering tasks, which become increasingly pronounced for
higher-order, higher-dimensional tensors.

</details>


### [69] [Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles](https://arxiv.org/abs/2508.08034)
*Roksana Yahyaabadi,Ghazal Farhani,Taufiq Rahman,Soodeh Nikan,Abdullah Jirjees,Fadi Araji*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动的方法，结合传统机器学习和深度神经网络，用于预测内燃机、电动汽车和混合动力汽车的瞬时和累计功耗，效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统功耗预测方法在大规模实际应用中不实用，亟需一种可扩展的数据驱动方法。

Method: 利用动力系统动态特征集，结合传统机器学习和深度神经网络（如Transformer和LSTM），预测多种车辆的功耗。

Result: 内燃机瞬时误差低至$10^{-3}$，累计误差低于3%；电动汽车和混合动力车的累计误差分别低于4.1%和2.1%。

Conclusion: 该方法适用于多种车辆类型，但电动汽车和混合动力车因复杂的电源管理需更稳健的模型。

Abstract: Accurate power consumption prediction is crucial for improving efficiency and
reducing environmental impact, yet traditional methods relying on specialized
instruments or rigid physical models are impractical for large-scale,
real-world deployment. This study introduces a scalable data-driven method
using powertrain dynamic feature sets and both traditional machine learning and
deep neural networks to estimate instantaneous and cumulative power consumption
in internal combustion engine (ICE), electric vehicle (EV), and hybrid electric
vehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with
mean absolute error and root mean squared error on the order of $10^{-3}$, and
cumulative errors under 3%. Transformer and long short-term memory models
performed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,
respectively. Results confirm the approach's effectiveness across vehicles and
models. Uncertainty analysis revealed greater variability in EV and HEV
datasets than ICE, due to complex power management, emphasizing the need for
robust models for advanced powertrains.

</details>


### [70] [Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities](https://arxiv.org/abs/2508.06800)
*Rui Liu,Haolin Zuo,Zheng Lian,Hongyu Yuan,Qi Fan*

Main category: cs.LG

TL;DR: 提出了一种名为HARDY-MER的动态课程学习框架，通过评估样本的难度并动态调整训练重点，显著提升了多模态情感识别中缺失模态的处理能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多模态情感识别中的缺失模态时，未能考虑不同样本的重建难度差异，导致对困难样本的处理能力不足。

Method: HARDY-MER框架分为两阶段：1) 通过多视角难度评估机制量化样本重建难度；2) 采用基于检索的动态课程学习策略，动态调整训练重点。

Result: 在基准数据集上的实验表明，HARDY-MER在缺失模态场景中显著优于现有方法。

Conclusion: HARDY-MER通过动态课程学习有效提升了模型对困难样本的处理能力，为多模态情感识别中的缺失模态问题提供了新思路。

Abstract: Missing modalities have recently emerged as a critical research direction in
multimodal emotion recognition (MER). Conventional approaches typically address
this issue through missing modality reconstruction. However, these methods fail
to account for variations in reconstruction difficulty across different
samples, consequently limiting the model's ability to handle hard samples
effectively. To overcome this limitation, we propose a novel Hardness-Aware
Dynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates
in two key stages: first, it estimates the hardness level of each sample, and
second, it strategically emphasizes hard samples during training to enhance
model performance on these challenging instances. Specifically, we first
introduce a Multi-view Hardness Evaluation mechanism that quantifies
reconstruction difficulty by considering both Direct Hardness (modality
reconstruction errors) and Indirect Hardness (cross-modal mutual information).
Meanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy
that dynamically adjusts the training curriculum by retrieving samples with
similar semantic information and balancing the learning focus between easy and
hard instances. Extensive experiments on benchmark datasets demonstrate that
HARDY-MER consistently outperforms existing methods in missing-modality
scenarios. Our code will be made publicly available at
https://github.com/HARDY-MER/HARDY-MER.

</details>


### [71] [Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion](https://arxiv.org/abs/2508.08216)
*Nicole Lai-Tan,Xiao Gu,Marios G. Philiastides,Fani Deligianni*

Main category: cs.LG

TL;DR: 论文提出了一种名为ITSA的新方法，通过结合个体特定的重新中心化、分布匹配和监督旋转对齐，提升脑机接口的跨主体泛化能力。


<details>
  <summary>Details</summary>
Motivation: 脑机接口在个性化音乐干预中具有潜力，但主体间EEG信号的变异性、运动伪影和运动规划差异限制了其泛化能力。

Method: 提出ITSA预对齐策略，结合RCSP和黎曼几何的混合架构，并行和顺序配置以增强分类分离性。

Result: ITSA在跨主体和条件下表现出显著性能提升，并行融合方法优于顺序方法。

Conclusion: ITSA为脑机接口的跨主体泛化提供了有效解决方案，代码将公开。

Abstract: Personalised music-based interventions offer a powerful means of supporting
motor rehabilitation by dynamically tailoring auditory stimuli to provide
external timekeeping cues, modulate affective states, and stabilise gait
patterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for
adapting these interventions across individuals. However, inter-subject
variability in EEG signals, further compounded by movement-induced artefacts
and motor planning differences, hinders the generalisability of BCIs and
results in lengthy calibration processes. We propose Individual Tangent Space
Alignment (ITSA), a novel pre-alignment strategy incorporating subject-specific
recentering, distribution matching, and supervised rotational alignment to
enhance cross-subject generalisation. Our hybrid architecture fuses Regularised
Common Spatial Patterns (RCSP) with Riemannian geometry in parallel and
sequential configurations, improving class separability while maintaining the
geometric structure of covariance matrices for robust statistical computation.
Using leave-one-subject-out cross-validation, `ITSA' demonstrates significant
performance improvements across subjects and conditions. The parallel fusion
approach shows the greatest enhancement over its sequential counterpart, with
robust performance maintained across varying data conditions and electrode
configurations. The code will be made publicly available at the time of
publication.

</details>


### [72] [Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation](https://arxiv.org/abs/2508.06806)
*Xiao Huang,Xu Liu,Enze Zhang,Tong Yu,Shuai Li*

Main category: cs.LG

TL;DR: 提出了一种新的数据增强方法CFDG，用于离线到在线强化学习，通过无分类器扩散生成提升数据质量，显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的数据与在线数据存在分布差距，限制了性能。

Method: 采用无分类器引导扩散生成（CFDG）和重加权方法，提升生成数据与在线数据的一致性。

Result: 在D4RL基准测试中，CFDG比现有方法平均提升15%的性能。

Conclusion: CFDG是一种通用且高效的方法，可显著提升离线到在线强化学习的性能。

Abstract: Offline-to-online Reinforcement Learning (O2O RL) aims to perform online
fine-tuning on an offline pre-trained policy to minimize costly online
interactions. Existing work used offline datasets to generate data that conform
to the online data distribution for data augmentation. However, generated data
still exhibits a gap with the online data, limiting overall performance. To
address this, we propose a new data augmentation approach, Classifier-Free
Diffusion Generation (CFDG). Without introducing additional classifier training
overhead, CFDG leverages classifier-free guidance diffusion to significantly
enhance the generation quality of offline and online data with different
distributions. Additionally, it employs a reweighting method to enable more
generated data to align with the online data, enhancing performance while
maintaining the agent's stability. Experimental results show that CFDG
outperforms replaying the two data types or using a standard diffusion model to
generate new data. Our method is versatile and can be integrated with existing
offline-to-online RL algorithms. By implementing CFDG to popular methods IQL,
PEX and APL, we achieve a notable 15% average improvement in empirical
performance on the D4RL benchmark such as MuJoCo and AntMaze.

</details>


### [73] [Technical Report: Full-Stack Fine-Tuning for the Q Programming Language](https://arxiv.org/abs/2508.06813)
*Brendan R. Hogan,Will Brown,Adel Boyarsky,Anderson Schneider,Yuriy Nevmyvaka*

Main category: cs.LG

TL;DR: 提出了一种开源方法，将大语言模型（LLMs）适配到Q编程语言，解决了其在小众语言和私有领域应用的挑战。通过预训练、监督微调和强化学习，训练了一系列模型，并在Q评测数据集上表现优异，超越了主流前沿模型。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在小众编程语言（如Q语言）和私有领域应用中的不足，填补其在非主流语言任务上的性能差距。

Method: 构建Q语言的Leetcode风格评测数据集，基于Qwen-2.5系列模型进行预训练、监督微调和强化学习，训练了五种参数规模的模型（1.5B至32B）。

Result: 最佳模型在Q评测数据集上的pass@1准确率达到59%，超越Claude Opus-4（29.5%），所有模型均优于GPT-4.1。

Conclusion: 提出的方法不仅适用于Q语言，还可推广到其他依赖软性或主观信号的任务，为小众领域LLMs应用提供了可行方案。

Abstract: Even though large language models are becoming increasingly capable, it is
still unreasonable to expect them to excel at tasks that are under-represented
on the Internet. Leveraging LLMs for specialized applications, particularly in
niche programming languages and private domains, remains challenging and
largely unsolved. In this work, we address this gap by presenting a
comprehensive, open-source approach for adapting LLMs to the Q programming
language, a popular tool in quantitative finance that is much less present on
the Internet compared to Python, C, Java, and other ``mainstream" languages and
is therefore not a strong suit of general-purpose AI models. We introduce a new
Leetcode style evaluation dataset for Q, benchmark major frontier models on the
dataset, then do pretraining, supervised fine tuning, and reinforcement
learning to train a suite of reasoning and non-reasoning models based on the
Qwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our
best model achieves a pass@1 accuracy of 59 percent on our Q benchmark,
surpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.
Additionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.
In addition to releasing models, code, and data, we provide a detailed
blueprint for dataset construction, model pretraining, supervised fine-tuning,
and reinforcement learning. Our methodology is broadly applicable, and we
discuss how these techniques can be extended to other tasks, including those
where evaluation may rely on soft or subjective signals.

</details>


### [74] [Who's the Evil Twin? Differential Auditing for Undesired Behavior](https://arxiv.org/abs/2508.06827)
*Ishwar Balappanawar,Venkata Hasith Vattikuti,Greta Kintzley,Ronan Azimi-Mancel,Satvik Golechha*

Main category: cs.LG

TL;DR: 论文探讨了通过对抗游戏检测神经网络中的隐藏行为，红队训练两种模型，蓝队尝试识别被篡改的模型。实验表明基于对抗攻击的方法效果最佳，而LLM审计需要额外提示。


<details>
  <summary>Details</summary>
Motivation: 检测神经网络中的隐藏行为具有挑战性，尤其是缺乏先验知识或存在对抗性混淆时。

Method: 通过对抗游戏框架，红队训练两种模型（良性数据和含隐藏有害行为的数据），蓝队尝试识别被篡改的模型。实验使用了CNN和多种蓝队策略。

Result: 基于对抗攻击的方法准确率最高（100%），其他方法表现不一。LLM审计需要额外提示才能有效。

Conclusion: 对抗攻击方法在检测隐藏行为中表现优异，LLM审计需依赖提示。开源审计游戏以促进更好的审计设计。

Abstract: Detecting hidden behaviors in neural networks poses a significant challenge
due to minimal prior knowledge and potential adversarial obfuscation. We
explore this problem by framing detection as an adversarial game between two
teams: the red team trains two similar models, one trained solely on benign
data and the other trained on data containing hidden harmful behavior, with the
performance of both being nearly indistinguishable on the benign dataset. The
blue team, with limited to no information about the harmful behaviour, tries to
identify the compromised model. We experiment using CNNs and try various blue
team strategies, including Gaussian noise analysis, model diffing, integrated
gradients, and adversarial attacks under different levels of hints provided by
the red team. Results show high accuracy for adversarial-attack-based methods
(100\% correct prediction, using hints), which is very promising, whilst the
other techniques yield more varied performance. During our LLM-focused rounds,
we find that there are not many parallel methods that we could apply from our
study with CNNs. Instead, we find that effective LLM auditing methods require
some hints about the undesired distribution, which can then used in standard
black-box and open-weight methods to probe the models further and reveal their
misalignment. We open-source our auditing games (with the model and data) and
hope that our findings contribute to designing better audits.

</details>


### [75] [Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning](https://arxiv.org/abs/2508.06871)
*Aleksandar Todorov,Juan Cardenas-Cartagena,Rafael F. Cunha,Marco Zullich,Matthia Sabatelli*

Main category: cs.LG

TL;DR: 研究探讨了稀疏化方法（GMP和SET）如何提升多任务强化学习中的可塑性，并改善性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习中可塑性丧失的问题，尤其是在多任务环境下。

Method: 采用Gradual Magnitude Pruning (GMP)和Sparse Evolutionary Training (SET)方法，并在不同MTRL架构上评估。

Result: 稀疏化方法有效缓解可塑性退化，提升多任务性能，优于密集基线和其他方法。

Conclusion: 动态稀疏化是提升MTRL系统适应性的有效工具，但需结合具体场景。

Abstract: Plasticity loss, a diminishing capacity to adapt as training progresses, is a
critical challenge in deep reinforcement learning. We examine this issue in
multi-task reinforcement learning (MTRL), where higher representational
flexibility is crucial for managing diverse and potentially conflicting task
demands. We systematically explore how sparsification methods, particularly
Gradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance
plasticity and consequently improve performance in MTRL agents. We evaluate
these approaches across distinct MTRL architectures (shared backbone, Mixture
of Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,
comparing against dense baselines, and a comprehensive range of alternative
plasticity-inducing or regularization methods. Our results demonstrate that
both GMP and SET effectively mitigate key indicators of plasticity degradation,
such as neuron dormancy and representational collapse. These plasticity
improvements often correlate with enhanced multi-task performance, with sparse
agents frequently outperforming dense counterparts and achieving competitive
results against explicit plasticity interventions. Our findings offer insights
into the interplay between plasticity, network sparsity, and MTRL designs,
highlighting dynamic sparsification as a robust but context-sensitive tool for
developing more adaptable MTRL systems.

</details>


### [76] [Conformal Prediction and Trustworthy AI](https://arxiv.org/abs/2508.06885)
*Anthony Bellotti,Xindi Zhao*

Main category: cs.LG

TL;DR: 本文回顾了保形预测在可信AI中的潜力，探讨了其超越边际有效性的应用，如泛化风险和AI治理，并通过实验展示了其作为校准预测器和偏差识别工具的作用。


<details>
  <summary>Details</summary>
Motivation: 保形预测作为一种提供置信度保证的机器学习方法，近年来在不确定性量化中受到广泛关注。其可靠的校准特性使其成为构建可信AI的重要工具，尤其在泛化风险和AI治理方面具有潜力。

Method: 文章通过实验和示例展示了保形预测的应用，包括作为校准预测器和用于偏差识别与缓解。

Result: 保形预测不仅提供了边际有效性，还能有效应对泛化风险和AI治理问题，实验证明了其作为校准预测器和偏差识别工具的实际效果。

Conclusion: 保形预测在可信AI领域具有广泛潜力，能够通过其校准特性和偏差缓解能力为AI的可靠性和公平性提供支持。

Abstract: Conformal predictors are machine learning algorithms developed in the 1990's
by Gammerman, Vovk, and their research team, to provide set predictions with
guaranteed confidence level. Over recent years, they have grown in popularity
and have become a mainstream methodology for uncertainty quantification in the
machine learning community. From its beginning, there was an understanding that
they enable reliable machine learning with well-calibrated uncertainty
quantification. This makes them extremely beneficial for developing trustworthy
AI, a topic that has also risen in interest over the past few years, in both
the AI community and society more widely. In this article, we review the
potential for conformal prediction to contribute to trustworthy AI beyond its
marginal validity property, addressing problems such as generalization risk and
AI governance. Experiments and examples are also provided to demonstrate its
use as a well-calibrated predictor and for bias identification and mitigation.

</details>


### [77] [QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting](https://arxiv.org/abs/2508.06915)
*Shichao Ma,Zhengyang Zhou,Qihe Huang,Binwu Wang,Kuo Yang,Huan Li,Yang Wang*

Main category: cs.LG

TL;DR: 论文提出QuiZSF框架，结合检索增强生成（RAG）与时间序列预训练模型（TSPMs），提升零样本时间序列预测（ZSF）性能。


<details>
  <summary>Details</summary>
Motivation: 传统模型在数据稀缺场景（如领域迁移或极端条件）下难以实现零样本时间序列预测，而现有TSPMs缺乏动态整合外部知识的机制。

Method: 提出QuiZSF框架，包括ChronoRAG Base（CRB）用于存储与检索、Multi-grained Series Interaction Learner（MSIL）提取特征、Model Cooperation Coherer（MCC）对齐知识与非LLM/LLM基TSPMs。

Result: QuiZSF在75%（非LLM基）和87.5%（LLM基）预测场景中排名Top1，同时保持高效内存和推理时间。

Conclusion: QuiZSF通过结合RAG与TSPMs，显著提升零样本时间序列预测性能，适用于多种场景。

Abstract: Time series forecasting has become increasingly important to empower diverse
applications with streaming data. Zero-shot time-series forecasting (ZSF),
particularly valuable in data-scarce scenarios, such as domain transfer or
forecasting under extreme conditions, is difficult for traditional models to
deal with. While time series pre-trained models (TSPMs) have demonstrated
strong performance in ZSF, they often lack mechanisms to dynamically
incorporate external knowledge. Fortunately, emerging retrieval-augmented
generation (RAG) offers a promising path for injecting such knowledge on
demand, yet they are rarely integrated with TSPMs. To leverage the strengths of
both worlds, we introduce RAG into TSPMs to enhance zero-shot time series
forecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series
Forecaster), a lightweight and modular framework that couples efficient
retrieval with representation learning and model adaptation for ZSF.
Specifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)
for scalable time-series storage and domain-aware retrieval, introduce a
Multi-grained Series Interaction Learner (MSIL) to extract fine- and
coarse-grained relational features, and develop a dual-branch Model Cooperation
Coherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM
based and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM
based and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and
87.5% of prediction settings, while maintaining high efficiency in memory and
inference time.

</details>


### [78] [Class Unbiasing for Generalization in Medical Diagnosis](https://arxiv.org/abs/2508.06943)
*Lishi Zuo,Man-Wai Mak,Lu Yi,Youzhi Tu*

Main category: cs.LG

TL;DR: 论文提出了一种解决医学诊断中类别特征偏差和类别不平衡的方法，通过类别不平等损失和类别加权优化目标，提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学诊断模型可能因类别特征偏差（依赖与部分类别强相关的特征）和类别不平衡导致性能偏差和泛化能力差。

Method: 提出类别不平等损失和类别加权的分布鲁棒优化目标，平衡正负类别样本的分类损失贡献，并优化表现不佳的类别。

Result: 实验证明类别特征偏差会损害模型性能，而所提方法能有效缓解偏差和不平衡，提升泛化能力。

Conclusion: 该方法通过同时解决类别特征偏差和类别不平衡，显著改善了模型的泛化性能。

Abstract: Medical diagnosis might fail due to bias. In this work, we identified
class-feature bias, which refers to models' potential reliance on features that
are strongly correlated with only a subset of classes, leading to biased
performance and poor generalization on other classes. We aim to train a
class-unbiased model (Cls-unbias) that mitigates both class imbalance and
class-feature bias simultaneously. Specifically, we propose a class-wise
inequality loss which promotes equal contributions of classification loss from
positive-class and negative-class samples. We propose to optimize a class-wise
group distributionally robust optimization objective-a class-weighted training
objective that upweights underperforming classes-to enhance the effectiveness
of the inequality loss under class imbalance. Through synthetic and real-world
datasets, we empirically demonstrate that class-feature bias can negatively
impact model performance. Our proposed method effectively mitigates both
class-feature bias and class imbalance, thereby improving the model's
generalization ability.

</details>


### [79] [AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance](https://arxiv.org/abs/2508.06944)
*Lixuan He,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为AMFT的单阶段算法，通过隐式奖励理论动态平衡监督微调（SFT）和强化学习（RL），解决了传统两阶段方法中的灾难性遗忘和探索-模仿权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段方法（SFT+RL）存在灾难性遗忘和探索-模仿权衡问题，而现有单阶段方法缺乏动态平衡机制。

Method: 提出AMFT算法，通过元梯度自适应权重控制器动态优化SFT和RL的平衡，并结合策略熵正则化以提高稳定性。

Result: 在数学推理、抽象视觉推理和视觉语言导航等任务上，AMFT取得了最先进的性能，并表现出更强的泛化能力。

Conclusion: AMFT通过动态平衡SFT和RL，提供了一种更有效和稳定的LLM对齐范式。

Abstract: Large Language Models (LLMs) are typically fine-tuned for reasoning tasks
through a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by
Reinforcement Learning (RL), a process fraught with catastrophic forgetting and
suboptimal trade-offs between imitation and exploration. Recent single-stage
methods attempt to unify SFT and RL using heuristics, but lack a principled
mechanism for dynamically balancing the two paradigms. In this paper, we
reframe this challenge through the theoretical lens of \textbf{implicit
rewards}, viewing SFT and RL not as distinct methods but as complementary
reward signals. We introduce \textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel
single-stage algorithm that learns the optimal balance between SFT's implicit,
path-level reward and RL's explicit, outcome-based reward. The core of AMFT is
a \textbf{meta-gradient adaptive weight controller} that treats the SFT-RL
balance as a learnable parameter, dynamically optimizing it to maximize
long-term task performance. This forward-looking approach, regularized by
policy entropy for stability, autonomously discovers an effective training
curriculum. We conduct a comprehensive evaluation on challenging benchmarks
spanning mathematical reasoning, abstract visual reasoning (General Points),
and vision-language navigation (V-IRL). AMFT consistently establishes a new
state-of-the-art and demonstrats superior generalization on out-of-distribution
(OOD) tasks. Ablation studies and training dynamic analysis confirm that the
meta-learning controller is crucial for AMFT's stability, sample efficiency,
and performance, offering a more principled and effective paradigm for LLM
alignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.

</details>


### [80] [BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity](https://arxiv.org/abs/2508.06953)
*Shiwei Li,Xiandi Luo,Haozhao Wang,Xing Tang,Ziqiang Cui,Dugang Liu,Yuhua Li,Xiuqiang He,Ruixuan Li*

Main category: cs.LG

TL;DR: BoRA是一种改进的低秩适应方法，通过块矩阵乘法和块间对角矩阵提升LoRA权重秩，仅需少量额外参数。


<details>
  <summary>Details</summary>
Motivation: LoRA在大型语言模型中广泛使用，但增加秩会显著增加可训练参数。BoRA旨在以更少的参数提升秩。

Method: BoRA将LoRA的权重矩阵分解为块矩阵，并引入块间对角矩阵，提升秩的同时仅需少量额外参数。

Result: 实验表明BoRA在多个数据集和模型上表现优越，且具有可扩展性。

Conclusion: BoRA通过块多样化和对角矩阵设计，高效提升了LoRA的性能，适用于大规模模型。

Abstract: Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method
widely used in large language models (LLMs). It approximates the update of a
pretrained weight matrix $W\in\mathbb{R}^{m\times n}$ by the product of two
low-rank matrices, $BA$, where $A \in\mathbb{R}^{r\times n}$ and
$B\in\mathbb{R}^{m\times r} (r\ll\min\{m,n\})$. Increasing the dimension $r$
can raise the rank of LoRA weights (i.e., $BA$), which typically improves
fine-tuning performance but also significantly increases the number of
trainable parameters. In this paper, we propose Block Diversified Low-Rank
Adaptation (BoRA), which improves the rank of LoRA weights with a small number
of additional parameters. Specifically, BoRA treats the product $BA$ as a block
matrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along
the columns and rows, respectively (i.e., $A=[A_1,\dots,A_b]$ and
$B=[B_1,\dots,B_b]^\top$). Consequently, the product $BA$ becomes the
concatenation of the block products $B_iA_j$ for $i,j\in[b]$. To enhance the
diversity of different block products, BoRA introduces a unique diagonal matrix
$\Sigma_{i,j} \in \mathbb{R}^{r\times r}$ for each block multiplication,
resulting in $B_i \Sigma_{i,j} A_j$. By leveraging these block-wise diagonal
matrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only
requiring $b^2r$ additional parameters. Extensive experiments across multiple
datasets and models demonstrate the superiority of BoRA, and ablation studies
further validate its scalability.

</details>


### [81] [Can Multitask Learning Enhance Model Explainability?](https://arxiv.org/abs/2508.06966)
*Hiba Najjar,Bushra Alshbib,Andreas Dengel*

Main category: cs.LG

TL;DR: 通过多任务学习利用卫星数据的多模态性，提升模型性能并增强可解释性，同时避免数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 多模态学习网络虽能提升性能，但牺牲了可解释性。本研究旨在通过多任务学习，利用卫星数据的多模态性，既提升性能又增强模型行为的解释性。

Method: 将某些模态作为附加目标与主任务一起预测，而非额外输入。利用卫星数据的丰富信息作为输入模态。

Result: 在数据稀缺时，附加模态无需收集；模型性能与多模态基线相当或更优；主任务预测误差可通过辅助任务行为解释。

Conclusion: 该方法在分割、分类和回归任务中均表现出高效性，代码已开源。

Abstract: Remote sensing provides satellite data in diverse types and formats. The
usage of multimodal learning networks exploits this diversity to improve model
performance, except that the complexity of such networks comes at the expense
of their interpretability. In this study, we explore how modalities can be
leveraged through multitask learning to intrinsically explain model behavior.
In particular, instead of additional inputs, we use certain modalities as
additional targets to be predicted along with the main task. The success of
this approach relies on the rich information content of satellite data, which
remains as input modalities. We show how this modeling context provides
numerous benefits: (1) in case of data scarcity, the additional modalities do
not need to be collected for model inference at deployment, (2) the model
performance remains comparable to the multimodal baseline performance, and in
some cases achieves better scores, (3) prediction errors in the main task can
be explained via the model behavior in the auxiliary task(s). We demonstrate
the efficiency of our approach on three datasets, including segmentation,
classification, and regression tasks. Code available at
git.opendfki.de/hiba.najjar/mtl_explainability/.

</details>


### [82] [Structure-Preserving Digital Twins via Conditional Neural Whitney Forms](https://arxiv.org/abs/2508.06981)
*Brooks Kinch,Benjamin Shaffer,Elizabeth Armstrong,Michael Meehan,John Hewson,Nathaniel Trask*

Main category: cs.LG

TL;DR: 提出了一种基于条件潜变量Z的结构保持降阶有限元模型框架，用于构建实时数字孪生体。该方法结合注意力机制和有限元外微积分（FEEC），确保数值适定性和守恒量的精确保持。


<details>
  <summary>Details</summary>
Motivation: 解决数据稀疏或优化误差下的实时数字孪生体构建问题，支持闭环推理和传感器数据校准。

Method: 使用条件注意力机制学习降阶有限元基和非线性守恒律，结合FEEC框架，非侵入式集成传统有限元技术。

Result: 在复杂几何和稀疏数据（25次LES模拟）下实现准确预测，包括湍流过渡，实时推理速度提升3.1x10^8倍。

Conclusion: 该框架高效且通用，适用于复杂几何和实时应用，开源实现已发布。

Abstract: We present a framework for constructing real-time digital twins based on
structure-preserving reduced finite element models conditioned on a latent
variable Z. The approach uses conditional attention mechanisms to learn both a
reduced finite element basis and a nonlinear conservation law within the
framework of finite element exterior calculus (FEEC). This guarantees numerical
well-posedness and exact preservation of conserved quantities, regardless of
data sparsity or optimization error. The conditioning mechanism supports
real-time calibration to parametric variables, allowing the construction of
digital twins which support closed loop inference and calibration to sensor
data. The framework interfaces with conventional finite element machinery in a
non-invasive manner, allowing treatment of complex geometries and integration
of learned models with conventional finite element techniques.
  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,
and a complex battery thermal runaway problem. The method achieves accurate
predictions on complex geometries with sparse data (25 LES simulations),
including capturing the transition to turbulence and achieving real-time
inference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source
implementation is available on GitHub.

</details>


### [83] [UniMove: A Unified Model for Multi-city Human Mobility Prediction](https://arxiv.org/abs/2508.06986)
*Chonghua Han,Yuan Yuan,Yukun Liu,Jingtao Ding,Jie Feng,Yong Li*

Main category: cs.LG

TL;DR: UniMove是一个统一的多城市人类移动预测模型，通过通用空间表示和异构模式建模，显著提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 人类移动预测对城市规划和服务优化至关重要，但现有模型因城市异质性需单独训练，缺乏统一解决方案。

Method: 提出双塔架构（轨迹塔和位置塔）和MoE Transformer块，实现通用空间编码和异构模式建模。

Result: 在多城市数据集上，UniMove通过联合训练提升预测准确性10.2%。

Conclusion: UniMove是实现人类移动预测基础模型的关键进展，支持统一架构和多城市数据增强。

Abstract: Human mobility prediction is vital for urban planning, transportation
optimization, and personalized services. However, the inherent randomness,
non-uniform time intervals, and complex patterns of human mobility, compounded
by the heterogeneity introduced by varying city structures, infrastructure, and
population densities, present significant challenges in modeling. Existing
solutions often require training separate models for each city due to distinct
spatial representations and geographic coverage. In this paper, we propose
UniMove, a unified model for multi-city human mobility prediction, addressing
two challenges: (1) constructing universal spatial representations for
effective token sharing across cities, and (2) modeling heterogeneous mobility
patterns from varying city characteristics. We propose a trajectory-location
dual-tower architecture, with a location tower for universal spatial encoding
and a trajectory tower for sequential mobility modeling. We also design MoE
Transformer blocks to adaptively select experts to handle diverse movement
patterns. Extensive experiments across multiple datasets from diverse cities
demonstrate that UniMove truly embodies the essence of a unified model. By
enabling joint training on multi-city data with mutual data enhancement, it
significantly improves mobility prediction accuracy by over 10.2\%. UniMove
represents a key advancement toward realizing a true foundational model with a
unified architecture for human mobility. We release the implementation at
https://github.com/tsinghua-fib-lab/UniMove/.

</details>


### [84] [A Comparative Study of Feature Selection in Tsetlin Machines](https://arxiv.org/abs/2508.06991)
*Vojtech Halenka,Ole-Christoffer Granmo,Lei Jiao,Per-Arne Andersen*

Main category: cs.LG

TL;DR: 该论文研究了特征选择（FS）在Tsetlin机器（TM）中的应用，评估了多种FS方法，包括经典方法和新兴解释方法，并提出了一种新的基于TM内部评分器的FS方法。


<details>
  <summary>Details</summary>
Motivation: TM缺乏特征重要性评估工具，研究旨在填补这一空白，提升TM的模型解释性和性能。

Method: 研究采用了多种FS技术，包括经典过滤和嵌入方法，以及SHAP、LIME等后解释方法，并提出了一种基于TM子句权重和Tsetlin自动机状态的新型评分器。

Result: 实验结果表明，TM内部评分器不仅性能优异，还能利用子句的可解释性揭示特征交互模式，且计算成本更低。

Conclusion: 该研究为TM中的FS建立了首个全面基线，并为开发专门的TM解释技术奠定了基础。

Abstract: Feature Selection (FS) is crucial for improving model interpretability,
reducing complexity, and sometimes for enhancing accuracy. The recently
introduced Tsetlin machine (TM) offers interpretable clause-based learning, but
lacks established tools for estimating feature importance. In this paper, we
adapt and evaluate a range of FS techniques for TMs, including classical filter
and embedded methods as well as post-hoc explanation methods originally
developed for neural networks (e.g., SHAP and LIME) and a novel family of
embedded scorers derived from TM clause weights and Tsetlin automaton (TA)
states. We benchmark all methods across 12 datasets, using evaluation
protocols, like Remove and Retrain (ROAR) strategy and Remove and Debias
(ROAD), to assess causal impact. Our results show that TM-internal scorers not
only perform competitively but also exploit the interpretability of clauses to
reveal interacting feature patterns. Simpler TM-specific scorers achieve
similar accuracy retention at a fraction of the computational cost. This study
establishes the first comprehensive baseline for FS in TM and paves the way for
developing specialized TM-specific interpretability techniques.

</details>


### [85] [Conformal Set-based Human-AI Complementarity with Multiple Experts](https://arxiv.org/abs/2508.06997)
*Helbert Paat,Guohao Shen*

Main category: cs.LG

TL;DR: 该论文研究了在多专家场景下，如何通过贪心算法选择特定实例的相关专家子集，以提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单专家场景，而多专家协作可能带来更好的分类效果，但需解决如何选择相关专家的问题。

Method: 提出一种贪心算法，利用保形预测集选择专家子集，并在CIFAR-10H和ImageNet-16H数据集上验证。

Result: 贪心算法能选择接近最优的专家子集，显著提升多专家分类性能。

Conclusion: 多专家协作中，实例相关的专家子集选择是关键，贪心算法为此提供了有效解决方案。

Abstract: Decision support systems are designed to assist human experts in
classification tasks by providing conformal prediction sets derived from a
pre-trained model. This human-AI collaboration has demonstrated enhanced
classification performance compared to using either the model or the expert
independently. In this study, we focus on the selection of instance-specific
experts from a pool of multiple human experts, contrasting it with existing
research that typically focuses on single-expert scenarios. We characterize the
conditions under which multiple experts can benefit from the conformal sets.
With the insight that only certain experts may be relevant for each instance,
we explore the problem of subset selection and introduce a greedy algorithm
that utilizes conformal sets to identify the subset of expert predictions that
will be used in classifying an instance. This approach is shown to yield better
performance compared to naive methods for human subset selection. Based on real
expert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation
study indicates that our proposed greedy algorithm achieves near-optimal
subsets, resulting in improved classification performance among multiple
experts.

</details>


### [86] [TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations](https://arxiv.org/abs/2508.07016)
*Jianfei Wu,Wenmian Yang,Bingning Liu,Weijia Jia*

Main category: cs.LG

TL;DR: 提出TLCCSP框架，通过整合时滞交叉相关序列提升预测精度，使用SSDTW算法和对比学习编码器，显著降低MSE并减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型常忽略时滞交叉相关性，而这对捕捉复杂时间关系至关重要。

Method: 提出TLCCSP框架，结合SSDTW算法捕捉时滞相关性，并使用对比学习编码器高效近似SSDTW距离。

Result: 在天气、金融和房地产数据集上，SSDTW和对比学习编码器显著降低MSE，计算时间减少99%。

Conclusion: TLCCSP框架有效提升了时间序列预测的准确性和效率，适用于多种领域。

Abstract: Time series forecasting is critical across various domains, such as weather,
finance and real estate forecasting, as accurate forecasts support informed
decision-making and risk mitigation. While recent deep learning models have
improved predictive capabilities, they often overlook time-lagged
cross-correlations between related sequences, which are crucial for capturing
complex temporal relationships. To address this, we propose the Time-Lagged
Cross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances
forecasting accuracy by effectively integrating time-lagged cross-correlated
sequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)
algorithm to capture lagged correlations and a contrastive learning-based
encoder to efficiently approximate SSDTW distances.
  Experimental results on weather, finance and real estate time series datasets
demonstrate the effectiveness of our framework. On the weather dataset, SSDTW
reduces mean squared error (MSE) by 16.01% compared with single-sequence
methods, while the contrastive learning encoder (CLE) further decreases MSE by
17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE
reduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by
21.29% and 8.62%, respectively. Additionally, the contrastive learning approach
decreases SSDTW computational time by approximately 99%, ensuring scalability
and real-time applicability across multiple time series forecasting tasks.

</details>


### [87] [A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling](https://arxiv.org/abs/2508.07032)
*Tiantian He,Keyue Jiang,An Zhao,Anna Schroder,Elinor Thompson,Sonja Soskic,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.LG

TL;DR: 提出了一种新型阶段感知的Mixture of Experts（MoE）框架，用于建模神经退行性疾病的动态进展，结合了时间依赖的专家权重和异构图神经扩散模型。


<details>
  <summary>Details</summary>
Motivation: 解决神经退行性疾病进展建模中的两个主要挑战：纵向数据稀缺和病理机制在不同阶段的复杂交互。

Method: 采用阶段感知的MoE框架，结合时间依赖的专家权重、异构图神经扩散模型（IGND）和局部神经反应模块。

Result: 模型能够动态整合不同阶段的病理机制，揭示了早期阶段图相关过程更显著，而后期其他未知物理过程占主导。

Conclusion: IGND-MoE模型为理解神经退行性疾病的阶段特异性机制提供了新方法，并具有临床意义。

Abstract: The long-term progression of neurodegenerative diseases is commonly
conceptualized as a spatiotemporal diffusion process that consists of a graph
diffusion process across the structural brain connectome and a localized
reaction process within brain regions. However, modeling this progression
remains challenging due to 1) the scarcity of longitudinal data obtained
through irregular and infrequent subject visits and 2) the complex interplay of
pathological mechanisms across brain regions and disease stages, where
traditional models assume fixed mechanisms throughout disease progression. To
address these limitations, we propose a novel stage-aware Mixture of Experts
(MoE) framework that explicitly models how different contributing mechanisms
dominate at different disease stages through time-dependent expert
weighting.Data-wise, we utilize an iterative dual optimization method to
properly estimate the temporal position of individual observations,
constructing a co hort-level progression trajectory from irregular snapshots.
Model-wise, we enhance the spatial component with an inhomogeneous graph neural
diffusion model (IGND) that allows diffusivity to vary based on node states and
time, providing more flexible representations of brain networks. We also
introduce a localized neural reaction module to capture complex dynamics beyond
standard processes.The resulting IGND-MoE model dynamically integrates these
components across temporal states, offering a principled way to understand how
stage-specific pathological mechanisms contribute to progression. The
stage-wise weights yield novel clinical insights that align with literature,
suggesting that graph-related processes are more influential at early stages,
while other unknown physical processes become dominant later on.

</details>


### [88] [Membership and Memorization in LLM Knowledge Distillation](https://arxiv.org/abs/2508.07054)
*Ziqi Zhang,Ali Shahin Shamsabadi,Hanxiao Lu,Yifeng Cai,Hamed Haddadi*

Main category: cs.LG

TL;DR: 本文系统分析了六种大型语言模型（LLM）知识蒸馏（KD）技术中的隐私风险，发现所有现有方法均存在成员和记忆隐私风险，但风险程度因技术而异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示知识蒸馏过程中从教师模型到学生模型的隐私泄露问题，尤其是在教师模型基于私有数据训练时。

Method: 通过指令微调设置，覆盖七种NLP任务，使用三种教师模型家族（GPT-2、LLAMA-2、OPT）和不同规模的学生模型，分析KD目标函数、学生训练数据和NLP任务对隐私风险的影响。

Result: 所有现有LLM KD方法均存在隐私风险，但风险程度因技术不同而异；记忆与成员隐私风险之间存在显著不一致；不同模块的隐私风险差异较大。

Conclusion: 知识蒸馏技术在隐私保护方面存在显著风险，需进一步研究以优化隐私安全。

Abstract: Recent advances in Knowledge Distillation (KD) aim to mitigate the high
computational demands of Large Language Models (LLMs) by transferring knowledge
from a large ''teacher'' to a smaller ''student'' model. However, students may
inherit the teacher's privacy when the teacher is trained on private data. In
this work, we systematically characterize and investigate membership and
memorization privacy risks inherent in six LLM KD techniques. Using
instruction-tuning settings that span seven NLP tasks, together with three
teacher model families (GPT-2, LLAMA-2, and OPT), and various size student
models, we demonstrate that all existing LLM KD approaches carry membership and
memorization privacy risks from the teacher to its students. However, the
extent of privacy risks varies across different KD techniques. We
systematically analyse how key LLM KD components (KD objective functions,
student training data and NLP tasks) impact such privacy risks. We also
demonstrate a significant disagreement between memorization and membership
privacy risks of LLM KD techniques. Finally, we characterize per-block privacy
risk and demonstrate that the privacy risk varies across different blocks by a
large margin.

</details>


### [89] [Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation](https://arxiv.org/abs/2508.07075)
*Stanley Ngugi*

Main category: cs.LG

TL;DR: 论文提出了一种“先遗忘后学习”的策略，通过参数高效微调技术（$IA^3$）解决大语言模型（LLMs）在动态知识更新中的冲突问题，显著提高了新知识的准确性和旧知识的遗忘率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在动态知识更新时难以处理新旧知识冲突，导致新知识难以采纳和无关知识的严重遗忘。

Method: 采用两阶段策略：1) 定位冲突知识的内部编码组件；2) 使用$IA^3$技术进行参数高效微调。

Result: 实验表明，该方法在新知识准确率（98.50%）和旧知识遗忘率（96.00%）上表现优异，同时显著缓解了灾难性遗忘问题。

Conclusion: 该方法实现了精准、局部化和安全的知识管理，为大语言模型的知识更新提供了新思路。

Abstract: Large Language Models (LLMs) struggle with dynamic knowledge updates,
especially when new information conflicts with deeply embedded facts. Such
conflicting factual edits often lead to two critical issues: resistance to
adopting the new fact and severe catastrophic forgetting of unrelated
knowledge. This paper introduces and evaluates a novel "unlearn-then-learn"
strategy for precise knowledge editing in LLMs, leveraging the
parameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting
and Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach
is powered by an initial circuit localization phase that identifies and targets
the specific internal components responsible for encoding the conflicting fact.
Through a rigorous experimental methodology on
microsoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically
informed two-stage approach achieves near-perfect accuracy (98.50%) for the
new, modulated fact while simultaneously effectively suppressing the original
conflicting fact (96.00% forget rate). Critically, our strategy exhibits
unprecedented localization (72.00% F_control accuracy), dramatically mitigating
catastrophic forgetting observed in direct fine-tuning approaches (which showed
as low as ~20% F_control accuracy), a direct benefit of our targeted
interpretability-guided intervention. Furthermore, qualitative analysis reveals
a nuanced mechanism of "soft forgetting," where original knowledge is
suppressed from default retrieval but remains latent and conditionally
accessible, enhancing model safety and control. These findings represent a
significant advancement towards precise, localized, and safe knowledge
management in compact LLMs.

</details>


### [90] [Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework](https://arxiv.org/abs/2508.07085)
*N Harshit,K Mounvik*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer和Autoencoder的混合框架，用于在线检测概念漂移，并通过Trust Score方法提高检测敏感性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 概念漂移（数据分布的变化）会显著降低模型性能，现有检测方法多为被动且对早期检测不敏感。

Method: 使用Transformer和Autoencoder建模复杂时间动态，结合Trust Score方法（包括统计和重构指标、预测不确定性、规则违反和分类器误差趋势）。

Result: 在航空乘客数据集上，该方法比基线方法更早、更敏感地检测到漂移，并减少了错误率和逻辑违反。

Conclusion: 开发了一个可靠的框架，用于实时监控概念漂移。

Abstract: In applied machine learning, concept drift, which is either gradual or abrupt
changes in data distribution, can significantly reduce model performance.
Typical detection methods,such as statistical tests or reconstruction-based
models,are generally reactive and not very sensitive to early detection. Our
study proposes a hybrid framework consisting of Transformers and Autoencoders
to model complex temporal dynamics and provide online drift detection. We
create a distinct Trust Score methodology, which includes signals on (1)
statistical and reconstruction-based drift metrics, more specifically, PSI,
JSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,
and (4) trend of classifier error aligned with the combined metrics defined by
the Trust Score. Using a time sequenced airline passenger data set with
synthetic drift, our proposed model allows for a better detection of drift
using as a whole and at different detection thresholds for both sensitivity and
interpretability compared to baseline methods and provides a strong pipeline
for drift detection in real time for applied machine learning. We evaluated
performance using a time-sequenced airline passenger dataset having the
gradually injected stimulus of drift in expectations,e.g. permuted ticket
prices in later batches, broken into 10 time segments [1].In the data, our
results support that the Transformation-Autoencoder detected drift earlier and
with more sensitivity than the autoencoders commonly used in the literature,
and provided improved modeling over more error rates and logical violations.
Therefore, a robust framework was developed to reliably monitor concept drift.

</details>


### [91] [Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria](https://arxiv.org/abs/2508.07102)
*Yang Cao,Yubin Chen,Zhao Song,Jiahao Zhang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Generative modelling has seen significant advances through simulation-free
paradigms such as Flow Matching, and in particular, the MeanFlow framework,
which replaces instantaneous velocity fields with average velocities to enable
efficient single-step sampling. In this work, we introduce a theoretical study
on Second-Order MeanFlow, a novel extension that incorporates average
acceleration fields into the MeanFlow objective. We first establish the
feasibility of our approach by proving that the average acceleration satisfies
a generalized consistency condition analogous to first-order MeanFlow, thereby
supporting stable, one-step sampling and tractable loss functions. We then
characterize its expressivity via circuit complexity analysis, showing that
under mild assumptions, the Second-Order MeanFlow sampling process can be
implemented by uniform threshold circuits within the $\mathsf{TC}^0$ class.
Finally, we derive provably efficient criteria for scalable implementation by
leveraging fast approximate attention computations: we prove that attention
operations within the Second-Order MeanFlow architecture can be approximated to
within $1/\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results
lay the theoretical foundation for high-order flow matching models that combine
rich dynamics with practical sampling efficiency.

</details>


### [92] [BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation](https://arxiv.org/abs/2508.07106)
*Yiran Huang,Amirhossein Nouranizadeh,Christine Ahrends,Mengjia Xu*

Main category: cs.LG

TL;DR: 提出了一种名为BrainATCL的无监督非参数框架，用于动态fMRI数据的自适应时间脑连接学习，解决了传统GNN在捕捉长时程依赖上的不足。


<details>
  <summary>Details</summary>
Motivation: fMRI信号的功能连接动态可能与行为和神经精神疾病相关，但传统GNN难以捕捉动态fMRI数据中的长时程依赖。

Method: BrainATCL通过动态调整每个快照的回溯窗口，结合GINE-Mamba2骨干网络学习时空表示，并融入脑结构和功能信息。

Result: 在功能链接预测和年龄估计任务中表现优异，具有强泛化能力。

Conclusion: BrainATCL为动态功能连接建模提供了有效工具，适用于神经科学研究。

Abstract: Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely
used to study human brain activity. fMRI signals in areas across the brain
transiently synchronise and desynchronise their activity in a highly structured
manner, even when an individual is at rest. These functional connectivity
dynamics may be related to behaviour and neuropsychiatric disease. To model
these dynamics, temporal brain connectivity representations are essential, as
they reflect evolving interactions between brain regions and provide insight
into transient neural states and network reconfigurations. However,
conventional graph neural networks (GNNs) often struggle to capture long-range
temporal dependencies in dynamic fMRI data. To address this challenge, we
propose BrainATCL, an unsupervised, nonparametric framework for adaptive
temporal brain connectivity learning, enabling functional link prediction and
age estimation. Our method dynamically adjusts the lookback window for each
snapshot based on the rate of newly added edges. Graph sequences are
subsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal
representations of dynamic functional connectivity in resting-state fMRI data
of 1,000 participants from the Human Connectome Project. To further improve
spatial modeling, we incorporate brain structure and function-informed edge
attributes, i.e., the left/right hemispheric identity and subnetwork membership
of brain regions, enabling the model to capture biologically meaningful
topological patterns. We evaluate our BrainATCL on two tasks: functional link
prediction and age estimation. The experimental results demonstrate superior
performance and strong generalization, including in cross-session prediction
scenarios.

</details>


### [93] [Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning](https://arxiv.org/abs/2508.07114)
*Atakan Azakli,Bernd Stelzer*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习方法，通过多实例学习（MIL）提高假设检验中参数的预测精度，并降低模型误差。


<details>
  <summary>Details</summary>
Motivation: 解决现有分类器在极端情况下难以准确预测的问题，并系统性地减少机器学习模型的预测误差。

Method: 采用多实例学习（MIL）方法，分析其相对于单实例学习的预测能力，并通过实例数量的缩放行为验证理论。

Result: 在标准模型有效场论（SMEFT）的约束应用中，展示了MIL方法可能提取数据集中的理论最大Fisher信息。

Conclusion: MIL方法在特定条件下能够显著提升预测精度，并可能达到理论最优的信息提取效果。

Abstract: In this work, we propose a new machine learning (ML) methodology to obtain
more precise predictions for some parameters of interest in a given hypotheses
testing problem. Our proposed method also allows ML models to have more
discriminative power in cases where it is extremely challenging for
state-of-the-art classifiers to have any level of accurate predictions. This
method can also allow us to systematically decrease the error from ML models in
their predictions. In this paper, we provide a mathematical motivation why
Multiple Instance Learning (MIL) would have more predictive power over their
single-instance counterparts. We support our theoretical claims by analyzing
the behavior of the MIL models through their scaling behaviors with respect to
the number of instances on which the model makes predictions. As a concrete
application, we constrain Wilson coefficients of the Standard Model Effective
Field Theory (SMEFT) using kinematic information from subatomic particle
collision events at the Large Hadron Collider (LHC). We show that under certain
circumstances, it might be possible to extract the theoretical maximum Fisher
Information latent in a dataset.

</details>


### [94] [From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context](https://arxiv.org/abs/2508.07117)
*Peyman Baghershahi,Gregoire Fournier,Pranav Nyati,Sourav Medya*

Main category: cs.LG

TL;DR: LOGIC是一个轻量级框架，利用LLM为GNN预测生成忠实且可解释的解释。


<details>
  <summary>Details</summary>
Motivation: 现有解释方法难以生成细粒度的解释，尤其是在节点属性包含丰富自然语言时。

Method: LOGIC将GNN节点嵌入投影到LLM嵌入空间，并构建混合提示，结合软提示和文本输入。

Result: 在四个真实TAG数据集上，LOGIC在忠实性和稀疏性之间取得平衡，并显著提升人类中心指标。

Conclusion: LOGIC为基于LLM的图学习可解释性设定了新方向，将GNN内部表示与人类推理对齐。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning over
structured data, including text-attributed graphs, which are common in domains
such as citation networks, social platforms, and knowledge graphs. GNNs are not
inherently interpretable and thus, many explanation methods have been proposed.
However, existing explanation methods often struggle to generate interpretable,
fine-grained rationales, especially when node attributes include rich natural
language. In this work, we introduce LOGIC, a lightweight, post-hoc framework
that uses large language models (LLMs) to generate faithful and interpretable
explanations for GNN predictions. LOGIC projects GNN node embeddings into the
LLM embedding space and constructs hybrid prompts that interleave soft prompts
with textual inputs from the graph structure. This enables the LLM to reason
about GNN internal representations and produce natural language explanations
along with concise explanation subgraphs. Our experiments across four
real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off
between fidelity and sparsity, while significantly improving human-centric
metrics such as insightfulness. LOGIC sets a new direction for LLM-based
explainability in graph learning by aligning GNN internals with human
reasoning.

</details>


### [95] [Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks](https://arxiv.org/abs/2508.07122)
*Zhihao Xue,Yun Zi,Nia Qi,Ming Gong,Yujun Zou*

Main category: cs.LG

TL;DR: 提出了一种基于时空图神经网络的性能预测算法，用于预测具有多级服务调用结构的分布式后端系统的性能波动。


<details>
  <summary>Details</summary>
Motivation: 解决分布式后端系统中性能波动的预测挑战，尤其是多级服务调用结构的复杂性。

Method: 将系统状态抽象为图结构序列，结合运行时特征和服务调用关系，构建统一的时空建模框架，使用图卷积网络提取依赖信息，门控循环网络捕捉动态演化，并引入时间编码机制。

Result: 实验表明，该模型在MAE、RMSE和R2等关键指标上优于现有方法，且在负载和结构变化下保持强鲁棒性。

Conclusion: 该模型在后台服务性能管理任务中具有实际应用潜力。

Abstract: This paper proposes a spatiotemporal graph neural network-based performance
prediction algorithm to address the challenge of forecasting performance
fluctuations in distributed backend systems with multi-level service call
structures. The method abstracts system states at different time slices into a
sequence of graph structures. It integrates the runtime features of service
nodes with the invocation relationships among services to construct a unified
spatiotemporal modeling framework. The model first applies a graph
convolutional network to extract high-order dependency information from the
service topology. Then it uses a gated recurrent network to capture the dynamic
evolution of performance metrics over time. A time encoding mechanism is also
introduced to enhance the model's ability to represent non-stationary temporal
sequences. The architecture is trained in an end-to-end manner, optimizing the
multi-layer nested structure to achieve high-precision regression of future
service performance metrics. To validate the effectiveness of the proposed
method, a large-scale public cluster dataset is used. A series of
multi-dimensional experiments are designed, including variations in time
windows and concurrent load levels. These experiments comprehensively evaluate
the model's predictive performance and stability. The experimental results show
that the proposed model outperforms existing representative methods across key
metrics such as MAE, RMSE, and R2. It maintains strong robustness under varying
load intensities and structural complexities. These results demonstrate the
model's practical potential for backend service performance management tasks.

</details>


### [96] [Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning](https://arxiv.org/abs/2508.07126)
*Zhengran Ji,Boyuan Chen*

Main category: cs.LG

TL;DR: Pref-GUIDE框架将实时标量反馈转化为偏好数据，提升奖励模型学习效果，优于传统标量反馈方法。


<details>
  <summary>Details</summary>
Motivation: 在在线强化学习中，标量反馈存在噪声和不一致性问题，影响奖励模型的准确性和泛化能力。

Method: Pref-GUIDE通过短窗口行为比较和过滤模糊反馈（Individual）及用户群体共识偏好（Voting）改进奖励模型。

Result: 在三个挑战性环境中，Pref-GUIDE显著优于标量反馈基线，Voting版本甚至超过专家设计的密集奖励。

Conclusion: Pref-GUIDE通过结构化偏好和群体反馈，为在线强化学习提供了一种可扩展且原则性的方法。

Abstract: Training reinforcement learning agents with human feedback is crucial when
task objectives are difficult to specify through dense reward functions. While
prior methods rely on offline trajectory comparisons to elicit human
preferences, such data is unavailable in online learning scenarios where agents
must adapt on the fly. Recent approaches address this by collecting real-time
scalar feedback to guide agent behavior and train reward models for continued
learning after human feedback becomes unavailable. However, scalar feedback is
often noisy and inconsistent, limiting the accuracy and generalization of
learned rewards. We propose Pref-GUIDE, a framework that transforms real-time
scalar feedback into preference-based data to improve reward model learning for
continual policy training. Pref-GUIDE Individual mitigates temporal
inconsistency by comparing agent behaviors within short windows and filtering
ambiguous feedback. Pref-GUIDE Voting further enhances robustness by
aggregating reward models across a population of users to form consensus
preferences. Across three challenging environments, Pref-GUIDE significantly
outperforms scalar-feedback baselines, with the voting variant exceeding even
expert-designed dense rewards. By reframing scalar feedback as structured
preferences with population feedback, Pref-GUIDE offers a scalable and
principled approach for harnessing human input in online reinforcement
learning.

</details>


### [97] [How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?](https://arxiv.org/abs/2508.07127)
*Niranjana Arun Menon,Iqra Farooq,Yulong Li,Sara Ahmed,Yutong Xie,Muhammad Awais,Imran Razzak*

Main category: cs.LG

TL;DR: 论文探讨了利用微调的大型语言模型（LLMs）预测心血管疾病（CVD）及其相关SNPs的潜力，通过基因组数据提取生物学意义，并评估其在个性化医疗中的应用。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病预测因多因素病因和高维噪声数据而具有挑战性，LLMs在生物序列分析中的成功应用激发了其在CVD预测中的潜力。

Method: 研究使用微调的LLMs，通过Chain of Thought（CoT）推理任务，从结构化和半结构化基因组数据中学习潜在生物学关系，预测疾病标签和临床推论。

Result: 结果表明，LLMs在早期检测、风险评估和个性化医疗方面具有潜力，能够有效处理高维基因组数据。

Conclusion: LLMs为心血管疾病的预测和个性化医疗提供了新的工具，展示了其在临床实践中的潜在价值。

Abstract: Cardiovascular disease (CVD) prediction remains a tremendous challenge due to
its multifactorial etiology and global burden of morbidity and mortality.
Despite the growing availability of genomic and electrophysiological data,
extracting biologically meaningful insights from such high-dimensional, noisy,
and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has
been applied effectively to predict structural variations in biological
sequences. In this work, we explore the potential of fine-tuned LLMs to predict
cardiac diseases and SNPs potentially leading to CVD risk using genetic markers
derived from high-throughput genomic profiling. We investigate the effect of
genetic patterns associated with cardiac conditions and evaluate how LLMs can
learn latent biological relationships from structured and semi-structured
genomic data obtained by mapping genetic aspects that are inherited from the
family tree. By framing the problem as a Chain of Thought (CoT) reasoning task,
the models are prompted to generate disease labels and articulate informed
clinical deductions across diverse patient profiles and phenotypes. The
findings highlight the promise of LLMs in contributing to early detection, risk
assessment, and ultimately, the advancement of personalized medicine in cardiac
care.

</details>


### [98] [A Globally Optimal Analytic Solution for Semi-Nonnegative Matrix Factorization with Nonnegative or Mixed Inputs](https://arxiv.org/abs/2508.07134)
*Lu Chenggang*

Main category: cs.LG

TL;DR: 本文提出了一种新的半非负矩阵分解（semi-NMF）方法，通过正交分解实现全局最优解，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半NMF算法多为迭代、非凸且易陷入局部最优，需改进。

Method: 基于输入数据的散布矩阵，通过正交分解获得全局最优解。

Result: 在Frobenius范数下，该方法达到重构误差的全局最小值，并在实验中表现优异。

Conclusion: 该方法提供了理论和实证优势，为矩阵分解优化提供了新视角。

Abstract: Semi-Nonnegative Matrix Factorization (semi-NMF) extends classical
Nonnegative Matrix Factorization (NMF) by allowing the basis matrix to contain
both positive and negative entries, making it suitable for decomposing data
with mixed signs. However, most existing semi-NMF algorithms are iterative,
non-convex, and prone to local minima. In this paper, we propose a novel method
that yields a globally optimal solution to the semi-NMF problem under the
Frobenius norm, through an orthogonal decomposition derived from the scatter
matrix of the input data. We rigorously prove that our solution attains the
global minimum of the reconstruction error. Furthermore, we demonstrate that
when the input matrix is nonnegative, our method often achieves lower
reconstruction error than standard NMF algorithms, although unfortunately the
basis matrix may not satisfy nonnegativity. In particular, in low-rank cases
such as rank 1 or 2, our solution reduces exactly to a nonnegative
factorization, recovering the NMF structure. We validate our approach through
experiments on both synthetic data and the UCI Wine dataset, showing that our
method consistently outperforms existing NMF and semi-NMF methods in terms of
reconstruction accuracy. These results confirm that our globally optimal,
non-iterative formulation offers both theoretical guarantees and empirical
advantages, providing a new perspective on matrix factorization in optimization
and data analysis.

</details>


### [99] [A Stable and Principled Loss Function for Direct Language Model Alignment](https://arxiv.org/abs/2508.07137)
*Yuandong Tan*

Main category: cs.LG

TL;DR: 论文提出了一种新的损失函数，解决了DPO方法中因无限最大化对数差异导致的训练不稳定和奖励黑客问题，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: DPO方法在理论上与其推导不一致，可能导致训练不稳定和奖励黑客行为，因此需要一种更稳定的替代方案。

Method: 从RLHF最优条件直接推导出新的损失函数，针对对数差异设定有限目标值，而非最大化。

Result: 新方法在Qwen2.5-7B模型上显著优于标准DPO基线，性能接近更大的Llama-3.1-8B模型。

Conclusion: 提出的损失函数更稳定且有效，避免了DPO的问题，提升了模型对齐效果。

Abstract: The alignment of large language models (LLMs) with human preferences is
commonly achieved through Reinforcement Learning from Human Feedback (RLHF).
Direct Preference Optimization (DPO) simplified this paradigm by establishing a
direct mapping between the optimal policy and a reward function, eliminating
the need for an explicit reward model. However, we argue that the DPO loss
function is theoretically misaligned with its own derivation, as it promotes
the indefinite maximization of a logits difference, which can lead to training
instability and reward hacking. In this paper, we propose a novel loss function
derived directly from the RLHF optimality condition. Our proposed loss targets
a specific, finite value for the logits difference, which is dictated by the
underlying reward, rather than its maximization. We provide a theoretical
analysis, including a gradient-based comparison, to demonstrate that our method
avoids the large gradients that plague DPO when the probability of dispreferred
responses approaches zero. This inherent stability prevents reward hacking and
leads to more effective alignment. We validate our approach by fine-tuning a
Qwen2.5-7B model, showing significant win-rate improvements over a standard DPO
baseline and achieving competitive performance against larger models like
Llama-3.1-8B.

</details>


### [100] [Strategic Incentivization for Locally Differentially Private Federated Learning](https://arxiv.org/abs/2508.07138)
*Yashwant Krishna Pagoti,Arunesh Sinha,Shamik Sural*

Main category: cs.LG

TL;DR: 论文研究了联邦学习中的隐私与准确性权衡问题，提出了一种基于令牌的激励机制，通过游戏理论分析优化噪声添加策略。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中因局部差分隐私（LDP）噪声添加导致的全局模型准确性下降问题。

Method: 将隐私-准确性权衡建模为游戏，引入令牌激励机制，激励客户端减少噪声添加。

Result: 通过实验验证了不同参数对游戏结果的影响，优化了隐私与准确性的平衡。

Conclusion: 令牌激励机制有效解决了联邦学习中的隐私与准确性冲突，为实际应用提供了理论支持。

Abstract: In Federated Learning (FL), multiple clients jointly train a machine learning
model by sharing gradient information, instead of raw data, with a server over
multiple rounds. To address the possibility of information leakage in spite of
sharing only the gradients, Local Differential Privacy (LDP) is often used. In
LDP, clients add a selective amount of noise to the gradients before sending
the same to the server. Although such noise addition protects the privacy of
clients, it leads to a degradation in global model accuracy. In this paper, we
model this privacy-accuracy trade-off as a game, where the sever incentivizes
the clients to add a lower degree of noise for achieving higher accuracy, while
the clients attempt to preserve their privacy at the cost of a potential loss
in accuracy. A token based incentivization mechanism is introduced in which the
quantum of tokens credited to a client in an FL round is a function of the
degree of perturbation of its gradients. The client can later access a newly
updated global model only after acquiring enough tokens, which are to be
deducted from its balance. We identify the players, their actions and payoff,
and perform a strategic analysis of the game. Extensive experiments were
carried out to study the impact of different parameters.

</details>


### [101] [SGD Convergence under Stepsize Shrinkage in Low-Precision Training](https://arxiv.org/abs/2508.07142)
*Vincent-Daniel Yun*

Main category: cs.LG

TL;DR: 论文研究了低精度训练中梯度量化对SGD收敛的影响，发现梯度收缩和量化噪声会减缓收敛速度并增加误差。


<details>
  <summary>Details</summary>
Motivation: 低精度训练虽能降低计算和内存成本，但梯度量化会引入收缩和噪声，可能影响SGD的收敛行为。

Method: 通过梯度收缩模型分析SGD收敛，将量化梯度视为缩放和噪声扰动，推导有效步长和收敛速率。

Result: 低精度SGD仍能收敛，但速率受最小收缩因子影响，且量化噪声会增加渐近误差。

Conclusion: 量化通过梯度收缩和噪声减缓训练速度，需权衡精度与效率。

Abstract: Low-precision training has become essential for reducing the computational
and memory costs of large-scale deep learning. However, quantization of
gradients introduces both magnitude shrinkage and additive noise, which can
alter the convergence behavior of stochastic gradient descent (SGD). In this
work, we study the convergence of SGD under a gradient shrinkage model, where
each stochastic gradient is scaled by a factor $q_k \in (0,1]$ and perturbed by
zero-mean quantization noise. We show that this shrinkage is equivalent to
replacing the nominal stepsize $\mu_k$ with an effective stepsize $\mu_k q_k$,
which slows convergence when $q_{\min} < 1$. Under standard smoothness and
bounded-variance assumptions, we prove that low-precision SGD still converges,
but at a reduced rate determined by $q_{\min}$, and with an increased
asymptotic error floor due to quantization noise. We theoretically analyze how
reduced numerical precision slows down training by modeling it as gradient
shrinkage in the standard SGD convergence framework.

</details>


### [102] [What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains](https://arxiv.org/abs/2508.07208)
*Chanakya Ekbote,Marco Bondaschi,Nived Rajaraman,Jason D. Lee,Michael Gastpar,Ashok Vardhan Makkuva,Paul Pu Liang*

Main category: cs.LG

TL;DR: 本文探讨了两层单头Transformer是否能表示任何k阶马尔可夫过程，并证明其可以表示条件k-gram，揭示了Transformer深度与ICL能力的紧密关系。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在上下文学习（ICL）中的能力，特别是浅层架构是否能够高效表示高阶马尔可夫过程。

Method: 通过理论分析，证明两层单头Transformer可以表示任何条件k-gram，并进一步分析其学习动态。

Result: 证明了两层单头Transformer能够表示高阶马尔可夫过程，并展示了训练过程中有效上下文表示的形成。

Conclusion: 浅层Transformer架构在结构化序列建模任务中表现出强大的ICL能力，深化了对Transformer ICL的理解。

Abstract: In-context learning (ICL) is a hallmark capability of transformers, through
which trained models learn to adapt to new tasks by leveraging information from
the input context. Prior work has shown that ICL emerges in transformers due to
the presence of special circuits called induction heads. Given the equivalence
between induction heads and conditional k-grams, a recent line of work modeling
sequential inputs as Markov processes has revealed the fundamental impact of
model depth on its ICL capabilities: while a two-layer transformer can
efficiently represent a conditional 1-gram model, its single-layer counterpart
cannot solve the task unless it is exponentially large. However, for higher
order Markov sources, the best known constructions require at least three
layers (each with a single attention head) - leaving open the question: can a
two-layer single-head transformer represent any kth-order Markov process? In
this paper, we precisely address this and theoretically show that a two-layer
transformer with one head per layer can indeed represent any conditional
k-gram. Thus, our result provides the tightest known characterization of the
interplay between transformer depth and Markov order for ICL. Building on this,
we further analyze the learning dynamics of our two-layer construction,
focusing on a simplified variant for first-order Markov chains, illustrating
how effective in-context representations emerge during training. Together,
these results deepen our current understanding of transformer-based ICL and
illustrate how even shallow architectures can surprisingly exhibit strong ICL
capabilities on structured sequence modeling tasks.

</details>


### [103] [Neural Bridge Processes](https://arxiv.org/abs/2508.07220)
*Jian Xu,Yican Liu,Qibin Zhao,John Paisley,Delu Zeng*

Main category: cs.LG

TL;DR: 提出了一种名为Neural Bridge Processes (NBPs)的新方法，用于建模随机函数，通过动态锚定输入x的扩散轨迹，显著提升了性能和理论一致性。


<details>
  <summary>Details</summary>
Motivation: 传统方法如高斯过程（GPs）和神经过程（NPs）在处理大规模数据或复杂多模态分布时存在局限性，而神经扩散过程（NDPs）在输入耦合和语义匹配方面表现不佳。

Method: NBPs通过重新设计前向核，使其显式依赖于输入x，从而约束扩散路径严格终止于监督目标，提供更强的梯度信号并保证终点一致性。

Result: 在合成数据、EEG信号回归和图像回归任务中，NBPs显著优于基线方法。

Conclusion: NBPs通过DDPM风格的桥采样，有效提升了结构化预测任务的性能和理论一致性。

Abstract: Learning stochastic functions from partially observed context-target pairs is
a fundamental problem in probabilistic modeling. Traditional models like
Gaussian Processes (GPs) face scalability issues with large datasets and assume
Gaussianity, limiting their applicability. While Neural Processes (NPs) offer
more flexibility, they struggle with capturing complex, multi-modal target
distributions. Neural Diffusion Processes (NDPs) enhance expressivity through a
learned diffusion process but rely solely on conditional signals in the
denoising network, resulting in weak input coupling from an unconditional
forward process and semantic mismatch at the diffusion endpoint. In this work,
we propose Neural Bridge Processes (NBPs), a novel method for modeling
stochastic functions where inputs x act as dynamic anchors for the entire
diffusion trajectory. By reformulating the forward kernel to explicitly depend
on x, NBP enforces a constrained path that strictly terminates at the
supervised target. This approach not only provides stronger gradient signals
but also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG
signal regression and image regression tasks, achieving substantial
improvements over baselines. These results underscore the effectiveness of
DDPM-style bridge sampling in enhancing both performance and theoretical
consistency for structured prediction tasks.

</details>


### [104] [LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference](https://arxiv.org/abs/2508.07221)
*Po-Han Lee,Yu-Cheng Lin,Chan-Tung Ku,Chan Hsu,Pei-Cing Huang,Ping-Hsun Wu,Yihuang Kang*

Main category: cs.LG

TL;DR: 论文提出了一种基于大型语言模型（LLM）的代理方法，用于自动发现混杂变量和进行亚组分析，以提升因果机器学习在复杂环境中的效果。


<details>
  <summary>Details</summary>
Motivation: 观测数据中的个体化治疗效果估计因未测量的混杂和结构偏差而具有挑战性，现有方法在复杂环境中效果有限且依赖专家知识。

Method: 利用LLM代理模拟领域专家，自动发现混杂变量和亚组，减少人工依赖并保持可解释性。

Result: 在真实医疗数据集上的实验表明，该方法通过缩小置信区间和发现未识别的混杂偏差，增强了治疗效果估计的鲁棒性。

Conclusion: LLM代理为可扩展、可信赖且语义感知的因果推断提供了有前景的路径。

Abstract: Estimating individualized treatment effects from observational data presents
a persistent challenge due to unmeasured confounding and structural bias.
Causal Machine Learning (causal ML) methods, such as causal trees and doubly
robust estimators, provide tools for estimating conditional average treatment
effects. These methods have limited effectiveness in complex real-world
environments due to the presence of latent confounders or those described in
unstructured formats. Moreover, reliance on domain experts for confounder
identification and rule interpretation introduces high annotation cost and
scalability concerns. In this work, we proposed Large Language Model-based
agents for automated confounder discovery and subgroup analysis that integrate
agents into the causal ML pipeline to simulate domain expertise. Our framework
systematically performs subgroup identification and confounding structure
discovery by leveraging the reasoning capabilities of LLM-based agents, which
reduces human dependency while preserving interpretability. Experiments on
real-world medical datasets show that our proposed approach enhances treatment
effect estimation robustness by narrowing confidence intervals and uncovering
unrecognized confounding biases. Our findings suggest that LLM-based agents
offer a promising path toward scalable, trustworthy, and semantically aware
causal inference.

</details>


### [105] [EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning](https://arxiv.org/abs/2508.07224)
*Ananda Prakash Verma*

Main category: cs.LG

TL;DR: EDGE是一个通用的、基于误解的自适应学习框架，包含评估、诊断、生成和练习四个阶段，结合了心理测量学、认知诊断和对比性题目生成技术。


<details>
  <summary>Details</summary>
Motivation: 旨在通过动态识别和纠正学习者的误解，提升自适应学习的效果。

Method: 结合IRT/Bayesian模型、认知诊断、对比性题目生成和基于索引的调度策略。

Result: 提出了EdgeScore指标，证明了其单调性和Lipschitz连续性，并推导出近乎最优的调度策略。

Conclusion: EDGE框架在理论上和实现上为纠正学习者误解提供了有效方法，未来需进一步实证研究。

Abstract: We present EDGE, a general-purpose, misconception-aware adaptive learning
framework composed of four stages: Evaluate (ability and state estimation),
Diagnose (posterior infer-ence of misconceptions), Generate (counterfactual
item synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies
psychometrics (IRT/Bayesian state space models), cog-nitive diagnostics
(misconception discovery from distractor patterns and response latencies),
contrastive item generation (minimal perturbations that invalidate learner
shortcuts while pre-serving psychometric validity), and principled scheduling
(a restless bandit approximation to spaced retrieval). We formalize a composite
readiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity,
and derive an index policy that is near-optimal under mild assumptions on
forgetting and learning gains. We further establish conditions under which
counterfactual items provably reduce the posterior probability of a targeted
misconception faster than standard practice. The paper focuses on theory and
implementable pseudocode; empirical study is left to future work.

</details>


### [106] [Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation](https://arxiv.org/abs/2508.07243)
*Chu Zhao,Eneng Yang,Yizhou Dang,Jianzhe Zhao,Guibing Guo,Xingwei Wang*

Main category: cs.LG

TL;DR: 论文提出了一种名为CNSDiff的新方法，通过扩散过程合成负样本，避免预定义候选池的偏差，并引入因果正则化以减少环境混杂因素的影响，从而提升推荐系统的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 启发式负采样可能因环境混杂因素（如曝光或流行度偏差）引入虚假硬负样本（FHNS），损害模型的泛化能力。

Method: 提出CNSDiff方法，利用条件扩散过程在潜在空间合成负样本，并加入因果正则化以减少混杂因素的影响。

Result: 在四种代表性分布偏移场景下，CNSDiff平均性能提升13.96%，优于现有基线方法。

Conclusion: CNSDiff通过避免候选池偏差和减少混杂因素影响，显著提升了推荐系统的OOD泛化能力。

Abstract: Heuristic negative sampling enhances recommendation performance by selecting
negative samples of varying hardness levels from predefined candidate pools to
guide the model toward learning more accurate decision boundaries. However, our
empirical and theoretical analyses reveal that unobserved environmental
confounders (e.g., exposure or popularity biases) in candidate pools may cause
heuristic sampling methods to introduce false hard negatives (FHNS). These
misleading samples can encourage the model to learn spurious correlations
induced by such confounders, ultimately compromising its generalization ability
under distribution shifts. To address this issue, we propose a novel method
named Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing
negative samples in the latent space via a conditional diffusion process,
CNSDiff avoids the bias introduced by predefined candidate pools and thus
reduces the likelihood of generating FHNS. Moreover, it incorporates a causal
regularization term to explicitly mitigate the influence of environmental
confounders during the negative sampling process, leading to robust negatives
that promote out-of-distribution (OOD) generalization. Comprehensive
experiments under four representative distribution shift scenarios demonstrate
that CNSDiff achieves an average improvement of 13.96% across all evaluation
metrics compared to state-of-the-art baselines, verifying its effectiveness and
robustness in OOD recommendation tasks.

</details>


### [107] [Policy Newton methods for Distortion Riskmetrics](https://arxiv.org/abs/2508.07249)
*Soumen Pachal,Mizhaan Prajit Maniyar,Prashanth L. A*

Main category: cs.LG

TL;DR: 论文提出了一种风险敏感的强化学习方法，通过最大化失真风险度量（DRM）来优化策略，并证明了其收敛到二阶稳定点。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习中风险敏感控制的问题，特别是在有限时间马尔可夫决策过程中优化DRM。

Method: 使用似然比方法推导DRM目标的策略Hessian定理，提出基于样本轨迹的Hessian估计器，并设计立方正则化的策略牛顿算法。

Result: 算法收敛到DRM目标的二阶稳定点，样本复杂度为O(ε^-3.5)。

Conclusion: 首次实现了风险敏感目标的二阶稳定点收敛，填补了现有研究的空白。

Abstract: We consider the problem of risk-sensitive control in a reinforcement learning
(RL) framework. In particular, we aim to find a risk-optimal policy by
maximizing the distortion riskmetric (DRM) of the discounted reward in a finite
horizon Markov decision process (MDP). DRMs are a rich class of risk measures
that include several well-known risk measures as special cases. We derive a
policy Hessian theorem for the DRM objective using the likelihood ratio method.
Using this result, we propose a natural DRM Hessian estimator from sample
trajectories of the underlying MDP. Next, we present a cubic-regularized policy
Newton algorithm for solving this problem in an on-policy RL setting using
estimates of the DRM gradient and Hessian. Our proposed algorithm is shown to
converge to an $\epsilon$-second-order stationary point ($\epsilon$-SOSP) of
the DRM objective, and this guarantee ensures the escaping of saddle points.
The sample complexity of our algorithms to find an $ \epsilon$-SOSP is
$\mathcal{O}(\epsilon^{-3.5})$. Our experiments validate the theoretical
findings. To the best of our knowledge, our is the first work to present
convergence to an $\epsilon$-SOSP of a risk-sensitive objective, while existing
works in the literature have either shown convergence to a first-order
stationary point of a risk-sensitive objective, or a SOSP of a risk-neutral
one.

</details>


### [108] [Revisiting Data Attribution for Influence Functions](https://arxiv.org/abs/2508.07297)
*Hongbo Zhu,Angelo Cangelosi*

Main category: cs.LG

TL;DR: 本文综述了影响函数在深度学习中的数据归因能力，探讨了其理论基础、高效逆Hessian-向量积估计算法，并评估了其在数据归因和错误标签检测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 理解训练数据如何影响模型预测对机器学习可解释性、数据调试和模型问责至关重要。影响函数提供了一种高效的一阶近似方法，无需重新训练即可估计数据点对模型参数和预测的影响。

Method: 通过理论分析和算法改进（如高效逆Hessian-向量积估计），研究影响函数在深度学习中的应用。

Result: 影响函数在数据归因和错误标签检测中表现出有效性，但仍面临大规模实际应用中的挑战。

Conclusion: 影响函数在深度学习中具有巨大潜力，但需进一步解决实际应用中的挑战，以充分发挥其作用。

Abstract: The goal of data attribution is to trace the model's predictions through the
learning algorithm and back to its training data. thereby identifying the most
influential training samples and understanding how the model's behavior leads
to particular predictions. Understanding how individual training examples
influence a model's predictions is fundamental for machine learning
interpretability, data debugging, and model accountability. Influence
functions, originating from robust statistics, offer an efficient, first-order
approximation to estimate the impact of marginally upweighting or removing a
data point on a model's learned parameters and its subsequent predictions,
without the need for expensive retraining. This paper comprehensively reviews
the data attribution capability of influence functions in deep learning. We
discuss their theoretical foundations, recent algorithmic advances for
efficient inverse-Hessian-vector product estimation, and evaluate their
effectiveness for data attribution and mislabel detection. Finally,
highlighting current challenges and promising directions for unleashing the
huge potential of influence functions in large-scale, real-world deep learning
scenarios.

</details>


### [109] [When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective](https://arxiv.org/abs/2508.07299)
*Lin-Han Jia,Si-Yu Han,Wen-Chao Hu,Jie-Jing Shao,Wen-Da Wei,Zhi Zhou,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: 论文通过扩展神经符号学习理论，统一了自监督学习与神经符号学习的理论框架，提出预测无监督任务有效性的方法。


<details>
  <summary>Details</summary>
Motivation: 当前无监督任务选择依赖启发式方法，缺乏理论依据，论文旨在解决这一问题。

Method: 基于知识可学习性、可靠性和完整性三个因素，提出理论分析框架，并设计预测方法。

Result: 实验验证了预测性能与实际性能高度相关，证实了理论和方法的有效性。

Conclusion: 论文为无监督任务选择提供了理论支持，提升了目标任务的性能。

Abstract: Neuro-symbolic (Nesy) learning improves the target task performance of models
by enabling them to satisfy knowledge, while semi/self-supervised learning
(SSL) improves the target task performance by designing unsupervised pretext
tasks for unlabeled data to make models satisfy corresponding assumptions. We
extend the Nesy theory based on reliable knowledge to the scenario of
unreliable knowledge (i.e., assumptions), thereby unifying the theoretical
frameworks of SSL and Nesy. Through rigorous theoretical analysis, we
demonstrate that, in theory, the impact of pretext tasks on target performance
hinges on three factors: knowledge learnability with respect to the model,
knowledge reliability with respect to the data, and knowledge completeness with
respect to the target. We further propose schemes to operationalize these
theoretical metrics, and thereby develop a method that can predict the
effectiveness of pretext tasks in advance. This will change the current status
quo in practical applications, where the selections of unsupervised tasks are
heuristic-based rather than theory-based, and it is difficult to evaluate the
rationality of unsupervised pretext task selection before testing the model on
the target task. In experiments, we verify a high correlation between the
predicted performance-estimated using minimal data-and the actual performance
achieved after large-scale semi-supervised or self-supervised learning, thus
confirming the validity of the theory and the effectiveness of the evaluation
method.

</details>


### [110] [Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative](https://arxiv.org/abs/2508.07329)
*Tuo Zhang,Ning Li,Xin Yuan,Wenchao Xu,Quan Chen,Song Guo,Haijun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种基于Hessian感知量化（HAQ）和CPU-GPU协同推理的高效MoE边缘部署方案，解决了量化精度和内存限制问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在资源受限的边缘设备上高效部署面临量化精度和内存管理的挑战。

Method: 采用Hessian感知量化（HAQ）实现8位量化，并设计专家级协同卸载与推理机制。

Result: 在OPT系列和Mixtral 8*7B等模型上，量化模型推理精度接近全精度模型，GPU内存减少60%，延迟显著降低。

Conclusion: 该方法有效解决了MoE架构在边缘设备部署中的量化精度和内存管理问题。

Abstract: With the breakthrough progress of large language models (LLMs) in natural
language processing and multimodal tasks, efficiently deploying them on
resource-constrained edge devices has become a critical challenge. The Mixture
of Experts (MoE) architecture enhances model capacity through sparse
activation, but faces two major difficulties in practical deployment: (1) The
presence of numerous outliers in activation distributions leads to severe
degradation in quantization accuracy for both activations and weights,
significantly impairing inference performance; (2) Under limited memory,
efficient offloading and collaborative inference of expert modules struggle to
balance latency and throughput. To address these issues, this paper proposes an
efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)
and CPU-GPU collaborative inference. First, by introducing smoothed Hessian
matrix quantization, we achieve joint 8-bit quantization of activations and
weights, which significantly alleviates the accuracy loss caused by outliers
while ensuring efficient implementation on mainstream hardware. Second, we
design an expert-level collaborative offloading and inference mechanism, which,
combined with expert activation path statistics, enables efficient deployment
and scheduling of expert modules between CPU and GPU, greatly reducing memory
footprint and inference latency. Extensive experiments validate the
effectiveness of our method on mainstream large models such as the OPT series
and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of
the low-bit quantized model approaches that of the full-precision model, while
GPU memory usage is reduced by about 60%, and inference latency is
significantly improved.

</details>


### [111] [Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants](https://arxiv.org/abs/2508.07333)
*Yuhao Liu,Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: 本文研究了随机插值在生成建模中的应用，分析了两种数值积分方法（前向欧拉法和Heun法）的有限时间收敛性，并提出了优化计算效率的调度策略。


<details>
  <summary>Details</summary>
Motivation: 随机插值为数据分布间的连续变换提供了强大框架，但其数值实现的有限时间收敛性尚未充分研究。

Method: 通过分析随机插值生成的ODE，建立了前向欧拉法和Heun法的有限时间误差界，并优化了迭代复杂度。

Result: 理论分析表明两种数值方法在有限时间内收敛，并通过实验验证了误差界和复杂度分析的准确性。

Conclusion: 研究为随机插值的数值实现提供了理论支持，优化了计算效率，推动了生成建模的发展。

Abstract: Stochastic interpolants offer a robust framework for continuously
transforming samples between arbitrary data distributions, holding significant
promise for generative modeling. Despite their potential, rigorous finite-time
convergence guarantees for practical numerical schemes remain largely
unexplored. In this work, we address the finite-time convergence analysis of
numerical implementations for ordinary differential equations (ODEs) derived
from stochastic interpolants. Specifically, we establish novel finite-time
error bounds in total variation distance for two widely used numerical
integrators: the first-order forward Euler method and the second-order Heun's
method. Furthermore, our analysis on the iteration complexity of specific
stochastic interpolant constructions provides optimized schedules to enhance
computational efficiency. Our theoretical findings are corroborated by
numerical experiments, which validate the derived error bounds and complexity
analyses.

</details>


### [112] [ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis](https://arxiv.org/abs/2508.07345)
*Samiha Afaf Neha,Abir Ahammed Bhuiyan,Md. Ishrak Khan*

Main category: cs.LG

TL;DR: 该论文提出了一种名为ProteoKnight的新型图像编码方法，用于噬菌体病毒蛋白（PVP）的分类，结合预训练的卷积神经网络和蒙特卡洛Dropout（MCD）评估预测不确定性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噬菌体病毒蛋白（PVP）分类中存在空间信息丢失的问题，需要更有效的序列编码方法。

Method: ProteoKnight基于DNA-Walk算法改进，通过像素颜色和调整行走距离编码蛋白质序列，使用预训练CNN进行分类，并通过MCD评估预测不确定性。

Result: 实验显示，该方法在二分类任务中达到90.8%的准确率，与现有最佳方法相当，但多分类任务表现欠佳。不确定性分析揭示了预测置信度受蛋白质类别和序列长度影响。

Conclusion: ProteoKnight克服了频率混沌游戏表示（FCGR）的空间信息丢失问题，提供了准确且稳健的PVP预测，并能识别低置信度预测。

Abstract: \textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is
essential for genomic studies due to their crucial role as structural elements
in bacteriophages. Computational tools, particularly machine learning, have
emerged for annotating phage protein sequences from high-throughput sequencing.
However, effective annotation requires specialized sequence encodings. Our
paper introduces ProteoKnight, a new image-based encoding method that addresses
spatial constraints in existing techniques, yielding competitive performance in
PVP classification using pre-trained convolutional neural networks.
Additionally, our study evaluates prediction uncertainty in binary PVP
classification through Monte Carlo Dropout (MCD). \textbf{Methods:}
ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences,
incorporating pixel colors and adjusting walk distances to capture intricate
protein features. Encoded sequences were classified using multiple pre-trained
CNNs. Variance and entropy measures assessed prediction uncertainty across
proteins of various classes and lengths. \textbf{Results:} Our experiments
achieved 90.8% accuracy in binary classification, comparable to
state-of-the-art methods. Multi-class classification accuracy remains
suboptimal. Our uncertainty analysis unveils variability in prediction
confidence influenced by protein class and sequence length.
\textbf{Conclusions:} Our study surpasses frequency chaos game representation
(FCGR) by introducing novel image encoding that mitigates spatial information
loss limitations. Our classification technique yields accurate and robust PVP
predictions while identifying low-confidence predictions.

</details>


### [113] [Intrinsic training dynamics of deep neural networks](https://arxiv.org/abs/2508.07370)
*Sibylle Marcotte,Gabriel Peyré,Rémi Gribonval*

Main category: cs.LG

TL;DR: 论文研究了高维参数空间中梯度流能否简化为低维结构，提出了内在动态性质，并应用于ReLU网络和线性网络。


<details>
  <summary>Details</summary>
Motivation: 理解高维参数空间中梯度训练是否能被低维结构捕获，即所谓的隐式偏差。

Method: 通过研究高维变量θ的梯度流是否隐含低维变量z=ϕ(θ)的内在梯度流，提出基于核包含的简单判据，并应用于ReLU网络和线性网络。

Result: 证明了在任意初始条件下，ReLU网络的梯度流可重写为仅依赖z和初始化的低维动态；线性网络中，松弛平衡初始化是唯一满足内在动态性质的初始化。

Conclusion: 论文为理解高维梯度流的低维简化提供了理论支持，并展示了其在深度网络中的适用性。

Abstract: A fundamental challenge in the theory of deep learning is to understand
whether gradient-based training in high-dimensional parameter spaces can be
captured by simpler, lower-dimensional structures, leading to so-called
implicit bias. As a stepping stone, we study when a gradient flow on a
high-dimensional variable $\theta$ implies an intrinsic gradient flow on a
lower-dimensional variable $z = \phi(\theta)$, for an architecture-related
function $\phi$. We express a so-called intrinsic dynamic property and show how
it is related to the study of conservation laws associated with the
factorization $\phi$. This leads to a simple criterion based on the inclusion
of kernels of linear maps which yields a necessary condition for this property
to hold. We then apply our theory to general ReLU networks of arbitrary depth
and show that, for any initialization, it is possible to rewrite the flow as an
intrinsic dynamic in a lower dimension that depends only on $z$ and the
initialization, when $\phi$ is the so-called path-lifting. In the case of
linear networks with $\phi$ the product of weight matrices, so-called balanced
initializations are also known to enable such a dimensionality reduction; we
generalize this result to a broader class of {\em relaxed balanced}
initializations, showing that, in certain configurations, these are the
\emph{only} initializations that ensure the intrinsic dynamic property.
Finally, for the linear neural ODE associated with the limit of infinitely deep
linear networks, with relaxed balanced initialization, we explicitly express
the corresponding intrinsic dynamics.

</details>


### [114] [Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs](https://arxiv.org/abs/2508.07395)
*Behnoush Khavari,Mehran Shakerinava,Jayesh Khullar,Jerry Huang,François Rivest,Siamak Ravanbakhsh,Sarath Chandar*

Main category: cs.LG

TL;DR: 论文探讨了LRNN模型（如S4D、Mamba和DeltaNet）在状态跟踪任务中的不足，指出输入依赖的转移矩阵对提升性能的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有LRNN模型因时间不变转移矩阵或受限特征值范围而缺乏状态跟踪能力，需研究输入依赖转移矩阵的作用。

Method: 研究多层SSM中结合输入独立和非负SSM的能力，分析其对简单状态跟踪任务（如奇偶校验）的影响。

Result: 实验表明，即使结合这两种SSM，仍无法解决奇偶校验任务，需输入依赖且含负特征值的递归层。

Conclusion: 输入依赖且含负特征值的转移矩阵是提升SSM状态跟踪能力的关键。

Abstract: Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack
state-tracking capability due to either time-invariant transition matrices or
restricted eigenvalue ranges. To address this, input-dependent transition
matrices, particularly those that are complex or non-triangular, have been
proposed to enhance SSM performance on such tasks. While existing theorems
demonstrate that both input-independent and non-negative SSMs are incapable of
solving simple state-tracking tasks, such as parity, regardless of depth, they
do not explore whether combining these two types in a multilayer SSM could
help. We investigate this question for efficient SSMs with diagonal transition
matrices and show that such combinations still fail to solve parity. This
implies that a recurrence layer must both be input-dependent and include
negative eigenvalues. Our experiments support this conclusion by analyzing an
SSM model that combines S4D and Mamba layers.

</details>


### [115] [Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors](https://arxiv.org/abs/2508.07400)
*Mohamad Louai Shehab,Alperen Tercan,Necmiye Ozay*

Main category: cs.LG

TL;DR: 论文研究了从最优策略或最大熵强化学习的演示中恢复时变奖励函数的问题，提出了两种奖励函数的先验假设，并分别转化为稀疏化和秩最小化问题，给出了高效算法。


<details>
  <summary>Details</summary>
Motivation: 奖励函数恢复问题通常是不适定的，但在许多应用中奖励函数具有稀疏性或可表示为少量特征的线性组合，因此需要有效的恢复方法。

Method: 1) 将奖励函数假设为稀疏变化，转化为稀疏化问题；2) 将奖励函数假设为少量特征的线性组合，转化为秩最小化问题。分别提出了多项式时间算法和凸松弛方法。

Result: 提出的算法能够高效准确地恢复奖励函数，并通过示例验证了其准确性和泛化能力。

Conclusion: 通过引入先验假设和优化方法，论文为时变奖励函数的恢复提供了高效且通用的解决方案。

Abstract: In this paper, we consider the problem of recovering time-varying reward
functions from either optimal policies or demonstrations coming from a max
entropy reinforcement learning problem. This problem is highly ill-posed
without additional assumptions on the underlying rewards. However, in many
applications, the rewards are indeed parsimonious, and some prior information
is available. We consider two such priors on the rewards: 1) rewards are mostly
constant and they change infrequently, 2) rewards can be represented by a
linear combination of a small number of feature functions. We first show that
the reward identification problem with the former prior can be recast as a
sparsification problem subject to linear constraints. Moreover, we give a
polynomial-time algorithm that solves this sparsification problem exactly.
Then, we show that identifying rewards representable with the minimum number of
features can be recast as a rank minimization problem subject to linear
constraints, for which convex relaxations of rank can be invoked. In both
cases, these observations lead to efficient optimization-based reward
identification algorithms. Several examples are given to demonstrate the
accuracy of the recovered rewards as well as their generalizability.

</details>


### [116] [Lightning Prediction under Uncertainty: DeepLight with Hazy Loss](https://arxiv.org/abs/2508.07428)
*Md Sultanul Arifin,Abu Nowshed Sakib,Yeasir Rayhan,Tanzima Hashem*

Main category: cs.LG

TL;DR: DeepLight是一种新型深度学习架构，用于预测闪电事件，通过多源气象数据和双编码器架构提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 闪电对人身安全和经济发展构成重大威胁，现有预测模型在动态空间上下文捕捉和数据利用方面存在不足。

Method: 利用雷达反射率、云属性和历史闪电数据，通过双编码器架构和多分支卷积技术动态捕捉空间相关性，并使用Hazy Loss函数处理时空不确定性。

Result: 实验表明，DeepLight将公平威胁评分（ETS）提高了18%-30%，优于现有方法。

Conclusion: DeepLight为闪电预测提供了更可靠的解决方案，能够有效减少闪电带来的风险。

Abstract: Lightning, a common feature of severe meteorological conditions, poses
significant risks, from direct human injuries to substantial economic losses.
These risks are further exacerbated by climate change. Early and accurate
prediction of lightning would enable preventive measures to safeguard people,
protect property, and minimize economic losses. In this paper, we present
DeepLight, a novel deep learning architecture for predicting lightning
occurrences. Existing prediction models face several critical limitations: they
often struggle to capture the dynamic spatial context and inherent uncertainty
of lightning events, underutilize key observational data, such as radar
reflectivity and cloud properties, and rely heavily on Numerical Weather
Prediction (NWP) systems, which are both computationally expensive and highly
sensitive to parameter settings. To overcome these challenges, DeepLight
leverages multi-source meteorological data, including radar reflectivity, cloud
properties, and historical lightning occurrences through a dual-encoder
architecture. By employing multi-branch convolution techniques, it dynamically
captures spatial correlations across varying extents. Furthermore, its novel
Hazy Loss function explicitly addresses the spatio-temporal uncertainty of
lightning by penalizing deviations based on proximity to true events, enabling
the model to better learn patterns amidst randomness. Extensive experiments
show that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over
state-of-the-art methods, establishing it as a robust solution for lightning
prediction.

</details>


### [117] [Unsupervised operator learning approach for dissipative equations via Onsager principle](https://arxiv.org/abs/2508.07440)
*Zhipeng Chang,Zhenye Wen,Xiaofei Zhao*

Main category: cs.LG

TL;DR: DOOL是一种无监督的深度算子学习方法，基于Onsager变分原理，无需标记数据，通过时空解耦策略提升效率，并在典型耗散方程上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统算子学习方法依赖高保真模拟数据，计算成本高，DOOL旨在通过无监督框架解决这一问题。

Method: DOOL基于Onsager变分原理，直接最小化Rayleighian泛函，采用时空解耦策略（空间坐标由主干网络处理，外部时间步进实现时间外推）。

Result: 数值实验验证了DOOL的有效性，与监督方法DeepONet和MIONet相比性能更优，并可扩展至不直接遵循OVP的二阶波动模型。

Conclusion: DOOL提供了一种高效的无监督算子学习方法，适用于耗散方程，并展示了在复杂模型中的扩展潜力。

Abstract: Existing operator learning methods rely on supervised training with
high-fidelity simulation data, introducing significant computational cost. In
this work, we propose the deep Onsager operator learning (DOOL) method, a novel
unsupervised framework for solving dissipative equations. Rooted in the Onsager
variational principle (OVP), DOOL trains a deep operator network by directly
minimizing the OVP-defined Rayleighian functional, requiring no labeled data,
and then proceeds in time explicitly through conservation/change laws for the
solution. Another key innovation here lies in the spatiotemporal decoupling
strategy: the operator's trunk network processes spatial coordinates
exclusively, thereby enhancing training efficiency, while integrated external
time stepping enables temporal extrapolation. Numerical experiments on typical
dissipative equations validate the effectiveness of the DOOL method, and
systematic comparisons with supervised DeepONet and MIONet demonstrate its
enhanced performance. Extensions are made to cover the second-order wave models
with dissipation that do not directly follow OVP.

</details>


### [118] [Stackelberg Coupling of Online Representation Learning and Reinforcement Learning](https://arxiv.org/abs/2508.07452)
*Fernando Martinez,Tao Li,Yingdong Lu,Juntao Chen*

Main category: cs.LG

TL;DR: SCORER框架通过博弈论动态优化感知与控制网络的交互，提升深度强化学习的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏奖励信号下特征学习的挑战，避免复杂辅助目标或完全解耦带来的设计复杂性。

Method: 提出SCORER框架，将感知与控制网络的交互建模为Stackelberg博弈，并设计两时间尺度算法近似均衡。

Result: 在标准DQN变体和基准任务上，SCORER提高了样本效率和最终性能。

Conclusion: 通过博弈论设计感知-控制动态，无需复杂辅助目标即可提升性能。

Abstract: Integrated, end-to-end learning of representations and policies remains a
cornerstone of deep reinforcement learning (RL). However, to address the
challenge of learning effective features from a sparse reward signal, recent
trends have shifted towards adding complex auxiliary objectives or fully
decoupling the two processes, often at the cost of increased design complexity.
This work proposes an alternative to both decoupling and naive end-to-end
learning, arguing that performance can be significantly improved by structuring
the interaction between distinct perception and control networks with a
principled, game-theoretic dynamic. We formalize this dynamic by introducing
the Stackelberg Coupled Representation and Reinforcement Learning (SCORER)
framework, which models the interaction between perception and control as a
Stackelberg game. The perception network (leader) strategically learns features
to benefit the control network (follower), whose own objective is to minimize
its Bellman error. We approximate the game's equilibrium with a practical
two-timescale algorithm. Applied to standard DQN variants on benchmark tasks,
SCORER improves sample efficiency and final performance. Our results show that
performance gains can be achieved through principled algorithmic design of the
perception-control dynamic, without requiring complex auxiliary objectives or
architectures.

</details>


### [119] [Towards Unveiling Predictive Uncertainty Vulnerabilities in the Context of the Right to Be Forgotten](https://arxiv.org/abs/2508.07458)
*Wei Qian,Chenxu Zhao,Yangyi Li,Wenqian Ye,Mengdi Huai*

Main category: cs.LG

TL;DR: 本文提出了一种针对预测不确定性的恶意遗忘攻击，填补了现有研究空白，并通过实验证明其比传统攻击更有效。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型预测不确定性量化方法的普及和遗忘权需求的增长，研究恶意遗忘攻击对预测不确定性的影响成为必要。

Method: 设计了针对预测不确定性的新型恶意遗忘攻击框架，包括优化方法和黑盒场景实验。

Result: 实验表明，该攻击比传统标签误分类攻击更有效，且现有防御方法对其无效。

Conclusion: 本文揭示了预测不确定性在恶意遗忘攻击下的脆弱性，为未来防御研究提供了方向。

Abstract: Currently, various uncertainty quantification methods have been proposed to
provide certainty and probability estimates for deep learning models' label
predictions. Meanwhile, with the growing demand for the right to be forgotten,
machine unlearning has been extensively studied as a means to remove the impact
of requested sensitive data from a pre-trained model without retraining the
model from scratch. However, the vulnerabilities of such generated predictive
uncertainties with regard to dedicated malicious unlearning attacks remain
unexplored. To bridge this gap, for the first time, we propose a new class of
malicious unlearning attacks against predictive uncertainties, where the
adversary aims to cause the desired manipulations of specific predictive
uncertainty results. We also design novel optimization frameworks for our
attacks and conduct extensive experiments, including black-box scenarios.
Notably, our extensive experiments show that our attacks are more effective in
manipulating predictive uncertainties than traditional attacks that focus on
label misclassifications, and existing defenses against conventional attacks
are ineffective against our attacks.

</details>


### [120] [Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach](https://arxiv.org/abs/2508.07505)
*Yueyang Quan,Chang Wang,Shengjie Zhai,Minghong Fang,Zhuqing Liu*

Main category: cs.LG

TL;DR: 提出了一种名为DPMixSGD的隐私保护算法，用于解决非凸分散式最小-最大优化问题，结合差分隐私技术，确保数据隐私的同时不影响收敛性能。


<details>
  <summary>Details</summary>
Motivation: 分散式最小-最大优化中，模型更新的共享可能导致敏感数据泄露，差分隐私虽能保护隐私，但噪声可能影响收敛，尤其在非凸场景下。

Method: 基于STORM算法，提出DPMixSGD，通过添加噪声到局部梯度，同时保证隐私和收敛性能。

Result: 理论证明噪声不影响收敛，实验验证了算法在多种任务和模型中的有效性。

Conclusion: DPMixSGD是一种高效且隐私保护的分散式最小-最大优化算法。

Abstract: Decentralized min-max optimization allows multi-agent systems to
collaboratively solve global min-max optimization problems by facilitating the
exchange of model updates among neighboring agents, eliminating the need for a
central server. However, sharing model updates in such systems carry a risk of
exposing sensitive data to inference attacks, raising significant privacy
concerns. To mitigate these privacy risks, differential privacy (DP) has become
a widely adopted technique for safeguarding individual data. Despite its
advantages, implementing DP in decentralized min-max optimization poses
challenges, as the added noise can hinder convergence, particularly in
non-convex scenarios with complex agent interactions in min-max optimization
problems. In this work, we propose an algorithm called DPMixSGD (Differential
Private Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving
algorithm specifically designed for non-convex decentralized min-max
optimization. Our method builds on the state-of-the-art STORM-based algorithm,
one of the fastest decentralized min-max solutions. We rigorously prove that
the noise added to local gradients does not significantly compromise
convergence performance, and we provide theoretical bounds to ensure privacy
guarantees. To validate our theoretical findings, we conduct extensive
experiments across various tasks and models, demonstrating the effectiveness of
our approach.

</details>


### [121] [Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning](https://arxiv.org/abs/2508.07536)
*Tasfiq E. Alam,Md Manjurul Ahsan,Shivakumar Raman*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的多模态CNN模型，用于轴承故障分类，结合振动和电机电流信号，并通过物理特征提取分支提升性能。实验表明其优于非物理基线，并在迁移学习中验证了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决变工况下轴承故障分类的准确性和可解释性问题，避免因领域偏移导致的模型性能下降。

Method: 采用多模态CNN架构，结合振动和电流信号，引入物理特征提取分支和物理损失函数。评估了三种迁移学习策略。

Result: 在Paderborn和KAIST数据集上表现优异，最高准确率达98%，统计测试验证了显著改进。

Conclusion: 结合领域知识与数据驱动学习，实现了鲁棒、可解释且泛化能力强的故障诊断框架。

Abstract: Accurate and interpretable bearing fault classification is critical for
ensuring the reliability of rotating machinery, particularly under variable
operating conditions where domain shifts can significantly degrade model
performance. This study proposes a physics-informed multimodal convolutional
neural network (CNN) with a late fusion architecture, integrating vibration and
motor current signals alongside a dedicated physics-based feature extraction
branch. The model incorporates a novel physics-informed loss function that
penalizes physically implausible predictions based on characteristic bearing
fault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass Frequency
Inner (BPFI) - derived from bearing geometry and shaft speed. Comprehensive
experiments on the Paderborn University dataset demonstrate that the proposed
physics-informed approach consistently outperforms a non-physics-informed
baseline, achieving higher accuracy, reduced false classifications, and
improved robustness across multiple data splits. To address performance
degradation under unseen operating conditions, three transfer learning (TL)
strategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy
(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LAS
yields the best generalization, with additional performance gains when combined
with physics-informed modeling. Validation on the KAIST bearing dataset
confirms the framework's cross-dataset applicability, achieving up to 98
percent accuracy. Statistical hypothesis testing further verifies significant
improvements (p < 0.01) in classification performance. The proposed framework
demonstrates the potential of integrating domain knowledge with data-driven
learning to achieve robust, interpretable, and generalizable fault diagnosis
for real-world industrial applications.

</details>


### [122] [Multimodal Remote Inference](https://arxiv.org/abs/2508.07555)
*Keyuan Zhang,Yin Sun,Bo Ji*

Main category: cs.LG

TL;DR: 研究多模态远程推理系统中的调度问题，提出基于索引的阈值策略以减少推理误差。


<details>
  <summary>Details</summary>
Motivation: 由于网络资源有限，无法实时传输所有模态的特征，而新鲜的特征对推理任务至关重要。

Method: 开发了一种基于索引的阈值策略，调度器在索引函数超过阈值时切换模态。

Result: 该策略在非单调、非加性AoI函数和异构传输时间下均最优，推理误差降低达55%。

Conclusion: 优化任务导向的AoI函数可显著提高远程推理准确性。

Abstract: We consider a remote inference system with multiple modalities, where a
multimodal machine learning (ML) model performs real-time inference using
features collected from remote sensors. As sensor observations may change
dynamically over time, fresh features are critical for inference tasks.
However, timely delivering features from all modalities is often infeasible due
to limited network resources. To this end, we study a two-modality scheduling
problem to minimize the ML model's inference error, which is expressed as a
penalty function of AoI for both modalities. We develop an index-based
threshold policy and prove its optimality. Specifically, the scheduler switches
modalities when the current modality's index function exceeds a threshold. We
show that the two modalities share the same threshold, and both the index
functions and the threshold can be computed efficiently. The optimality of our
policy holds for (i) general AoI functions that are \emph{non-monotonic} and
\emph{non-additive} and (ii) \emph{heterogeneous} transmission times. Numerical
results show that our policy reduces inference error by up to 55% compared to
round-robin and uniform random policies, which are oblivious to the AoI-based
inference error function. Our results shed light on how to improve remote
inference accuracy by optimizing task-oriented AoI functions.

</details>


### [123] [Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression](https://arxiv.org/abs/2508.07571)
*Xingwu Chen,Miao Lu,Beining Wu,Difan Zou*

Main category: cs.LG

TL;DR: 通过增加测试时计算（如生成更多中间思考或采样多个候选答案）提升语言模型性能，本文通过引入随机性和采样，结合理论与实际，分析推理行为。


<details>
  <summary>Details</summary>
Motivation: 弥合实际语言模型推理与理论分析之间的差距，探索随机性和采样在推理中的作用。

Method: 采用上下文线性回归框架，通过噪声注入和二元系数采样模拟语言模型解码。

Result: 理论和实证分析揭示了推理行为的新见解，支持对实际语言模型的理解。

Conclusion: 该框架为理解语言模型推理行为提供了新的理论支持，展示了随机性和采样的潜力。

Abstract: Using more test-time computation during language model inference, such as
generating more intermediate thoughts or sampling multiple candidate answers,
has proven effective in significantly improving model performance. This paper
takes an initial step toward bridging the gap between practical language model
inference and theoretical transformer analysis by incorporating randomness and
sampling. We focus on in-context linear regression with continuous/binary
coefficients, where our framework simulates language model decoding through
noise injection and binary coefficient sampling. Through this framework, we
provide detailed analyses of widely adopted inference techniques. Supported by
empirical results, our theoretical framework and analysis demonstrate the
potential for offering new insights into understanding inference behaviors in
real-world language models.

</details>


### [124] [When and how can inexact generative models still sample from the data manifold?](https://arxiv.org/abs/2508.07581)
*Nisha Chandramoorthy,Adriaan de Clercq*

Main category: cs.LG

TL;DR: 论文研究了生成模型中学习误差对数据分布支持的影响，发现微小误差仅导致预测密度在数据流形上变化，并揭示了支持鲁棒性的动力学机制。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型中学习误差如何影响生成样本的分布支持，尤其是为何误差不会使样本偏离数据流形。

Method: 采用动力学系统方法分析生成过程的概率流，通过扰动分析研究支持鲁棒性，并提出Lyapunov向量与数据流形切空间对齐的条件。

Result: 发现微小学习误差仅改变数据流形上的预测密度，证明了支持鲁棒性的动力学机制，并提供了高效计算对齐条件的方法。

Conclusion: 研究为生成模型的理论保证提供了新视角，适用于多种动态生成模型和目标分布。

Abstract: A curious phenomenon observed in some dynamical generative models is the
following: despite learning errors in the score function or the drift vector
field, the generated samples appear to shift \emph{along} the support of the
data distribution but not \emph{away} from it. In this work, we investigate
this phenomenon of \emph{robustness of the support} by taking a dynamical
systems approach on the generating stochastic/deterministic process. Our
perturbation analysis of the probability flow reveals that infinitesimal
learning errors cause the predicted density to be different from the target
density only on the data manifold for a wide class of generative models.
Further, what is the dynamical mechanism that leads to the robustness of the
support? We show that the alignment of the top Lyapunov vectors (most sensitive
infinitesimal perturbation directions) with the tangent spaces along the
boundary of the data manifold leads to robustness and prove a sufficient
condition on the dynamics of the generating process to achieve this alignment.
Moreover, the alignment condition is efficient to compute and, in practice, for
robust generative models, automatically leads to accurate estimates of the
tangent bundle of the data manifold. Using a finite-time linear perturbation
analysis on samples paths as well as probability flows, our work complements
and extends existing works on obtaining theoretical guarantees for generative
models from a stochastic analysis, statistical learning and uncertainty
quantification points of view. Our results apply across different dynamical
generative models, such as conditional flow-matching and score-based generative
models, and for different target distributions that may or may not satisfy the
manifold hypothesis.

</details>


### [125] [Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization](https://arxiv.org/abs/2508.07629)
*Zhenpeng Su,Leiyu Pan,Xue Bai,Dening Liu,Guanting Dong,Jiaming Huang,Wenping Hu,Guorui Zhou*

Main category: cs.LG

TL;DR: Klear-Reasoner是一个具有长推理能力的模型，通过详细的工作流程分析和改进的强化学习方法，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前社区中高性能推理模型的复现问题较多，主要由于训练细节披露不完整。本文旨在提供完整的训练流程分析，并提出改进方法。

Method: 包括数据准备、长链思维监督微调（long CoT SFT）和强化学习（RL），并提出GPPO方法解决RL中的裁剪问题。

Result: Klear-Reasoner在数学和编程任务中表现卓越，如AIME 2024（90.5%）和LiveCodeBench V6（58.1%）。

Conclusion: 高质量数据和小样本难例更有效，GPPO方法显著提升了模型的探索能力和学习效率。

Abstract: We present Klear-Reasoner, a model with long reasoning capabilities that
demonstrates careful deliberation during problem solving, achieving outstanding
performance across multiple benchmarks. Although there are already many
excellent works related to inference models in the current community, there are
still many problems with reproducing high-performance inference models due to
incomplete disclosure of training details. This report provides an in-depth
analysis of the reasoning model, covering the entire post-training workflow
from data preparation and long Chain-of-Thought supervised fine-tuning (long
CoT SFT) to reinforcement learning (RL), along with detailed ablation studies
for each experimental component. For SFT data, our experiments show that a
small number of high-quality data sources are more effective than a large
number of diverse data sources, and that difficult samples can achieve better
results without accuracy filtering. In addition, we investigate two key issues
with current clipping mechanisms in RL: Clipping suppresses critical
exploration signals and ignores suboptimal trajectories. To address these
challenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)
that gently backpropagates gradients from clipped tokens. GPPO not only
enhances the model's exploration capacity but also improves its efficiency in
learning from negative samples. Klear-Reasoner exhibits exceptional reasoning
abilities in mathematics and programming, scoring 90.5\% on AIME 2024, 83.2\%
on AIME 2025, 66.0\% on LiveCodeBench V5 and 58.1\% on LiveCodeBench V6.

</details>


### [126] [Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo](https://arxiv.org/abs/2508.07631)
*Advait Parulekar,Litu Rout,Karthikeyan Shanmugam,Sanjay Shakkottai*

Main category: cs.LG

TL;DR: 论文研究了基于分数的生成模型中后验采样的问题，提出了一种在多项式时间内近似采样的方法，确保样本与测量和先验一致。


<details>
  <summary>Details</summary>
Motivation: 尽管后验采样在KL散度下通常是难解的，但实际应用中许多算法仍能成功。本文旨在探索一种更通用的“倾斜”方法，无需严格假设即可实现近似采样。

Method: 通过将后验采样视为“倾斜”问题，提出了一种方法，能够在噪声先验的KL散度和真实后验的Fisher散度下同时接近目标分布。

Result: 证明了在多项式时间内可以采样到一个分布，该分布同时接近噪声先验的后验和真实后验。

Conclusion: 这是首次在多项式时间内实现近似后验采样的正式结果，为实际应用提供了理论支持。

Abstract: We study the problem of posterior sampling in the context of score based
generative models. We have a trained score network for a prior $p(x)$, a
measurement model $p(y|x)$, and are tasked with sampling from the posterior
$p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case)
under well-accepted computational hardness assumptions. Despite this, popular
algorithms for tasks such as image super-resolution, stylization, and
reconstruction enjoy empirical success. Rather than establishing distributional
assumptions or restricted settings under which exact posterior sampling is
tractable, we view this as a more general "tilting" problem of biasing a
distribution towards a measurement. Under minimal assumptions, we show that one
can tractably sample from a distribution that is simultaneously close to the
posterior of a noised prior in KL divergence and the true posterior in Fisher
divergence. Intuitively, this combination ensures that the resulting sample is
consistent with both the measurement and the prior. To the best of our
knowledge these are the first formal results for (approximate) posterior
sampling in polynomial time.

</details>


### [127] [Attribution Explanations for Deep Neural Networks: A Theoretical Perspective](https://arxiv.org/abs/2508.07636)
*Huiqi Deng,Hongbin Pei,Quanshi Zhang,Mengnan Du*

Main category: cs.LG

TL;DR: 论文探讨了深度神经网络（DNNs）的归因解释方法的可靠性问题，提出了三个核心挑战，并总结了解决这些挑战的理论进展。


<details>
  <summary>Details</summary>
Motivation: 归因解释方法在解释DNNs时缺乏统一性和理论基础，导致其可靠性受到质疑。

Method: 通过理论统一、理论基础和理论评估三个方向，系统比较和验证归因方法的有效性。

Result: 总结了当前的理论进展，为归因方法的理解和选择提供了指导。

Conclusion: 论文指出了未来研究的开放性问题，强调了理论深化和方法创新的重要性。

Abstract: Attribution explanation is a typical approach for explaining deep neural
networks (DNNs), inferring an importance or contribution score for each input
variable to the final output. In recent years, numerous attribution methods
have been developed to explain DNNs. However, a persistent concern remains
unresolved, i.e., whether and which attribution methods faithfully reflect the
actual contribution of input variables to the decision-making process. The
faithfulness issue undermines the reliability and practical utility of
attribution explanations. We argue that these concerns stem from three core
challenges. First, difficulties arise in comparing attribution methods due to
their unstructured heterogeneity, differences in heuristics, formulations, and
implementations that lack a unified organization. Second, most methods lack
solid theoretical underpinnings, with their rationales remaining absent,
ambiguous, or unverified. Third, empirically evaluating faithfulness is
challenging without ground truth. Recent theoretical advances provide a
promising way to tackle these challenges, attracting increasing attention. We
summarize these developments, with emphasis on three key directions: (i)
Theoretical unification, which uncovers commonalities and differences among
methods, enabling systematic comparisons; (ii) Theoretical rationale,
clarifying the foundations of existing methods; (iii) Theoretical evaluation,
rigorously proving whether methods satisfy faithfulness principles. Beyond a
comprehensive review, we provide insights into how these studies help deepen
theoretical understanding, inform method selection, and inspire new attribution
methods. We conclude with a discussion of promising open problems for further
work.

</details>


### [128] [Extracting Complex Topology from Multivariate Functional Approximation: Contours, Jacobi Sets, and Ridge-Valley Graphs](https://arxiv.org/abs/2508.07637)
*Guanqun Ma,David Lenz,Hanqi Guo,Tom Peterka,Bei Wang*

Main category: cs.LG

TL;DR: 提出首个直接从连续隐式模型（如MFA）中提取复杂拓扑特征的框架，无需离散化。


<details>
  <summary>Details</summary>
Motivation: 连续隐式模型（如MFA）为科学数据提供了连续、高阶、可微的表示，但缺乏直接提取拓扑特征的方法。

Method: 基于MFA模型，直接提取轮廓线、Jacobi集和脊谷图等拓扑特征，支持函数值和高阶导数查询。

Result: 实现了直接从连续隐式模型中提取复杂拓扑特征，适用于任何支持函数值和导数查询的模型。

Conclusion: 为连续隐式模型的拓扑数据分析和可视化奠定了基础。

Abstract: Implicit continuous models, such as functional models and implicit neural
networks, are an increasingly popular method for replacing discrete data
representations with continuous, high-order, and differentiable surrogates.
These models offer new perspectives on the storage, transfer, and analysis of
scientific data. In this paper, we introduce the first framework to directly
extract complex topological features -- contours, Jacobi sets, and ridge-valley
graphs -- from a type of continuous implicit model known as multivariate
functional approximation (MFA). MFA replaces discrete data with continuous
piecewise smooth functions. Given an MFA model as the input, our approach
enables direct extraction of complex topological features from the model,
without reverting to a discrete representation of the model. Our work is easily
generalizable to any continuous implicit model that supports the queries of
function values and high-order derivatives. Our work establishes the building
blocks for performing topological data analysis and visualization on implicit
continuous models.

</details>


### [129] [Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals](https://arxiv.org/abs/2508.07638)
*Jia Zhang,Yao Liu,Chen-Xi Zhang,Yi Liu,Yi-Xuan Jin,Lan-Zhe Guo,Yu-Feng Li*

Main category: cs.LG

TL;DR: 论文提出了一种基于数据选择的方法（DMPO），通过量化偏好分歧（PD）来优化LLM对齐，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如DPO）在细粒度偏好数据中存在噪声和冲突问题，需要更高效的数据选择策略。

Method: 提出DMPO目标，利用PD项量化偏好冲突，并设计数据选择原则，选择高共识数据用于训练。

Result: 在UltraFeedback数据集上，该方法相对现有方法提升了10%的性能，同时提高了训练效率。

Conclusion: 通过数据选择策略，DMPO有效解决了偏好冲突问题，为LLM对齐提供了新思路。

Abstract: Aligning Large Language Models (LLMs) with diverse human values requires
moving beyond a single holistic "better-than" preference criterion. While
collecting fine-grained, aspect-specific preference data is more reliable and
scalable, existing methods like Direct Preference Optimization (DPO) struggle
with the severe noise and conflicts inherent in such aggregated datasets. In
this paper, we tackle this challenge from a data-centric perspective. We first
derive the Direct Multi-Preference Optimization (DMPO) objective, and uncover a
key Preference Divergence (PD) term that quantifies inter-aspect preference
conflicts. Instead of using this term for direct optimization, we leverage it
to formulate a novel, theoretically-grounded data selection principle. Our
principle advocates for selecting a subset of high-consensus data-identified by
the most negative PD values-for efficient DPO training. We prove the optimality
of this strategy by analyzing the loss bounds of the DMPO objective in the
selection problem. To operationalize our approach, we introduce practical
methods of PD term estimation and length bias mitigation, thereby proposing our
PD selection method. Evaluation on the UltraFeedback dataset with three varying
conflict levels shows that our simple yet effective strategy achieves over 10%
relative improvement against both the standard holistic preference and a
stronger oracle using aggregated preference signals, all while boosting
training efficiency and obviating the need for intractable holistic preference
annotating, unlocking the potential of robust LLM alignment via fine-grained
preference signals.

</details>


### [130] [Multi-Turn Jailbreaks Are Simpler Than They Seem](https://arxiv.org/abs/2508.07646)
*Xiaoxue Yang,Jaeha Lee,Anna-Katharina Dick,Jasper Timm,Fei Xie,Diogo Cruz*

Main category: cs.LG

TL;DR: 论文分析了多轮越狱攻击对大型语言模型的威胁，发现其成功率与单轮攻击重复尝试相当，且攻击效果在相似模型间具有相关性。


<details>
  <summary>Details</summary>
Motivation: 研究多轮越狱攻击的漏洞，挑战其复杂性的认知，并为AI安全评估和防御设计提供依据。

Method: 使用StrongREJECT基准对GPT-4、Claude和Gemini等先进模型进行多轮越狱攻击的实证分析。

Result: 多轮攻击成功率与单轮攻击重复尝试相近，攻击效果在相似模型间相关，推理能力强的模型更容易被攻击。

Conclusion: 研究结果对AI安全评估和防御设计具有重要意义，代码已开源。

Abstract: While defenses against single-turn jailbreak attacks on Large Language Models
(LLMs) have improved significantly, multi-turn jailbreaks remain a persistent
vulnerability, often achieving success rates exceeding 70% against models
optimized for single-turn protection. This work presents an empirical analysis
of automated multi-turn jailbreak attacks across state-of-the-art models
including GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark.
Our findings challenge the perceived sophistication of multi-turn attacks: when
accounting for the attacker's ability to learn from how models refuse harmful
requests, multi-turn jailbreaking approaches are approximately equivalent to
simply resampling single-turn attacks multiple times. Moreover, attack success
is correlated among similar models, making it easier to jailbreak newly
released ones. Additionally, for reasoning models, we find surprisingly that
higher reasoning effort often leads to higher attack success rates. Our results
have important implications for AI safety evaluation and the design of
jailbreak-resistant systems. We release the source code at
https://github.com/diogo-cruz/multi_turn_simpler

</details>


### [131] [Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning](https://arxiv.org/abs/2508.07659)
*Hyeon-Ju Jeon,Jeon-Ho Kang,In-Hyuk Kwon,O-Joun Lee*

Main category: cs.LG

TL;DR: 该研究旨在通过时空图神经网络（STGNNs）和结构学习，动态捕捉地球观测与大气状态之间的空间相关性，以提高全球大气状态估计的预测精度。通过自适应调节节点度和空间距离，解决了结构学习中的信息丢失和过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 传统数值天气预报（NWP）系统在固定网格点上预测大气状态，但地球观测数据的位置不固定，导致动态的空间相关性难以捕捉。研究旨在解决这一问题。

Method: 采用时空图神经网络（STGNNs）结合结构学习，动态建模空间相关性。通过自适应节点度调节和空间距离考虑，优化边缘采样以避免信息丢失和过平滑。

Result: 在东亚地区的真实大气状态和观测数据上验证，该方法在高变异性区域优于现有STGNN模型（无论是否使用结构学习）。

Conclusion: 提出的方法有效解决了动态空间相关性建模的挑战，显著提升了大气状态估计的预测精度。

Abstract: This study aims to discover spatial correlations between Earth observations
and atmospheric states to improve the forecasting accuracy of global
atmospheric state estimation, which are usually conducted using conventional
numerical weather prediction (NWP) systems and is the beginning of weather
forecasting. NWP systems predict future atmospheric states at fixed locations,
which are called NWP grid points, by analyzing previous atmospheric states and
newly acquired Earth observations without fixed locations. Thus, surrounding
meteorological context and the changing locations of the observations make
spatial correlations between atmospheric states and observations over time. To
handle complicated spatial correlations, which change dynamically, we employ
spatiotemporal graph neural networks (STGNNs) with structure learning. However,
structure learning has an inherent limitation that this can cause structural
information loss and over-smoothing problem by generating excessive edges. To
solve this problem, we regulate edge sampling by adaptively determining node
degrees and considering the spatial distances between NWP grid points and
observations. We validated the effectiveness of the proposed method by using
real-world atmospheric state and observation data from East Asia. Even in areas
with high atmospheric variability, the proposed method outperformed existing
STGNN models with and without structure learning.

</details>


### [132] [GLiClass: Generalist Lightweight Model for Sequence Classification Tasks](https://arxiv.org/abs/2508.07662)
*Ihor Stepanov,Mykhailo Shtopko,Dmytro Vodianytskyi,Oleksandr Lukashov,Alexander Yavorskyi,Mykyta Yaroshenko*

Main category: cs.LG

TL;DR: GLiClass是一种基于GLiNER架构的新方法，用于序列分类任务，结合了高效率和零样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统需要高效且准确的分类方法，同时需适应动态变化的用户需求，而现有方法如生成式LLMs和交叉编码器各有不足。

Method: 提出GLiClass，改进GLiNER架构，并采用近端策略优化（PPO）进行多标签文本分类训练。

Result: GLiClass在准确性和效率上与基于嵌入的方法相当，同时支持零样本和少样本学习。

Conclusion: GLiClass为动态分类需求提供了一种高效灵活的解决方案。

Abstract: Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative LLMs have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-sparse conditions or from human feedback.

</details>


### [133] [AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting](https://arxiv.org/abs/2508.07668)
*Hyobin Park,Jinwook Jung,Minseok Seo,Hyunsoo Choi,Deukjae Cho,Sekil Park,Dong-Geol Choi*

Main category: cs.LG

TL;DR: AIS-LLM是一个结合时间序列AIS数据和大型语言模型（LLM）的新框架，用于同时处理船舶轨迹预测、异常检测和碰撞风险评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独处理海事任务，难以全面考虑复杂情况，因此提出AIS-LLM以整合多任务分析。

Method: AIS-LLM包括时间序列编码器、LLM提示编码器、跨模态对齐模块和多任务解码器，实现端到端多任务处理。

Result: 实验表明AIS-LLM在各项任务中优于现有方法，并能生成综合情境摘要。

Conclusion: AIS-LLM为海事交通管理提供了更智能高效的解决方案。

Abstract: With the increase in maritime traffic and the mandatory implementation of the
Automatic Identification System (AIS), the importance and diversity of maritime
traffic analysis tasks based on AIS data, such as vessel trajectory prediction,
anomaly detection, and collision risk assessment, is rapidly growing. However,
existing approaches tend to address these tasks individually, making it
difficult to holistically consider complex maritime situations. To address this
limitation, we propose a novel framework, AIS-LLM, which integrates time-series
AIS data with a large language model (LLM). AIS-LLM consists of a Time-Series
Encoder for processing AIS sequences, an LLM-based Prompt Encoder, a
Cross-Modality Alignment Module for semantic alignment between time-series data
and textual prompts, and an LLM-based Multi-Task Decoder. This architecture
enables the simultaneous execution of three key tasks: trajectory prediction,
anomaly detection, and risk assessment of vessel collisions within a single
end-to-end system. Experimental results demonstrate that AIS-LLM outperforms
existing methods across individual tasks, validating its effectiveness.
Furthermore, by integratively analyzing task outputs to generate situation
summaries and briefings, AIS-LLM presents the potential for more intelligent
and efficient maritime traffic management.

</details>


### [134] [Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation](https://arxiv.org/abs/2508.07675)
*Xutong Liu,Baran Atalar,Xiangxiang Dai,Jinhang Zuo,Siwei Wang,John C. S. Lui,Wei Chen,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 提出了一种基于学习的原则性语义缓存框架，解决LLM高推理成本问题，通过优化和在线学习算法提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM的高推理成本带来可扩展性和可持续性挑战，传统缓存方法无法处理语义相似性，现有语义缓存方法缺乏理论基础和适应性。

Method: 提出离线优化和在线学习算法，处理未知查询和成本分布，设计高效算法。

Result: 在合成数据集上验证，算法性能优于或匹配基线方法。

Conclusion: 该框架为语义缓存提供了理论基础和实用解决方案，适用于现实世界的不确定性。

Abstract: Large Language Models (LLMs) are revolutionizing how users interact with
information systems, yet their high inference cost poses serious scalability
and sustainability challenges. Caching inference responses, allowing them to be
retrieved without another forward pass through the LLM, has emerged as one
possible solution. Traditional exact-match caching, however, overlooks the
semantic similarity between queries, leading to unnecessary recomputation.
Semantic caching addresses this by retrieving responses based on semantic
similarity, but introduces a fundamentally different cache eviction problem:
one must account for mismatch costs between incoming queries and cached
responses. Moreover, key system parameters, such as query arrival probabilities
and serving costs, are often unknown and must be learned over time. Existing
semantic caching methods are largely ad-hoc, lacking theoretical foundations
and unable to adapt to real-world uncertainty. In this paper, we present a
principled, learning-based framework for semantic cache eviction under unknown
query and cost distributions. We formulate both offline optimization and online
learning variants of the problem, and develop provably efficient algorithms
with state-of-the-art guarantees. We also evaluate our framework on a synthetic
dataset, showing that our proposed algorithms perform matching or superior
performance compared with baselines.

</details>


### [135] [Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks](https://arxiv.org/abs/2508.07676)
*Chenchen Lin,Xuehe Wang*

Main category: cs.LG

TL;DR: 提出了一种社交感知的联邦学习隐私保护机制，通过多跳传播模型量化间接隐私泄露，采用Stackelberg博弈优化激励策略，提升客户端效用并降低服务器成本。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中社交网络连接的客户端隐私泄露不仅取决于自身策略，还受他人决策影响，需系统性解决间接隐私泄露问题。

Method: 设计两阶段Stackelberg博弈，服务器优化激励策略，客户端选择隐私预算；引入均值场估计器近似外部隐私风险。

Result: 理论证明均值场估计器的收敛性及Stackelberg纳什均衡的闭式解，实验显示客户端效用提升、服务器成本降低且模型性能保持。

Conclusion: 该机制在客户端激励视角下实现近似最优社会福利，优于社交无关基线及考虑社交外部性的方法。

Abstract: Federated learning (FL) enables collaborative model training across
decentralized clients without sharing local data, thereby enhancing privacy and
facilitating collaboration among clients connected via social networks.
However, these social connections introduce privacy externalities: a client's
privacy loss depends not only on its privacy protection strategy but also on
the privacy decisions of others, propagated through the network via multi-hop
interactions. In this work, we propose a socially-aware privacy-preserving FL
mechanism that systematically quantifies indirect privacy leakage through a
multi-hop propagation model. We formulate the server-client interaction as a
two-stage Stackelberg game, where the server, as the leader, optimizes
incentive policies, and clients, as followers, strategically select their
privacy budgets, which determine their privacy-preserving levels by controlling
the magnitude of added noise. To mitigate information asymmetry in networked
privacy estimation, we introduce a mean-field estimator to approximate the
average external privacy risk. We theoretically prove the existence and
convergence of the fixed point of the mean-field estimator and derive
closed-form expressions for the Stackelberg Nash Equilibrium. Despite being
designed from a client-centric incentive perspective, our mechanism achieves
approximately-optimal social welfare, as revealed by Price of Anarchy (PoA)
analysis. Experiments on diverse datasets demonstrate that our approach
significantly improves client utilities and reduces server costs while
maintaining model performance, outperforming both Social-Agnostic (SA)
baselines and methods that account for social externalities.

</details>


### [136] [MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation](https://arxiv.org/abs/2508.07681)
*Yooseok Lim,ByoungJun Jeon,Seong-A Park,Jisoo Lee,Sae Won Choi,Chang Wook Jeong,Ho-Geol Ryu,Hongyeol Lee,Hyun-Lim Yang*

Main category: cs.LG

TL;DR: MORE-CLEAR框架利用预训练语言模型和多模态数据提升脓毒症管理的强化学习效果。


<details>
  <summary>Details</summary>
Motivation: 脓毒症早期检测和管理至关重要，但现有强化学习方法依赖结构化数据，缺乏对患者状态的全面理解。

Method: 结合预训练语言模型提取临床笔记的语义信息，通过门控融合和跨模态注意力动态整合多模态数据。

Result: 在公开和私有数据集上验证，MORE-CLEAR显著提升生存率和策略性能。

Conclusion: 该框架首次将语言模型能力引入多模态离线强化学习，有望改善脓毒症治疗。

Abstract: Sepsis, a life-threatening inflammatory response to infection, causes organ
dysfunction, making early detection and optimal management critical. Previous
reinforcement learning (RL) approaches to sepsis management rely primarily on
structured data, such as lab results or vital signs, and on a dearth of a
comprehensive understanding of the patient's condition. In this work, we
propose a Multimodal Offline REinforcement learning for Clinical notes
Leveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis
control in intensive care units. MORE-CLEAR employs pre-trained large-scale
language models (LLMs) to facilitate the extraction of rich semantic
representations from clinical notes, preserving clinical context and improving
patient state representation. Gated fusion and cross-modal attention allow
dynamic weight adjustment in the context of time and the effective integration
of multimodal data. Extensive cross-validation using two public (MIMIC-III and
MIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly
improves estimated survival rate and policy performance compared to
single-modal RL approaches. To our knowledge, this is the first to leverage LLM
capabilities within a multimodal offline RL for better state representation in
medical applications. This approach can potentially expedite the treatment and
management of sepsis by enabling reinforcement learning models to propose
enhanced actions based on a more comprehensive understanding of patient
conditions.

</details>


### [137] [Semantic-Enhanced Time-Series Forecasting via Large Language Models](https://arxiv.org/abs/2508.07697)
*Hao Liu,Chun Yang,Zhang xiaoxing,Xiaobin Zhu*

Main category: cs.LG

TL;DR: 提出了一种语义增强的大型语言模型（SE-LLM），通过嵌入时间序列的周期性和异常特征来增强语义表示，解决了现有研究中模态对齐不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注语言模型在时间序列预测中的通用性，但忽略了语言知识与时间序列数据模式之间的内在模态差异，限制了语义表示能力。

Method: 提出SE-LLM，通过嵌入时间序列的周期性和异常特征增强语义空间，并设计了一个插件模块来同时建模长短期依赖关系。

Result: 实验表明，SE-LLM在性能上优于现有最先进方法。

Conclusion: SE-LLM通过语义增强和依赖关系建模，显著提升了时间序列预测的性能和解释性。

Abstract: Time series forecasting plays a significant role in finance, energy,
meteorology, and IoT applications. Recent studies have leveraged the
generalization capabilities of large language models (LLMs) to adapt to time
series forecasting, achieving promising performance. However, existing studies
focus on token-level modal alignment, instead of bridging the intrinsic
modality gap between linguistic knowledge structures and time series data
patterns, greatly limiting the semantic representation. To address this issue,
we propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent
periodicity and anomalous characteristics of time series to embed into the
semantic space to enhance the token embedding. This process enhances the
interpretability of tokens for LLMs, thereby activating the potential of LLMs
for temporal sequence analysis. Moreover, existing Transformer-based LLMs excel
at capturing long-range dependencies but are weak at modeling short-term
anomalies in time-series data. Hence, we propose a plugin module embedded
within self-attention that models long-term and short-term dependencies to
effectively adapt LLMs to time-series analysis. Our approach freezes the LLM
and reduces the sequence dimensionality of tokens, greatly reducing
computational consumption. Experiments demonstrate the superiority performance
of our SE-LLM against the state-of-the-art (SOTA) methods.

</details>


### [138] [Energy Consumption in Parallel Neural Network Training](https://arxiv.org/abs/2508.07706)
*Philipp Huber,David Li,Juan Pedro Gutiérrez Hermosillo Muriedas,Deifilia Kieckhefen,Markus Götz,Achim Streit,Charlotte Debus*

Main category: cs.LG

TL;DR: 论文研究了并行化训练神经网络对能源消耗的影响，发现能源消耗与GPU小时数近似线性相关，但具体因素因模型和硬件而异。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络训练对计算资源需求的增加，能源消耗问题日益突出，但并行化对能源消耗的影响尚未充分研究。

Method: 通过数据并行训练ResNet50和FourCastNet模型，评估GPU数量、全局批次大小和局部批次大小对性能、训练时间和能源消耗的影响。

Result: 能源消耗与GPU小时数近似线性相关，但具体比例因模型和硬件不同，且受每GPU小时的样本数和梯度更新数影响。

Conclusion: 研究揭示了神经网络训练扩展与能源消耗的复杂关系，为未来可持续AI研究提供了参考。

Abstract: The increasing demand for computational resources of training neural networks
leads to a concerning growth in energy consumption. While parallelization has
enabled upscaling model and dataset sizes and accelerated training, its impact
on energy consumption is often overlooked. To close this research gap, we
conducted scaling experiments for data-parallel training of two models,
ResNet50 and FourCastNet, and evaluated the impact of parallelization
parameters, i.e., GPU count, global batch size, and local batch size, on
predictive performance, training time, and energy consumption. We show that
energy consumption scales approximately linearly with the consumed resources,
i.e., GPU hours; however, the respective scaling factor differs substantially
between distinct model trainings and hardware, and is systematically influenced
by the number of samples and gradient updates per GPU hour. Our results shed
light on the complex interplay of scaling up neural network training and can
inform future developments towards more sustainable AI research.

</details>


### [139] [Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer](https://arxiv.org/abs/2508.07710)
*Jingya Wang,Xin Deng,Wenjie Wei,Dehao Zhang,Shuai Wang,Qian Sun,Jieyuan Zhang,Hanwen Liu,Ning Xie,Malu Zhang*

Main category: cs.LG

TL;DR: 提出了一种高性能、无需训练的ANN-to-SNN转换框架，用于Transformer架构，解决了现有方法在非线性操作和额外微调上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有ANN-to-SNN转换方法无法有效处理Transformer中的非线性操作，且需要额外微调，限制了其效率和可扩展性。

Method: 引入多基指数衰减（MBE）神经元，通过指数衰减策略和多基编码方法高效近似非线性操作，无需修改预训练ANN的权重。

Result: 在多种任务（CV、NLU、NLG）和主流Transformer架构（ViT、RoBERTa、GPT-2）上，实现了近乎无损的转换精度和显著降低的延迟。

Conclusion: 该方法为Spiking Transformer在实际应用中的高效和可扩展部署提供了可行路径。

Abstract: Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a
promising approach for constructing energy-efficient Transformer architectures.
Compared to directly trained Spiking Transformers, ANN-to-SNN conversion
methods bypass the high training costs. However, existing methods still suffer
from notable limitations, failing to effectively handle nonlinear operations in
Transformer architectures and requiring additional fine-tuning processes for
pre-trained ANNs. To address these issues, we propose a high-performance and
training-free ANN-to-SNN conversion framework tailored for Transformer
architectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE)
neuron, which employs an exponential decay strategy and multi-basis encoding
method to efficiently approximate various nonlinear operations. It removes the
requirement for weight modifications in pre-trained ANNs. Extensive experiments
across diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures
(ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless
conversion accuracy with significantly lower latency. This provides a promising
pathway for the efficient and scalable deployment of Spiking Transformers in
real-world applications.

</details>


### [140] [Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations](https://arxiv.org/abs/2508.07722)
*Pietro Talli,Federico Mason,Federico Chiariotti,Andrea Zanella*

Main category: cs.LG

TL;DR: 提出了一种名为HR3L的新架构，用于在非理想无线信道上训练远程强化学习（RL）代理，解决了传统方法在通信延迟和丢包情况下的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RL代理需要即时感知状态变化，但在无线通信网络中，由于延迟和丢包，代理只能获取部分和间歇性信息。现有解决方案计算负担大且不够通用。

Method: HR3L架构包含发射器和接收器两个单元，发射器编码环境状态，接收器解码并执行动作以最大化奖励信号，无需交换梯度信息。

Result: 实验表明，HR3L在样本效率和适应不同通信场景（如丢包、延迟和容量限制）方面显著优于基线方法。

Conclusion: HR3L为在非理想通信条件下训练RL代理提供了一种高效且通用的解决方案。

Abstract: In this work, we address the problem of training Reinforcement Learning (RL)
agents over communication networks. The RL paradigm requires the agent to
instantaneously perceive the state evolution to infer the effects of its
actions on the environment. This is impossible if the agent receives state
updates over lossy or delayed wireless systems and thus operates with partial
and intermittent information. In recent years, numerous frameworks have been
proposed to manage RL with imperfect feedback; however, they often offer
specific solutions with a substantial computational burden. To address these
limits, we propose a novel architecture, named Homomorphic Robust Remote
Reinforcement Learning (HR3L), that enables the training of remote RL agents
exchanging observations across a non-ideal wireless channel. HR3L considers two
units: the transmitter, which encodes meaningful representations of the
environment, and the receiver, which decodes these messages and performs
actions to maximize a reward signal. Importantly, HR3L does not require the
exchange of gradient information across the wireless channel, allowing for
quicker training and a lower communication overhead than state-of-the-art
solutions. Experimental results demonstrate that HR3L significantly outperforms
baseline methods in terms of sample efficiency and adapts to different
communication scenarios, including packet losses, delayed transmissions, and
capacity limitations.

</details>


### [141] [Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning](https://arxiv.org/abs/2508.07738)
*Jialu Zhou,Dianxi Shi,Shaowu Yang,Xinyu Wei,Mingyue Yang,Leqian Li,Mengzhu Wang,Chunping Qiu*

Main category: cs.LG

TL;DR: 论文提出了一种名为TRGE的方法，通过动态扩展预训练模型和设计两级路由策略，解决了多领域持续学习中的灾难性遗忘和前瞻性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 多领域持续学习面临任务间类别和分布变化的双重异质性挑战，现有方法难以同时解决灾难性遗忘和前瞻性遗忘。

Method: TRGE方法动态扩展CLIP模型，为每个任务分配专家组，并设计组内和组间路由策略，同时利用MLLMs生成任务描述以识别任务标识符。

Result: 实验表明，TRGE在多种设置下优于其他先进方法，且参数效率更高。

Conclusion: TRGE通过动态路由和任务标识符识别，有效解决了多领域持续学习中的遗忘问题，提升了模型性能。

Abstract: Multi-Domain Continual Learning (MDCL) acquires knowledge from sequential
tasks with shifting class sets and distribution. Despite the
Parameter-Efficient Fine-Tuning (PEFT) methods can adapt for this dual
heterogeneity, they still suffer from catastrophic forgetting and forward
forgetting. To address these challenges, we propose a Two-Level Routing Grouped
Mixture-of-Experts (TRGE) method. Firstly, TRGE dynamically expands the
pre-trained CLIP model, assigning specific expert group for each task to
mitigate catastrophic forgetting. With the number of experts continually grows
in this process, TRGE maintains the static experts count within the group and
introduces the intra-group router to alleviate routing overfitting caused by
the increasing routing complexity. Meanwhile, we design an inter-group routing
policy based on task identifiers and task prototype distance, which dynamically
selects relevant expert groups and combines their outputs to enhance inter-task
collaboration. Secondly, to get the correct task identifiers, we leverage
Multimodal Large Language Models (MLLMs) which own powerful multimodal
comprehension capabilities to generate semantic task descriptions and recognize
the correct task identifier. Finally, to mitigate forward forgetting, we
dynamically fuse outputs for unseen samples from the frozen CLIP model and TRGE
adapter based on training progress, leveraging both pre-trained and learned
knowledge. Through extensive experiments across various settings, our method
outperforms other advanced methods with fewer trainable parameters.

</details>


### [142] [Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment](https://arxiv.org/abs/2508.07750)
*Haowen Wang,Yun Yue,Zhiling Ye,Shuowen Zhang,Lei Fan,Jiaxin Liang,Jiadi Jiang,Cheng Wei,Jingyuan Deng,Xudong Han,Ji Li,Chunxiao Guo,Peng Wei,Jian Wang,Jinjie Gu*

Main category: cs.LG

TL;DR: GRAO（Group Relative Alignment Optimization）是一种结合SFT和RL优势的统一框架，通过多样本生成、组内相对优势加权和参考感知参数更新，显著提升了语言模型的对齐能力。


<details>
  <summary>Details</summary>
Motivation: 解决SFT和RL在语言模型对齐中的局限性，SFT受限于离线策略轨迹，RL样本效率低且依赖高质量基础模型。

Method: 提出GRAO框架，包含多样本生成策略、组内相对优势加权损失和参考感知参数更新。

Result: 在复杂任务中，GRAO相比SFT、DPO、PPO和GRPO分别提升了57.70%、17.65%、7.95%和5.18%。

Conclusion: GRAO为语言模型对齐提供了理论框架和实证支持，显著提升了效率和性能。

Abstract: Alignment methodologies have emerged as a critical pathway for enhancing
language model alignment capabilities. While SFT (supervised fine-tuning)
accelerates convergence through direct token-level loss intervention, its
efficacy is constrained by offline policy trajectory. In contrast,
RL(reinforcement learning) facilitates exploratory policy optimization, but
suffers from low sample efficiency and stringent dependency on high-quality
base models. To address these dual challenges, we propose GRAO (Group Relative
Alignment Optimization), a unified framework that synergizes the respective
strengths of SFT and RL through three key innovations: 1) A multi-sample
generation strategy enabling comparative quality assessment via reward
feedback; 2) A novel Group Direct Alignment Loss formulation leveraging
intra-group relative advantage weighting; 3) Reference-aware parameter updates
guided by pairwise preference dynamics. Our theoretical analysis establishes
GRAO's convergence guarantees and sample efficiency advantages over
conventional approaches. Comprehensive evaluations across complex human
alignment tasks demonstrate GRAO's superior performance, achieving
57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and
GRPO baselines respectively. This work provides both a theoretically grounded
alignment framework and empirical evidence for efficient capability evolution
in language models.

</details>


### [143] [Sparse Probabilistic Graph Circuits](https://arxiv.org/abs/2508.07763)
*Martin Rektoris,Milan Papež,Václav Šmídl,Tomáš Pevný*

Main category: cs.LG

TL;DR: 论文提出了一种稀疏概率图电路（SPGCs），解决了现有概率图电路（PGCs）在稠密图表示上的高复杂度问题，将复杂度从O(n²)降至O(n + m)，适用于稀疏图。


<details>
  <summary>Details</summary>
Motivation: 现有深度生成模型（DGMs）虽表达能力强，但因非线性导致概率推断不可解。PGCs虽解决了可解性问题，但其稠密图表示的高复杂度限制了可扩展性。

Method: 引入稀疏概率图电路（SPGCs），直接操作稀疏图表示，降低复杂度至O(n + m)。

Result: 实验证明SPGCs在药物设计中保留了精确推断能力，提升了内存效率和推断速度，性能与不可解DGMs相当。

Conclusion: SPGCs是一种高效且可扩展的生成模型，适用于稀疏图场景。

Abstract: Deep generative models (DGMs) for graphs achieve impressively high expressive
power thanks to very efficient and scalable neural networks. However, these
networks contain non-linearities that prevent analytical computation of many
standard probabilistic inference queries, i.e., these DGMs are considered
\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs)
address this issue by enabling \emph{tractable} probabilistic inference, they
operate on dense graph representations with $\mathcal{O}(n^2)$ complexity for
graphs with $n$ nodes and \emph{$m$ edges}. To address this scalability issue,
we introduce Sparse PGCs, a new class of tractable generative models that
operate directly on sparse graph representation, reducing the complexity to
$\mathcal{O}(n + m)$, which is particularly beneficial for $m \ll n^2$. In the
context of de novo drug design, we empirically demonstrate that SPGCs retain
exact inference capabilities, improve memory efficiency and inference speed,
and match the performance of intractable DGMs in key metrics.

</details>


### [144] [Pareto Multi-Objective Alignment for Language Models](https://arxiv.org/abs/2508.07768)
*Qiang He,Setareh Maghsudi*

Main category: cs.LG

TL;DR: 论文提出PAMA算法，解决大语言模型（LLM）多目标对齐问题，通过凸优化显著提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于RLHF的对齐方法仅优化单一奖励函数，无法捕捉人类偏好的多样性和复杂性，限制了LLM在实际场景中的适应性。

Method: 提出Pareto多目标对齐（PAMA）算法，将多目标RLHF转化为凸优化问题，计算复杂度从O(n^2*d)降至O(n)。

Result: PAMA在125M至7B参数的模型上表现出高效且稳健的多目标对齐能力，理论证明其收敛到Pareto稳定点。

Conclusion: PAMA为多目标对齐问题提供了高效且理论可靠的解决方案，推动了LLM在多样化实际场景中的应用。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications that require careful balancing of multiple, often conflicting,
objectives, such as informativeness versus conciseness, or helpfulness versus
creativity. However, current alignment methods, primarily based on RLHF,
optimize LLMs toward a single reward function, resulting in rigid behavior that
fails to capture the complexity and diversity of human preferences. This
limitation hinders the adaptability of LLMs to practical scenarios, making
multi-objective alignment (MOA) a critical yet underexplored area. To bridge
this gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and
computationally efficient algorithm designed explicitly for MOA in LLMs. In
contrast to computationally prohibitive multi-objective optimization (MOO)
methods, PAMA transforms multi-objective RLHF into a convex optimization with a
closed-form solution, significantly enhancing scalability. Traditional MOO
approaches suffer from prohibitive O(n^2*d) complexity, where d represents the
number of model parameters, typically in the billions for LLMs, rendering
direct optimization infeasible. PAMA reduces this complexity to O(n) where n is
the number of objectives, enabling optimization to be completed within
milliseconds. We provide theoretical guarantees that PAMA converges to a Pareto
stationary point, where no objective can be improved without degrading at least
one other. Extensive experiments across language models ranging from 125M to 7B
parameters demonstrate PAMA's robust and effective MOA capabilities, aligning
with its theoretical advantages. PAMA provides a highly efficient solution to
the MOA problem that was previously considered intractable, offering a
practical and theoretically grounded approach to aligning LLMs with diverse
human values, paving the way for versatile and adaptable real-world AI
deployments.

</details>


### [145] [Topological Feature Compression for Molecular Graph Neural Networks](https://arxiv.org/abs/2508.07807)
*Rahul Khorana*

Main category: cs.LG

TL;DR: 提出了一种结合高阶拓扑信号与标准分子特征的图神经网络（GNN）架构，平衡了预测准确性、可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 分子表示学习在化学信息学和生物信息学任务中取得了显著进展，但如何在预测准确性、可解释性和计算效率之间取得平衡仍是一个挑战。

Method: 引入了一种新型GNN架构，结合压缩的高阶拓扑信号与标准分子特征，捕捉全局几何信息并保持计算效率和可解释性。

Result: 在多个基准测试中表现优异，包括小分子和复杂材料数据集，实现了参数高效架构下的最佳性能和鲁棒性。

Conclusion: 该模型在准确性和鲁棒性上均优于现有方法，代码已开源。

Abstract: Recent advances in molecular representation learning have produced highly
effective encodings of molecules for numerous cheminformatics and
bioinformatics tasks. However, extracting general chemical insight while
balancing predictive accuracy, interpretability, and computational efficiency
remains a major challenge. In this work, we introduce a novel Graph Neural
Network (GNN) architecture that combines compressed higher-order topological
signals with standard molecular features. Our approach captures global
geometric information while preserving computational tractability and
human-interpretable structure. We evaluate our model across a range of
benchmarks, from small-molecule datasets to complex material datasets, and
demonstrate superior performance using a parameter-efficient architecture. We
achieve the best performing results in both accuracy and robustness across
almost all benchmarks. We open source all code \footnote{All code and results
can be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.

</details>


### [146] [EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning](https://arxiv.org/abs/2508.07809)
*Huanyu Liu,Jia Li,Chang Yu,Taozhi Chen,Yihong Dong,Lecheng Wang,Hu XiaoLong,Ge Li*

Main category: cs.LG

TL;DR: EvoCoT是一个自演化的课程学习框架，通过两阶段思维链优化解决强化学习中稀疏奖励问题，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在硬问题上因稀疏奖励导致学习效率低，且依赖更强模型或过滤难题，限制了扩展性和推理提升。

Method: EvoCoT通过自生成和验证思维链轨迹约束探索空间，逐步缩短轨迹以可控方式扩展空间。

Result: 实验表明EvoCoT能解决未解决问题，无需外部监督提升推理能力，兼容多种RL微调方法。

Conclusion: EvoCoT为稀疏奖励下的推理优化提供了有效解决方案，支持未来研究。

Abstract: Reinforcement learning with verifiable reward (RLVR) has become a promising
paradigm for post-training large language models (LLMs) to improve their
reasoning capability. However, when the rollout accuracy is low on hard
problems, the reward becomes sparse, limiting learning efficiency and causing
exploration bottlenecks. Existing approaches either rely on stronger LLMs for
distillation or filter out difficult problems, which limits scalability or
restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on
two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the
exploration space by self-generating and verifying CoT trajectories, then
gradually shortens them to expand the space in a controlled way. This enables
LLMs to stably learn from initially unsolved hard problems under sparse
rewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,
and Llama. Experiments show that EvoCoT enables LLMs to solve previously
unsolved problems, improves reasoning capability without external CoT
supervision, and is compatible with various RL fine-tuning methods. We release
the source code to support future research.

</details>


### [147] [Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant](https://arxiv.org/abs/2508.07887)
*Sabrina Namazova,Alessandra Brondetta,Younes Strittmatter,Matthew Nassar,Sebastian Musslick*

Main category: cs.LG

TL;DR: 论文探讨了模拟器在科学实践中的重要性，特别是行为科学中的参与者模拟器Centaur，但其生成行为与人类数据存在系统性差异。


<details>
  <summary>Details</summary>
Motivation: 模拟器在自然科学中已取得显著成果，如AlphaFold。行为科学需要类似的人类行为模拟器，以加速实验设计和假设测试。

Method: 评估Centaur（基于160个实验数据微调的LLM）作为参与者模拟器的核心标准，特别是其生成行为。

Result: Centaur预测准确度高，但生成行为与人类数据存在系统性差异。

Conclusion: Centaur虽在预测人类行为方面有进展，但尚未达到可靠参与者模拟器或准确认知模型的标准。

Abstract: Simulators have revolutionized scientific practice across the natural
sciences. By generating data that reliably approximate real-world phenomena,
they enable scientists to accelerate hypothesis testing and optimize
experimental designs. This is perhaps best illustrated by AlphaFold, a
Nobel-prize winning simulator in chemistry that predicts protein structures
from amino acid sequences, enabling rapid prototyping of molecular
interactions, drug targets, and protein functions. In the behavioral sciences,
a reliable participant simulator - a system capable of producing human-like
behavior across cognitive tasks - would represent a similarly transformative
advance. Recently, Binz et al. introduced Centaur, a large language model (LLM)
fine-tuned on human data from 160 experiments, proposing its use not only as a
model of cognition but also as a participant simulator for "in silico
prototyping of experimental studies", e.g., to advance automated cognitive
science. Here, we review the core criteria for a participant simulator and
assess how well Centaur meets them. Although Centaur demonstrates strong
predictive accuracy, its generative behavior - a critical criterion for a
participant simulator - systematically diverges from human data. This suggests
that, while Centaur is a significant step toward predicting human behavior, it
does not yet meet the standards of a reliable participant simulator or an
accurate model of cognition.

</details>


### [148] [Score Augmentation for Diffusion Models](https://arxiv.org/abs/2508.07926)
*Liang Hou,Yuan Gao,Boyuan Jiang,Xin Tao,Qi Yan,Renjie Liao,Pengfei Wan,Di Zhang,Kun Gai*

Main category: cs.LG

TL;DR: 论文提出ScoreAug，一种针对扩散模型的数据增强框架，通过处理噪声数据缓解过拟合问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成建模中表现优异，但在数据有限时存在过拟合问题，需要一种专门的数据增强方法。

Method: 提出ScoreAug框架，对噪声数据进行变换，并要求去噪器预测原始目标的增强结果，实现分数增强。

Result: 在多个基准测试中，ScoreAug显著提升了性能，有效缓解了过拟合，且与传统数据增强方法结合效果更佳。

Conclusion: ScoreAug是一种有效的扩散模型数据增强方法，能稳定收敛并避免数据泄漏问题。

Abstract: Diffusion models have achieved remarkable success in generative modeling.
However, this study confirms the existence of overfitting in diffusion model
training, particularly in data-limited regimes. To address this challenge, we
propose Score Augmentation (ScoreAug), a novel data augmentation framework
specifically designed for diffusion models. Unlike conventional augmentation
approaches that operate on clean data, ScoreAug applies transformations to
noisy data, aligning with the inherent denoising mechanism of diffusion.
Crucially, ScoreAug further requires the denoiser to predict the augmentation
of the original target. This design establishes an equivariant learning
objective, enabling the denoiser to learn scores across varied denoising
spaces, thereby realizing what we term score augmentation. We also
theoretically analyze the relationship between scores in different spaces under
general transformations. In experiments, we extensively validate ScoreAug on
multiple benchmarks including CIFAR-10, FFHQ, AFHQv2, and ImageNet, with
results demonstrating significant performance improvements over baselines.
Notably, ScoreAug effectively mitigates overfitting across diverse scenarios,
such as varying data scales and model capacities, while exhibiting stable
convergence properties. Another advantage of ScoreAug over standard data
augmentation lies in its ability to circumvent data leakage issues under
certain conditions. Furthermore, we show that ScoreAug can be synergistically
combined with traditional data augmentation techniques to achieve additional
performance gains.

</details>


### [149] [Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series Forecasting](https://arxiv.org/abs/2508.07927)
*Amal Saadallah,Abdulaziz Al-Ademi*

Main category: cs.LG

TL;DR: 提出了一种通过模型适应和选择提升DNN在非平稳时间序列预测中性能的框架。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境中时间序列预测的挑战，其中底层模式随时间变化。

Method: 离线训练基础DNN，分割验证子集以聚类主导模式，为每个聚类微调DNN，推理时根据相似度选择模型，并集成概念漂移检测。

Result: 在GluonTS库中的传统和先进DNN架构上均表现出显著性能提升。

Conclusion: 该框架通用性强，能有效适应非平稳环境中的模式变化。

Abstract: Time series forecasting poses significant challenges in non-stationary
environments where underlying patterns evolve over time. In this work, we
propose a novel framework that enhances deep neural network (DNN) performance
by leveraging specialized model adaptation and selection. Initially, a base DNN
is trained offline on historical time series data. A reserved validation subset
is then segmented to extract and cluster the most dominant patterns within the
series, thereby identifying distinct regimes. For each identified cluster, the
base DNN is fine-tuned to produce a specialized version that captures unique
pattern characteristics. At inference, the most recent input is matched against
the cluster centroids, and the corresponding fine-tuned version is deployed
based on the closest similarity measure. Additionally, our approach integrates
a concept drift detection mechanism to identify and adapt to emerging patterns
caused by non-stationary behavior. The proposed framework is generalizable
across various DNN architectures and has demonstrated significant performance
gains on both traditional DNNs and recent advanced architectures implemented in
the GluonTS library.

</details>


### [150] [Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters](https://arxiv.org/abs/2508.07952)
*Richard J. Fawley,Renato Cordeiro de Amorim*

Main category: cs.LG

TL;DR: SHARK是一种基于Shapley值的特征加权聚类算法，无需额外参数调整，通过分解k均值目标为Shapley值，高效计算特征相关性，并在实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高维或噪声数据中，传统聚类算法假设所有特征贡献均等，导致性能下降。特征加权方法虽有效，但通常需要额外参数调优。

Method: SHARK利用Shapley值量化特征相关性，将k均值目标分解为各特征的Shapley值，通过逆权重迭代调整特征重要性。

Result: 实验表明，SHARK在合成和真实数据集上表现优于现有方法，尤其在噪声场景中具有更高的鲁棒性和准确性。

Conclusion: SHARK提供了一种无需额外参数的特征加权聚类方法，显著提升了聚类性能，适用于高维和噪声数据。

Abstract: Clustering algorithms often assume all features contribute equally to the
data structure, an assumption that usually fails in high-dimensional or noisy
settings. Feature weighting methods can address this, but most require
additional parameter tuning. We propose SHARK (Shapley Reweighted $k$-means), a
feature-weighted clustering algorithm motivated by the use of Shapley values
from cooperative game theory to quantify feature relevance, which requires no
additional parameters beyond those in $k$-means. We prove that the $k$-means
objective can be decomposed into a sum of per-feature Shapley values, providing
an axiomatic foundation for unsupervised feature relevance and reducing Shapley
computation from exponential to polynomial time. SHARK iteratively re-weights
features by the inverse of their Shapley contribution, emphasising informative
dimensions and down-weighting irrelevant ones. Experiments on synthetic and
real-world data sets show that SHARK consistently matches or outperforms
existing methods, achieving superior robustness and accuracy, particularly in
scenarios where noise may be present. Software:
https://github.com/rickfawley/shark.

</details>


### [151] [WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer](https://arxiv.org/abs/2508.07970)
*Junyu Wu,Weiming Chang,Xiaotao Liu,Guanyou He,Tingfeng Xian,Haoqiang Hong,Boqi Chen,Haotao Tian,Tao Yang,Yunsheng Shi,Feng Lin,Ting Yao*

Main category: cs.LG

TL;DR: WeChat-YATT是一个简单、可扩展且平衡的RLHF训练框架，旨在解决现有框架在复杂多模态工作流和动态工作负载中的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF训练框架在扩展性和动态工作负载管理方面存在局限性，尤其是在大规模模型和复杂流程中。

Method: WeChat-YATT采用并行控制器编程模型和动态资源分配策略，优化了RLHF工作流的编排和资源利用。

Result: 实验表明，WeChat-YATT在吞吐量上显著优于现有框架，并已成功应用于微信产品的大规模模型训练。

Conclusion: WeChat-YATT有效解决了RLHF训练中的扩展性和效率问题，并在实际应用中验证了其鲁棒性。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent
paradigm for training large language models and multimodal systems. Despite
notable advances enabled by existing RLHF training frameworks, significant
challenges remain in scaling to complex multimodal workflows and adapting to
dynamic workloads. In particular, current systems often encounter limitations
related to controller scalability when managing large models, as well as
inefficiencies in orchestrating intricate RLHF pipelines, especially in
scenarios that require dynamic sampling and resource allocation. In this paper,
we introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,
scalable, and balanced RLHF training framework specifically designed to address
these challenges. WeChat-YATT features a parallel controller programming model
that enables flexible and efficient orchestration of complex RLHF workflows,
effectively mitigating the bottlenecks associated with centralized controller
architectures and facilitating scalability in large-scale data scenarios. In
addition, we propose a dynamic placement schema that adaptively partitions
computational resources and schedules workloads, thereby significantly reducing
hardware idle time and improving GPU utilization under variable training
conditions. We evaluate WeChat-YATT across a range of experimental scenarios,
demonstrating that it achieves substantial improvements in throughput compared
to state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been
successfully deployed to train models supporting WeChat product features for a
large-scale user base, underscoring its effectiveness and robustness in
real-world applications.

</details>


### [152] [A Physics-informed Deep Operator for Real-Time Freeway Traffic State Estimation](https://arxiv.org/abs/2508.08002)
*Hongxin Yu,Yibing Wang,Fengyue Jin,Meng Zhang,Anni Chen*

Main category: cs.LG

TL;DR: 本文提出了一种基于物理信息深度算子网络（PI-DeepONet）的新型实时高速公路交通状态估计方法，通过扩展架构支持2D数据输入、引入非线性扩展层和注意力机制，显著提升了估计精度。


<details>
  <summary>Details</summary>
Motivation: 传统交通状态估计方法分为模型驱动、数据驱动和模型-数据双驱动三类，各有局限性。本文旨在结合模型与数据的优势，提出更精确的实时估计方法。

Method: 扩展了PI-DeepONet架构，支持2D数据输入（CNN计算）、引入非线性扩展层、注意力机制和MIMO机制，并设计专用神经网络自适应识别交通流模型参数。

Result: 在NGSIM短路段和中国大规模城市快速路的评估中，该方法在流量和平均速度估计上优于四种基线方法，表现出高精度。

Conclusion: 基于扩展PI-DeepONet架构的交通状态估计方法显著提升了实时估计的准确性，为交通管理提供了更可靠的工具。

Abstract: Traffic state estimation (TSE) falls methodologically into three categories:
model-driven, data-driven, and model-data dual-driven. Model-driven TSE relies
on macroscopic traffic flow models originated from hydrodynamics. Data-driven
TSE leverages historical sensing data and employs statistical models or machine
learning methods to infer traffic state. Model-data dual-driven traffic state
estimation attempts to harness the strengths of both aspects to achieve more
accurate TSE. From the perspective of mathematical operator theory, TSE can be
viewed as a type of operator that maps available measurements of inerested
traffic state into unmeasured traffic state variables in real time. For the
first time this paper proposes to study real-time freeway TSE in the idea of
physics-informed deep operator network (PI-DeepONet), which is an
operator-oriented architecture embedding traffic flow models based on deep
neural networks. The paper has developed an extended architecture from the
original PI-DeepONet. The extended architecture is featured with: (1) the
acceptance of 2-D data input so as to support CNN-based computations; (2) the
introduction of a nonlinear expansion layer, an attention mechanism, and a MIMO
mechanism; (3) dedicated neural network design for adaptive identification of
traffic flow model parameters. A traffic state estimator built on the basis of
this extended PI-DeepONet architecture was evaluated with respect to a short
freeway stretch of NGSIM and a large-scale urban expressway in China, along
with other four baseline TSE methods. The evaluation results demonstrated that
this novel TSE method outperformed the baseline methods with high-precision
estimation results of flow and mean speed.

</details>


### [153] [Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP](https://arxiv.org/abs/2508.08005)
*Xiang Li,Shanshan Wang,Chenglong Xiao*

Main category: cs.LG

TL;DR: 提出了一种基于学习的框架，结合传统机器学习和图神经网络，用于解决最大团问题（MCP）的算法选择问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，没有单一的最大团算法在所有实例中表现最佳，因此需要根据实例特征选择合适的算法。然而，目前缺乏针对MCP的算法选择研究。

Method: 构建了一个标记数据集，运行四种精确MCP算法，并提取图的结构和全局统计特征。评估了四种传统分类器（SVM、RF、DT、KNN），并开发了双通道模型GAT-MLP，结合图注意力网络（GAT）和多层感知机（MLP）。

Result: 随机森林（RF）表现稳定且优异，而GAT-MLP模型在所有指标上均表现出色，验证了双通道架构的有效性。

Conclusion: 双通道架构和图神经网络在组合算法选择中具有潜力，GAT-MLP模型为解决MCP算法选择问题提供了有效方案。

Abstract: Extensive experiments and prior studies show that no single maximum clique
algorithm consistently performs best across all instances, highlighting the
importance of selecting suitable algorithms based on instance features. Through
an extensive analysis of relevant studies, it is found that there is a lack of
research work concerning algorithm selection oriented toward the Maximum Clique
Problem (MCP). In this work, we propose a learning-based framework that
integrates both traditional machine learning and graph neural networks to
address this gap. We construct a labeled dataset by running four exact MCP
algorithms on a diverse collection of graph instances, accompanied by
structural and global statistical features extracted from each graph. We first
evaluate four conventional classifiers: Support Vector Machine (SVM), Random
Forest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple
dataset variants. Experimental results show that RF consistently shows strong
performance across metrics and dataset variants, making it a reliable baseline.
In addition, feature importance analysis indicates that connectivity and
topological structure are strong predictors of algorithm performance. Building
on these findings, we develop a dual-channel model named GAT-MLP, which
combines a Graph Attention Network (GAT) for local structural encoding with a
Multilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model
shows strong and consistent performance across all metrics. Our results
highlight the effectiveness of dual-channel architectures and the promise of
graph neural networks in combinatorial algorithm selection.

</details>


### [154] [Communication-Efficient Zero-Order and First-Order Federated Learning Methods over Wireless Networks](https://arxiv.org/abs/2508.08013)
*Mohamad Assaad,Zeinab Nehme,Merouane Debbah*

Main category: cs.LG

TL;DR: 论文提出了两种通信高效的联邦学习方法，通过减少通信开销并利用信道信息来提高效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）面临通信开销大的问题，尤其是在无线系统中。本文旨在通过减少通信量和利用信道信息来解决这一问题。

Method: 提出了两种方法：一是基于零阶优化的两点梯度估计器，二是基于一阶梯度计算策略。两种方法均利用信道信息，无需额外资源获取信道状态信息（CSI）。

Result: 通过理论分析，论文为两种方法提供了收敛保证和性能界限。

Conclusion: 所提方法显著减少了通信开销，同时保持了模型性能，适用于异步设备环境。

Abstract: Federated Learning (FL) is an emerging learning framework that enables edge
devices to collaboratively train ML models without sharing their local data. FL
faces, however, a significant challenge due to the high amount of information
that must be exchanged between the devices and the aggregator in the training
phase, which can exceed the limited capacity of wireless systems. In this
paper, two communication-efficient FL methods are considered where
communication overhead is reduced by communicating scalar values instead of
long vectors and by allowing high number of users to send information
simultaneously. The first approach employs a zero-order optimization technique
with two-point gradient estimator, while the second involves a first-order
gradient computation strategy. The novelty lies in leveraging channel
information in the learning algorithms, eliminating hence the need for
additional resources to acquire channel state information (CSI) and to remove
its impact, as well as in considering asynchronous devices. We provide a
rigorous analytical framework for the two methods, deriving convergence
guarantees and establishing appropriate performance bounds.

</details>


### [155] [BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models](https://arxiv.org/abs/2508.08040)
*Maozhen Zhang,Mengnan Zhao,Bo Wang*

Main category: cs.LG

TL;DR: BadPromptFL是一种针对多模态对比模型中基于提示的联邦学习的后门攻击方法，通过优化本地后门触发器和提示嵌入，在不修改模型参数的情况下实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 探索基于提示的联邦学习在安全方面的潜在漏洞，填补这一领域的研究空白。

Method: 通过被攻击的客户端联合优化本地后门触发器和提示嵌入，将中毒提示注入全局聚合过程。

Result: 攻击成功率超过90%，且在多种数据集和聚合协议下验证了其有效性、隐蔽性和通用性。

Conclusion: BadPromptFL揭示了基于提示的联邦学习在实际部署中的脆弱性，引发了对该技术安全性的关注。

Abstract: Prompt-based tuning has emerged as a lightweight alternative to full
fine-tuning in large vision-language models, enabling efficient adaptation via
learned contextual prompts. This paradigm has recently been extended to
federated learning settings (e.g., PromptFL), where clients collaboratively
train prompts under data privacy constraints. However, the security
implications of prompt-based aggregation in federated multimodal learning
remain largely unexplored, leaving a critical attack surface unaddressed. In
this paper, we introduce \textbf{BadPromptFL}, the first backdoor attack
targeting prompt-based federated learning in multimodal contrastive models. In
BadPromptFL, compromised clients jointly optimize local backdoor triggers and
prompt embeddings, injecting poisoned prompts into the global aggregation
process. These prompts are then propagated to benign clients, enabling
universal backdoor activation at inference without modifying model parameters.
Leveraging the contextual learning behavior of CLIP-style architectures,
BadPromptFL achieves high attack success rates (e.g., \(>90\%\)) with minimal
visibility and limited client participation. Extensive experiments across
multiple datasets and aggregation protocols validate the effectiveness,
stealth, and generalizability of our attack, raising critical concerns about
the robustness of prompt-based federated learning in real-world deployments.

</details>


### [156] [On Understanding of the Dynamics of Model Capacity in Continual Learning](https://arxiv.org/abs/2508.08052)
*Supriyo Chakraborty,Krishnan Raghavan*

Main category: cs.LG

TL;DR: 论文提出了一种称为CLEMC的模型，用于描述持续学习中稳定性与可塑性平衡的动态行为，并证明这种平衡点是非静态的。


<details>
  <summary>Details</summary>
Motivation: 解决持续学习中的稳定性与可塑性困境，研究神经网络在不同任务分布下的表现能力。

Method: 通过建立差分方程建模神经网络、任务数据和优化过程的交互，并利用CLEMC分析有效容量。

Result: 实验表明，无论网络架构或优化方法如何，当新任务分布与之前不同时，神经网络的表示能力会下降。

Conclusion: CLEMC揭示了持续学习中稳定性与可塑性平衡的动态特性，为未来研究提供了理论基础。

Abstract: The stability-plasticity dilemma, closely related to a neural network's (NN)
capacity-its ability to represent tasks-is a fundamental challenge in continual
learning (CL). Within this context, we introduce CL's effective model capacity
(CLEMC) that characterizes the dynamic behavior of the stability-plasticity
balance point. We develop a difference equation to model the evolution of the
interplay between the NN, task data, and optimization procedure. We then
leverage CLEMC to demonstrate that the effective capacity-and, by extension,
the stability-plasticity balance point is inherently non-stationary. We show
that regardless of the NN architecture or optimization method, a NN's ability
to represent new tasks diminishes when incoming task distributions differ from
previous ones. We conduct extensive experiments to support our theoretical
findings, spanning a range of architectures-from small feedforward network and
convolutional networks to medium-sized graph neural networks and
transformer-based large language models with millions of parameters.

</details>


### [157] [From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations](https://arxiv.org/abs/2508.08061)
*Sven Weinzierl,Sandra Zilker,Annina Liessmann,Martin Käppel,Weixin Wang,Martin Matzner*

Main category: cs.LG

TL;DR: 提出了一种基于迁移学习的预测流程监控技术，帮助资源不足的组织实现有效的决策支持。


<details>
  <summary>Details</summary>
Motivation: 现有预测流程监控技术需要大量事件数据或资源，限制了部分组织的应用。

Method: 采用迁移学习技术，将知识从一个业务流程迁移到相似业务流程，支持跨组织应用。

Result: 实验表明，该方法在跨组织环境中能有效实现预测流程监控。

Conclusion: 该技术为资源有限的组织提供了可行的预测流程监控解决方案。

Abstract: Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

</details>


### [158] [C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction](https://arxiv.org/abs/2508.08071)
*Yunqing Li,Zixiang Tang,Jiaying Zhuang,Zhenyu Yang,Farhad Ameri,Jianbang Zhang*

Main category: cs.LG

TL;DR: 论文提出了PMGraph基准和C-MAG架构，用于解决供应链中制造商与产品匹配的复杂性问题，通过多模态数据融合和图传播提升链接预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以捕捉制造商和产品之间的复杂能力、认证、地理约束和多模态数据，影响了供应链的效率和韧性。

Method: 提出PMGraph基准，包含多模态供应链图数据；设计C-MAG架构，分两阶段对齐和聚合多模态属性，并通过多尺度消息传递增强链接预测。

Result: C-MAG在噪声环境下仍能保持预测性能，显著提升了制造商与产品匹配的准确性。

Conclusion: PMGraph和C-MAG为供应链中的复杂匹配问题提供了有效解决方案，具有实际应用价值。

Abstract: Connecting an ever-expanding catalogue of products with suitable
manufacturers and suppliers is critical for resilient, efficient global supply
chains, yet traditional methods struggle to capture complex capabilities,
certifications, geographic constraints, and rich multimodal data of real-world
manufacturer profiles. To address these gaps, we introduce PMGraph, a public
benchmark of bipartite and heterogeneous multimodal supply-chain graphs linking
8,888 manufacturers, over 70k products, more than 110k manufacturer-product
edges, and over 29k product images. Building on this benchmark, we propose the
Cascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first
aligns and aggregates textual and visual attributes into intermediate group
embeddings, then propagates them through a manufacturer-product hetero-graph
via multiscale message passing to enhance link prediction accuracy. C-MAG also
provides practical guidelines for modality-aware fusion, preserving predictive
performance in noisy, real-world settings.

</details>


### [159] [ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring](https://arxiv.org/abs/2508.08073)
*Dimitris Tsaras,Xing Li,Lei Chen,Zhiyao Xie,Mingxuan Yuan*

Main category: cs.LG

TL;DR: 提出一种基于分类器的预剪枝方法，显著提升逻辑优化速度。


<details>
  <summary>Details</summary>
Motivation: 传统逻辑优化算子计算成本高，且大部分尝试失败，需减少无效操作。

Method: 利用分类器预判并剪枝不成功的切割，避免不必要的重合成操作。

Result: 在EPFL基准套件和10个大型工业设计上，平均提速3.9倍。

Conclusion: 分类器预剪枝方法有效降低计算成本，提升逻辑优化效率。

Abstract: In electronic design automation, logic optimization operators play a crucial
role in minimizing the gate count of logic circuits. However, their computation
demands are high. Operators such as refactor conventionally form iterative cuts
for each node, striving for a more compact representation - a task which often
fails 98% on average. Prior research has sought to mitigate computational cost
through parallelization. In contrast, our approach leverages a classifier to
prune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis
operations. Experiments on the refactor operator using the EPFL benchmark suite
and 10 large industrial designs demonstrate that this technique can speedup
logic optimization by 3.9x on average compared with the state-of-the-art ABC
implementation.

</details>


### [160] [Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles](https://arxiv.org/abs/2508.08080)
*Cas Oude Hoekstra,Floris den Hengst*

Main category: cs.LG

TL;DR: 本文介绍了符号分位数回归（SQR），一种利用符号回归（SR）预测条件分位数的方法，在透明性和性能上表现优异。


<details>
  <summary>Details</summary>
Motivation: 符号回归虽能生成可解释的预测模型，但现有方法仅能估计目标变量的平均值，无法全面描述变量间关系。SQR旨在填补这一空白，适用于高风险的领域。

Method: 提出符号分位数回归（SQR），通过符号回归预测条件分位数，并在广泛评估中验证其性能。

Result: SQR在透明模型中表现最佳，与黑盒基线模型性能相当，同时保持可解释性。案例研究展示了其在极端值和中心值预测中的应用。

Conclusion: SQR适用于预测条件分位数，并能帮助理解不同分位数下的特征影响，为高风险领域提供透明且准确的模型。

Abstract: Symbolic Regression (SR) is a well-established framework for generating
interpretable or white-box predictive models. Although SR has been successfully
applied to create interpretable estimates of the average of the outcome, it is
currently not well understood how it can be used to estimate the relationship
between variables at other points in the distribution of the target variable.
Such estimates of e.g. the median or an extreme value provide a fuller picture
of how predictive variables affect the outcome and are necessary in
high-stakes, safety-critical application domains. This study introduces
Symbolic Quantile Regression (SQR), an approach to predict conditional
quantiles with SR. In an extensive evaluation, we find that SQR outperforms
transparent models and performs comparably to a strong black-box baseline
without compromising transparency. We also show how SQR can be used to explain
differences in the target distribution by comparing models that predict extreme
and central outcomes in an airline fuel usage case study. We conclude that SQR
is suitable for predicting conditional quantiles and understanding interesting
feature influences at varying quantiles.

</details>


### [161] [Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation](https://arxiv.org/abs/2508.08087)
*Amir Ali Panahi,Daniel Luder,Billy Wu,Gregory Offer,Dirk Uwe Sauer,Weihan Li*

Main category: cs.LG

TL;DR: 论文比较了三种算子学习替代模型（DeepONets、FNOs和PE-FNO）在锂离子电池单粒子模型（SPM）中的应用，提出PE-FNO在速度和参数灵活性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为满足锂离子电池数字孪生对高物理保真度和亚毫秒级速度的需求，研究探索了高效的神经网络替代模型。

Method: 训练了三种模型（DeepONets、FNOs和PE-FNO）在不同电流负载和SOC范围内的模拟轨迹，并评估其性能。

Result: PE-FNO在速度和参数灵活性上表现最优，比传统方法快200倍，且在参数估计任务中误差较低。

Conclusion: PE-FNO为实时电池管理和大规模推理提供了高速度、高保真度的解决方案，优于传统神经网络替代模型。

Abstract: Reliable digital twins of lithium-ion batteries must achieve high physical
fidelity with sub-millisecond speed. In this work, we benchmark three
operator-learning surrogates for the Single Particle Model (SPM): Deep Operator
Networks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed
parameter-embedded Fourier Neural Operator (PE-FNO), which conditions each
spectral layer on particle radius and solid-phase diffusivity. Models are
trained on simulated trajectories spanning four current families (constant,
triangular, pulse-train, and Gaussian-random-field) and a full range of
State-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates
constant-current behaviour but struggles with more dynamic loads. The basic FNO
maintains mesh invariance and keeps concentration errors below 1 %, with
voltage mean-absolute errors under 1.7 mV across all load types. Introducing
parameter embedding marginally increases error, but enables generalisation to
varying radii and diffusivities. PE-FNO executes approximately 200 times faster
than a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse
tasks are explored in a parameter estimation task with Bayesian optimisation,
recovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute
percentage error, respectively, and 0.5918 percentage points higher error in
comparison with classical methods. These results pave the way for neural
operators to meet the accuracy, speed and parametric flexibility demands of
real-time battery management, design-of-experiments and large-scale inference.
PE-FNO outperforms conventional neural surrogates, offering a practical path
towards high-speed and high-fidelity electrochemical digital twins.

</details>


### [162] [Grid2Guide: A* Enabled Small Language Model for Indoor Navigation](https://arxiv.org/abs/2508.08100)
*Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: Grid2Guide结合A*算法和小型语言模型（SLM），生成清晰、易读的室内导航指令。


<details>
  <summary>Details</summary>
Motivation: 复杂环境中缺乏外部定位信号和专用基础设施时，可靠的室内导航仍具挑战性。

Method: 通过二进制占用矩阵和A*算法计算最优路径，再用SLM将路径转换为自然语言指令。

Result: 实验证明该方法能生成准确、及时的导航指导。

Conclusion: Grid2Guide是一种轻量级、无需基础设施的实时室内导航解决方案。

Abstract: Reliable indoor navigation remains a significant challenge in complex
environments, particularly where external positioning signals and dedicated
infrastructures are unavailable. This research presents Grid2Guide, a hybrid
navigation framework that combines the A* search algorithm with a Small
Language Model (SLM) to generate clear, human-readable route instructions. The
framework first conducts a binary occupancy matrix from a given indoor map.
Using this matrix, the A* algorithm computes the optimal path between origin
and destination, producing concise textual navigation steps. These steps are
then transformed into natural language instructions by the SLM, enhancing
interpretability for end users. Experimental evaluations across various indoor
scenarios demonstrate the method's effectiveness in producing accurate and
timely navigation guidance. The results validate the proposed approach as a
lightweight, infrastructure-free solution for real-time indoor navigation
support.

</details>


### [163] [Vision-Based Localization and LLM-based Navigation for Indoor Environments](https://arxiv.org/abs/2508.08120)
*Keyan Rahimi,Md. Wasiul Haque,Sagar Dasgupta,Mizanur Rahman*

Main category: cs.LG

TL;DR: 提出了一种结合视觉定位和大型语言模型（LLM）导航的室内导航方法，定位准确率达96%，导航指令准确率为75%。


<details>
  <summary>Details</summary>
Motivation: 解决室内环境中GPS信号不可靠和建筑复杂性导致的导航难题。

Method: 使用ResNet-50进行视觉定位，结合LLM生成导航指令。

Result: 定位准确率96%，导航指令准确率75%。

Conclusion: 展示了利用普通摄像头和公开平面图实现无基础设施室内导航的潜力。

Abstract: Indoor navigation remains a complex challenge due to the absence of reliable
GPS signals and the architectural intricacies of large enclosed environments.
This study presents an indoor localization and navigation approach that
integrates vision-based localization with large language model (LLM)-based
navigation. The localization system utilizes a ResNet-50 convolutional neural
network fine-tuned through a two-stage process to identify the user's position
using smartphone camera input. To complement localization, the navigation
module employs an LLM, guided by a carefully crafted system prompt, to
interpret preprocessed floor plan images and generate step-by-step directions.
Experimental evaluation was conducted in a realistic office corridor with
repetitive features and limited visibility to test localization robustness. The
model achieved high confidence and an accuracy of 96% across all tested
waypoints, even under constrained viewing conditions and short-duration
queries. Navigation tests using ChatGPT on real building floor maps yielded an
average instruction accuracy of 75%, with observed limitations in zero-shot
reasoning and inference time. This research demonstrates the potential for
scalable, infrastructure-free indoor navigation using off-the-shelf cameras and
publicly available floor plans, particularly in resource-constrained settings
like hospitals, airports, and educational institutions.

</details>


### [164] [MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing](https://arxiv.org/abs/2508.08122)
*Mingrong Lin,Ke Deng,Zhengyang Wu,Zetao Zheng,Jie Li*

Main category: cs.LG

TL;DR: 提出了一种基于时序变分自编码器的知识追踪模型memoryKT，通过三阶段过程模拟记忆动态，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖单一遗忘机制，忽视了个性化遗忘模式及其他记忆过程。

Method: 采用时序变分自编码器，分三阶段模拟记忆动态：学习知识记忆特征分布、重构练习反馈、嵌入个性化遗忘模块。

Result: 在四个公开数据集上显著优于现有基线方法。

Conclusion: memoryKT通过完整建模编码-存储-检索循环，增强了对个体差异的感知能力。

Abstract: Knowledge Tracing (KT) is committed to capturing students' knowledge mastery
from their historical interactions. Simulating students' memory states is a
promising approach to enhance both the performance and interpretability of
knowledge tracing models. Memory consists of three fundamental processes:
encoding, storage, and retrieval. Although forgetting primarily manifests
during the storage stage, most existing studies rely on a single,
undifferentiated forgetting mechanism, overlooking other memory processes as
well as personalized forgetting patterns. To address this, this paper proposes
memoryKT, a knowledge tracing model based on a novel temporal variational
autoencoder. The model simulates memory dynamics through a three-stage process:
(i) Learning the distribution of students' knowledge memory features, (ii)
Reconstructing their exercise feedback, while (iii) Embedding a personalized
forgetting module within the temporal workflow to dynamically modulate memory
storage strength. This jointly models the complete encoding-storage-retrieval
cycle, significantly enhancing the model's perception capability for individual
differences. Extensive experiments on four public datasets demonstrate that our
proposed approach significantly outperforms state-of-the-art baselines.

</details>


### [165] [NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection](https://arxiv.org/abs/2508.08124)
*Guanghao Jin,Yuan Liang,Yihan Ma,Jingpei Wu,Guoyang Liu*

Main category: cs.LG

TL;DR: NeuroDx-LM是一种新型大规模模型，用于检测基于EEG的神经系统疾病，通过选择性时频嵌入机制和渐进特征感知训练策略，在CHB-MIT和精神分裂症数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决EEG大规模模型在实际部署中面临的标记数据有限和临床场景性能不佳的问题。

Method: 提出选择性时频嵌入机制和渐进特征感知训练策略，分两阶段优化特征表示。

Result: 在CHB-MIT和精神分裂症数据集上实现了最先进的性能。

Conclusion: EEG大规模模型在临床应用中具有巨大潜力。

Abstract: Large-scale models pre-trained on Electroencephalography (EEG) have shown
promise in clinical applications such as neurological disorder detection.
However, the practical deployment of EEG-based large-scale models faces
critical challenges such as limited labeled EEG data and suboptimal performance
in clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel
large-scale model specifically designed for detecting EEG-based neurological
disorders. Our key contributions include (i) a Selective Temporal-Frequency
Embedding mechanism that adaptively captures complex temporal and spectral
patterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy
that refines feature representation in a two-stage process. In the first stage,
our model learns the fundamental discriminative features of EEG activities; in
the second stage, the model further extracts more specialized fine-grained
features for accurate diagnostic performance. We evaluated NeuroDx-LM on the
CHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in
EEG-based seizure and schizophrenia detection, respectively. These results
demonstrate the great potential of EEG-based large-scale models to advance
clinical applicability. Our code is available at
https://github.com/LetItBe12345/NeuroDx-LM.

</details>


### [166] [OFAL: An Oracle-Free Active Learning Framework](https://arxiv.org/abs/2508.08126)
*Hadi Khorsand,Vahid Pourahmadi*

Main category: cs.LG

TL;DR: OFAL是一种无需人工标注的主动学习方法，利用神经网络的不确定性生成信息丰富的样本，提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习依赖人工标注，成本高且复杂。面对大量未标注数据，研究旨在减少对人工标注的依赖。

Method: 1. 分离和量化不确定性，使用蒙特卡洛Dropout近似贝叶斯神经网络。2. 通过变分自编码器从置信样本生成新的不确定样本。3. 结合其他主动学习方法。

Result: OFAL能够生成信息丰富的样本，提升模型准确性。

Conclusion: OFAL提供了一种高效且低成本的主动学习方案，减少了对人工标注的依赖。

Abstract: In the active learning paradigm, using an oracle to label data has always
been a complex and expensive task, and with the emersion of large unlabeled
data pools, it would be highly beneficial If we could achieve better results
without relying on an oracle. This research introduces OFAL, an oracle-free
active learning scheme that utilizes neural network uncertainty. OFAL uses the
model's own uncertainty to transform highly confident unlabeled samples into
informative uncertain samples. First, we start with separating and quantifying
different parts of uncertainty and introduce Monte Carlo Dropouts as an
approximation of the Bayesian Neural Network model. Secondly, by adding a
variational autoencoder, we go on to generate new uncertain samples by stepping
toward the uncertain part of latent space starting from a confidence seed
sample. By generating these new informative samples, we can perform active
learning and enhance the model's accuracy. Lastly, we try to compare and
integrate our method with other widely used active learning sampling methods.

</details>


### [167] [FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks](https://arxiv.org/abs/2508.08151)
*Moses Openja,Paolo Arcaini,Foutse Khomh,Fuyuki Ishikawa*

Main category: cs.LG

TL;DR: FairFLRep是一种自动化的公平性感知故障定位和修复技术，用于识别和修正DNN分类器中可能导致偏见的神经元。


<details>
  <summary>Details</summary>
Motivation: DNN在高风险决策应用中可能放大数据偏见，导致不公平行为，需要有效方法识别和修正。

Method: 通过调整与敏感属性（如种族或性别）相关的神经元权重，分析输入输出关系以修正偏见。

Result: 在多个数据集上，FairFLRep在提高公平性的同时保持准确性，优于现有方法。

Conclusion: FairFLRep在故障定位和修复阶段均考虑公平性，效率更高，是解决DNN偏见问题的有效工具。

Abstract: Deep neural networks (DNNs) are being utilized in various aspects of our
daily lives, including high-stakes decision-making applications that impact
individuals. However, these systems reflect and amplify bias from the data used
during training and testing, potentially resulting in biased behavior and
inaccurate decisions. For instance, having different misclassification rates
between white and black sub-populations. However, effectively and efficiently
identifying and correcting biased behavior in DNNs is a challenge. This paper
introduces FairFLRep, an automated fairness-aware fault localization and repair
technique that identifies and corrects potentially bias-inducing neurons in DNN
classifiers. FairFLRep focuses on adjusting neuron weights associated with
sensitive attributes, such as race or gender, that contribute to unfair
decisions. By analyzing the input-output relationships within the network,
FairFLRep corrects neurons responsible for disparities in predictive quality
parity. We evaluate FairFLRep on four image classification datasets using two
DNN classifiers, and four tabular datasets with a DNN model. The results show
that FairFLRep consistently outperforms existing methods in improving fairness
while preserving accuracy. An ablation study confirms the importance of
considering fairness during both fault localization and repair stages. Our
findings also show that FairFLRep is more efficient than the baseline
approaches in repairing the network.

</details>


### [168] [Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets](https://arxiv.org/abs/2508.08159)
*Cem Ata Baykara,Saurav Raj Pandey,Ali Burak Ünal,Harlin Lee,Mete Akgün*

Main category: cs.LG

TL;DR: 论文研究了使用联邦学习（FL）在多临床站点中开发癫痫发作预测模型，提出了一种随机子集聚合策略以解决数据异质性问题，显著提高了模型的泛化性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 由于患者隐私法规和数据异质性（非独立同分布特性），开发跨多个临床站点的准确且可泛化的癫痫发作预测模型具有挑战性。联邦学习提供了一种隐私保护的协作训练框架，但标准聚合方法（如FedAvg）在异质环境下可能受主导数据集影响而产生偏差。

Method: 论文采用单通道EEG数据，覆盖四个不同公共数据集（Siena、CHB-MIT、Helsinki、NCH），代表不同患者群体和记录条件。提出隐私保护的全局归一化和随机子集聚合策略，确保每个客户端在每轮训练中使用固定大小的随机数据子集，实现公平聚合。

Result: 本地训练的模型无法跨站点泛化，标准加权FedAvg性能不均衡（如CHB-MIT准确率89.0%，Helsinki和NCH仅50.8%和50.6%）。随机子集聚合显著提升了弱势客户端的性能（Helsinki准确率提升至81.7%，NCH至68.7%），全局宏平均准确率达77.1%，池化准确率80.0%。

Conclusion: 研究表明，平衡的联邦学习方法在尊重数据隐私的同时，能够在异质多医院环境中构建有效且可泛化的癫痫发作预测系统。

Abstract: Developing accurate and generalizable epileptic seizure prediction models
from electroencephalography (EEG) data across multiple clinical sites is
hindered by patient privacy regulations and significant data heterogeneity
(non-IID characteristics). Federated Learning (FL) offers a privacy-preserving
framework for collaborative training, but standard aggregation methods like
Federated Averaging (FedAvg) can be biased by dominant datasets in
heterogeneous settings. This paper investigates FL for seizure prediction using
a single EEG channel across four diverse public datasets (Siena, CHB-MIT,
Helsinki, NCH), representing distinct patient populations (adult, pediatric,
neonate) and recording conditions. We implement privacy-preserving global
normalization and propose a Random Subset Aggregation strategy, where each
client trains on a fixed-size random subset of its data per round, ensuring
equal contribution during aggregation. Our results show that locally trained
models fail to generalize across sites, and standard weighted FedAvg yields
highly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on
Helsinki and 50.6% on NCH). In contrast, Random Subset Aggregation
significantly improves performance on under-represented clients (accuracy
increases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior
macro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites,
demonstrating a more robust and fair global model. This work highlights the
potential of balanced FL approaches for building effective and generalizable
seizure prediction systems in realistic, heterogeneous multi-hospital
environments while respecting data privacy.

</details>


### [169] [Neural Logic Networks for Interpretable Classification](https://arxiv.org/abs/2508.08172)
*Vincent Perreault,Katsumi Inoue,Richard Labib,Alain Hertz*

Main category: cs.LG

TL;DR: 论文提出了一种可解释的神经网络结构，通过逻辑操作（AND、OR、NOT）和偏差项改进传统神经网络，并提出了因子化的IF-THEN规则结构和学习算法。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络性能强大但缺乏可解释性，而逻辑神经网络虽可解释但功能有限。本文旨在通过扩展逻辑操作和改进结构，提升其性能和适用性。

Method: 通过引入NOT操作和偏差项扩展逻辑神经网络，提出因子化的IF-THEN规则结构，并开发新的学习算法。

Result: 方法在布尔网络发现任务中达到最先进水平，并在表格分类（如医疗领域）中学习到可解释的规则。

Conclusion: 该方法不仅提升了逻辑神经网络的性能，还保持了可解释性，特别适用于需要透明决策的领域。

Abstract: Traditional neural networks have an impressive classification performance,
but what they learn cannot be inspected, verified or extracted. Neural Logic
Networks on the other hand have an interpretable structure that enables them to
learn a logical mechanism relating the inputs and outputs with AND and OR
operations. We generalize these networks with NOT operations and biases that
take into account unobserved data and develop a rigorous logical and
probabilistic modeling in terms of concept combinations to motivate their use.
We also propose a novel factorized IF-THEN rule structure for the model as well
as a modified learning algorithm. Our method improves the state-of-the-art in
Boolean networks discovery and is able to learn relevant, interpretable rules
in tabular classification, notably on an example from the medical field where
interpretability has tangible value.

</details>


### [170] [Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning](https://arxiv.org/abs/2508.08221)
*Zihe Liu,Jiashun Liu,Yancheng He,Weixun Wang,Jiaheng Liu,Ling Pan,Xinyu Hu,Shaopan Xiong,Ju Huang,Jian Hu,Shengyi Huang,Siran Yang,Jiamang Wang,Wenbo Su,Bo Zheng*

Main category: cs.LG

TL;DR: 本文系统回顾了强化学习在LLM推理中的应用，通过统一框架分析了不同技术的内部机制、适用场景和核心原则，并提出了一种简化的组合方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在LLM推理领域发展迅速，但缺乏标准化指南和对其机制的深入理解，导致实验结果不一致，实践者难以选择合适的技术。

Method: 通过严格的复现和独立评估，在统一的开源框架下分析不同RL技术，包括数据集难度、模型大小和架构的细粒度实验。

Result: 研究发现，两种技术的简约组合可以显著提升性能，超越现有策略如GRPO和DAPO。

Conclusion: 本文提供了清晰的RL技术选择指南，并为LLM领域的实践者提供了可靠的路线图，同时揭示了简约组合方法的有效性。

Abstract: Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [171] [Federated Online Learning for Heterogeneous Multisource Streaming Data](https://arxiv.org/abs/2508.06652)
*Jingmao Li,Yuanxing Chen,Shuangge Ma,Kuangnan Fang*

Main category: stat.ML

TL;DR: 提出了一种联邦在线学习（FOL）方法，用于分布式多源流数据分析，解决了隐私和高维数据存储问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中数据是动态流式的，现有联邦学习方法多针对静态数据，无法满足需求。

Method: 采用个性化模型和“子组”假设，结合惩罚可再生估计和近端梯度下降进行训练。

Result: 理论证明了模型估计、变量选择和子组结构恢复的一致性，仿真和实际数据应用表现优异。

Conclusion: FOL方法在隐私保护和存储效率方面具有优势，适用于流式数据场景。

Abstract: Federated learning has emerged as an essential paradigm for distributed
multi-source data analysis under privacy concerns. Most existing federated
learning methods focus on the ``static" datasets. However, in many real-world
applications, data arrive continuously over time, forming streaming datasets.
This introduces additional challenges for data storage and algorithm design,
particularly under high-dimensional settings. In this paper, we propose a
federated online learning (FOL) method for distributed multi-source streaming
data analysis. To account for heterogeneity, a personalized model is
constructed for each data source, and a novel ``subgroup" assumption is
employed to capture potential similarities, thereby enhancing model
performance. We adopt the penalized renewable estimation method and the
efficient proximal gradient descent for model training. The proposed method
aligns with both federated and online learning frameworks: raw data are not
exchanged among sources, ensuring data privacy, and only summary statistics of
previous data batches are required for model updates, significantly reducing
storage demands. Theoretically, we establish the consistency properties for
model estimation, variable selection, and subgroup structure recovery,
demonstrating optimal statistical efficiency. Simulations illustrate the
effectiveness of the proposed method. Furthermore, when applied to the
financial lending data and the web log data, the proposed method also exhibits
advantageous prediction performance. Results of the analysis also provide some
practical insights.

</details>


### [172] [MOCA-HESP: Meta High-dimensional Bayesian Optimization for Combinatorial and Mixed Spaces via Hyper-ellipsoid Partitioning](https://arxiv.org/abs/2508.06847)
*Lam Ngo,Huong Ha,Jeffrey Chan,Hongyu Zhang*

Main category: stat.ML

TL;DR: 提出了一种名为MOCA-HESP的高维贝叶斯优化方法，用于组合和混合变量，通过超椭球空间划分技术和多臂老虎机选择最优编码器，显著提升了优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有高维贝叶斯优化方法主要针对连续域，而组合和混合域的优化仍具挑战性。

Method: 结合超椭球空间划分技术和不同分类编码器，利用多臂老虎机自适应选择最优编码器，并作为元算法整合其他优化器。

Result: 在合成和实际基准测试中，MOCA-HESP优于现有基线方法。

Conclusion: MOCA-HESP为高维组合和混合变量优化提供了有效解决方案，并展示了其优越性能。

Abstract: High-dimensional Bayesian Optimization (BO) has attracted significant
attention in recent research. However, existing methods have mainly focused on
optimizing in continuous domains, while combinatorial (ordinal and categorical)
and mixed domains still remain challenging. In this paper, we first propose
MOCA-HESP, a novel high-dimensional BO method for combinatorial and mixed
variables. The key idea is to leverage the hyper-ellipsoid space partitioning
(HESP) technique with different categorical encoders to work with
high-dimensional, combinatorial and mixed spaces, while adaptively selecting
the optimal encoders for HESP using a multi-armed bandit technique. Our method,
MOCA-HESP, is designed as a \textit{meta-algorithm} such that it can
incorporate other combinatorial and mixed BO optimizers to further enhance the
optimizers' performance. Finally, we develop three practical BO methods by
integrating MOCA-HESP with state-of-the-art BO optimizers for combinatorial and
mixed variables: standard BO, CASMOPOLITAN, and Bounce. Our experimental
results on various synthetic and real-world benchmarks show that our methods
outperform existing baselines. Our code implementation can be found at
https://github.com/LamNgo1/moca-hesp

</details>


### [173] [Statistical Inference for Autoencoder-based Anomaly Detection after Representation Learning-based Domain Adaptation](https://arxiv.org/abs/2508.07049)
*Tran Tuan Kiet,Nguyen Thang Loi,Vo Nguyen Le Duy*

Main category: stat.ML

TL;DR: STAND-DA是一个基于选择性推断（SI）的框架，用于在表示学习域适应（DA）后进行统计严格的异常检测（AD），通过计算有效的p值并控制假阳性率。


<details>
  <summary>Details</summary>
Motivation: 解决在目标域数据有限时，异常检测性能下降及域适应引入的统计不确定性问题。

Method: 提出STAND-DA框架，结合表示学习域适应和选择性推断，开发GPU加速的SI实现以提高计算效率。

Result: 在合成和真实数据集上的实验验证了STAND-DA的理论有效性和计算效率。

Conclusion: STAND-DA为异常检测提供了统计严格的解决方案，适用于大规模深度学习架构。

Abstract: Anomaly detection (AD) plays a vital role across a wide range of domains, but
its performance might deteriorate when applied to target domains with limited
data. Domain Adaptation (DA) offers a solution by transferring knowledge from a
related source domain with abundant data. However, this adaptation process can
introduce additional uncertainty, making it difficult to draw statistically
valid conclusions from AD results. In this paper, we propose STAND-DA -- a
novel framework for statistically rigorous Autoencoder-based AD after
Representation Learning-based DA. Built on the Selective Inference (SI)
framework, STAND-DA computes valid $p$-values for detected anomalies and
rigorously controls the false positive rate below a pre-specified level
$\alpha$ (e.g., 0.05). To address the computational challenges of applying SI
to deep learning models, we develop the GPU-accelerated SI implementation,
significantly enhancing both scalability and runtime performance. This
advancement makes SI practically feasible for modern, large-scale deep
architectures. Extensive experiments on synthetic and real-world datasets
validate the theoretical results and computational efficiency of the proposed
STAND-DA method.

</details>


### [174] [Membership Inference Attacks with False Discovery Rate Control](https://arxiv.org/abs/2508.07066)
*Chenxu Zhao,Wei Qian,Aobo Chen,Mengdi Huai*

Main category: stat.ML

TL;DR: 本文提出了一种新型的成员推理攻击方法，能够控制错误发现率（FDR），并展示其在不同场景下的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击方法缺乏对错误发现率的保证，且底层分布未知，估计概率相互依赖，亟需解决这些问题。

Method: 设计了一种新型成员推理攻击方法，可作为包装器与现有方法无缝集成，并提供FDR控制。

Result: 理论分析和广泛实验验证了该方法在多种设置下的优越性能。

Conclusion: 该方法不仅提供FDR保证，还能标记真实非成员数据的边际概率，具有实际应用价值。

Abstract: Recent studies have shown that deep learning models are vulnerable to
membership inference attacks (MIAs), which aim to infer whether a data record
was used to train a target model or not. To analyze and study these
vulnerabilities, various MIA methods have been proposed. Despite the
significance and popularity of MIAs, existing works on MIAs are limited in
providing guarantees on the false discovery rate (FDR), which refers to the
expected proportion of false discoveries among the identified positive
discoveries. However, it is very challenging to ensure the false discovery rate
guarantees, because the underlying distribution is usually unknown, and the
estimated non-member probabilities often exhibit interdependence. To tackle the
above challenges, in this paper, we design a novel membership inference attack
method, which can provide the guarantees on the false discovery rate.
Additionally, we show that our method can also provide the marginal probability
guarantee on labeling true non-member data as member data. Notably, our method
can work as a wrapper that can be seamlessly integrated with existing MIA
methods in a post-hoc manner, while also providing the FDR control. We perform
the theoretical analysis for our method. Extensive experiments in various
settings (e.g., the black-box setting and the lifelong learning setting) are
also conducted to verify the desirable performance of our method.

</details>


### [175] [Stochastic dynamics learning with state-space systems](https://arxiv.org/abs/2508.07876)
*Juan-Pablo Ortega,Florian Rossmannek*

Main category: stat.ML

TL;DR: 本文通过统一处理确定性及随机性设置中的衰减记忆和回声状态特性（ESP），推进了储层计算（RC）的理论基础。


<details>
  <summary>Details</summary>
Motivation: 研究旨在为RC模型的实证成功提供理论支持，尤其是在缺乏严格收缩条件的情况下，并扩展对非自主动力系统的理解。

Method: 通过分析状态空间系统，探讨衰减记忆和解决方案稳定性，并在随机情况下提出基于概率分布吸引子动力学的新视角。

Result: 结果表明，衰减记忆和稳定性在一般情况下成立，且随机回声状态的新理论框架为RC提供了更丰富的理论基础。

Conclusion: 研究为确定性及随机性时间序列数据的可靠生成建模奠定了基础，并深化了对RC模型中因果性、稳定性和记忆的理解。

Abstract: This work advances the theoretical foundations of reservoir computing (RC) by
providing a unified treatment of fading memory and the echo state property
(ESP) in both deterministic and stochastic settings. We investigate state-space
systems, a central model class in time series learning, and establish that
fading memory and solution stability hold generically -- even in the absence of
the ESP -- offering a robust explanation for the empirical success of RC models
without strict contractivity conditions. In the stochastic case, we critically
assess stochastic echo states, proposing a novel distributional perspective
rooted in attractor dynamics on the space of probability distributions, which
leads to a rich and coherent theory. Our results extend and generalize previous
work on non-autonomous dynamical systems, offering new insights into causality,
stability, and memory in RC models. This lays the groundwork for reliable
generative modeling of temporal data in both deterministic and stochastic
regimes.

</details>


### [176] [Meta Off-Policy Estimation](https://arxiv.org/abs/2508.07914)
*Olivier Jeunen*

Main category: stat.ML

TL;DR: 本文提出了一种基于元分析的OPE方法，通过结合多个估计器及其置信区间，生成更准确的单一估计。


<details>
  <summary>Details</summary>
Motivation: 现有OPE方法众多，但缺乏一种有效整合多个估计器的方法，以提升估计精度和统计效率。

Method: 采用相关固定效应元分析框架，考虑估计器间的依赖关系，生成最佳线性无偏估计（BLUE）和保守置信区间。

Result: 在模拟和真实数据上验证了方法的有效性，统计效率优于现有单个估计器。

Conclusion: 该方法为OPE提供了一种更高效的整合策略，适用于实际推荐系统评估。

Abstract: Off-policy estimation (OPE) methods enable unbiased offline evaluation of
recommender systems, directly estimating the online reward some target policy
would have obtained, from offline data and with statistical guarantees. The
theoretical elegance of the framework combined with practical successes have
led to a surge of interest, with many competing estimators now available to
practitioners and researchers. Among these, Doubly Robust methods provide a
prominent strategy to combine value- and policy-based estimators.
  In this work, we take an alternative perspective to combine a set of OPE
estimators and their associated confidence intervals into a single, more
accurate estimate. Our approach leverages a correlated fixed-effects
meta-analysis framework, explicitly accounting for dependencies among
estimators that arise due to shared data. This yields a best linear unbiased
estimate (BLUE) of the target policy's value, along with an appropriately
conservative confidence interval that reflects inter-estimator correlation. We
validate our method on both simulated and real-world data, demonstrating
improved statistical efficiency over existing individual estimators.

</details>


### [177] [Gaussian Approximation for Two-Timescale Linear Stochastic Approximation](https://arxiv.org/abs/2508.07928)
*Bogdan Butyrin,Artemy Rubtsov,Alexey Naumov,Vladimir Ulyanov,Sergey Samsonov*

Main category: stat.ML

TL;DR: 论文研究了线性双时间尺度随机逼近（TTSA）算法的正态逼近精度，分析了最后迭代和Polyak-Ruppert平均两种情况下凸距离的概率分布界限。


<details>
  <summary>Details</summary>
Motivation: 探讨线性TTSA算法在马尔可夫噪声或鞅差驱动下的正态逼近精度，揭示快慢时间尺度之间的非平凡交互作用。

Method: 通过凸距离分析概率分布，研究最后迭代和Polyak-Ruppert平均两种情形，并推导高阶矩误差界限。

Result: 发现最后迭代的正态逼近速率随时间尺度分离增加而提高，而Polyak-Ruppert平均下则降低。

Conclusion: 研究揭示了时间尺度分离对TTSA算法正态逼近的影响，并提供了高阶矩误差界限的独立结果。

Abstract: In this paper, we establish non-asymptotic bounds for accuracy of normal
approximation for linear two-timescale stochastic approximation (TTSA)
algorithms driven by martingale difference or Markov noise. Focusing on both
the last iterate and Polyak-Ruppert averaging regimes, we derive bounds for
normal approximation in terms of the convex distance between probability
distributions. Our analysis reveals a non-trivial interaction between the fast
and slow timescales: the normal approximation rate for the last iterate
improves as the timescale separation increases, while it decreases in the
Polyak-Ruppert averaged setting. We also provide the high-order moment bounds
for the error of linear TTSA algorithm, which may be of independent interest.

</details>


### [178] [Likelihood Ratio Tests by Kernel Gaussian Embedding](https://arxiv.org/abs/2508.07982)
*Leonardo V. Santoro,Victor M. Panaretos*

Main category: stat.ML

TL;DR: 提出了一种基于核的非参数两样本检验方法，结合核均值和核协方差嵌入，利用高斯嵌入的相对熵构造检验统计量，显著提升了高维和弱信号场景下的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高维和弱信号场景下性能不足，需要一种更有效的两样本检验方法。

Method: 结合核均值和核协方差嵌入，构造基于高斯嵌入相对熵的检验统计量，并通过正则化和排列校准实现有限样本下的检验。

Result: 在合成和真实数据上表现出优于现有方法的检测能力，特别是在高维和弱信号场景下。

Conclusion: 该方法统一并扩展了现有基于谱正则化MMD的方法，具有一致性和均匀功率保证。

Abstract: We propose a novel kernel-based nonparametric two-sample test, employing the
combined use of kernel mean and kernel covariance embedding. Our test builds on
recent results showing how such combined embeddings map distinct probability
measures to mutually singular Gaussian measures on the kernel's RKHS.
Leveraging this result, we construct a test statistic based on the relative
entropy between the Gaussian embeddings, i.e.\ the likelihood ratio. The
likelihood ratio is specifically tailored to detect equality versus singularity
of two Gaussians, and satisfies a ``$0/\infty$" law, in that it vanishes under
the null and diverges under the alternative. To implement the test in finite
samples, we introduce a regularised version, calibrated by way of permutation.
We prove consistency, establish uniform power guarantees under mild conditions,
and discuss how our framework unifies and extends prior approaches based on
spectrally regularized MMD. Empirical results on synthetic and real data
demonstrate remarkable gains in power compared to state-of-the-art methods,
particularly in high-dimensional and weak-signal regimes.

</details>
