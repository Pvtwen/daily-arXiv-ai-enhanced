<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 15]
- [cs.LG](#cs.LG) [Total: 50]
- [stat.ML](#stat.ML) [Total: 3]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Gesture Recognition from body-Worn RFID under Missing Data](https://arxiv.org/abs/2601.16301)
*Sahar Golipoor,Richard T. Brophy,Ying Liu,Reza Ghazalian,Stephan Sigg*

Main category: eess.SP

TL;DR: 该论文提出了一种基于被动反射标签的手势识别系统，通过数据插补和图神经网络方法，在21种手势识别上达到98.13%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决被动身体佩戴反射标签在手势识别中数据缺失的问题，提高手势识别的准确性和鲁棒性。

Method: 提出数据处理管道解决缺失数据问题，使用线性和指数插值外推、数据填补和邻近推断。将标签表示为时序图中的节点，基于RSS和相位值相关性构建边，训练基于图的自注意力卷积神经网络。

Result: 系统在21种手势识别上达到98.13%的准确率，优于现有方法。留一人出交叉验证准确率为89.28%。手臂标签对识别贡献更大，移除手臂标签使准确率下降超过10%，而移除手腕标签仅下降约2%。

Conclusion: 基于被动反射标签和图神经网络的手势识别系统效果显著，手臂标签比手腕标签在手势识别中更具表达力。

Abstract: We explore hand-gesture recognition through the use of passive body-worn reflective tags. A data processing pipeline is proposed to address the issue of missing data. Specifically, missing information is recovered through linear and exponential interpolation and extrapolation. Furthermore, imputation and proximity-based inference are employed. We represent tags as nodes in a temporal graph, with edges formed based on correlations between received signal strength (RSS) and phase values across successive timestamps, and we train a graph-based convolutional neural network that exploits graph-based self-attention. The system outperforms state-of-the-art methods with an accuracy of 98.13% for the recognition of 21 gestures. We achieve 89.28% accuracy under leave-one-person-out cross-validation. We further investigate the contribution of various body locations on the recognition accuracy. Removing tags from the arms reduces accuracy by more than 10%, while removing the wrist tag only reduces accuracy by around 2%. Therefore, tag placements on the arms are more expressive for gesture recognition than on the wrist.

</details>


### [2] [Angle of Arrival Estimation for Gesture Recognition from reflective body-worn tags](https://arxiv.org/abs/2601.16303)
*Sahar Golipoor,Reza Ghazalian,Ines Lobato Mesquita,Stephan Sigg*

Main category: eess.SP

TL;DR: 本文研究利用佩戴在身体上的被动反射标签进行手势识别，通过角度到达(AoA)跟踪技术显著提升识别性能


<details>
  <summary>Details</summary>
Motivation: 传统基于接收信号强度(RSS)和相位特征的手势识别方法在处理大量手势时存在局限性，因为不同手势的信号特征往往相似，难以区分

Method: 提出基于角度到达(AoA)跟踪的方法：1) 使用MUSIC算法验证AoA估计；2) 基于卡尔曼平滑的AoA跟踪方法；3) 将AoA特征集成到手势识别系统中

Result: AoA跟踪能有效区分RSS和相位特征无法区分的手势数据，集成AoA特征后手势识别系统性能提升高达15%

Conclusion: AoA跟踪作为区分特征能显著提升被动反射标签手势识别系统的性能，为解决传统信号特征相似性问题提供了有效解决方案

Abstract: We investigate hand gesture recognition by leveraging passive reflective tags worn on the body. Considering a large set of gestures, distinct patterns are difficult to be captured by learning algorithms using backscattered received signal strength (RSS) and phase signals. This is because these features often exhibit similarities across signals from different gestures. To address this limitation, we explore the estimation of Angle of Arrival (AoA) as a distinguishing feature, since AoA characteristically varies during body motion. To ensure reliable estimation in our system, which employs Smart Antenna Switching (SAS), we first validate AoA estimation using the Multiple SIgnal Classification (MUSIC) algorithm while the tags are fixed at specific angles. Building on this, we propose an AoA tracking method based on Kalman smoothing. Our analysis demonstrates that, while RSS and phase alone are insufficient for distinguishing certain gesture data, AoA tracking can effectively differentiate them. To evaluate the effectiveness of AoA tracking, we implement gesture recognition system benchmarks and show that incorporating AoA features significantly boosts their performance. Improvements of up to 15% confirm the value of AoA-based enhancement.

</details>


### [3] [TransfoREM: Transformer aided 3D Radio Environment Mapping](https://arxiv.org/abs/2601.16421)
*Gautham Reddy,Ismail Guvenc,Mihail L. Sichitiu,Arupjyoti Bhuyan,Bryton Petersen,Jason Abrahamson*

Main category: eess.SP

TL;DR: TransfoREM：一种基于Transformer的3D无线电环境地图生成方法，用于预测无人机在更高海拔的蜂窝网络覆盖


<details>
  <summary>Details</summary>
Motivation: 现有地面网络主要针对地面覆盖部署，无法为无人机提供可靠的蜂窝连接。无人机飞行高度只能获得有限的侧瓣覆盖，且飞行动态会进一步恶化连接质量。

Method: 结合确定性信道模型和真实世界数据，将无线电传播映射转化为序列预测任务，使用Transformer模型构建3D无线电环境地图。

Result: 相比传统的Kriging和其他机器学习技术，TransfoREM在真实世界数据上展现出更好的插值能力。

Conclusion: TransfoREM可集成到基站级别的蜂窝网络中，构建的REM可用于增强资源分配、干扰管理和空间频谱利用。

Abstract: Providing reliable cellular connectivity to Unmanned Aerial Vehicles (UAV) is a key challenge, as existing terrestrial networks are deployed mainly for ground-level coverage. The cellular network coverage may be available for a limited range from the antenna side lobes, with poor connectivity further exacerbated by UAV flight dynamics. In this work, we propose TransfoREM, a 3D Radio Environment Map (REM) generation method that combines deterministic channel models and real-world data to map terrestrial network coverage at higher altitudes. At the core of our solution is a transformer model that translates radio propagation mapping into a sequence prediction task to construct REMs. Our results demonstrate that TransfoREM offers improved interpolation capability on real-world data compared against conventional Kriging and other machine learning (ML) techniques. Furthermore, TransfoREM is designed for holistic integration into cellular networks at the base station (BS) level, where it can build REMs, which can then be leveraged for enhanced resource allocation, interference management, and spatial spectrum utilization.

</details>


### [4] [Auditory Attention Decoding without Spatial Information: A Diotic EEG Study](https://arxiv.org/abs/2601.16442)
*Masahiro Yoshino,Haruki Yokota,Junya Hara,Yuichi Tanaka,Hiroshi Higashi*

Main category: eess.SP

TL;DR: 提出了一种用于双耳同声环境的听觉注意解码框架，通过将脑电和语音信号映射到共享潜在空间，实现了72.70%的准确率，比现有基于方向的方法提高了22.58%。


<details>
  <summary>Details</summary>
Motivation: 现有听觉注意解码研究主要依赖双耳分听环境，利用空间方向线索来识别注意对象，但这限制了在真实世界场景（如鸡尾酒会）中的应用，因为现实中说话者可能重叠或动态移动。需要开发不依赖空间线索的方法。

Method: 提出双耳同声环境下的AAD框架：1) 使用wav2vec 2.0提取语音特征，用2层1D CNN编码；2) 使用BrainNetwork架构编码EEG信号；3) 将EEG和语音信号映射到共享潜在空间；4) 通过计算EEG和语音表示之间的余弦相似度来识别注意的语音流。

Result: 在双耳同声EEG数据集上实现了72.70%的准确率，比最先进的基于方向的AAD方法提高了22.58%。

Conclusion: 提出的方法成功解决了现有AAD技术对空间线索的依赖问题，为真实世界听觉场景中的注意解码提供了有效解决方案，对智能助听器和客观听力测试系统的发展具有重要意义。

Abstract: Auditory attention decoding (AAD) identifies the attended speech stream in multi-speaker environments by decoding brain signals such as electroencephalography (EEG). This technology is essential for realizing smart hearing aids that address the cocktail party problem and for facilitating objective audiometry systems. Existing AAD research mainly utilizes dichotic environments where different speech signals are presented to the left and right ears, enabling models to classify directional attention rather than speech content. However, this spatial reliance limits applicability to real-world scenarios, such as the "cocktail party" situation, where speakers overlap or move dynamically. To address this challenge, we propose an AAD framework for diotic environments where identical speech mixtures are presented to both ears, eliminating spatial cues. Our approach maps EEG and speech signals into a shared latent space using independent encoders. We extract speech features using wav2vec 2.0 and encode them with a 2-layer 1D convolutional neural network (CNN), while employing the BrainNetwork architecture for EEG encoding. The model identifies the attended speech by calculating the cosine similarity between EEG and speech representations. We evaluate our method on a diotic EEG dataset and achieve 72.70% accuracy, which is 22.58% higher than the state-of-the-art direction-based AAD method.

</details>


### [5] [Cell-Free MIMO with Rotatable Antennas: When Macro-Diversity Meets Antenna Directivity](https://arxiv.org/abs/2601.16543)
*Xingxiang Peng,Qingqing Wu,Ziyuan Zheng,Yanze Zhu,Wen Chen,Penghui Huang,Ying Gao,Honghao Wang*

Main category: eess.SP

TL;DR: 本文研究了配备可旋转天线的无蜂窝下行网络，通过联合优化发射波束成形和天线方向来最大化最差用户速率，提出了交替优化算法和高效的两阶段方案。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝网络虽然通过分布式接入点实现宏分集，但用户几何位置和遮挡造成的信道质量差异限制了性能。可旋转天线通过调整天线主瓣方向来增强不利链路，使网络能更好地利用宏分集，实现更高且更均匀的性能。

Method: 1. 提出基于交替优化的算法：迭代更新波束成形（通过二阶锥规划）和优化天线方向（使用逐次凸逼近）。2. 提出高效的两阶段方案：第一阶段通过流形感知的Frank-Wolfe更新最大化比例公平对数效用函数来设计天线方向；第二阶段使用基于SOCP的设计计算波束成形。

Result: 仿真结果表明，所提出的方向感知设计相比传统的仅波束成形基准方案，实现了显著更高的最差用户速率。此外，更大的天线方向性在正确方向下能增强公平性，但方向不当则会降低最差用户性能。

Conclusion: 可旋转天线是无蜂窝网络中增强宏分集利用的有效硬件自由度。联合优化天线方向和波束成形能显著提升最差用户性能，但天线方向性的增益高度依赖于正确的方向设计。

Abstract: Cell-free networks leverage distributed access points (APs) to achieve macro-diversity, yet their performance is often constrained by large disparities in channel quality arising from user geometry and blockages. To address this, rotatable antennas (RAs) add a lightweight hardware degree of freedom by steering the antenna boresight toward dominant propagation directions to strengthen unfavorable links, thereby enabling the network to better exploit macro-diversity for higher and more uniform performance. This paper investigates an RA-enabled cell-free downlink network and formulates a max-min rate problem that jointly optimizes transmit beamforming and antenna orientations. To tackle this challenging problem, we develop an alternating-optimization-based algorithm that iteratively updates the beamformers via a second-order cone program (SOCP) and optimizes the antenna orientations using successive convex approximation. To reduce complexity, we further propose an efficient two-stage scheme that first designs orientations by maximizing a proportional-fair log-utility using manifold-aware Frank-Wolfe updates, and then computes the beamformers using an SOCP-based design. Simulation results demonstrate that the proposed orientation-aware designs achieve a substantially higher worst-user rate than conventional beamforming-only benchmarks. Furthermore, larger antenna directivity enhances fairness with proper orientation but can degrade the worst-user performance otherwise.

</details>


### [6] [Spiking Neural Networks for Communication Systems: Encoding Schemes, Learning Algorithms, and Equalization~Techniques](https://arxiv.org/abs/2601.16550)
*Eike-Manuel Edelmann*

Main category: eess.SP

TL;DR: 该论文研究了基于脉冲神经网络(SNN)的接收机设计，用于非线性时不变频率选择性信道，提出了量化编码(QE)和策略梯度更新(PGU)方法，显著降低了功耗和复杂度，同时性能优于传统人工神经网络(ANN)接收机。


<details>
  <summary>Details</summary>
Motivation: 现代通信系统复杂度不断增加导致功耗急剧上升，而脉冲神经网络(SNN)受高效人脑启发，具有事件驱动和低功耗特性，有望解决传统人工神经网络(ANN)接收机的高能耗问题。

Method: 1) 使用带替代梯度的随时间反向传播(BPTT)作为更新规则；2) 提出量化编码(QE)作为神经编码方法；3) 在强度调制直接检测链路模型上比较两种接收机架构；4) 引入基于强化学习的策略梯度更新(PGU)算法，无需反向传播。

Result: 1) 结合判决反馈和QE的SNN接收机在均衡性能和脉冲计数方面表现优异；2) SNN接收机显著优于ANN接收机；3) PGU优化编码参数，大幅减少运行时间、复杂度和每次推理的脉冲数，同时保持性能。

Conclusion: 该论文成功开发了SNN接收机的设计和优化框架，解决了SNN优化的关键挑战，为未来设计和部署高能效SNN接收机奠定了基础，推动了低功耗实时信号处理技术的发展。

Abstract: Machine learning with artificial neural networks (ANNs), provides solutions for the growing complexity of modern communication systems. This complexity, however, increases power consumption, making the systems energy-intensive. Spiking neural networks (SNNs) represent a novel generation of neural networks inspired by the highly efficient human brain. By emulating its event-driven and energy-efficient mechanisms, SNNs enable low-power, real-time signal processing. They differ from ANNs in two key ways: they exhibit inherent temporal dynamics and process and transmit information as short binary signals called spikes. Despite their promise, major challenges remain, e.g., identifying optimal learning rules and effective neural encoding. This thesis investigates the design of SNN-based receivers for nonlinear time-invariant frequency-selective channels. Backpropagation through time with surrogate gradients is identified as a promising update rule and the novel quantization encoding (QE) as promising neural encoding. Given the model of the intensity modulation with direct detection link, we compare two different receiver architectures based on equalization performance and spike count. Using decision feedback and QE achieves both strong performance and low spike counts. Notably, SNN-based receivers significantly outperform ANN-based counterparts. We furthermore introduce policy gradient-based update (PGU), an reinforcement learning-based update algorithm that requires no backpropagation. Using PGU, encoding parameters are optimized, drastically reducing runtime, complexity, and spikes per inference while maintaining performance. This thesis contributes a successful design and optimization framework for SNN-based receivers. By addressing key challenges in SNN optimization, it facilitates future advances in the design and deployment of energy-efficient SNN receivers.

</details>


### [7] [Real-Time Evaluation of an Ultra-Tight GNSS/INS Integration Based on Adaptive PLL Bandwidth](https://arxiv.org/abs/2601.16577)
*Gaël Pages,Priot Benoît,Guillaume Beaugendre*

Main category: eess.SP

TL;DR: 提出一种基于矢量跟踪环架构的GNSS/INS超紧耦合系统，通过INS信息自适应调整PLL带宽，可在FPGA上实现且占用资源少


<details>
  <summary>Details</summary>
Motivation: 传统矢量跟踪方案需要并行运行标量环或存储预下载星历数据，增加了FPGA面积和存储资源消耗。本文旨在设计一种更高效、资源占用更少的GNSS/INS超紧耦合架构

Method: 采用矢量跟踪环架构，利用惯性导航系统信息自适应调整相位锁定环带宽。在环路内解码导航消息，无需并行标量环或预存星历数据。系统基于FPGA实现，包含1个捕获模块和16个跟踪通道（8个GPS L1/C和8个Galileo E1）

Result: 提出的架构在Zynq-Ultrascale FPGA上实现，相比传统矢量方案不增加FPGA面积，也不使用额外存储资源，同时保持GPS和Galileo信号的跟踪能力

Conclusion: 该GNSS/INS超紧耦合架构具有资源效率高、易于在现有GNSS接收机平台上实现的优势，为高性能导航系统提供了可行的FPGA实现方案

Abstract: In this contribution, we propose a GNSS/INS ultra-tight coupling in which the GNSS receiver architecture is based on a vector tracking loop type architecture. In the proposed approach, the phase lock loop bandwidth is adapted according to the inertial navigation system information. The latter has the advantage to be easily implementable on a System-on-Chip component such as an FPGA (Field-Programmable Gate Arrays), and can be implemented with minor modifications on an existing GNSS receiver platform. Moreover, compared to classical vector-based solutions, the proposed architecture decodes the navigation message in the loop, without the need to run scalar loops in parallel or having to store pre-downloaded ephemeris data. This architecture therefore does not increase the area occupied on the FPGA and does not use additional resources for storage. The proposed GNSS receiver architecture uses GPS L1/C and Galileo E1 signals and is composed of one acquisition module and 16 tracking channels (8 GPS and 8 Galileo) which are implemented within a FPGA (Zynq-Ultrascale).

</details>


### [8] [Learning Successive Interference Cancellation for Low-Complexity Soft-Output MIMO Detection](https://arxiv.org/abs/2601.16586)
*Benedikt Fesl,Fatih Capar*

Main category: eess.SP

TL;DR: recurSIC：一种轻量级学习型MIMO检测框架，结合SIC结构和学习处理，在低复杂度下提供可靠的软硬检测性能，适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 5G RedCap和IoT设备需要低复杂度的MIMO检测方案，同时要支持高阶调制并生成可靠的软信息用于信道解码。现有方案在计算复杂度和内存限制与性能需求之间存在矛盾。

Method: 提出recurSIC框架，结构上受连续干扰消除(SIC)启发，融入学习处理阶段。通过多路径假设跟踪生成软信息，只需单次前向传播，参数量极少，复杂度可调。

Result: 在现实无线场景中的数值结果表明，recurSIC在极低复杂度下实现了强大的硬检测和软检测性能，适合边缘受限的MIMO接收器。

Conclusion: recurSIC为边缘设备提供了一种平衡性能与复杂度的有效MIMO检测解决方案，特别适合5G RedCap和IoT应用场景。

Abstract: Low-complexity multiple-input multiple-output (MIMO) detection remains a key challenge in modern wireless systems, particularly for 5G reduced capability (RedCap) and internet-of-things (IoT) devices. In this context, the growing interest in deploying machine learning on edge devices must be balanced against stringent constraints on computational complexity and memory while supporting high-order modulation. Beyond accurate hard detection, reliable soft information is equally critical, as modern receivers rely on soft-input channel decoding, imposing additional requirements on the detector design. In this work, we propose recurSIC, a lightweight learning-based MIMO detection framework that is structurally inspired by successive interference cancellation (SIC) and incorporates learned processing stages. It generates reliable soft information via multi-path hypothesis tracking with a tunable complexity parameter while requiring only a single forward pass and a minimal parameter count. Numerical results in realistic wireless scenarios show that recurSIC achieves strong hard- and soft-detection performance at very low complexity, making it well suited for edge-constrained MIMO receivers.

</details>


### [9] [Assessment of Errors of Fundamental Frequency Estimation Methods in the Presence of Voltage Fluctuations and Distortions](https://arxiv.org/abs/2601.16606)
*Antonio Bracale,Pasquale De Falco,Piotr Kuwałek,Grzegorz Wiczyński*

Main category: eess.SP

TL;DR: 该论文通过数值模拟研究评估了现代电网条件下各种基频估计方法的误差，包括IEC 61000-4-30标准方法，特别关注电压波动和畸变同时存在的情况。


<details>
  <summary>Details</summary>
Motivation: 基频是定义电能质量的关键参数之一。在现代电网条件下准确确定该参数至关重要，特别是对于诊断目的需要在短时间窗口内高效估计基频的需求。

Method: 采用数值模拟研究方法，创建模拟现代电网状态的测试信号（包括电压波动和畸变同时发生的情况），评估包括IEC 61000-4-30标准方法在内的各种基频估计方法的误差。

Result: 通过研究得出了不同基频估计方法在模拟现代电网条件下的误差评估结果，为方法选择提供了依据。

Conclusion: 基于研究结果提出了结论，为现代电网条件下基频估计方法的选择和应用提供了指导。

Abstract: The fundamental frequency is one of the parameters that define power quality. Correctly determining this parameter under the conditions that prevail in modern power grids is crucial. Diagnostic purposes often require an efficient estimation of this parameter within short time windows. Therefore, this article presents the results of numerical simulation studies that allow the assessment of errors in various fundamental frequency estimation methods, including the standard IEC 61000-4-30 method, when the analyzed signal has a form similar to that found in modern power grids. For the purposes of this study, a test signal was adopted recreating the states of the power grid, including the simultaneous occurrence of voltage fluctuations and distortions. Conclusions are presented based on conducted research.

</details>


### [10] [Low-Power On-Device Gesture Recognition with Einsum Networks](https://arxiv.org/abs/2601.16662)
*Sahar Golipoor,Lingyun Yao,Martin Andraud,Stephan Sigg*

Main category: eess.SP

TL;DR: 提出基于Einsum网络的分布式资源受限设备手势识别系统，在低功耗RFID场景中优于基准模型


<details>
  <summary>Details</summary>
Motivation: 为分布式资源受限设备网络设计高效的手势识别系统，需要满足推理可处理性、可解释性和能效要求

Method: 使用Einsum网络构建概率电路，每个设备包含RSS/相位处理或AoA估计单元、特征提取模块和专用Einsum硬件，最后通过决策聚合模块融合所有设备输出

Result: 实验结果表明该方法在低功耗、可穿戴、被动RFID手势识别场景中优于基准模型

Conclusion: 基于Einsum网络的分布式手势识别系统在资源受限设备网络中具有优越性能，适用于低功耗RFID手势识别应用

Abstract: We design a gesture-recognition pipeline for networks of distributed, resource constrained devices utilising Einsum Networks. Einsum Networks are probabilistic circuits that feature a tractable inference, explainability, and energy efficiency. The system is validated in a scenario of low-power, body-worn, passive Radio Frequency Identification-based gesture recognition. Each constrained device includes task-specific processing units responsible for Received Signal Strength (RSS) and phase processing or Angle of Arrival (AoA) estimation, along with feature extraction, as well as dedicated Einsum hardware that processes the extracted features. The output of all constrained devices is then fused in a decision aggregation module to predict gestures. Experimental results demonstrate that the method outperforms the benchmark models.

</details>


### [11] [OFDM-Based ISAC Imaging of Extended Targets via Inverse Virtual Aperture Processing](https://arxiv.org/abs/2601.16664)
*Michael Negosanti,Lorenzo Pucci,Andrea Giorgetti*

Main category: eess.SP

TL;DR: 该研究探讨了利用逆虚拟孔径（IVA）进行车载场景中移动扩展目标成像的集成感知与通信（ISAC）系统性能，使用MIMO-OFDM波形和运动补偿技术形成IVA距离-多普勒图像。


<details>
  <summary>Details</summary>
Motivation: 研究动机是在下一代无线网络中设计有效的感知策略，特别是在车载场景中，需要同时满足通信和感知需求，探索ISAC系统在移动扩展目标成像方面的性能。

Method: 采用MIMO-OFDM波形，基站作为单站传感器，通过运动补偿技术处理目标反射的回波，形成IVA距离-多普勒（跨距离）图像。使用5G NR波形（中高频段），目标模型基于3GPP Release 19定义，将车辆建模为空间分布的散射点集合。

Result: 通过图像对比度（IC）和目标质心距离估计的均方根误差（RMSE）评估性能。通过改变用于IVA成像的子载波分配，研究了感知精度与通信效率之间的权衡关系。

Conclusion: 研究结果为下一代无线网络中设计有效的感知策略提供了见解，展示了ISAC系统在车载场景中移动目标成像的可行性，并揭示了感知精度与通信效率之间的重要权衡关系。

Abstract: This work investigates the performance of an integrated sensing and communication (ISAC) system exploiting inverse virtual aperture (IVA) for imaging moving extended targets in vehicular scenarios. A base station (BS) operates as a monostatic sensor using MIMO-OFDM waveforms. Echoes reflected by the target are processed through motion-compensation techniques to form an IVA range-Doppler (cross-range) image. A case study considers a 5G NR waveform in the upper mid-band, with the target model defined in 3GPP Release 19, representing a vehicle as a set of spatially distributed scatterers. Performance is evaluated in terms of image contrast (IC) and the root mean squared error (RMSE) of the estimated target-centroid range. Finally, the trade-off between sensing accuracy and communication efficiency is examined by varying the subcarrier allocation for IVA imaging. The results provide insights for designing effective sensing strategies in next-generation radio networks.

</details>


### [12] [Precise Low-Current Measurement Techniques for IoT Devices: A Case Study on MoleNet](https://arxiv.org/abs/2601.16727)
*Julian Block,Andreas Könsgen,Jens Dede,Anna Förster*

Main category: eess.SP

TL;DR: 比较专用源测量单元(SMU)在测量物联网设备微小电流方面的性能，并以MoleNet传感器板为例进行应用演示


<details>
  <summary>Details</summary>
Motivation: 物联网设备通常需要长时间电池供电，功耗是关键因素。传统万用表和示波器不适合测量睡眠模式下的微小电流，需要更精确的测量工具

Method: 比较专用的源测量单元(SMU)，这些设备能够以高精度测量微小电流，并以MoleNet物联网传感器板作为应用示例进行电流测量演示

Result: 展示了专用SMU在测量物联网设备微小电流方面的适用性和优势，特别是对于设备睡眠模式下的低功耗测量

Conclusion: 专用源测量单元是测量物联网设备微小电流的有效工具，特别适合评估设备在睡眠模式下的功耗表现

Abstract: Power consumption is a crucial aspect of IoT devices which often have to run on a battery for an extended period of time. Therefore, supply current measurements are crucial before deploying a device in the field. Multimeters and oscilloscopes are not well suited when it comes to measuring very small currents which occur e.g. when an IoT device is in sleep mode. In this report, we compare dedicated source measurement units (SMUs) which allow to measure very small currents with high precision. As an application example, we demonstrate current measurements on our MoleNet IoT sensor board.

</details>


### [13] [A Dynamic Parametric Simulator for Fetal Heart Sounds](https://arxiv.org/abs/2601.16792)
*Yingtong Zhou,Yiang Zhou,Zhengxian Qu,Kang Liu,Ting Tan*

Main category: eess.SP

TL;DR: 开发了一个可重复的动态参数模拟器，用于生成腹部胎儿心音图信号，支持fPCG处理方法的快速、可重复评估


<details>
  <summary>Details</summary>
Motivation: 胎儿心音图研究面临腹部记录数量有限、母体干扰严重、信号衰减明显等问题，导致难以进行可重复的基准测试

Method: 结合周期级胎儿S1/S2事件合成与卷积传输模块，加入可配置的干扰和背景噪声，从真实腹部记录中周期性地校准模型参数以捕捉心跳间变异性

Result: 生成的信号在包络时间结构和频域特性方面与真实记录验证一致，模拟器已作为开源软件发布

Conclusion: 该模拟器支持在受控采集条件下对fPCG处理方法进行快速、可重复的评估，解决了现有研究中的可重复性挑战

Abstract: Research on fetal phonocardiogram (fPCG) is challenged by the limited number of abdominal recordings, substantial maternal interference, and marked transmissioninduced signal attenuation that complicate reproducible benchmarking. We present a reproducible dynamic parametric simulator that generates long abdominal fPCG sequences by combining cycle-level fetal S1/S2 event synthesis with a convolutional transmission module and configurable interference and background noise. Model parameters are calibrated cyclewise from real abdominal recordings to capture beat-to-beat variability and to define data-driven admissible ranges for controllable synthesis. The generated signals are validated against real recordings in terms of envelope-based temporal structure and frequency-domain characteristics. The simulator is released as open software to support rapid, reproducible evaluation of fPCG processing methods under controlled acquisition conditions.

</details>


### [14] [Hierarchical Distribution Matcher Design for Probabilistic Constellation Shaping Based on a Novel Semi-Analytical Optimization Approach](https://arxiv.org/abs/2601.16847)
*Pantea Nadimi Goki,Luca Potì*

Main category: eess.SP

TL;DR: 提出了一种实用的分层分布匹配器设计方法，用于概率整形星座系统，通过半解析优化框架联合优化速率和能量损失，在200Gbps速率下实现2.8%的整形增益提升。


<details>
  <summary>Details</summary>
Motivation: 概率整形技术能提高通信系统的频谱效率，但现有的分层分布匹配器设计缺乏系统化的优化方法，难以在实际硬件约束下实现最佳性能。

Method: 提出分层分布匹配器的设计流程，包括：1）分析估计能量损失、速率损失和内存需求的下界；2）采用半解析优化框架联合优化速率和能量损失；3）基于下界能量损失选择分层数、内存大小和块长度；4）在AWGN信道上评估归一化广义互信息性能。

Result: 提出的设计方法在16QAM概率整形系统中验证了模型准确性，在200Gbps净数据速率、25%前向纠错开销下，相比已有方案实现了2.8%的整形增益提升。

Conclusion: 该研究提供了一种实用的分层分布匹配器设计工具，能够在硬件实现约束下优化信道容量，为ASIC和FPGA等实际硬件实现提供了有效的设计指导。

Abstract: A novel design procedure for practical hierarchical distribution matchers (HiDMs) in probabilistically shaped constellation systems is presented. The proposed approach enables the determination of optimal parameters for any target distribution matcher rate. Specifically, lower bounds on energy loss, rate loss, and memory requirements are analytically estimated for HiDM architectures approximating the Maxwell Boltzmann (MB) distribution. A semi analytical optimization framework is employed to jointly optimize rate and energy loss, allowing the selection of the number of hierarchical layers, memory size, and block length required to optimize channel capacity. The accuracy of the proposed model is validated through probabilistic amplitude shaping of 16QAM (PAS 16QAM), showing good agreement between analytical predictions and simulated results. The proposed analytical tool facilitates the design of HiDM structures that are compatible with practical hardware and implementation constraints, such as those imposed by state-of-the-art application-specific integrated circuits (ASICs) and field-programmable gate arrays (FPGAs). Furthermore, the performance of the optimized HiDM structure, incorporating layer selection based on lower-bound energy loss, is evaluated over the AWGN channel in terms of normalized generalized mutual information (NGMI) as a function of the optical signal-to-noise ratio (OSNR). At a net data rate of 200 Gbps with 25% forward error correction (FEC) overhead, the proposed scheme achieves a shaping gain improvement of 2.8% compared to previously reported solutions.

</details>


### [15] [IRS Compensation of Hyper-Rayleigh Fading: How Many Elements Are Needed?](https://arxiv.org/abs/2601.16915)
*Aleksey S. Gvozdarev*

Main category: eess.SP

TL;DR: 研究确定了在超瑞利衰落条件下，智能反射表面所需的最小元素数量，以补偿多径衰落信道中的严重衰落。


<details>
  <summary>Details</summary>
Motivation: 在严重衰落条件下，需要确定智能反射表面所需的最小元素数量来补偿信道质量，量化超瑞利衰落程度对于优化IRS设计至关重要。

Method: 使用逆幂洛马克斯分布作为信道模型，推导了单IRS元素信道的闭式统计特性，并对总IRS辅助信道的信道系数和瞬时信噪比统计提供了紧密近似。

Result: 当两个单链路都处于完全超瑞利衰落时，需要至少6个IRS元素才能使总链路脱离完全超瑞利衰落，需要14个元素才能使总链路进入无超瑞利衰落状态。

Conclusion: 该研究为IRS系统设计提供了关键指导，确定了在不同衰落严重程度下所需的最小IRS元素数量，有助于优化系统性能和成本效益。

Abstract: The letter introduces and studies the problem of defining the minimum number of Intelligent Reflecting Surface (IRS) elements needed to compensate for heavy fading conditions in multipath fading channels. The fading severity is quantified in terms of Hyper-Rayleigh Regimes (HRRs) (i.e., full-HRR (worst-case conditions), strong-, weak-, and no-HRR), and the channel model used (Inverse Power Lomax (IPL)) was chosen since it can account for all HRRs. The research presents the derived closed-form channel coefficient envelope statistics for the single IRS-element channel with IPL statistics in both subchannels and total IRS-assisted channel, as well as tight approximations for the channel coefficient and instantaneous signal-to-noise ratio (SNR) statistics for the latter. The derived expressions helped estimate channel parameters corresponding to the specific HRRs of the total channel and demonstrate that while both single links (i.e., ''source-IRS'' and ''IRS-destination'') are in full-HRR, the minimum number of IRS elements needed to bring the total IRS-assisted link (''source-IRS-destination'') out of full-HRR is no less than $6$ (for the whole range on the IPL scale parameter corresponding full-HRR). Furthermore, the minimum number of IRS elements required to bring the total IRS-assisted link into no-HRR is $14$ (under the same conditions).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [16] [Multigrade Neural Network Approximation](https://arxiv.org/abs/2601.16884)
*Shijun Zhang,Zuowei Shen,Yuesheng Xu*

Main category: cs.LG

TL;DR: MGDL提出了一种通过逐级训练深度网络的方法，每级学习前一级的残差，实现了可解释的层次化误差细化过程，并证明了该方法能保证逼近误差严格递减并收敛到零。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练面临高度非凸和病态优化问题，而浅层网络（特别是单隐层ReLU模型）存在凸重构和全局保证。MGDL旨在利用这一洞察，通过逐级训练来改善深度网络的稳定性和可扩展性。

Method: MGDL采用逐级训练策略：冻结已学习的层级，每个新的残差块仅训练以减少剩余逼近误差。该方法基于算子理论框架，构建了固定宽度的多级ReLU方案。

Result: 理论证明：对于任意连续目标函数，存在固定宽度的多级ReLU方案，其残差在各级间严格递减并一致收敛到零。这是首个证明逐级训练能在深度网络中实现可证明逼近误差消失的理论保证。

Conclusion: MGDL提供了一种理论上有保证的深度网络训练框架，通过层次化误差细化过程实现了稳定、可解释的深度学习，数值实验进一步验证了理论结果。

Abstract: We study multigrade deep learning (MGDL) as a principled framework for structured error refinement in deep neural networks. While the approximation power of neural networks is now relatively well understood, training very deep architectures remains challenging due to highly non-convex and often ill-conditioned optimization landscapes. In contrast, for relatively shallow networks, most notably one-hidden-layer $\texttt{ReLU}$ models, training admits convex reformulations with global guarantees, motivating learning paradigms that improve stability while scaling to depth. MGDL builds upon this insight by training deep networks grade by grade: previously learned grades are frozen, and each new residual block is trained solely to reduce the remaining approximation error, yielding an interpretable and stable hierarchical refinement process. We develop an operator-theoretic foundation for MGDL and prove that, for any continuous target function, there exists a fixed-width multigrade $\texttt{ReLU}$ scheme whose residuals decrease strictly across grades and converge uniformly to zero. To the best of our knowledge, this work provides the first rigorous theoretical guarantee that grade-wise training yields provable vanishing approximation error in deep networks. Numerical experiments further illustrate the theoretical results.

</details>


### [17] [Ordering-based Causal Discovery via Generalized Score Matching](https://arxiv.org/abs/2601.16249)
*Vy Vo,He Zhao,Trung Le,Edwin V. Bonilla,Dinh Phung*

Main category: cs.LG

TL;DR: 该论文提出了一种基于离散评分函数的叶节点判别准则，扩展了因果发现的评分匹配框架，能够从纯观测离散数据中准确推断因果顺序。


<details>
  <summary>Details</summary>
Motivation: 从纯观测数据中学习有向无环图（DAG）结构是跨科学领域的长期挑战。现有基于评分匹配的因果发现方法主要针对连续数据，缺乏对离散数据的有效处理。

Method: 扩展评分匹配框架至离散数据，提出基于离散评分函数的叶节点判别准则，通过叶节点检测识别拓扑顺序，然后进行边剪枝实现图恢复。

Result: 通过模拟和真实世界实验证明，该方法能够从观测离散数据中准确推断真实因果顺序，且识别的顺序能显著提升现有因果发现基线的准确性。

Conclusion: 提出的基于离散评分函数的叶节点判别准则有效扩展了因果发现的评分匹配框架，为从离散观测数据中学习DAG结构提供了新方法。

Abstract: Learning DAG structures from purely observational data remains a long-standing challenge across scientific domains. An emerging line of research leverages the score of the data distribution to initially identify a topological order of the underlying DAG via leaf node detection and subsequently performs edge pruning for graph recovery. This paper extends the score matching framework for causal discovery, which is originally designated for continuous data, and introduces a novel leaf discriminant criterion based on the discrete score function. Through simulated and real-world experiments, we demonstrate that our theory enables accurate inference of true causal orders from observed discrete data and the identified ordering can significantly boost the accuracy of existing causal discovery baselines on nearly all of the settings.

</details>


### [18] [FedSGM: A Unified Framework for Constraint Aware, Bidirectionally Compressed, Multi-Step Federated Optimization](https://arxiv.org/abs/2601.16897)
*Antesh Upadhyay,Sang Bin Moon,Abolfazl Hashemi*

Main category: cs.LG

TL;DR: FedSGM是一个统一的联邦约束优化框架，解决了联邦学习中的四个主要挑战：函数约束、通信瓶颈、本地更新和部分客户端参与。该框架基于切换梯度方法，提供无投影、仅原变量的更新，避免了昂贵的对偶变量调整或内部求解器。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临四个主要挑战：1）函数约束（如公平性、隐私要求），2）通信瓶颈（带宽限制），3）本地更新（减少通信轮次），4）部分客户端参与（实际部署中）。现有方法未能统一解决这些挑战，特别是缺乏对约束联邦学习的理论基础。

Method: 基于切换梯度方法，FedSGM提供投影自由、仅原变量的更新。采用双向误差反馈处理通信压缩，校正压缩引入的偏差，并明确理解压缩噪声与多步本地更新之间的相互作用。还引入了软切换版本以稳定在可行性边界附近的更新。

Result: 理论分析表明平均迭代达到规范的$\mathcal{O}(1/\sqrt{T})$收敛率，并提供了高概率界限，将优化进展与部分参与引起的采样噪声解耦。实验验证了FedSGM在Neyman-Pearson分类和约束马尔可夫决策过程任务上的有效性。

Conclusion: FedSGM是第一个统一函数约束、压缩、多步本地更新和部分客户端参与的框架，为约束联邦学习建立了理论基础。该框架在实际应用中具有重要价值，特别是在通信受限和部分参与的现实场景中。

Abstract: We introduce FedSGM, a unified framework for federated constrained optimization that addresses four major challenges in federated learning (FL): functional constraints, communication bottlenecks, local updates, and partial client participation. Building on the switching gradient method, FedSGM provides projection-free, primal-only updates, avoiding expensive dual-variable tuning or inner solvers. To handle communication limits, FedSGM incorporates bi-directional error feedback, correcting the bias introduced by compression while explicitly understanding the interaction between compression noise and multi-step local updates. We derive convergence guarantees showing that the averaged iterate achieves the canonical $\boldsymbol{\mathcal{O}}(1/\sqrt{T})$ rate, with additional high-probability bounds that decouple optimization progress from sampling noise due to partial participation. Additionally, we introduce a soft switching version of FedSGM to stabilize updates near the feasibility boundary. To our knowledge, FedSGM is the first framework to unify functional constraints, compression, multiple local updates, and partial client participation, establishing a theoretically grounded foundation for constrained federated learning. Finally, we validate the theoretical guarantees of FedSGM via experimentation on Neyman-Pearson classification and constrained Markov decision process (CMDP) tasks.

</details>


### [19] [Student Mental Health Screening via Fitbit Data Collected During the COVID-19 Pandemic](https://arxiv.org/abs/2601.16324)
*Rebecca Lopez,Avantika Shrestha,ML Tlachac,Kevin Hickey,Xingtong Guo,Shichao Liu,Elke Rundensteiner*

Main category: cs.LG

TL;DR: 使用Fitbit可穿戴设备数据通过机器学习模型筛查大学生焦虑、抑郁和压力，心率与睡眠数据表现最佳，F1分数最高达0.79


<details>
  <summary>Details</summary>
Motivation: 大学生面临高压力导致焦虑抑郁，现有研究在心理评估工具、生理模态和时间序列参数方面有限。可穿戴设备提供无干扰传感器数据，可用于精神疾病的早期检测。

Method: 收集疫情期间学生的StudentMEH Fitbit数据集，使用不同Fitbit模态（心率、睡眠等）构建预测性机器学习模型，评估其筛查抑郁、焦虑和压力的能力。

Result: 生理模态如心率和睡眠在精神疾病筛查中表现出潜力：焦虑筛查F1分数最高0.79，压力筛查心率模态达0.77，抑郁筛查睡眠模态达0.78。

Conclusion: 可穿戴设备具有支持持续心理健康监测的潜力，识别最佳数据聚合水平和适当模态对于筛查不同精神疾病至关重要。

Abstract: College students experience many stressors, resulting in high levels of anxiety and depression. Wearable technology provides unobtrusive sensor data that can be used for the early detection of mental illness. However, current research is limited concerning the variety of psychological instruments administered, physiological modalities, and time series parameters. In this research, we collect the Student Mental and Environmental Health (StudentMEH) Fitbit dataset from students at our institution during the pandemic. We provide a comprehensive assessment of the ability of predictive machine learning models to screen for depression, anxiety, and stress using different Fitbit modalities. Our findings indicate potential in physiological modalities such as heart rate and sleep to screen for mental illness with the F1 scores as high as 0.79 for anxiety, the former modality reaching 0.77 for stress screening, and the latter modality achieving 0.78 for depression. This research highlights the potential of wearable devices to support continuous mental health monitoring, the importance of identifying best data aggregation levels and appropriate modalities for screening for different mental ailments.

</details>


### [20] [Group-realizable multi-group learning by minimizing empirical risk](https://arxiv.org/abs/2601.16922)
*Navid Ardeshir,Samuel Deng,Daniel Hsu,Jingwen Liu*

Main category: cs.LG

TL;DR: 多组学习在组可实现设置下的样本复杂度优于不可知设置，即使组族无限但具有有限VC维。改进通过经验风险最小化实现，但计算不可行，建议使用非适当学习替代。


<details>
  <summary>Details</summary>
Motivation: 研究多组学习在不同设置下的样本复杂度差异，探索在组族无限但VC维有限的情况下，如何获得比不可知设置更好的样本复杂度。

Method: 使用组可实现概念类的经验风险最小化方法，虽然该方法本身可能具有无限VC维，但能获得改进的样本复杂度。同时指出该方法计算不可行，建议采用非适当学习作为替代方案。

Result: 在组可实现设置下，即使组族无限但具有有限VC维，多组学习的样本复杂度优于不可知设置。然而，实现该方法的计算是难处理的。

Conclusion: 组可实现设置能提供更好的样本复杂度，但需要非适当学习等替代方法来解决计算可行性问题。

Abstract: The sample complexity of multi-group learning is shown to improve in the group-realizable setting over the agnostic setting, even when the family of groups is infinite so long as it has finite VC dimension. The improved sample complexity is obtained by empirical risk minimization over the class of group-realizable concepts, which itself could have infinite VC dimension. Implementing this approach is also shown to be computationally intractable, and an alternative approach is suggested based on improper learning.

</details>


### [21] [Efficient Gaussian process learning via subspace projections](https://arxiv.org/abs/2601.16332)
*Felipe Tobar,Elsa Cazelles*

Main category: cs.LG

TL;DR: 提出一种基于数据低维线性投影的高斯过程训练新目标——投影似然(PL)，相比精确GP和变分稀疏GP在精度和计算效率上表现更优


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在大规模数据集上计算复杂度高，需要开发更高效的训练方法。现有稀疏GP方法如变分自由能虽然能降低计算成本，但仍有改进空间

Method: 使用数据低维线性投影构建高斯过程，提出投影似然(PL)训练目标。通过随机投影在单位球面上减少信息损失，提供信息损失的闭式表达式

Result: 在中等规模数据集上，投影似然方法在精度和计算效率上优于精确GP训练和变分自由能稀疏GP方法，且适用于不同优化器、核函数和数据集

Conclusion: 投影似然为高斯过程提供了一种有效且计算高效的训练方法，特别适用于中等规模数据集，在保持精度的同时显著降低计算复杂度

Abstract: We propose a novel training objective for GPs constructed using lower-dimensional linear projections of the data, referred to as \emph{projected likelihood} (PL). We provide a closed-form expression for the information loss related to the PL and empirically show that it can be reduced with random projections on the unit sphere. We show the superiority of the PL, in terms of accuracy and computational efficiency, over the exact GP training and the variational free energy approach to sparse GPs over different optimisers, kernels and datasets of moderately large sizes.

</details>


### [22] [A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs](https://arxiv.org/abs/2601.16979)
*Dayal Singh Kalra,Jean-Christophe Gagnon-Audet,Andrey Gromov,Ishita Mediratta,Kelvin Niu,Alexander H Miller,Michael Shvartsman*

Main category: cs.LG

TL;DR: 提出了一种计算高效的临界锐度度量，用于替代传统计算昂贵的Hessian锐度，首次在大规模语言模型（高达70亿参数）中展示了渐进锐化和稳定性边缘等曲率现象。


<details>
  <summary>Details</summary>
Motivation: Hessian锐度是分析神经网络训练动态的重要指标，但对于大型语言模型来说直接计算成本过高。需要一种计算高效的曲率度量方法来研究大规模模型的训练动态。

Method: 提出了临界锐度（λ_c），一种仅需少于10次前向传播的计算高效度量方法，基于参数更新方向Δθ。还引入了相对临界锐度（λ_c^{1→2}），用于量化优化一个损失函数时的另一个损失函数的曲率。

Result: 首次在高达70亿参数的OLMo-2模型中展示了渐进锐化和稳定性边缘等曲率现象，涵盖了预训练和中期训练。相对临界锐度成功分析了从预训练到微调的过渡，并指导了数据混合策略。

Conclusion: 临界锐度为从业者提供了实用的曲率动态诊断工具，可用于指导大规模训练中的数据组合选择。研究表明可扩展的曲率度量能够为大规模训练提供可操作的见解。

Abstract: Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($λ_{\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and interacts with the learning rate throughout training. Despite its significance in analyzing training dynamics, direct measurement of Hessian sharpness remains prohibitive for Large Language Models (LLMs) due to high computational cost. We analyze $\textit{critical sharpness}$ ($λ_c$), a computationally efficient measure requiring fewer than $10$ forward passes given the update direction $Δ\mathbfθ$. Critically, this measure captures well-documented Hessian sharpness phenomena, including progressive sharpening and Edge of Stability. Using this measure, we provide the first demonstration of these sharpness phenomena at scale, up to $7$B parameters, spanning both pre-training and mid-training of OLMo-2 models. We further introduce $\textit{relative critical sharpness}$ ($λ_c^{1\to 2}$), which quantifies the curvature of one loss landscape while optimizing another, to analyze the transition from pre-training to fine-tuning and guide data mixing strategies. Critical sharpness provides practitioners with a practical tool for diagnosing curvature dynamics and informing data composition choices at scale. More broadly, our work shows that scalable curvature measures can provide actionable insights for large-scale training.

</details>


### [23] [Analyzing Neural Network Information Flow Using Differential Geometry](https://arxiv.org/abs/2601.16366)
*Shuhang Tan,Jayson Sia,Paul Bogdan,Radoslav Ivanov*

Main category: cs.LG

TL;DR: 该论文从图论角度重新审视神经网络数据流问题，使用Ollivier-Ricci曲率识别关键连接，通过剪枝实验验证负曲率边对性能影响更大。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络数据流分析基于信息论，本文从图论视角出发，利用图曲率理论来识别神经网络中最重要的连接，为符号神经网络分析（如鲁棒性分析或模型修复）提供新工具。

Method: 1) 基于神经网络结构构建图，引入基于Ollivier-Ricci曲率的神经曲率概念；2) 根据输入示例的激活模式计算曲率；3) 使用神经曲率对边按重要性排序。通过剪枝实验评估方法，移除负曲率边会快速降低性能，而正曲率边影响很小。

Result: 在MNIST、CIFAR-10和CIFAR-100三个图像数据集上的实验表明，该方法能比现有剪枝方法识别更多不重要的边，负曲率边对神经网络整体功能至关重要。

Conclusion: 图曲率理论为神经网络数据流分析提供了有效的新视角，神经曲率能准确识别关键连接，在模型压缩和分析方面具有应用潜力。

Abstract: This paper provides a fresh view of the neural network (NN) data flow problem, i.e., identifying the NN connections that are most important for the performance of the full model, through the lens of graph theory. Understanding the NN data flow provides a tool for symbolic NN analysis, e.g.,~robustness analysis or model repair. Unlike the standard approach to NN data flow analysis, which is based on information theory, we employ the notion of graph curvature, specifically Ollivier-Ricci curvature (ORC). The ORC has been successfully used to identify important graph edges in various domains such as road traffic analysis, biological and social networks. In particular, edges with negative ORC are considered bottlenecks and as such are critical to the graph's overall connectivity, whereas positive-ORC edges are not essential. We use this intuition for the case of NNs as well: we 1)~construct a graph induced by the NN structure and introduce the notion of neural curvature (NC) based on the ORC; 2)~calculate curvatures based on activation patterns for a set of input examples; 3)~aim to demonstrate that NC can indeed be used to rank edges according to their importance for the overall NN functionality. We evaluate our method through pruning experiments and show that removing negative-ORC edges quickly degrades the overall NN performance, whereas positive-ORC edges have little impact. The proposed method is evaluated on a variety of models trained on three image datasets, namely MNIST, CIFAR-10 and CIFAR-100. The results indicate that our method can identify a larger number of unimportant edges as compared to state-of-the-art pruning methods.

</details>


### [24] [A Regularized Actor-Critic Algorithm for Bi-Level Reinforcement Learning](https://arxiv.org/abs/2601.16399)
*Sihan Zeng,Sujay Bhatt,Sumitra Ganesh,Alec Koppel*

Main category: cs.LG

TL;DR: 提出单循环一阶actor-critic算法，通过惩罚重构和衰减熵正则化解决双层优化问题，其中上层优化奖励函数，下层进行MDP策略优化


<details>
  <summary>Details</summary>
Motivation: 现有双层优化和强化学习方法需要二阶信息、强正则化或低效的嵌套循环，需要更高效的单循环一阶方法

Method: 提出单循环一阶actor-critic算法，通过惩罚重构将双层问题转化为单层，在下层RL目标中引入衰减熵正则化，实现渐进无偏的上层超梯度估计

Result: 在特殊Polyak-Lojasiewicz条件下，证明了算法在有限时间和有限样本下收敛到原始无正则化双层优化问题的稳定点

Conclusion: 方法在GridWorld目标位置问题和基于人类反馈的强化学习（RLHF）快乐推文生成任务中验证有效

Abstract: We study a structured bi-level optimization problem where the upper-level objective is a smooth function and the lower-level problem is policy optimization in a Markov decision process (MDP). The upper-level decision variable parameterizes the reward of the lower-level MDP, and the upper-level objective depends on the optimal induced policy. Existing methods for bi-level optimization and RL often require second-order information, impose strong regularization at the lower level, or inefficiently use samples through nested-loop procedures. In this work, we propose a single-loop, first-order actor-critic algorithm that optimizes the bi-level objective via a penalty-based reformulation. We introduce into the lower-level RL objective an attenuating entropy regularization, which enables asymptotically unbiased upper-level hyper-gradient estimation without solving the unregularized RL problem exactly. We establish the finite-time and finite-sample convergence of the proposed algorithm to a stationary point of the original, unregularized bi-level optimization problem through a novel lower-level residual analysis under a special type of Polyak-Lojasiewicz condition. We validate the performance of our method through experiments on a GridWorld goal position problem and on happy tweet generation through reinforcement learning from human feedback (RLHF).

</details>


### [25] [Towards a Theoretical Understanding to the Generalization of RLHF](https://arxiv.org/abs/2601.16403)
*Zhaochun Li,Mingyang Yi,Yue Wang,Shisheng Cui,Yong Liu*

Main category: cs.LG

TL;DR: 本文为RLHF在LLMs中的泛化理论提供了理论分析，在线性奖励模型下证明了策略模型的经验最优解具有O(n^{-1/2})的泛化界，并通过算法稳定性框架将结果推广到梯度上升算法。


<details>
  <summary>Details</summary>
Motivation: 虽然RLHF及其变体在实践中被证明能有效对齐大语言模型与人类意图，但这些方法在高维设置下的理论泛化特性尚未得到充分探索。现有研究主要基于奖励模型的最大似然估计一致性，而本文旨在建立与端到端学习实践一致的理论框架。

Method: 采用算法稳定性框架，在线性奖励模型假设下分析RLHF的泛化特性。通过特征覆盖条件，证明了策略模型经验最优解的泛化界，并将结果扩展到梯度上升(GA)和随机梯度上升(SGA)等基于梯度的学习算法。

Result: 在关键的特征覆盖条件下，证明了策略模型经验最优解具有O(n^{-1/2})的泛化界。该结果可推广到梯度上升算法获得的参数，为RLHF后LLMs观察到的经验泛化现象提供了新的理论证据。

Conclusion: 本文为RLHF在大语言模型中的泛化特性提供了首个端到端理论分析，填补了现有理论研究的空白，为实践中观察到的RLHF泛化能力提供了理论支撑，并建立了与算法稳定性框架的联系。

Abstract: Reinforcement Learning from Human Feedback (RLHF) and its variants have emerged as the dominant approaches for aligning Large Language Models with human intent. While empirically effective, the theoretical generalization properties of these methods in high-dimensional settings remain to be explored. To this end, we build the generalization theory on RLHF of LLMs under the linear reward model, through the framework of algorithmic stability. In contrast to the existing works built upon the consistency of maximum likelihood estimations on reward model, our analysis is presented under an end-to-end learning framework, which is consistent with practice. Concretely, we prove that under a key \textbf{feature coverage} condition, the empirical optima of policy model have a generalization bound of order $\mathcal{O}(n^{-\frac{1}{2}})$. Moreover, the results can be extrapolated to parameters obtained by gradient-based learning algorithms, i.e., Gradient Ascent (GA) and Stochastic Gradient Ascent (SGA). Thus, we argue that our results provide new theoretical evidence for the empirically observed generalization of LLMs after RLHF.

</details>


### [26] [Reasoning-Enhanced Rare-Event Prediction with Balanced Outcome Correction](https://arxiv.org/abs/2601.16406)
*Vitaly Bulgakov,Alexander Turchin*

Main category: cs.LG

TL;DR: LPCORP是一个两阶段框架，通过推理增强预测和置信度校正来改善罕见事件预测，在医疗和消费服务领域显著提升性能，特别是精确度。


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融、可靠性工程等领域，罕见事件预测至关重要，但极端的类别不平衡会使传统模型偏向多数类预测，限制了召回率、校准和实际应用价值。

Method: 提出LPCORP两阶段框架：1) 推理模型从叙述性输入生成增强预测；2) 轻量级逻辑回归分类器评估并选择性校正这些输出，以减轻流行度驱动的偏差。

Result: 该方法将高度不平衡设置转化为平衡设置，同时保留原始样本数量且不应用任何重采样策略。测试集评估显示性能显著提升，特别是在低流行度数据中已知较弱的精确度方面。成本降低分析显示在某些情况下可减少50%以上的费用。

Conclusion: LPCORP通过推理增强预测和置信度校正有效解决了罕见事件预测中的类别不平衡问题，在真实世界数据集中表现出优越性能，并展示了显著的成本节约潜力。

Abstract: Rare-event prediction is critical in domains such as healthcare, finance, reliability engineering, customer support, aviation safety, where positive outcomes are infrequent yet potentially catastrophic. Extreme class imbalance biases conventional models toward majority-class predictions, limiting recall, calibration, and operational usefulness. We propose LPCORP (Low-Prevalence CORrector for Prediction)*, a two-stage framework that combines reasoningenhanced prediction with confidence-based outcome correction. A reasoning model first produces enriched predictions from narrative inputs, after which a lightweight logistic-regression classifier evaluates and selectively corrects these outputs to mitigate prevalence-driven bias. We evaluate LPCORP on real-world datasets from medical and consumer service domains. The results show that this method transforms a highly imbalanced setting into a well-balanced one while preserving the original number of samples and without applying any resampling strategies. Test-set evaluation demonstrates substantially improved performance, particularly in precision, which is a known weakness in low-prevalence data. We further provide a costreduction analysis comparing the expenses associated with rare-event damage control without preventive measures to those incurred when low-cost, prediction-based preventive interventions are applied that showed more than 50% reduction in some cases. * Patent pending: U.S. Provisional 63/933,518, filed 8 December 2025.

</details>


### [27] [A Refinement of Vapnik--Chervonenkis' Theorem](https://arxiv.org/abs/2601.16411)
*A. Iosevich,A. Vagharshakyan,E. Wyman*

Main category: cs.LG

TL;DR: 本文改进了经典的Vapnik-Chervonenkis定理，通过使用具有显式Berry-Esseen误差控制的正态逼近代替Hoeffding不等式，在ε√n较大时获得了更精确的VC估计，在主要指数项中增加了(ε√n)^{-1}因子。


<details>
  <summary>Details</summary>
Motivation: 经典的VC定理是机器学习中的基础结果，它建立了经验概率一致收敛到理论概率的充分条件。然而，传统的证明方法在最后一步使用Hoeffding不等式，作者希望改进这一概率成分以获得更精确的估计。

Method: 作者重新审视了经典VC证明中的概率成分，不使用Hoeffding不等式，而是采用具有显式Berry-Esseen误差控制的正态逼近方法。这种方法能够提供更精确的误差控制，特别是在ε√n较大的情况下。

Result: 获得了VC估计的中偏差锐化结果，当ε√n较大时，在主要指数项中增加了(ε√n)^{-1}的因子，这比传统的VC估计更加精确。

Conclusion: 通过使用正态逼近代替Hoeffding不等式，本文改进了经典的VC定理，获得了更精确的一致收敛速率估计，特别是在中偏差区域提供了更优的误差控制。

Abstract: Vapnik--Chervonenkis' theorem is a seminal result in machine learning. It establishes sufficient conditions for empirical probabilities to converge to theoretical probabilities, uniformly over families of events. It also provides an estimate for the rate of such uniform convergence.
  We revisit the probabilistic component of the classical argument. Instead of applying Hoeffding's inequality at the final step, we use a normal approximation with explicit Berry--Esseen error control. This yields a moderate-deviation sharpening of the usual VC estimate, with an additional factor of order $(\varepsilon\sqrt{n})^{-1}$ in the leading exponential term when $\varepsilon\sqrt{n}$ is large.

</details>


### [28] [PyHealth 2.0: A Comprehensive Open-Source Toolkit for Accessible and Reproducible Clinical Deep Learning](https://arxiv.org/abs/2601.16414)
*John Wu,Yongda Fan,Zhenbang Wu,Paul Landes,Eric Schrock,Sayeed Sajjad Razin,Arjun Chatterjee,Naveen Baskaran,Joshua Steier,Andrea Fitzpatrick,Bilal Arif,Rian Atri,Jathurshan Pradeepkumar,Siddhartha Laghuvarapu,Junyi Gao,Adam R. Cross,Jimeng Sun*

Main category: cs.LG

TL;DR: PyHealth 2.0是一个增强的临床深度学习工具包，旨在通过统一框架、优化性能和活跃社区支持，降低临床AI研究的门槛，实现仅需7行代码即可完成预测建模。


<details>
  <summary>Details</summary>
Motivation: 临床AI研究面临基线难以复现、计算成本高、需要领域专业知识等持续障碍，这些因素阻碍了研究的可访问性和可重复性。

Method: 开发PyHealth 2.0工具包，提供统一框架整合15+数据集、20+临床任务、25+模型、5+可解释性方法和不确定性量化；优化性能实现39倍加速和20倍内存降低；建立400+成员的活跃开源社区。

Result: 创建了一个支持信号、影像和电子健康记录等多模态数据的综合工具包，显著提升了处理效率和内存使用效率，降低了领域专业知识门槛，支持从16GB笔记本到生产系统的部署。

Conclusion: PyHealth 2.0为可访问、可重复的医疗AI研究建立了开源基础和社区支持，通过简化工作流程和降低技术门槛，推动了临床AI研究的民主化。

Abstract: Difficulty replicating baselines, high computational costs, and required domain expertise create persistent barriers to clinical AI research. To address these challenges, we introduce PyHealth 2.0, an enhanced clinical deep learning toolkit that enables predictive modeling in as few as 7 lines of code. PyHealth 2.0 offers three key contributions: (1) a comprehensive toolkit addressing reproducibility and compatibility challenges by unifying 15+ datasets, 20+ clinical tasks, 25+ models, 5+ interpretability methods, and uncertainty quantification including conformal prediction within a single framework that supports diverse clinical data modalities - signals, imaging, and electronic health records - with translation of 5+ medical coding standards; (2) accessibility-focused design accommodating multimodal data and diverse computational resources with up to 39x faster processing and 20x lower memory usage, enabling work from 16GB laptops to production systems; and (3) an active open-source community of 400+ members lowering domain expertise barriers through extensive documentation, reproducible research contributions, and collaborations with academic health systems and industry partners, including multi-language support via RHealth. PyHealth 2.0 establishes an open-source foundation and community advancing accessible, reproducible healthcare AI. Available at pip install pyhealth.

</details>


### [29] [Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance](https://arxiv.org/abs/2601.16425)
*Huchen Yang,Xinghao Dong,Jin-Long Wu*

Main category: cs.LG

TL;DR: 该论文比较了贝叶斯实验设计中KL散度和Wasserstein距离两种效用函数的优劣，指出KL散度在无模型失配时收敛更快，而Wasserstein距离在存在模型失配时更稳健。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯实验设计(BED)中效用函数的选择一直是一个重要但未完全解决的问题。虽然KL散度是最常用的选择，但最近的研究提出了Wasserstein距离作为替代。需要系统比较这两种准则在实际应用中的表现，为实践者提供选择指导。

Method: 首先通过一个玩具示例说明Wasserstein距离的问题：固定形状后验的Wasserstein距离值取决于其主要质量在支撑集中的相对位置，可能产生与信息增益无关的虚假奖励。然后通过BED文献中的经典源反演问题，系统比较KL散度和Wasserstein距离在有无模型失配情况下的表现。

Result: 研究发现：1) Wasserstein距离对后验分布的位置敏感，可能产生与信息增益无关的虚假奖励；2) 在没有模型失配时，KL散度能带来更快的收敛速度；3) 当模型失配不可忽略时，Wasserstein距离能提供更稳健的序贯BED结果。

Conclusion: 该研究阐明了KL散度和Wasserstein距离作为效用函数的权衡关系，为实际BED应用中如何选择合适的准则提供了指导：在无模型失配时优先使用KL散度以获得更快收敛，在存在显著模型失配时使用Wasserstein距离以获得更稳健的结果。

Abstract: Designing experiments that systematically gather data from complex physical systems is central to accelerating scientific discovery. While Bayesian experimental design (BED) provides a principled, information-based framework that integrates experimental planning with probabilistic inference, the selection of utility functions in BED is a long-standing and active topic, where different criteria emphasize different notions of information. Although Kullback--Leibler (KL) divergence has been one of the most common choices, recent studies have proposed Wasserstein distance as an alternative. In this work, we first employ a toy example to illustrate an issue of Wasserstein distance - the value of Wasserstein distance of a fixed-shape posterior depends on the relative position of its main mass within the support and can exhibit false rewards unrelated to information gain, especially with a non-informative prior (e.g., uniform distribution). We then further provide a systematic comparison between these two criteria through a classical source inversion problem in the BED literature, revealing that the KL divergence tends to lead to faster convergence in the absence of model discrepancy, while Wasserstein metrics provide more robust sequential BED results if model discrepancy is non-negligible. These findings clarify the trade-offs between KL divergence and Wasserstein metrics for the utility function and provide guidelines for selecting suitable criteria in practical BED applications.

</details>


### [30] [Safe Multitask Molecular Graph Networks for Vapor Pressure and Odor Threshold Prediction](https://arxiv.org/abs/2601.16426)
*Shuang Wu,Meijie Wang,Lun Yu*

Main category: cs.LG

TL;DR: 该研究系统评估了分子图神经网络在蒸汽压和气味阈值预测任务上的表现，提出了一种安全的跨任务学习方法，在保持主任务性能的同时提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决气味相关性质建模中的两个关键任务：蒸汽压和气味阈值预测，特别关注模型在分布外数据上的泛化能力，并探索多任务学习的有效策略。

Method: 采用Bemis-Murcko骨架分割评估OOD能力；引入A20/E17分子图特征（20维原子特征+17维键特征）；系统比较GINE和PNA网络架构；提出"安全多任务"方法：以VP为主任务，OP为辅助任务，采用延迟激活+梯度裁剪+小权重策略。

Result: VP任务上，PNA网络在归一化空间中的验证MSE约为0.21；OP单任务使用A20/E17特征和鲁棒训练达到验证MSE 0.60-0.61；安全多任务方法在保持主任务性能的同时获得了最佳的VP泛化性能。

Conclusion: 该研究提供了完整的可重复实验、消融研究和误差相似性分析，证明了所提出的安全多任务学习方法的有效性，同时讨论了数据噪声的影响和方法局限性。

Abstract: We investigate two important tasks in odor-related property modeling: Vapor Pressure (VP) and Odor Threshold (OP). To evaluate the model's out-of-distribution (OOD) capability, we adopt the Bemis-Murcko scaffold split. In terms of features, we introduce the rich A20/E17 molecular graph features (20-dimensional atom features + 17-dimensional bond features) and systematically compare GINE and PNA backbones. The results show: for VP, PNA with a simple regression head achieves Val MSE $\approx$ 0.21 (normalized space); for the OP single task under the same scaffold split, using A20/E17 with robust training (Huber/winsor) achieves Val MSE $\approx$ 0.60-0.61. For multitask training, we propose a **"safe multitask"** approach: VP as the primary task and OP as the auxiliary task, using delayed activation + gradient clipping + small weight, which avoids harming the primary task and simultaneously yields the best VP generalization performance. This paper provides complete reproducible experiments, ablation studies, and error-similarity analysis while discussing the impact of data noise and method limitations.

</details>


### [31] [Endless Terminals: Scaling RL Environments for Terminal Agents](https://arxiv.org/abs/2601.16443)
*Kanishk Gandhi,Shivam Garg,Noah D. Goodman,Dimitris Papailiopoulos*

Main category: cs.LG

TL;DR: Endless Terminals是一个自主生成终端任务的环境管道，通过简单PPO训练显著提升模型在终端任务上的表现


<details>
  <summary>Details</summary>
Motivation: 当前终端基准测试主要用于评估而非训练，强化学习需要可扩展的环境管道而非静态数据集

Method: 开发四阶段管道：生成任务描述、构建验证容器环境、生成完成测试、筛选可解任务；使用简单PPO和二进制奖励训练

Result: 生成3255个任务，模型性能大幅提升：Llama-3.2-3B从4.0%到18.2%，Qwen2.5-7B从10.7%到53.3%；在TerminalBench 2.0上也显著改进

Conclusion: 当环境可扩展时，简单的强化学习就能成功，无需复杂的多智能体协调或专门工具

Abstract: Environments are the bottleneck for self-improving agents. Current terminal benchmarks were built for evaluation, not training; reinforcement learning requires a scalable pipeline, not just a dataset. We introduce Endless Terminals, a fully autonomous pipeline that procedurally generates terminal-use tasks without human annotation. The pipeline has four stages: generating diverse task descriptions, building and validating containerized environments, producing completion tests, and filtering for solvability. From this pipeline we obtain 3255 tasks spanning file operations, log management, data processing, scripting, and database operations. We train agents using vanilla PPO with binary episode level rewards and a minimal interaction loop: no retrieval, multi-agent coordination, or specialized tools. Despite this simplicity, models trained on Endless Terminals show substantial gains: on our held-out dev set, Llama-3.2-3B improves from 4.0% to 18.2%, Qwen2.5-7B from 10.7% to 53.3%, and Qwen3-8B-openthinker-sft from 42.6% to 59.0%. These improvements transfer to human-curated benchmarks: models trained on Endless Terminals show substantial gains on held out human curated benchmarks: on TerminalBench 2.0, Llama-3.2-3B improves from 0.0% to 2.2%, Qwen2.5-7B from 2.2% to 3.4%, and Qwen3-8B-openthinker-sft from 1.1% to 6.7%, in each case outperforming alternative approaches including models with more complex agentic scaffolds. These results demonstrate that simple RL succeeds when environments scale.

</details>


### [32] [Brownian ReLU(Br-ReLU): A New Activation Function for a Long-Short Term Memory (LSTM) Network](https://arxiv.org/abs/2601.16446)
*George Awiakye-Marfo,Elijah Agbosu,Victoria Mawuena Barns,Samuel Asante Gyamerah*

Main category: cs.LG

TL;DR: 提出基于布朗运动的随机激活函数BrownianReLU，解决金融时间序列中传统激活函数的梯度不稳定问题，在LSTM网络中提升预测精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统激活函数（如ReLU、LeakyReLU、PReLU）在处理噪声大、非平稳的金融时间序列时存在梯度不稳定问题，需要更适应金融数据特性的激活函数。

Method: 提出BrownianReLU激活函数，基于布朗运动引入随机性，使用蒙特卡洛模拟实现平滑自适应响应，解决"dying ReLU"问题，并在LSTM网络中应用。

Result: 在苹果、GCB、标普500等金融时间序列以及LendingClub贷款分类数据上，BrownianReLU相比传统激活函数获得更低的均方误差和更高的R²值，预测精度和泛化能力显著提升。

Conclusion: BrownianReLU通过布朗运动随机性有效改善了金融时间序列建模中的梯度传播和学习稳定性，激活函数选择在准确率和敏感性权衡中具有实际意义。

Abstract: Deep learning models are effective for sequential data modeling, yet commonly used activation functions such as ReLU, LeakyReLU, and PReLU often exhibit gradient instability when applied to noisy, non-stationary financial time series. This study introduces BrownianReLU, a stochastic activation function induced by Brownian motion that enhances gradient propagation and learning stability in Long Short-Term Memory (LSTM) networks. Using Monte Carlo simulation, BrownianReLU provides a smooth, adaptive response for negative inputs, mitigating the dying ReLU problem. The proposed activation is evaluated on financial time series from Apple, GCB, and the S&P 500, as well as LendingClub loan data for classification. Results show consistently lower Mean Squared Error and higher $R^2$ values, indicating improved predictive accuracy and generalization. Although ROC-AUC metric is limited in classification tasks, activation choice significantly affects the trade-off between accuracy and sensitivity, with Brownian ReLU and the selected activation functions yielding practically meaningful performance.

</details>


### [33] [On the Expressive Power of Floating-Point Transformers](https://arxiv.org/abs/2601.16450)
*Sejun Park,Yeachan Park,Geonho Hwang*

Main category: cs.LG

TL;DR: 研究浮点数Transformer的表达能力，发现与实数参数下的理论结果不同，浮点数Transformer可以表示非置换等变函数，且序列长度有限时能表示所有置换等变函数，但序列长度大时则不能。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer表达能力研究基于实数参数和精确运算，但实际计算机实现使用有限浮点数和含舍入误差的机器运算。需要研究浮点数参数和浮点运算下Transformer的真实表达能力。

Method: 分析使用浮点数参数和浮点运算的Transformer模型，研究其在有限精度计算条件下的数学性质。通过理论证明探讨浮点数Transformer的表示能力边界。

Result: 1. 浮点数Transformer即使没有位置编码也能表示一类非置换等变函数；2. 序列长度有限时能表示所有置换等变函数，但序列长度大时不能；3. 发现浮点数Transformer的最小等变结构；4. 所有非平凡加法位置编码都会损害浮点数Transformer的表示能力。

Conclusion: 浮点数实现显著改变了Transformer的理论表达能力，实际计算机实现与理想数学模型存在重要差异。浮点运算的有限精度特性既带来新的表示能力（非等变函数），又限制了某些理论能力（大序列长度下的等变函数）。

Abstract: The study on the expressive power of transformers shows that transformers are permutation equivariant, and they can approximate all permutation-equivariant continuous functions on a compact domain. However, these results are derived under real parameters and exact operations, while real implementations on computers can only use a finite set of numbers and inexact machine operations with round-off errors. In this work, we investigate the representability of floating-point transformers that use floating-point parameters and floating-point operations. Unlike existing results under exact operations, we first show that floating-point transformers can represent a class of non-permutation-equivariant functions even without positional encoding. Furthermore, we prove that floating-point transformers can represent all permutation-equivariant functions when the sequence length is bounded, but they cannot when the sequence length is large. We also found the minimal equivariance structure in floating-point transformers, and show that all non-trivial additive positional encoding can harm the representability of floating-point transformers.

</details>


### [34] [On the Effects of Adversarial Perturbations on Distribution Robustness](https://arxiv.org/abs/2601.16464)
*Yipei Wang,Zhaoying Pan,Xiaoqian Wang*

Main category: cs.LG

TL;DR: 论文分析了对抗鲁棒性和分布鲁棒性之间的权衡关系，发现ℓ∞扰动在适度偏置数据上能提升分布鲁棒性，特征可分性在这一权衡中起关键作用。


<details>
  <summary>Details</summary>
Motivation: 对抗鲁棒性和分布鲁棒性都旨在确保模型可靠性能，但先前研究发现两者存在权衡。对抗训练可能增加对虚假特征的依赖，从而损害分布鲁棒性，特别是在某些代表性不足的子群体上。需要深入理解这种权衡关系的机制。

Method: 通过理论分析，研究在扰动数据上训练的模型，为每步对抗训练提供可处理的替代方案。分析ℓ∞扰动在不同偏置程度数据上的影响，并考察特征可分性在权衡中的作用。

Result: 发现ℓ∞扰动在适度偏置数据上能提升分布鲁棒性；在高度偏斜数据上，当简单性偏置诱导对核心特征的依赖时，分布鲁棒性的增益仍然存在；特征可分性在权衡关系中起关键作用。

Conclusion: 研究扩展了对对抗鲁棒性和分布鲁棒性权衡关系的理解，强调了特征可分性在这一权衡中的重要作用。尽管在许多情况下权衡仍然存在，但忽视特征可分性的作用可能导致对鲁棒性的误导性结论。

Abstract: Adversarial robustness refers to a model's ability to resist perturbation of inputs, while distribution robustness evaluates the performance of the model under data shifts. Although both aim to ensure reliable performance, prior work has revealed a tradeoff in distribution and adversarial robustness. Specifically, adversarial training might increase reliance on spurious features, which can harm distribution robustness, especially the performance on some underrepresented subgroups. We present a theoretical analysis of adversarial and distribution robustness that provides a tractable surrogate for per-step adversarial training by studying models trained on perturbed data. In addition to the tradeoff, our work further identified a nuanced phenomenon that $\ell_\infty$ perturbations on data with moderate bias can yield an increase in distribution robustness. Moreover, the gain in distribution robustness remains on highly skewed data when simplicity bias induces reliance on the core feature, characterized as greater feature separability. Our theoretical analysis extends the understanding of the tradeoff by highlighting the interplay of the tradeoff and the feature separability. Despite the tradeoff that persists in many cases, overlooking the role of feature separability may lead to misleading conclusions about robustness.

</details>


### [35] [A Cautionary Tale of Self-Supervised Learning for Imaging Biomarkers: Alzheimer's Disease Case Study](https://arxiv.org/abs/2601.16467)
*Maxwell Reynolds,Chaitanya Srinivasan,Vijay Cherupally,Michael Leone,Ke Yu,Li Sun,Tigmanshu Chaudhary,Andreas Pfenning,Kayhan Batmanghelich*

Main category: cs.LG

TL;DR: 该论文提出了一种新的自监督学习框架R-NCE，用于从结构MRI中提取更强大的阿尔茨海默病生物标志物，该方法整合了FreeSurfer特征并超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病早期检测需要敏感且具有生物学基础的生物标志物。虽然结构MRI广泛可用，但通常依赖手工特征（如皮质厚度或体积）。作者探索自监督学习是否能从相同数据中发现更强大的生物标志物。

Method: 提出Residual Noise Contrastive Estimation (R-NCE)框架，整合辅助的FreeSurfer特征，同时最大化增强不变性信息。该方法结合了传统特征和自监督学习的优势。

Result: R-NCE在疾病分类、转化预测和淀粉样蛋白状态预测等多个基准测试中超越了传统特征和现有自监督学习方法。R-NCE衍生的脑年龄差距(BAG)测量显示高遗传性，并与MAPT和IRAG1基因相关，在星形胶质细胞和少突胶质细胞中富集。

Conclusion: R-NCE能够从结构MRI中发现更敏感且具有生物学意义的阿尔茨海默病生物标志物，对神经退行性和脑血管过程具有敏感性，为早期检测和监测提供了新工具。

Abstract: Discovery of sensitive and biologically grounded biomarkers is essential for early detection and monitoring of Alzheimer's disease (AD). Structural MRI is widely available but typically relies on hand-crafted features such as cortical thickness or volume. We ask whether self-supervised learning (SSL) can uncover more powerful biomarkers from the same data. Existing SSL methods underperform FreeSurfer-derived features in disease classification, conversion prediction, and amyloid status prediction. We introduce Residual Noise Contrastive Estimation (R-NCE), a new SSL framework that integrates auxiliary FreeSurfer features while maximizing additional augmentation-invariant information. R-NCE outperforms traditional features and existing SSL methods across multiple benchmarks, including AD conversion prediction. To assess biological relevance, we derive Brain Age Gap (BAG) measures and perform genome-wide association studies. R-NCE-BAG shows high heritability and associations with MAPT and IRAG1, with enrichment in astrocytes and oligodendrocytes, indicating sensitivity to neurodegenerative and cerebrovascular processes.

</details>


### [36] [Robust Categorical Data Clustering Guided by Multi-Granular Competitive Learning](https://arxiv.org/abs/2601.16491)
*Shenghong Cai,Yiqun Zhang,Xiaopeng Luo,Yiu-Ming Cheung,Hong Jia,Peng Liu*

Main category: cs.LG

TL;DR: 提出MCDC方法用于类别数据聚类，通过多粒度竞争惩罚学习(MGCPL)和基于编码的聚类聚合(CAME)，能自动探索嵌套的多粒度聚类分布，具有线性时间复杂度和高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 类别数据在大数据分析中很常见，但由于其定性特征，在隐式离散距离空间中存在嵌套粒度聚类效应，传统聚类方法难以处理这种特性，需要专门的方法来有效分析类别数据。

Method: 提出MCDC方法：1) MGCPL算法让潜在聚类通过竞争惩罚学习在不同粒度上交互调整和收敛；2) CAME策略基于MGCPL编码将数据对象编码为嵌入表示，然后在嵌入空间进行最终聚类。

Result: MCDC能自动探索嵌套的多粒度聚类分布，对各种领域的类别数据集具有高鲁棒性，线性时间复杂度使其能扩展到大规模数据集，实验证明其在多个公开数据集上优于现有方法。

Conclusion: MCDC方法能有效处理类别数据的嵌套粒度聚类问题，具有自动性、鲁棒性和可扩展性，可用于大数据集的预分区或分布式计算节点划分，提升计算效率。

Abstract: Data set composed of categorical features is very common in big data analysis tasks. Since categorical features are usually with a limited number of qualitative possible values, the nested granular cluster effect is prevalent in the implicit discrete distance space of categorical data. That is, data objects frequently overlap in space or subspace to form small compact clusters, and similar small clusters often form larger clusters. However, the distance space cannot be well-defined like the Euclidean distance due to the qualitative categorical data values, which brings great challenges to the cluster analysis of categorical data. In view of this, we design a Multi-Granular Competitive Penalization Learning (MGCPL) algorithm to allow potential clusters to interactively tune themselves and converge in stages with different numbers of naturally compact clusters. To leverage MGCPL, we also propose a Cluster Aggregation strategy based on MGCPL Encoding (CAME) to first encode the data objects according to the learned multi-granular distributions, and then perform final clustering on the embeddings. It turns out that the proposed MGCPL-guided Categorical Data Clustering (MCDC) approach is competent in automatically exploring the nested distribution of multi-granular clusters and highly robust to categorical data sets from various domains. Benefiting from its linear time complexity, MCDC is scalable to large-scale data sets and promising in pre-partitioning data sets or compute nodes for boosting distributed computing. Extensive experiments with statistical evidence demonstrate its superiority compared to state-of-the-art counterparts on various real public data sets.

</details>


### [37] [BoostFGL: Boosting Fairness in Federated Graph Learning](https://arxiv.org/abs/2601.16496)
*Zekai Chen,Kairui Yang,Xunkai Li,Henan Sun,Zhihan Zhang,Jia Li,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: BoostFGL是一个公平感知的联邦图学习框架，通过客户端节点增强、拓扑增强和服务端模型增强三个机制，解决联邦图学习中节点组间性能差异问题，在保持整体性能的同时显著提升公平性。


<details>
  <summary>Details</summary>
Motivation: 现有联邦图学习方法虽然整体准确率高，但会掩盖弱势节点组的严重性能退化问题。这种差异源于三个耦合因素：标签偏向多数模式、消息传播中的拓扑混淆，以及困难客户端更新的聚合稀释。

Method: BoostFGL采用提升式框架，包含三个协调机制：1) 客户端节点增强：重塑本地训练信号，强调系统性服务不足的节点；2) 客户端拓扑增强：重新分配传播重点，偏向可靠但未充分利用的结构，减弱误导性邻域；3) 服务端模型增强：执行难度和可靠性感知的聚合，保留困难客户端的有效更新同时稳定全局模型。

Result: 在9个数据集上的广泛实验表明，BoostFGL带来显著的公平性提升，Overall-F1提高了8.43%，同时相对于强大的联邦图学习基线保持有竞争力的整体性能。

Conclusion: BoostFGL成功解决了联邦图学习中的公平性问题，通过协调的增强机制有效减少了节点组间的性能差异，为公平感知的联邦图学习提供了有效的解决方案。

Abstract: Federated graph learning (FGL) enables collaborative training of graph neural networks (GNNs) across decentralized subgraphs without exposing raw data. While existing FGL methods often achieve high overall accuracy, we show that this average performance can conceal severe degradation on disadvantaged node groups. From a fairness perspective, these disparities arise systematically from three coupled sources: label skew toward majority patterns, topology confounding in message propagation, and aggregation dilution of updates from hard clients. To address this, we propose \textbf{BoostFGL}, a boosting-style framework for fairness-aware FGL. BoostFGL introduces three coordinated mechanisms: \ding{182} \emph{Client-side node boosting}, which reshapes local training signals to emphasize systematically under-served nodes; \ding{183} \emph{Client-side topology boosting}, which reallocates propagation emphasis toward reliable yet underused structures and attenuates misleading neighborhoods; and \ding{184} \emph{Server-side model boosting}, which performs difficulty- and reliability-aware aggregation to preserve informative updates from hard clients while stabilizing the global model. Extensive experiments on 9 datasets show that BoostFGL delivers substantial fairness gains, improving Overall-F1 by 8.43\%, while preserving competitive overall performance against strong FGL baselines.

</details>


### [38] [kNN-Graph: An adaptive graph model for $k$-nearest neighbors](https://arxiv.org/abs/2601.16509)
*Jiaye Li,Gang Chen,Hang Xu,Shichao Zhang*

Main category: cs.LG

TL;DR: 提出自适应图模型，将kNN的邻居选择和权重计算转移到训练阶段，通过HNSW图结构实现实时推理而不损失精度


<details>
  <summary>Details</summary>
Motivation: 传统kNN算法在大规模应用中面临推理速度与精度的计算权衡，现有近似最近邻方法虽然加速检索但会降低分类精度，且缺乏自适应选择最优邻居数量(k)的能力

Method: 提出自适应图模型，集成分层可导航小世界(HNSW)图和预计算投票机制，将邻居选择和权重计算完全转移到训练阶段。高层图实现快速导航，低层图编码精确的节点特定决策边界和自适应邻居数量

Result: 在六个不同数据集上对八个最先进基线进行基准测试，该架构显著加速推理速度，实现实时性能，同时不损害分类精度

Conclusion: 为解决kNN长期存在的推理瓶颈提供了可扩展、鲁棒的解决方案，建立了基于图的非参数学习的新结构范式

Abstract: The k-nearest neighbors (kNN) algorithm is a cornerstone of non-parametric classification in artificial intelligence, yet its deployment in large-scale applications is persistently constrained by the computational trade-off between inference speed and accuracy. Existing approximate nearest neighbor solutions accelerate retrieval but often degrade classification precision and lack adaptability in selecting the optimal neighborhood size (k). Here, we present an adaptive graph model that decouples inference latency from computational complexity. By integrating a Hierarchical Navigable Small World (HNSW) graph with a pre-computed voting mechanism, our framework completely transfers the computational burden of neighbor selection and weighting to the training phase. Within this topological structure, higher graph layers enable rapid navigation, while lower layers encode precise, node-specific decision boundaries with adaptive neighbor counts. Benchmarking against eight state-of-the-art baselines across six diverse datasets, we demonstrate that this architecture significantly accelerates inference speeds, achieving real-time performance, without compromising classification accuracy. These findings offer a scalable, robust solution to the long-standing inference bottleneck of kNN, establishing a new structural paradigm for graph-based nonparametric learning.

</details>


### [39] [Finite-Time Analysis of Gradient Descent for Shallow Transformers](https://arxiv.org/abs/2601.16514)
*Enes Arda,Semih Cayci,Atilla Eryilmaz*

Main category: cs.LG

TL;DR: 浅层Transformer在核机制下训练，宽度仅需对数级样本量，优化误差与序列长度无关，但内存需求随序列长度增长。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer为何表现优异具有挑战性，因为其优化景观是非凸的。本文旨在分析浅层Transformer在核机制下的训练行为，并与循环架构对比。

Method: 分析具有m个独立头的浅层Transformer，使用投影梯度下降在核机制下训练。理论分析结合数值验证，在教师-学生设置中测试预测的缩放规律。

Result: 发现两个主要结果：(1) 非渐近保证所需的宽度仅与样本量n呈对数关系，(2) 优化误差与序列长度T无关。这与循环架构形成鲜明对比，后者的优化误差可能随T指数增长。代价是内存需求随T增长。

Conclusion: Transformer在优化方面具有优势，其优化误差与序列长度无关，宽度需求对数级缩放，但需要更多内存来保持完整上下文。这解释了Transformer相对于循环架构的优越性能。

Abstract: Understanding why Transformers perform so well remains challenging due to their non-convex optimization landscape. In this work, we analyze a shallow Transformer with $m$ independent heads trained by projected gradient descent in the kernel regime. Our analysis reveals two main findings: (i) the width required for nonasymptotic guarantees scales only logarithmically with the sample size $n$, and (ii) the optimization error is independent of the sequence length $T$. This contrasts sharply with recurrent architectures, where the optimization error can grow exponentially with $T$. The trade-off is memory: to keep the full context, the Transformer's memory requirement grows with the sequence length. We validate our theoretical results numerically in a teacher-student setting and confirm the predicted scaling laws for Transformers.

</details>


### [40] [Rethinking Large Language Models For Irregular Time Series Classification In Critical Care](https://arxiv.org/abs/2601.16516)
*Feixiang Zheng,Yu Wu,Cecilia Mascolo,Ting Dang*

Main category: cs.LG

TL;DR: 该研究系统评估了大型语言模型在ICU不规则时间序列数据上的应用，发现编码器设计比对齐策略更重要，但LLM方法训练时间显著更长且在小样本场景下表现不佳。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在时间序列建模中显示出潜力，但其在ICU不规则数据（高缺失率）上的有效性尚未充分探索，需要系统评估编码器设计和对齐策略的影响。

Method: 建立系统测试平台，评估不同LLM方法在ICU基准数据集上的表现，包括时间序列编码器和多模态对齐策略，并与强监督和自监督基线比较。

Result: 编码器设计比对齐策略更关键，能显式建模不规则性的编码器比普通Transformer平均AUPRC提升12.8%；最佳对齐策略比交叉注意力提升2.9%；但LLM方法训练时间至少长10倍，性能仅与最佳监督模型相当，在小样本学习中表现不佳。

Conclusion: LLM在ICU不规则时间序列分析中既有潜力也有局限：编码器设计至关重要，但训练成本高且小样本性能有限，需要进一步优化才能实用化。

Abstract: Time series data from the Intensive Care Unit (ICU) provides critical information for patient monitoring. While recent advancements in applying Large Language Models (LLMs) to time series modeling (TSM) have shown great promise, their effectiveness on the irregular ICU data, characterized by particularly high rates of missing values, remains largely unexplored. This work investigates two key components underlying the success of LLMs for TSM: the time series encoder and the multimodal alignment strategy. To this end, we establish a systematic testbed to evaluate their impact across various state-of-the-art LLM-based methods on benchmark ICU datasets against strong supervised and self-supervised baselines. Results reveal that the encoder design is more critical than the alignment strategy. Encoders that explicitly model irregularity achieve substantial performance gains, yielding an average AUPRC increase of $12.8\%$ over the vanilla Transformer. While less impactful, the alignment strategy is also noteworthy, with the best-performing semantically rich, fusion-based strategy achieving a modest $2.9\%$ improvement over cross-attention. However, LLM-based methods require at least 10$\times$ longer training than the best-performing irregular supervised models, while delivering only comparable performance. They also underperform in data-scarce few-shot learning settings. These findings highlight both the promise and current limitations of LLMs for irregular ICU time series. The code is available at https://github.com/mHealthUnimelb/LLMTS.

</details>


### [41] [DANCE: Dynamic, Available, Neighbor-gated Condensation for Federated Text-Attributed Graphs](https://arxiv.org/abs/2601.16519)
*Zekai Chen,Haodong Lu,Xunkai Li,Henan Sun,Jia Li,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: DANCE提出了一种新的TAG-FGL范式，通过轮次式模型内循环图压缩刷新和可追溯的证据包，解决了现有方法在开销、次优性能和可解释性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前文本属性图联邦学习（TAG-FGL）方法面临三个主要挑战：1）处理长文本的LLM带来高计算开销；2）引入图压缩后的一步式压缩缺乏客户端适应性，导致性能次优；3）LLM压缩引入黑盒瓶颈，摘要缺乏可追溯性和可解释性。

Method: DANCE采用轮次式、模型内循环的图压缩刷新机制，使用最新的全局模型动态更新压缩核心。同时保留可追溯的证据包，将预测结果追溯到选定的邻居和源文本片段，增强可解释性。

Result: 在8个TAG数据集上，DANCE在8%压缩率下将准确率提高了2.33%，同时比基线方法减少了33.42%的token使用量。

Conclusion: DANCE通过动态图压缩刷新和可追溯的证据包，有效解决了TAG-FGL中的开销、性能次优和可解释性问题，为实用的文本属性图联邦学习提供了新范式。

Abstract: Federated graph learning (FGL) enables collaborative training on graph data across multiple clients. With the rise of large language models (LLMs), textual attributes in FGL graphs are gaining attention. Text-attributed graph federated learning (TAG-FGL) improves FGL by explicitly leveraging LLMs to process and integrate these textual features. However, current TAG-FGL methods face three main challenges: \textbf{(1) Overhead.} LLMs for processing long texts incur high token and computation costs. To make TAG-FGL practical, we introduce graph condensation (GC) to reduce computation load, but this choice also brings new issues. \textbf{(2) Suboptimal.} To reduce LLM overhead, we introduce GC into TAG-FGL by compressing multi-hop texts/neighborhoods into a condensed core with fixed LLM surrogates. However, this one-shot condensation is often not client-adaptive, leading to suboptimal performance. \textbf{(3) Interpretability.} LLM-based condensation further introduces a black-box bottleneck: summaries lack faithful attribution and clear grounding to specific source spans, making local inspection and auditing difficult. To address the above issues, we propose \textbf{DANCE}, a new TAG-FGL paradigm with GC. To improve \textbf{suboptimal} performance, DANCE performs round-wise, model-in-the-loop condensation refresh using the latest global model. To enhance \textbf{interpretability}, DANCE preserves provenance by storing locally inspectable evidence packs that trace predictions to selected neighbors and source text spans. Across 8 TAG datasets, DANCE improves accuracy by \textbf{2.33\%} at an \textbf{8\%} condensation ratio, with \textbf{33.42\%} fewer tokens than baselines.

</details>


### [42] [Beyond Superficial Unlearning: Sharpness-Aware Robust Erasure of Hallucinations in Multimodal LLMs](https://arxiv.org/abs/2601.16527)
*Xianya Fang,Feiyang Ren,Xiang Chen,Yu Tian,Zhen Bi,Haiyang Yu,Sheng-Jun Huang*

Main category: cs.LG

TL;DR: SARE提出了一种针对多模态LLM对象幻觉的结构化遗忘方法，通过目标最小-最大优化和Targeted-SAM机制来平坦化损失景观，确保对幻觉概念的稳健移除。


<details>
  <summary>Details</summary>
Motivation: 多模态LLM存在对象幻觉问题，描述不存在的实体，损害可靠性。现有遗忘方法存在结构性脆弱缺陷，只能实现表面抑制，模型陷入尖锐最小值，在轻量级再学习后幻觉会灾难性复发。

Method: 提出SARE框架，将遗忘建模为目标最小-最大优化问题，使用Targeted-SAM机制在幻觉概念周围显式平坦化损失景观。通过在最坏情况参数扰动下抑制幻觉，确保对权重偏移的稳健移除。

Result: SARE在遗忘效果上显著优于基线方法，同时保持一般生成质量。关键的是，它能持续抑制幻觉，抵抗再学习和参数更新，验证了几何稳定化的有效性。

Conclusion: 通过几何稳定化方法，SARE能够实现多模态LLM中对象幻觉的稳健遗忘，解决了现有方法的结构脆弱性问题，确保幻觉概念被持久移除。

Abstract: Multimodal LLMs are powerful but prone to object hallucinations, which describe non-existent entities and harm reliability. While recent unlearning methods attempt to mitigate this, we identify a critical flaw: structural fragility. We empirically demonstrate that standard erasure achieves only superficial suppression, trapping the model in sharp minima where hallucinations catastrophically resurge after lightweight relearning. To ensure geometric stability, we propose SARE, which casts unlearning as a targeted min-max optimization problem and uses a Targeted-SAM mechanism to explicitly flatten the loss landscape around hallucinated concepts. By suppressing hallucinations under simulated worst-case parameter perturbations, our framework ensures robust removal stable against weight shifts. Extensive experiments demonstrate that SARE significantly outperforms baselines in erasure efficacy while preserving general generation quality. Crucially, it maintains persistent hallucination suppression against relearning and parameter updates, validating the effectiveness of geometric stabilization.

</details>


### [43] [A Collision-Free Hot-Tier Extension for Engram-Style Conditional Memory: A Controlled Study of Training Dynamics](https://arxiv.org/abs/2601.16531)
*Tao Lin*

Main category: cs.LG

TL;DR: 高频键冲突不是Engram式条件记忆的主要瓶颈，消除冲突的碰撞自由设计在同等参数下并未持续改善验证损失，冲突反而可能提供有益的隐式正则化。


<details>
  <summary>Details</summary>
Motivation: 研究高频键冲突是否是Engram式条件记忆的主要瓶颈，探索消除冲突是否能改善模型性能。

Method: 引入Engram-Nine碰撞自由热层扩展，通过最小完美哈希函数映射最频繁的n-gram，同时保留原始多头哈希查找作为冷层。在严格等参数设置下比较碰撞自由设计与碰撞易发基线，并通过路由分层评估分解每个token损失为热/冷贡献。

Result: 碰撞自由设计并未持续改善验证损失。发现训练中一致的"热到冷优势翻转"现象：热位置初始损失较低，但冷位置最终超越。碰撞自由配置比碰撞易发基线翻转更早，表明冲突起到隐式正则化作用。还发现门控不匹配：门控早期学习偏好热位置，但即使在翻转后仍保持这种偏好，将更高权重分配给损失更高的位置。

Conclusion: 单纯提高查找精度不能保证更好的训练结果。主要限制可能在于门控信用分配而非索引准确性，碰撞引起的噪声可能提供有益的正则化，不应简单消除。

Abstract: We investigate whether high-frequency key collisions are a primary bottleneck in Engram-style conditional memory. To isolate the effect of collisions, we introduce Engram-Nine, a collision-free hot-tier extension that maps the most frequent n-grams through a Minimal Perfect Hash Function (MPHF) while retaining the original multi-head hashed lookup as a cold tier. Under a strictly iso-parameter setup, the collision-free design does not consistently improve validation loss.
  Through route-stratified evaluation (decomposing per-token loss into hot/cold contributions), we uncover a consistent "hot-to-cold advantage flip" during training: hot (high-frequency) positions initially have lower loss, but cold positions eventually surpass them. Crucially, collision-free configurations flip earlier than collision-prone baselines, suggesting that collisions act as implicit regularization. We also identify a gating mismatch: the gate learns to favor hot positions early in training, but this preference persists even after the flip, assigning higher weights to positions with higher loss.
  Our findings suggest that improving lookup precision alone does not guarantee better training outcomes. The dominant limitation may lie in gating credit assignment rather than index accuracy, and collision-induced noise may provide beneficial regularization that should not be naively eliminated.

</details>


### [44] [Understanding and Improving UMAP with Geometric and Topological Priors: The JORC-UMAP Algorithm](https://arxiv.org/abs/2601.16552)
*Xiaobin Li,Run Zhang*

Main category: cs.LG

TL;DR: JORC-UMAP：通过引入Ollivier-Ricci曲率和Jaccard相似性作为几何与拓扑先验，改进UMAP在非线性降维中的表现，减少拓扑撕裂和结构塌陷问题。


<details>
  <summary>Details</summary>
Motivation: UMAP作为广泛使用的非线性降维方法，其局部欧氏距离假设经常无法捕捉内在流形几何结构，导致拓扑撕裂和结构塌陷问题。作者发现UMAP对k近邻图的敏感性是主要原因。

Method: 提出JORC-UMAP方法：1）引入Ollivier-Ricci曲率作为几何先验，增强几何瓶颈处的边连接并减少冗余链接；2）由于曲率估计对噪声敏感，同时引入基于Jaccard相似性的拓扑先验，确保邻域一致性；3）结合这两种先验来更好地区分真实流形结构与虚假连接。

Result: 在合成和真实数据集上的实验表明，JORC-UMAP相比标准UMAP和其他降维方法，能更有效地减少撕裂和塌陷问题，通过SVM准确率和三元组保持分数等指标验证了其优越性，同时保持了计算效率。

Conclusion: JORC-UMAP为UMAP提供了一个几何感知的增强方法，能够实现更忠实的数据可视化，通过结合几何和拓扑先验来更好地捕捉数据的内在结构。

Abstract: Nonlinear dimensionality reduction techniques, particularly UMAP, are widely used for visualizing high-dimensional data. However, UMAP's local Euclidean distance assumption often fails to capture intrinsic manifold geometry, leading to topological tearing and structural collapse. We identify UMAP's sensitivity to the k-nearest neighbor graph as a key cause. To address this, we introduce Ollivier-Ricci curvature as a geometric prior, reinforcing edges at geometric bottlenecks and reducing redundant links. Since curvature estimation is noise-sensitive, we also incorporate a topological prior using Jaccard similarity to ensure neighborhood consistency. The resulting method, JORC-UMAP, better distinguishes true manifold structure from spurious connections. Experiments on synthetic and real-world datasets show that JORC-UMAP reduces tearing and collapse more effectively than standard UMAP and other DR methods, as measured by SVM accuracy and triplet preservation scores, while maintaining computational efficiency. This work offers a geometry-aware enhancement to UMAP for more faithful data visualization.

</details>


### [45] [Process-Tensor Tomography of SGD: Measuring Non-Markovian Memory via Back-Flow of Distinguishability](https://arxiv.org/abs/2601.16563)
*Vasileios Sevetlidis,George Pavlidis*

Main category: cs.LG

TL;DR: 提出将神经训练视为过程张量，引入基于可区分性回流的训练记忆见证，证明实际SGD偏离马尔可夫理想化，为优化器、课程和调度比较提供统一框架。


<details>
  <summary>Details</summary>
Motivation: 传统上将神经训练视为马尔可夫过程，但实际训练中存在记忆效应。需要一种原则性的诊断方法来量化训练记忆，证明SGD偏离马尔可夫理想化，并为优化器、课程和调度比较提供统一框架。

Method: 将训练建模为过程张量，引入基于可区分性回流的记忆见证。通过受控两步协议比较干预后的结果分布：单次干预与两次干预。当Δ_BF = D₂ - D₁ > 0时（D为TV/JS/Hellinger距离），证明非马尔可夫性。使用因果中断（重置优化器状态）验证记忆来源。

Result: 观察到一致的正向回流，具有紧密的自举置信区间。在更高动量、更大批次重叠和更多微步时放大，在因果中断时崩溃。见证对TV/JS/Hellinger距离稳健，计算成本低，无需架构修改。提供了训练记忆的经验证据。

Conclusion: 提出了一种原则性的训练记忆测量方法，证明实际SGD偏离马尔可夫理想化。框架为优化器、课程和调度比较提供了统一平台，将"数据顺序重要"转化为可测试的操作符。探索性案例展示了微观信号如何指导课程排序。

Abstract: This work proposes neural training as a \emph{process tensor}: a multi-time map that takes a sequence of controllable instruments (batch choices, augmentations, optimizer micro-steps) and returns an observable of the trained model. Building on this operational lens, we introduce a simple, model-agnostic witness of training memory based on \emph{back-flow of distinguishability}. In a controlled two-step protocol, we compare outcome distributions after one intervention versus two; the increase $Δ_{\mathrm{BF}} = D_2 - D_1>0$ (with $D\in\{\mathrm{TV}, \mathrm{JS}, \mathrm{H}\}$ measured on softmax predictions over a fixed probe set) certifies non-Markovianity. We observe consistent positive back-flow with tight bootstrap confidence intervals, amplification under higher momentum, larger batch overlap, and more micro-steps, and collapse under a \emph{causal break} (resetting optimizer state), directly attributing the effect to optimizer/data-state memory. The witness is robust across TV/JS/Hellinger, inexpensive to compute, and requires no architectural changes. We position this as a \emph{measurement} contribution: a principled diagnostic and empirical evidence that practical SGD deviates from the Markov idealization. An exploratory case study illustrates how the micro-level signal can inform curriculum orderings. "Data order matters" turns into a testable operator with confidence bounds, our framework offers a common stage to compare optimizers, curricula, and schedules through their induced training memory.

</details>


### [46] [Predicting Startup Success Using Large Language Models: A Novel In-Context Learning Approach](https://arxiv.org/abs/2601.16568)
*Abdurahman Maarouf,Alket Bakiaj,Stefan Feuerriegel*

Main category: cs.LG

TL;DR: 提出基于k近邻的上下文学习框架kNN-ICL，用于预测早期初创企业成功，无需模型训练，仅需少量标注数据，在数据稀缺的VC投资场景中表现优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 早期初创企业成功预测面临数据稀缺挑战，传统机器学习方法需要大量标注数据，而VC公司通常只有几十个早期初创企业的信息，这限制了传统方法的有效性。

Method: 提出kNN-ICL框架：基于k近邻的上下文学习方法，通过相似性选择最相关的历史初创企业作为示例，利用大型语言模型进行预测，无需模型训练。

Result: 使用Crunchbase真实数据，kNN-ICL方法比监督机器学习基线和普通上下文学习获得更高的预测准确率；仅需50个示例即可达到较高的平衡准确率。

Conclusion: 上下文学习可以作为VC公司在数据稀缺环境中的决策工具，kNN-ICL框架为早期初创企业成功预测提供了有效的解决方案。

Abstract: Venture capital (VC) investments in early-stage startups that end up being successful can yield high returns. However, predicting early-stage startup success remains challenging due to data scarcity (e.g., many VC firms have information about only a few dozen of early-stage startups and whether they were successful). This limits the effectiveness of traditional machine learning methods that rely on large labeled datasets for model training. To address this challenge, we propose an in-context learning framework for startup success prediction using large language models (LLMs) that requires no model training and leverages only a small set of labeled startups as demonstration examples. Specifically, we propose a novel k-nearest-neighbor-based in-context learning framework, called kNN-ICL, which selects the most relevant past startups as examples based on similarity. Using real-world profiles from Crunchbase, we find that the kNN-ICL approach achieves higher prediction accuracy than supervised machine learning baselines and vanilla in-context learning. Further, we study how performance varies with the number of in-context examples and find that a high balanced accuracy can be achieved with as few as 50 examples. Together, we demonstrate that in-context learning can serve as a decision-making tool for VC firms operating in data-scarce environments.

</details>


### [47] [Integrating Meteorological and Operational Data: A Novel Approach to Understanding Railway Delays in Finland](https://arxiv.org/abs/2601.16592)
*Vinicius Pozzobon Borin,Jean Michel de Souza Sant'Ana,Usama Raheel,Nurul Huda Mahmood*

Main category: cs.LG

TL;DR: 首个公开的芬兰铁路运营与气象观测数据集（2018-2024），整合了运营指标和209个气象站数据，包含3850万条观测记录，用于铁路延误预测和天气影响分析。


<details>
  <summary>Details</summary>
Motivation: 现有数据集很少整合气象信息与铁路运营数据，而天气（尤其在北欧地区）对铁路可靠性有重要影响。需要创建首个公开的综合数据集来支持铁路运营研究。

Method: 整合芬兰Digitraffic铁路交通服务的运营指标和209个环境监测站的气象观测数据，使用Haversine距离进行时空对齐。包含28个工程特征，采用空间回退算法处理缺失数据，时间特征循环编码，天气数据稳健缩放。

Result: 数据集包含约3850万条观测记录，覆盖芬兰5915公里铁路网络。分析显示冬季延误率超过25%，中部和北部芬兰存在高延误走廊的地理聚类。XGBoost回归基线实验的MAE为2.73分钟。

Conclusion: 该数据集为机器学习应用提供了灵活资源，支持列车延误预测、天气影响评估和基础设施脆弱性映射等多种应用，填补了铁路运营研究中综合气象-运营数据集的空白。

Abstract: Train delays result from complex interactions between operational, technical, and environmental factors. While weather impacts railway reliability, particularly in Nordic regions, existing datasets rarely integrate meteorological information with operational train data. This study presents the first publicly available dataset combining Finnish railway operations with synchronized meteorological observations from 2018-2024. The dataset integrates operational metrics from Finland Digitraffic Railway Traffic Service with weather measurements from 209 environmental monitoring stations, using spatial-temporal alignment via Haversine distance. It encompasses 28 engineered features across operational variables and meteorological measurements, covering approximately 38.5 million observations from Finland's 5,915-kilometer rail network. Preprocessing includes strategic missing data handling through spatial fallback algorithms, cyclical encoding of temporal features, and robust scaling of weather data to address sensor outliers. Analysis reveals distinct seasonal patterns, with winter months exhibiting delay rates exceeding 25\% and geographic clustering of high-delay corridors in central and northern Finland. Furthermore, the work demonstrates applications of the data set in analysing the reliability of railway traffic in Finland. A baseline experiment using XGBoost regression achieved a Mean Absolute Error of 2.73 minutes for predicting station-specific delays, demonstrating the dataset's utility for machine learning applications. The dataset enables diverse applications, including train delay prediction, weather impact assessment, and infrastructure vulnerability mapping, providing researchers with a flexible resource for machine learning applications in railway operations research.

</details>


### [48] [E2Former-V2: On-the-Fly Equivariant Attention with Linear Activation Memory](https://arxiv.org/abs/2601.16622)
*Lin Huang,Chengxiang Huang,Ziang Wang,Yiyue Du,Chu Wang,Haocheng Lu,Yunyang Li,Xiaoli Liu,Arthur Jiang,Jia Zhang*

Main category: cs.LG

TL;DR: E2Former-V2通过代数稀疏化和硬件感知执行解决了等变图神经网络的可扩展性瓶颈，在保持预测性能的同时显著加速推理。


<details>
  <summary>Details</summary>
Motivation: 主流等变图神经网络架构在建模3D原子系统时面临可扩展性瓶颈，因为需要在每条边上显式构建几何特征或密集张量积，计算开销巨大。

Method: 提出等变轴对齐稀疏化(EAAS)，利用SO(3)→SO(2)基变换将密集张量收缩转化为稀疏奇偶重索引操作；在此基础上引入基于定制Triton内核的即时等变注意力机制，消除边张量物化并最大化SRAM利用率。

Result: 在SPICE和OMol25数据集上，E2Former-V2保持可比预测性能的同时显著加速推理，内核实现相比标准版本达到20倍的TFLOPS提升。

Conclusion: 通过代数稀疏化和硬件优化，大型等变变换器可以在广泛可用的GPU平台上高效训练，解决了等变图神经网络的可扩展性挑战。

Abstract: Equivariant Graph Neural Networks (EGNNs) have become a widely used approach for modeling 3D atomistic systems. However, mainstream architectures face critical scalability bottlenecks due to the explicit construction of geometric features or dense tensor products on \textit{every} edge. To overcome this, we introduce \textbf{E2Former-V2}, a scalable architecture that integrates algebraic sparsity with hardware-aware execution. We first propose \textbf{E}quivariant \textbf{A}xis-\textbf{A}ligned \textbf{S}parsification (EAAS). EAAS builds on Wigner-$6j$ convolution by exploiting an $\mathrm{SO}(3) \rightarrow \mathrm{SO}(2)$ change of basis to transform computationally expensive dense tensor contractions into efficient, sparse parity re-indexing operations. Building on this representation, we introduce \textbf{On-the-Fly Equivariant Attention}, a fully node-centric mechanism implemented via a custom fused Triton kernel. By eliminating materialized edge tensors and maximizing SRAM utilization, our kernel achieves a \textbf{20$\times$ improvement in TFLOPS} compared to standard implementations. Extensive experiments on the SPICE and OMol25 datasets demonstrate that E2Former-V2 maintains comparable predictive performance while notably accelerating inference. This work demonstrates that large equivariant transformers can be trained efficiently using widely accessible GPU platforms. The code is avalible at https://github.com/IQuestLab/UBio-MolFM/tree/e2formerv2.

</details>


### [49] [Dual-Prototype Disentanglement: A Context-Aware Enhancement Framework for Time Series Forecasting](https://arxiv.org/abs/2601.16632)
*Haonan Yang,Jianchao Tang,Zhuo Li*

Main category: cs.LG

TL;DR: DPAD是一个模型无关的辅助框架，通过双原型自适应解耦机制，使预测模型具备模式解耦和上下文感知能力，提升时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法虽然通过改进架构或引入增强策略来提升预测性能，但往往无法动态解耦和利用时间序列中复杂交织的时序模式，导致学习到静态、平均化的表征，缺乏上下文感知能力。

Method: 提出双原型自适应解耦框架（DPAD），包括：1）动态双原型库（DDP），包含具有强时序先验的通用模式库和动态记忆关键罕见事件的罕见模式库；2）双路径上下文感知路由机制（DPC），从DDP中选择性检索上下文特定的模式表征来增强输出；3）解耦引导损失（DGLoss），确保每个原型库专注于其指定角色同时保持全面覆盖。

Result: 综合实验表明，DPAD在各种真实世界基准测试中，能够一致地提升最先进模型的预测性能和可靠性。

Conclusion: DPAD是一个模型无关的辅助方法，通过模式解耦和上下文感知适应，有效解决了现有方法在动态处理复杂时序模式方面的不足，显著提升了时间序列预测的性能和可靠性。

Abstract: Time series forecasting has witnessed significant progress with deep learning. While prevailing approaches enhance forecasting performance by modifying architectures or introducing novel enhancement strategies, they often fail to dynamically disentangle and leverage the complex, intertwined temporal patterns inherent in time series, thus resulting in the learning of static, averaged representations that lack context-aware capabilities. To address this, we propose the Dual-Prototype Adaptive Disentanglement framework (DPAD), a model-agnostic auxiliary method that equips forecasting models with the ability of pattern disentanglement and context-aware adaptation. Specifically, we construct a Dynamic Dual-Prototype bank (DDP), comprising a common pattern bank with strong temporal priors to capture prevailing trend or seasonal patterns, and a rare pattern bank dynamically memorizing critical yet infrequent events, and then an Dual-Path Context-aware routing (DPC) mechanism is proposed to enhance outputs with selectively retrieved context-specific pattern representations from the DDP. Additionally, we introduce a Disentanglement-Guided Loss (DGLoss) to ensure that each prototype bank specializes in its designated role while maintaining comprehensive coverage. Comprehensive experiments demonstrate that DPAD consistently improves forecasting performance and reliability of state-of-the-art models across diverse real-world benchmarks.

</details>


### [50] [Provably Robust Bayesian Counterfactual Explanations under Model Changes](https://arxiv.org/abs/2601.16659)
*Jamie Duell,Xiuyi Fan*

Main category: cs.LG

TL;DR: 提出PSCE方法，生成具有概率安全保证的反事实解释，确保在模型更新时仍保持高预测置信度和低预测方差。


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器学习模型频繁更新，现有反事实解释容易失效或不可靠，需要一种能适应模型变化、提供可靠解释的方法。

Method: 基于贝叶斯原理的PSCE方法，生成δ-安全（高预测置信度）和ε-鲁棒（低预测方差）的反事实解释，通过不确定性感知约束集成到优化框架中。

Result: 在多个数据集上验证，相比现有贝叶斯反事实方法，PSCE生成的反事实解释更合理、更具区分度，且在模型变化下具有可证明的鲁棒性。

Conclusion: PSCE为反事实解释提供了形式化的概率安全保证，确保在模型更新时解释的可靠性和有效性，解决了现实部署中的关键挑战。

Abstract: Counterfactual explanations (CEs) offer interpretable insights into machine learning predictions by answering ``what if?" questions. However, in real-world settings where models are frequently updated, existing counterfactual explanations can quickly become invalid or unreliable. In this paper, we introduce Probabilistically Safe CEs (PSCE), a method for generating counterfactual explanations that are $δ$-safe, to ensure high predictive confidence, and $ε$-robust to ensure low predictive variance. Based on Bayesian principles, PSCE provides formal probabilistic guarantees for CEs under model changes which are adhered to in what we refer to as the $\langle δ, ε\rangle$-set. Uncertainty-aware constraints are integrated into our optimization framework and we validate our method empirically across diverse datasets. We compare our approach against state-of-the-art Bayesian CE methods, where PSCE produces counterfactual explanations that are not only more plausible and discriminative, but also provably robust under model change.

</details>


### [51] [Dynamic Expert-Guided Model Averaging for Causal Discovery](https://arxiv.org/abs/2601.16715)
*Adrick Tench,Thomas Demeester*

Main category: cs.LG

TL;DR: 提出一种利用动态专家知识（包括LLM）来集成多种因果发现算法的灵活模型平均方法，在干净和噪声数据上验证了效果，并分析了专家正确性程度和LLM在临床因果发现中的能力。


<details>
  <summary>Details</summary>
Motivation: 因果发现算法众多但缺乏明确最佳选择，而实际应用经常违反算法假设，需要依赖专家知识。集成方法自然成为实际应用的选择，但如何有效结合专家知识是关键挑战。

Method: 提出灵活的模型平均方法，利用动态请求的专家知识（包括LLM作为专家）来集成多样化的因果发现算法。方法能够处理不完美的专家知识。

Result: 实验证明该方法在干净和噪声数据上对不完美专家（如LLM）都有效。分析了不同专家正确性程度的影响，并评估了LLM在临床因果发现中的能力，为实践者提供了有价值的见解。

Conclusion: 提出的动态专家知识集成方法能够有效结合多种因果发现算法，即使在不完美专家知识的情况下也能取得良好效果，为临床因果发现提供了实用的解决方案。

Abstract: Understanding causal relationships is critical for healthcare. Accurate causal models provide a means to enhance the interpretability of predictive models, and furthermore a basis for counterfactual and interventional reasoning and the estimation of treatment effects. However, would-be practitioners of causal discovery face a dizzying array of algorithms without a clear best choice. This abundance of competitive algorithms makes ensembling a natural choice for practical applications. At the same time, real-world use cases frequently face challenges that violate the assumptions of common causal discovery algorithms, forcing heavy reliance on expert knowledge. Inspired by recent work on dynamically requested expert knowledge and LLMs as experts, we present a flexible model averaging method leveraging dynamically requested expert knowledge to ensemble a diverse array of causal discovery algorithms. Experiments demonstrate the efficacy of our method with imperfect experts such as LLMs on both clean and noisy data. We also analyze the impact of different degrees of expert correctness and assess the capabilities of LLMs for clinical causal discovery, providing valuable insights for practitioners.

</details>


### [52] [Sample-wise Constrained Learning via a Sequential Penalty Approach with Applications in Image Processing](https://arxiv.org/abs/2601.16812)
*Francesca Lanzillotta,Chiara Albisani,Davide Pucci,Daniele Baracchi,Alessandro Piva,Matteo Lapucci*

Main category: cs.LG

TL;DR: 提出一种基于序列惩罚方法的深度学习算法，能够将数据处理要求作为严格约束而非任意惩罚项，并在图像处理任务中验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 在许多学习任务中，对单个数据样本的处理要求应该被形式化为优化问题中的严格约束，而不是任意的惩罚项。当前方法往往使用惩罚项来处理这些要求，但严格约束更能准确反映实际需求。

Method: 采用序列惩罚方法（sequential penalty method）来处理约束，该方法允许在深度学习场景中适当处理约束条件。算法在深度学习场景的合理假设下具有收敛保证。

Result: 在图像处理任务上的实验结果表明，该方法在实践中确实可行。算法在深度学习场景下具有收敛性保证。

Conclusion: 序列惩罚方法能够有效处理深度学习中的约束优化问题，将数据处理要求作为严格约束而非惩罚项，在实际应用中具有可行性。

Abstract: In many learning tasks, certain requirements on the processing of individual data samples should arguably be formalized as strict constraints in the underlying optimization problem, rather than by means of arbitrary penalties. We show that, in these scenarios, learning can be carried out exploiting a sequential penalty method that allows to properly deal with constraints. The proposed algorithm is shown to possess convergence guarantees under assumptions that are reasonable in deep learning scenarios. Moreover, the results of experiments on image processing tasks show that the method is indeed viable to be used in practice.

</details>


### [53] [Uncertainty propagation through trained multi-layer perceptrons: Exact analytical results](https://arxiv.org/abs/2601.16830)
*Andrew Thompson,Miles McCrory*

Main category: cs.LG

TL;DR: 提出多层感知机（MLP）在输入为多元高斯分布时输出均值和方差的精确解析表达式，无需级数展开


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用级数展开来近似MLP输出的不确定性传播，但缺乏精确的解析表达式。本文旨在为具有单隐藏层和ReLU激活函数的MLP提供输入为多元高斯分布时输出不确定性的精确分析结果。

Method: 针对单隐藏层、ReLU激活函数的MLP，在输入服从多元高斯分布的条件下，推导输出均值和方差的精确数学表达式，不依赖级数展开近似。

Result: 获得了MLP输出均值和方差的精确解析表达式，能够准确计算输入不确定性通过神经网络传播后的输出统计特性。

Conclusion: 为MLP的不确定性传播提供了精确的数学分析框架，相比基于级数展开的近似方法具有更高的准确性，为神经网络可靠性分析提供了理论基础。

Abstract: We give analytical results for propagation of uncertainty through trained multi-layer perceptrons (MLPs) with a single hidden layer and ReLU activation functions. More precisely, we give expressions for the mean and variance of the output when the input is multivariate Gaussian. In contrast to previous results, we obtain exact expressions without resort to a series expansion.

</details>


### [54] [Calibrated Probabilistic Interpolation for GEDI Biomass](https://arxiv.org/abs/2601.16834)
*Robin Young,Srinivasan Keshav*

Main category: cs.LG

TL;DR: 该论文提出了Attentive Neural Processes（ANPs）方法，用于从NASA GEDI任务的稀疏LiDAR观测中可靠地绘制墙到墙的生物量地图，解决了传统机器学习方法在异质景观中不确定性校准不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法（如随机森林和XGBoost）在处理GEDI稀疏LiDAR观测时，将空间预测视为独立任务，无法适应异质景观的复杂性，导致预测区间校准失败。主要问题在于混淆了集成方差与偶然不确定性，并忽略了局部空间上下文。

Method: 提出了Attentive Neural Processes（ANPs），这是一个概率元学习框架，通过显式地将预测条件化于局部观测集和地理空间基础模型嵌入。与静态集成不同，ANPs学习灵活的空间协方差函数，使不确定性估计在复杂景观中扩大，在均匀区域收缩。

Result: 在从热带亚马逊森林到北方和高山生态系统的五个不同生物群落中验证了该方法，ANPs实现了竞争性精度，同时保持了接近理想的不确定性校准。通过少样本适应展示了操作实用性，模型使用最少本地数据即可恢复跨区域转移的大部分性能差距。

Conclusion: 该工作为大陆尺度地球观测提供了可扩展、理论严谨的替代方案，超越了传统的集成方差方法，能够可靠地处理异质景观中的不确定性估计问题。

Abstract: Reliable wall-to-wall biomass mapping from NASA's GEDI mission requires interpolating sparse LiDAR observations across heterogeneous landscapes. While machine learning approaches like Random Forest and XGBoost are standard for this task, they treat spatial predictions of GEDI observations from multispectral or SAR remote sensing data as independent without adapting to the varying difficulty of heterogeneous landscapes. We demonstrate these approaches generally fail to produce calibrated prediction intervals. We identify that this stems from conflating ensemble variance with aleatoric uncertainty and ignoring local spatial context.
  To resolve this, we introduce Attentive Neural Processes (ANPs), a probabilistic meta-learning framework that explicitly conditions predictions on local observation sets and geospatial foundation model embeddings. Unlike static ensembles, ANPs learn a flexible spatial covariance function, allowing uncertainty estimates to expand in complex landscapes and contract in homogeneous areas. We validate this approach across five distinct biomes ranging from Tropical Amazonian forests to Boreal and Alpine ecosystems, demonstrating that ANPs achieve competitive accuracy while maintaining near-ideal uncertainty calibration. We demonstrate the operational utility of the method through few-shot adaptation, where the model recovers most of the performance gap in cross-region transfer using minimal local data. This work provides a scalable, theoretically rigorous alternative to ensemble variance for continental scale earth observation.

</details>


### [55] [The Art of Being Difficult: Combining Human and AI Strengths to Find Adversarial Instances for Heuristics](https://arxiv.org/abs/2601.16849)
*Henri Nikoleit,Ankit Anand,Anurag Murty Naredla,Heiko Röglin*

Main category: cs.LG

TL;DR: 本文展示了人类与LLM协作在解决理论计算机科学开放问题中的强大能力，通过改进FunSearch算法的输出来获得组合优化问题中标准启发式算法的最新下界。


<details>
  <summary>Details</summary>
Motivation: 尽管FunSearch等LLM进化方法能提供初始模式，但需要人类专业知识将这些模式转化为数学上严谨且有洞察力的构造，以突破长期存在的理论障碍。

Method: 通过迭代改进FunSearch算法的输出，针对分层k-median聚类、装箱问题、背包问题和Lovász汽油问题的推广，生成这些启发式算法表现不佳的对抗性实例。

Result: 获得了标准启发式算法的最新下界，其中一些问题的构造在过去十多年中几乎没有改进，尽管期间有间断的关注。

Conclusion: LLM是数学和计算机科学研究中的强大协作工具，虽然LLM提供关键初始模式，但人类专业知识对于将这些模式转化为数学上严谨且有洞察力的构造至关重要。

Abstract: We demonstrate the power of human-LLM collaboration in tackling open problems in theoretical computer science. Focusing on combinatorial optimization, we refine outputs from the FunSearch algorithm [Romera-Paredes et al., Nature 2023] to derive state-of-the-art lower bounds for standard heuristics. Specifically, we target the generation of adversarial instances where these heuristics perform poorly. By iterating on FunSearch's outputs, we identify improved constructions for hierarchical $k$-median clustering, bin packing, the knapsack problem, and a generalization of Lovász's gasoline problem - some of these have not seen much improvement for over a decade, despite intermittent attention. These results illustrate how expert oversight can effectively extrapolate algorithmic insights from LLM-based evolutionary methods to break long-standing barriers.
  Our findings demonstrate that while LLMs provide critical initial patterns, human expertise is essential for transforming these patterns into mathematically rigorous and insightful constructions. This work highlights that LLMs are a strong collaborative tool in mathematics and computer science research.

</details>


### [56] [Provably Learning Attention with Queries](https://arxiv.org/abs/2601.16873)
*Satwik Bhattamishra,Kulin Shah,Michael Hahn,Varun Kanade*

Main category: cs.LG

TL;DR: 本文研究了通过黑盒访问输出学习基于Transformer的序列模型的问题，提出了单头注意力参数学习算法，并分析了多头注意力的不可识别性问题。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过黑盒查询（仅能观察模型输出）来学习Transformer序列模型的参数，这对于理解模型内部机制、模型压缩和可解释性具有重要意义。

Method: 1. 对于单头softmax注意力回归器，提出了O(d²)查询的基本算法；2. 在头维度r≪d时，通过压缩感知技术提出了O(rd)查询的随机算法；3. 分析了噪声鲁棒性，证明在温和条件下可用多项式查询实现ε精度估计；4. 研究了多头注意力的不可识别性问题。

Result: 1. 单头注意力参数可通过O(d²)查询精确学习；2. 当r≪d时，可通过O(rd)查询学习；3. 在噪声条件下仍可实现多项式查询的ε精度估计；4. 多头注意力参数从值查询中一般不可识别，需要额外结构假设。

Conclusion: 单头注意力模型可以通过高效的黑盒查询算法学习，而多头注意力由于参数不可识别性需要额外假设。该研究为通过输出查询理解Transformer内部机制提供了理论基础。

Abstract: We study the problem of learning Transformer-based sequence models with black-box access to their outputs. In this setting, a learner may adaptively query the oracle with any sequence of vectors and observe the corresponding real-valued output. We begin with the simplest case, a single-head softmax-attention regressor. We show that for a model with width $d$, there is an elementary algorithm to learn the parameters of single-head attention exactly with $O(d^2)$ queries. Further, we show that if there exists an algorithm to learn ReLU feedforward networks (FFNs), then the single-head algorithm can be easily adapted to learn one-layer Transformers with single-head attention. Next, motivated by the regime where the head dimension $r \ll d$, we provide a randomised algorithm that learns single-head attention-based models with $O(rd)$ queries via compressed sensing arguments. We also study robustness to noisy oracle access, proving that under mild norm and margin conditions, the parameters can be estimated to $\varepsilon$ accuracy with a polynomial number of queries even when outputs are only provided up to additive tolerance. Finally, we show that multi-head attention parameters are not identifiable from value queries in general -- distinct parameterisations can induce the same input-output map. Hence, guarantees analogous to the single-head setting are impossible without additional structural assumptions.

</details>


### [57] [Theory of Minimal Weight Perturbations in Deep Networks and its Applications for Low-Rank Activated Backdoor Attacks](https://arxiv.org/abs/2601.16880)
*Bethan Evans,Jared Tanner*

Main category: cs.LG

TL;DR: 论文推导了深度神经网络实现指定输出变化所需的最小范数权重扰动，分析了影响因素，并应用于后门攻击的防御和激活研究。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络对权重扰动的敏感性，为理解模型鲁棒性提供理论框架，并应用于后门攻击的检测和防御。

Method: 推导单层精确公式计算最小范数权重扰动，对比多层Lipschitz常数鲁棒性保证，应用于精度修改激活的后门攻击分析。

Result: 单层精确公式与多层Lipschitz保证具有相同数量级，建立了后门攻击无法成功的可证明压缩阈值，低秩压缩可可靠激活潜在后门同时保持全精度准确率。

Conclusion: 反向传播的边界控制层间敏感性，为与期望输出变化一致的最小参数更新提供可证明保证，为后门攻击防御提供理论依据。

Abstract: The minimal norm weight perturbations of DNNs required to achieve a specified change in output are derived and the factors determining its size are discussed. These single-layer exact formulae are contrasted with more generic multi-layer Lipschitz constant based robustness guarantees; both are observed to be of the same order which indicates similar efficacy in their guarantees. These results are applied to precision-modification-activated backdoor attacks, establishing provable compression thresholds below which such attacks cannot succeed, and show empirically that low-rank compression can reliably activate latent backdoors while preserving full-precision accuracy. These expressions reveal how back-propagated margins govern layer-wise sensitivity and provide certifiable guarantees on the smallest parameter updates consistent with a desired output shift.

</details>


### [58] [Embedding -based Crop Type Classification in the Groundnut Basin of Senegal](https://arxiv.org/abs/2601.16900)
*Madeline C. Lisaius,Srinivasan Keshav,Andrew Blake,Clement Atzberger*

Main category: cs.LG

TL;DR: 评估地理空间基础模型嵌入方法在小农地区作物类型制图中的应用，发现TESSERA方法在性能、合理性、可迁移性和可访问性方面表现最佳


<details>
  <summary>Details</summary>
Motivation: 卫星遥感作物类型图对全球小农地区的粮食安全、生计支持和气候变化减缓很重要，但现有卫星方法不太适合小农条件，需要开发更适用的方法

Method: 建立四部分标准（性能、合理性、可迁移性、可访问性），评估TESSERA和AlphaEarth地理空间基础模型嵌入方法，与现有基线方法在塞内加尔花生盆地地区进行比较

Result: TESSERA方法在四个标准中表现最佳，在一个时间迁移示例中比次优方法准确率高28%，表明TESSERA嵌入在塞内加尔作物类型分类和制图任务中有效

Conclusion: TESSERA嵌入方法是小农地区作物类型制图的有效方法，满足实用嵌入方法的所有标准，在塞内加尔地区表现优异

Abstract: Crop type maps from satellite remote sensing are important tools for food security, local livelihood support and climate change mitigation in smallholder regions of the world, but most satellite-based methods are not well suited to smallholder conditions. To address this gap, we establish a four-part criteria for a useful embedding-based approach consisting of 1) performance, 2) plausibility, 3) transferability and 4) accessibility and evaluate geospatial foundation model (FM) embeddings -based approaches using TESSERA and AlphaEarth against current baseline methods for a region in the groundnut basin of Senegal. We find that the TESSERA -based approach to land cover and crop type mapping fulfills the selection criteria best, and in one temporal transfer example shows 28% higher accuracy compared to the next best method. These results indicate that TESSERA embeddings are an effective approach for crop type classification and mapping tasks in Senegal.

</details>


### [59] [GRIP: Algorithm-Agnostic Machine Unlearning for Mixture-of-Experts via Geometric Router Constraints](https://arxiv.org/abs/2601.16905)
*Andy Zhu,Rongzhe Wei,Yupu Gu,Pan Li*

Main category: cs.LG

TL;DR: GRIP框架通过几何约束保护MoE模型的路由稳定性，防止传统遗忘方法利用路由器漏洞进行表面遗忘，确保知识真正从专家参数中擦除。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法无法有效应用于MoE架构，因为它们会利用MoE的路由器漏洞——通过操纵路由器将查询从知识专家处重定向，而不是真正擦除知识，导致模型效用损失和表面遗忘。

Method: 提出几何路由不变性保持（GRIP）框架，通过将路由器梯度更新投影到专家特定零空间来实现几何约束。这解耦了路由稳定性与参数刚性：离散专家选择保持稳定以保留知识，而连续路由器参数在零空间内保持可塑性，允许模型进行必要的内部重构以满足遗忘目标。

Result: 在大规模MoE模型上的实验表明，GRIP适配器消除了专家选择偏移（实现超过95%的路由稳定性），同时保持了所有测试遗忘方法的效用。通过防止现有算法利用MoE模型的路由器漏洞，GRIP将现有遗忘研究从密集架构适配到MoE。

Conclusion: GRIP作为一个算法无关的适配器框架，通过几何约束保护MoE模型的路由稳定性，强制遗忘优化直接擦除专家参数中的知识，而不是利用路由器操纵的捷径，从而实现了有效的MoE模型遗忘。

Abstract: Machine unlearning (MU) for large language models has become critical for AI safety, yet existing methods fail to generalize to Mixture-of-Experts (MoE) architectures. We identify that traditional unlearning methods exploit MoE's architectural vulnerability: they manipulate routers to redirect queries away from knowledgeable experts rather than erasing knowledge, causing a loss of model utility and superficial forgetting. We propose Geometric Routing Invariance Preservation (GRIP), an algorithm-agnostic framework for unlearning for MoE. Our core contribution is a geometric constraint, implemented by projecting router gradient updates into an expert-specific null-space. Crucially, this decouples routing stability from parameter rigidity: while discrete expert selections remain stable for retained knowledge, the continuous router parameters remain plastic within the null space, allowing the model to undergo necessary internal reconfiguration to satisfy unlearning objectives. This forces the unlearning optimization to erase knowledge directly from expert parameters rather than exploiting the superficial router manipulation shortcut. GRIP functions as an adapter, constraining router parameter updates without modifying the underlying unlearning algorithm. Extensive experiments on large-scale MoE models demonstrate that our adapter eliminates expert selection shift (achieving over 95% routing stability) across all tested unlearning methods while preserving their utility. By preventing existing algorithms from exploiting MoE model's router vulnerability, GRIP adapts existing unlearning research from dense architectures to MoEs.

</details>


### [60] [The Trajectory Alignment Coefficient in Two Acts: From Reward Tuning to Reward Learning](https://arxiv.org/abs/2601.16906)
*Calarina Muslimani,Yunshu Du,Kenta Kawamoto,Kaushik Subramanian,Peter Stone,Peter Wurman*

Main category: cs.LG

TL;DR: 本文提出TAC作为奖励函数设计的评估指标，既能指导人工调优，又能作为可微损失函数Soft-TAC直接学习奖励模型，在复杂领域实现更准确的偏好对齐。


<details>
  <summary>Details</summary>
Motivation: 强化学习的成功依赖于准确反映任务目标的奖励函数，但人工设计奖励函数耗时且容易出错。需要工具来支持RL从业者指定合适的奖励权重，并探索直接从人类偏好数据学习奖励模型的方法。

Method: 1) 提出轨迹对齐系数(TAC)评估奖励函数与专家偏好的匹配程度；2) 进行人因研究验证TAC在奖励调优中的有效性；3) 提出可微近似Soft-TAC作为损失函数，直接从人类偏好数据训练奖励模型。

Result: 人因研究表明，使用TAC指导奖励调优能产生性能更好的奖励函数并降低认知负荷。在Gran Turismo 7中，使用Soft-TAC训练的奖励模型能捕捉特定偏好目标，产生比标准交叉熵损失更明显的行为差异。

Conclusion: TAC既能作为指导奖励调优的实用工具，又能作为复杂领域中奖励学习的目标函数，为RL奖励设计提供了有效的双重解决方案。

Abstract: The success of reinforcement learning (RL) is fundamentally tied to having a reward function that accurately reflects the task objective. Yet, designing reward functions is notoriously time-consuming and prone to misspecification. To address this issue, our first goal is to understand how to support RL practitioners in specifying appropriate weights for a reward function. We leverage the Trajectory Alignment Coefficient (TAC), a metric that evaluates how closely a reward function's induced preferences match those of a domain expert. To evaluate whether TAC provides effective support in practice, we conducted a human-subject study in which RL practitioners tuned reward weights for Lunar Lander. We found that providing TAC during reward tuning led participants to produce more performant reward functions and report lower cognitive workload relative to standard tuning without TAC. However, the study also underscored that manual reward design, even with TAC, remains labor-intensive. This limitation motivated our second goal: to learn a reward model that maximizes TAC directly. Specifically, we propose Soft-TAC, a differentiable approximation of TAC that can be used as a loss function to train reward models from human preference data. Validated in the racing simulator Gran Turismo 7, reward models trained using Soft-TAC successfully captured preference-specific objectives, resulting in policies with qualitatively more distinct behaviors than models trained with standard Cross-Entropy loss. This work demonstrates that TAC can serve as both a practical tool for guiding reward tuning and a reward learning objective in complex domains.

</details>


### [61] [Calibrated Similarity for Reliable Geometric Analysis of Embedding Spaces](https://arxiv.org/abs/2601.16907)
*Nicolas Tacheny*

Main category: cs.LG

TL;DR: 提出一种基于等张回归的单调校准方法，解决预训练嵌入空间中余弦相似度的各向异性导致的绝对数值校准问题，同时保持排序相关性和局部稳定性。


<details>
  <summary>Details</summary>
Motivation: 预训练嵌入空间中的原始余弦相似度虽然与人类判断有强排序相关性，但由于各向异性导致绝对数值系统性校准错误：分数集中在狭窄的高相似度区间，限制了其作为定量度量的可解释性。

Method: 使用基于人类相似度判断训练的等张回归构建单调变换，实现近乎完美的校准，同时保持排序相关性和局部稳定性（在七种扰动类型上达到98%）。

Result: 该方法在保持余弦相似度排序性质的同时恢复了绝对数值的可解释性，所有基于排序的构造（角度排序、最近邻、阈值图和分位数决策）在此变换下保持不变。

Conclusion: 等张校准作为一种保序重参数化，能够解决余弦相似度的校准问题而不改变其几何结构，无需重新计算所有嵌入，为预训练嵌入相似度的实际应用提供了更好的可解释性。

Abstract: While raw cosine similarity in pretrained embedding spaces exhibits strong rank correlation with human judgments, anisotropy induces systematic miscalibration of absolute values: scores concentrate in a narrow high-similarity band regardless of actual semantic relatedness, limiting interpretability as a quantitative measure. Prior work addresses this by modifying the embedding space (whitening, contrastive fine tuning), but such transformations alter geometric structure and require recomputing all embeddings.
  Using isotonic regression trained on human similarity judgments, we construct a monotonic transformation that achieves near-perfect calibration while preserving rank correlation and local stability(98% across seven perturbation types). Our contribution is not to replace cosine similarity, but to restore interpretability of its absolute values through monotone calibration, without altering its ranking properties.
  We characterize isotonic calibration as an order-preserving reparameterization and prove that all order-based constructions (angular ordering, nearest neighbors, threshold graphs and quantile-based decisions) are invariant under this transformation.

</details>


### [62] [Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles](https://arxiv.org/abs/2601.16936)
*Anton Zamyatin,Patrick Indri,Sagar Malhotra,Thomas Gärtner*

Main category: cs.LG

TL;DR: BatchEnsemble在资源受限场景下声称能以低成本提供类似Deep Ensembles的不确定性估计，但研究发现其性能接近单模型基线，而非真正的集成方法


<details>
  <summary>Details</summary>
Motivation: 在资源受限和低延迟场景中，需要高效获取不确定性估计。Deep Ensembles能提供鲁棒的认知不确定性，但需要训练多个完整模型，成本高昂

Method: BatchEnsemble通过对共享基础网络应用学习的秩-1扰动，旨在以更低的参数和内存成本提供类似集成的认知不确定性估计

Result: 在CIFAR10/10C/SVHN上，BatchEnsemble不仅表现不如Deep Ensembles，在准确性、校准和分布外检测方面都接近单模型基线。在MNIST上的受控研究发现，其成员在函数和参数空间几乎相同，无法实现不同的预测模式

Conclusion: BatchEnsemble的行为更像单模型而非真正的集成方法，无法有效提供Deep Ensembles级别的认知不确定性估计

Abstract: In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network. We show that BatchEnsemble not only underperforms Deep Ensembles but closely tracks a single model baseline in terms of accuracy, calibration and out-of-distribution (OOD) detection on CIFAR10/10C/SVHN. A controlled study on MNIST finds members are near-identical in function and parameter space, indicating limited capacity to realize distinct predictive modes. Thus, BatchEnsemble behaves more like a single model than a true ensemble.

</details>


### [63] [3D Molecule Generation from Rigid Motifs via SE(3) Flows](https://arxiv.org/abs/2601.16955)
*Roman Poletukhin,Marcel Kollovieh,Eike Eberhard,Stephan Günnemann*

Main category: cs.LG

TL;DR: 该论文提出了一种基于刚性基元的三维分子生成方法，相比传统原子级方法，在生成步骤、表示压缩和原子稳定性方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统三维分子生成通常在原子级别进行，而分子图生成技术常使用片段作为结构单元。受蛋白质结构生成中框架方法的启发，作者希望将这种片段化思想扩展到三维分子生成，将分子视为刚性基元的集合。

Method: 将分子表示为刚性基元（刚性体基序）的集合，采用SE(3)-等变生成模型进行三维分子生成。该方法基于刚性基元而非单个原子进行建模。

Result: 在多个基准测试中达到或优于当前最先进方法，在GEOM-Drugs上表现出更好的原子稳定性。相比标准原子级方法，生成步骤减少2-10倍，分子表示压缩3.5倍。

Conclusion: 基于刚性基元的三维分子生成方法在效率、表示压缩和原子稳定性方面优于传统原子级方法，为分子设计提供了更高效的框架。

Abstract: Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation, we employ SE(3)-equivariant generative modelling for de novo 3D molecule generation from rigid motifs. In our evaluations, we observe comparable or superior results to state-of-the-art across benchmarks, surpassing it in atom stability on GEOM-Drugs, while yielding a 2x to 10x reduction in generation steps and offering 3.5x compression in molecular representations compared to the standard atom-based methods.

</details>


### [64] [Auto-Regressive Masked Diffusion Models](https://arxiv.org/abs/2601.16971)
*Mahdi Karami,Ali Ghodsi*

Main category: cs.LG

TL;DR: ARMD模型通过将掩码扩散过程重构为块级因果模型，统一了自回归模型的训练效率和扩散模型的并行生成能力，在语言建模基准上实现了最先进性能，显著减少了训练步骤。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型在语言建模中存在性能差距，需要更多训练迭代。作者希望结合自回归模型的训练效率和扩散模型的并行生成优势，缩小与自回归模型的性能差距。

Method: 将掩码扩散过程重构为块级因果模型，设计严格因果、置换等变架构，在单次并行前向传递中计算所有条件概率。采用渐进置换训练方案，支持跨步并行生成策略。

Result: 在标准语言建模基准上实现最先进性能，超越现有扩散基线，显著减少训练步骤，为并行文本生成设立新基准，有效弥合并行与顺序解码的性能差距。

Conclusion: ARMD模型成功统一了自回归和扩散方法的优势，在保持并行生成能力的同时实现了与自回归模型相当的性能，为高效语言建模提供了新方向。

Abstract: Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.

</details>


### [65] [Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection](https://arxiv.org/abs/2601.16976)
*Estela Sánchez-Carballo,Francisco M. Melgarejo-Meseguer,José Luis Rojo-Álvarez*

Main category: cs.LG

TL;DR: 本文提出使用潜在扩散模型(LDM)进行物联网入侵检测中的攻击数据增强，以解决类别不平衡问题，相比现有方法在样本保真度、多样性和计算效率方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 物联网入侵检测系统中，基于机器学习的检测器常因良性流量与攻击流量之间的严重类别不平衡而导致性能下降。现有数据增强方法通常依赖简单的过采样技术或生成模型，难以同时实现高样本保真度、多样性和计算效率。

Method: 提出使用潜在扩散模型(LDM)进行物联网攻击数据增强，并与最先进的基线方法进行全面比较。实验针对三种代表性物联网攻击类型（DDoS、Mirai和中间人攻击），评估下游IDS性能和内在生成质量，使用分布、依赖性和多样性指标。

Result: 使用LDM生成的样本平衡训练数据显著提高了IDS性能，DDoS和Mirai攻击的F1分数达到0.99，始终优于竞争方法。LDM有效保留了特征依赖性，生成多样化样本，采样时间比直接在数据空间操作的扩散模型减少约25%。

Conclusion: 潜在扩散模型是合成物联网攻击数据生成的有效且可扩展解决方案，能显著减轻物联网场景中基于机器学习的入侵检测系统的类别不平衡影响。

Abstract: Intrusion Detection Systems (IDSs) are a key component for protecting Internet of Things (IoT) environments. However, in Machine Learning-based (ML-based) IDSs, performance is often degraded by the strong class imbalance between benign and attack traffic. Although data augmentation has been widely explored to mitigate this issue, existing approaches typically rely on simple oversampling techniques or generative models that struggle to simultaneously achieve high sample fidelity, diversity, and computational efficiency. To address these limitations, we propose the use of a Latent Diffusion Model (LDM) for attack data augmentation in IoT intrusion detection and provide a comprehensive comparison against state-of-the-art baselines. Experiments were conducted on three representative IoT attack types, specifically Distributed Denial-of-Service (DDoS), Mirai, and Man-in-the-Middle, evaluating both downstream IDS performance and intrinsic generative quality using distributional, dependency-based, and diversity metrics. Results show that balancing the training data with LDM-generated samples substantially improves IDS performance, achieving F1-scores of up to 0.99 for DDoS and Mirai attacks and consistently outperforming competing methods. Additionally, quantitative and qualitative analyses demonstrate that LDMs effectively preserve feature dependencies while generating diverse samples and reduce sampling time by approximately 25\% compared to diffusion models operating directly in data space. These findings highlight latent diffusion as an effective and scalable solution for synthetic IoT attack data generation, substantially mitigating the impact of class imbalance in ML-based IDSs for IoT scenarios.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [66] [Distributional Computational Graphs: Error Bounds](https://arxiv.org/abs/2601.16250)
*Olof Hallqvist Elias,Michael Selby,Phillip Stanley-Marbell*

Main category: stat.ML

TL;DR: 研究分布计算图的离散化误差，建立基于Wasserstein-1距离的非渐近误差界


<details>
  <summary>Details</summary>
Motivation: 分布计算图（输入为概率分布而非点值）在实际计算中通常需要离散化近似，需要量化这种近似带来的误差

Method: 分析分布计算图的离散化误差，建立基于Wasserstein-1距离的非渐近误差界，不要求计算图的结构假设

Result: 建立了分布计算图离散化误差的理论界限，为使用有限近似评估连续概率分布提供了误差保证

Conclusion: 该框架为分布计算图的误差分析提供了理论基础，有助于在实际应用中控制离散化误差

Abstract: We study a general framework of distributional computational graphs: computational graphs whose inputs are probability distributions rather than point values. We analyze the discretization error that arises when these graphs are evaluated using finite approximations of continuous probability distributions. Such an approximation might be the result of representing a continuous real-valued distribution using a discrete representation or from constructing an empirical distribution from samples (or might be the output of another distributional computational graph). We establish non-asymptotic error bounds in terms of the Wasserstein-1 distance, without imposing structural assumptions on the computational graph.

</details>


### [67] [Perfect Clustering for Sparse Directed Stochastic Block Models](https://arxiv.org/abs/2601.16427)
*Behzad Aalipur,Yichen Qin*

Main category: stat.ML

TL;DR: 提出了一种完全非谱的两阶段方法，用于稀疏有向随机块模型中的社区检测，该方法在稀疏、不对称网络中避免了特征值/奇异值分解的限制，并首次为这类方法提供了精确恢复保证。


<details>
  <summary>Details</summary>
Motivation: 有向随机块模型中的精确恢复在稀疏、有向网络中研究不足，特别是当社区数量发散时。谱方法在不对称、低度网络中缺乏稳定性，现有非谱方法主要关注无向或密集网络。

Method: 提出完全非谱的两阶段方法：1）使用针对不对称设置定制的邻域平滑方案估计有向概率矩阵；2）对估计的行应用K-means聚类，避免在稀疏、不对称网络中使用特征值或奇异值分解。

Result: 获得了平滑估计器的均匀行向集中界，通过控制不对称邻域和分离入度/出度效应的新论证。在允许γ_n→0和K_n→∞的温和稀疏性和分离条件下，以概率趋于1实现所有社区标签的精确恢复。

Conclusion: 该方法在高度有向、稀疏和非对称块结构中表现可靠，在有向谱方法和基于分数的方法性能下降的机制中表现良好。这是首次为稀疏有向设置中的这类非谱邻域平滑方法提供精确恢复保证。

Abstract: Exact recovery in stochastic block models (SBMs) is well understood in undirected settings, but remains considerably less developed for directed and sparse networks, particularly when the number of communities diverges. Spectral methods for directed SBMs often lack stability in asymmetric, low-degree regimes, and existing non-spectral approaches focus primarily on undirected or dense settings.
  We propose a fully non-spectral, two-stage procedure for community detection in sparse directed SBMs with potentially growing numbers of communities. The method first estimates the directed probability matrix using a neighborhood-smoothing scheme tailored to the asymmetric setting, and then applies $K$-means clustering to the estimated rows, thereby avoiding the limitations of eigen- or singular value decompositions in sparse, asymmetric networks. Our main theoretical contribution is a uniform row-wise concentration bound for the smoothed estimator, obtained through new arguments that control asymmetric neighborhoods and separate in- and out-degree effects. These results imply the exact recovery of all community labels with probability tending to one, under mild sparsity and separation conditions that allow both $γ_n \to 0$ and $K_n \to \infty$.
  Simulation studies, including highly directed, sparse, and non-symmetric block structures, demonstrate that the proposed procedure performs reliably in regimes where directed spectral and score-based methods deteriorate. To the best of our knowledge, this provides the first exact recovery guarantee for this class of non-spectral, neighborhood-smoothing methods in the sparse, directed setting.

</details>


### [68] [Efficient Learning of Stationary Diffusions with Stein-type Discrepancies](https://arxiv.org/abs/2601.16597)
*Fabian Bleile,Sarah Lumpp,Mathias Drton*

Main category: stat.ML

TL;DR: 提出Stein-type KDS (SKDS)作为KDS的替代公式，用于学习平稳扩散过程，计算成本更低且保持可比精度


<details>
  <summary>Details</summary>
Motivation: 学习平稳扩散过程需要估计随机微分方程参数，使其平稳分布匹配目标分布。KDS通过再生核希尔伯特空间评估扩散生成器的期望来强制平稳性，但计算成本较高

Method: 利用KDS与Stein差异之间的连接，引入Stein-type KDS (SKDS)作为替代公式。证明SKDS趋近于零时能保证学习扩散的平稳分布与目标对齐，且SKDS在广泛参数化下是凸的

Result: SKDS的经验版本以高概率是ε-拟凸的。实验表明，使用SKDS学习能达到与KDS相当的精度，同时显著降低计算成本，并在大多数竞争基线中表现更优

Conclusion: SKDS提供了KDS的有效替代方案，通过利用Stein差异的连接，在保持理论保证的同时显著降低了计算复杂度，为平稳扩散学习提供了更实用的工具

Abstract: Learning a stationary diffusion amounts to estimating the parameters of a stochastic differential equation whose stationary distribution matches a target distribution. We build on the recently introduced kernel deviation from stationarity (KDS), which enforces stationarity by evaluating expectations of the diffusion's generator in a reproducing kernel Hilbert space. Leveraging the connection between KDS and Stein discrepancies, we introduce the Stein-type KDS (SKDS) as an alternative formulation. We prove that a vanishing SKDS guarantees alignment of the learned diffusion's stationary distribution with the target. Furthermore, under broad parametrizations, SKDS is convex with an empirical version that is $ε$-quasiconvex with high probability. Empirically, learning with SKDS attains comparable accuracy to KDS while substantially reducing computational cost and yields improvements over the majority of competitive baselines.

</details>
