{"id": "2508.00110", "categories": ["stat.ML", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00110", "abs": "https://arxiv.org/abs/2508.00110", "authors": ["Katharine M. Clark", "Paul D. McNicholas"], "title": "funOCLUST: Clustering Functional Data with Outliers", "comment": null, "summary": "Functional data present unique challenges for clustering due to their\ninfinite-dimensional nature and potential sensitivity to outliers. An extension\nof the OCLUST algorithm to the functional setting is proposed to address these\nissues. The approach leverages the OCLUST framework, creating a robust method\nto cluster curves and trim outliers. The methodology is evaluated on both\nsimulated and real-world functional datasets, demonstrating strong performance\nin clustering and outlier identification.", "AI": {"tldr": "\u63d0\u51fa\u4e86OCLUST\u7b97\u6cd5\u7684\u529f\u80fd\u6570\u636e\u6269\u5c55\u7248\u672c\uff0c\u7528\u4e8e\u5904\u7406\u9ad8\u7ef4\u548c\u5f02\u5e38\u503c\u95ee\u9898\u3002", "motivation": "\u529f\u80fd\u6570\u636e\u7684\u65e0\u9650\u7ef4\u6027\u548c\u5bf9\u5f02\u5e38\u503c\u7684\u654f\u611f\u6027\u7ed9\u805a\u7c7b\u5e26\u6765\u6311\u6218\u3002", "method": "\u6269\u5c55OCLUST\u7b97\u6cd5\u6846\u67b6\uff0c\u521b\u5efa\u9c81\u68d2\u7684\u66f2\u7ebf\u805a\u7c7b\u548c\u5f02\u5e38\u503c\u4fee\u526a\u65b9\u6cd5\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u805a\u7c7b\u548c\u5f02\u5e38\u503c\u8bc6\u522b\u7684\u4f18\u5f02\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u529f\u80fd\u6570\u636e\u805a\u7c7b\u548c\u5f02\u5e38\u503c\u5904\u7406\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.00247", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.00247", "abs": "https://arxiv.org/abs/2508.00247", "authors": ["Sergei Gleyzer", "Hanh Nguyen", "Dinesh P. Ramakrishnan", "Eric A. F. Reinhardt"], "title": "Sinusoidal Approximation Theorem for Kolmogorov-Arnold Networks", "comment": "15 pages, 3 figures", "summary": "The Kolmogorov-Arnold representation theorem states that any continuous\nmultivariable function can be exactly represented as a finite superposition of\ncontinuous single variable functions. Subsequent simplifications of this\nrepresentation involve expressing these functions as parameterized sums of a\nsmaller number of unique monotonic functions. These developments led to the\nproof of the universal approximation capabilities of multilayer perceptron\nnetworks with sigmoidal activations, forming the alternative theoretical\ndirection of most modern neural networks.\n  Kolmogorov-Arnold Networks (KANs) have been recently proposed as an\nalternative to multilayer perceptrons. KANs feature learnable nonlinear\nactivations applied directly to input values, modeled as weighted sums of basis\nspline functions. This approach replaces the linear transformations and\nsigmoidal post-activations used in traditional perceptrons. Subsequent works\nhave explored alternatives to spline-based activations. In this work, we\npropose a novel KAN variant by replacing both the inner and outer functions in\nthe Kolmogorov-Arnold representation with weighted sinusoidal functions of\nlearnable frequencies. Inspired by simplifications introduced by Lorentz and\nSprecher, we fix the phases of the sinusoidal activations to linearly spaced\nconstant values and provide a proof of its theoretical validity. We also\nconduct numerical experiments to evaluate its performance on a range of\nmultivariable functions, comparing it with fixed-frequency Fourier transform\nmethods and multilayer perceptrons (MLPs). We show that it outperforms the\nfixed-frequency Fourier transform and achieves comparable performance to MLPs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Kolmogorov-Arnold\u7f51\u7edc\uff08KAN\uff09\u53d8\u4f53\uff0c\u7528\u53ef\u5b66\u4e60\u9891\u7387\u7684\u52a0\u6743\u6b63\u5f26\u51fd\u6570\u66ff\u6362\u539f\u59cb\u8868\u793a\u4e2d\u7684\u5185\u5916\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6027\u80fd\u4f18\u4e8e\u56fa\u5b9a\u9891\u7387\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u4e0e\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u76f8\u5f53\u3002", "motivation": "\u4f20\u7edf\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u7684\u6fc0\u6d3b\u51fd\u6570\u4e3a\u7ebf\u6027\u53d8\u6362\u548csigmoid\u51fd\u6570\uff0c\u800cKAN\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u6fc0\u6d3b\u51fd\u6570\uff08\u5982\u6837\u6761\u51fd\u6570\uff09\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u7684KAN\u53d8\u4f53\uff0c\u8fdb\u4e00\u6b65\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684KAN\u53d8\u4f53\uff0c\u7528\u52a0\u6743\u6b63\u5f26\u51fd\u6570\u66ff\u6362Kolmogorov-Arnold\u8868\u793a\u4e2d\u7684\u5185\u5916\u51fd\u6570\uff0c\u5e76\u56fa\u5b9a\u6b63\u5f26\u6fc0\u6d3b\u7684\u76f8\u4f4d\u4e3a\u7ebf\u6027\u95f4\u9694\u7684\u5e38\u6570\u503c\u3002\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u53d8\u4f53\u5728\u591a\u5143\u51fd\u6570\u903c\u8fd1\u4efb\u52a1\u4e2d\u4f18\u4e8e\u56fa\u5b9a\u9891\u7387\u5085\u91cc\u53f6\u53d8\u6362\u65b9\u6cd5\uff0c\u4e14\u6027\u80fd\u4e0eMLPs\u76f8\u5f53\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u52a0\u6743\u6b63\u5f26\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u672c\u6587\u63d0\u51fa\u7684KAN\u53d8\u4f53\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2507.22786", "categories": ["cs.LG", "quant-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.22786", "abs": "https://arxiv.org/abs/2507.22786", "authors": ["Adit Vishnu", "Abhay Shastry", "Dhruva Kashyap", "Chiranjib Bhattacharyya"], "title": "DO-EM: Density Operator Expectation Maximization", "comment": "Main text: 9 pages 1 Figure. Total: 23 pages 3 Figures", "summary": "Density operators, quantum generalizations of probability distributions, are\ngaining prominence in machine learning due to their foundational role in\nquantum computing. Generative modeling based on density operator models\n(\\textbf{DOMs}) is an emerging field, but existing training algorithms -- such\nas those for the Quantum Boltzmann Machine -- do not scale to real-world data,\nsuch as the MNIST dataset. The Expectation-Maximization algorithm has played a\nfundamental role in enabling scalable training of probabilistic latent variable\nmodels on real-world datasets. \\textit{In this paper, we develop an\nExpectation-Maximization framework to learn latent variable models defined\nthrough \\textbf{DOMs} on classical hardware, with resources comparable to those\nused for probabilistic models, while scaling to real-world data.} However,\ndesigning such an algorithm is nontrivial due to the absence of a well-defined\nquantum analogue to conditional probability, which complicates the Expectation\nstep. To overcome this, we reformulate the Expectation step as a quantum\ninformation projection (QIP) problem and show that the Petz Recovery Map\nprovides a solution under sufficient conditions. Using this formulation, we\nintroduce the Density Operator Expectation Maximization (DO-EM) algorithm -- an\niterative Minorant-Maximization procedure that optimizes a quantum evidence\nlower bound. We show that the \\textbf{DO-EM} algorithm ensures non-decreasing\nlog-likelihood across iterations for a broad class of models. Finally, we\npresent Quantum Interleaved Deep Boltzmann Machines (\\textbf{QiDBMs}), a\n\\textbf{DOM} that can be trained with the same resources as a DBM. When trained\nwith \\textbf{DO-EM} under Contrastive Divergence, a \\textbf{QiDBM} outperforms\nlarger classical DBMs in image generation on the MNIST dataset, achieving a\n40--60\\% reduction in the Fr\\'echet Inception Distance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bc6\u5ea6\u7b97\u5b50\u7684\u671f\u671b\u6700\u5927\u5316\u6846\u67b6\uff08DO-EM\uff09\uff0c\u7528\u4e8e\u5728\u7ecf\u5178\u786c\u4ef6\u4e0a\u8bad\u7ec3\u91cf\u5b50\u6f5c\u53d8\u91cf\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u6269\u5c55\u5230\u771f\u5b9e\u6570\u636e\u7684\u95ee\u9898\u3002\u901a\u8fc7\u91cf\u5b50\u4fe1\u606f\u6295\u5f71\uff08QIP\uff09\u91cd\u65b0\u5b9a\u4e49E\u6b65\u9aa4\uff0c\u5e76\u5f15\u5165QiDBM\u6a21\u578b\uff0c\u5728MNIST\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\u3002", "motivation": "\u5bc6\u5ea6\u7b97\u5b50\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u5177\u6709\u57fa\u7840\u6027\u4f5c\u7528\uff0c\u4f46\u73b0\u6709\u8bad\u7ec3\u7b97\u6cd5\uff08\u5982\u91cf\u5b50\u73bb\u5c14\u5179\u66fc\u673a\uff09\u65e0\u6cd5\u6269\u5c55\u5230\u771f\u5b9e\u6570\u636e\uff08\u5982MNIST\uff09\u3002\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u5728\u7ecf\u5178\u6f5c\u53d8\u91cf\u6a21\u578b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u91cf\u5b50\u6761\u4ef6\u6982\u7387\u7684\u660e\u786e\u5b9a\u4e49\uff0c\u963b\u788d\u4e86\u5176\u5728\u91cf\u5b50\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faDO-EM\u7b97\u6cd5\uff0c\u5c06E\u6b65\u9aa4\u91cd\u65b0\u5b9a\u4e49\u4e3a\u91cf\u5b50\u4fe1\u606f\u6295\u5f71\uff08QIP\uff09\u95ee\u9898\uff0c\u5229\u7528Petz\u6062\u590d\u6620\u5c04\u89e3\u51b3\u6761\u4ef6\u6982\u7387\u7f3a\u5931\u95ee\u9898\u3002\u5f15\u5165QiDBM\u6a21\u578b\uff0c\u7ed3\u5408\u5bf9\u6bd4\u6563\u5ea6\u8bad\u7ec3\u3002", "result": "DO-EM\u7b97\u6cd5\u786e\u4fdd\u4e86\u5bf9\u6570\u4f3c\u7136\u7684\u975e\u9012\u51cf\u6027\u3002QiDBM\u5728MNIST\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\uff0cFr\u00e9chet Inception Distance\u6bd4\u7ecf\u5178DBM\u964d\u4f4e\u4e8640-60%\u3002", "conclusion": "DO-EM\u6846\u67b6\u4e3a\u91cf\u5b50\u6f5c\u53d8\u91cf\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0cQiDBM\u5c55\u793a\u4e86\u91cf\u5b50\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00603", "categories": ["eess.SP", "cs.SY", "eess.AS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00603", "abs": "https://arxiv.org/abs/2508.00603", "authors": ["Hong-Cheng Liang", "Man-Wai Mak", "Kong Aik Lee"], "title": "Subband Architecture Aided Selective Fixed-Filter Active Noise Control", "comment": null, "summary": "The feedforward selective fixed-filter method selects the most suitable\npre-trained control filter based on the spectral features of the detected\nreference signal, effectively avoiding slow convergence in conventional\nadaptive algorithms. However, it can only handle limited types of noises, and\nthe performance degrades when the input noise exhibits non-uniform power\nspectral density. To address these limitations, this paper devises a novel\nselective fixed-filter scheme based on a delayless subband structure. In the\noff-line training stage, subband control filters are pre-trained for different\nfrequency ranges and stored in a dedicated sub-filter database. During the\non-line control stage, the incoming noise is decomposed using a polyphase FFT\nfilter bank, and a frequency-band-matching mechanism assigns each subband\nsignal the most appropriate control filter. Subsequently, a weight stacking\ntechnique is employed to combine all subband weights into a fullband filter,\nenabling real-time noise suppression. Experimental results demonstrate that the\nproposed scheme provides fast convergence, effective noise reduction, and\nstrong robustness in handling more complicated noisy environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5ef6\u8fdf\u5b50\u5e26\u7ed3\u6784\u7684\u65b0\u578b\u9009\u62e9\u6027\u56fa\u5b9a\u6ee4\u6ce2\u5668\u65b9\u6848\uff0c\u7528\u4e8e\u5904\u7406\u975e\u5747\u5300\u529f\u7387\u8c31\u5bc6\u5ea6\u7684\u566a\u58f0\uff0c\u63d0\u9ad8\u4e86\u566a\u58f0\u6291\u5236\u6548\u679c\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u524d\u9988\u9009\u62e9\u6027\u56fa\u5b9a\u6ee4\u6ce2\u5668\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u6709\u9650\u7c7b\u578b\u7684\u566a\u58f0\uff0c\u4e14\u5728\u8f93\u5165\u566a\u58f0\u529f\u7387\u8c31\u5bc6\u5ea6\u975e\u5747\u5300\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u5ef6\u8fdf\u5b50\u5e26\u7ed3\u6784\uff0c\u79bb\u7ebf\u8bad\u7ec3\u5b50\u5e26\u63a7\u5236\u6ee4\u6ce2\u5668\u5e76\u5b58\u50a8\uff1b\u5728\u7ebf\u63a7\u5236\u9636\u6bb5\u901a\u8fc7\u9891\u7387\u5e26\u5339\u914d\u673a\u5236\u5206\u914d\u6ee4\u6ce2\u5668\uff0c\u5e76\u4f7f\u7528\u6743\u91cd\u53e0\u52a0\u6280\u672f\u5408\u6210\u5168\u5e26\u6ee4\u6ce2\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6848\u5177\u6709\u5feb\u901f\u6536\u655b\u3001\u6709\u6548\u964d\u566a\u548c\u5f3a\u9c81\u68d2\u6027\u3002", "conclusion": "\u65b0\u65b9\u6848\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u566a\u58f0\u73af\u5883\u4e0b\u7684\u566a\u58f0\u6291\u5236\u6027\u80fd\u3002"}}
{"id": "2508.00040", "categories": ["cs.LG", "math.PR", "stat.AP", "stat.ML", "60J20, 68T07"], "pdf": "https://arxiv.org/pdf/2508.00040", "abs": "https://arxiv.org/abs/2508.00040", "authors": ["Abhinav Das", "Stephan Schl\u00fcter"], "title": "Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting", "comment": null, "summary": "This work integrates Bayesian regime detection with conditional neural\nprocesses for 24-hour electricity price prediction in the German market. Our\nmethodology integrates regime detection using a disentangled sticky\nhierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to\ndaily electricity prices. Each identified regime is subsequently modeled by an\nindependent conditional neural process (CNP), trained to learn localized\nmappings from input contexts to 24-dimensional hourly price trajectories, with\nfinal predictions computed as regime-weighted mixtures of these CNP outputs. We\nrigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated\nauto-regressive (LEAR) models by integrating their forecasts into diverse\nbattery storage optimization frameworks, including price arbitrage, risk\nmanagement, grid services, and cost minimization. This operational utility\nassessment revealed complex performance trade-offs: LEAR often yielded superior\nabsolute profits or lower costs, while DNN showed exceptional optimality in\nspecific cost-minimization contexts. Recognizing that raw prediction accuracy\ndoesn't always translate to optimal operational outcomes, we employed TOPSIS as\na comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified\nLEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model\nemerged as the most balanced and preferred solution for 2021, 2022 and 2023.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8d1d\u53f6\u65af\u673a\u5236\u68c0\u6d4b\u548c\u6761\u4ef6\u795e\u7ecf\u8fc7\u7a0b\u7684\u65b9\u6cd5\uff08R-NP\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u5fb7\u56fd\u7535\u529b\u5e02\u573a\u768424\u5c0f\u65f6\u7535\u4ef7\uff0c\u5e76\u901a\u8fc7\u591a\u6807\u51c6\u8bc4\u4f30\uff08TOPSIS\uff09\u9a8c\u8bc1\u5176\u7efc\u5408\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "motivation": "\u7535\u529b\u5e02\u573a\u7535\u4ef7\u9884\u6d4b\u5bf9\u7535\u6c60\u5b58\u50a8\u4f18\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\u5728\u9884\u6d4b\u7cbe\u5ea6\u548c\u5b9e\u9645\u5e94\u7528\u6548\u679c\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528DS-HDP-HMM\u8fdb\u884c\u673a\u5236\u68c0\u6d4b\uff0c\u6bcf\u4e2a\u673a\u5236\u7531\u72ec\u7acb\u7684CNP\u5efa\u6a21\uff0c\u6700\u7ec8\u9884\u6d4b\u4e3a\u673a\u5236\u52a0\u6743\u6df7\u5408\u3002", "result": "R-NP\u5728TOPSIS\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u5e73\u8861\uff0c\u4f18\u4e8eDNN\u548cLEAR\u6a21\u578b\u3002", "conclusion": "R-NP\u662f\u4e00\u79cd\u7efc\u5408\u6027\u80fd\u66f4\u4f18\u7684\u7535\u4ef7\u9884\u6d4b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.00692", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00692", "abs": "https://arxiv.org/abs/2508.00692", "authors": ["Young-ho Cho", "Hao Zhu", "Duehee Lee", "Ross Baldick"], "title": "Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network", "comment": null, "summary": "For conducting resource adequacy studies, we synthesize multiple long-term\nwind power scenarios of distributed wind farms simultaneously by using the\nspatio-temporal features: spatial and temporal correlation, waveforms, marginal\nand ramp rates distributions of waveform, power spectral densities, and\nstatistical characteristics. Generating the spatial correlation in scenarios\nrequires the design of common factors for neighboring wind farms and\nantithetical factors for distant wind farms. The generalized dynamic factor\nmodel (GDFM) can extract the common factors through cross spectral density\nanalysis, but it cannot closely imitate waveforms. The GAN can synthesize\nplausible samples representing the temporal correlation by verifying samples\nthrough a fake sample discriminator. To combine the advantages of GDFM and GAN,\nwe use the GAN to provide a filter that extracts dynamic factors with temporal\ninformation from the observation data, and we then apply this filter in the\nGDFM to represent both spatial and frequency correlations of plausible\nwaveforms. Numerical tests on the combination of GDFM and GAN have demonstrated\nperformance improvements over competing alternatives in synthesizing wind power\nscenarios from Australia, better realizing plausible statistical\ncharacteristics of actual wind power compared to alternatives such as the GDFM\nwith a filter synthesized from distributions of actual dynamic filters and the\nGAN with direct synthesis without dynamic factors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5e7f\u4e49\u52a8\u6001\u56e0\u5b50\u6a21\u578b\uff08GDFM\uff09\u548c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5408\u6210\u5206\u5e03\u5f0f\u98ce\u7535\u573a\u7684\u957f\u671f\u98ce\u7535\u529f\u7387\u573a\u666f\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u65f6\u7a7a\u76f8\u5173\u6027\u548c\u7edf\u8ba1\u7279\u6027\u3002", "motivation": "\u8d44\u6e90\u5145\u8db3\u6027\u7814\u7a76\u9700\u8981\u51c6\u786e\u7684\u98ce\u7535\u529f\u7387\u573a\u666f\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982GDFM\u6216GAN\u5355\u72ec\u4f7f\u7528\uff09\u5728\u6355\u6349\u65f6\u7a7a\u76f8\u5173\u6027\u548c\u6ce2\u5f62\u7279\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408GDFM\u548cGAN\uff0c\u5229\u7528GAN\u63d0\u53d6\u52a8\u6001\u56e0\u5b50\u5e76\u4f5c\u4e3aGDFM\u7684\u6ee4\u6ce2\u5668\uff0c\u4ee5\u540c\u65f6\u8868\u793a\u65f6\u7a7a\u548c\u9891\u7387\u76f8\u5173\u6027\u3002", "result": "\u6570\u503c\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6fb3\u5927\u5229\u4e9a\u98ce\u7535\u529f\u7387\u573a\u666f\u65f6\u4f18\u4e8e\u5176\u4ed6\u66ff\u4ee3\u65b9\u6cd5\uff0c\u80fd\u66f4\u597d\u5730\u6a21\u62df\u5b9e\u9645\u98ce\u7535\u7684\u7edf\u8ba1\u7279\u6027\u3002", "conclusion": "GDFM\u4e0eGAN\u7684\u7ed3\u5408\u5728\u98ce\u7535\u529f\u7387\u573a\u666f\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u5145\u8db3\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8f93\u5165\u3002"}}
{"id": "2508.00037", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00037", "abs": "https://arxiv.org/abs/2508.00037", "authors": ["Tong Nie", "Jian Sun", "Wei Ma"], "title": "Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion", "comment": "Accepted at IEEE Transactions on Industrial Informatics", "summary": "Networked urban systems facilitate the flow of people, resources, and\nservices, and are essential for economic and social interactions. These systems\noften involve complex processes with unknown governing rules, observed by\nsensor-based time series. To aid decision-making in industrial and engineering\ncontexts, data-driven predictive models are used to forecast spatiotemporal\ndynamics of urban systems. Current models such as graph neural networks have\nshown promise but face a trade-off between efficacy and efficiency due to\ncomputational demands. Hence, their applications in large-scale networks still\nrequire further efforts. This paper addresses this trade-off challenge by\ndrawing inspiration from physical laws to inform essential model designs that\nalign with fundamental principles and avoid architectural redundancy. By\nunderstanding both micro- and macro-processes, we present a principled\ninterpretable neural diffusion scheme based on Transformer-like structures\nwhose attention layers are induced by low-dimensional embeddings. The proposed\nscalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is\nvalidated on large-scale urban systems including traffic flow, solar power, and\nsmart meters, showing state-of-the-art performance and remarkable scalability.\nOur results constitute a fresh perspective on the dynamics prediction in\nlarge-scale urban networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5b9a\u5f8b\u542f\u53d1\u7684\u53ef\u6269\u5c55\u65f6\u7a7aTransformer\uff08ScaleSTF\uff09\uff0c\u7528\u4e8e\u9884\u6d4b\u5927\u89c4\u6a21\u57ce\u5e02\u7f51\u7edc\u7684\u52a8\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u6548\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u57ce\u5e02\u7f51\u7edc\u7cfb\u7edf\u6d89\u53ca\u590d\u6742\u4e14\u672a\u77e5\u7684\u89c4\u5219\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\uff08\u5982\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\u5728\u6548\u80fd\u4e0e\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7ed3\u6784\u7684\u53ef\u89e3\u91ca\u795e\u7ecf\u6269\u6563\u65b9\u6848\uff0c\u5176\u6ce8\u610f\u529b\u5c42\u7531\u4f4e\u7ef4\u5d4c\u5165\u8bf1\u5bfc\uff0c\u5177\u6709\u7ebf\u6027\u590d\u6742\u5ea6\u3002", "result": "ScaleSTF\u5728\u4ea4\u901a\u6d41\u91cf\u3001\u592a\u9633\u80fd\u53d1\u7535\u548c\u667a\u80fd\u7535\u8868\u7b49\u5927\u89c4\u6a21\u57ce\u5e02\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5148\u8fdb\u6027\u80fd\u548c\u5353\u8d8a\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5927\u89c4\u6a21\u57ce\u5e02\u7f51\u7edc\u52a8\u6001\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u7269\u7406\u5b9a\u5f8b\u542f\u53d1\u7684\u6a21\u578b\u8bbe\u8ba1\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00093", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00093", "abs": "https://arxiv.org/abs/2508.00093", "authors": ["Lucas Alves Zischler", "Chiara Lasagni", "Paolo Serena", "Alberto Bononi", "Giammarco Di Sciullo", "Divya A. Shaji", "Antonio Mecozzi", "Cristian Antonelli"], "title": "Closed-form Expression for the Power Profile in Wideband Systems with Inter-channel Stimulated Raman Scattering", "comment": "Submitted for the Journal of Lightwave Technology", "summary": "Wideband systems experience significant inter-channel stimulated Raman\nscattering (ISRS) and channel-dependent losses. Due to the non-uniform\nattenuation profile, the combined effects of ISRS and fiber loss can only be\naccurately estimated using numerical methods. In this work, we present an\napproximate closed-form expression for the channels' power profile accounting\nfor these combined effects. We validate the proposed expression against\nnumerical solutions in the case of CLU transmission, showing high accuracy for\nboth single-span and multi-span fiber-optic links. Additionally, we derive an\ninverse expression, formulated as a function of the output power, which can be\nutilized to target a desired optical signal-to-noise ratio (OSNR) profile\nthrough pre-emphasis of the launched channel powers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u4f3c\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u7528\u4e8e\u4f30\u8ba1\u5bbd\u5e26\u7cfb\u7edf\u4e2dISRS\u548c\u5149\u7ea4\u635f\u8017\u7684\u8054\u5408\u6548\u5e94\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u5bbd\u5e26\u7cfb\u7edf\u4e2dISRS\u548c\u901a\u9053\u76f8\u5173\u635f\u8017\u7684\u975e\u5747\u5300\u8870\u51cf\u7279\u6027\u9700\u8981\u7cbe\u786e\u4f30\u8ba1\uff0c\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u590d\u6742\u3002", "method": "\u63d0\u51fa\u95ed\u5f0f\u8868\u8fbe\u5f0f\u63cf\u8ff0\u901a\u9053\u529f\u7387\u5206\u5e03\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u89e3\u9a8c\u8bc1\u5176\u51c6\u786e\u6027\u3002", "result": "\u9a8c\u8bc1\u8868\u660e\u8be5\u8868\u8fbe\u5f0f\u5728\u5355\u8de8\u548c\u591a\u8de8\u5149\u7ea4\u94fe\u8def\u4e2d\u5747\u5177\u6709\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u63a8\u5bfc\u4e86\u9006\u8868\u8fbe\u5f0f\u7528\u4e8e\u76ee\u6807OSNR\u914d\u7f6e\u3002", "conclusion": "\u8be5\u95ed\u5f0f\u8868\u8fbe\u5f0f\u4e3a\u5bbd\u5e26\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u5de5\u5177\uff0c\u53ef\u7528\u4e8e\u4f18\u5316\u53d1\u5c04\u529f\u7387\u914d\u7f6e\u3002"}}
{"id": "2508.00180", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00180", "abs": "https://arxiv.org/abs/2508.00180", "authors": ["Adam Block", "Cyril Zhang"], "title": "EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes", "comment": null, "summary": "Stochasticity in language model fine-tuning, often caused by the small batch\nsizes typically used in this regime, can destabilize training by introducing\nlarge oscillations in generation quality. A popular approach to mitigating this\ninstability is to take an Exponential moving average (EMA) of weights\nthroughout training. While EMA reduces stochasticity, thereby smoothing\ntraining, the introduction of bias from old iterates often creates a lag in\noptimization relative to vanilla training. In this work, we propose the\nBias-Corrected Exponential Moving Average (BEMA), a simple and practical\naugmentation of EMA that retains variance-reduction benefits while eliminating\nbias. BEMA is motivated by a simple theoretical model wherein we demonstrate\nprovable acceleration of BEMA over both a standard EMA and vanilla training.\nThrough an extensive suite of experiments on Language Models, we show that BEMA\nleads to significantly improved convergence rates and final performance over\nboth EMA and vanilla training in a variety of standard LM benchmarks, making\nBEMA a practical and theoretically motivated intervention for more stable and\nefficient fine-tuning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBEMA\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6b63EMA\u4e2d\u7684\u504f\u5dee\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3EMA\u65b9\u6cd5\u5728\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u4e2d\u56e0\u65e7\u8fed\u4ee3\u5f15\u5165\u504f\u5dee\u800c\u5bfc\u81f4\u7684\u4f18\u5316\u6ede\u540e\u95ee\u9898\u3002", "method": "\u63d0\u51faBias-Corrected Exponential Moving Average (BEMA)\uff0c\u4fdd\u7559EMA\u7684\u65b9\u5dee\u51cf\u5c11\u4f18\u52bf\uff0c\u540c\u65f6\u6d88\u9664\u504f\u5dee\u3002", "result": "BEMA\u5728\u591a\u79cd\u6807\u51c6LM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8eEMA\u548c\u666e\u901a\u8bad\u7ec3\uff0c\u63d0\u5347\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "BEMA\u662f\u4e00\u79cd\u5b9e\u7528\u4e14\u7406\u8bba\u652f\u6301\u7684\u65b9\u6cd5\uff0c\u80fd\u66f4\u7a33\u5b9a\u9ad8\u6548\u5730\u8fdb\u884c\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u3002"}}
{"id": "2508.00039", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00039", "abs": "https://arxiv.org/abs/2508.00039", "authors": ["Kaustav Chatterjee", "Joshua Q. Li", "Fatemeh Ansari", "Masud Rana Munna", "Kundan Parajulee", "Jared Schwennesen"], "title": "Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings", "comment": null, "summary": "Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose\nsafety risks to highway vehicles due to potential hang-ups. These crossings\ntypically result from post-construction railway track maintenance activities or\nnon-compliance with design guidelines for HRGC vertical alignments.\nConventional methods for measuring HRGC profiles are costly, time-consuming,\ntraffic-disruptive, and present safety challenges. To address these issues,\nthis research employed advanced, cost-effective techniques and innovative\nmodeling approaches for HRGC profile measurement. A novel hybrid deep learning\nframework combining Long Short-Term Memory (LSTM) and Transformer architectures\nwas developed by utilizing instrumentation and ground truth data.\nInstrumentation data were gathered using a highway testing vehicle equipped\nwith Inertial Measurement Unit (IMU) and Global Positioning System (GPS)\nsensors, while ground truth data were obtained via an industrial-standard\nwalking profiler. Field data was collected at the Red Rock Railroad Corridor in\nOklahoma. Three advanced deep learning models Transformer-LSTM sequential\n(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel\n(model 3) were evaluated to identify the most efficient architecture. Models 2\nand 3 outperformed the others and were deployed to generate 2D/3D HRGC\nprofiles. The deep learning models demonstrated significant potential to\nenhance highway and railroad safety by enabling rapid and accurate assessment\nof HRGC hang-up susceptibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LSTM\u548cTransformer\u7684\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u6d4b\u91cf\u516c\u8def\u94c1\u8def\u5e73\u4ea4\u9053\u53e3\uff08HRGC\uff09\u7684\u5782\u76f4\u5256\u9762\uff0c\u4ee5\u51cf\u5c11\u4f20\u7edf\u65b9\u6cd5\u7684\u6210\u672c\u548c\u65f6\u95f4\u6d88\u8017\u3002", "motivation": "\u4f20\u7edfHRGC\u5256\u9762\u6d4b\u91cf\u65b9\u6cd5\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u5b89\u5168\u4e14\u7ecf\u6d4e\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cd\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08Transformer-LSTM\u3001LSTM-Transformer\u5e8f\u5217\u548c\u5e76\u884c\u6a21\u578b\uff09\uff0c\u5229\u7528IMU\u548cGPS\u4f20\u611f\u5668\u6570\u636e\u4e0e\u5730\u9762\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u6a21\u578b2\u548c3\u8868\u73b0\u6700\u4f73\uff0c\u80fd\u591f\u751f\u62102D/3D HRGC\u5256\u9762\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u91cf\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u4e3aHRGC\u7684\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u5feb\u901f\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u516c\u8def\u548c\u94c1\u8def\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2508.00274", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00274", "abs": "https://arxiv.org/abs/2508.00274", "authors": ["Yunfei Liu", "Mingxuan Liu", "Wupeng Xie", "Xinzhu Liu", "Wenxue Liu", "Yangang Sun", "Xin Qiu", "Cui Yuan", "Jinhai Li"], "title": "RIS-MAE: A Self-Supervised Modulation Classification Method Based on Raw IQ Signals and Masked Autoencoder", "comment": null, "summary": "Automatic modulation classification (AMC) is a basic technology in\nintelligent wireless communication systems. It is important for tasks such as\nspectrum monitoring, cognitive radio, and secure communications. In recent\nyears, deep learning methods have made great progress in AMC. However,\nmainstream methods still face two key problems. First, they often use\ntime-frequency images instead of raw signals. This causes loss of key\nmodulation features and reduces adaptability to different communication\nconditions. Second, most methods rely on supervised learning. This needs a\nlarge amount of labeled data, which is hard to get in real-world environments.\nTo solve these problems, we propose a self-supervised learning framework called\nRIS-MAE. RIS-MAE uses masked autoencoders to learn signal features from\nunlabeled data. It takes raw IQ sequences as input. By applying random masking\nand reconstruction, it captures important time-domain features such as\namplitude, phase, etc. This helps the model learn useful and transferable\nrepresentations. RIS-MAE is tested on four datasets. The results show that it\nperforms better than existing methods in few-shot and cross-domain tasks.\nNotably, it achieves high classification accuracy on previously unseen datasets\nwith only a small number of fine-tuning samples, confirming its generalization\nability and potential for real-world deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRIS-MAE\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5982\u7279\u5f81\u4e22\u5931\u548c\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u3002", "motivation": "\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\uff08AMC\uff09\u5728\u667a\u80fd\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u4e22\u5931\u548c\u5bf9\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u7684\u95ee\u9898\u3002", "method": "RIS-MAE\u4f7f\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\u4ece\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u5b66\u4e60\u4fe1\u53f7\u7279\u5f81\uff0c\u76f4\u63a5\u5904\u7406\u539f\u59cbIQ\u5e8f\u5217\uff0c\u901a\u8fc7\u968f\u673a\u63a9\u7801\u548c\u91cd\u5efa\u6355\u83b7\u65f6\u57df\u7279\u5f81\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0cRIS-MAE\u5728\u5c11\u6837\u672c\u548c\u8de8\u57df\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RIS-MAE\u4e3a\u89e3\u51b3AMC\u4e2d\u7684\u5173\u952e\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00264", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00264", "abs": "https://arxiv.org/abs/2508.00264", "authors": ["Jerry Huang", "Peng Lu", "Qiuhao Zeng"], "title": "Calibrated Language Models and How to Find Them with Label Smoothing", "comment": "Accepted to the Forty-second International Conference on Machine\n  Learning (ICML) 2025. First two authors contributed equally", "summary": "Recent advances in natural language processing (NLP) have opened up greater\nopportunities to enable fine-tuned large language models (LLMs) to behave as\nmore powerful interactive agents through improved instruction-following\nability. However, understanding how this impacts confidence calibration for\nreliable model output has not been researched in full. In this work, we examine\nvarious open-sourced LLMs, identifying significant calibration degradation\nafter instruction tuning in each. Seeking a practical solution, we look towards\nlabel smoothing, which has been shown as an effective method to regularize for\noverconfident predictions but has yet to be widely adopted in the supervised\nfine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing\nis sufficient to maintain calibration throughout the SFT process. However,\nsettings remain where the effectiveness of smoothing is severely diminished, in\nparticular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to\nstem from the ability to become over-confident, which has a direct relationship\nwith the hidden size and vocabulary size, and justify this theoretically and\nexperimentally. Finally, we address an outstanding issue regarding the memory\nfootprint of the cross-entropy loss computation in the label smoothed loss\nsetting, designing a customized kernel to dramatically reduce memory\nconsumption without sacrificing speed or performance in comparison to existing\nsolutions for non-smoothed losses.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6307\u4ee4\u8c03\u4f18\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u6807\u7b7e\u5e73\u6ed1\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u5185\u5b58\u6d88\u8017\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u6307\u4ee4\u8c03\u4f18\u63d0\u5347\u4e86LLM\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u4f46\u5176\u5bf9\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u5206\u6790\u4e86\u591a\u79cd\u5f00\u6e90LLM\uff0c\u63d0\u51fa\u6807\u7b7e\u5e73\u6ed1\u4f5c\u4e3a\u6821\u51c6\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5b9a\u5236\u5316\u5185\u6838\u4ee5\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "result": "\u6807\u7b7e\u5e73\u6ed1\u80fd\u6709\u6548\u7ef4\u6301\u6821\u51c6\uff0c\u4f46\u5728\u5927\u8bcd\u6c47\u91cfLLM\u4e2d\u6548\u679c\u53d7\u9650\uff0c\u7406\u8bba\u53ca\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4e0e\u9690\u85cf\u5c42\u548c\u8bcd\u6c47\u91cf\u7684\u5173\u7cfb\u3002", "conclusion": "\u6807\u7b7e\u5e73\u6ed1\u662f\u89e3\u51b3\u6821\u51c6\u95ee\u9898\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u5b9a\u5236\u5316\u5185\u6838\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u5185\u5b58\u6548\u7387\u3002"}}
{"id": "2508.00326", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00326", "abs": "https://arxiv.org/abs/2508.00326", "authors": ["Chengwang Ji", "Kehui Li", "Haiquan Lu", "Qiaoyan Peng", "Jintao Wang", "Shaodan Ma"], "title": "Model-Driven Deep Learning Enhanced Joint Beamforming and Mode Switching for RDARS-Aided MIMO Systems", "comment": null, "summary": "Reconfigurable distributed antenna and reflecting surface (RDARS) is a\npromising architecture for future sixth-generation (6G) wireless networks. In\nparticular, the dynamic working mode configuration for the RDARS-aided system\nbrings an extra selection gain compared to the existing reconfigurable\nintelligent surface (RIS)-aided system and distributed antenna system (DAS). In\nthis paper, we consider the RDARS-aided downlink multiple-input multiple-output\n(MIMO) system and aim to maximize the weighted sum rate (WSR) by jointly\noptimizing the beamforming matrices at the based station (BS) and RDARS, as\nwell as mode switching matrix at RDARS. The optimization problem is challenging\nto be solved due to the non-convex objective function and mixed integer binary\nconstraint. To this end, a penalty term-based weight minimum mean square error\n(PWM) algorithm is proposed by integrating the majorization-minimization (MM)\nand weight minimum mean square error (WMMSE) methods. To further escape the\nlocal optimum point in the PWM algorithm, a model-driven DL method is\nintegrated into this algorithm, where the key variables related to the\nconvergence of PWM algorithm are trained to accelerate the convergence speed\nand improve the system performance. Simulation results are provided to show\nthat the PWM-based beamforming network (PWM-BFNet) can reduce the number of\niterations by half and achieve performance improvements of 26.53% and 103.2% at\nthe scenarios of high total transmit power and a large number of RDARS transmit\nelements (TEs), respectively.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eRDARS\u67b6\u6784\u76846G\u65e0\u7ebf\u7f51\u7edc\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u57fa\u7ad9\u548cRDARS\u7684\u6ce2\u675f\u6210\u5f62\u77e9\u9635\u4ee5\u53caRDARS\u7684\u6a21\u5f0f\u5207\u6362\u77e9\u9635\uff0c\u6700\u5927\u5316\u52a0\u6743\u548c\u901f\u7387\uff08WSR\uff09\u3002\u91c7\u7528PWM\u7b97\u6cd5\u7ed3\u5408MM\u548cWMMSE\u65b9\u6cd5\u89e3\u51b3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u6a21\u578b\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u52a0\u901f\u6536\u655b\u3002", "motivation": "RDARS\u67b6\u6784\u57286G\u7f51\u7edc\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u52a8\u6001\u5de5\u4f5c\u6a21\u5f0f\u914d\u7f6e\u7684\u4f18\u5316\u95ee\u9898\u590d\u6742\u4e14\u975e\u51f8\uff0c\u9700\u8981\u9ad8\u6548\u7b97\u6cd5\u89e3\u51b3\u3002", "method": "\u63d0\u51faPWM\u7b97\u6cd5\uff0c\u7ed3\u5408MM\u548cWMMSE\u65b9\u6cd5\uff0c\u5e76\u96c6\u6210\u6a21\u578b\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u4ee5\u52a0\u901f\u6536\u655b\u3002", "result": "\u4eff\u771f\u663e\u793aPWM-BFNet\u7b97\u6cd5\u8fed\u4ee3\u6b21\u6570\u51cf\u534a\uff0c\u5728\u9ad8\u53d1\u5c04\u529f\u7387\u548c\u5927RDARS\u4f20\u8f93\u5355\u5143\u6570\u573a\u666f\u4e0b\u6027\u80fd\u5206\u522b\u63d0\u534726.53%\u548c103.2%\u3002", "conclusion": "PWM-BFNet\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86RDARS\u7cfb\u7edf\u7684\u4f18\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2508.00286", "categories": ["cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00286", "abs": "https://arxiv.org/abs/2508.00286", "authors": ["Mohsen Zaker Esteghamati"], "title": "Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem", "comment": null, "summary": "This study presents a methodology to treat performance-based seismic design\nas an inverse engineering problem, where design parameters are directly derived\nto achieve specific performance objectives. By implementing explainable machine\nlearning models, this methodology directly maps design variables and\nperformance metrics, tackling computational inefficiencies of performance-based\ndesign. The resultant machine learning model is integrated as an evaluation\nfunction into a genetic optimization algorithm to solve the inverse problem.\nThe developed methodology is then applied to two different inventories of steel\nand concrete moment frames in Los Angeles and Charleston to obtain sectional\nproperties of frame members that minimize expected annualized seismic loss in\nterms of repair costs. The results show high accuracy of the surrogate models\n(e.g., R2> 90%) across a diverse set of building types, geometries, seismic\ndesign, and site hazard, where the optimization algorithm could identify the\noptimum values of members' properties for a fixed set of geometric variables,\nconsistent with engineering principles.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6027\u80fd\u5bfc\u5411\u7684\u6297\u9707\u8bbe\u8ba1\u89c6\u4e3a\u9006\u5de5\u7a0b\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76f4\u63a5\u6620\u5c04\u8bbe\u8ba1\u53d8\u91cf\u4e0e\u6027\u80fd\u6307\u6807\uff0c\u89e3\u51b3\u4e86\u6027\u80fd\u5bfc\u5411\u8bbe\u8ba1\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6027\u80fd\u5bfc\u5411\u6297\u9707\u8bbe\u8ba1\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76f4\u63a5\u6620\u5c04\u8bbe\u8ba1\u53d8\u91cf\u4e0e\u6027\u80fd\u76ee\u6807\uff0c\u63d0\u9ad8\u8bbe\u8ba1\u6548\u7387\u3002", "method": "\u91c7\u7528\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4f5c\u4e3a\u8bc4\u4f30\u51fd\u6570\uff0c\u7ed3\u5408\u9057\u4f20\u4f18\u5316\u7b97\u6cd5\u89e3\u51b3\u9006\u5de5\u7a0b\u95ee\u9898\uff0c\u5e94\u7528\u4e8e\u6d1b\u6749\u77f6\u548c\u67e5\u5c14\u65af\u987f\u7684\u94a2\u548c\u6df7\u51dd\u571f\u6846\u67b6\u7ed3\u6784\u3002", "result": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\uff08R2>90%\uff09\uff0c\u4f18\u5316\u7b97\u6cd5\u80fd\u591f\u8bc6\u522b\u7b26\u5408\u5de5\u7a0b\u539f\u7406\u7684\u6784\u4ef6\u6700\u4f18\u5c5e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u4e0e\u4f18\u5316\u7b97\u6cd5\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6027\u80fd\u5bfc\u5411\u6297\u9707\u8bbe\u8ba1\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u5efa\u7b51\u7c7b\u578b\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.00041", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00041", "abs": "https://arxiv.org/abs/2508.00041", "authors": ["Yebo Wu", "Jingguang Li", "Zhijiang Guo", "Li Li"], "title": "Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages", "comment": null, "summary": "Federated fine-tuning enables Large Language Models (LLMs) to adapt to\ndownstream tasks while preserving data privacy, but its resource-intensive\nnature limits deployment on edge devices. In this paper, we introduce\nDevelopmental Federated Tuning (DevFT), a resource-efficient approach inspired\nby cognitive development that progressively builds a powerful LLM from a\ncompact foundation. DevFT decomposes the fine-tuning process into developmental\nstages, each optimizing submodels with increasing parameter capacity. Knowledge\nfrom earlier stages transfers to subsequent submodels, providing optimized\ninitialization parameters that prevent convergence to local minima and\naccelerate training. This paradigm mirrors human learning, gradually\nconstructing comprehensive knowledge structure while refining existing skills.\nTo efficiently build stage-specific submodels, DevFT introduces\ndeconfliction-guided layer grouping and differential-based layer fusion to\ndistill essential information and construct representative layers. Evaluations\nacross multiple benchmarks demonstrate that DevFT significantly outperforms\nstate-of-the-art methods, achieving up to 4.59$\\times$ faster convergence,\n10.67$\\times$ reduction in communication overhead, and 9.07% average\nperformance improvement, while maintaining compatibility with existing\napproaches.", "AI": {"tldr": "DevFT\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u8054\u90a6\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u6784\u5efaLLM\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5fae\u8c03\u4e2d\u8d44\u6e90\u5bc6\u96c6\u548c\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u5206\u9636\u6bb5\u5fae\u8c03\uff0c\u9010\u6b65\u589e\u52a0\u53c2\u6570\u5bb9\u91cf\uff0c\u901a\u8fc7\u77e5\u8bc6\u8f6c\u79fb\u548c\u4f18\u5316\u521d\u59cb\u5316\u53c2\u6570\u52a0\u901f\u8bad\u7ec3\u3002", "result": "\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb4.59\u500d\uff0c\u901a\u4fe1\u5f00\u9500\u51cf\u5c1110.67\u500d\uff0c\u6027\u80fd\u63d0\u53479.07%\u3002", "conclusion": "DevFT\u9ad8\u6548\u4e14\u517c\u5bb9\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u3002"}}
{"id": "2508.00409", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00409", "abs": "https://arxiv.org/abs/2508.00409", "authors": ["Mohammad Soleymani", "Ignacio Santamaria", "Eduard Jorswieck", "Robert Schober", "Lajos Hanzo"], "title": "STAR-RIS-aided RSMA for the URLLC multi-user MIMO Downlink", "comment": "Accepted at 28th International Workshop on Smart Antennas 2025", "summary": "Rate splitting multiple access (RSMA) is intrinsically amalgamated with\nsimultaneously transmitting and reflecting (STAR) reconfigurable intelligent\nsurfaces (RIS) to enhance energy efficiency (EE) of the finite block length\n(FBL) multiple-input multiple-output (MIMO) downlink. An alternating\noptimization-based algorithm is proposed to jointly optimize the transmit\nbeamforming matrices, STAR-RIS configurations, and rate-splitting parameters.\nSTAR-RIS attains 360-degree full-plane coverage, while RSMA provides a\nprominent gain by efficiently managing interference. Numerical results reveal a\nstrong synergy between RSMA and STAR-RIS, demonstreating significant EE gains\nover reflective RIS and spatial division multiple access (SDMA).", "AI": {"tldr": "RSMA\u4e0eSTAR-RIS\u7ed3\u5408\u4f18\u5316FBL MIMO\u4e0b\u884c\u94fe\u8def\u7684\u80fd\u6548\uff0c\u63d0\u51fa\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u3002", "motivation": "\u63d0\u5347\u6709\u9650\u5757\u957f\u5ea6MIMO\u4e0b\u884c\u94fe\u8def\u7684\u80fd\u6548\u3002", "method": "\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u53d1\u5c04\u6ce2\u675f\u6210\u5f62\u77e9\u9635\u3001STAR-RIS\u914d\u7f6e\u548c\u901f\u7387\u5206\u5272\u53c2\u6570\u3002", "result": "RSMA\u4e0eSTAR-RIS\u534f\u540c\u663e\u8457\u63d0\u5347\u80fd\u6548\uff0c\u4f18\u4e8e\u53cd\u5c04RIS\u548cSDMA\u3002", "conclusion": "RSMA\u4e0eSTAR-RIS\u7ed3\u5408\u5728\u80fd\u6548\u4f18\u5316\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.00545", "categories": ["cs.LG", "cs.AI", "cs.NE", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.00545", "abs": "https://arxiv.org/abs/2508.00545", "authors": ["Pietro Barbiero", "Mateo Espinosa Zarlenga", "Alberto Termine", "Mateja Jamnik", "Giuseppe Marra"], "title": "Foundations of Interpretable Models", "comment": null, "summary": "We argue that existing definitions of interpretability are not actionable in\nthat they fail to inform users about general, sound, and robust interpretable\nmodel design. This makes current interpretability research fundamentally\nill-posed. To address this issue, we propose a definition of interpretability\nthat is general, simple, and subsumes existing informal notions within the\ninterpretable AI community. We show that our definition is actionable, as it\ndirectly reveals the foundational properties, underlying assumptions,\nprinciples, data structures, and architectural features necessary for designing\ninterpretable models. Building on this, we propose a general blueprint for\ndesigning interpretable models and introduce the first open-sourced library\nwith native support for interpretable data structures and processes.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u53ef\u64cd\u4f5c\u6027\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u8bbe\u8ba1\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u901a\u7528\u84dd\u56fe\u548c\u5f00\u6e90\u5e93\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\u7f3a\u4e4f\u64cd\u4f5c\u6027\uff0c\u65e0\u6cd5\u6307\u5bfc\u7528\u6237\u8bbe\u8ba1\u901a\u7528\u3001\u53ef\u9760\u4e14\u7a33\u5065\u7684\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u5bfc\u81f4\u7814\u7a76\u95ee\u9898\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u7b80\u5355\u7684\u53ef\u89e3\u91ca\u6027\u5b9a\u4e49\uff0c\u6db5\u76d6\u73b0\u6709\u975e\u6b63\u5f0f\u6982\u5ff5\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86\u53ef\u89e3\u91ca\u6a21\u578b\u7684\u84dd\u56fe\u548c\u5f00\u6e90\u5e93\u3002", "result": "\u65b0\u5b9a\u4e49\u63ed\u793a\u4e86\u8bbe\u8ba1\u53ef\u89e3\u91ca\u6a21\u578b\u6240\u9700\u7684\u57fa\u7840\u5c5e\u6027\u3001\u5047\u8bbe\u3001\u539f\u5219\u3001\u6570\u636e\u7ed3\u6784\u548c\u67b6\u6784\u7279\u5f81\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u89e3\u91ca\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91caAI\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.00043", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00043", "abs": "https://arxiv.org/abs/2508.00043", "authors": ["Nhut Truong", "Uri Hasson"], "title": "Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity", "comment": null, "summary": "Topographic neural networks are computational models that can simulate the\nspatial and functional organization of the brain. Topographic constraints in\nneural networks can be implemented in multiple ways, with potentially different\nimpacts on the representations learned by the network. The impact of such\ndifferent implementations has not been systematically examined. To this end,\nhere we compare topographic convolutional neural networks trained with two\nspatial constraints: Weight Similarity (WS), which pushes neighboring units to\ndevelop similar incoming weights, and Activation Similarity (AS), which\nenforces similarity in unit activations. We evaluate the resulting models on\nclassification accuracy, robustness to weight perturbations and input\ndegradation, and the spatial organization of learned representations. Compared\nto both AS and standard CNNs, WS provided three main advantages: i) improved\nrobustness to noise, also showing higher accuracy under weight corruption; ii)\ngreater input sensitivity, reflected in higher activation variance; and iii)\nstronger functional localization, with units showing similar activations\npositioned at closer distances. In addition, WS produced differences in\norientation tuning, symmetry sensitivity, and eccentricity profiles of units,\nindicating an influence of this spatial constraint on the representational\ngeometry of the network. Our findings suggest that during end-to-end training,\nWS constraints produce more robust representations than AS or non-topographic\nCNNs. These findings also suggest that weight-based spatial constraints can\nshape feature learning and functional organization in biophysical inspired\nmodels.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4e24\u79cd\u7a7a\u95f4\u7ea6\u675f\uff08\u6743\u91cd\u76f8\u4f3c\u6027\u548c\u6fc0\u6d3b\u76f8\u4f3c\u6027\uff09\u5bf9\u5730\u5f62\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6743\u91cd\u76f8\u4f3c\u6027\u5728\u9c81\u68d2\u6027\u3001\u8f93\u5165\u654f\u611f\u6027\u548c\u529f\u80fd\u5b9a\u4f4d\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u540c\u5730\u5f62\u7ea6\u675f\u5bf9\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u8868\u793a\u7684\u5f71\u54cd\uff0c\u586b\u8865\u7cfb\u7edf\u6027\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u8bad\u7ec3\u4e24\u79cd\u5730\u5f62\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08\u6743\u91cd\u76f8\u4f3c\u6027\u548c\u6fc0\u6d3b\u76f8\u4f3c\u6027\uff09\uff0c\u6bd4\u8f83\u5176\u5728\u5206\u7c7b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u8868\u793a\u7a7a\u95f4\u7ec4\u7ec7\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6743\u91cd\u76f8\u4f3c\u6027\u5728\u566a\u58f0\u9c81\u68d2\u6027\u3001\u8f93\u5165\u654f\u611f\u6027\u548c\u529f\u80fd\u5b9a\u4f4d\u65b9\u9762\u4f18\u4e8e\u6fc0\u6d3b\u76f8\u4f3c\u6027\u548c\u6807\u51c6CNN\uff0c\u5e76\u5f71\u54cd\u4e86\u7f51\u7edc\u8868\u793a\u51e0\u4f55\u3002", "conclusion": "\u6743\u91cd\u76f8\u4f3c\u6027\u7ea6\u675f\u80fd\u4ea7\u751f\u66f4\u9c81\u68d2\u7684\u8868\u793a\uff0c\u5e76\u5f71\u54cd\u7279\u5f81\u5b66\u4e60\u548c\u529f\u80fd\u7ec4\u7ec7\u3002"}}
{"id": "2508.00456", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00456", "abs": "https://arxiv.org/abs/2508.00456", "authors": ["Ji Wang", "Bin Tang", "Jian Xiao", "Qimei Cui", "Xingwang Li", "Tony Q. S. Quek"], "title": "When Vision-Language Model (VLM) Meets Beam Prediction: A Multimodal Contrastive Learning Framework", "comment": null, "summary": "As the real propagation environment becomes in creasingly complex and\ndynamic, millimeter wave beam prediction faces huge challenges. However, the\npowerful cross modal representation capability of vision-language model (VLM)\nprovides a promising approach. The traditional methods that rely on real-time\nchannel state information (CSI) are computationally expensive and often fail to\nmaintain accuracy in such environments. In this paper, we present a VLM-driven\ncontrastive learning based multimodal beam prediction framework that integrates\nmultimodal data via modality-specific encoders. To enforce cross-modal\nconsistency, we adopt a contrastive pretraining strategy to align image and\nLiDAR features in the latent space. We use location information as text prompts\nand connect it to the text encoder to introduce language modality, which\nfurther improves cross-modal consistency. Experiments on the DeepSense-6G\ndataset show that our VLM backbone provides additional semantic grounding.\nCompared with existing methods, the overall distance-based accuracy score\n(DBA-Score) of 0.9016, corresponding to 1.46% average improvement.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u591a\u6a21\u6001\u6ce2\u675f\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6beb\u7c73\u6ce2\u6ce2\u675f\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u590d\u6742\u52a8\u6001\u7684\u4f20\u64ad\u73af\u5883\u4f7f\u6beb\u7c73\u6ce2\u6ce2\u675f\u9884\u6d4b\u9762\u4e34\u6311\u6218\uff0c\u4f20\u7edf\u57fa\u4e8e\u5b9e\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u7684\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u51c6\u786e\u6027\u4e0d\u8db3\u3002", "method": "\u91c7\u7528VLM\u9a71\u52a8\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u901a\u8fc7\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\u5bf9\u9f50\u56fe\u50cf\u548cLiDAR\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u8bed\u8a00\u6a21\u6001\u589e\u5f3a\u4e00\u81f4\u6027\u3002", "result": "\u5728DeepSense-6G\u6570\u636e\u96c6\u4e0a\uff0cDBA-Score\u8fbe\u52300.9016\uff0c\u5e73\u5747\u63d0\u53471.46%\u3002", "conclusion": "VLM\u6846\u67b6\u4e3a\u6beb\u7c73\u6ce2\u6ce2\u675f\u9884\u6d4b\u63d0\u4f9b\u4e86\u8bed\u4e49\u57fa\u7840\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.00046", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00046", "abs": "https://arxiv.org/abs/2508.00046", "authors": ["Ruo Yu Tao", "Kaicheng Guo", "Cameron Allen", "George Konidaris"], "title": "Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains", "comment": "To appear at RLC 2025. 1 cover page, 10 pages, 3 reference pages + 13\n  pages for supplementary material", "summary": "Mitigating partial observability is a necessary but challenging task for\ngeneral reinforcement learning algorithms. To improve an algorithm's ability to\nmitigate partial observability, researchers need comprehensive benchmarks to\ngauge progress. Most algorithms tackling partial observability are only\nevaluated on benchmarks with simple forms of state aliasing, such as feature\nmasking and Gaussian noise. Such benchmarks do not represent the many forms of\npartial observability seen in real domains, like visual occlusion or unknown\nopponent intent. We argue that a partially observable benchmark should have two\nkey properties. The first is coverage in its forms of partial observability, to\nensure an algorithm's generalizability. The second is a large gap between the\nperformance of a agents with more or less state information, all other factors\nroughly equal. This gap implies that an environment is memory improvable: where\nperformance gains in a domain are from an algorithm's ability to cope with\npartial observability as opposed to other factors. We introduce best-practice\nguidelines for empirically benchmarking reinforcement learning under partial\nobservability, as well as the open-source library POBAX: Partially Observable\nBenchmarks in JAX. We characterize the types of partial observability present\nin various environments and select representative environments for our\nbenchmark. These environments include localization and mapping, visual control,\ngames, and more. Additionally, we show that these tasks are all memory\nimprovable and require hard-to-learn memory functions, providing a concrete\nsignal for partial observability research. This framework includes recommended\nhyperparameters as well as algorithm implementations for fast, out-of-the-box\nevaluation, as well as highly performant environments implemented in JAX for\nGPU-scalable experimentation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u6027\u80fd\u7684\u57fa\u51c6\u6846\u67b6POBAX\uff0c\u5f3a\u8c03\u8986\u76d6\u591a\u79cd\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u5f62\u5f0f\u548c\u73af\u5883\u7684\u5185\u5b58\u53ef\u6539\u8fdb\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u57fa\u51c6\u8fc7\u4e8e\u7b80\u5355\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u63d0\u51faPOBAX\u5f00\u6e90\u5e93\uff0c\u5305\u542b\u591a\u6837\u5316\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u73af\u5883\uff0c\u5e76\u63d0\u4f9b\u8d85\u53c2\u6570\u548c\u7b97\u6cd5\u5b9e\u73b0\u3002", "result": "POBAX\u4e2d\u7684\u4efb\u52a1\u5747\u5177\u6709\u5185\u5b58\u53ef\u6539\u8fdb\u6027\uff0c\u4e14\u9700\u8981\u96be\u4ee5\u5b66\u4e60\u7684\u5185\u5b58\u529f\u80fd\u3002", "conclusion": "POBAX\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u3001\u9ad8\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2508.00494", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00494", "abs": "https://arxiv.org/abs/2508.00494", "authors": ["Youngsun Kong", "Farnoush Baghestani", "I-Ping Chen", "Ki Chon"], "title": "Feasibility of Extracting Skin Nerve Activity from Electrocardiogram Recorded at A Low Sampling Frequency", "comment": "Accepted and presented at the 47th Annual International Conference of\n  the IEEE Engineering in Medicine and Biology Society (EMBC 2025)", "summary": "Skin nerve activity (SKNA) derived from electrocardiogram (ECG) signals has\nbeen a promising non-invasive surrogate for accurate and effective assessment\nof the sympathetic nervous system (SNS). Typically, SKNA extraction requires a\nhigher sampling frequency than the typical ECG recording requirement (> 2 kHz)\nbecause analysis tools extract SKNA from the 0.5-1 kHz frequency band. However,\nECG recording systems commonly provide a sampling frequency of 1 kHz or lower,\nparticularly for wearable devices. Our recent power spectral analysis exhibited\nthat 150-500 Hz frequency bands are dominant during sympathetic stimulation.\nTherefore, we hypothesize that SKNA can be extracted from ECG sampled at a\nlower sampling frequency. We collected ECG signals from 16 participants during\nSNS stimulation and resampled the signals at 0.5, 1, and 4 kHz. Our statistical\nanalyses of significance, classification performance, and reliability indicate\nno significant difference between SKNA indices derived from ECG signals sampled\nat 0.5, 1, and 4 kHz. Our findings indicate that conventional ECG devices,\nwhich are limited to low sampling rates due to resource constraints or outdated\nguidelines, can be used to reliably collect SKNA if muscle artifact\ncontamination is minimal.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u76ae\u80a4\u795e\u7ecf\u6d3b\u52a8\uff08SKNA\uff09\u53ef\u4ee5\u4ece\u4f4e\u91c7\u6837\u9891\u7387\uff08\u59820.5\u62161 kHz\uff09\u7684ECG\u4fe1\u53f7\u4e2d\u63d0\u53d6\uff0c\u65e0\u9700\u4f20\u7edf\u7684\u9ad8\u91c7\u6837\u9891\u7387\uff08>2 kHz\uff09\u3002", "motivation": "\u4f20\u7edfSKNA\u63d0\u53d6\u9700\u8981\u9ad8\u91c7\u6837\u9891\u7387\uff08>2 kHz\uff09\uff0c\u4f46\u8bb8\u591aECG\u8bbe\u5907\uff08\u5c24\u5176\u662f\u53ef\u7a7f\u6234\u8bbe\u5907\uff09\u91c7\u6837\u9891\u7387\u8f83\u4f4e\uff08\u22641 kHz\uff09\uff0c\u9650\u5236\u4e86SKNA\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc716\u540d\u53c2\u4e0e\u8005\u7684ECG\u4fe1\u53f7\uff0c\u57280.5\u30011\u548c4 kHz\u91c7\u6837\u9891\u7387\u4e0b\u63d0\u53d6SKNA\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u4e0d\u540c\u91c7\u6837\u9891\u7387\uff080.5\u30011\u548c4 kHz\uff09\u63d0\u53d6\u7684SKNA\u6307\u6807\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u4f4e\u91c7\u6837\u9891\u7387\u7684ECG\u8bbe\u5907\u53ef\u53ef\u9760\u5730\u6536\u96c6SKNA\uff0c\u524d\u63d0\u662f\u808c\u8089\u4f2a\u5f71\u6c61\u67d3\u6700\u5c0f\u3002"}}
{"id": "2508.00047", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00047", "abs": "https://arxiv.org/abs/2508.00047", "authors": ["Yuan-Cheng Yu", "Yen-Chieh Ouyang", "Chun-An Lin"], "title": "TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection", "comment": "11 pages, 2 figures", "summary": "Time-series anomaly detection plays a central role across a wide range of\napplication domains. With the increasing proliferation of the Internet of\nThings (IoT) and smart manufacturing, time-series data has dramatically\nincreased in both scale and dimensionality. This growth has exposed the\nlimitations of traditional statistical methods in handling the high\nheterogeneity and complexity of such data. Inspired by the recent success of\nlarge language models (LLMs) in multimodal tasks across language and vision\ndomains, we propose a novel unsupervised anomaly detection framework: A\nTri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly\nDetection (TriP-LLM). TriP-LLM integrates local and global temporal features\nthrough a tri-branch design-Patching, Selection, and Global-to encode the input\ntime series into patch-wise tokens, which are then processed by a frozen,\npretrained LLM. A lightweight patch-wise decoder reconstructs the input, from\nwhich anomaly scores are derived. We evaluate TriP-LLM on several public\nbenchmark datasets using PATE, a recently proposed threshold-free evaluation\nmetric, and conduct all comparisons within a unified open-source framework to\nensure fairness. Experimental results show that TriP-LLM consistently\noutperforms recent state-of-the-art methods across all datasets, demonstrating\nstrong detection capabilities. Furthermore, through extensive ablation studies,\nwe verify the substantial contribution of the LLM to the overall architecture.\nCompared to LLM-based approaches using Channel Independence (CI) patch\nprocessing, TriP-LLM achieves significantly lower memory consumption, making it\nmore suitable for GPU memory-constrained environments. All code and model\ncheckpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git", "AI": {"tldr": "\u63d0\u51fa\u4e86TriP-LLM\u6846\u67b6\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u5185\u5b58\u6d88\u8017\u66f4\u4f4e\u3002", "motivation": "\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u542f\u53d1\uff0c\u63d0\u51fa\u65b0\u6846\u67b6\u3002", "method": "TriP-LLM\u901a\u8fc7\u4e09\u652f\u8def\u8bbe\u8ba1\uff08\u5206\u5757\u3001\u9009\u62e9\u3001\u5168\u5c40\uff09\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\uff0c\u5229\u7528\u51bb\u7ed3\u9884\u8bad\u7ec3LLM\u5904\u7406\u5206\u5757\u6807\u8bb0\uff0c\u8f7b\u91cf\u89e3\u7801\u5668\u91cd\u6784\u8f93\u5165\u5e76\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5185\u5b58\u6d88\u8017\u663e\u8457\u4f4e\u4e8e\u57fa\u4e8eCI\u5206\u5757\u5904\u7406\u7684LLM\u65b9\u6cd5\u3002", "conclusion": "TriP-LLM\u5728\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u9002\u5408GPU\u5185\u5b58\u53d7\u9650\u73af\u5883\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.00078", "categories": ["cs.LG", "cs.AI", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2508.00078", "abs": "https://arxiv.org/abs/2508.00078", "authors": ["Imen Mahmoud", "Andrei Velichko"], "title": "Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization", "comment": "22 pages, 5 figures", "summary": "This study proposes a novel methodological framework integrating a LightGBM\nregression model and genetic algorithm (GA) optimization to systematically\nevaluate the contribution of COVID-19-related indicators to Bitcoin return\nprediction. The primary objective was not merely to forecast Bitcoin returns\nbut rather to determine whether including pandemic-related health data\nsignificantly enhances prediction accuracy. A comprehensive dataset comprising\ndaily Bitcoin returns and COVID-19 metrics (vaccination rates,\nhospitalizations, testing statistics) was constructed. Predictive models,\ntrained with and without COVID-19 features, were optimized using GA over 31\nindependent runs, allowing robust statistical assessment. Performance metrics\n(R2, RMSE, MAE) were statistically compared through distribution overlaps and\nMann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified\nindividual feature contributions. Results indicate that COVID-19 indicators\nsignificantly improved model performance, particularly in capturing extreme\nmarket fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly\nsignificant statistically). Among COVID-19 features, vaccination metrics,\nespecially the 75th percentile of fully vaccinated individuals, emerged as\ndominant predictors. The proposed methodology extends existing financial\nanalytics tools by incorporating public health signals, providing investors and\npolicymakers with refined indicators to navigate market uncertainty during\nsystemic crises.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LightGBM\u56de\u5f52\u6a21\u578b\u548c\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u8bc4\u4f30COVID-19\u76f8\u5173\u6307\u6807\u5bf9\u6bd4\u7279\u5e01\u56de\u62a5\u9884\u6d4b\u7684\u8d21\u732e\u3002\u7ed3\u679c\u8868\u660e\uff0c\u75ab\u60c5\u6307\u6807\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u75ab\u82d7\u63a5\u79cd\u6570\u636e\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u786e\u5b9a\u75ab\u60c5\u76f8\u5173\u5065\u5eb7\u6570\u636e\u662f\u5426\u80fd\u663e\u8457\u63d0\u5347\u6bd4\u7279\u5e01\u56de\u62a5\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u6295\u8d44\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u5e02\u573a\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u3002", "method": "\u4f7f\u7528LightGBM\u56de\u5f52\u6a21\u578b\u548c\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\uff0c\u6784\u5efa\u5305\u542b\u6bd4\u7279\u5e01\u56de\u62a5\u548cCOVID-19\u6307\u6807\u7684\u5168\u9762\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc731\u6b21\u72ec\u7acb\u8fd0\u884c\u8fdb\u884c\u7edf\u8ba1\u8bc4\u4f30\u3002", "result": "COVID-19\u6307\u6807\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff08R2\u589e\u52a040%\uff0cRMSE\u964d\u4f4e2%\uff09\uff0c\u75ab\u82d7\u63a5\u79cd\u6570\u636e\u662f\u4e3b\u8981\u9884\u6d4b\u56e0\u5b50\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u516c\u5171\u536b\u751f\u4fe1\u53f7\u6269\u5c55\u4e86\u91d1\u878d\u5206\u6790\u5de5\u5177\uff0c\u4e3a\u7cfb\u7edf\u6027\u5371\u673a\u671f\u95f4\u7684\u5e02\u573a\u4e0d\u786e\u5b9a\u6027\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u6307\u6807\u3002"}}
{"id": "2508.00800", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00800", "abs": "https://arxiv.org/abs/2508.00800", "authors": ["Rui Chen", "Wen-Xuan Long", "Bing-Qian Wang", "Yuan He", "Rui-Jin Sun", "Nan Cheng", "Gan Zheng", "Dusit Niyato"], "title": "Multibeam High Throughput Satellite: Hardware Foundation, Resource Allocation, and Precoding", "comment": "38 pages, 18 figures", "summary": "With its wide coverage and uninterrupted service, satellite communication is\na critical technology for next-generation 6G communications. High throughput\nsatellite (HTS) systems, utilizing multipoint beam and frequency multiplexing\ntechniques, enable satellite communication capacity of up to Tbps to meet the\ngrowing traffic demand. Therefore, it is imperative to review\nthe-state-of-the-art of multibeam HTS systems and identify their associated\nchallenges and perspectives. Firstly, we summarize the multibeam HTS hardware\nfoundations, including ground station systems, on-board payloads, and user\nterminals. Subsequently, we review the flexible on-board radio resource\nallocation approaches of bandwidth, power, time slot, and joint allocation\nschemes of HTS systems to optimize resource utilization and cater to\nnon-uniform service demand. Additionally, we survey multibeam precoding methods\nfor the HTS system to achieve full-frequency reuse and interference\ncancellation, which are classified according to different deployments such as\nsingle gateway precoding, multiple gateway precoding, on-board precoding, and\nhybrid on-board/on-ground precoding. Finally, we disscuss the challenges\nrelated to Q/V band link outage, time and frequency synchronization of\ngateways, the accuracy of channel state information (CSI), payload light-weight\ndevelopment, and the application of deep learning (DL). Research on these\ntopics will contribute to enhancing the performance of HTS systems and finally\ndelivering high-speed data to areas underserved by terrestrial networks.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u591a\u6ce2\u675f\u9ad8\u541e\u5410\u91cf\u536b\u661f\uff08HTS\uff09\u7cfb\u7edf\u7684\u73b0\u72b6\u3001\u6311\u6218\u4e0e\u524d\u666f\uff0c\u6db5\u76d6\u786c\u4ef6\u57fa\u7840\u3001\u8d44\u6e90\u5206\u914d\u3001\u9884\u7f16\u7801\u65b9\u6cd5\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u536b\u661f\u901a\u4fe1\u662f6G\u7684\u5173\u952e\u6280\u672f\uff0cHTS\u7cfb\u7edf\u901a\u8fc7\u591a\u6ce2\u675f\u548c\u9891\u5206\u590d\u7528\u6280\u672f\u5b9e\u73b0Tbps\u7ea7\u5bb9\u91cf\uff0c\u9700\u603b\u7ed3\u5176\u6700\u65b0\u8fdb\u5c55\u4e0e\u6311\u6218\u3002", "method": "\u603b\u7ed3HTS\u786c\u4ef6\u57fa\u7840\uff08\u5730\u9762\u7ad9\u3001\u661f\u8f7d\u8f7d\u8377\u3001\u7528\u6237\u7ec8\u7aef\uff09\uff0c\u56de\u987e\u7075\u6d3b\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff08\u5e26\u5bbd\u3001\u529f\u7387\u3001\u65f6\u9699\uff09\uff0c\u5206\u7c7b\u591a\u6ce2\u675f\u9884\u7f16\u7801\u65b9\u6cd5\uff08\u5355\u7f51\u5173\u3001\u591a\u7f51\u5173\u3001\u661f\u8f7d\u3001\u6df7\u5408\uff09\u3002", "result": "HTS\u7cfb\u7edf\u901a\u8fc7\u4f18\u5316\u8d44\u6e90\u5206\u914d\u548c\u9884\u7f16\u7801\u5b9e\u73b0\u5168\u9891\u590d\u7528\u548c\u5e72\u6270\u6d88\u9664\uff0c\u4f46\u4ecd\u9762\u4e34Q/V\u9891\u6bb5\u94fe\u8def\u4e2d\u65ad\u3001\u540c\u6b65\u3001CSI\u51c6\u786e\u6027\u7b49\u6311\u6218\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u9700\u89e3\u51b3HTS\u7cfb\u7edf\u7684\u6280\u672f\u6311\u6218\uff0c\u4ee5\u63d0\u5347\u6027\u80fd\u5e76\u4e3a\u5730\u9762\u7f51\u7edc\u8986\u76d6\u4e0d\u8db3\u5730\u533a\u63d0\u4f9b\u9ad8\u901f\u6570\u636e\u670d\u52a1\u3002"}}
{"id": "2508.00098", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00098", "abs": "https://arxiv.org/abs/2508.00098", "authors": ["Ashkan Shakarami", "Yousef Yeganeh", "Azade Farshad", "Lorenzo Nicole", "Stefano Ghidoni", "Nassir Navab"], "title": "Stress-Aware Resilient Neural Training", "comment": "16 pages, 11 figures", "summary": "This paper introduces Stress-Aware Learning, a resilient neural training\nparadigm in which deep neural networks dynamically adjust their optimization\nbehavior - whether under stable training regimes or in settings with uncertain\ndynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)\nDeformation, inspired by structural fatigue in materials science. To\ninstantiate this concept, we propose Plastic Deformation Optimizer, a\nstress-aware mechanism that injects adaptive noise into model parameters\nwhenever an internal stress signal - reflecting stagnation in training loss and\naccuracy - indicates persistent optimization difficulty. This enables the model\nto escape sharp minima and converge toward flatter, more generalizable regions\nof the loss landscape. Experiments across six architectures, four optimizers,\nand seven vision benchmarks demonstrate improved robustness and generalization\nwith minimal computational overhead. The code and 3D visuals will be available\non GitHub: https://github.com/Stress-Aware-Learning/SAL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6750\u6599\u79d1\u5b66\u4e2d\u7ed3\u6784\u75b2\u52b3\u6982\u5ff5\u7684\u5f39\u6027\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f18\u5316\u884c\u4e3a\u63d0\u5347\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u53d7\u6750\u6599\u79d1\u5b66\u4e2d\u4e34\u65f6\uff08\u5f39\u6027\uff09\u548c\u6c38\u4e45\uff08\u5851\u6027\uff09\u53d8\u5f62\u6982\u5ff5\u7684\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u795e\u7ecf\u7f51\u7edc\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u56e0\u4f18\u5316\u56f0\u96be\u800c\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u5851\u6027\u53d8\u5f62\u4f18\u5316\u5668\uff08Plastic Deformation Optimizer\uff09\uff0c\u901a\u8fc7\u5185\u90e8\u538b\u529b\u4fe1\u53f7\u68c0\u6d4b\u8bad\u7ec3\u505c\u6ede\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u5411\u6a21\u578b\u53c2\u6570\u6ce8\u5165\u566a\u58f0\uff0c\u5e2e\u52a9\u6a21\u578b\u9003\u79bb\u5c16\u9510\u6700\u5c0f\u503c\u3002", "result": "\u5728\u516d\u79cd\u67b6\u6784\u3001\u56db\u79cd\u4f18\u5316\u5668\u548c\u4e03\u4e2a\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\u3002", "conclusion": "Stress-Aware Learning\u4e3a\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5f39\u6027\u4f18\u5316\u8303\u5f0f\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00202", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00202", "abs": "https://arxiv.org/abs/2508.00202", "authors": ["Ecem Bozkurt", "Antonio Ortega"], "title": "Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models", "comment": "5 pages, 2 figures, under review at CAMSAP 2025", "summary": "Foundation models (FMs) pretrained on large datasets have become fundamental\nfor various downstream machine learning tasks, in particular in scenarios where\nobtaining perfectly labeled data is prohibitively expensive. In this paper, we\nassume an FM has to be fine-tuned with noisy data and present a two-stage\nframework to ensure robust classification in the presence of label noise\nwithout model retraining. Recent work has shown that simple k-nearest neighbor\n(kNN) approaches using an embedding derived from an FM can achieve good\nperformance even in the presence of severe label noise. Our work is motivated\nby the fact that these methods make use of local geometry. In this paper,\nfollowing a similar two-stage procedure, reliability estimation followed by\nreliability-weighted inference, we show that improved performance can be\nachieved by introducing geometry information. For a given instance, our\nproposed inference uses a local neighborhood of training data, obtained using\nthe non-negative kernel (NNK) neighborhood construction. We propose several\nmethods for reliability estimation that can rely less on distance and local\nneighborhood as the label noise increases. Our evaluation on CIFAR-10 and\nDermaMNIST shows that our methods improve robustness across various noise\nconditions, surpassing standard K-NN approaches and recent\nadaptive-neighborhood baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5229\u7528\u51e0\u4f55\u4fe1\u606f\u5728\u6807\u7b7e\u566a\u58f0\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u5206\u7c7b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u5728\u9700\u8981\u5fae\u8c03\u566a\u58f0\u6570\u636e\u7684\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u573a\u666f\u4e0b\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5c40\u90e8\u51e0\u4f55\u4fe1\u606f\uff0c\u4f46\u6027\u80fd\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u53ef\u9760\u6027\u4f30\u8ba1\u548c\u53ef\u9760\u6027\u52a0\u6743\u63a8\u65ad\uff0c\u5f15\u5165\u975e\u8d1f\u6838\uff08NNK\uff09\u90bb\u57df\u6784\u5efa\u548c\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5728CIFAR-10\u548cDermaMNIST\u6570\u636e\u96c6\u4e0a\uff0c\u65b9\u6cd5\u5728\u591a\u79cd\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8e\u6807\u51c6K-NN\u548c\u81ea\u9002\u5e94\u90bb\u57df\u57fa\u7ebf\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u4fe1\u606f\u548c\u53ef\u9760\u6027\u52a0\u6743\u63a8\u65ad\uff0c\u663e\u8457\u63d0\u5347\u4e86\u566a\u58f0\u6807\u7b7e\u6761\u4ef6\u4e0b\u7684\u5206\u7c7b\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00117", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00117", "abs": "https://arxiv.org/abs/2508.00117", "authors": ["Md. Ehsanul Haque", "S. M. Jahidul Islam", "Shakil Mia", "Rumana Sharmin", "Ashikuzzaman", "Md Samir Morshed", "Md. Tahmidul Huque"], "title": "StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection", "comment": "Accepted and presented paper of THE 16th INTERNATIONAL IEEE\n  CONFERENCE ON COMPUTING, COMMUNICATION AND NETWORKING TECHNOLOGIES (ICCCNT)\n  INDIA", "summary": "Liver diseases are a serious health concern in the world, which requires\nprecise and timely diagnosis to enhance the survival chances of patients. The\ncurrent literature implemented numerous machine learning and deep learning\nmodels to classify liver diseases, but most of them had some issues like high\nmisclassification error, poor interpretability, prohibitive computational\nexpense, and lack of good preprocessing strategies. In order to address these\ndrawbacks, we introduced StackLiverNet in this study; an interpretable stacked\nensemble model tailored to the liver disease detection task. The framework uses\nadvanced data preprocessing and feature selection technique to increase model\nrobustness and predictive ability. Random undersampling is performed to deal\nwith class imbalance and make the training balanced. StackLiverNet is an\nensemble of several hyperparameter-optimized base classifiers, whose\ncomplementary advantages are used through a LightGBM meta-model. The provided\nmodel demonstrates excellent performance, with the testing accuracy of 99.89%,\nCohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and\nefficient training and inference speeds that are amenable to clinical practice\n(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local\nInterpretable Model-Agnostic Explanations (LIME) are applied to generate\ntransparent explanations of individual predictions, revealing high\nconcentrations of Alkaline Phosphatase and moderate SGOT as important\nobservations of liver disease. Also, SHAP was used to rank features by their\nglobal contribution to predictions, while the Morris method confirmed the most\ninfluential features through sensitivity analysis.", "AI": {"tldr": "StackLiverNet\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5806\u53e0\u96c6\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u809d\u75c5\u68c0\u6d4b\uff0c\u901a\u8fc7\u9ad8\u7ea7\u6570\u636e\u9884\u5904\u7406\u548c\u7279\u5f81\u9009\u62e9\u63d0\u9ad8\u6027\u80fd\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe99.89%\u3002", "motivation": "\u73b0\u6709\u809d\u75c5\u5206\u7c7b\u6a21\u578b\u5b58\u5728\u9ad8\u8bef\u5206\u7c7b\u7387\u3001\u89e3\u91ca\u6027\u5dee\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u7b49\u95ee\u9898\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u968f\u673a\u6b20\u91c7\u6837\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u7ed3\u5408\u591a\u4e2a\u8d85\u53c2\u6570\u4f18\u5316\u7684\u57fa\u5206\u7c7b\u5668\uff0c\u901a\u8fc7LightGBM\u5143\u6a21\u578b\u96c6\u6210\u3002", "result": "\u6d4b\u8bd5\u51c6\u786e\u738799.89%\uff0cCohen Kappa 0.9974\uff0cAUC 0.9993\uff0c\u4ec55\u6b21\u8bef\u5206\u7c7b\u3002", "conclusion": "StackLiverNet\u6027\u80fd\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u5b9e\u8df5\uff0c\u4e14\u901a\u8fc7LIME\u548cSHAP\u63d0\u4f9b\u900f\u660e\u89e3\u91ca\u3002"}}
{"id": "2508.00127", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00127", "abs": "https://arxiv.org/abs/2508.00127", "authors": ["Saleh Nikooroo", "Thomas Engel"], "title": "Structured Transformations for Stable and Interpretable Neural Computation", "comment": null, "summary": "Despite their impressive performance, contemporary neural networks often lack\nstructural safeguards that promote stable learning and interpretable behavior.\nIn this work, we introduce a reformulation of layer-level transformations that\ndeparts from the standard unconstrained affine paradigm. Each transformation is\ndecomposed into a structured linear operator and a residual corrective\ncomponent, enabling more disciplined signal propagation and improved training\ndynamics. Our formulation encourages internal consistency and supports stable\ninformation flow across depth, while remaining fully compatible with standard\nlearning objectives and backpropagation. Through a series of synthetic and\nreal-world experiments, we demonstrate that models constructed with these\nstructured transformations exhibit improved gradient conditioning, reduced\nsensitivity to perturbations, and layer-wise robustness. We further show that\nthese benefits persist across architectural scales and training regimes. This\nstudy serves as a foundation for a more principled class of neural\narchitectures that prioritize stability and transparency-offering new tools for\nreasoning about learning behavior without sacrificing expressive power.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\u53d8\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7ebf\u6027\u7b97\u5b50\u548c\u6b8b\u5dee\u4fee\u6b63\u7ec4\u4ef6\uff0c\u63d0\u5347\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5f53\u524d\u795e\u7ecf\u7f51\u7edc\u7f3a\u4e4f\u7ed3\u6784\u4fdd\u969c\uff0c\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u884c\u4e3a\u96be\u4ee5\u89e3\u91ca\u3002", "method": "\u5c06\u5c42\u53d8\u6362\u5206\u89e3\u4e3a\u7ed3\u6784\u5316\u7ebf\u6027\u7b97\u5b50\u548c\u6b8b\u5dee\u4fee\u6b63\u7ec4\u4ef6\uff0c\u652f\u6301\u7a33\u5b9a\u4fe1\u606f\u6d41\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6539\u5584\u4e86\u68af\u5ea6\u6761\u4ef6\u3001\u964d\u4f4e\u6270\u52a8\u654f\u611f\u6027\uff0c\u5e76\u589e\u5f3a\u5c42\u95f4\u9c81\u68d2\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u66f4\u7a33\u5b9a\u3001\u900f\u660e\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e0d\u727a\u7272\u8868\u8fbe\u80fd\u529b\u3002"}}
{"id": "2508.00131", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00131", "abs": "https://arxiv.org/abs/2508.00131", "authors": ["Christopher Harvey", "Sumaiya Shomaji", "Zijun Yao", "Amit Noheria"], "title": "ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks", "comment": "arXiv admin note: substantial text overlap with arXiv:2410.02937", "summary": "The electrocardiogram (ECG) is an inexpensive and widely available tool for\ncardiac assessment. Despite its standardized format and small file size, the\nhigh complexity and inter-individual variability of ECG signals (typically a\n60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep\nlearning models, especially when only small training datasets are available.\nThis study addresses these challenges by exploring feature generation methods\nfrom representative beat ECGs, focusing on Principal Component Analysis (PCA)\nand Autoencoders to reduce data complexity. We introduce three novel\nVariational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed\nbeta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their\neffectiveness in maintaining signal fidelity and enhancing downstream\nprediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE\nachieved superior signal reconstruction, reducing the mean absolute error (MAE)\nto 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE\nencodings, when combined with traditional ECG summary features, improved the\nprediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an\nholdout test set area under the receiver operating characteristic curve (AUROC)\nof 0.901 with a LGBM classifier. This performance nearly matches the 0.909\nAUROC of state-of-the-art CNN model but requires significantly less\ncomputational resources. Further, the ECG feature extraction-LGBM pipeline\navoids overfitting and retains predictive performance when trained with less\ndata. Our findings demonstrate that these VAE encodings are not only effective\nin simplifying ECG data but also provide a practical solution for applying deep\nlearning in contexts with limited-scale labeled training data.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u65b0\u578b\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u53d8\u4f53\uff0c\u7528\u4e8e\u7b80\u5316\u5fc3\u7535\u4fe1\u53f7\u6570\u636e\u5e76\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5fc3\u7535\u4fe1\u53f7\uff08ECG\uff09\u590d\u6742\u5ea6\u9ad8\u4e14\u4e2a\u4f53\u5dee\u5f02\u5927\uff0c\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5c0f\u6570\u636e\u96c6\u4e0a\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u7279\u5f81\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u7814\u7a76\u91c7\u7528PCA\u548c\u81ea\u7f16\u7801\u5668\u964d\u7ef4\uff0c\u5e76\u5f15\u5165\u4e09\u79cdVAE\u53d8\u4f53\uff08SAE\u3001A beta-VAE\u3001C beta-VAE\uff09\uff0c\u7ed3\u5408LGBM\u8fdb\u884c\u9884\u6d4b\u4efb\u52a1\u3002", "result": "A beta-VAE\u4fe1\u53f7\u91cd\u5efa\u8bef\u5dee\u6700\u4f4e\uff08MAE 15.7\u00b13.2 \u03bcV\uff09\uff0cSAE\u7f16\u7801\u7ed3\u5408\u4f20\u7edf\u7279\u5f81\u9884\u6d4bLVEF\u7684AUROC\u8fbe0.901\uff0c\u63a5\u8fd1CNN\u6a21\u578b\u4f46\u8ba1\u7b97\u8d44\u6e90\u66f4\u5c11\u3002", "conclusion": "\u65b0\u578bVAE\u7f16\u7801\u80fd\u6709\u6548\u7b80\u5316ECG\u6570\u636e\uff0c\u4e3a\u5c0f\u89c4\u6a21\u6807\u6ce8\u6570\u636e\u4e0b\u7684\u6df1\u5ea6\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00141", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00141", "abs": "https://arxiv.org/abs/2508.00141", "authors": ["Mohit Gupta", "Debjit Bhowmick", "Rhys Newbury", "Meead Saberi", "Shirui Pan", "Ben Beck"], "title": "INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks", "comment": null, "summary": "Accurate link-level bicycling volume estimation is essential for sustainable\nurban transportation planning. However, many cities face significant challenges\nof high data sparsity due to limited bicycling count sensor coverage. To\naddress this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning\n(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize\nsensor placement and improve link-level bicycling volume estimation in\ndata-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks\n(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL\nagent, enabling a data-driven strategic selection of sensor locations to\nmaximize estimation performance. Applied to Melbourne's bicycling network,\ncomprising 15,933 road segments with sensor coverage on only 141 road segments\n(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume\nestimation by strategically selecting additional sensor locations in\ndeployments of 50, 100, 200 and 500 sensors. Our framework outperforms\ntraditional heuristic methods for sensor placement such as betweenness\ncentrality, closeness centrality, observed bicycling activity and random\nplacement, across key metrics such as Mean Squared Error (MSE), Root Mean\nSquared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our\nexperiments benchmark INSPIRE-GNN against standard machine learning and deep\nlearning models in the bicycle volume estimation performance, underscoring its\neffectiveness. Our proposed framework provides transport planners actionable\ninsights to effectively expand sensor networks, optimize sensor placement and\nmaximize volume estimation accuracy and reliability of bicycling data for\ninformed transportation planning decisions.", "AI": {"tldr": "INSPIRE-GNN\u662f\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u81ea\u884c\u8f66\u6d41\u91cf\u4f20\u611f\u5668\u7684\u653e\u7f6e\u5e76\u63d0\u9ad8\u6570\u636e\u7a00\u758f\u73af\u5883\u4e0b\u7684\u6d41\u91cf\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u57ce\u5e02\u81ea\u884c\u8f66\u6d41\u91cf\u6570\u636e\u7a00\u758f\uff0c\u4f20\u7edf\u4f20\u611f\u5668\u8986\u76d6\u6709\u9650\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u65b9\u6cd5\u4f18\u5316\u4f20\u611f\u5668\u653e\u7f6e\u4ee5\u63d0\u9ad8\u4f30\u8ba1\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u3001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u548c\u6df1\u5ea6Q\u7f51\u7edc\uff08DQN\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u6570\u636e\u9a71\u52a8\u5730\u9009\u62e9\u4f20\u611f\u5668\u4f4d\u7f6e\u3002", "result": "\u5728\u58a8\u5c14\u672c\u81ea\u884c\u8f66\u7f51\u7edc\u4e2d\uff0cINSPIRE-GNN\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u964d\u4f4e\u4e86MSE\u3001RMSE\u548cMAE\u7b49\u8bef\u5dee\u6307\u6807\u3002", "conclusion": "INSPIRE-GNN\u4e3a\u4ea4\u901a\u89c4\u5212\u8005\u63d0\u4f9b\u4e86\u4f18\u5316\u4f20\u611f\u5668\u7f51\u7edc\u548c\u63d0\u5347\u6570\u636e\u53ef\u9760\u6027\u7684\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.00161", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00161", "abs": "https://arxiv.org/abs/2508.00161", "authors": ["Ziqian Zhong", "Aditi Raghunathan"], "title": "Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs", "comment": null, "summary": "The releases of powerful open-weight large language models (LLMs) are often\nnot accompanied by access to their full training data. Existing\ninterpretability methods, particularly those based on activations, often\nrequire or assume distributionally similar data. This is a significant\nlimitation when detecting and defending against novel potential threats like\nbackdoors, which are by definition out-of-distribution.\n  In this work, we introduce a new method for understanding, monitoring and\ncontrolling fine-tuned LLMs that interprets weights, rather than activations,\nthereby side stepping the need for data that is distributionally similar to the\nunknown training data. We demonstrate that the top singular vectors of the\nweight difference between a fine-tuned model and its base model correspond to\nnewly acquired behaviors. By monitoring the cosine similarity of activations\nalong these directions, we can detect salient behaviors introduced during\nfine-tuning with high precision.\n  For backdoored models that bypasses safety mechanisms when a secret trigger\nis present, our method stops up to 100% of attacks with a false positive rate\nbelow 1.2%. For models that have undergone unlearning, we detect inference on\nerased topics with accuracy up to 95.42% and can even steer the model to\nrecover \"unlearned\" information. Besides monitoring, our method also shows\npotential for pre-deployment model auditing: by analyzing commercial\ninstruction-tuned models (OLMo, Llama, Qwen), we are able to uncover\nmodel-specific fine-tuning focus including marketing strategies and Midjourney\nprompt generation.\n  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6743\u91cd\u800c\u975e\u6fc0\u6d3b\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u76d1\u63a7\u548c\u63a7\u5236\u5fae\u8c03\u540e\u7684LLM\uff0c\u65e0\u9700\u4f9d\u8d56\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u76f8\u4f3c\u7684\u8f93\u5165\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6fc0\u6d3b\u7684\u89e3\u91ca\u65b9\u6cd5\u9700\u8981\u5206\u5e03\u76f8\u4f3c\u7684\u6570\u636e\uff0c\u9650\u5236\u4e86\u68c0\u6d4b\u548c\u9632\u5fa1\u65b0\u578b\u5a01\u80c1\uff08\u5982\u540e\u95e8\uff09\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5fae\u8c03\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u6743\u91cd\u5dee\u5f02\u7684\u9876\u90e8\u5947\u5f02\u5411\u91cf\uff0c\u76d1\u63a7\u6fc0\u6d3b\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3002", "result": "\u6210\u529f\u68c0\u6d4b\u540e\u95e8\u653b\u51fb\uff08100%\u963b\u6b62\u7387\uff0c\u5047\u9633\u6027\u7387\u4f4e\u4e8e1.2%\uff09\uff0c\u5e76\u8bc6\u522b\u672a\u5b66\u4e60\u4e3b\u9898\uff08\u51c6\u786e\u738795.42%\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u578b\u76d1\u63a7\u548c\u9884\u90e8\u7f72\u5ba1\u8ba1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u63ed\u793a\u5fae\u8c03\u884c\u4e3a\u3002"}}
{"id": "2508.00172", "categories": ["cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00172", "abs": "https://arxiv.org/abs/2508.00172", "authors": ["Fupei Guo", "Hao Zheng", "Xiang Zhang", "Li Chen", "Yue Wang", "Songyang Zhang"], "title": "DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission", "comment": "To appear in 2025 IEEE Global Communications Conference (Globecom)", "summary": "The rapid development of artificial intelligence has driven smart health with\nnext-generation wireless communication technologies, stimulating exciting\napplications in remote diagnosis and intervention. To enable a timely and\neffective response for remote healthcare, efficient transmission of medical\ndata through noisy channels with limited bandwidth emerges as a critical\nchallenge. In this work, we propose a novel diffusion-based semantic\ncommunication framework, namely DiSC-Med, for the medical image transmission,\nwhere medical-enhanced compression and denoising blocks are developed for\nbandwidth efficiency and robustness, respectively. Unlike conventional\npixel-wise communication framework, our proposed DiSC-Med is able to capture\nthe key semantic information and achieve superior reconstruction performance\nwith ultra-high bandwidth efficiency against noisy channels. Extensive\nexperiments on real-world medical datasets validate the effectiveness of our\nframework, demonstrating its potential for robust and efficient telehealth\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6DiSC-Med\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u533b\u5b66\u56fe\u50cf\u4f20\u8f93\u3002", "motivation": "\u89e3\u51b3\u5728\u6709\u9650\u5e26\u5bbd\u548c\u566a\u58f0\u4fe1\u9053\u4e2d\u9ad8\u6548\u4f20\u8f93\u533b\u7597\u6570\u636e\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u533b\u7597\u589e\u5f3a\u7684\u538b\u7f29\u548c\u53bb\u566a\u6a21\u5757\uff0c\u4ee5\u63d0\u5347\u5e26\u5bbd\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728\u771f\u5b9e\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8fdc\u7a0b\u533b\u7597\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "DiSC-Med\u80fd\u591f\u6355\u6349\u5173\u952e\u8bed\u4e49\u4fe1\u606f\uff0c\u5728\u566a\u58f0\u4fe1\u9053\u4e2d\u5b9e\u73b0\u9ad8\u6548\u91cd\u5efa\uff0c\u9002\u7528\u4e8e\u8fdc\u7a0b\u533b\u7597\u3002"}}
{"id": "2508.00174", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00174", "abs": "https://arxiv.org/abs/2508.00174", "authors": ["Yongchao Huang"], "title": "RL as Regressor: A Reinforcement Learning Approach for Function Approximation", "comment": "7 pages", "summary": "Standard regression techniques, while powerful, are often constrained by\npredefined, differentiable loss functions such as mean squared error. These\nfunctions may not fully capture the desired behavior of a system, especially\nwhen dealing with asymmetric costs or complex, non-differentiable objectives.\nIn this paper, we explore an alternative paradigm: framing regression as a\nReinforcement Learning (RL) problem. We demonstrate this by treating a model's\nprediction as an action and defining a custom reward signal based on the\nprediction error, and we can leverage powerful RL algorithms to perform\nfunction approximation. Through a progressive case study of learning a noisy\nsine wave, we illustrate the development of an Actor-Critic agent, iteratively\nenhancing it with Prioritized Experience Replay, increased network capacity,\nand positional encoding to enable a capable RL agent for this regression task.\nOur results show that the RL framework not only successfully solves the\nregression problem but also offers enhanced flexibility in defining objectives\nand guiding the learning process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5c06\u56de\u5f52\u95ee\u9898\u8f6c\u5316\u4e3a\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u5b9a\u4e49\u5956\u52b1\u4fe1\u53f7\u548cRL\u7b97\u6cd5\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u76ee\u6807\u5b9a\u4e49\u548c\u5b66\u4e60\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u56de\u5f52\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u53ef\u5fae\u635f\u5931\u51fd\u6570\uff0c\u96be\u4ee5\u5904\u7406\u975e\u5bf9\u79f0\u6210\u672c\u6216\u590d\u6742\u76ee\u6807\uff0c\u56e0\u6b64\u63a2\u7d22RL\u6846\u67b6\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u5c06\u6a21\u578b\u9884\u6d4b\u89c6\u4e3a\u52a8\u4f5c\uff0c\u57fa\u4e8e\u9884\u6d4b\u8bef\u5dee\u5b9a\u4e49\u81ea\u5b9a\u4e49\u5956\u52b1\u4fe1\u53f7\uff0c\u5e76\u5229\u7528Actor-Critic\u7b97\u6cd5\u9010\u6b65\u4f18\u5316\u6a21\u578b\u3002", "result": "RL\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u56de\u5f52\u95ee\u9898\uff0c\u5e76\u5728\u76ee\u6807\u5b9a\u4e49\u548c\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "RL\u4e3a\u56de\u5f52\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u548c\u975e\u53ef\u5fae\u7684\u76ee\u6807\u3002"}}
{"id": "2508.00201", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00201", "abs": "https://arxiv.org/abs/2508.00201", "authors": ["Mehdi Ben Ayed", "Fei Feng", "Jay Adams", "Vishwakarma Singh", "Kritarth Anand", "Jiajing Xu"], "title": "RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems", "comment": null, "summary": "Existing web-scale recommendation systems commonly use supervised learning\nmethods that prioritize immediate user feedback. Although reinforcement\nlearning (RL) offers a solution to optimize longer-term goals, such as\nin-session engagement, applying it at web scale is challenging due to the\nextremely large action space and engineering complexity. In this paper, we\nintroduce RecoMind, a simulator-based RL framework designed for the effective\noptimization of session-based goals at web-scale. RecoMind leverages existing\nrecommendation models to establish a simulation environment and to bootstrap\nthe RL policy to optimize immediate user interactions from the outset. This\nmethod integrates well with existing industry pipelines, simplifying the\ntraining and deployment of RL policies. Additionally, RecoMind introduces a\ncustom exploration strategy to efficiently explore web-scale action spaces with\nhundreds of millions of items. We evaluated RecoMind through extensive offline\nsimulations and online A/B testing on a video streaming platform. Both methods\nshowed that the RL policy trained using RecoMind significantly outperforms\ntraditional supervised learning recommendation approaches in in-session user\nsatisfaction. In online A/B tests, the RL policy increased videos watched for\nmore than 10 seconds by 15.81\\% and improved session depth by 4.71\\% for\nsessions with at least 10 interactions. As a result, RecoMind presents a\nsystematic and scalable approach for embedding RL into web-scale recommendation\nsystems, showing great promise for optimizing session-based user satisfaction.", "AI": {"tldr": "RecoMind\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u62df\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4f1a\u8bdd\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "motivation": "\u73b0\u6709\u63a8\u8350\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff0c\u96be\u4ee5\u4f18\u5316\u957f\u671f\u76ee\u6807\uff08\u5982\u4f1a\u8bdd\u53c2\u4e0e\u5ea6\uff09\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u5728\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u9762\u4e34\u5de5\u7a0b\u590d\u6742\u6027\u6311\u6218\u3002", "method": "\u5229\u7528\u73b0\u6709\u63a8\u8350\u6a21\u578b\u6784\u5efa\u6a21\u62df\u73af\u5883\uff0c\u5e76\u901a\u8fc7\u81ea\u5b9a\u4e49\u63a2\u7d22\u7b56\u7565\u9ad8\u6548\u63a2\u7d22\u5927\u89c4\u6a21\u52a8\u4f5c\u7a7a\u95f4\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b56\u7565\u3002", "result": "\u79bb\u7ebf\u6a21\u62df\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\uff0cRecoMind\u8bad\u7ec3\u7684RL\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u89c6\u9891\u89c2\u770b\u65f6\u957f\u548c\u4f1a\u8bdd\u6df1\u5ea6\u5747\u6709\u63d0\u5347\u3002", "conclusion": "RecoMind\u4e3a\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u4f18\u5316\u4f1a\u8bdd\u76ee\u6807\u3002"}}
{"id": "2508.00230", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00230", "abs": "https://arxiv.org/abs/2508.00230", "authors": ["Paul Albert", "Frederic Z. Zhang", "Hemanth Saratchandran", "Anton van den Hengel", "Ehsan Abbasnejad"], "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product", "comment": "To appear in ICCV 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.", "AI": {"tldr": "\u6bd4\u8f83\u4e86\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u4e0e\u5168\u79e9PEFT\u65b9\u6cd5\uff0c\u63d0\u51faKRAdapter\u7b97\u6cd5\u4ee5\u89e3\u51b3LoRA\u5728\u8fd1\u4f3c\u9ad8\u6709\u6548\u79e9\u77e9\u9635\u65f6\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u5927\u6a21\u578b\u4e0a\u9a8c\u8bc1\u5176\u6027\u80fd\u3002", "motivation": "LoRA\u5728\u591a\u6a21\u6001\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u6709\u6548\u79e9\u77e9\u9635\u8fd1\u4f3c\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5408\u6210\u77e9\u9635\u8fd1\u4f3c\u57fa\u51c6\u5bf9\u6bd4\u5168\u79e9\u548c\u4f4e\u79e9PEFT\u65b9\u6cd5\uff0c\u63d0\u51fa\u57fa\u4e8eKhatri-Rao\u79ef\u7684KRAdapter\u7b97\u6cd5\u3002", "result": "KRAdapter\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8eLoRA\uff0c\u5c24\u5176\u5728\u672a\u89c1\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u3002", "conclusion": "KRAdapter\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684PEFT\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5341\u4ebf\u7ea7\u53c2\u6570\u6a21\u578b\u7684\u5fae\u8c03\u3002"}}
{"id": "2508.00270", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00270", "abs": "https://arxiv.org/abs/2508.00270", "authors": ["Robin Schmucker", "Nimish Pachapurkar", "Shanmuga Bala", "Miral Shah", "Tom Mitchell"], "title": "Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring", "comment": null, "summary": "We present an online tutoring system that learns to provide effective\nfeedback to students after they answer questions incorrectly. Using data from\none million students, the system learns which assistance action (e.g., one of\nmultiple hints) to provide for each question to optimize student learning.\nEmploying the multi-armed bandit (MAB) framework and offline policy evaluation,\nwe assess 43,000 assistance actions, and identify trade-offs between assistance\npolicies optimized for different student outcomes (e.g., response correctness,\nsession completion). We design an algorithm that for each question decides on a\nsuitable policy training objective to enhance students' immediate second\nattempt success and overall practice session performance. We evaluate the\nresulting MAB policies in 166,000 practice sessions, verifying significant\nimprovements in student outcomes. While MAB policies optimize feedback for the\noverall student population, we further investigate whether contextual bandit\n(CB) policies can enhance outcomes by personalizing feedback based on\nindividual student features (e.g., ability estimates, response times). Using\ncausal inference, we examine (i) how effects of assistance actions vary across\nstudents and (ii) whether CB policies, which leverage such effect\nheterogeneity, outperform MAB policies. While our analysis reveals that some\nactions for some questions exhibit effect heterogeneity, effect sizes may often\nbe too small for CB policies to provide significant improvements beyond what\nwell-optimized MAB policies that deliver the same action to all students\nalready achieve. We discuss insights gained from deploying data-driven systems\nat scale and implications for future refinements. Today, the teaching policies\noptimized by our system support thousands of students daily.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5728\u7ebf\u8f85\u5bfc\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u548c\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\u4f18\u5316\u5b66\u751f\u53cd\u9988\uff0c\u63d0\u5347\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u4f18\u5316\u5b66\u751f\u53cd\u9988\uff0c\u4ee5\u63d0\u9ad8\u5b66\u4e60\u6548\u679c\u548c\u7ec3\u4e60\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u591a\u81c2\u8001\u864e\u673a\uff08MAB\uff09\u6846\u67b6\u548c\u79bb\u7ebf\u7b56\u7565\u8bc4\u4f30\uff0c\u5206\u679043,000\u4e2a\u8f85\u52a9\u52a8\u4f5c\uff0c\u5e76\u8bbe\u8ba1\u7b97\u6cd5\u9009\u62e9\u9002\u5408\u7684\u7b56\u7565\u76ee\u6807\u3002\u8fdb\u4e00\u6b65\u7814\u7a76\u4e0a\u4e0b\u6587\u8001\u864e\u673a\uff08CB\uff09\u7b56\u7565\u7684\u4e2a\u6027\u5316\u6548\u679c\u3002", "result": "\u5728166,000\u6b21\u7ec3\u4e60\u4f1a\u8bdd\u4e2d\u9a8c\u8bc1\uff0cMAB\u7b56\u7565\u663e\u8457\u63d0\u5347\u5b66\u751f\u8868\u73b0\uff0c\u4f46CB\u7b56\u7565\u7684\u4e2a\u6027\u5316\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u53cd\u9988\u4f18\u5316\u7cfb\u7edf\u5df2\u5927\u89c4\u6a21\u90e8\u7f72\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6539\u8fdb\u4e2a\u6027\u5316\u7b56\u7565\u3002"}}
{"id": "2508.00304", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00304", "abs": "https://arxiv.org/abs/2508.00304", "authors": ["Tianyin Liao", "Ziwei Zhang", "Yufei Sun", "Chunyu Hu", "Jianxin Li"], "title": "Invariant Graph Transformer for Out-of-Distribution Generalization", "comment": null, "summary": "Graph Transformers (GTs) have demonstrated great effectiveness across various\ngraph analytical tasks. However, the existing GTs focus on training and testing\ngraph data originated from the same distribution, but fail to generalize under\ndistribution shifts. Graph invariant learning, aiming to capture generalizable\ngraph structural patterns with labels under distribution shifts, is potentially\na promising solution, but how to design attention mechanisms and positional and\nstructural encodings (PSEs) based on graph invariant learning principles\nremains challenging. To solve these challenges, we introduce Graph\nOut-Of-Distribution generalized Transformer (GOODFormer), aiming to learn\ngeneralized graph representations by capturing invariant relationships between\npredictive graph structures and labels through jointly optimizing three\nmodules. Specifically, we first develop a GT-based entropy-guided invariant\nsubgraph disentangler to separate invariant and variant subgraphs while\npreserving the sharpness of the attention function. Next, we design an evolving\nsubgraph positional and structural encoder to effectively and efficiently\ncapture the encoding information of dynamically changing subgraphs during\ntraining. Finally, we propose an invariant learning module utilizing subgraph\nnode representations and encodings to derive generalizable graph\nrepresentations that can to unseen graphs. We also provide theoretical\njustifications for our method. Extensive experiments on benchmark datasets\ndemonstrate the superiority of our method over state-of-the-art baselines under\ndistribution shifts.", "AI": {"tldr": "GOODFormer\u662f\u4e00\u79cd\u56feTransformer\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e09\u4e2a\u6a21\u5757\uff0c\u89e3\u51b3\u56fe\u6570\u636e\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u56feTransformer\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u8981\u6355\u6349\u4e0d\u53d8\u6027\u56fe\u7ed3\u6784\u6a21\u5f0f\u3002", "method": "\u5f00\u53d1\u71b5\u5f15\u5bfc\u4e0d\u53d8\u5b50\u56fe\u89e3\u8026\u5668\u3001\u52a8\u6001\u5b50\u56fe\u7f16\u7801\u5668\u548c\u4e0d\u53d8\u5b66\u4e60\u6a21\u5757\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u5e94\u5206\u5e03\u504f\u79fb\u3002", "conclusion": "GOODFormer\u901a\u8fc7\u4e0d\u53d8\u6027\u5b66\u4e60\u5b9e\u73b0\u56fe\u6570\u636e\u7684\u6cdb\u5316\u8868\u793a\u3002"}}
{"id": "2508.00325", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.00325", "abs": "https://arxiv.org/abs/2508.00325", "authors": ["Yongquan Qu", "Matthieu Blanke", "Sara Shamekh", "Pierre Gentine"], "title": "PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models", "comment": null, "summary": "Earth system modeling presents a fundamental challenge in scientific\ncomputing: capturing complex, multiscale nonlinear dynamics in computationally\nefficient models while minimizing forecast errors caused by necessary\nsimplifications. Even the most powerful AI- or physics-based forecast system\nsuffer from gradual error accumulation. Data assimilation (DA) aims to mitigate\nthese errors by optimally blending (noisy) observations with prior model\nforecasts, but conventional variational methods often assume Gaussian error\nstatistics that fail to capture the true, non-Gaussian behavior of chaotic\ndynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates\n(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance\nmisfit on new observations) with (2) a single forward pass through a pretrained\ngenerative prior conditioned on the background forecast via a conditional\nWasserstein coupling. This strategy relaxes restrictive statistical assumptions\nand leverages rich historical data without requiring an explicit regularization\nfunctional, and it also avoids the need to backpropagate gradients through the\ncomplex neural network that encodes the prior during assimilation cycles.\nExperiments on standard chaotic testbeds demonstrate that this strategy\nconsistently reduces forecast errors across a range of observation sparsities\nand noise levels, outperforming classical variational methods.", "AI": {"tldr": "PnP-DA\u662f\u4e00\u79cd\u6570\u636e\u540c\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u68af\u5ea6\u66f4\u65b0\u548c\u751f\u6210\u5148\u9a8c\uff0c\u51cf\u5c11\u5730\u7403\u7cfb\u7edf\u5efa\u6a21\u4e2d\u7684\u8bef\u5dee\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u5730\u7403\u7cfb\u7edf\u5efa\u6a21\u4e2d\u56e0\u7b80\u5316\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5047\u8bbe\u9ad8\u65af\u8bef\u5dee\u7edf\u8ba1\uff0c\u65e0\u6cd5\u6355\u6349\u6df7\u6c8c\u7cfb\u7edf\u7684\u771f\u5b9e\u884c\u4e3a\u3002", "method": "PnP-DA\u4ea4\u66ff\u4f7f\u7528\u68af\u5ea6\u66f4\u65b0\u548c\u9884\u8bad\u7ec3\u7684\u751f\u6210\u5148\u9a8c\uff0c\u907f\u514d\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u7684\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5728\u6df7\u6c8c\u6d4b\u8bd5\u4e2d\uff0cPnP-DA\u663e\u8457\u51cf\u5c11\u9884\u6d4b\u8bef\u5dee\uff0c\u4f18\u4e8e\u7ecf\u5178\u53d8\u5206\u65b9\u6cd5\u3002", "conclusion": "PnP-DA\u901a\u8fc7\u653e\u677e\u7edf\u8ba1\u5047\u8bbe\u548c\u5229\u7528\u5386\u53f2\u6570\u636e\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u636e\u540c\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00331", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00331", "abs": "https://arxiv.org/abs/2508.00331", "authors": ["George Wang", "Garrett Baker", "Andrew Gordon", "Daniel Murfet"], "title": "Embryology of a Language Model", "comment": null, "summary": "Understanding how language models develop their internal computational\nstructure is a central problem in the science of deep learning. While\nsusceptibilities, drawn from statistical physics, offer a promising analytical\ntool, their full potential for visualizing network organization remains\nuntapped. In this work, we introduce an embryological approach, applying UMAP\nto the susceptibility matrix to visualize the model's structural development\nover training. Our visualizations reveal the emergence of a clear ``body\nplan,'' charting the formation of known features like the induction circuit and\ndiscovering previously unknown structures, such as a ``spacing fin'' dedicated\nto counting space tokens. This work demonstrates that susceptibility analysis\ncan move beyond validation to uncover novel mechanisms, providing a powerful,\nholistic lens for studying the developmental principles of complex neural\nnetworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u80da\u80ce\u5b66\u65b9\u6cd5\uff0c\u5229\u7528UMAP\u53ef\u89c6\u5316\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u7684\u7ed3\u6784\u53d1\u5c55\uff0c\u63ed\u793a\u4e86\u65b0\u7684\u5185\u90e8\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8ba1\u7b97\u7ed3\u6784\u7684\u5f62\u6210\u673a\u5236\uff0c\u63a2\u7d22\u7edf\u8ba1\u7269\u7406\u5b66\u4e2d\u7684\u654f\u611f\u6027\u5206\u6790\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5e94\u7528UMAP\u5bf9\u654f\u611f\u6027\u77e9\u9635\u8fdb\u884c\u5206\u6790\uff0c\u53ef\u89c6\u5316\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u53d1\u5c55\u3002", "result": "\u53d1\u73b0\u4e86\u5df2\u77e5\u7279\u5f81\uff08\u5982\u611f\u5e94\u7535\u8def\uff09\u7684\u5f62\u6210\uff0c\u4ee5\u53ca\u65b0\u7ed3\u6784\uff08\u5982\u7528\u4e8e\u8ba1\u6570\u7a7a\u683c\u6807\u8bb0\u7684\u201c\u95f4\u8ddd\u9ccd\u201d\uff09\u3002", "conclusion": "\u654f\u611f\u6027\u5206\u6790\u4e0d\u4ec5\u80fd\u9a8c\u8bc1\u6a21\u578b\uff0c\u8fd8\u80fd\u63ed\u793a\u65b0\u673a\u5236\uff0c\u4e3a\u7814\u7a76\u590d\u6742\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u80b2\u539f\u7406\u63d0\u4f9b\u4e86\u5168\u9762\u89c6\u89d2\u3002"}}
{"id": "2508.00350", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00350", "abs": "https://arxiv.org/abs/2508.00350", "authors": ["Qilin Liao", "Shuo Yang", "Bo Zhao", "Ping Luo", "Hengshuang Zhao"], "title": "BOOD: Boundary-based Out-Of-Distribution Data Generation", "comment": "14 pages, 8 figures, To be published in the Proceedings of the\n  International Conference on Machine Learning (ICML) 2025", "summary": "Harnessing the power of diffusion models to synthesize auxiliary training\ndata based on latent space features has proven effective in enhancing\nout-of-distribution (OOD) detection performance. However, extracting effective\nfeatures outside the in-distribution (ID) boundary in latent space remains\nchallenging due to the difficulty of identifying decision boundaries between\nclasses. This paper proposes a novel framework called Boundary-based\nOut-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD\nfeatures and generates human-compatible outlier images using diffusion models.\nBOOD first learns a text-conditioned latent feature space from the ID dataset,\nselects ID features closest to the decision boundary, and perturbs them to\ncross the decision boundary to form OOD features. These synthetic OOD features\nare then decoded into images in pixel space by a diffusion model. Compared to\nprevious works, BOOD provides a more training efficient strategy for\nsynthesizing informative OOD features, facilitating clearer distinctions\nbetween ID and OOD data. Extensive experimental results on common benchmarks\ndemonstrate that BOOD surpasses the state-of-the-art method significantly,\nachieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%\nimprovement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.", "AI": {"tldr": "BOOD\u6846\u67b6\u5229\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cfOOD\u7279\u5f81\u548c\u56fe\u50cf\uff0c\u663e\u8457\u63d0\u5347OOD\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u63d0\u53d6\u6709\u6548\u7684OOD\u7279\u5f81\u4ecd\u5177\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6e05\u6670\u754c\u5b9a\u51b3\u7b56\u8fb9\u754c\u3002", "method": "BOOD\u901a\u8fc7\u5b66\u4e60\u6587\u672c\u6761\u4ef6\u5316\u7684\u6f5c\u5728\u7279\u5f81\u7a7a\u95f4\uff0c\u6270\u52a8\u63a5\u8fd1\u51b3\u7b56\u8fb9\u754c\u7684ID\u7279\u5f81\u751f\u6210OOD\u7279\u5f81\uff0c\u5e76\u7528\u6269\u6563\u6a21\u578b\u89e3\u7801\u4e3a\u56fe\u50cf\u3002", "result": "\u5728CIFAR-100\u6570\u636e\u96c6\u4e0a\uff0cBOOD\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cFPR95\u964d\u4f4e29.64%\uff0cAUROC\u63d0\u53477.27%\u3002", "conclusion": "BOOD\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u751f\u6210OOD\u7279\u5f81\u7684\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86OOD\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2508.00357", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00357", "abs": "https://arxiv.org/abs/2508.00357", "authors": ["Yoonhyuk Choi", "Jiho Choi", "Chong-Kwon Kim"], "title": "Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization", "comment": null, "summary": "Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct\nnode features, particularly on heterophilic graphs where adjacent nodes often\nhave dissimilar labels. Although sheaf neural networks partially mitigate this\nproblem, they typically rely on static or heavily parameterized sheaf\nstructures that hinder generalization and scalability. Existing sheaf-based\nmodels either predefine restriction maps or introduce excessive complexity, yet\nfail to provide rigorous stability guarantees. In this paper, we introduce a\nnovel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified\narchitecture that combines cellular-sheaf message passing with several\nmechanisms, including optimal transport-based lifting, variance-reduced\ndiffusion, and PAC-Bayes spectral regularization for robust semi-supervised\nnode classification. We establish performance bounds theoretically and\ndemonstrate that the resulting bound-aware objective can be achieved via\nend-to-end training in linear computational complexity. Experiments on nine\nhomophilic and heterophilic benchmarks show that SGPC outperforms\nstate-of-the-art spectral and sheaf-based GNNs while providing certified\nconfidence intervals on unseen nodes.", "AI": {"tldr": "SGPC\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ec6\u80de\u9798\u6d88\u606f\u4f20\u9012\u7684GNN\u67b6\u6784\uff0c\u901a\u8fc7\u4f18\u5316\u4f20\u8f93\u63d0\u5347\u3001\u65b9\u5dee\u51cf\u5c11\u6269\u6563\u548cPAC-Bayes\u8c31\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u5f02\u8d28\u56fe\u4e0a\u7684\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3GNN\u5728\u5f02\u8d28\u56fe\u4e0a\u56e0\u8fc7\u5e73\u6ed1\u5bfc\u81f4\u7684\u8282\u70b9\u7279\u5f81\u5d29\u6e83\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u73b0\u6709\u9798\u7f51\u7edc\u6a21\u578b\u7684\u9759\u6001\u6216\u8fc7\u5ea6\u53c2\u6570\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faSGPC\u67b6\u6784\uff0c\u7ed3\u5408\u7ec6\u80de\u9798\u6d88\u606f\u4f20\u9012\u3001\u4f18\u5316\u4f20\u8f93\u63d0\u5347\u3001\u65b9\u5dee\u51cf\u5c11\u6269\u6563\u548cPAC-Bayes\u8c31\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u8bad\u7ec3\u3002", "result": "\u5728\u4e5d\u4e2a\u540c\u8d28\u548c\u5f02\u8d28\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSGPC\u4f18\u4e8e\u73b0\u6709\u5149\u8c31\u548c\u9798\u57faGNN\uff0c\u5e76\u63d0\u4f9b\u672a\u89c1\u8fc7\u8282\u70b9\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "conclusion": "SGPC\u901a\u8fc7\u7406\u8bba\u6027\u80fd\u754c\u9650\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u8d28\u56fe\u4e0a\u7684\u8282\u70b9\u5206\u7c7b\u6027\u80fd\u3002"}}
{"id": "2508.00364", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00364", "abs": "https://arxiv.org/abs/2508.00364", "authors": ["Chanyoung Yoon", "Sangbong Yoo", "Soobin Yim", "Chansoo Kim", "Yun Jang"], "title": "OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions", "comment": null, "summary": "Designing residential interiors strongly impacts occupant satisfaction but\nremains challenging due to unstructured spatial layouts, high computational\ndemands, and reliance on expert knowledge. Existing methods based on\noptimization or deep learning are either computationally expensive or\nconstrained by data scarcity. Reinforcement learning (RL) approaches often\nlimit furniture placement to discrete positions and fail to incorporate design\nprinciples adequately. We propose OID-PPO, a novel RL framework for Optimal\nInterior Design using Proximal Policy Optimization, which integrates\nexpert-defined functional and visual guidelines into a structured reward\nfunction. OID-PPO utilizes a diagonal Gaussian policy for continuous and\nflexible furniture placement, effectively exploring latent environmental\ndynamics under partial observability. Experiments conducted across diverse room\nshapes and furniture configurations demonstrate that OID-PPO significantly\noutperforms state-of-the-art methods in terms of layout quality and\ncomputational efficiency. Ablation studies further demonstrate the impact of\nstructured guideline integration and reveal the distinct contributions of\nindividual design constraints.", "AI": {"tldr": "OID-PPO\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5ba4\u5185\u8bbe\u8ba1\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u4e13\u5bb6\u5b9a\u4e49\u7684\u529f\u80fd\u548c\u89c6\u89c9\u51c6\u5219\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e03\u5c40\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f4f\u5b85\u5ba4\u5185\u8bbe\u8ba1\u5bf9\u5c45\u4f4f\u8005\u6ee1\u610f\u5ea6\u5f71\u54cd\u91cd\u5927\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u3001\u6570\u636e\u7a00\u7f3a\u6027\u548c\u8bbe\u8ba1\u539f\u5219\u6574\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faOID-PPO\u6846\u67b6\uff0c\u5229\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u548c\u7ed3\u6784\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5b9e\u73b0\u8fde\u7eed\u7075\u6d3b\u7684\u5bb6\u5177\u5e03\u5c40\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cOID-PPO\u5728\u591a\u6837\u5316\u7684\u623f\u95f4\u5f62\u72b6\u548c\u5bb6\u5177\u914d\u7f6e\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "OID-PPO\u901a\u8fc7\u6574\u5408\u8bbe\u8ba1\u51c6\u5219\u548c\u4f18\u5316\u7b56\u7565\uff0c\u4e3a\u5ba4\u5185\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00392", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00392", "abs": "https://arxiv.org/abs/2508.00392", "authors": ["Lijun Zhang", "Wenhao Yang", "Guanghui Wang", "Wei Jiang", "Zhi-Hua Zhou"], "title": "Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions", "comment": "arXiv admin note: text overlap with arXiv:1906.10851", "summary": "To deal with changing environments, a new performance measure -- adaptive\nregret, defined as the maximum static regret over any interval, was proposed in\nonline learning. Under the setting of online convex optimization, several\nalgorithms have been successfully developed to minimize the adaptive regret.\nHowever, existing algorithms lack universality in the sense that they can only\nhandle one type of convex functions and need apriori knowledge of parameters,\nwhich hinders their application in real-world scenarios. To address this\nlimitation, this paper investigates universal algorithms with dual adaptivity,\nwhich automatically adapt to the property of functions (convex, exponentially\nconcave, or strongly convex), as well as the nature of environments (stationary\nor changing). Specifically, we propose a meta-expert framework for dual\nadaptive algorithms, where multiple experts are created dynamically and\naggregated by a meta-algorithm. The meta-algorithm is required to yield a\nsecond-order bound, which can accommodate unknown function types. We further\nincorporate the technique of sleeping experts to capture the changing\nenvironments. For the construction of experts, we introduce two strategies\n(increasing the number of experts or enhancing the capabilities of experts) to\nachieve universality. Theoretical analysis shows that our algorithms are able\nto minimize the adaptive regret for multiple types of convex functions\nsimultaneously, and also allow the type of functions to switch between rounds.\nMoreover, we extend our meta-expert framework to online composite optimization,\nand develop a universal algorithm for minimizing the adaptive regret of\ncomposite functions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u53cc\u91cd\u9002\u5e94\u6027\u7684\u901a\u7528\u7b97\u6cd5\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u5b66\u4e60\u4e2d\u6700\u5c0f\u5316\u81ea\u9002\u5e94\u9057\u61be\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u51f8\u51fd\u6570\u7c7b\u578b\u548c\u52a8\u6001\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u7b97\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\uff0c\u53ea\u80fd\u5904\u7406\u5355\u4e00\u7c7b\u578b\u7684\u51f8\u51fd\u6570\u4e14\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5143\u4e13\u5bb6\u6846\u67b6\u7684\u53cc\u91cd\u81ea\u9002\u5e94\u7b97\u6cd5\uff0c\u52a8\u6001\u521b\u5efa\u591a\u4e2a\u4e13\u5bb6\u5e76\u901a\u8fc7\u5143\u7b97\u6cd5\u805a\u5408\uff0c\u7ed3\u5408\u7761\u7720\u4e13\u5bb6\u6280\u672f\u6355\u6349\u73af\u5883\u53d8\u5316\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u7b97\u6cd5\u80fd\u540c\u65f6\u6700\u5c0f\u5316\u591a\u79cd\u51f8\u51fd\u6570\u7684\u81ea\u9002\u5e94\u9057\u61be\uff0c\u5e76\u5141\u8bb8\u51fd\u6570\u7c7b\u578b\u5728\u8f6e\u6b21\u95f4\u5207\u6362\u3002", "conclusion": "\u8be5\u6846\u67b6\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u5728\u7ebf\u590d\u5408\u4f18\u5316\uff0c\u5f00\u53d1\u4e86\u9488\u5bf9\u590d\u5408\u51fd\u6570\u7684\u901a\u7528\u7b97\u6cd5\u3002"}}
{"id": "2508.00394", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00394", "abs": "https://arxiv.org/abs/2508.00394", "authors": ["Antonis Klironomos", "Baifan Zhou", "Zhipeng Tan", "Zhuoxun Zheng", "Mohamed H. Gad-Elrab", "Heiko Paulheim", "Evgeny Kharlamov"], "title": "ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs", "comment": null, "summary": "Nowadays machine learning (ML) practitioners have access to numerous ML\nlibraries available online. Such libraries can be used to create ML pipelines\nthat consist of a series of steps where each step may invoke up to several ML\nlibraries that are used for various data-driven analytical tasks. Development\nof high-quality ML pipelines is non-trivial; it requires training, ML\nexpertise, and careful development of each step. At the same time, domain\nexperts in science and engineering may not possess such ML expertise and\ntraining while they are in pressing need of ML-based analytics. In this paper,\nwe present our ExeKGLib, a Python library enhanced with a graphical interface\nlayer that allows users with minimal ML knowledge to build ML pipelines. This\nis achieved by relying on knowledge graphs that encode ML knowledge in simple\nterms accessible to non-ML experts. ExeKGLib also allows improving the\ntransparency and reusability of the built ML workflows and ensures that they\nare executable. We show the usability and usefulness of ExeKGLib by presenting\nreal use cases.", "AI": {"tldr": "ExeKGLib\u662f\u4e00\u4e2aPython\u5e93\uff0c\u901a\u8fc7\u56fe\u5f62\u754c\u9762\u548c\u77e5\u8bc6\u56fe\u8c31\u5e2e\u52a9\u975eML\u4e13\u5bb6\u6784\u5efaML\u7ba1\u9053\u3002", "motivation": "\u89e3\u51b3\u975eML\u4e13\u5bb6\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u9886\u57df\u6784\u5efa\u9ad8\u8d28\u91cfML\u7ba1\u9053\u7684\u56f0\u96be\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7f16\u7801ML\u77e5\u8bc6\uff0c\u63d0\u4f9b\u56fe\u5f62\u754c\u9762\u7b80\u5316\u64cd\u4f5c\u3002", "result": "ExeKGLib\u63d0\u9ad8\u4e86ML\u7ba1\u9053\u7684\u900f\u660e\u5ea6\u3001\u53ef\u91cd\u7528\u6027\u548c\u53ef\u6267\u884c\u6027\u3002", "conclusion": "ExeKGLib\u4e3a\u975eML\u4e13\u5bb6\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684ML\u5de5\u5177\uff0c\u5c55\u793a\u4e86\u5176\u53ef\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.00410", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00410", "abs": "https://arxiv.org/abs/2508.00410", "authors": ["Zizhuo Zhang", "Jianing Zhu", "Xinmu Ge", "Zihua Zhao", "Zhanke Zhou", "Xuan Li", "Xiao Feng", "Jiangchao Yao", "Bo Han"], "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement", "comment": null, "summary": "Although reinforcement learning with verifiable rewards (RLVR) shows promise\nin improving the reasoning ability of large language models (LLMs), the scaling\nup dilemma remains due to the reliance on human annotated labels especially for\ncomplex tasks. Recent alternatives that explore various self-reward signals\nexhibit the eliciting potential of LLM reasoning, but suffer from the\nnon-negligible collapse issue. Inspired by the success of self-supervised\nlearning, we propose \\textit{Co-Reward}, a novel RL framework that leverages\ncontrastive agreement across semantically analogical questions as a reward\nbasis. Specifically, we construct a similar question for each training sample\n(without labels) and synthesize their individual surrogate labels through a\nsimple rollout voting, and then the reward is constructed by cross-referring\nthe labels of each question pair to enforce the internal reasoning consistency\nacross analogical inputs. Intuitively, such a self-supervised reward-shaping\nmechanism increases the difficulty of learning collapse into a trivial\nsolution, and promotes stable reasoning elicitation and improvement through\nexpanding the input sample variants. Empirically, Co-Reward achieves superior\nperformance compared to other self-reward baselines on multiple reasoning\nbenchmarks and LLM series, and reaches or even surpasses ground-truth (GT)\nlabeled reward, with improvements of up to $+6.8\\%$ on MATH500 over GT reward\non Llama-3.2-3B-Instruct. Our code is publicly available at\nhttps://github.com/tmlr-group/Co-Reward.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCo-Reward\u7684\u65b0\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u8bed\u4e49\u7c7b\u6bd4\u95ee\u9898\u7684\u4e00\u81f4\u6027\u4f5c\u4e3a\u5956\u52b1\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u5956\u52b1\u65b9\u6cd5\u4e2d\u7684\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u81ea\u5956\u52b1\u57fa\u7ebf\u3002", "motivation": "\u5c3d\u7ba1\u5e26\u6709\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u5728\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6807\u7b7e\u7684\u95ee\u9898\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5c24\u4e3a\u7a81\u51fa\u3002\u73b0\u6709\u7684\u81ea\u5956\u52b1\u65b9\u6cd5\u867d\u7136\u5c55\u793a\u4e86LLM\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u5d29\u6e83\u95ee\u9898\u3002", "method": "\u63d0\u51faCo-Reward\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u8bed\u4e49\u7c7b\u6bd4\u95ee\u9898\u5bf9\uff0c\u5229\u7528\u7b80\u5355\u7684\u6295\u7968\u673a\u5236\u751f\u6210\u4ee3\u7406\u6807\u7b7e\uff0c\u5e76\u901a\u8fc7\u4ea4\u53c9\u5f15\u7528\u95ee\u9898\u5bf9\u7684\u6807\u7b7e\u6765\u5f3a\u5316\u5185\u90e8\u63a8\u7406\u4e00\u81f4\u6027\u3002\u8fd9\u79cd\u81ea\u76d1\u7763\u5956\u52b1\u673a\u5236\u589e\u52a0\u4e86\u5b66\u4e60\u5d29\u6e83\u4e3a\u5e73\u51e1\u89e3\u7684\u96be\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548cLLM\u7cfb\u5217\u4e2d\uff0cCo-Reward\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u81ea\u5956\u52b1\u57fa\u7ebf\uff0c\u5e76\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fbe\u5230\u6216\u8d85\u8fc7\u57fa\u4e8e\u771f\u5b9e\u6807\u7b7e\u7684\u5956\u52b1\uff0c\u5982\u5728MATH500\u4e0a\u5bf9Llama-3.2-3B-Instruct\u7684\u6539\u8fdb\u9ad8\u8fbe+6.8%\u3002", "conclusion": "Co-Reward\u901a\u8fc7\u81ea\u76d1\u7763\u5956\u52b1\u673a\u5236\u6709\u6548\u63d0\u5347\u4e86LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u81ea\u5956\u52b1\u65b9\u6cd5\u4e2d\u7684\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.00415", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00415", "abs": "https://arxiv.org/abs/2508.00415", "authors": ["Yue Yang", "Yuxiang Lin", "Ying Zhang", "Zihan Su", "Chang Chuan Goh", "Tangtangfang Fang", "Anthony Graham Bellotti", "Boon Giin Lee"], "title": "Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection", "comment": null, "summary": "Prediction of post-loan default is an important task in credit risk\nmanagement, and can be addressed by detection of financial anomalies using\nmachine learning. This study introduces a ResE-BiLSTM model, using a sliding\nwindow technique, and is evaluated on 44 independent cohorts from the extensive\nFreddie Mac US mortgage dataset, to improve prediction performance. The\nResE-BiLSTM is compared with five baseline models: Long Short-Term Memory\n(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks\n(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including\nAccuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to\nevaluate the contribution of individual components in the ResE-BiLSTM\narchitecture. Additionally, SHAP analysis was employed to interpret the\nunderlying features the model relied upon for its predictions. Experimental\nresults demonstrate that ResE-BiLSTM achieves superior predictive performance\ncompared to baseline models, underscoring its practical value and applicability\nin real-world scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aResE-BiLSTM\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u8d37\u6b3e\u8fdd\u7ea6\uff0c\u901a\u8fc7\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\u548c\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u9a8c\u8bc1\u5176\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8d37\u6b3e\u8fdd\u7ea6\u9884\u6d4b\u5bf9\u4fe1\u7528\u98ce\u9669\u7ba1\u7406\u81f3\u5173\u91cd\u8981\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u53ef\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u4f7f\u7528ResE-BiLSTM\u6a21\u578b\uff0c\u7ed3\u5408\u6ed1\u52a8\u7a97\u53e3\u6280\u672f\uff0c\u5e76\u5728Freddie Mac\u62b5\u62bc\u8d37\u6b3e\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4e0e\u4e94\u79cd\u57fa\u7ebf\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "ResE-BiLSTM\u5728\u51c6\u786e\u6027\u3001\u7cbe\u786e\u5ea6\u3001\u53ec\u56de\u7387\u3001F1\u548cAUC\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "ResE-BiLSTM\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2508.00472", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00472", "abs": "https://arxiv.org/abs/2508.00472", "authors": ["Leonidas Akritidis", "Panayiotis Bozanis"], "title": "A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces", "comment": null, "summary": "The tabular form constitutes the standard way of representing data in\nrelational database systems and spreadsheets. But, similarly to other forms,\ntabular data suffers from class imbalance, a problem that causes serious\nperformance degradation in a wide variety of machine learning tasks. One of the\nmost effective solutions dictates the usage of Generative Adversarial Networks\n(GANs) in order to synthesize artificial data instances for the\nunder-represented classes. Despite their good performance, none of the proposed\nGAN models takes into account the vector subspaces of the input samples in the\nreal data space, leading to data generation in arbitrary locations. Moreover,\nthe class labels are treated in the same manner as the other categorical\nvariables during training, so conditional sampling by class is rendered less\neffective. To overcome these problems, this study presents ctdGAN, a\nconditional GAN for alleviating class imbalance in tabular datasets. Initially,\nctdGAN executes a space partitioning step to assign cluster labels to the input\nsamples. Subsequently, it utilizes these labels to synthesize samples via a\nnovel probabilistic sampling strategy and a new loss function that penalizes\nboth cluster and class mis-predictions. In this way, ctdGAN is trained to\ngenerate samples in subspaces that resemble those of the original data\ndistribution. We also introduce several other improvements, including a simple,\nyet effective cluster-wise scaling technique that captures multiple feature\nmodes without affecting data dimensionality. The exhaustive evaluation of\nctdGAN with 14 imbalanced datasets demonstrated its superiority in generating\nhigh fidelity samples and improving classification accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3actdGAN\u7684\u6761\u4ef6GAN\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u8868\u683c\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u901a\u8fc7\u7a7a\u95f4\u5206\u533a\u548c\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0cctdGAN\u80fd\u751f\u6210\u66f4\u63a5\u8fd1\u539f\u59cb\u6570\u636e\u5206\u5e03\u7684\u5b50\u7a7a\u95f4\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u8868\u683c\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u7684\u6027\u80fd\u3002\u73b0\u6709GAN\u6a21\u578b\u672a\u8003\u8651\u8f93\u5165\u6837\u672c\u7684\u5411\u91cf\u5b50\u7a7a\u95f4\uff0c\u5bfc\u81f4\u751f\u6210\u6570\u636e\u4f4d\u7f6e\u4e0d\u51c6\u786e\uff0c\u4e14\u7c7b\u522b\u6807\u7b7e\u5904\u7406\u65b9\u5f0f\u4e0d\u591f\u6709\u6548\u3002", "method": "ctdGAN\u901a\u8fc7\u7a7a\u95f4\u5206\u533a\u4e3a\u8f93\u5165\u6837\u672c\u5206\u914d\u805a\u7c7b\u6807\u7b7e\uff0c\u5229\u7528\u65b0\u7684\u6982\u7387\u91c7\u6837\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\u751f\u6210\u6837\u672c\uff0c\u5e76\u7ed3\u5408\u805a\u7c7b\u7f29\u653e\u6280\u672f\u6355\u83b7\u591a\u7279\u5f81\u6a21\u5f0f\u3002", "result": "\u572814\u4e2a\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cctdGAN\u80fd\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u5206\u7c7b\u51c6\u786e\u7387\u3002", "conclusion": "ctdGAN\u901a\u8fc7\u6539\u8fdb\u751f\u6210\u7b56\u7565\u548c\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8868\u683c\u6570\u636e\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u751f\u6210\u6837\u672c\u8d28\u91cf\u9ad8\u4e14\u5206\u7c7b\u6027\u80fd\u4f18\u8d8a\u3002"}}
{"id": "2508.00507", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00507", "abs": "https://arxiv.org/abs/2508.00507", "authors": ["Yiming Xu", "Jiarun Chen", "Zhen Peng", "Zihan Chen", "Qika Lin", "Lan Ma", "Bin Shi", "Bo Dong"], "title": "Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection", "comment": "Accepted by ACM Multimedia 2025 (MM '25)", "summary": "The natural combination of intricate topological structures and rich textual\ninformation in text-attributed graphs (TAGs) opens up a novel perspective for\ngraph anomaly detection (GAD). However, existing GAD methods primarily focus on\ndesigning complex optimization objectives within the graph domain, overlooking\nthe complementary value of the textual modality, whose features are often\nencoded by shallow embedding techniques, such as bag-of-words or skip-gram, so\nthat semantic context related to anomalies may be missed. To unleash the\nenormous potential of textual modality, large language models (LLMs) have\nemerged as promising alternatives due to their strong semantic understanding\nand reasoning capabilities. Nevertheless, their application to TAG anomaly\ndetection remains nascent, and they struggle to encode high-order structural\ninformation inherent in graphs due to input length constraints. For\nhigh-quality anomaly detection in TAGs, we propose CoLL, a novel framework that\ncombines LLMs and graph neural networks (GNNs) to leverage their complementary\nstrengths. CoLL employs multi-LLM collaboration for evidence-augmented\ngeneration to capture anomaly-relevant contexts while delivering human-readable\nrationales for detected anomalies. Moreover, CoLL integrates a GNN equipped\nwith a gating mechanism to adaptively fuse textual features with evidence while\npreserving high-order topological information. Extensive experiments\ndemonstrate the superiority of CoLL, achieving an average improvement of 13.37%\nin AP. This study opens a new avenue for incorporating LLMs in advancing GAD.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faCoLL\u6846\u67b6\uff0c\u7ed3\u5408LLMs\u548cGNNs\uff0c\u7528\u4e8e\u6587\u672c\u5c5e\u6027\u56fe\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5ffd\u89c6\u6587\u672c\u6a21\u6001\u7684\u6f5c\u529b\uff0c\u4e14\u6d45\u5c42\u6587\u672c\u7f16\u7801\u53ef\u80fd\u4e22\u5931\u8bed\u4e49\u4fe1\u606f\u3002LLMs\u867d\u5f3a\u4f46\u96be\u4ee5\u5904\u7406\u56fe\u7ed3\u6784\u4fe1\u606f\u3002", "method": "CoLL\u6846\u67b6\u901a\u8fc7\u591aLLM\u534f\u4f5c\u751f\u6210\u8bc1\u636e\u589e\u5f3a\u7684\u6587\u672c\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u5e26\u95e8\u63a7\u673a\u5236\u7684GNN\u878d\u5408\u6587\u672c\u4e0e\u56fe\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u663e\u793aCoLL\u5e73\u5747AP\u63d0\u534713.37%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CoLL\u4e3aLLMs\u5728\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00513", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00513", "abs": "https://arxiv.org/abs/2508.00513", "authors": ["Yiming Xu", "Xu Hua", "Zhen Peng", "Bin Shi", "Jiarun Chen", "Xingbo Fu", "Song Wang", "Bo Dong"], "title": "Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning", "comment": "Accepted by ECAI 2025", "summary": "The widespread application of graph data in various high-risk scenarios has\nincreased attention to graph anomaly detection (GAD). Faced with real-world\ngraphs that often carry node descriptions in the form of raw text sequences,\ntermed text-attributed graphs (TAGs), existing graph anomaly detection\npipelines typically involve shallow embedding techniques to encode such textual\ninformation into features, and then rely on complex self-supervised tasks\nwithin the graph domain to detect anomalies. However, this text encoding\nprocess is separated from the anomaly detection training objective in the graph\ndomain, making it difficult to ensure that the extracted textual features focus\non GAD-relevant information, seriously constraining the detection capability.\nHow to seamlessly integrate raw text and graph topology to unleash the vast\npotential of cross-modal data in TAGs for anomaly detection poses a challenging\nissue. This paper presents a novel end-to-end paradigm for text-attributed\ngraph anomaly detection, named CMUCL. We simultaneously model data from both\ntext and graph structures, and jointly train text and graph encoders by\nleveraging cross-modal and uni-modal multi-scale consistency to uncover\npotential anomaly-related information. Accordingly, we design an anomaly score\nestimator based on inconsistency mining to derive node-specific anomaly scores.\nConsidering the lack of benchmark datasets tailored for anomaly detection on\nTAGs, we release 8 datasets to facilitate future research. Extensive\nevaluations show that CMUCL significantly advances in text-attributed graph\nanomaly detection, delivering an 11.13% increase in average accuracy (AP) over\nthe suboptimal.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCMUCL\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u7528\u4e8e\u6587\u672c\u5c5e\u6027\u56fe\u7684\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u6587\u672c\u548c\u56fe\u7f16\u7801\u5668\uff0c\u5229\u7528\u8de8\u6a21\u6001\u548c\u591a\u5c3a\u5ea6\u4e00\u81f4\u6027\u63d0\u5347\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6587\u672c\u5c5e\u6027\u56fe\u65f6\uff0c\u6587\u672c\u7f16\u7801\u4e0e\u5f02\u5e38\u68c0\u6d4b\u76ee\u6807\u5206\u79bb\uff0c\u5bfc\u81f4\u68c0\u6d4b\u80fd\u529b\u53d7\u9650\u3002\u5982\u4f55\u6574\u5408\u6587\u672c\u548c\u56fe\u62d3\u6251\u4fe1\u606f\u4ee5\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6548\u679c\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51faCMUCL\u65b9\u6cd5\uff0c\u8054\u5408\u8bad\u7ec3\u6587\u672c\u548c\u56fe\u7f16\u7801\u5668\uff0c\u5229\u7528\u8de8\u6a21\u6001\u548c\u591a\u5c3a\u5ea6\u4e00\u81f4\u6027\u6316\u6398\u5f02\u5e38\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u4e0d\u4e00\u81f4\u6027\u6316\u6398\u7684\u5f02\u5e38\u8bc4\u5206\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCMUCL\u5728\u6587\u672c\u5c5e\u6027\u56fe\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u51c6\u786e\u7387\uff08AP\uff09\u63d0\u534711.13%\u3002", "conclusion": "CMUCL\u901a\u8fc7\u6574\u5408\u6587\u672c\u548c\u56fe\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u4e868\u4e2a\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.00523", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00523", "abs": "https://arxiv.org/abs/2508.00523", "authors": ["Sifan Yang", "Yuanyu Wan", "Lijun Zhang"], "title": "Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting", "comment": null, "summary": "We investigate the online nonsubmodular optimization with delayed feedback in\nthe bandit setting, where the loss function is $\\alpha$-weakly DR-submodular\nand $\\beta$-weakly DR-supermodular. Previous work has established an\n$(\\alpha,\\beta)$-regret bound of $\\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is\nthe dimensionality and $d$ is the maximum delay. However, its regret bound\nrelies on the maximum delay and is thus sensitive to irregular delays.\nAdditionally, it couples the effects of delays and bandit feedback as its bound\nis the product of the delay term and the $\\mathcal{O}(nT^{2/3})$ regret bound\nin the bandit setting without delayed feedback. In this paper, we develop two\nalgorithms to address these limitations, respectively. Firstly, we propose a\nnovel method, namely DBGD-NF, which employs the one-point gradient estimator\nand utilizes all the available estimated gradients in each round to update the\ndecision. It achieves a better $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ regret\nbound, which is relevant to the average delay $\\bar{d} =\n\\frac{1}{T}\\sum_{t=1}^T d_t\\leq d$. Secondly, we extend DBGD-NF by employing a\nblocking update mechanism to decouple the joint effect of the delays and bandit\nfeedback, which enjoys an $\\mathcal{O}(n(T^{2/3} + \\sqrt{dT}))$ regret bound.\nWhen $d = \\mathcal{O}(T^{1/3})$, our regret bound matches the\n$\\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.\nCompared to our first $\\mathcal{O}(n\\bar{d}^{1/3}T^{2/3})$ bound, it is more\nadvantageous when the maximum delay $d = o(\\bar{d}^{2/3}T^{1/3})$. Finally, we\nconduct experiments on structured sparse learning to demonstrate the\nsuperiority of our methods.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5e26\u5ef6\u8fdf\u53cd\u9988\u7684\u5728\u7ebf\u975e\u5b50\u6a21\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u7b97\u6cd5DBGD-NF\u548c\u5176\u6269\u5c55\u7248\uff0c\u5206\u522b\u6539\u8fdb\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u9057\u61be\u754c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7684\u6700\u5927\u5ef6\u8fdf\u4f9d\u8d56\u6027\u548c\u5ef6\u8fdf\u4e0e\u53cd\u9988\u7684\u8026\u5408\u6548\u5e94\u9650\u5236\u4e86\u6027\u80fd\uff0c\u9700\u8981\u66f4\u4f18\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86DBGD-NF\u7b97\u6cd5\uff0c\u5229\u7528\u5355\u70b9\u68af\u5ea6\u4f30\u8ba1\u5668\uff0c\u5e76\u6269\u5c55\u4e86\u5176\u66f4\u65b0\u673a\u5236\u4ee5\u89e3\u8026\u5ef6\u8fdf\u548c\u53cd\u9988\u6548\u5e94\u3002", "result": "DBGD-NF\u5b9e\u73b0\u4e86\u4e0e\u5e73\u5747\u5ef6\u8fdf\u76f8\u5173\u7684\u9057\u61be\u754c\uff0c\u6269\u5c55\u7248\u8fdb\u4e00\u6b65\u89e3\u8026\u6548\u5e94\uff0c\u83b7\u5f97\u4e86\u66f4\u4f18\u7684\u9057\u61be\u754c\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u5728\u5ef6\u8fdf\u548c\u53cd\u9988\u5904\u7406\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.00539", "categories": ["cs.LG", "Cs"], "pdf": "https://arxiv.org/pdf/2508.00539", "abs": "https://arxiv.org/abs/2508.00539", "authors": ["Judy X Yang"], "title": "Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery", "comment": "8 pages, 6 figures", "summary": "Hyperspectral imaging offers detailed spectral information for mineral\nmapping; however, weak mineral signatures are often masked by noisy and\nredundant bands, limiting detection performance. To address this, we propose a\ntwo-stage integrated framework for enhanced mineral detection in the Cuprite\nmining district. In the first stage, we compute the signal-to-noise ratio (SNR)\nfor each spectral band and apply a phase-locked thresholding technique to\ndiscard low-SNR bands, effectively removing redundancy and suppressing\nbackground noise. Savitzky-Golay filtering is then employed for spectral\nsmoothing, serving a dual role first to stabilize trends during band selection,\nand second to preserve fine-grained spectral features during preprocessing. In\nthe second stage, the refined HSI data is reintroduced into the model, where\nKMeans clustering is used to extract 12 endmember spectra (W1 custom), followed\nby non negative least squares (NNLS) for abundance unmixing. The resulting\nendmembers are quantitatively compared with laboratory spectra (W1 raw) using\ncosine similarity and RMSE metrics. Experimental results confirm that our\nproposed pipeline improves unmixing accuracy and enhances the detection of weak\nmineral zones. This two-pass strategy demonstrates a practical and reproducible\nsolution for spectral dimensionality reduction and unmixing in geological HSI\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u566a\u6bd4\u9608\u503c\u548cSavitzky-Golay\u6ee4\u6ce2\u4f18\u5316\u9ad8\u5149\u8c31\u6570\u636e\uff0c\u7ed3\u5408KMeans\u548cNNLS\u63d0\u5347\u77ff\u7269\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u9ad8\u5149\u8c31\u6210\u50cf\u4e2d\u5f31\u77ff\u7269\u4fe1\u53f7\u88ab\u566a\u58f0\u548c\u5197\u4f59\u6ce2\u6bb5\u63a9\u76d6\u7684\u95ee\u9898\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4fe1\u566a\u6bd4\u9608\u503c\u548c\u6ee4\u6ce2\u964d\u566a\uff1b2) KMeans\u805a\u7c7b\u548cNNLS\u89e3\u6df7\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u89e3\u6df7\u7cbe\u5ea6\u548c\u5f31\u77ff\u7269\u533a\u57df\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "\u4e24\u9636\u6bb5\u7b56\u7565\u4e3a\u5730\u8d28\u9ad8\u5149\u8c31\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u91cd\u590d\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00578", "categories": ["cs.LG", "cond-mat.mtrl-sci", "physics.chem-ph", "physics.comp-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2508.00578", "abs": "https://arxiv.org/abs/2508.00578", "authors": ["Marlen Neubert", "Patrick Reiser", "Frauke Gr\u00e4ter", "Pascal Friederich"], "title": "Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides", "comment": "19 pages, 12 figures, and 4 tables (references and SI included)", "summary": "Hydrogen atom transfer (HAT) reactions are essential in many biological\nprocesses, such as radical migration in damaged proteins, but their mechanistic\npathways remain incompletely understood. Simulating HAT is challenging due to\nthe need for quantum chemical accuracy at biologically relevant scales; thus,\nneither classical force fields nor DFT-based molecular dynamics are applicable.\nMachine-learned potentials offer an alternative, able to learn potential energy\nsurfaces (PESs) with near-quantum accuracy. However, training these models to\ngeneralize across diverse HAT configurations, especially at radical positions\nin proteins, requires tailored data generation and careful model selection.\nHere, we systematically generate HAT configurations in peptides to build large\ndatasets using semiempirical methods and DFT. We benchmark three graph neural\nnetwork architectures (SchNet, Allegro, and MACE) on their ability to learn HAT\nPESs and indirectly predict reaction barriers from energy predictions. MACE\nconsistently outperforms the others in energy, force, and barrier prediction,\nachieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT\nbarrier predictions. This accuracy enables integration of ML potentials into\nlarge-scale collagen simulations to compute reaction rates from predicted\nbarriers, advancing mechanistic understanding of HAT and radical migration in\npeptides. We analyze scaling laws, model transferability, and cost-performance\ntrade-offs, and outline strategies for improvement by combining ML potentials\nwith transition state search algorithms and active learning. Our approach is\ngeneralizable to other biomolecular systems, enabling quantum-accurate\nsimulations of chemical reactivity in complex environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u6a21\u62df\u6c22\u539f\u5b50\u8f6c\u79fb\uff08HAT\uff09\u53cd\u5e94\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5927\u91cf\u80bd\u6bb5HAT\u914d\u7f6e\u6570\u636e\uff0c\u5e76\u6bd4\u8f83\u4e09\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u6027\u80fd\uff0c\u6700\u7ec8MACE\u8868\u73b0\u6700\u4f73\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u3002", "motivation": "HAT\u53cd\u5e94\u5728\u751f\u7269\u8fc7\u7a0b\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u673a\u7406\u5c1a\u4e0d\u5b8c\u5168\u6e05\u695a\uff0c\u4f20\u7edf\u6a21\u62df\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u91cf\u5b50\u5316\u5b66\u7cbe\u5ea6\u9700\u6c42\u3002", "method": "\u7cfb\u7edf\u751f\u6210\u80bd\u6bb5HAT\u914d\u7f6e\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u534a\u7ecf\u9a8c\u65b9\u6cd5\u548cDFT\uff0c\u5e76\u6bd4\u8f83\u4e09\u79cd\u56fe\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08SchNet\u3001Allegro\u3001MACE\uff09\u7684\u6027\u80fd\u3002", "result": "MACE\u5728\u80fd\u91cf\u3001\u529b\u548c\u53cd\u5e94\u52bf\u5792\u9884\u6d4b\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a1.13 kcal/mol\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u80f6\u539f\u6a21\u62df\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u751f\u7269\u5206\u5b50\u7cfb\u7edf\uff0c\u5b9e\u73b0\u590d\u6742\u73af\u5883\u4e2d\u5316\u5b66\u53cd\u5e94\u7684\u9ad8\u7cbe\u5ea6\u6a21\u62df\u3002"}}
{"id": "2508.00586", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00586", "abs": "https://arxiv.org/abs/2508.00586", "authors": ["Thorben Werner", "Lars Schmidt-Thieme", "Vijaya Krishna Yalavarthi"], "title": "The Role of Active Learning in Modern Machine Learning", "comment": null, "summary": "Even though Active Learning (AL) is widely studied, it is rarely applied in\ncontexts outside its own scientific literature. We posit that the reason for\nthis is AL's high computational cost coupled with the comparatively small lifts\nit is typically able to generate in scenarios with few labeled points. In this\nwork we study the impact of different methods to combat this low data scenario,\nnamely data augmentation (DA), semi-supervised learning (SSL) and AL. We find\nthat AL is by far the least efficient method of solving the low data problem,\ngenerating a lift of only 1-4\\% over random sampling, while DA and SSL methods\ncan generate up to 60\\% lift in combination with random sampling. However, when\nAL is combined with strong DA and SSL techniques, it surprisingly is still able\nto provide improvements. Based on these results, we frame AL not as a method to\ncombat missing labels, but as the final building block to squeeze the last bits\nof performance out of data after appropriate DA and SSL methods as been\napplied.", "AI": {"tldr": "AL\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u6548\u7387\u8f83\u4f4e\uff0c\u4f46\u7ed3\u5408DA\u548cSSL\u540e\u4ecd\u80fd\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u7814\u7a76AL\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u5176\u4e0e\u5176\u4ed6\u65b9\u6cd5\uff08DA\u3001SSL\uff09\u7684\u7ed3\u5408\u6548\u679c\u3002", "method": "\u6bd4\u8f83AL\u3001DA\u548cSSL\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5206\u6790\u5176\u7ec4\u5408\u6548\u679c\u3002", "result": "AL\u5355\u72ec\u6548\u679c\u6709\u9650\uff08\u63d0\u53471-4%\uff09\uff0c\u4f46\u7ed3\u5408DA\u548cSSL\u540e\u4ecd\u6709\u989d\u5916\u63d0\u5347\u3002", "conclusion": "AL\u5e94\u4f5c\u4e3aDA\u548cSSL\u540e\u7684\u8865\u5145\u65b9\u6cd5\uff0c\u800c\u975e\u4e3b\u8981\u89e3\u51b3\u4f4e\u6570\u636e\u95ee\u9898\u7684\u5de5\u5177\u3002"}}
{"id": "2508.00615", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00615", "abs": "https://arxiv.org/abs/2508.00615", "authors": ["Mukesh Kumar Sahu", "Pinki Roy"], "title": "Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data", "comment": null, "summary": "Accurately predicting the criticalness of ICU patients (such as in-ICU\nmortality risk) is vital for early intervention in critical care. However,\nconventional models often treat each patient in isolation and struggle to\nexploit the relational structure in Electronic Health Records (EHR). We propose\na Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds\na patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN\narchitecture that operates on this graph to predict patient mortality and a\ncontinuous criticalness score. SBSCGM uses a hybrid similarity measure\n(combining feature-based and structural similarities) to connect patients with\nanalogous clinical profiles in real-time. The HybridGraphMedGNN integrates\nGraph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)\nlayers to learn robust patient representations, leveraging both local and\nglobal graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III\ndataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)\noutperforming baseline classifiers and single-type GNN models. We also\ndemonstrate improved precision/recall and show that the attention mechanism\nprovides interpretable insights into model predictions. Our framework offers a\nscalable and interpretable solution for critical care risk prediction, with\npotential to support clinicians in real-world ICU deployment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u81ea\u6784\u5efa\u56fe\u6a21\u578b\uff08SBSCGM\uff09\u548c\u6df7\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HybridGraphMedGNN\uff09\uff0c\u7528\u4e8e\u9884\u6d4bICU\u60a3\u8005\u7684\u6b7b\u4ea1\u98ce\u9669\u548c\u5173\u952e\u6027\u8bc4\u5206\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u7684\u5173\u7cfb\u7ed3\u6784\uff0c\u9700\u8981\u4e00\u79cd\u52a8\u6001\u6784\u5efa\u60a3\u8005\u76f8\u4f3c\u56fe\u7684\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "SBSCGM\u901a\u8fc7\u6df7\u5408\u76f8\u4f3c\u6027\u5ea6\u91cf\u52a8\u6001\u6784\u5efa\u60a3\u8005\u76f8\u4f3c\u56fe\uff0cHybridGraphMedGNN\u7ed3\u5408GCN\u3001GraphSAGE\u548cGAT\u5c42\u5b66\u4e60\u60a3\u8005\u8868\u793a\u3002", "result": "\u5728MIMIC-III\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578bAUC-ROC\u8fbe0.94\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u91cd\u75c7\u76d1\u62a4\u98ce\u9669\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u9645ICU\u90e8\u7f72\u3002"}}
{"id": "2508.00627", "categories": ["cs.LG", "I.4.9; I.4.6"], "pdf": "https://arxiv.org/pdf/2508.00627", "abs": "https://arxiv.org/abs/2508.00627", "authors": ["Paul Tresson", "Pierre Le Coz", "Hadrien Tulet", "Anthony Malkassian", "Maxime R\u00e9jou M\u00e9chain"], "title": "IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources", "comment": "11 pages, 5 figures", "summary": "Remote sensing has entered a new era with the rapid development of artificial\nintelligence approaches. However, the implementation of deep learning has\nlargely remained restricted to specialists and has been impractical because it\noften requires (i) large reference datasets for model training and validation;\n(ii) substantial computing resources; and (iii) strong coding skills. Here, we\nintroduce IAMAP, a user-friendly QGIS plugin that addresses these three\nchallenges in an easy yet flexible way. IAMAP builds on recent advancements in\nself-supervised learning strategies, which now provide robust feature\nextractors, often referred to as foundation models. These generalist models can\noften be reliably used in few-shot or zero-shot scenarios (i.e., with little to\nno fine-tuning). IAMAP's interface allows users to streamline several key steps\nin remote sensing image analysis: (i) extracting image features using a wide\nrange of deep learning architectures; (ii) reducing dimensionality with\nbuilt-in algorithms; (iii) performing clustering on features or their reduced\nrepresentations; (iv) generating feature similarity maps; and (v) calibrating\nand validating supervised machine learning models for prediction. By enabling\nnon-AI specialists to leverage the high-quality features provided by recent\ndeep learning approaches without requiring GPU capacity or extensive reference\ndatasets, IAMAP contributes to the democratization of computationally efficient\nand energy-conscious deep learning methods.", "AI": {"tldr": "IAMAP\u662f\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684QGIS\u63d2\u4ef6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u9065\u611f\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff1a\u5927\u6570\u636e\u9700\u6c42\u3001\u9ad8\u8ba1\u7b97\u8d44\u6e90\u548c\u7f16\u7801\u6280\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5728\u9065\u611f\u5e94\u7528\u4e2d\u56e0\u5927\u6570\u636e\u9700\u6c42\u3001\u9ad8\u8ba1\u7b97\u8d44\u6e90\u548c\u7f16\u7801\u6280\u80fd\u9650\u5236\u800c\u96be\u4ee5\u666e\u53ca\u7684\u95ee\u9898\u3002", "method": "IAMAP\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u7684\u901a\u7528\u6a21\u578b\uff08\u57fa\u7840\u6a21\u578b\uff09\uff0c\u63d0\u4f9b\u7279\u5f81\u63d0\u53d6\u3001\u964d\u7ef4\u3001\u805a\u7c7b\u3001\u76f8\u4f3c\u6027\u6620\u5c04\u548c\u6a21\u578b\u9a8c\u8bc1\u7b49\u529f\u80fd\uff0c\u65e0\u9700GPU\u6216\u5927\u91cf\u53c2\u8003\u6570\u636e\u3002", "result": "IAMAP\u4f7f\u975eAI\u4e13\u5bb6\u80fd\u591f\u9ad8\u6548\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7279\u5f81\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u9ad8\u6548\u548c\u8282\u80fd\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u666e\u53ca\u3002", "conclusion": "IAMAP\u901a\u8fc7\u7b80\u5316\u6d41\u7a0b\u548c\u964d\u4f4e\u6280\u672f\u95e8\u69db\uff0c\u63a8\u52a8\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u9065\u611f\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2508.00628", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00628", "abs": "https://arxiv.org/abs/2508.00628", "authors": ["Xiong Xiong", "Zhuo Zhang", "Rongchun Hu", "Chen Gao", "Zichen Deng"], "title": "Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs", "comment": null, "summary": "Solving high-frequency oscillatory partial differential equations (PDEs) is a\ncritical challenge in scientific computing, with applications in fluid\nmechanics, quantum mechanics, and electromagnetic wave propagation. Traditional\nphysics-informed neural networks (PINNs) suffer from spectral bias, limiting\ntheir ability to capture high-frequency solution components. We introduce\nSeparated-Variable Spectral Neural Networks (SV-SNN), a novel framework that\naddresses these limitations by integrating separation of variables with\nadaptive spectral methods. Our approach features three key innovations: (1)\ndecomposition of multivariate functions into univariate function products,\nenabling independent spatial and temporal networks; (2) adaptive Fourier\nspectral features with learnable frequency parameters for high-frequency\ncapture; and (3) theoretical framework based on singular value decomposition to\nquantify spectral bias. Comprehensive evaluation on benchmark problems\nincluding Heat equation, Helmholtz equation, Poisson equations and\nNavier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of\nmagnitude improvement in accuracy while reducing parameter count by over 90\\%\nand training time by 60\\%. These results establish SV-SNN as an effective\nsolution to the spectral bias problem in neural PDE solving. The implementation\nwill be made publicly available upon acceptance at\nhttps://github.com/xgxgnpu/SV-SNN.", "AI": {"tldr": "SV-SNN\u662f\u4e00\u79cd\u65b0\u578b\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u53d8\u91cf\u548c\u81ea\u9002\u5e94\u8c31\u65b9\u6cd5\u89e3\u51b3\u9ad8\u9891\u632f\u8361PDE\u6c42\u89e3\u4e2d\u7684\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7cbe\u5ea6\u5e76\u51cf\u5c11\u53c2\u6570\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u4f20\u7edfPINNs\u5728\u9ad8\u9891\u632f\u8361PDE\u6c42\u89e3\u4e2d\u5b58\u5728\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u9ad8\u9891\u6210\u5206\u3002", "method": "\u63d0\u51faSV-SNN\u6846\u67b6\uff0c\u5305\u62ec\u53d8\u91cf\u5206\u79bb\u3001\u81ea\u9002\u5e94\u5085\u91cc\u53f6\u8c31\u7279\u5f81\u548c\u5b66\u4e60\u9891\u7387\u53c2\u6570\uff0c\u4ee5\u53ca\u57fa\u4e8eSVD\u7684\u7406\u8bba\u6846\u67b6\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u95ee\u9898\u4e0a\uff0cSV-SNN\u7cbe\u5ea6\u63d0\u53471-3\u4e2a\u6570\u91cf\u7ea7\uff0c\u53c2\u6570\u51cf\u5c1190%\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1160%\u3002", "conclusion": "SV-SNN\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecfPDE\u6c42\u89e3\u4e2d\u7684\u8c31\u504f\u5dee\u95ee\u9898\uff0c\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.00635", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00635", "abs": "https://arxiv.org/abs/2508.00635", "authors": ["Changning Wu", "Gao Wu", "Rongyao Cai", "Yong Liu", "Kexin Zhang"], "title": "KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting", "comment": null, "summary": "Multi-scale decomposition architectures have emerged as predominant\nmethodologies in time series forecasting. However, real-world time series\nexhibit noise interference across different scales, while heterogeneous\ninformation distribution among frequency components at varying scales leads to\nsuboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks\n(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency\nSelection learning architecture (KFS) to address these challenges. This\nframework tackles prediction challenges stemming from cross-scale noise\ninterference and complex pattern modeling through its FreK module, which\nperforms energy-distribution-based dominant frequency selection in the spectral\ndomain. Simultaneously, KAN enables sophisticated pattern representation while\ntimestamp embedding alignment synchronizes temporal representations across\nscales. The feature mixing module then fuses scale-specific patterns with\naligned temporal features. Extensive experiments across multiple real-world\ntime series datasets demonstrate that KT achieves state-of-the-art performance\nas a simple yet effective architecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKAN\u7684\u81ea\u9002\u5e94\u9891\u7387\u9009\u62e9\u5b66\u4e60\u67b6\u6784\uff08KFS\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u591a\u5c3a\u5ea6\u566a\u58f0\u5e72\u6270\u548c\u9891\u7387\u4fe1\u606f\u5206\u5e03\u4e0d\u5747\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u5728\u4e0d\u540c\u5c3a\u5ea6\u4e0a\u5b58\u5728\u566a\u58f0\u5e72\u6270\uff0c\u4e14\u9891\u7387\u4fe1\u606f\u5206\u5e03\u4e0d\u5747\u5bfc\u81f4\u591a\u5c3a\u5ea6\u8868\u793a\u4e0d\u7406\u60f3\u3002", "method": "\u7ed3\u5408Kolmogorov-Arnold Networks\uff08KAN\uff09\u548cParseval\u5b9a\u7406\uff0c\u8bbe\u8ba1\u4e86KFS\u67b6\u6784\uff0c\u5305\u62ecFreK\u6a21\u5757\uff08\u57fa\u4e8e\u80fd\u91cf\u5206\u5e03\u9009\u62e9\u4e3b\u5bfc\u9891\u7387\uff09\u3001\u65f6\u95f4\u6233\u5d4c\u5165\u5bf9\u9f50\u548c\u7279\u5f81\u6df7\u5408\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86KFS\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "KFS\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u67b6\u6784\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u5c3a\u5ea6\u566a\u58f0\u5e72\u6270\u548c\u590d\u6742\u6a21\u5f0f\u5efa\u6a21\u95ee\u9898\u3002"}}
{"id": "2508.00641", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00641", "abs": "https://arxiv.org/abs/2508.00641", "authors": ["Alessandro Palmas"], "title": "Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense", "comment": "11 pages, 10 figures", "summary": "The growing threat of low-cost kamikaze drone swarms poses a critical\nchallenge to modern defense systems demanding rapid and strategic\ndecision-making to prioritize interceptions across multiple effectors and\nhigh-value target zones. In this work, we present a case study demonstrating\nthe practical advantages of reinforcement learning in addressing this\nchallenge. We introduce a high-fidelity simulation environment that captures\nrealistic operational constraints, within which a decision-level reinforcement\nlearning agent learns to coordinate multiple effectors for optimal interception\nprioritization. Operating in a discrete action space, the agent selects which\ndrone to engage per effector based on observed state features such as\npositions, classes, and effector status. We evaluate the learned policy against\na handcrafted rule-based baseline across hundreds of simulated attack\nscenarios. The reinforcement learning based policy consistently achieves lower\naverage damage and higher defensive efficiency in protecting critical zones.\nThis case study highlights the potential of reinforcement learning as a\nstrategic layer within defense architectures, enhancing resilience without\ndisplacing existing control systems. All code and simulation assets are\npublicly released for full reproducibility, and a video demonstration\nillustrates the policy's qualitative behavior.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u9632\u5fa1\u4f4e\u6210\u672c\u81ea\u6740\u5f0f\u65e0\u4eba\u673a\u7fa4\u653b\u51fb\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u9ad8\u4eff\u771f\u6a21\u62df\u73af\u5883\u548c\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u4f18\u5316\u62e6\u622a\u4f18\u5148\u7ea7\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5e73\u5747\u635f\u5bb3\u5e76\u63d0\u9ad8\u4e86\u9632\u5fa1\u6548\u7387\u3002", "motivation": "\u4f4e\u6210\u672c\u81ea\u6740\u5f0f\u65e0\u4eba\u673a\u7fa4\u5bf9\u73b0\u4ee3\u9632\u5fa1\u7cfb\u7edf\u6784\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u9700\u8981\u5feb\u901f\u6218\u7565\u51b3\u7b56\u4ee5\u4f18\u5316\u62e6\u622a\u4f18\u5148\u7ea7\u3002", "method": "\u63d0\u51fa\u9ad8\u4eff\u771f\u6a21\u62df\u73af\u5883\uff0c\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u5728\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u9009\u62e9\u62e6\u622a\u76ee\u6807\uff0c\u57fa\u4e8e\u72b6\u6001\u7279\u5f81\uff08\u5982\u4f4d\u7f6e\u3001\u7c7b\u522b\u548c\u6548\u5e94\u5668\u72b6\u6001\uff09\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u6570\u767e\u6b21\u6a21\u62df\u653b\u51fb\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\uff0c\u5e73\u5747\u635f\u5bb3\u66f4\u4f4e\uff0c\u9632\u5fa1\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u53ef\u4f5c\u4e3a\u9632\u5fa1\u67b6\u6784\u7684\u6218\u7565\u5c42\uff0c\u63d0\u5347\u97e7\u6027\u800c\u4e0d\u53d6\u4ee3\u73b0\u6709\u63a7\u5236\u7cfb\u7edf\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6a21\u62df\u8d44\u6e90\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.00643", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00643", "abs": "https://arxiv.org/abs/2508.00643", "authors": ["Albert Matveev", "Sanmitra Ghosh", "Aamal Hussain", "James-Michael Leahy", "Michalis Michaelides"], "title": "Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators", "comment": null, "summary": "Operator learning is a powerful paradigm for solving partial differential\nequations, with Fourier Neural Operators serving as a widely adopted\nfoundation. However, FNOs face significant scalability challenges due to\noverparameterization and offer no native uncertainty quantification -- a key\nrequirement for reliable scientific and engineering applications. Instead,\nneural operators rely on post hoc UQ methods that ignore geometric inductive\nbiases. In this work, we introduce DINOZAUR: a diffusion-based neural operator\nparametrization with uncertainty quantification. Inspired by the structure of\nthe heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a\ndimensionality-independent diffusion multiplier that has a single learnable\ntime parameter per channel, drastically reducing parameter count and memory\nfootprint without compromising predictive performance. By defining priors over\nthose time parameters, we cast DINOZAUR as a Bayesian neural operator to yield\nspatially correlated outputs and calibrated uncertainty estimates. Our method\nachieves competitive or superior performance across several PDE benchmarks\nwhile providing efficient uncertainty quantification.", "AI": {"tldr": "DINOZAUR\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86FNO\u7684\u53ef\u6269\u5c55\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u51cf\u5c11\u53c2\u6570\u548c\u5185\u5b58\u5360\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "FNO\u5b58\u5728\u53c2\u6570\u8fc7\u591a\u548c\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u79d1\u5b66\u548c\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "DINOZAUR\u91c7\u7528\u6269\u6563\u4e58\u5b50\u66ff\u4ee3FNO\u4e2d\u7684\u5bc6\u96c6\u5f20\u91cf\u4e58\u5b50\uff0c\u51cf\u5c11\u53c2\u6570\u548c\u5185\u5b58\u5360\u7528\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u65b9\u6cd5\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "DINOZAUR\u5728\u591a\u4e2aPDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u63d0\u4f9b\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "conclusion": "DINOZAUR\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u7684\u795e\u7ecf\u7b97\u5b50\u65b9\u6cd5\u3002"}}
{"id": "2508.00657", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00657", "abs": "https://arxiv.org/abs/2508.00657", "authors": ["Sihang Zeng", "Lucas Jing Liu", "Jun Wen", "Meliha Yetisgen", "Ruth Etzioni", "Gang Luo"], "title": "TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction", "comment": "Accepted by MLHC 2025", "summary": "Trustworthy survival prediction is essential for clinical decision making.\nLongitudinal electronic health records (EHRs) provide a uniquely powerful\nopportunity for the prediction. However, it is challenging to accurately model\nthe continuous clinical progression of patients underlying the irregularly\nsampled clinical features and to transparently link the progression to survival\noutcomes. To address these challenges, we develop TrajSurv, a model that learns\ncontinuous latent trajectories from longitudinal EHR data for trustworthy\nsurvival prediction. TrajSurv employs a neural controlled differential equation\n(NCDE) to extract continuous-time latent states from the irregularly sampled\ndata, forming continuous latent trajectories. To ensure the latent trajectories\nreflect the clinical progression, TrajSurv aligns the latent state space with\npatient state space through a time-aware contrastive learning approach. To\ntransparently link clinical progression to the survival outcome, TrajSurv uses\nlatent trajectories in a two-step divide-and-conquer interpretation process.\nFirst, it explains how the changes in clinical features translate into the\nlatent trajectory's evolution using a learned vector field. Second, it clusters\nthese latent trajectories to identify key clinical progression patterns\nassociated with different survival outcomes. Evaluations on two real-world\nmedical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and\nsuperior transparency over existing deep learning methods.", "AI": {"tldr": "TrajSurv\u662f\u4e00\u4e2a\u5229\u7528\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u8fdb\u884c\u751f\u5b58\u9884\u6d4b\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\uff08NCDE\uff09\u63d0\u53d6\u8fde\u7eed\u65f6\u95f4\u6f5c\u5728\u72b6\u6001\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u4e24\u9636\u6bb5\u89e3\u91ca\u65b9\u6cd5\u63d0\u9ad8\u900f\u660e\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u9700\u8981\u53ef\u9760\u7684\u751f\u5b58\u9884\u6d4b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\u7684\u4e34\u5e8a\u6570\u636e\u5e76\u900f\u660e\u5730\u5173\u8054\u4e34\u5e8a\u8fdb\u5c55\u4e0e\u751f\u5b58\u7ed3\u679c\u3002", "method": "TrajSurv\u4f7f\u7528NCDE\u4eceEHR\u4e2d\u63d0\u53d6\u8fde\u7eed\u6f5c\u5728\u8f68\u8ff9\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u6f5c\u5728\u72b6\u6001\u4e0e\u60a3\u8005\u72b6\u6001\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u89e3\u91ca\u65b9\u6cd5\uff08\u5411\u91cf\u573a\u5206\u6790\u548c\u805a\u7c7b\uff09\u5173\u8054\u4e34\u5e8a\u8fdb\u5c55\u4e0e\u751f\u5b58\u7ed3\u679c\u3002", "result": "\u5728MIMIC-III\u548ceICU\u6570\u636e\u96c6\u4e0a\uff0cTrajSurv\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "TrajSurv\u901a\u8fc7\u8fde\u7eed\u6f5c\u5728\u8f68\u8ff9\u548c\u900f\u660e\u89e3\u91ca\u65b9\u6cd5\uff0c\u4e3a\u4e34\u5e8a\u751f\u5b58\u9884\u6d4b\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00664", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00664", "abs": "https://arxiv.org/abs/2508.00664", "authors": ["Jialun Zheng", "Jie Liu", "Jiannong Cao", "Xiao Wang", "Hanchen Yang", "Yankai Chen", "Philip S. Yu"], "title": "DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes", "comment": null, "summary": "Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies\nin evolving graphs across domains such as finance, traffic, and social\nnetworks. Recently, generalist graph anomaly detection (GAD) models have shown\npromising results. They are pretrained on multiple source datasets and\ngeneralize across domains. While effective on static graphs, they struggle to\ncapture evolving anomalies in dynamic graphs. Moreover, the continuous\nemergence of new domains and the lack of labeled data further challenge\ngeneralist DGAD. Effective cross-domain DGAD requires both domain-specific and\ndomain-agnostic anomalous patterns. Importantly, these patterns evolve\ntemporally within and across domains. Building on these insights, we propose a\nDGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and\ndomain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,\nevolving representations of normal and anomalous patterns, from temporal\nego-graphs and stores them in a memory buffer. The buffer is selectively\nupdated to retain general, domain-agnostic patterns while incorporating new\ndomain-specific ones. Then, an anomaly scorer compares incoming data with\ndynamic prototypes to flag both general and domain-specific anomalies. Finally,\nDP-DGAD employs confidence-based pseudo-labeling for effective self-supervised\nadaptation in target domains. Extensive experiments demonstrate\nstate-of-the-art performance across ten real-world datasets from different\ndomains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u539f\u578b\uff08DP\uff09\u7684DGAD\u6a21\u578b\uff0c\u7528\u4e8e\u6355\u6349\u52a8\u6001\u56fe\u4e2d\u6f14\u53d8\u7684\u5f02\u5e38\u6a21\u5f0f\uff0c\u5e76\u5728\u591a\u9886\u57df\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u52a8\u6001\u56fe\u5f02\u5e38\u68c0\u6d4b\uff08DGAD\uff09\u5728\u591a\u4e2a\u9886\u57df\uff08\u5982\u91d1\u878d\u3001\u4ea4\u901a\u3001\u793e\u4ea4\u7f51\u7edc\uff09\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u901a\u7528GAD\u6a21\u578b\u96be\u4ee5\u6355\u6349\u52a8\u6001\u56fe\u4e2d\u7684\u5f02\u5e38\u6f14\u53d8\uff0c\u4e14\u65b0\u9886\u57df\u6570\u636e\u7f3a\u4e4f\u6807\u7b7e\u3002", "method": "\u63d0\u51faDP-DGAD\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u539f\u578b\u63d0\u53d6\u548c\u5b58\u50a8\u6f14\u53d8\u7684\u6b63\u5e38\u4e0e\u5f02\u5e38\u6a21\u5f0f\uff0c\u9009\u62e9\u6027\u66f4\u65b0\u5185\u5b58\u7f13\u51b2\u533a\uff0c\u5e76\u7ed3\u5408\u5f02\u5e38\u8bc4\u5206\u5668\u548c\u4f2a\u6807\u7b7e\u8fdb\u884c\u81ea\u76d1\u7763\u9002\u5e94\u3002", "result": "\u5728\u5341\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "DP-DGAD\u80fd\u6709\u6548\u6355\u6349\u8de8\u9886\u57df\u7684\u52a8\u6001\u5f02\u5e38\u6a21\u5f0f\uff0c\u9002\u7528\u4e8e\u65e0\u6807\u7b7e\u7684\u65b0\u9886\u57df\u3002"}}
{"id": "2508.00695", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00695", "abs": "https://arxiv.org/abs/2508.00695", "authors": ["Sergio Rubio-Mart\u00edn", "Mar\u00eda Teresa Garc\u00eda-Ord\u00e1s", "Antonio Serrano-Garc\u00eda", "Clara Margarita Franch-Pato", "Arturo Crespo-\u00c1lvaro", "Jos\u00e9 Alberto Ben\u00edtez-Andrades"], "title": "Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach", "comment": null, "summary": "The classification of clinical notes into specific diagnostic categories is\ncritical in healthcare, especially for mental health conditions like Anxiety\nand Adjustment Disorder. In this study, we compare the performance of various\nArtificial Intelligence models, including both traditional Machine Learning\napproaches (Random Forest, Support Vector Machine, K-nearest neighbors,\nDecision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT\nand SciBERT), to classify clinical notes into these two diagnoses.\nAdditionally, we implemented three oversampling strategies: No Oversampling,\nRandom Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to\nassess their impact on model performance. Hyperparameter tuning was also\napplied to optimize model accuracy. Our results indicate that oversampling\ntechniques had minimal impact on model performance overall. The only exception\nwas SMOTE, which showed a positive effect specifically with BERT-based models.\nHowever, hyperparameter optimization significantly improved accuracy across the\nmodels, enhancing their ability to generalize and perform on the dataset. The\nDecision Tree and eXtreme Gradient Boost models achieved the highest accuracy\namong machine learning approaches, both reaching 96%, while the DistilBERT and\nSciBERT models also attained 96% accuracy in the deep learning category. These\nfindings underscore the importance of hyperparameter tuning in maximizing model\nperformance. This study contributes to the ongoing research on AI-assisted\ndiagnostic tools in mental health by providing insights into the efficacy of\ndifferent model architectures and data balancing methods.", "AI": {"tldr": "\u6bd4\u8f83\u591a\u79cdAI\u6a21\u578b\uff08\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\uff09\u5728\u5206\u7c7b\u4e34\u5e8a\u7b14\u8bb0\u4e3a\u7126\u8651\u548c\u9002\u5e94\u969c\u788d\u8bca\u65ad\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8d85\u53c2\u6570\u8c03\u4f18\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u800c\u6570\u636e\u5e73\u8861\u65b9\u6cd5\u5f71\u54cd\u6709\u9650\u3002", "motivation": "\u4e34\u5e8a\u7b14\u8bb0\u5206\u7c7b\u5bf9\u5fc3\u7406\u5065\u5eb7\u8bca\u65ad\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540cAI\u6a21\u578b\u548c\u6570\u636e\u5e73\u8861\u65b9\u6cd5\u7684\u6548\u80fd\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u4f20\u7edf\u673a\u5668\u5b66\u4e60\uff08\u5982\u968f\u673a\u68ee\u6797\u3001SVM\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982DistilBERT\u3001SciBERT\uff09\uff0c\u7ed3\u5408\u4e09\u79cd\u8fc7\u91c7\u6837\u7b56\u7565\u548c\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u8d85\u53c2\u6570\u8c03\u4f18\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0cSMOTE\u4ec5\u5bf9BERT\u6a21\u578b\u6709\u6b63\u9762\u5f71\u54cd\u3002\u51b3\u7b56\u6811\u548cXGBoost\u5728\u4f20\u7edf\u65b9\u6cd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0896%\u51c6\u786e\u7387\uff09\uff0cBERT\u6a21\u578b\u540c\u6837\u8fbe\u523096%\u3002", "conclusion": "\u8d85\u53c2\u6570\u8c03\u4f18\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7814\u7a76\u4e3aAI\u8f85\u52a9\u5fc3\u7406\u5065\u5eb7\u8bca\u65ad\u63d0\u4f9b\u4e86\u6a21\u578b\u67b6\u6784\u548c\u6570\u636e\u5e73\u8861\u65b9\u6cd5\u7684\u53c2\u8003\u3002"}}
{"id": "2508.00706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00706", "abs": "https://arxiv.org/abs/2508.00706", "authors": ["Haozhe Tian", "Pietro Ferraro", "Robert Shorten", "Mahdi Jalili", "Homayoun Hamedmoghadam"], "title": "Learning Network Dismantling without Handcrafted Inputs", "comment": null, "summary": "The application of message-passing Graph Neural Networks has been a\nbreakthrough for important network science problems. However, the competitive\nperformance often relies on using handcrafted structural features as inputs,\nwhich increases computational cost and introduces bias into the otherwise\npurely data-driven network representations. Here, we eliminate the need for\nhandcrafted features by introducing an attention mechanism and utilizing\nmessage-iteration profiles, in addition to an effective algorithmic approach to\ngenerate a structurally diverse training set of small synthetic networks.\nThereby, we build an expressive message-passing framework and use it to\nefficiently solve the NP-hard problem of Network Dismantling, virtually\nequivalent to vital node identification, with significant real-world\napplications. Trained solely on diversified synthetic networks, our proposed\nmodel -- MIND: Message Iteration Network Dismantler -- generalizes to large,\nunseen real networks with millions of nodes, outperforming state-of-the-art\nnetwork dismantling methods. Increased efficiency and generalizability of the\nproposed model can be leveraged beyond dismantling in a range of complex\nnetwork problems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u548c\u6d88\u606f\u8fed\u4ee3\u914d\u7f6e\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\u6846\u67b6MIND\uff0c\u7528\u4e8e\u89e3\u51b3\u7f51\u7edc\u62c6\u89e3\u95ee\u9898\uff0c\u65e0\u9700\u624b\u5de5\u7279\u5f81\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u7f51\u7edc\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\u4f9d\u8d56\u624b\u5de5\u7279\u5f81\uff0c\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u5e76\u5f15\u5165\u504f\u5dee\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u901a\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u548c\u6d88\u606f\u8fed\u4ee3\u914d\u7f6e\uff0c\u5229\u7528\u5408\u6210\u7f51\u7edc\u751f\u6210\u591a\u6837\u5316\u8bad\u7ec3\u96c6\uff0c\u6784\u5efaMIND\u6846\u67b6\u3002", "result": "MIND\u5728\u5927\u578b\u771f\u5b9e\u7f51\u7edc\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MIND\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u7f51\u7edc\u62c6\u89e3\uff0c\u8fd8\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u590d\u6742\u7f51\u7edc\u95ee\u9898\u3002"}}
{"id": "2508.00707", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00707", "abs": "https://arxiv.org/abs/2508.00707", "authors": ["Yannik Schnitzer", "Alessandro Abate", "David Parker"], "title": "Efficient Solution and Learning of Robust Factored MDPs", "comment": null, "summary": "Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling\nepistemic uncertainty about transition dynamics. Learning r-MDPs from\ninteractions with an unknown environment enables the synthesis of robust\npolicies with provable (PAC) guarantees on performance, but this can require a\nlarge number of sample interactions. We propose novel methods for solving and\nlearning r-MDPs based on factored state-space representations that leverage the\nindependence between model uncertainty across system components. Although\npolicy synthesis for factored r-MDPs leads to hard, non-convex optimisation\nproblems, we show how to reformulate these into tractable linear programs.\nBuilding on these, we also propose methods to learn factored model\nrepresentations directly. Our experimental results show that exploiting\nfactored structure can yield dimensional gains in sample efficiency, producing\nmore effective robust policies with tighter performance guarantees than\nstate-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u89e3\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u548c\u5b66\u4e60\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08r-MDPs\uff09\uff0c\u901a\u8fc7\u5229\u7528\u7cfb\u7edf\u7ec4\u4ef6\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u72ec\u7acb\u6027\uff0c\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u7684\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684r-MDPs\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6837\u672c\u4ea4\u4e92\uff0c\u4e14\u96be\u4ee5\u5904\u7406\u9ad8\u7ef4\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5206\u89e3\u72b6\u6001\u7a7a\u95f4\u8868\u793a\uff0c\u5229\u7528\u7cfb\u7edf\u7ec4\u4ef6\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\u72ec\u7acb\u6027\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u7b56\u7565\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5206\u89e3\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u7684\u65b9\u6cd5\uff0c\u5c06\u975e\u51f8\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u5e76\u76f4\u63a5\u5b66\u4e60\u5206\u89e3\u6a21\u578b\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5229\u7528\u5206\u89e3\u7ed3\u6784\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u751f\u6210\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u7684\u9c81\u68d2\u7b56\u7565\uff0c\u5e76\u63d0\u4f9b\u66f4\u4e25\u683c\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u5206\u89e3\u72b6\u6001\u7a7a\u95f4\u8868\u793a\u548c\u4f18\u5316\u95ee\u9898\u8f6c\u5316\uff0c\u672c\u6587\u65b9\u6cd5\u5728r-MDPs\u7684\u5b66\u4e60\u548c\u7b56\u7565\u5408\u6210\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2508.00712", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00712", "abs": "https://arxiv.org/abs/2508.00712", "authors": ["Dien Nguyen", "Diego Perez-Liebana", "Simon Lucas"], "title": "JSON-Bag: A generic game trajectory representation", "comment": "8 pages, 3 figures, 6 tables, to be published in IEEE Conference on\n  Games 2025", "summary": "We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically\nrepresent game trajectories by tokenizing their JSON descriptions and apply\nJensen-Shannon distance (JSD) as distance metric for them. Using a\nprototype-based nearest-neighbor search (P-NNS), we evaluate the validity of\nJSON-Bag with JSD on six tabletop games -- \\textit{7 Wonders},\n\\textit{Dominion}, \\textit{Sea Salt and Paper}, \\textit{Can't Stop},\n\\textit{Connect4}, \\textit{Dots and boxes} -- each over three game trajectory\nclassification tasks: classifying the playing agents, game parameters, or game\nseeds that were used to generate the trajectories.\n  Our approach outperforms a baseline using hand-crafted features in the\nmajority of tasks. Evaluating on N-shot classification suggests using JSON-Bag\nprototype to represent game trajectory classes is also sample efficient.\nAdditionally, we demonstrate JSON-Bag ability for automatic feature extraction\nby treating tokens as individual features to be used in Random Forest to solve\nthe tasks above, which significantly improves accuracy on underperforming\ntasks. Finally, we show that, across all six games, the JSD between JSON-Bag\nprototypes of agent classes highly correlates with the distances between\nagents' policies.", "AI": {"tldr": "JSON-Bag\u6a21\u578b\u901a\u8fc7JSON\u63cf\u8ff0\u7684\u6e38\u620f\u8f68\u8ff9\u8fdb\u884c\u901a\u7528\u8868\u793a\uff0c\u5e76\u4f7f\u7528Jensen-Shannon\u8ddd\u79bb\uff08JSD\uff09\u4f5c\u4e3a\u5ea6\u91cf\u6807\u51c6\u3002\u5728\u516d\u6b3e\u684c\u6e38\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8868\u73b0\u4f18\u4e8e\u624b\u5de5\u7279\u5f81\u57fa\u7ebf\uff0c\u5e76\u5c55\u793a\u4e86\u6837\u672c\u9ad8\u6548\u6027\u548c\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u7528\u5730\u8868\u793a\u6e38\u620f\u8f68\u8ff9\u5e76\u8bc4\u4f30\u5176\u6709\u6548\u6027\uff0c\u4ee5\u89e3\u51b3\u6e38\u620f\u8f68\u8ff9\u5206\u7c7b\u4efb\u52a1\u3002", "method": "\u4f7f\u7528JSON-Bag\u6a21\u578b\u5bf9\u6e38\u620f\u8f68\u8ff9\u8fdb\u884c\u4ee4\u724c\u5316\uff0c\u91c7\u7528JSD\u4f5c\u4e3a\u8ddd\u79bb\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u6700\u8fd1\u90bb\u641c\u7d22\uff08P-NNS\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u591a\u6570\u4efb\u52a1\u4e2d\u4f18\u4e8e\u624b\u5de5\u7279\u5f81\u57fa\u7ebf\uff0c\u6837\u672c\u6548\u7387\u9ad8\uff0c\u4e14\u81ea\u52a8\u7279\u5f81\u63d0\u53d6\u663e\u8457\u63d0\u5347\u4f4e\u6548\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "JSON-Bag\u6a21\u578b\u662f\u6e38\u620f\u8f68\u8ff9\u8868\u793a\u7684\u6709\u6548\u65b9\u6cd5\uff0cJSD\u4e0e\u7b56\u7565\u8ddd\u79bb\u9ad8\u5ea6\u76f8\u5173\u3002"}}
{"id": "2508.00716", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00716", "abs": "https://arxiv.org/abs/2508.00716", "authors": ["Yingxu Wang", "Mengzhu Wang", "Zhichao Huang", "Suyu Liu"], "title": "Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning", "comment": null, "summary": "Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled\nsource graphs to unlabeled target graphs by learning domain-invariant\nrepresentations, which is essential in applications such as molecular property\nprediction and social network analysis. However, most existing GDA methods rely\non the assumption of clean source labels, which rarely holds in real-world\nscenarios where annotation noise is pervasive. This label noise severely\nimpairs feature alignment and degrades adaptation performance under domain\nshifts. To address this challenge, we propose Nested Graph Pseudo-Label\nRefinement (NeGPR), a novel framework tailored for graph-level domain\nadaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,\nsemantic and topology branches, by enforcing neighborhood consistency in the\nfeature space, thereby reducing the influence of noisy supervision. To bridge\ndomain gaps, NeGPR employs a nested refinement mechanism in which one branch\nselects high-confidence target samples to guide the adaptation of the other,\nenabling progressive cross-domain learning. Furthermore, since pseudo-labels\nmay still contain noise and the pre-trained branches are already overfitted to\nthe noisy labels in the source domain, NeGPR incorporates a noise-aware\nregularization strategy. This regularization is theoretically proven to\nmitigate the adverse effects of pseudo-label noise, even under the presence of\nsource overfitting, thus enhancing the robustness of the adaptation process.\nExtensive experiments on benchmark datasets demonstrate that NeGPR consistently\noutperforms state-of-the-art methods under severe label noise, achieving gains\nof up to 12.7% in accuracy.", "AI": {"tldr": "NeGPR\u662f\u4e00\u79cd\u9488\u5bf9\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u56fe\u7ea7\u57df\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5206\u652f\u9884\u8bad\u7ec3\u548c\u5d4c\u5957\u4f2a\u6807\u7b7e\u7ec6\u5316\u673a\u5236\u63d0\u5347\u8de8\u57df\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u56fe\u57df\u9002\u5e94\u65b9\u6cd5\u5047\u8bbe\u6e90\u6807\u7b7e\u5e72\u51c0\uff0c\u4f46\u5b9e\u9645\u573a\u666f\u4e2d\u6807\u7b7e\u566a\u58f0\u666e\u904d\uff0c\u4e25\u91cd\u5f71\u54cd\u57df\u9002\u5e94\u6027\u80fd\u3002", "method": "NeGPR\u91c7\u7528\u53cc\u5206\u652f\uff08\u8bed\u4e49\u548c\u62d3\u6251\uff09\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u90bb\u57df\u4e00\u81f4\u6027\u51cf\u5c11\u566a\u58f0\u5f71\u54cd\uff0c\u5e76\u5229\u7528\u5d4c\u5957\u7ec6\u5316\u673a\u5236\u548c\u566a\u58f0\u611f\u77e5\u6b63\u5219\u5316\u7b56\u7565\u3002", "result": "\u5728\u4e25\u91cd\u6807\u7b7e\u566a\u58f0\u4e0b\uff0cNeGPR\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe12.7%\u3002", "conclusion": "NeGPR\u6709\u6548\u89e3\u51b3\u4e86\u5e26\u566a\u58f0\u6807\u7b7e\u7684\u56fe\u57df\u9002\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.00718", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00718", "abs": "https://arxiv.org/abs/2508.00718", "authors": ["Ivona Krchova", "Mariana Vargas Vieyra", "Mario Scriminaci", "Andrey Sidorenko"], "title": "Democratizing Tabular Data Access with an Open$\\unicode{x2013}$Source Synthetic$\\unicode{x2013}$Data SDK", "comment": null, "summary": "Machine learning development critically depends on access to high-quality\ndata. However, increasing restrictions due to privacy, proprietary interests,\nand ethical concerns have created significant barriers to data accessibility.\nSynthetic data offers a viable solution by enabling safe, broad data usage\nwithout compromising sensitive information. This paper presents the MOSTLY AI\nSynthetic Data Software Development Kit (SDK), an open-source toolkit designed\nspecifically for synthesizing high-quality tabular data. The SDK integrates\nrobust features such as differential privacy guarantees, fairness-aware data\ngeneration, and automated quality assurance into a flexible and accessible\nPython interface. Leveraging the TabularARGN autoregressive framework, the SDK\nsupports diverse data types and complex multi-table and sequential datasets,\ndelivering competitive performance with notable improvements in speed and\nusability. Currently deployed both as a cloud service and locally installable\nsoftware, the SDK has seen rapid adoption, highlighting its practicality in\naddressing real-world data bottlenecks and promoting widespread data\ndemocratization.", "AI": {"tldr": "MOSTLY AI SDK\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5408\u6210\u8868\u683c\u6570\u636e\uff0c\u89e3\u51b3\u6570\u636e\u9690\u79c1\u548c\u8bbf\u95ee\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u9690\u79c1\u3001\u4e13\u6709\u5229\u76ca\u548c\u4f26\u7406\u95ee\u9898\uff0c\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u83b7\u53d6\u53d7\u9650\uff0c\u5408\u6210\u6570\u636e\u6210\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eTabularARGN\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u96c6\u6210\u5dee\u5206\u9690\u79c1\u3001\u516c\u5e73\u6027\u751f\u6210\u548c\u81ea\u52a8\u5316\u8d28\u91cf\u4fdd\u8bc1\u3002", "result": "SDK\u5728\u901f\u5ea6\u548c\u53ef\u7528\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u591a\u79cd\u6570\u636e\u7c7b\u578b\u548c\u590d\u6742\u6570\u636e\u96c6\u3002", "conclusion": "SDK\u5b9e\u7528\u6027\u5f3a\uff0c\u4fc3\u8fdb\u4e86\u6570\u636e\u6c11\u4e3b\u5316\uff0c\u5df2\u5feb\u901f\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002"}}
{"id": "2508.00734", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00734", "abs": "https://arxiv.org/abs/2508.00734", "authors": ["Liuyun Xu", "Seymour M. J. Spence"], "title": "Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems", "comment": null, "summary": "Existing variance reduction techniques used in stochastic simulations for\nrare event analysis still require a substantial number of model evaluations to\nestimate small failure probabilities. In the context of complex, nonlinear\nfinite element modeling environments, this can become computationally\nchallenging-particularly for systems subjected to stochastic excitation. To\naddress this challenge, a multi-fidelity stratified sampling scheme with\nadaptive machine learning metamodels is introduced for efficiently propagating\nuncertainties and estimating small failure probabilities. In this approach, a\nhigh-fidelity dataset generated through stratified sampling is used to train a\ndeep learning-based metamodel, which then serves as a cost-effective and highly\ncorrelated low-fidelity model. An adaptive training scheme is proposed to\nbalance the trade-off between approximation quality and computational demand\nassociated with the development of the low-fidelity model. By integrating the\nlow-fidelity outputs with additional high-fidelity results, an unbiased\nestimate of the strata-wise failure probabilities is obtained using a\nmulti-fidelity Monte Carlo framework. The overall probability of failure is\nthen computed using the total probability theorem. Application to a full-scale\nhigh-rise steel building subjected to stochastic wind excitation demonstrates\nthat the proposed scheme can accurately estimate exceedance probability curves\nfor nonlinear responses of interest, while achieving significant computational\nsavings compared to single-fidelity variance reduction approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u4fdd\u771f\u5ea6\u5206\u5c42\u91c7\u6837\u548c\u81ea\u9002\u5e94\u673a\u5668\u5b66\u4e60\u5143\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba1\u5c0f\u5931\u6548\u6982\u7387\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u975e\u7ebf\u6027\u6709\u9650\u5143\u6a21\u578b\u4e2d\u8ba1\u7b97\u5c0f\u5931\u6548\u6982\u7387\u65f6\u4ecd\u9700\u8981\u5927\u91cf\u6a21\u578b\u8bc4\u4f30\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u5206\u5c42\u91c7\u6837\u751f\u6210\u9ad8\u4fdd\u771f\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u5143\u6a21\u578b\u4f5c\u4e3a\u4f4e\u4fdd\u771f\u6a21\u578b\uff0c\u7ed3\u5408\u591a\u4fdd\u771f\u5ea6\u8499\u7279\u5361\u7f57\u6846\u67b6\u4f30\u8ba1\u5931\u6548\u6982\u7387\u3002", "result": "\u5e94\u7528\u4e8e\u9ad8\u5c42\u94a2\u7ed3\u6784\u5efa\u7b51\uff0c\u51c6\u786e\u4f30\u8ba1\u975e\u7ebf\u6027\u54cd\u5e94\u7684\u8d85\u8d8a\u6982\u7387\u66f2\u7ebf\uff0c\u663e\u8457\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7cfb\u7edf\u7684\u7f55\u89c1\u4e8b\u4ef6\u5206\u6790\u3002"}}
{"id": "2508.00754", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00754", "abs": "https://arxiv.org/abs/2508.00754", "authors": ["Yaxin Ma", "Benjamin Colburn", "Jose C. Principe"], "title": "A Simple and Effective Method for Uncertainty Quantification and OOD Detection", "comment": null, "summary": "Bayesian neural networks and deep ensemble methods have been proposed for\nuncertainty quantification; however, they are computationally intensive and\nrequire large storage. By utilizing a single deterministic model, we can solve\nthe above issue. We propose an effective method based on feature space density\nto quantify uncertainty for distributional shifts and out-of-distribution (OOD)\ndetection. Specifically, we leverage the information potential field derived\nfrom kernel density estimation to approximate the feature space density of the\ntraining set. By comparing this density with the feature space representation\nof test samples, we can effectively determine whether a distributional shift\nhas occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons\nand Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The\nresults demonstrate that our method outperforms baseline models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u7a7a\u95f4\u5bc6\u5ea6\u7684\u5355\u786e\u5b9a\u6027\u6a21\u578b\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u5206\u5e03\u504f\u79fb\u548cOOD\u68c0\u6d4b\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u548c\u6df1\u5ea6\u96c6\u6210\u65b9\u6cd5\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u9ad8\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u4fe1\u606f\u52bf\u573a\u8fd1\u4f3c\u8bad\u7ec3\u96c6\u7279\u5f81\u7a7a\u95f4\u5bc6\u5ea6\uff0c\u901a\u8fc7\u6bd4\u8f83\u6d4b\u8bd5\u6837\u672c\u7279\u5f81\u7a7a\u95f4\u8868\u793a\u68c0\u6d4b\u5206\u5e03\u504f\u79fb\u3002", "result": "\u57282D\u5408\u6210\u6570\u636e\u96c6\u548cOOD\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5355\u786e\u5b9a\u6027\u6a21\u578b\u65b9\u6cd5\u80fd\u9ad8\u6548\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u5206\u5e03\u504f\u79fb\u548cOOD\u68c0\u6d4b\u3002"}}
{"id": "2508.00758", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00758", "abs": "https://arxiv.org/abs/2508.00758", "authors": ["Timur Sattarov", "Marco Schreyer", "Damian Borth"], "title": "Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data", "comment": "22 pages, 16 figures, 7 tables, preprint version", "summary": "Anomaly detection in tabular data remains challenging due to complex feature\ninteractions and the scarcity of anomalous examples. Denoising autoencoders\nrely on fixed-magnitude noise, limiting adaptability to diverse data\ndistributions. Diffusion models introduce scheduled noise and iterative\ndenoising, but lack explicit reconstruction mappings. We propose the\nDiffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates\ndiffusion-based noise scheduling and contrastive learning into the encoding\nprocess to improve anomaly detection. We evaluated DDAE on 57 datasets from\nADBench. Our method outperforms in semi-supervised settings and achieves\ncompetitive results in unsupervised settings, improving PR-AUC by up to 65%\n(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)\nmodel baselines. We observed that higher noise levels benefit unsupervised\ntraining, while lower noise with linear scheduling is optimal in\nsemi-supervised settings. These findings underscore the importance of\nprincipled noise strategies in tabular anomaly detection.", "AI": {"tldr": "DDAE\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u63d0\u5347\u8868\u683c\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u5728ADBench\u768457\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u8868\u683c\u6570\u636e\u5f02\u5e38\u68c0\u6d4b\u56e0\u590d\u6742\u7279\u5f81\u4ea4\u4e92\u548c\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\u800c\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u53bb\u566a\u81ea\u7f16\u7801\u5668\u548c\u6269\u6563\u6a21\u578b\u5404\u6709\u5c40\u9650\u3002", "method": "\u63d0\u51faDDAE\u6846\u67b6\uff0c\u6574\u5408\u6269\u6563\u566a\u58f0\u8c03\u5ea6\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4f18\u5316\u7f16\u7801\u8fc7\u7a0b\u3002", "result": "\u5728\u534a\u76d1\u7763\u548c\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0cDDAE\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0cPR-AUC\u548cROC-AUC\u63d0\u5347\u663e\u8457\u3002", "conclusion": "\u566a\u58f0\u7b56\u7565\u5bf9\u8868\u683c\u5f02\u5e38\u68c0\u6d4b\u81f3\u5173\u91cd\u8981\uff0c\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u9002\u7528\u4e8e\u4e0d\u540c\u8bad\u7ec3\u573a\u666f\u3002"}}
{"id": "2508.00768", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00768", "abs": "https://arxiv.org/abs/2508.00768", "authors": ["Antonio Tudisco", "Andrea Marchesin", "Maurizio Zamboni", "Mariagrazia Graziano", "Giovanna Turvani"], "title": "Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy", "comment": null, "summary": "Recent advancements in Quantum Computing and Machine Learning have increased\nattention to Quantum Machine Learning (QML), which aims to develop machine\nlearning models by exploiting the quantum computing paradigm. One of the widely\nused models in this area is the Variational Quantum Circuit (VQC), a hybrid\nmodel where the quantum circuit handles data inference while classical\noptimization adjusts the parameters of the circuit. The quantum circuit\nconsists of an encoding layer, which loads data into the circuit, and a\ntemplate circuit, known as the ansatz, responsible for processing the data.\nThis work involves performing an analysis by considering both Amplitude- and\nAngle-encoding models, and examining how the type of rotational gate applied\naffects the classification performance of the model. This comparison is carried\nout by training the different models on two datasets, Wine and Diabetes, and\nevaluating their performance. The study demonstrates that, under identical\nmodel topologies, the difference in accuracy between the best and worst models\nranges from 10% to 30%, with differences reaching up to 41%. Moreover, the\nresults highlight how the choice of rotational gates used in encoding can\nsignificantly impact the model's classification performance. The findings\nconfirm that the embedding represents a hyperparameter for VQC models.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u7684\u6027\u80fd\uff0c\u6bd4\u8f83\u4e86\u632f\u5e45\u7f16\u7801\u548c\u89d2\u5ea6\u7f16\u7801\u6a21\u578b\u5728\u4e0d\u540c\u65cb\u8f6c\u95e8\u4e0b\u7684\u5206\u7c7b\u8868\u73b0\uff0c\u53d1\u73b0\u7f16\u7801\u65b9\u5f0f\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u548c\u673a\u5668\u5b66\u4e60\u7684\u7ed3\u5408\u63a8\u52a8\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u7684\u53d1\u5c55\uff0c\u5176\u4e2d\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQC\uff09\u662f\u4e00\u79cd\u5e38\u7528\u6a21\u578b\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u7f16\u7801\u65b9\u5f0f\u548c\u65cb\u8f6c\u95e8\u5bf9VQC\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u6bd4\u8f83\u4e86\u632f\u5e45\u7f16\u7801\u548c\u89d2\u5ea6\u7f16\u7801\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5728Wine\u548cDiabetes\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\u6a21\u578b\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u65cb\u8f6c\u95e8\u5bf9\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u540c\u6a21\u578b\u62d3\u6251\u4e0b\uff0c\u6700\u4f73\u548c\u6700\u5dee\u6a21\u578b\u7684\u51c6\u786e\u7387\u5dee\u5f02\u53ef\u8fbe10%\u81f330%\uff0c\u751a\u81f3\u9ad8\u8fbe41%\u3002\u7f16\u7801\u65b9\u5f0f\u7684\u9009\u62e9\u663e\u8457\u5f71\u54cd\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u7f16\u7801\u65b9\u5f0f\u662fVQC\u6a21\u578b\u7684\u4e00\u4e2a\u91cd\u8981\u8d85\u53c2\u6570\uff0c\u5176\u9009\u62e9\u5bf9\u6a21\u578b\u6027\u80fd\u6709\u51b3\u5b9a\u6027\u5f71\u54cd\u3002"}}
{"id": "2508.00785", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00785", "abs": "https://arxiv.org/abs/2508.00785", "authors": ["Bushra Akter", "Md Biplob Hosen", "Sabbir Ahmed", "Mehrin Anannya", "Md. Farhad Hossain"], "title": "Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors", "comment": null, "summary": "Academic performance depends on a multivariable nexus of socio-academic and\nfinancial factors. This study investigates these influences to develop\neffective strategies for optimizing students' CGPA. To achieve this, we\nreviewed various literature to identify key influencing factors and constructed\nan initial hypothetical causal graph based on the findings. Additionally, an\nonline survey was conducted, where 1,050 students participated, providing\ncomprehensive data for analysis. Rigorous data preprocessing techniques,\nincluding cleaning and visualization, ensured data quality before analysis.\nCausal analysis validated the relationships among variables, offering deeper\ninsights into their direct and indirect effects on CGPA. Regression models were\nimplemented for CGPA prediction, while classification models categorized\nstudents based on performance levels. Ridge Regression demonstrated strong\npredictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared\nError of 0.023. Random Forest outperformed in classification, attaining an\nF1-score near perfection and an accuracy of 98.68%. Explainable AI techniques\nsuch as SHAP, LIME, and Interpret enhanced model interpretability, highlighting\ncritical factors such as study hours, scholarships, parental education, and\nprior academic performance. The study culminated in the development of a\nweb-based application that provides students with personalized insights,\nallowing them to predict academic performance, identify areas for improvement,\nand make informed decisions to enhance their outcomes.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5f71\u54cd\u5b66\u751fCGPA\u7684\u591a\u53d8\u91cf\u56e0\u7d20\uff0c\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u3001\u8c03\u67e5\u6570\u636e\u5206\u6790\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u9884\u6d4b\u548c\u4f18\u5316\u5b66\u672f\u8868\u73b0\u7684\u7f51\u7edc\u5e94\u7528\u3002", "motivation": "\u5b66\u672f\u8868\u73b0\u53d7\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u7814\u7a76\u65e8\u5728\u8bc6\u522b\u8fd9\u4e9b\u56e0\u7d20\u5e76\u5f00\u53d1\u7b56\u7565\u4ee5\u4f18\u5316\u5b66\u751fCGPA\u3002", "method": "\u7ed3\u5408\u6587\u732e\u7efc\u8ff0\u6784\u5efa\u56e0\u679c\u56fe\uff0c\u901a\u8fc7\u5728\u7ebf\u8c03\u67e5\u6536\u96c6\u6570\u636e\uff0c\u4f7f\u7528\u56de\u5f52\u548c\u5206\u7c7b\u6a21\u578b\u5206\u6790\uff0c\u5e76\u5e94\u7528\u53ef\u89e3\u91caAI\u6280\u672f\u3002", "result": "Ridge\u56de\u5f52\u9884\u6d4bCGPA\u8bef\u5dee\u4f4e\uff08MAE=0.12\uff09\uff0c\u968f\u673a\u68ee\u6797\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\uff0898.68%\uff09\u3002\u5173\u952e\u56e0\u7d20\u5305\u62ec\u5b66\u4e60\u65f6\u95f4\u3001\u5956\u5b66\u91d1\u7b49\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u9884\u6d4b\u5de5\u5177\uff0c\u5e2e\u52a9\u5b66\u751f\u4f18\u5316\u5b66\u672f\u8868\u73b0\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2508.00806", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.00806", "abs": "https://arxiv.org/abs/2508.00806", "authors": ["Ping Chen", "Zhuohong Deng", "Ping Li", "Shuibing He", "Hongzi Zhu", "Yi Zheng", "Zhefeng Wang", "Baoxing Huai", "Minyi Guo"], "title": "Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management", "comment": "8 pages", "summary": "Training large language models often employs recomputation to alleviate\nmemory pressure, which can introduce up to 30% overhead in real-world\nscenarios. In this paper, we propose Adacc, a novel memory management framework\nthat combines adaptive compression and activation checkpointing to reduce the\nGPU memory footprint. It comprises three modules: (1) We design layer-specific\ncompression algorithms that account for outliers in LLM tensors, instead of\ndirectly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We\npropose an optimal scheduling policy that employs MILP to determine the best\nmemory optimization for each tensor. (3) To accommodate changes in training\ntensors, we introduce an adaptive policy evolution mechanism that adjusts the\npolicy during training to enhance throughput. Experimental results show that\nAdacc can accelerate the LLM training by 1.01x to 1.37x compared to\nstate-of-the-art frameworks, while maintaining comparable model accuracy to the\nBaseline.", "AI": {"tldr": "Adacc\u662f\u4e00\u4e2a\u7ed3\u5408\u81ea\u9002\u5e94\u538b\u7f29\u548c\u6fc0\u6d3b\u68c0\u67e5\u70b9\u7684\u65b0\u578b\u5185\u5b58\u7ba1\u7406\u6846\u67b6\uff0c\u65e8\u5728\u51cf\u5c11GPU\u5185\u5b58\u5360\u7528\uff0c\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u91cd\u8ba1\u7b97\u4f1a\u5f15\u5165\u9ad8\u8fbe30%\u7684\u5f00\u9500\uff0cAdacc\u65e8\u5728\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u7ba1\u7406\u51cf\u5c11\u8fd9\u79cd\u5f00\u9500\u3002", "method": "Adacc\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u5c42\u7279\u5b9a\u538b\u7f29\u7b97\u6cd5\u3001\u57fa\u4e8eMILP\u7684\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u548c\u81ea\u9002\u5e94\u7b56\u7565\u6f14\u5316\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cAdacc\u80fd\u5c06\u8bad\u7ec3\u901f\u5ea6\u63d0\u53471.01x\u81f31.37x\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "Adacc\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29\u548c\u7b56\u7565\u4f18\u5316\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5185\u5b58\u5360\u7528\u5e76\u52a0\u901f\u4e86\u8bad\u7ec3\u3002"}}
