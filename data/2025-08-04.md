<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 8]
- [cs.LG](#cs.LG) [Total: 63]
- [stat.ML](#stat.ML) [Total: 2]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Subband Architecture Aided Selective Fixed-Filter Active Noise Control](https://arxiv.org/abs/2508.00603)
*Hong-Cheng Liang,Man-Wai Mak,Kong Aik Lee*

Main category: eess.SP

TL;DR: 提出了一种基于延迟子带结构的新型选择性固定滤波器方案，用于处理非均匀功率谱密度的噪声，提高了噪声抑制效果和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统前馈选择性固定滤波器方法只能处理有限类型的噪声，且在输入噪声功率谱密度非均匀时性能下降。

Method: 采用延迟子带结构，离线训练子带控制滤波器并存储；在线控制阶段通过频率带匹配机制分配滤波器，并使用权重叠加技术合成全带滤波器。

Result: 实验表明，该方案具有快速收敛、有效降噪和强鲁棒性。

Conclusion: 新方案显著提升了复杂噪声环境下的噪声抑制性能。

Abstract: The feedforward selective fixed-filter method selects the most suitable
pre-trained control filter based on the spectral features of the detected
reference signal, effectively avoiding slow convergence in conventional
adaptive algorithms. However, it can only handle limited types of noises, and
the performance degrades when the input noise exhibits non-uniform power
spectral density. To address these limitations, this paper devises a novel
selective fixed-filter scheme based on a delayless subband structure. In the
off-line training stage, subband control filters are pre-trained for different
frequency ranges and stored in a dedicated sub-filter database. During the
on-line control stage, the incoming noise is decomposed using a polyphase FFT
filter bank, and a frequency-band-matching mechanism assigns each subband
signal the most appropriate control filter. Subsequently, a weight stacking
technique is employed to combine all subband weights into a fullband filter,
enabling real-time noise suppression. Experimental results demonstrate that the
proposed scheme provides fast convergence, effective noise reduction, and
strong robustness in handling more complicated noisy environments.

</details>


### [2] [Closed-form Expression for the Power Profile in Wideband Systems with Inter-channel Stimulated Raman Scattering](https://arxiv.org/abs/2508.00093)
*Lucas Alves Zischler,Chiara Lasagni,Paolo Serena,Alberto Bononi,Giammarco Di Sciullo,Divya A. Shaji,Antonio Mecozzi,Cristian Antonelli*

Main category: eess.SP

TL;DR: 提出了一种近似闭式表达式，用于估计宽带系统中ISRS和光纤损耗的联合效应，并通过数值验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 宽带系统中ISRS和通道相关损耗的非均匀衰减特性需要精确估计，传统数值方法复杂。

Method: 提出闭式表达式描述通道功率分布，并通过数值解验证其准确性。

Result: 验证表明该表达式在单跨和多跨光纤链路中均具有高精度，并推导了逆表达式用于目标OSNR配置。

Conclusion: 该闭式表达式为宽带系统设计提供了高效工具，可用于优化发射功率配置。

Abstract: Wideband systems experience significant inter-channel stimulated Raman
scattering (ISRS) and channel-dependent losses. Due to the non-uniform
attenuation profile, the combined effects of ISRS and fiber loss can only be
accurately estimated using numerical methods. In this work, we present an
approximate closed-form expression for the channels' power profile accounting
for these combined effects. We validate the proposed expression against
numerical solutions in the case of CLU transmission, showing high accuracy for
both single-span and multi-span fiber-optic links. Additionally, we derive an
inverse expression, formulated as a function of the output power, which can be
utilized to target a desired optical signal-to-noise ratio (OSNR) profile
through pre-emphasis of the launched channel powers.

</details>


### [3] [RIS-MAE: A Self-Supervised Modulation Classification Method Based on Raw IQ Signals and Masked Autoencoder](https://arxiv.org/abs/2508.00274)
*Yunfei Liu,Mingxuan Liu,Wupeng Xie,Xinzhu Liu,Wenxue Liu,Yangang Sun,Xin Qiu,Cui Yuan,Jinhai Li*

Main category: eess.SP

TL;DR: 论文提出了一种名为RIS-MAE的自监督学习框架，用于解决自动调制分类中的关键问题，如特征丢失和依赖标注数据。


<details>
  <summary>Details</summary>
Motivation: 自动调制分类（AMC）在智能无线通信系统中至关重要，但现有方法存在特征丢失和对标注数据依赖的问题。

Method: RIS-MAE使用掩码自编码器从无标签数据中学习信号特征，直接处理原始IQ序列，通过随机掩码和重建捕获时域特征。

Result: 在四个数据集上的测试表明，RIS-MAE在少样本和跨域任务中优于现有方法，并展示了强大的泛化能力。

Conclusion: RIS-MAE为解决AMC中的关键问题提供了有效方案，具有实际部署的潜力。

Abstract: Automatic modulation classification (AMC) is a basic technology in
intelligent wireless communication systems. It is important for tasks such as
spectrum monitoring, cognitive radio, and secure communications. In recent
years, deep learning methods have made great progress in AMC. However,
mainstream methods still face two key problems. First, they often use
time-frequency images instead of raw signals. This causes loss of key
modulation features and reduces adaptability to different communication
conditions. Second, most methods rely on supervised learning. This needs a
large amount of labeled data, which is hard to get in real-world environments.
To solve these problems, we propose a self-supervised learning framework called
RIS-MAE. RIS-MAE uses masked autoencoders to learn signal features from
unlabeled data. It takes raw IQ sequences as input. By applying random masking
and reconstruction, it captures important time-domain features such as
amplitude, phase, etc. This helps the model learn useful and transferable
representations. RIS-MAE is tested on four datasets. The results show that it
performs better than existing methods in few-shot and cross-domain tasks.
Notably, it achieves high classification accuracy on previously unseen datasets
with only a small number of fine-tuning samples, confirming its generalization
ability and potential for real-world deployment.

</details>


### [4] [Model-Driven Deep Learning Enhanced Joint Beamforming and Mode Switching for RDARS-Aided MIMO Systems](https://arxiv.org/abs/2508.00326)
*Chengwang Ji,Kehui Li,Haiquan Lu,Qiaoyan Peng,Jintao Wang,Shaodan Ma*

Main category: eess.SP

TL;DR: 论文提出了一种基于RDARS架构的6G无线网络优化方法，通过联合优化基站和RDARS的波束成形矩阵以及RDARS的模式切换矩阵，最大化加权和速率（WSR）。采用PWM算法结合MM和WMMSE方法解决非凸优化问题，并引入模型驱动的深度学习加速收敛。


<details>
  <summary>Details</summary>
Motivation: RDARS架构在6G网络中具有潜力，但其动态工作模式配置的优化问题复杂且非凸，需要高效算法解决。

Method: 提出PWM算法，结合MM和WMMSE方法，并集成模型驱动的深度学习以加速收敛。

Result: 仿真显示PWM-BFNet算法迭代次数减半，在高发射功率和大RDARS传输单元数场景下性能分别提升26.53%和103.2%。

Conclusion: PWM-BFNet算法有效解决了RDARS系统的优化问题，显著提升了系统性能。

Abstract: Reconfigurable distributed antenna and reflecting surface (RDARS) is a
promising architecture for future sixth-generation (6G) wireless networks. In
particular, the dynamic working mode configuration for the RDARS-aided system
brings an extra selection gain compared to the existing reconfigurable
intelligent surface (RIS)-aided system and distributed antenna system (DAS). In
this paper, we consider the RDARS-aided downlink multiple-input multiple-output
(MIMO) system and aim to maximize the weighted sum rate (WSR) by jointly
optimizing the beamforming matrices at the based station (BS) and RDARS, as
well as mode switching matrix at RDARS. The optimization problem is challenging
to be solved due to the non-convex objective function and mixed integer binary
constraint. To this end, a penalty term-based weight minimum mean square error
(PWM) algorithm is proposed by integrating the majorization-minimization (MM)
and weight minimum mean square error (WMMSE) methods. To further escape the
local optimum point in the PWM algorithm, a model-driven DL method is
integrated into this algorithm, where the key variables related to the
convergence of PWM algorithm are trained to accelerate the convergence speed
and improve the system performance. Simulation results are provided to show
that the PWM-based beamforming network (PWM-BFNet) can reduce the number of
iterations by half and achieve performance improvements of 26.53% and 103.2% at
the scenarios of high total transmit power and a large number of RDARS transmit
elements (TEs), respectively.

</details>


### [5] [STAR-RIS-aided RSMA for the URLLC multi-user MIMO Downlink](https://arxiv.org/abs/2508.00409)
*Mohammad Soleymani,Ignacio Santamaria,Eduard Jorswieck,Robert Schober,Lajos Hanzo*

Main category: eess.SP

TL;DR: RSMA与STAR-RIS结合优化FBL MIMO下行链路的能效，提出交替优化算法，显著提升能效。


<details>
  <summary>Details</summary>
Motivation: 提升有限块长度MIMO下行链路的能效。

Method: 交替优化算法联合优化发射波束成形矩阵、STAR-RIS配置和速率分割参数。

Result: RSMA与STAR-RIS协同显著提升能效，优于反射RIS和SDMA。

Conclusion: RSMA与STAR-RIS结合在能效优化方面具有显著优势。

Abstract: Rate splitting multiple access (RSMA) is intrinsically amalgamated with
simultaneously transmitting and reflecting (STAR) reconfigurable intelligent
surfaces (RIS) to enhance energy efficiency (EE) of the finite block length
(FBL) multiple-input multiple-output (MIMO) downlink. An alternating
optimization-based algorithm is proposed to jointly optimize the transmit
beamforming matrices, STAR-RIS configurations, and rate-splitting parameters.
STAR-RIS attains 360-degree full-plane coverage, while RSMA provides a
prominent gain by efficiently managing interference. Numerical results reveal a
strong synergy between RSMA and STAR-RIS, demonstreating significant EE gains
over reflective RIS and spatial division multiple access (SDMA).

</details>


### [6] [When Vision-Language Model (VLM) Meets Beam Prediction: A Multimodal Contrastive Learning Framework](https://arxiv.org/abs/2508.00456)
*Ji Wang,Bin Tang,Jian Xiao,Qimei Cui,Xingwang Li,Tony Q. S. Quek*

Main category: eess.SP

TL;DR: 提出了一种基于视觉语言模型（VLM）的多模态波束预测框架，通过对比学习提升跨模态一致性，显著提高了毫米波波束预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 复杂动态的传播环境使毫米波波束预测面临挑战，传统基于实时信道状态信息（CSI）的方法计算成本高且准确性不足。

Method: 采用VLM驱动的对比学习框架，整合多模态数据，通过模态特定编码器对齐图像和LiDAR特征，并引入语言模态增强一致性。

Result: 在DeepSense-6G数据集上，DBA-Score达到0.9016，平均提升1.46%。

Conclusion: VLM框架为毫米波波束预测提供了语义基础，显著优于现有方法。

Abstract: As the real propagation environment becomes in creasingly complex and
dynamic, millimeter wave beam prediction faces huge challenges. However, the
powerful cross modal representation capability of vision-language model (VLM)
provides a promising approach. The traditional methods that rely on real-time
channel state information (CSI) are computationally expensive and often fail to
maintain accuracy in such environments. In this paper, we present a VLM-driven
contrastive learning based multimodal beam prediction framework that integrates
multimodal data via modality-specific encoders. To enforce cross-modal
consistency, we adopt a contrastive pretraining strategy to align image and
LiDAR features in the latent space. We use location information as text prompts
and connect it to the text encoder to introduce language modality, which
further improves cross-modal consistency. Experiments on the DeepSense-6G
dataset show that our VLM backbone provides additional semantic grounding.
Compared with existing methods, the overall distance-based accuracy score
(DBA-Score) of 0.9016, corresponding to 1.46% average improvement.

</details>


### [7] [Feasibility of Extracting Skin Nerve Activity from Electrocardiogram Recorded at A Low Sampling Frequency](https://arxiv.org/abs/2508.00494)
*Youngsun Kong,Farnoush Baghestani,I-Ping Chen,Ki Chon*

Main category: eess.SP

TL;DR: 研究表明，皮肤神经活动（SKNA）可以从低采样频率（如0.5或1 kHz）的ECG信号中提取，无需传统的高采样频率（>2 kHz）。


<details>
  <summary>Details</summary>
Motivation: 传统SKNA提取需要高采样频率（>2 kHz），但许多ECG设备（尤其是可穿戴设备）采样频率较低（≤1 kHz），限制了SKNA的应用。

Method: 通过16名参与者的ECG信号，在0.5、1和4 kHz采样频率下提取SKNA，并进行统计分析。

Result: 不同采样频率（0.5、1和4 kHz）提取的SKNA指标无显著差异。

Conclusion: 低采样频率的ECG设备可可靠地收集SKNA，前提是肌肉伪影污染最小。

Abstract: Skin nerve activity (SKNA) derived from electrocardiogram (ECG) signals has
been a promising non-invasive surrogate for accurate and effective assessment
of the sympathetic nervous system (SNS). Typically, SKNA extraction requires a
higher sampling frequency than the typical ECG recording requirement (> 2 kHz)
because analysis tools extract SKNA from the 0.5-1 kHz frequency band. However,
ECG recording systems commonly provide a sampling frequency of 1 kHz or lower,
particularly for wearable devices. Our recent power spectral analysis exhibited
that 150-500 Hz frequency bands are dominant during sympathetic stimulation.
Therefore, we hypothesize that SKNA can be extracted from ECG sampled at a
lower sampling frequency. We collected ECG signals from 16 participants during
SNS stimulation and resampled the signals at 0.5, 1, and 4 kHz. Our statistical
analyses of significance, classification performance, and reliability indicate
no significant difference between SKNA indices derived from ECG signals sampled
at 0.5, 1, and 4 kHz. Our findings indicate that conventional ECG devices,
which are limited to low sampling rates due to resource constraints or outdated
guidelines, can be used to reliably collect SKNA if muscle artifact
contamination is minimal.

</details>


### [8] [Multibeam High Throughput Satellite: Hardware Foundation, Resource Allocation, and Precoding](https://arxiv.org/abs/2508.00800)
*Rui Chen,Wen-Xuan Long,Bing-Qian Wang,Yuan He,Rui-Jin Sun,Nan Cheng,Gan Zheng,Dusit Niyato*

Main category: eess.SP

TL;DR: 本文综述了多波束高吞吐量卫星（HTS）系统的现状、挑战与前景，涵盖硬件基础、资源分配、预编码方法及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 卫星通信是6G的关键技术，HTS系统通过多波束和频分复用技术实现Tbps级容量，需总结其最新进展与挑战。

Method: 总结HTS硬件基础（地面站、星载载荷、用户终端），回顾灵活资源分配方案（带宽、功率、时隙），分类多波束预编码方法（单网关、多网关、星载、混合）。

Result: HTS系统通过优化资源分配和预编码实现全频复用和干扰消除，但仍面临Q/V频段链路中断、同步、CSI准确性等挑战。

Conclusion: 未来研究需解决HTS系统的技术挑战，以提升性能并为地面网络覆盖不足地区提供高速数据服务。

Abstract: With its wide coverage and uninterrupted service, satellite communication is
a critical technology for next-generation 6G communications. High throughput
satellite (HTS) systems, utilizing multipoint beam and frequency multiplexing
techniques, enable satellite communication capacity of up to Tbps to meet the
growing traffic demand. Therefore, it is imperative to review
the-state-of-the-art of multibeam HTS systems and identify their associated
challenges and perspectives. Firstly, we summarize the multibeam HTS hardware
foundations, including ground station systems, on-board payloads, and user
terminals. Subsequently, we review the flexible on-board radio resource
allocation approaches of bandwidth, power, time slot, and joint allocation
schemes of HTS systems to optimize resource utilization and cater to
non-uniform service demand. Additionally, we survey multibeam precoding methods
for the HTS system to achieve full-frequency reuse and interference
cancellation, which are classified according to different deployments such as
single gateway precoding, multiple gateway precoding, on-board precoding, and
hybrid on-board/on-ground precoding. Finally, we disscuss the challenges
related to Q/V band link outage, time and frequency synchronization of
gateways, the accuracy of channel state information (CSI), payload light-weight
development, and the application of deep learning (DL). Research on these
topics will contribute to enhancing the performance of HTS systems and finally
delivering high-speed data to areas underserved by terrestrial networks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [DO-EM: Density Operator Expectation Maximization](https://arxiv.org/abs/2507.22786)
*Adit Vishnu,Abhay Shastry,Dhruva Kashyap,Chiranjib Bhattacharyya*

Main category: cs.LG

TL;DR: 本文提出了一种基于密度算子的期望最大化框架（DO-EM），用于在经典硬件上训练量子潜变量模型，解决了传统方法无法扩展到真实数据的问题。通过量子信息投影（QIP）重新定义E步骤，并引入QiDBM模型，在MNIST数据集上表现优于经典模型。


<details>
  <summary>Details</summary>
Motivation: 密度算子在量子计算中具有基础性作用，但现有训练算法（如量子玻尔兹曼机）无法扩展到真实数据（如MNIST）。期望最大化算法在经典潜变量模型中表现优异，但缺乏量子条件概率的明确定义，阻碍了其在量子模型中的应用。

Method: 提出DO-EM算法，将E步骤重新定义为量子信息投影（QIP）问题，利用Petz恢复映射解决条件概率缺失问题。引入QiDBM模型，结合对比散度训练。

Result: DO-EM算法确保了对数似然的非递减性。QiDBM在MNIST图像生成任务中，Fréchet Inception Distance比经典DBM降低了40-60%。

Conclusion: DO-EM框架为量子潜变量模型提供了可扩展的训练方法，QiDBM展示了量子模型在真实数据上的潜力。

Abstract: Density operators, quantum generalizations of probability distributions, are
gaining prominence in machine learning due to their foundational role in
quantum computing. Generative modeling based on density operator models
(\textbf{DOMs}) is an emerging field, but existing training algorithms -- such
as those for the Quantum Boltzmann Machine -- do not scale to real-world data,
such as the MNIST dataset. The Expectation-Maximization algorithm has played a
fundamental role in enabling scalable training of probabilistic latent variable
models on real-world datasets. \textit{In this paper, we develop an
Expectation-Maximization framework to learn latent variable models defined
through \textbf{DOMs} on classical hardware, with resources comparable to those
used for probabilistic models, while scaling to real-world data.} However,
designing such an algorithm is nontrivial due to the absence of a well-defined
quantum analogue to conditional probability, which complicates the Expectation
step. To overcome this, we reformulate the Expectation step as a quantum
information projection (QIP) problem and show that the Petz Recovery Map
provides a solution under sufficient conditions. Using this formulation, we
introduce the Density Operator Expectation Maximization (DO-EM) algorithm -- an
iterative Minorant-Maximization procedure that optimizes a quantum evidence
lower bound. We show that the \textbf{DO-EM} algorithm ensures non-decreasing
log-likelihood across iterations for a broad class of models. Finally, we
present Quantum Interleaved Deep Boltzmann Machines (\textbf{QiDBMs}), a
\textbf{DOM} that can be trained with the same resources as a DBM. When trained
with \textbf{DO-EM} under Contrastive Divergence, a \textbf{QiDBM} outperforms
larger classical DBMs in image generation on the MNIST dataset, achieving a
40--60\% reduction in the Fr\'echet Inception Distance.

</details>


### [10] [Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting](https://arxiv.org/abs/2508.00040)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 该论文提出了一种结合贝叶斯机制检测和条件神经过程的方法（R-NP），用于预测德国电力市场的24小时电价，并通过多标准评估（TOPSIS）验证其综合性能优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 电力市场电价预测对电池存储优化至关重要，但传统模型在预测精度和实际应用效果之间存在差距。

Method: 使用DS-HDP-HMM进行机制检测，每个机制由独立的CNP建模，最终预测为机制加权混合。

Result: R-NP在TOPSIS评估中表现最平衡，优于DNN和LEAR模型。

Conclusion: R-NP是一种综合性能更优的电价预测方法，适用于多种应用场景。

Abstract: This work integrates Bayesian regime detection with conditional neural
processes for 24-hour electricity price prediction in the German market. Our
methodology integrates regime detection using a disentangled sticky
hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to
daily electricity prices. Each identified regime is subsequently modeled by an
independent conditional neural process (CNP), trained to learn localized
mappings from input contexts to 24-dimensional hourly price trajectories, with
final predictions computed as regime-weighted mixtures of these CNP outputs. We
rigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated
auto-regressive (LEAR) models by integrating their forecasts into diverse
battery storage optimization frameworks, including price arbitrage, risk
management, grid services, and cost minimization. This operational utility
assessment revealed complex performance trade-offs: LEAR often yielded superior
absolute profits or lower costs, while DNN showed exceptional optimality in
specific cost-minimization contexts. Recognizing that raw prediction accuracy
doesn't always translate to optimal operational outcomes, we employed TOPSIS as
a comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified
LEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model
emerged as the most balanced and preferred solution for 2021, 2022 and 2023.

</details>


### [11] [Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network](https://arxiv.org/abs/2508.00692)
*Young-ho Cho,Hao Zhu,Duehee Lee,Ross Baldick*

Main category: cs.LG

TL;DR: 该论文提出了一种结合广义动态因子模型（GDFM）和生成对抗网络（GAN）的方法，用于合成分布式风电场的长期风电功率场景，以更好地捕捉时空相关性和统计特性。


<details>
  <summary>Details</summary>
Motivation: 资源充足性研究需要准确的风电功率场景，现有方法（如GDFM或GAN单独使用）在捕捉时空相关性和波形特性方面存在不足。

Method: 结合GDFM和GAN，利用GAN提取动态因子并作为GDFM的滤波器，以同时表示时空和频率相关性。

Result: 数值测试表明，该方法在合成澳大利亚风电功率场景时优于其他替代方法，能更好地模拟实际风电的统计特性。

Conclusion: GDFM与GAN的结合在风电功率场景合成中表现出优越性能，为资源充足性研究提供了更可靠的输入。

Abstract: For conducting resource adequacy studies, we synthesize multiple long-term
wind power scenarios of distributed wind farms simultaneously by using the
spatio-temporal features: spatial and temporal correlation, waveforms, marginal
and ramp rates distributions of waveform, power spectral densities, and
statistical characteristics. Generating the spatial correlation in scenarios
requires the design of common factors for neighboring wind farms and
antithetical factors for distant wind farms. The generalized dynamic factor
model (GDFM) can extract the common factors through cross spectral density
analysis, but it cannot closely imitate waveforms. The GAN can synthesize
plausible samples representing the temporal correlation by verifying samples
through a fake sample discriminator. To combine the advantages of GDFM and GAN,
we use the GAN to provide a filter that extracts dynamic factors with temporal
information from the observation data, and we then apply this filter in the
GDFM to represent both spatial and frequency correlations of plausible
waveforms. Numerical tests on the combination of GDFM and GAN have demonstrated
performance improvements over competing alternatives in synthesizing wind power
scenarios from Australia, better realizing plausible statistical
characteristics of actual wind power compared to alternatives such as the GDFM
with a filter synthesized from distributions of actual dynamic filters and the
GAN with direct synthesis without dynamic factors.

</details>


### [12] [Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion](https://arxiv.org/abs/2508.00037)
*Tong Nie,Jian Sun,Wei Ma*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理定律启发的可扩展时空Transformer（ScaleSTF），用于预测大规模城市网络的动态，解决了现有模型在效能与效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 城市网络系统涉及复杂且未知的规则，现有数据驱动模型（如图神经网络）在效能与效率之间存在权衡，难以应用于大规模网络。

Method: 提出了一种基于Transformer结构的可解释神经扩散方案，其注意力层由低维嵌入诱导，具有线性复杂度。

Result: ScaleSTF在交通流量、太阳能发电和智能电表等大规模城市系统中验证了其先进性能和卓越的可扩展性。

Conclusion: 该研究为大规模城市网络动态预测提供了新视角，展示了物理定律启发的模型设计的潜力。

Abstract: Networked urban systems facilitate the flow of people, resources, and
services, and are essential for economic and social interactions. These systems
often involve complex processes with unknown governing rules, observed by
sensor-based time series. To aid decision-making in industrial and engineering
contexts, data-driven predictive models are used to forecast spatiotemporal
dynamics of urban systems. Current models such as graph neural networks have
shown promise but face a trade-off between efficacy and efficiency due to
computational demands. Hence, their applications in large-scale networks still
require further efforts. This paper addresses this trade-off challenge by
drawing inspiration from physical laws to inform essential model designs that
align with fundamental principles and avoid architectural redundancy. By
understanding both micro- and macro-processes, we present a principled
interpretable neural diffusion scheme based on Transformer-like structures
whose attention layers are induced by low-dimensional embeddings. The proposed
scalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is
validated on large-scale urban systems including traffic flow, solar power, and
smart meters, showing state-of-the-art performance and remarkable scalability.
Our results constitute a fresh perspective on the dynamics prediction in
large-scale urban networks.

</details>


### [13] [EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes](https://arxiv.org/abs/2508.00180)
*Adam Block,Cyril Zhang*

Main category: cs.LG

TL;DR: 论文提出BEMA方法，通过修正EMA中的偏差，提升语言模型微调的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决EMA方法在语言模型微调中因旧迭代引入偏差而导致的优化滞后问题。

Method: 提出Bias-Corrected Exponential Moving Average (BEMA)，保留EMA的方差减少优势，同时消除偏差。

Result: BEMA在多种标准LM基准测试中显著优于EMA和普通训练，提升收敛速度和最终性能。

Conclusion: BEMA是一种实用且理论支持的方法，能更稳定高效地进行语言模型微调。

Abstract: Stochasticity in language model fine-tuning, often caused by the small batch
sizes typically used in this regime, can destabilize training by introducing
large oscillations in generation quality. A popular approach to mitigating this
instability is to take an Exponential moving average (EMA) of weights
throughout training. While EMA reduces stochasticity, thereby smoothing
training, the introduction of bias from old iterates often creates a lag in
optimization relative to vanilla training. In this work, we propose the
Bias-Corrected Exponential Moving Average (BEMA), a simple and practical
augmentation of EMA that retains variance-reduction benefits while eliminating
bias. BEMA is motivated by a simple theoretical model wherein we demonstrate
provable acceleration of BEMA over both a standard EMA and vanilla training.
Through an extensive suite of experiments on Language Models, we show that BEMA
leads to significantly improved convergence rates and final performance over
both EMA and vanilla training in a variety of standard LM benchmarks, making
BEMA a practical and theoretically motivated intervention for more stable and
efficient fine-tuning.

</details>


### [14] [Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings](https://arxiv.org/abs/2508.00039)
*Kaustav Chatterjee,Joshua Q. Li,Fatemeh Ansari,Masud Rana Munna,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 论文提出了一种结合LSTM和Transformer的混合深度学习框架，用于高效测量公路铁路平交道口（HRGC）的垂直剖面，以减少传统方法的成本和时间消耗。


<details>
  <summary>Details</summary>
Motivation: 传统HRGC剖面测量方法成本高、耗时长且存在安全隐患，需要一种更高效、安全且经济的方法。

Method: 开发了三种混合深度学习模型（Transformer-LSTM、LSTM-Transformer序列和并行模型），利用IMU和GPS传感器数据与地面真实数据结合进行训练。

Result: 模型2和3表现最佳，能够生成2D/3D HRGC剖面，显著提升了测量效率和准确性。

Conclusion: 该深度学习框架为HRGC的安全评估提供了一种快速、准确的解决方案，有助于提升公路和铁路的安全性。

Abstract: Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose
safety risks to highway vehicles due to potential hang-ups. These crossings
typically result from post-construction railway track maintenance activities or
non-compliance with design guidelines for HRGC vertical alignments.
Conventional methods for measuring HRGC profiles are costly, time-consuming,
traffic-disruptive, and present safety challenges. To address these issues,
this research employed advanced, cost-effective techniques and innovative
modeling approaches for HRGC profile measurement. A novel hybrid deep learning
framework combining Long Short-Term Memory (LSTM) and Transformer architectures
was developed by utilizing instrumentation and ground truth data.
Instrumentation data were gathered using a highway testing vehicle equipped
with Inertial Measurement Unit (IMU) and Global Positioning System (GPS)
sensors, while ground truth data were obtained via an industrial-standard
walking profiler. Field data was collected at the Red Rock Railroad Corridor in
Oklahoma. Three advanced deep learning models Transformer-LSTM sequential
(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel
(model 3) were evaluated to identify the most efficient architecture. Models 2
and 3 outperformed the others and were deployed to generate 2D/3D HRGC
profiles. The deep learning models demonstrated significant potential to
enhance highway and railroad safety by enabling rapid and accurate assessment
of HRGC hang-up susceptibility.

</details>


### [15] [Calibrated Language Models and How to Find Them with Label Smoothing](https://arxiv.org/abs/2508.00264)
*Jerry Huang,Peng Lu,Qiuhao Zeng*

Main category: cs.LG

TL;DR: 研究探讨了指令调优对大型语言模型（LLM）置信度校准的影响，并提出标签平滑作为解决方案，同时解决了内存消耗问题。


<details>
  <summary>Details</summary>
Motivation: 尽管指令调优提升了LLM的交互能力，但其对置信度校准的影响尚未充分研究。

Method: 分析了多种开源LLM，提出标签平滑作为校准方法，并设计了定制化内核以减少内存消耗。

Result: 标签平滑能有效维持校准，但在大词汇量LLM中效果受限，理论及实验验证了其与隐藏层和词汇量的关系。

Conclusion: 标签平滑是解决校准问题的实用方法，定制化内核进一步优化了内存效率。

Abstract: Recent advances in natural language processing (NLP) have opened up greater
opportunities to enable fine-tuned large language models (LLMs) to behave as
more powerful interactive agents through improved instruction-following
ability. However, understanding how this impacts confidence calibration for
reliable model output has not been researched in full. In this work, we examine
various open-sourced LLMs, identifying significant calibration degradation
after instruction tuning in each. Seeking a practical solution, we look towards
label smoothing, which has been shown as an effective method to regularize for
overconfident predictions but has yet to be widely adopted in the supervised
fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing
is sufficient to maintain calibration throughout the SFT process. However,
settings remain where the effectiveness of smoothing is severely diminished, in
particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to
stem from the ability to become over-confident, which has a direct relationship
with the hidden size and vocabulary size, and justify this theoretically and
experimentally. Finally, we address an outstanding issue regarding the memory
footprint of the cross-entropy loss computation in the label smoothed loss
setting, designing a customized kernel to dramatically reduce memory
consumption without sacrificing speed or performance in comparison to existing
solutions for non-smoothed losses.

</details>


### [16] [Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem](https://arxiv.org/abs/2508.00286)
*Mohsen Zaker Esteghamati*

Main category: cs.LG

TL;DR: 该研究提出了一种将性能导向的抗震设计视为逆工程问题的方法，通过可解释的机器学习模型直接映射设计变量与性能指标，解决了性能导向设计的计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统性能导向抗震设计存在计算效率低的问题，本研究旨在通过机器学习模型直接映射设计变量与性能目标，提高设计效率。

Method: 采用可解释的机器学习模型作为评估函数，结合遗传优化算法解决逆工程问题，应用于洛杉矶和查尔斯顿的钢和混凝土框架结构。

Result: 机器学习模型表现出高准确性（R2>90%），优化算法能够识别符合工程原理的构件最优属性。

Conclusion: 该方法通过机器学习与优化算法的结合，有效解决了性能导向抗震设计的计算效率问题，并验证了其在不同建筑类型中的适用性。

Abstract: This study presents a methodology to treat performance-based seismic design
as an inverse engineering problem, where design parameters are directly derived
to achieve specific performance objectives. By implementing explainable machine
learning models, this methodology directly maps design variables and
performance metrics, tackling computational inefficiencies of performance-based
design. The resultant machine learning model is integrated as an evaluation
function into a genetic optimization algorithm to solve the inverse problem.
The developed methodology is then applied to two different inventories of steel
and concrete moment frames in Los Angeles and Charleston to obtain sectional
properties of frame members that minimize expected annualized seismic loss in
terms of repair costs. The results show high accuracy of the surrogate models
(e.g., R2> 90%) across a diverse set of building types, geometries, seismic
design, and site hazard, where the optimization algorithm could identify the
optimum values of members' properties for a fixed set of geometric variables,
consistent with engineering principles.

</details>


### [17] [Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages](https://arxiv.org/abs/2508.00041)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.LG

TL;DR: DevFT是一种资源高效的联邦微调方法，通过分阶段构建LLM，显著提升性能并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决联邦微调中资源密集和边缘设备部署受限的问题。

Method: 分阶段微调，逐步增加参数容量，通过知识转移和优化初始化参数加速训练。

Result: 比现有方法快4.59倍，通信开销减少10.67倍，性能提升9.07%。

Conclusion: DevFT高效且兼容现有方法，适用于边缘设备。

Abstract: Federated fine-tuning enables Large Language Models (LLMs) to adapt to
downstream tasks while preserving data privacy, but its resource-intensive
nature limits deployment on edge devices. In this paper, we introduce
Developmental Federated Tuning (DevFT), a resource-efficient approach inspired
by cognitive development that progressively builds a powerful LLM from a
compact foundation. DevFT decomposes the fine-tuning process into developmental
stages, each optimizing submodels with increasing parameter capacity. Knowledge
from earlier stages transfers to subsequent submodels, providing optimized
initialization parameters that prevent convergence to local minima and
accelerate training. This paradigm mirrors human learning, gradually
constructing comprehensive knowledge structure while refining existing skills.
To efficiently build stage-specific submodels, DevFT introduces
deconfliction-guided layer grouping and differential-based layer fusion to
distill essential information and construct representative layers. Evaluations
across multiple benchmarks demonstrate that DevFT significantly outperforms
state-of-the-art methods, achieving up to 4.59$\times$ faster convergence,
10.67$\times$ reduction in communication overhead, and 9.07% average
performance improvement, while maintaining compatibility with existing
approaches.

</details>


### [18] [Foundations of Interpretable Models](https://arxiv.org/abs/2508.00545)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Alberto Termine,Mateja Jamnik,Giuseppe Marra*

Main category: cs.LG

TL;DR: 论文提出了一种新的可解释性定义，并展示了其可操作性，同时提出了设计可解释模型的通用蓝图和开源库。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性定义缺乏操作性，无法指导用户设计通用、可靠且稳健的可解释模型，导致研究问题不明确。

Method: 提出了一种通用且简单的可解释性定义，涵盖现有非正式概念，并基于此设计了可解释模型的蓝图和开源库。

Result: 新定义揭示了设计可解释模型所需的基础属性、假设、原则、数据结构和架构特征。

Conclusion: 该研究为可解释模型设计提供了理论基础和实用工具，推动了可解释AI领域的发展。

Abstract: We argue that existing definitions of interpretability are not actionable in
that they fail to inform users about general, sound, and robust interpretable
model design. This makes current interpretability research fundamentally
ill-posed. To address this issue, we propose a definition of interpretability
that is general, simple, and subsumes existing informal notions within the
interpretable AI community. We show that our definition is actionable, as it
directly reveals the foundational properties, underlying assumptions,
principles, data structures, and architectural features necessary for designing
interpretable models. Building on this, we propose a general blueprint for
designing interpretable models and introduce the first open-sourced library
with native support for interpretable data structures and processes.

</details>


### [19] [Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity](https://arxiv.org/abs/2508.00043)
*Nhut Truong,Uri Hasson*

Main category: cs.LG

TL;DR: 比较了两种空间约束（权重相似性和激活相似性）对地形卷积神经网络的影响，发现权重相似性在鲁棒性、输入敏感性和功能定位方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨不同地形约束对神经网络学习表示的影响，填补系统性研究的空白。

Method: 训练两种地形卷积神经网络（权重相似性和激活相似性），比较其在分类准确性、鲁棒性和表示空间组织上的表现。

Result: 权重相似性在噪声鲁棒性、输入敏感性和功能定位方面优于激活相似性和标准CNN，并影响了网络表示几何。

Conclusion: 权重相似性约束能产生更鲁棒的表示，并影响特征学习和功能组织。

Abstract: Topographic neural networks are computational models that can simulate the
spatial and functional organization of the brain. Topographic constraints in
neural networks can be implemented in multiple ways, with potentially different
impacts on the representations learned by the network. The impact of such
different implementations has not been systematically examined. To this end,
here we compare topographic convolutional neural networks trained with two
spatial constraints: Weight Similarity (WS), which pushes neighboring units to
develop similar incoming weights, and Activation Similarity (AS), which
enforces similarity in unit activations. We evaluate the resulting models on
classification accuracy, robustness to weight perturbations and input
degradation, and the spatial organization of learned representations. Compared
to both AS and standard CNNs, WS provided three main advantages: i) improved
robustness to noise, also showing higher accuracy under weight corruption; ii)
greater input sensitivity, reflected in higher activation variance; and iii)
stronger functional localization, with units showing similar activations
positioned at closer distances. In addition, WS produced differences in
orientation tuning, symmetry sensitivity, and eccentricity profiles of units,
indicating an influence of this spatial constraint on the representational
geometry of the network. Our findings suggest that during end-to-end training,
WS constraints produce more robust representations than AS or non-topographic
CNNs. These findings also suggest that weight-based spatial constraints can
shape feature learning and functional organization in biophysical inspired
models.

</details>


### [20] [Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains](https://arxiv.org/abs/2508.00046)
*Ruo Yu Tao,Kaicheng Guo,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 论文提出了一个用于评估强化学习算法在部分可观测性下性能的基准框架POBAX，强调覆盖多种部分可观测性形式和环境的内存可改进性。


<details>
  <summary>Details</summary>
Motivation: 现有的部分可观测性基准过于简单，无法反映真实场景中的复杂性，需要更全面的评估工具。

Method: 提出POBAX开源库，包含多样化的部分可观测性环境，并提供超参数和算法实现。

Result: POBAX中的任务均具有内存可改进性，且需要难以学习的内存功能。

Conclusion: POBAX为部分可观测性研究提供了标准化、高效的评估框架。

Abstract: Mitigating partial observability is a necessary but challenging task for
general reinforcement learning algorithms. To improve an algorithm's ability to
mitigate partial observability, researchers need comprehensive benchmarks to
gauge progress. Most algorithms tackling partial observability are only
evaluated on benchmarks with simple forms of state aliasing, such as feature
masking and Gaussian noise. Such benchmarks do not represent the many forms of
partial observability seen in real domains, like visual occlusion or unknown
opponent intent. We argue that a partially observable benchmark should have two
key properties. The first is coverage in its forms of partial observability, to
ensure an algorithm's generalizability. The second is a large gap between the
performance of a agents with more or less state information, all other factors
roughly equal. This gap implies that an environment is memory improvable: where
performance gains in a domain are from an algorithm's ability to cope with
partial observability as opposed to other factors. We introduce best-practice
guidelines for empirically benchmarking reinforcement learning under partial
observability, as well as the open-source library POBAX: Partially Observable
Benchmarks in JAX. We characterize the types of partial observability present
in various environments and select representative environments for our
benchmark. These environments include localization and mapping, visual control,
games, and more. Additionally, we show that these tasks are all memory
improvable and require hard-to-learn memory functions, providing a concrete
signal for partial observability research. This framework includes recommended
hyperparameters as well as algorithm implementations for fast, out-of-the-box
evaluation, as well as highly performant environments implemented in JAX for
GPU-scalable experimentation.

</details>


### [21] [TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection](https://arxiv.org/abs/2508.00047)
*Yuan-Cheng Yu,Yen-Chieh Ouyang,Chun-An Lin*

Main category: cs.LG

TL;DR: 提出了TriP-LLM框架，利用预训练大语言模型进行时间序列异常检测，表现优于现有方法且内存消耗更低。


<details>
  <summary>Details</summary>
Motivation: 传统统计方法难以处理高维异构时间序列数据，受大语言模型在多模态任务中的成功启发，提出新框架。

Method: TriP-LLM通过三支路设计（分块、选择、全局）整合局部和全局特征，利用冻结预训练LLM处理分块标记，轻量解码器重构输入并计算异常分数。

Result: 在多个公共数据集上表现优于现有方法，内存消耗显著低于基于CI分块处理的LLM方法。

Conclusion: TriP-LLM在时间序列异常检测中表现出色，适合GPU内存受限环境，代码和模型已开源。

Abstract: Time-series anomaly detection plays a central role across a wide range of
application domains. With the increasing proliferation of the Internet of
Things (IoT) and smart manufacturing, time-series data has dramatically
increased in both scale and dimensionality. This growth has exposed the
limitations of traditional statistical methods in handling the high
heterogeneity and complexity of such data. Inspired by the recent success of
large language models (LLMs) in multimodal tasks across language and vision
domains, we propose a novel unsupervised anomaly detection framework: A
Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly
Detection (TriP-LLM). TriP-LLM integrates local and global temporal features
through a tri-branch design-Patching, Selection, and Global-to encode the input
time series into patch-wise tokens, which are then processed by a frozen,
pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from
which anomaly scores are derived. We evaluate TriP-LLM on several public
benchmark datasets using PATE, a recently proposed threshold-free evaluation
metric, and conduct all comparisons within a unified open-source framework to
ensure fairness. Experimental results show that TriP-LLM consistently
outperforms recent state-of-the-art methods across all datasets, demonstrating
strong detection capabilities. Furthermore, through extensive ablation studies,
we verify the substantial contribution of the LLM to the overall architecture.
Compared to LLM-based approaches using Channel Independence (CI) patch
processing, TriP-LLM achieves significantly lower memory consumption, making it
more suitable for GPU memory-constrained environments. All code and model
checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git

</details>


### [22] [Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization](https://arxiv.org/abs/2508.00078)
*Imen Mahmoud,Andrei Velichko*

Main category: cs.LG

TL;DR: 该研究提出了一种结合LightGBM回归模型和遗传算法优化的新方法，评估COVID-19相关指标对比特币回报预测的贡献。结果表明，疫情指标显著提升了模型性能，尤其是疫苗接种数据。


<details>
  <summary>Details</summary>
Motivation: 研究旨在确定疫情相关健康数据是否能显著提升比特币回报预测的准确性，为投资者和政策制定者提供更精确的市场不确定性指标。

Method: 使用LightGBM回归模型和遗传算法优化，构建包含比特币回报和COVID-19指标的全面数据集，并通过31次独立运行进行统计评估。

Result: COVID-19指标显著提升模型性能（R2增加40%，RMSE降低2%），疫苗接种数据是主要预测因子。

Conclusion: 该方法通过整合公共卫生信号扩展了金融分析工具，为系统性危机期间的市场不确定性提供了更精细的指标。

Abstract: This study proposes a novel methodological framework integrating a LightGBM
regression model and genetic algorithm (GA) optimization to systematically
evaluate the contribution of COVID-19-related indicators to Bitcoin return
prediction. The primary objective was not merely to forecast Bitcoin returns
but rather to determine whether including pandemic-related health data
significantly enhances prediction accuracy. A comprehensive dataset comprising
daily Bitcoin returns and COVID-19 metrics (vaccination rates,
hospitalizations, testing statistics) was constructed. Predictive models,
trained with and without COVID-19 features, were optimized using GA over 31
independent runs, allowing robust statistical assessment. Performance metrics
(R2, RMSE, MAE) were statistically compared through distribution overlaps and
Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified
individual feature contributions. Results indicate that COVID-19 indicators
significantly improved model performance, particularly in capturing extreme
market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly
significant statistically). Among COVID-19 features, vaccination metrics,
especially the 75th percentile of fully vaccinated individuals, emerged as
dominant predictors. The proposed methodology extends existing financial
analytics tools by incorporating public health signals, providing investors and
policymakers with refined indicators to navigate market uncertainty during
systemic crises.

</details>


### [23] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: 论文提出了一种基于材料科学中结构疲劳概念的弹性神经网络训练方法，通过动态调整优化行为提升模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 受材料科学中临时（弹性）和永久（塑性）变形概念的启发，旨在解决神经网络在训练过程中因优化困难而陷入局部最优的问题。

Method: 提出了塑性变形优化器（Plastic Deformation Optimizer），通过内部压力信号检测训练停滞，并自适应地向模型参数注入噪声，帮助模型逃离尖锐最小值。

Result: 在六种架构、四种优化器和七个视觉基准测试中验证了方法的有效性，显著提升了模型的鲁棒性和泛化能力，且计算开销极小。

Conclusion: Stress-Aware Learning为神经网络训练提供了一种新的弹性优化范式，具有广泛的应用潜力。

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


### [24] [Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models](https://arxiv.org/abs/2508.00202)
*Ecem Bozkurt,Antonio Ortega*

Main category: cs.LG

TL;DR: 论文提出了一种两阶段框架，利用几何信息在标签噪声条件下实现鲁棒分类，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 在需要微调噪声数据的基础模型（FM）场景下，现有方法依赖局部几何信息，但性能仍有提升空间。

Method: 采用两阶段流程：可靠性估计和可靠性加权推断，引入非负核（NNK）邻域构建和几何信息。

Result: 在CIFAR-10和DermaMNIST数据集上，方法在多种噪声条件下表现优于标准K-NN和自适应邻域基线。

Conclusion: 通过几何信息和可靠性加权推断，显著提升了噪声标签条件下的分类鲁棒性。

Abstract: Foundation models (FMs) pretrained on large datasets have become fundamental
for various downstream machine learning tasks, in particular in scenarios where
obtaining perfectly labeled data is prohibitively expensive. In this paper, we
assume an FM has to be fine-tuned with noisy data and present a two-stage
framework to ensure robust classification in the presence of label noise
without model retraining. Recent work has shown that simple k-nearest neighbor
(kNN) approaches using an embedding derived from an FM can achieve good
performance even in the presence of severe label noise. Our work is motivated
by the fact that these methods make use of local geometry. In this paper,
following a similar two-stage procedure, reliability estimation followed by
reliability-weighted inference, we show that improved performance can be
achieved by introducing geometry information. For a given instance, our
proposed inference uses a local neighborhood of training data, obtained using
the non-negative kernel (NNK) neighborhood construction. We propose several
methods for reliability estimation that can rely less on distance and local
neighborhood as the label noise increases. Our evaluation on CIFAR-10 and
DermaMNIST shows that our methods improve robustness across various noise
conditions, surpassing standard K-NN approaches and recent
adaptive-neighborhood baselines.

</details>


### [25] [StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection](https://arxiv.org/abs/2508.00117)
*Md. Ehsanul Haque,S. M. Jahidul Islam,Shakil Mia,Rumana Sharmin,Ashikuzzaman,Md Samir Morshed,Md. Tahmidul Huque*

Main category: cs.LG

TL;DR: StackLiverNet是一种可解释的堆叠集成模型，用于肝病检测，通过高级数据预处理和特征选择提高性能，测试准确率达99.89%。


<details>
  <summary>Details</summary>
Motivation: 现有肝病分类模型存在高误分类率、解释性差、计算成本高等问题，需改进。

Method: 采用随机欠采样处理类别不平衡，结合多个超参数优化的基分类器，通过LightGBM元模型集成。

Result: 测试准确率99.89%，Cohen Kappa 0.9974，AUC 0.9993，仅5次误分类。

Conclusion: StackLiverNet性能优异，适用于临床实践，且通过LIME和SHAP提供透明解释。

Abstract: Liver diseases are a serious health concern in the world, which requires
precise and timely diagnosis to enhance the survival chances of patients. The
current literature implemented numerous machine learning and deep learning
models to classify liver diseases, but most of them had some issues like high
misclassification error, poor interpretability, prohibitive computational
expense, and lack of good preprocessing strategies. In order to address these
drawbacks, we introduced StackLiverNet in this study; an interpretable stacked
ensemble model tailored to the liver disease detection task. The framework uses
advanced data preprocessing and feature selection technique to increase model
robustness and predictive ability. Random undersampling is performed to deal
with class imbalance and make the training balanced. StackLiverNet is an
ensemble of several hyperparameter-optimized base classifiers, whose
complementary advantages are used through a LightGBM meta-model. The provided
model demonstrates excellent performance, with the testing accuracy of 99.89%,
Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and
efficient training and inference speeds that are amenable to clinical practice
(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local
Interpretable Model-Agnostic Explanations (LIME) are applied to generate
transparent explanations of individual predictions, revealing high
concentrations of Alkaline Phosphatase and moderate SGOT as important
observations of liver disease. Also, SHAP was used to rank features by their
global contribution to predictions, while the Morris method confirmed the most
influential features through sensitivity analysis.

</details>


### [26] [Structured Transformations for Stable and Interpretable Neural Computation](https://arxiv.org/abs/2508.00127)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.LG

TL;DR: 论文提出了一种新的神经网络层变换方法，通过结构化线性算子和残差修正组件，提升训练的稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络缺乏结构保障，导致学习不稳定和行为难以解释。

Method: 将层变换分解为结构化线性算子和残差修正组件，支持稳定信息流。

Result: 实验表明，该方法改善了梯度条件、降低扰动敏感性，并增强层间鲁棒性。

Conclusion: 研究为更稳定、透明的神经网络架构奠定了基础，不牺牲表达能力。

Abstract: Despite their impressive performance, contemporary neural networks often lack
structural safeguards that promote stable learning and interpretable behavior.
In this work, we introduce a reformulation of layer-level transformations that
departs from the standard unconstrained affine paradigm. Each transformation is
decomposed into a structured linear operator and a residual corrective
component, enabling more disciplined signal propagation and improved training
dynamics. Our formulation encourages internal consistency and supports stable
information flow across depth, while remaining fully compatible with standard
learning objectives and backpropagation. Through a series of synthetic and
real-world experiments, we demonstrate that models constructed with these
structured transformations exhibit improved gradient conditioning, reduced
sensitivity to perturbations, and layer-wise robustness. We further show that
these benefits persist across architectural scales and training regimes. This
study serves as a foundation for a more principled class of neural
architectures that prioritize stability and transparency-offering new tools for
reasoning about learning behavior without sacrificing expressive power.

</details>


### [27] [ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks](https://arxiv.org/abs/2508.00131)
*Christopher Harvey,Sumaiya Shomaji,Zijun Yao,Amit Noheria*

Main category: cs.LG

TL;DR: 论文提出三种新型变分自编码器（VAE）变体，用于简化心电信号数据并提升预测性能，尤其在数据有限的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 心电信号（ECG）复杂度高且个体差异大，传统深度学习方法在小数据集上效果不佳，需探索更高效的特征生成方法。

Method: 研究采用PCA和自编码器降维，并引入三种VAE变体（SAE、A beta-VAE、C beta-VAE），结合LGBM进行预测任务。

Result: A beta-VAE信号重建误差最低（MAE 15.7±3.2 μV），SAE编码结合传统特征预测LVEF的AUROC达0.901，接近CNN模型但计算资源更少。

Conclusion: 新型VAE编码能有效简化ECG数据，为小规模标注数据下的深度学习应用提供实用解决方案。

Abstract: The electrocardiogram (ECG) is an inexpensive and widely available tool for
cardiac assessment. Despite its standardized format and small file size, the
high complexity and inter-individual variability of ECG signals (typically a
60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep
learning models, especially when only small training datasets are available.
This study addresses these challenges by exploring feature generation methods
from representative beat ECGs, focusing on Principal Component Analysis (PCA)
and Autoencoders to reduce data complexity. We introduce three novel
Variational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed
beta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their
effectiveness in maintaining signal fidelity and enhancing downstream
prediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE
achieved superior signal reconstruction, reducing the mean absolute error (MAE)
to 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE
encodings, when combined with traditional ECG summary features, improved the
prediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an
holdout test set area under the receiver operating characteristic curve (AUROC)
of 0.901 with a LGBM classifier. This performance nearly matches the 0.909
AUROC of state-of-the-art CNN model but requires significantly less
computational resources. Further, the ECG feature extraction-LGBM pipeline
avoids overfitting and retains predictive performance when trained with less
data. Our findings demonstrate that these VAE encodings are not only effective
in simplifying ECG data but also provide a practical solution for applying deep
learning in contexts with limited-scale labeled training data.

</details>


### [28] [INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks](https://arxiv.org/abs/2508.00141)
*Mohit Gupta,Debjit Bhowmick,Rhys Newbury,Meead Saberi,Shirui Pan,Ben Beck*

Main category: cs.LG

TL;DR: INSPIRE-GNN是一种结合强化学习和图神经网络的框架，用于优化自行车流量传感器的放置并提高数据稀疏环境下的流量估计精度。


<details>
  <summary>Details</summary>
Motivation: 城市自行车流量数据稀疏，传统传感器覆盖有限，需要更智能的方法优化传感器放置以提高估计准确性。

Method: 结合图卷积网络（GCN）、图注意力网络（GAT）和深度Q网络（DQN）的强化学习代理，数据驱动地选择传感器位置。

Result: 在墨尔本自行车网络中，INSPIRE-GNN显著优于传统启发式方法，降低了MSE、RMSE和MAE等误差指标。

Conclusion: INSPIRE-GNN为交通规划者提供了优化传感器网络和提升数据可靠性的有效工具。

Abstract: Accurate link-level bicycling volume estimation is essential for sustainable
urban transportation planning. However, many cities face significant challenges
of high data sparsity due to limited bicycling count sensor coverage. To
address this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning
(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize
sensor placement and improve link-level bicycling volume estimation in
data-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks
(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL
agent, enabling a data-driven strategic selection of sensor locations to
maximize estimation performance. Applied to Melbourne's bicycling network,
comprising 15,933 road segments with sensor coverage on only 141 road segments
(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume
estimation by strategically selecting additional sensor locations in
deployments of 50, 100, 200 and 500 sensors. Our framework outperforms
traditional heuristic methods for sensor placement such as betweenness
centrality, closeness centrality, observed bicycling activity and random
placement, across key metrics such as Mean Squared Error (MSE), Root Mean
Squared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our
experiments benchmark INSPIRE-GNN against standard machine learning and deep
learning models in the bicycle volume estimation performance, underscoring its
effectiveness. Our proposed framework provides transport planners actionable
insights to effectively expand sensor networks, optimize sensor placement and
maximize volume estimation accuracy and reliability of bicycling data for
informed transportation planning decisions.

</details>


### [29] [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
*Ziqian Zhong,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 提出了一种基于权重而非激活的新方法，用于监控和控制微调后的LLM，无需依赖与训练数据分布相似的输入数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的解释方法需要分布相似的数据，限制了检测和防御新型威胁（如后门）的能力。

Method: 通过分析微调模型与基础模型权重差异的顶部奇异向量，监控激活的余弦相似度。

Result: 成功检测后门攻击（100%阻止率，假阳性率低于1.2%），并识别未学习主题（准确率95.42%）。

Conclusion: 该方法在模型监控和预部署审计中具有潜力，可揭示微调行为。

Abstract: The releases of powerful open-weight large language models (LLMs) are often
not accompanied by access to their full training data. Existing
interpretability methods, particularly those based on activations, often
require or assume distributionally similar data. This is a significant
limitation when detecting and defending against novel potential threats like
backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and
controlling fine-tuned LLMs that interprets weights, rather than activations,
thereby side stepping the need for data that is distributionally similar to the
unknown training data. We demonstrate that the top singular vectors of the
weight difference between a fine-tuned model and its base model correspond to
newly acquired behaviors. By monitoring the cosine similarity of activations
along these directions, we can detect salient behaviors introduced during
fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger
is present, our method stops up to 100% of attacks with a false positive rate
below 1.2%. For models that have undergone unlearning, we detect inference on
erased topics with accuracy up to 95.42% and can even steer the model to
recover "unlearned" information. Besides monitoring, our method also shows
potential for pre-deployment model auditing: by analyzing commercial
instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover
model-specific fine-tuning focus including marketing strategies and Midjourney
prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

</details>


### [30] [DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission](https://arxiv.org/abs/2508.00172)
*Fupei Guo,Hao Zheng,Xiang Zhang,Li Chen,Yue Wang,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的语义通信框架DiSC-Med，用于高效、鲁棒的医学图像传输。


<details>
  <summary>Details</summary>
Motivation: 解决在有限带宽和噪声信道中高效传输医疗数据的关键挑战。

Method: 开发了医疗增强的压缩和去噪模块，以提升带宽效率和鲁棒性。

Result: 在真实医学数据集上验证了框架的有效性，展示了其在远程医疗中的潜力。

Conclusion: DiSC-Med能够捕捉关键语义信息，在噪声信道中实现高效重建，适用于远程医疗。

Abstract: The rapid development of artificial intelligence has driven smart health with
next-generation wireless communication technologies, stimulating exciting
applications in remote diagnosis and intervention. To enable a timely and
effective response for remote healthcare, efficient transmission of medical
data through noisy channels with limited bandwidth emerges as a critical
challenge. In this work, we propose a novel diffusion-based semantic
communication framework, namely DiSC-Med, for the medical image transmission,
where medical-enhanced compression and denoising blocks are developed for
bandwidth efficiency and robustness, respectively. Unlike conventional
pixel-wise communication framework, our proposed DiSC-Med is able to capture
the key semantic information and achieve superior reconstruction performance
with ultra-high bandwidth efficiency against noisy channels. Extensive
experiments on real-world medical datasets validate the effectiveness of our
framework, demonstrating its potential for robust and efficient telehealth
applications.

</details>


### [31] [RL as Regressor: A Reinforcement Learning Approach for Function Approximation](https://arxiv.org/abs/2508.00174)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 论文提出将回归问题转化为强化学习问题，通过自定义奖励信号和RL算法实现更灵活的目标定义和学习过程。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法受限于预定义的可微损失函数，难以处理非对称成本或复杂目标，因此探索RL框架的替代方案。

Method: 将模型预测视为动作，基于预测误差定义自定义奖励信号，并利用Actor-Critic算法逐步优化模型。

Result: RL框架成功解决了回归问题，并在目标定义和学习过程中展现出更高的灵活性。

Conclusion: RL为回归任务提供了新的范式，能够处理更复杂和非可微的目标。

Abstract: Standard regression techniques, while powerful, are often constrained by
predefined, differentiable loss functions such as mean squared error. These
functions may not fully capture the desired behavior of a system, especially
when dealing with asymmetric costs or complex, non-differentiable objectives.
In this paper, we explore an alternative paradigm: framing regression as a
Reinforcement Learning (RL) problem. We demonstrate this by treating a model's
prediction as an action and defining a custom reward signal based on the
prediction error, and we can leverage powerful RL algorithms to perform
function approximation. Through a progressive case study of learning a noisy
sine wave, we illustrate the development of an Actor-Critic agent, iteratively
enhancing it with Prioritized Experience Replay, increased network capacity,
and positional encoding to enable a capable RL agent for this regression task.
Our results show that the RL framework not only successfully solves the
regression problem but also offers enhanced flexibility in defining objectives
and guiding the learning process.

</details>


### [32] [RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems](https://arxiv.org/abs/2508.00201)
*Mehdi Ben Ayed,Fei Feng,Jay Adams,Vishwakarma Singh,Kritarth Anand,Jiajing Xu*

Main category: cs.LG

TL;DR: RecoMind是一个基于模拟器的强化学习框架，用于优化大规模推荐系统中的会话目标，显著提升用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要依赖监督学习，难以优化长期目标（如会话参与度），而强化学习在大规模应用中面临工程复杂性挑战。

Method: 利用现有推荐模型构建模拟环境，并通过自定义探索策略高效探索大规模动作空间，结合强化学习优化策略。

Result: 离线模拟和在线A/B测试显示，RecoMind训练的RL策略显著优于传统监督学习方法，视频观看时长和会话深度均有提升。

Conclusion: RecoMind为大规模推荐系统提供了一种可扩展的强化学习解决方案，有效优化会话目标。

Abstract: Existing web-scale recommendation systems commonly use supervised learning
methods that prioritize immediate user feedback. Although reinforcement
learning (RL) offers a solution to optimize longer-term goals, such as
in-session engagement, applying it at web scale is challenging due to the
extremely large action space and engineering complexity. In this paper, we
introduce RecoMind, a simulator-based RL framework designed for the effective
optimization of session-based goals at web-scale. RecoMind leverages existing
recommendation models to establish a simulation environment and to bootstrap
the RL policy to optimize immediate user interactions from the outset. This
method integrates well with existing industry pipelines, simplifying the
training and deployment of RL policies. Additionally, RecoMind introduces a
custom exploration strategy to efficiently explore web-scale action spaces with
hundreds of millions of items. We evaluated RecoMind through extensive offline
simulations and online A/B testing on a video streaming platform. Both methods
showed that the RL policy trained using RecoMind significantly outperforms
traditional supervised learning recommendation approaches in in-session user
satisfaction. In online A/B tests, the RL policy increased videos watched for
more than 10 seconds by 15.81\% and improved session depth by 4.71\% for
sessions with at least 10 interactions. As a result, RecoMind presents a
systematic and scalable approach for embedding RL into web-scale recommendation
systems, showing great promise for optimizing session-based user satisfaction.

</details>


### [33] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: 比较了低秩适应（LoRA）与全秩PEFT方法，提出KRAdapter算法以解决LoRA在近似高有效秩矩阵时的不足，并在大模型上验证其性能。


<details>
  <summary>Details</summary>
Motivation: LoRA在多模态和大语言模型中表现受限，尤其是在高有效秩矩阵近似时效果不佳。

Method: 通过合成矩阵近似基准对比全秩和低秩PEFT方法，提出基于Khatri-Rao积的KRAdapter算法。

Result: KRAdapter在视觉语言模型和大型语言模型上表现优于LoRA，尤其在未见常识推理任务中。

Conclusion: KRAdapter是一种高效且实用的PEFT方法，适用于十亿级参数模型的微调。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


### [34] [Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring](https://arxiv.org/abs/2508.00270)
*Robin Schmucker,Nimish Pachapurkar,Shanmuga Bala,Miral Shah,Tom Mitchell*

Main category: cs.LG

TL;DR: 论文介绍了一种在线辅导系统，通过多臂老虎机框架和离线策略评估优化学生反馈，提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 旨在通过数据驱动的方法优化学生反馈，以提高学习效果和练习表现。

Method: 使用多臂老虎机（MAB）框架和离线策略评估，分析43,000个辅助动作，并设计算法选择适合的策略目标。进一步研究上下文老虎机（CB）策略的个性化效果。

Result: 在166,000次练习会话中验证，MAB策略显著提升学生表现，但CB策略的个性化效果有限。

Conclusion: 数据驱动的反馈优化系统已大规模部署，未来可进一步改进个性化策略。

Abstract: We present an online tutoring system that learns to provide effective
feedback to students after they answer questions incorrectly. Using data from
one million students, the system learns which assistance action (e.g., one of
multiple hints) to provide for each question to optimize student learning.
Employing the multi-armed bandit (MAB) framework and offline policy evaluation,
we assess 43,000 assistance actions, and identify trade-offs between assistance
policies optimized for different student outcomes (e.g., response correctness,
session completion). We design an algorithm that for each question decides on a
suitable policy training objective to enhance students' immediate second
attempt success and overall practice session performance. We evaluate the
resulting MAB policies in 166,000 practice sessions, verifying significant
improvements in student outcomes. While MAB policies optimize feedback for the
overall student population, we further investigate whether contextual bandit
(CB) policies can enhance outcomes by personalizing feedback based on
individual student features (e.g., ability estimates, response times). Using
causal inference, we examine (i) how effects of assistance actions vary across
students and (ii) whether CB policies, which leverage such effect
heterogeneity, outperform MAB policies. While our analysis reveals that some
actions for some questions exhibit effect heterogeneity, effect sizes may often
be too small for CB policies to provide significant improvements beyond what
well-optimized MAB policies that deliver the same action to all students
already achieve. We discuss insights gained from deploying data-driven systems
at scale and implications for future refinements. Today, the teaching policies
optimized by our system support thousands of students daily.

</details>


### [35] [Invariant Graph Transformer for Out-of-Distribution Generalization](https://arxiv.org/abs/2508.00304)
*Tianyin Liao,Ziwei Zhang,Yufei Sun,Chunyu Hu,Jianxin Li*

Main category: cs.LG

TL;DR: GOODFormer是一种图Transformer，通过联合优化三个模块，解决图数据分布偏移下的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer在分布偏移下泛化能力不足，需要捕捉不变性图结构模式。

Method: 开发熵引导不变子图解耦器、动态子图编码器和不变学习模块。

Result: 在基准数据集上优于现有方法，适应分布偏移。

Conclusion: GOODFormer通过不变性学习实现图数据的泛化表示。

Abstract: Graph Transformers (GTs) have demonstrated great effectiveness across various
graph analytical tasks. However, the existing GTs focus on training and testing
graph data originated from the same distribution, but fail to generalize under
distribution shifts. Graph invariant learning, aiming to capture generalizable
graph structural patterns with labels under distribution shifts, is potentially
a promising solution, but how to design attention mechanisms and positional and
structural encodings (PSEs) based on graph invariant learning principles
remains challenging. To solve these challenges, we introduce Graph
Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn
generalized graph representations by capturing invariant relationships between
predictive graph structures and labels through jointly optimizing three
modules. Specifically, we first develop a GT-based entropy-guided invariant
subgraph disentangler to separate invariant and variant subgraphs while
preserving the sharpness of the attention function. Next, we design an evolving
subgraph positional and structural encoder to effectively and efficiently
capture the encoding information of dynamically changing subgraphs during
training. Finally, we propose an invariant learning module utilizing subgraph
node representations and encodings to derive generalizable graph
representations that can to unseen graphs. We also provide theoretical
justifications for our method. Extensive experiments on benchmark datasets
demonstrate the superiority of our method over state-of-the-art baselines under
distribution shifts.

</details>


### [36] [PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models](https://arxiv.org/abs/2508.00325)
*Yongquan Qu,Matthieu Blanke,Sara Shamekh,Pierre Gentine*

Main category: cs.LG

TL;DR: PnP-DA是一种数据同化算法，通过结合梯度更新和生成先验，减少地球系统建模中的误差，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决地球系统建模中因简化导致的误差累积问题，传统方法假设高斯误差统计，无法捕捉混沌系统的真实行为。

Method: PnP-DA交替使用梯度更新和预训练的生成先验，避免复杂神经网络的反向传播。

Result: 在混沌测试中，PnP-DA显著减少预测误差，优于经典变分方法。

Conclusion: PnP-DA通过放松统计假设和利用历史数据，提供了一种高效的数据同化解决方案。

Abstract: Earth system modeling presents a fundamental challenge in scientific
computing: capturing complex, multiscale nonlinear dynamics in computationally
efficient models while minimizing forecast errors caused by necessary
simplifications. Even the most powerful AI- or physics-based forecast system
suffer from gradual error accumulation. Data assimilation (DA) aims to mitigate
these errors by optimally blending (noisy) observations with prior model
forecasts, but conventional variational methods often assume Gaussian error
statistics that fail to capture the true, non-Gaussian behavior of chaotic
dynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates
(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance
misfit on new observations) with (2) a single forward pass through a pretrained
generative prior conditioned on the background forecast via a conditional
Wasserstein coupling. This strategy relaxes restrictive statistical assumptions
and leverages rich historical data without requiring an explicit regularization
functional, and it also avoids the need to backpropagate gradients through the
complex neural network that encodes the prior during assimilation cycles.
Experiments on standard chaotic testbeds demonstrate that this strategy
consistently reduces forecast errors across a range of observation sparsities
and noise levels, outperforming classical variational methods.

</details>


### [37] [Embryology of a Language Model](https://arxiv.org/abs/2508.00331)
*George Wang,Garrett Baker,Andrew Gordon,Daniel Murfet*

Main category: cs.LG

TL;DR: 论文提出了一种胚胎学方法，利用UMAP可视化语言模型在训练中的结构发展，揭示了新的内部机制。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型内部计算结构的形成机制，探索统计物理学中的敏感性分析在深度学习中的应用潜力。

Method: 应用UMAP对敏感性矩阵进行分析，可视化模型在训练过程中的结构发展。

Result: 发现了已知特征（如感应电路）的形成，以及新结构（如用于计数空格标记的“间距鳍”）。

Conclusion: 敏感性分析不仅能验证模型，还能揭示新机制，为研究复杂神经网络的发育原理提供了全面视角。

Abstract: Understanding how language models develop their internal computational
structure is a central problem in the science of deep learning. While
susceptibilities, drawn from statistical physics, offer a promising analytical
tool, their full potential for visualizing network organization remains
untapped. In this work, we introduce an embryological approach, applying UMAP
to the susceptibility matrix to visualize the model's structural development
over training. Our visualizations reveal the emergence of a clear ``body
plan,'' charting the formation of known features like the induction circuit and
discovering previously unknown structures, such as a ``spacing fin'' dedicated
to counting space tokens. This work demonstrates that susceptibility analysis
can move beyond validation to uncover novel mechanisms, providing a powerful,
holistic lens for studying the developmental principles of complex neural
networks.

</details>


### [38] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: BOOD框架利用扩散模型生成高质量OOD特征和图像，显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 在潜在空间中提取有效的OOD特征仍具挑战性，现有方法难以清晰界定决策边界。

Method: BOOD通过学习文本条件化的潜在特征空间，扰动接近决策边界的ID特征生成OOD特征，并用扩散模型解码为图像。

Result: 在CIFAR-100数据集上，BOOD显著优于现有方法，FPR95降低29.64%，AUROC提升7.27%。

Conclusion: BOOD提供了一种高效生成OOD特征的策略，显著提升了OOD检测性能。

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training
data based on latent space features has proven effective in enhancing
out-of-distribution (OOD) detection performance. However, extracting effective
features outside the in-distribution (ID) boundary in latent space remains
challenging due to the difficulty of identifying decision boundaries between
classes. This paper proposes a novel framework called Boundary-based
Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD
features and generates human-compatible outlier images using diffusion models.
BOOD first learns a text-conditioned latent feature space from the ID dataset,
selects ID features closest to the decision boundary, and perturbs them to
cross the decision boundary to form OOD features. These synthetic OOD features
are then decoded into images in pixel space by a diffusion model. Compared to
previous works, BOOD provides a more training efficient strategy for
synthesizing informative OOD features, facilitating clearer distinctions
between ID and OOD data. Extensive experimental results on common benchmarks
demonstrate that BOOD surpasses the state-of-the-art method significantly,
achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%
improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [39] [Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization](https://arxiv.org/abs/2508.00357)
*Yoonhyuk Choi,Jiho Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SGPC提出了一种结合细胞鞘消息传递的GNN架构，通过优化传输提升、方差减少扩散和PAC-Bayes谱正则化，解决了异质图上的过平滑问题，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决GNN在异质图上因过平滑导致的节点特征崩溃问题，同时避免现有鞘网络模型的静态或过度参数化问题。

Method: 提出SGPC架构，结合细胞鞘消息传递、优化传输提升、方差减少扩散和PAC-Bayes谱正则化，实现端到端训练。

Result: 在九个同质和异质基准测试中，SGPC优于现有光谱和鞘基GNN，并提供未见过节点的置信区间。

Conclusion: SGPC通过理论性能界限和实验验证，提供了一种高效且可扩展的解决方案，显著提升了异质图上的节点分类性能。

Abstract: Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct
node features, particularly on heterophilic graphs where adjacent nodes often
have dissimilar labels. Although sheaf neural networks partially mitigate this
problem, they typically rely on static or heavily parameterized sheaf
structures that hinder generalization and scalability. Existing sheaf-based
models either predefine restriction maps or introduce excessive complexity, yet
fail to provide rigorous stability guarantees. In this paper, we introduce a
novel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified
architecture that combines cellular-sheaf message passing with several
mechanisms, including optimal transport-based lifting, variance-reduced
diffusion, and PAC-Bayes spectral regularization for robust semi-supervised
node classification. We establish performance bounds theoretically and
demonstrate that the resulting bound-aware objective can be achieved via
end-to-end training in linear computational complexity. Experiments on nine
homophilic and heterophilic benchmarks show that SGPC outperforms
state-of-the-art spectral and sheaf-based GNNs while providing certified
confidence intervals on unseen nodes.

</details>


### [40] [OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions](https://arxiv.org/abs/2508.00364)
*Chanyoung Yoon,Sangbong Yoo,Soobin Yim,Chansoo Kim,Yun Jang*

Main category: cs.LG

TL;DR: OID-PPO是一种基于强化学习的室内设计优化框架，通过结合专家定义的功能和视觉准则，显著提升了布局质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 住宅室内设计对居住者满意度影响重大，但现有方法在计算效率、数据稀缺性和设计原则整合方面存在不足。

Method: 提出OID-PPO框架，利用近端策略优化（PPO）和结构化奖励函数，实现连续灵活的家具布局。

Result: 实验表明，OID-PPO在多样化的房间形状和家具配置中，显著优于现有方法。

Conclusion: OID-PPO通过整合设计准则和优化策略，为室内设计提供了高效且高质量的解决方案。

Abstract: Designing residential interiors strongly impacts occupant satisfaction but
remains challenging due to unstructured spatial layouts, high computational
demands, and reliance on expert knowledge. Existing methods based on
optimization or deep learning are either computationally expensive or
constrained by data scarcity. Reinforcement learning (RL) approaches often
limit furniture placement to discrete positions and fail to incorporate design
principles adequately. We propose OID-PPO, a novel RL framework for Optimal
Interior Design using Proximal Policy Optimization, which integrates
expert-defined functional and visual guidelines into a structured reward
function. OID-PPO utilizes a diagonal Gaussian policy for continuous and
flexible furniture placement, effectively exploring latent environmental
dynamics under partial observability. Experiments conducted across diverse room
shapes and furniture configurations demonstrate that OID-PPO significantly
outperforms state-of-the-art methods in terms of layout quality and
computational efficiency. Ablation studies further demonstrate the impact of
structured guideline integration and reveal the distinct contributions of
individual design constraints.

</details>


### [41] [Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions](https://arxiv.org/abs/2508.00392)
*Lijun Zhang,Wenhao Yang,Guanghui Wang,Wei Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种具有双重适应性的通用算法框架，用于在线学习中最小化自适应遗憾，适用于多种凸函数类型和动态环境。


<details>
  <summary>Details</summary>
Motivation: 现有算法缺乏通用性，只能处理单一类型的凸函数且需要先验知识，限制了实际应用。

Method: 提出基于元专家框架的双重自适应算法，动态创建多个专家并通过元算法聚合，结合睡眠专家技术捕捉环境变化。

Result: 理论分析表明，算法能同时最小化多种凸函数的自适应遗憾，并允许函数类型在轮次间切换。

Conclusion: 该框架进一步扩展到在线复合优化，开发了针对复合函数的通用算法。

Abstract: To deal with changing environments, a new performance measure -- adaptive
regret, defined as the maximum static regret over any interval, was proposed in
online learning. Under the setting of online convex optimization, several
algorithms have been successfully developed to minimize the adaptive regret.
However, existing algorithms lack universality in the sense that they can only
handle one type of convex functions and need apriori knowledge of parameters,
which hinders their application in real-world scenarios. To address this
limitation, this paper investigates universal algorithms with dual adaptivity,
which automatically adapt to the property of functions (convex, exponentially
concave, or strongly convex), as well as the nature of environments (stationary
or changing). Specifically, we propose a meta-expert framework for dual
adaptive algorithms, where multiple experts are created dynamically and
aggregated by a meta-algorithm. The meta-algorithm is required to yield a
second-order bound, which can accommodate unknown function types. We further
incorporate the technique of sleeping experts to capture the changing
environments. For the construction of experts, we introduce two strategies
(increasing the number of experts or enhancing the capabilities of experts) to
achieve universality. Theoretical analysis shows that our algorithms are able
to minimize the adaptive regret for multiple types of convex functions
simultaneously, and also allow the type of functions to switch between rounds.
Moreover, we extend our meta-expert framework to online composite optimization,
and develop a universal algorithm for minimizing the adaptive regret of
composite functions.

</details>


### [42] [ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs](https://arxiv.org/abs/2508.00394)
*Antonis Klironomos,Baifan Zhou,Zhipeng Tan,Zhuoxun Zheng,Mohamed H. Gad-Elrab,Heiko Paulheim,Evgeny Kharlamov*

Main category: cs.LG

TL;DR: ExeKGLib是一个Python库，通过图形界面和知识图谱帮助非ML专家构建ML管道。


<details>
  <summary>Details</summary>
Motivation: 解决非ML专家在科学和工程领域构建高质量ML管道的困难。

Method: 利用知识图谱编码ML知识，提供图形界面简化操作。

Result: ExeKGLib提高了ML管道的透明度、可重用性和可执行性。

Conclusion: ExeKGLib为非ML专家提供了实用的ML工具，展示了其可用性和实用性。

Abstract: Nowadays machine learning (ML) practitioners have access to numerous ML
libraries available online. Such libraries can be used to create ML pipelines
that consist of a series of steps where each step may invoke up to several ML
libraries that are used for various data-driven analytical tasks. Development
of high-quality ML pipelines is non-trivial; it requires training, ML
expertise, and careful development of each step. At the same time, domain
experts in science and engineering may not possess such ML expertise and
training while they are in pressing need of ML-based analytics. In this paper,
we present our ExeKGLib, a Python library enhanced with a graphical interface
layer that allows users with minimal ML knowledge to build ML pipelines. This
is achieved by relying on knowledge graphs that encode ML knowledge in simple
terms accessible to non-ML experts. ExeKGLib also allows improving the
transparency and reusability of the built ML workflows and ensures that they
are executable. We show the usability and usefulness of ExeKGLib by presenting
real use cases.

</details>


### [43] [Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement](https://arxiv.org/abs/2508.00410)
*Zizhuo Zhang,Jianing Zhu,Xinmu Ge,Zihua Zhao,Zhanke Zhou,Xuan Li,Xiao Feng,Jiangchao Yao,Bo Han*

Main category: cs.LG

TL;DR: 论文提出了一种名为Co-Reward的新型强化学习框架，通过对比语义类比问题的一致性作为奖励基础，解决了现有自奖励方法中的崩溃问题，并在多个推理基准测试中表现优于其他自奖励基线。


<details>
  <summary>Details</summary>
Motivation: 尽管带有可验证奖励的强化学习（RLVR）在提升大型语言模型（LLMs）的推理能力方面显示出潜力，但其依赖人工标注标签的问题在复杂任务中尤为突出。现有的自奖励方法虽然展示了LLM推理的潜力，但存在崩溃问题。

Method: 提出Co-Reward框架，通过构建语义类比问题对，利用简单的投票机制生成代理标签，并通过交叉引用问题对的标签来强化内部推理一致性。这种自监督奖励机制增加了学习崩溃为平凡解的难度。

Result: 在多个推理基准测试和LLM系列中，Co-Reward表现优于其他自奖励基线，并在某些情况下达到或超过基于真实标签的奖励，如在MATH500上对Llama-3.2-3B-Instruct的改进高达+6.8%。

Conclusion: Co-Reward通过自监督奖励机制有效提升了LLM的推理能力，解决了自奖励方法中的崩溃问题，并在性能上超越了其他基线方法。

Abstract: Although reinforcement learning with verifiable rewards (RLVR) shows promise
in improving the reasoning ability of large language models (LLMs), the scaling
up dilemma remains due to the reliance on human annotated labels especially for
complex tasks. Recent alternatives that explore various self-reward signals
exhibit the eliciting potential of LLM reasoning, but suffer from the
non-negligible collapse issue. Inspired by the success of self-supervised
learning, we propose \textit{Co-Reward}, a novel RL framework that leverages
contrastive agreement across semantically analogical questions as a reward
basis. Specifically, we construct a similar question for each training sample
(without labels) and synthesize their individual surrogate labels through a
simple rollout voting, and then the reward is constructed by cross-referring
the labels of each question pair to enforce the internal reasoning consistency
across analogical inputs. Intuitively, such a self-supervised reward-shaping
mechanism increases the difficulty of learning collapse into a trivial
solution, and promotes stable reasoning elicitation and improvement through
expanding the input sample variants. Empirically, Co-Reward achieves superior
performance compared to other self-reward baselines on multiple reasoning
benchmarks and LLM series, and reaches or even surpasses ground-truth (GT)
labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward
on Llama-3.2-3B-Instruct. Our code is publicly available at
https://github.com/tmlr-group/Co-Reward.

</details>


### [44] [Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection](https://arxiv.org/abs/2508.00415)
*Yue Yang,Yuxiang Lin,Ying Zhang,Zihan Su,Chang Chuan Goh,Tangtangfang Fang,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 该研究提出了一种名为ResE-BiLSTM的模型，用于预测贷款违约，通过滑动窗口技术和多种评估指标验证其优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 贷款违约预测对信用风险管理至关重要，机器学习方法可提高预测性能。

Method: 使用ResE-BiLSTM模型，结合滑动窗口技术，并在Freddie Mac抵押贷款数据集上评估，与五种基线模型对比。

Result: ResE-BiLSTM在准确性、精确度、召回率、F1和AUC等指标上优于基线模型。

Conclusion: ResE-BiLSTM具有实际应用价值，适用于真实场景。

Abstract: Prediction of post-loan default is an important task in credit risk
management, and can be addressed by detection of financial anomalies using
machine learning. This study introduces a ResE-BiLSTM model, using a sliding
window technique, and is evaluated on 44 independent cohorts from the extensive
Freddie Mac US mortgage dataset, to improve prediction performance. The
ResE-BiLSTM is compared with five baseline models: Long Short-Term Memory
(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks
(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including
Accuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to
evaluate the contribution of individual components in the ResE-BiLSTM
architecture. Additionally, SHAP analysis was employed to interpret the
underlying features the model relied upon for its predictions. Experimental
results demonstrate that ResE-BiLSTM achieves superior predictive performance
compared to baseline models, underscoring its practical value and applicability
in real-world scenarios.

</details>


### [45] [A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces](https://arxiv.org/abs/2508.00472)
*Leonidas Akritidis,Panayiotis Bozanis*

Main category: cs.LG

TL;DR: 论文提出了一种名为ctdGAN的条件GAN模型，用于解决表格数据中的类别不平衡问题。通过空间分区和新的损失函数，ctdGAN能生成更接近原始数据分布的子空间样本，显著提升了分类性能。


<details>
  <summary>Details</summary>
Motivation: 表格数据中的类别不平衡问题严重影响了机器学习任务的性能。现有GAN模型未考虑输入样本的向量子空间，导致生成数据位置不准确，且类别标签处理方式不够有效。

Method: ctdGAN通过空间分区为输入样本分配聚类标签，利用新的概率采样策略和损失函数生成样本，并结合聚类缩放技术捕获多特征模式。

Result: 在14个不平衡数据集上的实验表明，ctdGAN能生成高保真样本，显著提升分类准确率。

Conclusion: ctdGAN通过改进生成策略和损失函数，有效解决了表格数据中的类别不平衡问题，生成样本质量高且分类性能优越。

Abstract: The tabular form constitutes the standard way of representing data in
relational database systems and spreadsheets. But, similarly to other forms,
tabular data suffers from class imbalance, a problem that causes serious
performance degradation in a wide variety of machine learning tasks. One of the
most effective solutions dictates the usage of Generative Adversarial Networks
(GANs) in order to synthesize artificial data instances for the
under-represented classes. Despite their good performance, none of the proposed
GAN models takes into account the vector subspaces of the input samples in the
real data space, leading to data generation in arbitrary locations. Moreover,
the class labels are treated in the same manner as the other categorical
variables during training, so conditional sampling by class is rendered less
effective. To overcome these problems, this study presents ctdGAN, a
conditional GAN for alleviating class imbalance in tabular datasets. Initially,
ctdGAN executes a space partitioning step to assign cluster labels to the input
samples. Subsequently, it utilizes these labels to synthesize samples via a
novel probabilistic sampling strategy and a new loss function that penalizes
both cluster and class mis-predictions. In this way, ctdGAN is trained to
generate samples in subspaces that resemble those of the original data
distribution. We also introduce several other improvements, including a simple,
yet effective cluster-wise scaling technique that captures multiple feature
modes without affecting data dimensionality. The exhaustive evaluation of
ctdGAN with 14 imbalanced datasets demonstrated its superiority in generating
high fidelity samples and improving classification accuracy.

</details>


### [46] [Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection](https://arxiv.org/abs/2508.00507)
*Yiming Xu,Jiarun Chen,Zhen Peng,Zihan Chen,Qika Lin,Lan Ma,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出CoLL框架，结合LLMs和GNNs，用于文本属性图的异常检测，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法忽视文本模态的潜力，且浅层文本编码可能丢失语义信息。LLMs虽强但难以处理图结构信息。

Method: CoLL框架通过多LLM协作生成证据增强的文本特征，并结合带门控机制的GNN融合文本与图结构信息。

Result: 实验显示CoLL平均AP提升13.37%，显著优于现有方法。

Conclusion: CoLL为LLMs在图异常检测中的应用开辟了新途径，展示了多模态融合的潜力。

Abstract: The natural combination of intricate topological structures and rich textual
information in text-attributed graphs (TAGs) opens up a novel perspective for
graph anomaly detection (GAD). However, existing GAD methods primarily focus on
designing complex optimization objectives within the graph domain, overlooking
the complementary value of the textual modality, whose features are often
encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so
that semantic context related to anomalies may be missed. To unleash the
enormous potential of textual modality, large language models (LLMs) have
emerged as promising alternatives due to their strong semantic understanding
and reasoning capabilities. Nevertheless, their application to TAG anomaly
detection remains nascent, and they struggle to encode high-order structural
information inherent in graphs due to input length constraints. For
high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that
combines LLMs and graph neural networks (GNNs) to leverage their complementary
strengths. CoLL employs multi-LLM collaboration for evidence-augmented
generation to capture anomaly-relevant contexts while delivering human-readable
rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped
with a gating mechanism to adaptively fuse textual features with evidence while
preserving high-order topological information. Extensive experiments
demonstrate the superiority of CoLL, achieving an average improvement of 13.37%
in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

</details>


### [47] [Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning](https://arxiv.org/abs/2508.00513)
*Yiming Xu,Xu Hua,Zhen Peng,Bin Shi,Jiarun Chen,Xingbo Fu,Song Wang,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种名为CMUCL的端到端方法，用于文本属性图的异常检测，通过联合训练文本和图编码器，利用跨模态和多尺度一致性提升检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理文本属性图时，文本编码与异常检测目标分离，导致检测能力受限。如何整合文本和图拓扑信息以提升异常检测效果是一个挑战。

Method: 提出CMUCL方法，联合训练文本和图编码器，利用跨模态和多尺度一致性挖掘异常信息，并设计基于不一致性挖掘的异常评分器。

Result: 实验表明，CMUCL在文本属性图异常检测中表现优异，平均准确率（AP）提升11.13%。

Conclusion: CMUCL通过整合文本和图信息，显著提升了异常检测性能，并发布了8个数据集以支持未来研究。

Abstract: The widespread application of graph data in various high-risk scenarios has
increased attention to graph anomaly detection (GAD). Faced with real-world
graphs that often carry node descriptions in the form of raw text sequences,
termed text-attributed graphs (TAGs), existing graph anomaly detection
pipelines typically involve shallow embedding techniques to encode such textual
information into features, and then rely on complex self-supervised tasks
within the graph domain to detect anomalies. However, this text encoding
process is separated from the anomaly detection training objective in the graph
domain, making it difficult to ensure that the extracted textual features focus
on GAD-relevant information, seriously constraining the detection capability.
How to seamlessly integrate raw text and graph topology to unleash the vast
potential of cross-modal data in TAGs for anomaly detection poses a challenging
issue. This paper presents a novel end-to-end paradigm for text-attributed
graph anomaly detection, named CMUCL. We simultaneously model data from both
text and graph structures, and jointly train text and graph encoders by
leveraging cross-modal and uni-modal multi-scale consistency to uncover
potential anomaly-related information. Accordingly, we design an anomaly score
estimator based on inconsistency mining to derive node-specific anomaly scores.
Considering the lack of benchmark datasets tailored for anomaly detection on
TAGs, we release 8 datasets to facilitate future research. Extensive
evaluations show that CMUCL significantly advances in text-attributed graph
anomaly detection, delivering an 11.13% increase in average accuracy (AP) over
the suboptimal.

</details>


### [48] [Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting](https://arxiv.org/abs/2508.00523)
*Sifan Yang,Yuanyu Wan,Lijun Zhang*

Main category: cs.LG

TL;DR: 论文研究了带延迟反馈的在线非子模优化问题，提出了两种算法DBGD-NF和其扩展版，分别改进了现有方法的遗憾界，并通过实验验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法的最大延迟依赖性和延迟与反馈的耦合效应限制了性能，需要更优的算法来解决这些问题。

Method: 提出了DBGD-NF算法，利用单点梯度估计器，并扩展了其更新机制以解耦延迟和反馈效应。

Result: DBGD-NF实现了与平均延迟相关的遗憾界，扩展版进一步解耦效应，获得了更优的遗憾界。

Conclusion: 新算法在延迟和反馈处理上优于现有方法，实验验证了其有效性。

Abstract: We investigate the online nonsubmodular optimization with delayed feedback in
the bandit setting, where the loss function is $\alpha$-weakly DR-submodular
and $\beta$-weakly DR-supermodular. Previous work has established an
$(\alpha,\beta)$-regret bound of $\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is
the dimensionality and $d$ is the maximum delay. However, its regret bound
relies on the maximum delay and is thus sensitive to irregular delays.
Additionally, it couples the effects of delays and bandit feedback as its bound
is the product of the delay term and the $\mathcal{O}(nT^{2/3})$ regret bound
in the bandit setting without delayed feedback. In this paper, we develop two
algorithms to address these limitations, respectively. Firstly, we propose a
novel method, namely DBGD-NF, which employs the one-point gradient estimator
and utilizes all the available estimated gradients in each round to update the
decision. It achieves a better $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ regret
bound, which is relevant to the average delay $\bar{d} =
\frac{1}{T}\sum_{t=1}^T d_t\leq d$. Secondly, we extend DBGD-NF by employing a
blocking update mechanism to decouple the joint effect of the delays and bandit
feedback, which enjoys an $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$ regret bound.
When $d = \mathcal{O}(T^{1/3})$, our regret bound matches the
$\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.
Compared to our first $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ bound, it is more
advantageous when the maximum delay $d = o(\bar{d}^{2/3}T^{1/3})$. Finally, we
conduct experiments on structured sparse learning to demonstrate the
superiority of our methods.

</details>


### [49] [Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery](https://arxiv.org/abs/2508.00539)
*Judy X Yang*

Main category: cs.LG

TL;DR: 提出一种两阶段框架，通过信噪比阈值和Savitzky-Golay滤波优化高光谱数据，结合KMeans和NNLS提升矿物检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱成像中弱矿物信号被噪声和冗余波段掩盖的问题。

Method: 两阶段方法：1) 信噪比阈值和滤波降噪；2) KMeans聚类和NNLS解混。

Result: 实验表明，该方法提高了解混精度和弱矿物区域的检测能力。

Conclusion: 两阶段策略为地质高光谱应用提供了实用且可重复的解决方案。

Abstract: Hyperspectral imaging offers detailed spectral information for mineral
mapping; however, weak mineral signatures are often masked by noisy and
redundant bands, limiting detection performance. To address this, we propose a
two-stage integrated framework for enhanced mineral detection in the Cuprite
mining district. In the first stage, we compute the signal-to-noise ratio (SNR)
for each spectral band and apply a phase-locked thresholding technique to
discard low-SNR bands, effectively removing redundancy and suppressing
background noise. Savitzky-Golay filtering is then employed for spectral
smoothing, serving a dual role first to stabilize trends during band selection,
and second to preserve fine-grained spectral features during preprocessing. In
the second stage, the refined HSI data is reintroduced into the model, where
KMeans clustering is used to extract 12 endmember spectra (W1 custom), followed
by non negative least squares (NNLS) for abundance unmixing. The resulting
endmembers are quantitatively compared with laboratory spectra (W1 raw) using
cosine similarity and RMSE metrics. Experimental results confirm that our
proposed pipeline improves unmixing accuracy and enhances the detection of weak
mineral zones. This two-pass strategy demonstrates a practical and reproducible
solution for spectral dimensionality reduction and unmixing in geological HSI
applications.

</details>


### [50] [Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides](https://arxiv.org/abs/2508.00578)
*Marlen Neubert,Patrick Reiser,Frauke Gräter,Pascal Friederich*

Main category: cs.LG

TL;DR: 论文提出了一种利用机器学习势能模拟氢原子转移（HAT）反应的方法，通过生成大量肽段HAT配置数据，并比较三种图神经网络架构的性能，最终MACE表现最佳，实现了高精度预测。


<details>
  <summary>Details</summary>
Motivation: HAT反应在生物过程中至关重要，但其机理尚不完全清楚，传统模拟方法无法满足量子化学精度需求。

Method: 系统生成肽段HAT配置数据集，使用半经验方法和DFT，并比较三种图神经网络架构（SchNet、Allegro、MACE）的性能。

Result: MACE在能量、力和反应势垒预测上表现最优，平均绝对误差为1.13 kcal/mol，适用于大规模胶原模拟。

Conclusion: 该方法可推广至其他生物分子系统，实现复杂环境中化学反应的高精度模拟。

Abstract: Hydrogen atom transfer (HAT) reactions are essential in many biological
processes, such as radical migration in damaged proteins, but their mechanistic
pathways remain incompletely understood. Simulating HAT is challenging due to
the need for quantum chemical accuracy at biologically relevant scales; thus,
neither classical force fields nor DFT-based molecular dynamics are applicable.
Machine-learned potentials offer an alternative, able to learn potential energy
surfaces (PESs) with near-quantum accuracy. However, training these models to
generalize across diverse HAT configurations, especially at radical positions
in proteins, requires tailored data generation and careful model selection.
Here, we systematically generate HAT configurations in peptides to build large
datasets using semiempirical methods and DFT. We benchmark three graph neural
network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT
PESs and indirectly predict reaction barriers from energy predictions. MACE
consistently outperforms the others in energy, force, and barrier prediction,
achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT
barrier predictions. This accuracy enables integration of ML potentials into
large-scale collagen simulations to compute reaction rates from predicted
barriers, advancing mechanistic understanding of HAT and radical migration in
peptides. We analyze scaling laws, model transferability, and cost-performance
trade-offs, and outline strategies for improvement by combining ML potentials
with transition state search algorithms and active learning. Our approach is
generalizable to other biomolecular systems, enabling quantum-accurate
simulations of chemical reactivity in complex environments.

</details>


### [51] [The Role of Active Learning in Modern Machine Learning](https://arxiv.org/abs/2508.00586)
*Thorben Werner,Lars Schmidt-Thieme,Vijaya Krishna Yalavarthi*

Main category: cs.LG

TL;DR: AL在低数据场景下效率较低，但结合DA和SSL后仍能提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究AL在低数据场景下的表现，探索其与其他方法（DA、SSL）的结合效果。

Method: 比较AL、DA和SSL在低数据场景下的性能，分析其组合效果。

Result: AL单独效果有限（提升1-4%），但结合DA和SSL后仍有额外提升。

Conclusion: AL应作为DA和SSL后的补充方法，而非主要解决低数据问题的工具。

Abstract: Even though Active Learning (AL) is widely studied, it is rarely applied in
contexts outside its own scientific literature. We posit that the reason for
this is AL's high computational cost coupled with the comparatively small lifts
it is typically able to generate in scenarios with few labeled points. In this
work we study the impact of different methods to combat this low data scenario,
namely data augmentation (DA), semi-supervised learning (SSL) and AL. We find
that AL is by far the least efficient method of solving the low data problem,
generating a lift of only 1-4\% over random sampling, while DA and SSL methods
can generate up to 60\% lift in combination with random sampling. However, when
AL is combined with strong DA and SSL techniques, it surprisingly is still able
to provide improvements. Based on these results, we frame AL not as a method to
combat missing labels, but as the final building block to squeeze the last bits
of performance out of data after appropriate DA and SSL methods as been
applied.

</details>


### [52] [Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data](https://arxiv.org/abs/2508.00615)
*Mukesh Kumar Sahu,Pinki Roy*

Main category: cs.LG

TL;DR: 提出了一种基于相似性的自构建图模型（SBSCGM）和混合图神经网络（HybridGraphMedGNN），用于预测ICU患者的死亡风险和关键性评分，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以利用电子健康记录（EHR）中的关系结构，需要一种动态构建患者相似图的方法以提高预测准确性。

Method: SBSCGM通过混合相似性度量动态构建患者相似图，HybridGraphMedGNN结合GCN、GraphSAGE和GAT层学习患者表示。

Result: 在MIMIC-III数据集上，模型AUC-ROC达0.94，优于基线模型，并提供可解释的预测。

Conclusion: 该框架为重症监护风险预测提供了可扩展且可解释的解决方案，适用于实际ICU部署。

Abstract: Accurately predicting the criticalness of ICU patients (such as in-ICU
mortality risk) is vital for early intervention in critical care. However,
conventional models often treat each patient in isolation and struggle to
exploit the relational structure in Electronic Health Records (EHR). We propose
a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds
a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN
architecture that operates on this graph to predict patient mortality and a
continuous criticalness score. SBSCGM uses a hybrid similarity measure
(combining feature-based and structural similarities) to connect patients with
analogous clinical profiles in real-time. The HybridGraphMedGNN integrates
Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)
layers to learn robust patient representations, leveraging both local and
global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III
dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)
outperforming baseline classifiers and single-type GNN models. We also
demonstrate improved precision/recall and show that the attention mechanism
provides interpretable insights into model predictions. Our framework offers a
scalable and interpretable solution for critical care risk prediction, with
potential to support clinicians in real-world ICU deployment.

</details>


### [53] [IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources](https://arxiv.org/abs/2508.00627)
*Paul Tresson,Pierre Le Coz,Hadrien Tulet,Anthony Malkassian,Maxime Réjou Méchain*

Main category: cs.LG

TL;DR: IAMAP是一个用户友好的QGIS插件，通过自监督学习策略解决深度学习在遥感中的三大挑战：大数据需求、高计算资源和编码技能。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在遥感应用中因大数据需求、高计算资源和编码技能限制而难以普及的问题。

Method: IAMAP基于自监督学习的通用模型（基础模型），提供特征提取、降维、聚类、相似性映射和模型验证等功能，无需GPU或大量参考数据。

Result: IAMAP使非AI专家能够高效利用深度学习特征，推动了计算高效和节能的深度学习方法普及。

Conclusion: IAMAP通过简化流程和降低技术门槛，推动了深度学习在遥感领域的广泛应用。

Abstract: Remote sensing has entered a new era with the rapid development of artificial
intelligence approaches. However, the implementation of deep learning has
largely remained restricted to specialists and has been impractical because it
often requires (i) large reference datasets for model training and validation;
(ii) substantial computing resources; and (iii) strong coding skills. Here, we
introduce IAMAP, a user-friendly QGIS plugin that addresses these three
challenges in an easy yet flexible way. IAMAP builds on recent advancements in
self-supervised learning strategies, which now provide robust feature
extractors, often referred to as foundation models. These generalist models can
often be reliably used in few-shot or zero-shot scenarios (i.e., with little to
no fine-tuning). IAMAP's interface allows users to streamline several key steps
in remote sensing image analysis: (i) extracting image features using a wide
range of deep learning architectures; (ii) reducing dimensionality with
built-in algorithms; (iii) performing clustering on features or their reduced
representations; (iv) generating feature similarity maps; and (v) calibrating
and validating supervised machine learning models for prediction. By enabling
non-AI specialists to leverage the high-quality features provided by recent
deep learning approaches without requiring GPU capacity or extensive reference
datasets, IAMAP contributes to the democratization of computationally efficient
and energy-conscious deep learning methods.

</details>


### [54] [Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs](https://arxiv.org/abs/2508.00628)
*Xiong Xiong,Zhuo Zhang,Rongchun Hu,Chen Gao,Zichen Deng*

Main category: cs.LG

TL;DR: SV-SNN是一种新型神经网络框架，通过分离变量和自适应谱方法解决高频振荡PDE求解中的谱偏差问题，显著提升精度并减少参数和训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs在高频振荡PDE求解中存在谱偏差问题，无法有效捕捉高频成分。

Method: 提出SV-SNN框架，包括变量分离、自适应傅里叶谱特征和学习频率参数，以及基于SVD的理论框架。

Result: 在多个基准问题上，SV-SNN精度提升1-3个数量级，参数减少90%，训练时间减少60%。

Conclusion: SV-SNN有效解决了神经PDE求解中的谱偏差问题，具有显著优势。

Abstract: Solving high-frequency oscillatory partial differential equations (PDEs) is a
critical challenge in scientific computing, with applications in fluid
mechanics, quantum mechanics, and electromagnetic wave propagation. Traditional
physics-informed neural networks (PINNs) suffer from spectral bias, limiting
their ability to capture high-frequency solution components. We introduce
Separated-Variable Spectral Neural Networks (SV-SNN), a novel framework that
addresses these limitations by integrating separation of variables with
adaptive spectral methods. Our approach features three key innovations: (1)
decomposition of multivariate functions into univariate function products,
enabling independent spatial and temporal networks; (2) adaptive Fourier
spectral features with learnable frequency parameters for high-frequency
capture; and (3) theoretical framework based on singular value decomposition to
quantify spectral bias. Comprehensive evaluation on benchmark problems
including Heat equation, Helmholtz equation, Poisson equations and
Navier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of
magnitude improvement in accuracy while reducing parameter count by over 90\%
and training time by 60\%. These results establish SV-SNN as an effective
solution to the spectral bias problem in neural PDE solving. The implementation
will be made publicly available upon acceptance at
https://github.com/xgxgnpu/SV-SNN.

</details>


### [55] [KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting](https://arxiv.org/abs/2508.00635)
*Changning Wu,Gao Wu,Rongyao Cai,Yong Liu,Kexin Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于KAN的自适应频率选择学习架构（KFS），用于解决时间序列预测中的多尺度噪声干扰和频率信息分布不均问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列在不同尺度上存在噪声干扰，且频率信息分布不均导致多尺度表示不理想。

Method: 结合Kolmogorov-Arnold Networks（KAN）和Parseval定理，设计了KFS架构，包括FreK模块（基于能量分布选择主导频率）、时间戳嵌入对齐和特征混合模块。

Result: 在多个真实时间序列数据集上验证了KFS的优越性能，达到了最先进水平。

Conclusion: KFS是一种简单而有效的架构，能够有效解决多尺度噪声干扰和复杂模式建模问题。

Abstract: Multi-scale decomposition architectures have emerged as predominant
methodologies in time series forecasting. However, real-world time series
exhibit noise interference across different scales, while heterogeneous
information distribution among frequency components at varying scales leads to
suboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks
(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency
Selection learning architecture (KFS) to address these challenges. This
framework tackles prediction challenges stemming from cross-scale noise
interference and complex pattern modeling through its FreK module, which
performs energy-distribution-based dominant frequency selection in the spectral
domain. Simultaneously, KAN enables sophisticated pattern representation while
timestamp embedding alignment synchronizes temporal representations across
scales. The feature mixing module then fuses scale-specific patterns with
aligned temporal features. Extensive experiments across multiple real-world
time series datasets demonstrate that KT achieves state-of-the-art performance
as a simple yet effective architecture.

</details>


### [56] [Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense](https://arxiv.org/abs/2508.00641)
*Alessandro Palmas*

Main category: cs.LG

TL;DR: 论文探讨了强化学习在防御低成本自杀式无人机群攻击中的应用，通过高仿真模拟环境和强化学习代理优化拦截优先级，显著降低了平均损害并提高了防御效率。


<details>
  <summary>Details</summary>
Motivation: 低成本自杀式无人机群对现代防御系统构成严重威胁，需要快速战略决策以优化拦截优先级。

Method: 提出高仿真模拟环境，训练强化学习代理在离散动作空间中选择拦截目标，基于状态特征（如位置、类别和效应器状态）。

Result: 强化学习策略在数百次模拟攻击中表现优于基于规则的基线，平均损害更低，防御效率更高。

Conclusion: 强化学习可作为防御架构的战略层，提升韧性而不取代现有控制系统，相关代码和模拟资源已公开。

Abstract: The growing threat of low-cost kamikaze drone swarms poses a critical
challenge to modern defense systems demanding rapid and strategic
decision-making to prioritize interceptions across multiple effectors and
high-value target zones. In this work, we present a case study demonstrating
the practical advantages of reinforcement learning in addressing this
challenge. We introduce a high-fidelity simulation environment that captures
realistic operational constraints, within which a decision-level reinforcement
learning agent learns to coordinate multiple effectors for optimal interception
prioritization. Operating in a discrete action space, the agent selects which
drone to engage per effector based on observed state features such as
positions, classes, and effector status. We evaluate the learned policy against
a handcrafted rule-based baseline across hundreds of simulated attack
scenarios. The reinforcement learning based policy consistently achieves lower
average damage and higher defensive efficiency in protecting critical zones.
This case study highlights the potential of reinforcement learning as a
strategic layer within defense architectures, enhancing resilience without
displacing existing control systems. All code and simulation assets are
publicly released for full reproducibility, and a video demonstration
illustrates the policy's qualitative behavior.

</details>


### [57] [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](https://arxiv.org/abs/2508.00643)
*Albert Matveev,Sanmitra Ghosh,Aamal Hussain,James-Michael Leahy,Michalis Michaelides*

Main category: cs.LG

TL;DR: DINOZAUR是一种基于扩散的神经算子参数化方法，解决了FNO的可扩展性和不确定性量化问题，通过减少参数和内存占用，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: FNO存在参数过多和缺乏不确定性量化的问题，限制了其在科学和工程应用中的可靠性。

Method: DINOZAUR采用扩散乘子替代FNO中的密集张量乘子，减少参数和内存占用，并通过贝叶斯方法实现不确定性量化。

Result: DINOZAUR在多个PDE基准测试中表现优异，同时提供高效的不确定性量化。

Conclusion: DINOZAUR是一种高效、可扩展且具有不确定性量化能力的神经算子方法。

Abstract: Operator learning is a powerful paradigm for solving partial differential
equations, with Fourier Neural Operators serving as a widely adopted
foundation. However, FNOs face significant scalability challenges due to
overparameterization and offer no native uncertainty quantification -- a key
requirement for reliable scientific and engineering applications. Instead,
neural operators rely on post hoc UQ methods that ignore geometric inductive
biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator
parametrization with uncertainty quantification. Inspired by the structure of
the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a
dimensionality-independent diffusion multiplier that has a single learnable
time parameter per channel, drastically reducing parameter count and memory
footprint without compromising predictive performance. By defining priors over
those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield
spatially correlated outputs and calibrated uncertainty estimates. Our method
achieves competitive or superior performance across several PDE benchmarks
while providing efficient uncertainty quantification.

</details>


### [58] [TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction](https://arxiv.org/abs/2508.00657)
*Sihang Zeng,Lucas Jing Liu,Jun Wen,Meliha Yetisgen,Ruth Etzioni,Gang Luo*

Main category: cs.LG

TL;DR: TrajSurv是一个利用纵向电子健康记录（EHR）进行生存预测的模型，通过神经控制微分方程（NCDE）提取连续时间潜在状态，并结合对比学习和两阶段解释方法提高透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要可靠的生存预测，但现有方法难以处理不规则采样的临床数据并透明地关联临床进展与生存结果。

Method: TrajSurv使用NCDE从EHR中提取连续潜在轨迹，通过对比学习对齐潜在状态与患者状态，并采用两阶段解释方法（向量场分析和聚类）关联临床进展与生存结果。

Result: 在MIMIC-III和eICU数据集上，TrajSurv表现出优于现有深度学习方法的准确性和透明度。

Conclusion: TrajSurv通过连续潜在轨迹和透明解释方法，为临床生存预测提供了可靠且可解释的解决方案。

Abstract: Trustworthy survival prediction is essential for clinical decision making.
Longitudinal electronic health records (EHRs) provide a uniquely powerful
opportunity for the prediction. However, it is challenging to accurately model
the continuous clinical progression of patients underlying the irregularly
sampled clinical features and to transparently link the progression to survival
outcomes. To address these challenges, we develop TrajSurv, a model that learns
continuous latent trajectories from longitudinal EHR data for trustworthy
survival prediction. TrajSurv employs a neural controlled differential equation
(NCDE) to extract continuous-time latent states from the irregularly sampled
data, forming continuous latent trajectories. To ensure the latent trajectories
reflect the clinical progression, TrajSurv aligns the latent state space with
patient state space through a time-aware contrastive learning approach. To
transparently link clinical progression to the survival outcome, TrajSurv uses
latent trajectories in a two-step divide-and-conquer interpretation process.
First, it explains how the changes in clinical features translate into the
latent trajectory's evolution using a learned vector field. Second, it clusters
these latent trajectories to identify key clinical progression patterns
associated with different survival outcomes. Evaluations on two real-world
medical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and
superior transparency over existing deep learning methods.

</details>


### [59] [DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes](https://arxiv.org/abs/2508.00664)
*Jialun Zheng,Jie Liu,Jiannong Cao,Xiao Wang,Hanchen Yang,Yankai Chen,Philip S. Yu*

Main category: cs.LG

TL;DR: 论文提出了一种动态原型（DP）的DGAD模型，用于捕捉动态图中演变的异常模式，并在多领域数据上表现出色。


<details>
  <summary>Details</summary>
Motivation: 动态图异常检测（DGAD）在多个领域（如金融、交通、社交网络）中至关重要，但现有通用GAD模型难以捕捉动态图中的异常演变，且新领域数据缺乏标签。

Method: 提出DP-DGAD模型，通过动态原型提取和存储演变的正常与异常模式，选择性更新内存缓冲区，并结合异常评分器和伪标签进行自监督适应。

Result: 在十个真实数据集上实现了最先进的性能。

Conclusion: DP-DGAD能有效捕捉跨领域的动态异常模式，适用于无标签的新领域。

Abstract: Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies
in evolving graphs across domains such as finance, traffic, and social
networks. Recently, generalist graph anomaly detection (GAD) models have shown
promising results. They are pretrained on multiple source datasets and
generalize across domains. While effective on static graphs, they struggle to
capture evolving anomalies in dynamic graphs. Moreover, the continuous
emergence of new domains and the lack of labeled data further challenge
generalist DGAD. Effective cross-domain DGAD requires both domain-specific and
domain-agnostic anomalous patterns. Importantly, these patterns evolve
temporally within and across domains. Building on these insights, we propose a
DGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and
domain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,
evolving representations of normal and anomalous patterns, from temporal
ego-graphs and stores them in a memory buffer. The buffer is selectively
updated to retain general, domain-agnostic patterns while incorporating new
domain-specific ones. Then, an anomaly scorer compares incoming data with
dynamic prototypes to flag both general and domain-specific anomalies. Finally,
DP-DGAD employs confidence-based pseudo-labeling for effective self-supervised
adaptation in target domains. Extensive experiments demonstrate
state-of-the-art performance across ten real-world datasets from different
domains.

</details>


### [60] [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
*Sergio Rubio-Martín,María Teresa García-Ordás,Antonio Serrano-García,Clara Margarita Franch-Pato,Arturo Crespo-Álvaro,José Alberto Benítez-Andrades*

Main category: cs.LG

TL;DR: 比较多种AI模型（传统机器学习和深度学习）在分类临床笔记为焦虑和适应障碍诊断中的表现，发现超参数调优显著提升模型性能，而数据平衡方法影响有限。


<details>
  <summary>Details</summary>
Motivation: 临床笔记分类对心理健康诊断至关重要，研究旨在评估不同AI模型和数据平衡方法的效能。

Method: 使用多种传统机器学习（如随机森林、SVM）和深度学习模型（如DistilBERT、SciBERT），结合三种过采样策略和超参数调优。

Result: 超参数调优显著提升模型性能，SMOTE仅对BERT模型有正面影响。决策树和XGBoost在传统方法中表现最佳（96%准确率），BERT模型同样达到96%。

Conclusion: 超参数调优对模型性能至关重要，研究为AI辅助心理健康诊断提供了模型架构和数据平衡方法的参考。

Abstract: The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

</details>


### [61] [Learning Network Dismantling without Handcrafted Inputs](https://arxiv.org/abs/2508.00706)
*Haozhe Tian,Pietro Ferraro,Robert Shorten,Mahdi Jalili,Homayoun Hamedmoghadam*

Main category: cs.LG

TL;DR: 论文提出了一种基于注意力机制和消息迭代配置的图神经网络框架MIND，用于解决网络拆解问题，无需手工特征，并在大规模真实网络上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络依赖手工特征，增加了计算成本并引入偏差，因此需要一种更高效且通用的方法。

Method: 引入注意力机制和消息迭代配置，利用合成网络生成多样化训练集，构建MIND框架。

Result: MIND在大型真实网络上表现优于现有方法，具有高效性和泛化能力。

Conclusion: MIND框架不仅适用于网络拆解，还可推广到其他复杂网络问题。

Abstract: The application of message-passing Graph Neural Networks has been a
breakthrough for important network science problems. However, the competitive
performance often relies on using handcrafted structural features as inputs,
which increases computational cost and introduces bias into the otherwise
purely data-driven network representations. Here, we eliminate the need for
handcrafted features by introducing an attention mechanism and utilizing
message-iteration profiles, in addition to an effective algorithmic approach to
generate a structurally diverse training set of small synthetic networks.
Thereby, we build an expressive message-passing framework and use it to
efficiently solve the NP-hard problem of Network Dismantling, virtually
equivalent to vital node identification, with significant real-world
applications. Trained solely on diversified synthetic networks, our proposed
model -- MIND: Message Iteration Network Dismantler -- generalizes to large,
unseen real networks with millions of nodes, outperforming state-of-the-art
network dismantling methods. Increased efficiency and generalizability of the
proposed model can be leveraged beyond dismantling in a range of complex
network problems.

</details>


### [62] [Efficient Solution and Learning of Robust Factored MDPs](https://arxiv.org/abs/2508.00707)
*Yannik Schnitzer,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 本文提出了一种基于分解状态空间表示的方法，用于解决和学习鲁棒马尔可夫决策过程（r-MDPs），通过利用系统组件间的不确定性独立性，将非凸优化问题转化为可处理的线性规划问题，从而显著提高样本效率和策略性能。


<details>
  <summary>Details</summary>
Motivation: 传统的r-MDPs学习方法需要大量样本交互，且难以处理高维问题。本文旨在通过分解状态空间表示，利用系统组件间的不确定性独立性，提高样本效率和策略性能。

Method: 提出基于分解状态空间表示的方法，将非凸优化问题转化为线性规划问题，并直接学习分解模型表示。

Result: 实验结果表明，利用分解结构可以显著提高样本效率，生成比现有方法更有效的鲁棒策略，并提供更严格的性能保证。

Conclusion: 通过分解状态空间表示和优化问题转化，本文方法在r-MDPs的学习和策略合成中实现了更高的效率和性能。

Abstract: Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling
epistemic uncertainty about transition dynamics. Learning r-MDPs from
interactions with an unknown environment enables the synthesis of robust
policies with provable (PAC) guarantees on performance, but this can require a
large number of sample interactions. We propose novel methods for solving and
learning r-MDPs based on factored state-space representations that leverage the
independence between model uncertainty across system components. Although
policy synthesis for factored r-MDPs leads to hard, non-convex optimisation
problems, we show how to reformulate these into tractable linear programs.
Building on these, we also propose methods to learn factored model
representations directly. Our experimental results show that exploiting
factored structure can yield dimensional gains in sample efficiency, producing
more effective robust policies with tighter performance guarantees than
state-of-the-art methods.

</details>


### [63] [JSON-Bag: A generic game trajectory representation](https://arxiv.org/abs/2508.00712)
*Dien Nguyen,Diego Perez-Liebana,Simon Lucas*

Main category: cs.LG

TL;DR: JSON-Bag模型通过JSON描述的游戏轨迹进行通用表示，并使用Jensen-Shannon距离（JSD）作为度量标准。在六款桌游上验证了其有效性，表现优于手工特征基线，并展示了样本高效性和自动特征提取能力。


<details>
  <summary>Details</summary>
Motivation: 研究如何通用地表示游戏轨迹并评估其有效性，以解决游戏轨迹分类任务。

Method: 使用JSON-Bag模型对游戏轨迹进行令牌化，采用JSD作为距离度量，并通过原型最近邻搜索（P-NNS）进行评估。

Result: 在多数任务中优于手工特征基线，样本效率高，且自动特征提取显著提升低效任务准确率。

Conclusion: JSON-Bag模型是游戏轨迹表示的有效方法，JSD与策略距离高度相关。

Abstract: We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically
represent game trajectories by tokenizing their JSON descriptions and apply
Jensen-Shannon distance (JSD) as distance metric for them. Using a
prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of
JSON-Bag with JSD on six tabletop games -- \textit{7 Wonders},
\textit{Dominion}, \textit{Sea Salt and Paper}, \textit{Can't Stop},
\textit{Connect4}, \textit{Dots and boxes} -- each over three game trajectory
classification tasks: classifying the playing agents, game parameters, or game
seeds that were used to generate the trajectories.
  Our approach outperforms a baseline using hand-crafted features in the
majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag
prototype to represent game trajectory classes is also sample efficient.
Additionally, we demonstrate JSON-Bag ability for automatic feature extraction
by treating tokens as individual features to be used in Random Forest to solve
the tasks above, which significantly improves accuracy on underperforming
tasks. Finally, we show that, across all six games, the JSD between JSON-Bag
prototypes of agent classes highly correlates with the distances between
agents' policies.

</details>


### [64] [Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning](https://arxiv.org/abs/2508.00716)
*Yingxu Wang,Mengzhu Wang,Zhichao Huang,Suyu Liu*

Main category: cs.LG

TL;DR: NeGPR是一种针对带噪声标签的图级域适应框架，通过双分支预训练和嵌套伪标签细化机制提升跨域学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有图域适应方法假设源标签干净，但实际场景中标签噪声普遍，严重影响域适应性能。

Method: NeGPR采用双分支（语义和拓扑）预训练，通过邻域一致性减少噪声影响，并利用嵌套细化机制和噪声感知正则化策略。

Result: 在严重标签噪声下，NeGPR性能优于现有方法，准确率提升高达12.7%。

Conclusion: NeGPR有效解决了带噪声标签的图域适应问题，显著提升了跨域学习的鲁棒性。

Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled
source graphs to unlabeled target graphs by learning domain-invariant
representations, which is essential in applications such as molecular property
prediction and social network analysis. However, most existing GDA methods rely
on the assumption of clean source labels, which rarely holds in real-world
scenarios where annotation noise is pervasive. This label noise severely
impairs feature alignment and degrades adaptation performance under domain
shifts. To address this challenge, we propose Nested Graph Pseudo-Label
Refinement (NeGPR), a novel framework tailored for graph-level domain
adaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,
semantic and topology branches, by enforcing neighborhood consistency in the
feature space, thereby reducing the influence of noisy supervision. To bridge
domain gaps, NeGPR employs a nested refinement mechanism in which one branch
selects high-confidence target samples to guide the adaptation of the other,
enabling progressive cross-domain learning. Furthermore, since pseudo-labels
may still contain noise and the pre-trained branches are already overfitted to
the noisy labels in the source domain, NeGPR incorporates a noise-aware
regularization strategy. This regularization is theoretically proven to
mitigate the adverse effects of pseudo-label noise, even under the presence of
source overfitting, thus enhancing the robustness of the adaptation process.
Extensive experiments on benchmark datasets demonstrate that NeGPR consistently
outperforms state-of-the-art methods under severe label noise, achieving gains
of up to 12.7% in accuracy.

</details>


### [65] [Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK](https://arxiv.org/abs/2508.00718)
*Ivona Krchova,Mariana Vargas Vieyra,Mario Scriminaci,Andrey Sidorenko*

Main category: cs.LG

TL;DR: MOSTLY AI SDK是一个开源工具包，用于生成高质量的合成表格数据，解决数据隐私和访问问题。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、专有利益和伦理问题，高质量数据的获取受限，合成数据成为解决方案。

Method: 基于TabularARGN自回归框架，集成差分隐私、公平性生成和自动化质量保证。

Result: SDK在速度和可用性上表现优异，支持多种数据类型和复杂数据集。

Conclusion: SDK实用性强，促进了数据民主化，已快速应用于实际场景。

Abstract: Machine learning development critically depends on access to high-quality
data. However, increasing restrictions due to privacy, proprietary interests,
and ethical concerns have created significant barriers to data accessibility.
Synthetic data offers a viable solution by enabling safe, broad data usage
without compromising sensitive information. This paper presents the MOSTLY AI
Synthetic Data Software Development Kit (SDK), an open-source toolkit designed
specifically for synthesizing high-quality tabular data. The SDK integrates
robust features such as differential privacy guarantees, fairness-aware data
generation, and automated quality assurance into a flexible and accessible
Python interface. Leveraging the TabularARGN autoregressive framework, the SDK
supports diverse data types and complex multi-table and sequential datasets,
delivering competitive performance with notable improvements in speed and
usability. Currently deployed both as a cloud service and locally installable
software, the SDK has seen rapid adoption, highlighting its practicality in
addressing real-world data bottlenecks and promoting widespread data
democratization.

</details>


### [66] [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](https://arxiv.org/abs/2508.00734)
*Liuyun Xu,Seymour M. J. Spence*

Main category: cs.LG

TL;DR: 提出了一种结合多保真度分层采样和自适应机器学习元模型的方法，用于高效估计小失效概率，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂非线性有限元模型中计算小失效概率时仍需要大量模型评估，计算成本高。

Method: 采用分层采样生成高保真数据集，训练深度学习元模型作为低保真模型，结合多保真度蒙特卡罗框架估计失效概率。

Result: 应用于高层钢结构建筑，准确估计非线性响应的超越概率曲线，显著节省计算资源。

Conclusion: 该方法在保证精度的同时，显著提高了计算效率，适用于复杂系统的罕见事件分析。

Abstract: Existing variance reduction techniques used in stochastic simulations for
rare event analysis still require a substantial number of model evaluations to
estimate small failure probabilities. In the context of complex, nonlinear
finite element modeling environments, this can become computationally
challenging-particularly for systems subjected to stochastic excitation. To
address this challenge, a multi-fidelity stratified sampling scheme with
adaptive machine learning metamodels is introduced for efficiently propagating
uncertainties and estimating small failure probabilities. In this approach, a
high-fidelity dataset generated through stratified sampling is used to train a
deep learning-based metamodel, which then serves as a cost-effective and highly
correlated low-fidelity model. An adaptive training scheme is proposed to
balance the trade-off between approximation quality and computational demand
associated with the development of the low-fidelity model. By integrating the
low-fidelity outputs with additional high-fidelity results, an unbiased
estimate of the strata-wise failure probabilities is obtained using a
multi-fidelity Monte Carlo framework. The overall probability of failure is
then computed using the total probability theorem. Application to a full-scale
high-rise steel building subjected to stochastic wind excitation demonstrates
that the proposed scheme can accurately estimate exceedance probability curves
for nonlinear responses of interest, while achieving significant computational
savings compared to single-fidelity variance reduction approaches.

</details>


### [67] [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](https://arxiv.org/abs/2508.00754)
*Yaxin Ma,Benjamin Colburn,Jose C. Principe*

Main category: cs.LG

TL;DR: 提出一种基于特征空间密度的单确定性模型方法，用于量化分布偏移和OOD检测，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络和深度集成方法计算和存储成本高，需更高效的不确定性量化方法。

Method: 利用核密度估计的信息势场近似训练集特征空间密度，通过比较测试样本特征空间表示检测分布偏移。

Result: 在2D合成数据集和OOD检测任务中表现优于基线模型。

Conclusion: 单确定性模型方法能高效量化不确定性，适用于分布偏移和OOD检测。

Abstract: Bayesian neural networks and deep ensemble methods have been proposed for
uncertainty quantification; however, they are computationally intensive and
require large storage. By utilizing a single deterministic model, we can solve
the above issue. We propose an effective method based on feature space density
to quantify uncertainty for distributional shifts and out-of-distribution (OOD)
detection. Specifically, we leverage the information potential field derived
from kernel density estimation to approximate the feature space density of the
training set. By comparing this density with the feature space representation
of test samples, we can effectively determine whether a distributional shift
has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons
and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The
results demonstrate that our method outperforms baseline models.

</details>


### [68] [Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data](https://arxiv.org/abs/2508.00758)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: DDAE结合扩散模型和对比学习，提升表格数据异常检测性能，在ADBench的57个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 表格数据异常检测因复杂特征交互和异常样本稀缺而困难，现有方法如去噪自编码器和扩散模型各有局限。

Method: 提出DDAE框架，整合扩散噪声调度和对比学习，优化编码过程。

Result: 在半监督和无监督设置下，DDAE显著优于基线模型，PR-AUC和ROC-AUC提升显著。

Conclusion: 噪声策略对表格异常检测至关重要，不同噪声水平适用于不同训练场景。

Abstract: Anomaly detection in tabular data remains challenging due to complex feature
interactions and the scarcity of anomalous examples. Denoising autoencoders
rely on fixed-magnitude noise, limiting adaptability to diverse data
distributions. Diffusion models introduce scheduled noise and iterative
denoising, but lack explicit reconstruction mappings. We propose the
Diffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates
diffusion-based noise scheduling and contrastive learning into the encoding
process to improve anomaly detection. We evaluated DDAE on 57 datasets from
ADBench. Our method outperforms in semi-supervised settings and achieves
competitive results in unsupervised settings, improving PR-AUC by up to 65%
(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)
model baselines. We observed that higher noise levels benefit unsupervised
training, while lower noise with linear scheduling is optimal in
semi-supervised settings. These findings underscore the importance of
principled noise strategies in tabular anomaly detection.

</details>


### [69] [Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy](https://arxiv.org/abs/2508.00768)
*Antonio Tudisco,Andrea Marchesin,Maurizio Zamboni,Mariagrazia Graziano,Giovanna Turvani*

Main category: cs.LG

TL;DR: 论文分析了量子机器学习中变分量子电路（VQC）的性能，比较了振幅编码和角度编码模型在不同旋转门下的分类表现，发现编码方式对模型性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 量子计算和机器学习的结合推动了量子机器学习（QML）的发展，其中变分量子电路（VQC）是一种常用模型。研究旨在探索不同编码方式和旋转门对VQC分类性能的影响。

Method: 研究比较了振幅编码和角度编码模型，并通过在Wine和Diabetes数据集上训练和评估模型，分析了不同旋转门对分类性能的影响。

Result: 实验表明，在相同模型拓扑下，最佳和最差模型的准确率差异可达10%至30%，甚至高达41%。编码方式的选择显著影响分类性能。

Conclusion: 编码方式是VQC模型的一个重要超参数，其选择对模型性能有决定性影响。

Abstract: Recent advancements in Quantum Computing and Machine Learning have increased
attention to Quantum Machine Learning (QML), which aims to develop machine
learning models by exploiting the quantum computing paradigm. One of the widely
used models in this area is the Variational Quantum Circuit (VQC), a hybrid
model where the quantum circuit handles data inference while classical
optimization adjusts the parameters of the circuit. The quantum circuit
consists of an encoding layer, which loads data into the circuit, and a
template circuit, known as the ansatz, responsible for processing the data.
This work involves performing an analysis by considering both Amplitude- and
Angle-encoding models, and examining how the type of rotational gate applied
affects the classification performance of the model. This comparison is carried
out by training the different models on two datasets, Wine and Diabetes, and
evaluating their performance. The study demonstrates that, under identical
model topologies, the difference in accuracy between the best and worst models
ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the
results highlight how the choice of rotational gates used in encoding can
significantly impact the model's classification performance. The findings
confirm that the embedding represents a hyperparameter for VQC models.

</details>


### [70] [Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors](https://arxiv.org/abs/2508.00785)
*Bushra Akter,Md Biplob Hosen,Sabbir Ahmed,Mehrin Anannya,Md. Farhad Hossain*

Main category: cs.LG

TL;DR: 研究探讨了影响学生CGPA的多变量因素，通过文献综述、调查数据分析和机器学习模型，开发了一个预测和优化学术表现的网络应用。


<details>
  <summary>Details</summary>
Motivation: 学术表现受多种因素影响，研究旨在识别这些因素并开发策略以优化学生CGPA。

Method: 结合文献综述构建因果图，通过在线调查收集数据，使用回归和分类模型分析，并应用可解释AI技术。

Result: Ridge回归预测CGPA误差低（MAE=0.12），随机森林分类准确率高（98.68%）。关键因素包括学习时间、奖学金等。

Conclusion: 研究成功开发了预测工具，帮助学生优化学术表现，并提供了可解释的决策支持。

Abstract: Academic performance depends on a multivariable nexus of socio-academic and
financial factors. This study investigates these influences to develop
effective strategies for optimizing students' CGPA. To achieve this, we
reviewed various literature to identify key influencing factors and constructed
an initial hypothetical causal graph based on the findings. Additionally, an
online survey was conducted, where 1,050 students participated, providing
comprehensive data for analysis. Rigorous data preprocessing techniques,
including cleaning and visualization, ensured data quality before analysis.
Causal analysis validated the relationships among variables, offering deeper
insights into their direct and indirect effects on CGPA. Regression models were
implemented for CGPA prediction, while classification models categorized
students based on performance levels. Ridge Regression demonstrated strong
predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared
Error of 0.023. Random Forest outperformed in classification, attaining an
F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques
such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting
critical factors such as study hours, scholarships, parental education, and
prior academic performance. The study culminated in the development of a
web-based application that provides students with personalized insights,
allowing them to predict academic performance, identify areas for improvement,
and make informed decisions to enhance their outcomes.

</details>


### [71] [Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management](https://arxiv.org/abs/2508.00806)
*Ping Chen,Zhuohong Deng,Ping Li,Shuibing He,Hongzi Zhu,Yi Zheng,Zhefeng Wang,Baoxing Huai,Minyi Guo*

Main category: cs.LG

TL;DR: Adacc是一个结合自适应压缩和激活检查点的新型内存管理框架，旨在减少GPU内存占用，加速大型语言模型训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中的重计算会引入高达30%的开销，Adacc旨在通过优化内存管理减少这种开销。

Method: Adacc包含三个模块：层特定压缩算法、基于MILP的最优调度策略和自适应策略演化机制。

Result: 实验表明，Adacc能将训练速度提升1.01x至1.37x，同时保持与基线相当的模型精度。

Conclusion: Adacc通过自适应压缩和策略优化，有效减少了内存占用并加速了训练。

Abstract: Training large language models often employs recomputation to alleviate
memory pressure, which can introduce up to 30% overhead in real-world
scenarios. In this paper, we propose Adacc, a novel memory management framework
that combines adaptive compression and activation checkpointing to reduce the
GPU memory footprint. It comprises three modules: (1) We design layer-specific
compression algorithms that account for outliers in LLM tensors, instead of
directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We
propose an optimal scheduling policy that employs MILP to determine the best
memory optimization for each tensor. (3) To accommodate changes in training
tensors, we introduce an adaptive policy evolution mechanism that adjusts the
policy during training to enhance throughput. Experimental results show that
Adacc can accelerate the LLM training by 1.01x to 1.37x compared to
state-of-the-art frameworks, while maintaining comparable model accuracy to the
Baseline.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [72] [funOCLUST: Clustering Functional Data with Outliers](https://arxiv.org/abs/2508.00110)
*Katharine M. Clark,Paul D. McNicholas*

Main category: stat.ML

TL;DR: 提出了OCLUST算法的功能数据扩展版本，用于处理高维和异常值问题。


<details>
  <summary>Details</summary>
Motivation: 功能数据的无限维性和对异常值的敏感性给聚类带来挑战。

Method: 扩展OCLUST算法框架，创建鲁棒的曲线聚类和异常值修剪方法。

Result: 在模拟和真实数据集上验证了聚类和异常值识别的优异性能。

Conclusion: 该方法在功能数据聚类和异常值处理中表现出色。

Abstract: Functional data present unique challenges for clustering due to their
infinite-dimensional nature and potential sensitivity to outliers. An extension
of the OCLUST algorithm to the functional setting is proposed to address these
issues. The approach leverages the OCLUST framework, creating a robust method
to cluster curves and trim outliers. The methodology is evaluated on both
simulated and real-world functional datasets, demonstrating strong performance
in clustering and outlier identification.

</details>


### [73] [Sinusoidal Approximation Theorem for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.00247)
*Sergei Gleyzer,Hanh Nguyen,Dinesh P. Ramakrishnan,Eric A. F. Reinhardt*

Main category: stat.ML

TL;DR: 本文提出了一种新的Kolmogorov-Arnold网络（KAN）变体，用可学习频率的加权正弦函数替换原始表示中的内外函数，并通过实验验证其性能优于固定频率傅里叶变换，与多层感知机（MLP）相当。


<details>
  <summary>Details</summary>
Motivation: 传统多层感知机（MLP）的激活函数为线性变换和sigmoid函数，而KAN通过可学习的非线性激活函数（如样条函数）提供了替代方案。本文旨在探索更高效的KAN变体，进一步提升其性能。

Method: 提出了一种新的KAN变体，用加权正弦函数替换Kolmogorov-Arnold表示中的内外函数，并固定正弦激活的相位为线性间隔的常数值。通过理论证明和数值实验验证其有效性。

Result: 实验表明，该变体在多元函数逼近任务中优于固定频率傅里叶变换方法，且性能与MLPs相当。

Conclusion: 通过引入加权正弦函数作为激活函数，本文提出的KAN变体在理论和实验上均表现出色，为神经网络设计提供了新的思路。

Abstract: The Kolmogorov-Arnold representation theorem states that any continuous
multivariable function can be exactly represented as a finite superposition of
continuous single variable functions. Subsequent simplifications of this
representation involve expressing these functions as parameterized sums of a
smaller number of unique monotonic functions. These developments led to the
proof of the universal approximation capabilities of multilayer perceptron
networks with sigmoidal activations, forming the alternative theoretical
direction of most modern neural networks.
  Kolmogorov-Arnold Networks (KANs) have been recently proposed as an
alternative to multilayer perceptrons. KANs feature learnable nonlinear
activations applied directly to input values, modeled as weighted sums of basis
spline functions. This approach replaces the linear transformations and
sigmoidal post-activations used in traditional perceptrons. Subsequent works
have explored alternatives to spline-based activations. In this work, we
propose a novel KAN variant by replacing both the inner and outer functions in
the Kolmogorov-Arnold representation with weighted sinusoidal functions of
learnable frequencies. Inspired by simplifications introduced by Lorentz and
Sprecher, we fix the phases of the sinusoidal activations to linearly spaced
constant values and provide a proof of its theoretical validity. We also
conduct numerical experiments to evaluate its performance on a range of
multivariable functions, comparing it with fixed-frequency Fourier transform
methods and multilayer perceptrons (MLPs). We show that it outperforms the
fixed-frequency Fourier transform and achieves comparable performance to MLPs.

</details>
