<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [cs.LG](#cs.LG) [Total: 72]
- [stat.ML](#stat.ML) [Total: 7]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Deep Learning-based Human Gesture Channel Modeling for Integrated Sensing and Communication Scenarios](https://arxiv.org/abs/2507.06588)
*Zhengyu Zhang,Neeraj Varshney,Jelena Senic,Raied Caromi,Samuel Berweger,Camillo Gentile,Enrico M. Vitucci,Ruisi He,Vittorio Degli-Esposti*

Main category: eess.SP

TL;DR: 本文提出了一种基于深度学习的人体手势信道建模框架，用于6G无线系统中的集成感知与通信（ISAC）场景，通过分解人体部位并利用真实测量数据学习手势与多径特性的映射关系。


<details>
  <summary>Details</summary>
Motivation: 随着6G无线系统中集成感知与通信（ISAC）的发展，非接触式人体识别成为关键应用场景之一。由于手势运动会导致无线多径传播的微妙随机变化，如何准确建模人体手势信道成为ISAC系统设计与验证的关键问题。

Method: 提出了一种深度学习框架，将人体分解为多个部位，利用泊松神经网络预测每个部位的多径分量数量，并通过条件变分自编码器（C-VAE）生成散射点，进而重构连续信道脉冲响应和微多普勒特征。

Result: 仿真结果表明，该方法在不同手势和受试者中均表现出高准确性和泛化能力，为数据增强和手势ISAC系统评估提供了可解释的方法。

Conclusion: 该研究为ISAC系统中人体手势信道的建模提供了一种高效且可解释的解决方案，具有广泛的应用潜力。

Abstract: With the development of Integrated Sensing and Communication (ISAC) for
Sixth-Generation (6G) wireless systems, contactless human recognition has
emerged as one of the key application scenarios. Since human gesture motion
induces subtle and random variations in wireless multipath propagation, how to
accurately model human gesture channels has become a crucial issue for the
design and validation of ISAC systems. To this end, this paper proposes a deep
learning-based human gesture channel modeling framework for ISAC scenarios, in
which the human body is decomposed into multiple body parts, and the mapping
between human gestures and their corresponding multipath characteristics is
learned from real-world measurements. Specifically, a Poisson neural network is
employed to predict the number of Multi-Path Components (MPCs) for each human
body part, while Conditional Variational Auto-Encoders (C-VAEs) are reused to
generate the scattering points, which are further used to reconstruct
continuous channel impulse responses and micro-Doppler signatures. Simulation
results demonstrate that the proposed method achieves high accuracy and
generalization across different gestures and subjects, providing an
interpretable approach for data augmentation and the evaluation of
gesture-based ISAC systems.

</details>


### [2] [Graph Learning for Cooperative Cell-Free ISAC Systems: From Optimization to Estimation](https://arxiv.org/abs/2507.06612)
*Peng Jiang,Ming Li,Rang Liu,Qian Liu*

Main category: eess.SP

TL;DR: 本文提出两种图学习框架（动态图学习和轻量级镜像GAT）用于无蜂窝ISAC网络，显著提升多目标定位与测速精度，并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 无蜂窝ISAC系统在6G网络中潜力巨大，但需统一设计以优化系统级性能与多目标参数估计。

Method: 通过异构图建模网络，提出动态图学习框架（结合3D-CNN）和轻量级镜像GAT框架。

Result: 仿真显示两种框架在多目标定位与测速精度上优于传统方法，镜像GAT显著降低计算开销。

Conclusion: 图学习方法为无蜂窝ISAC提供了高效解决方案，镜像GAT尤其适合实际部署。

Abstract: Cell-free integrated sensing and communication (ISAC) systems have emerged as
a promising paradigm for sixth-generation (6G) networks, enabling simultaneous
high-rate data transmission and high-precision radar sensing through
cooperative distributed access points (APs). Fully exploiting these
capabilities requires a unified design that bridges system-level optimization
with multi-target parameter estimation. This paper proposes an end-to-end graph
learning approach to close this gap, modeling the entire cell-free ISAC network
as a heterogeneous graph to jointly design the AP mode selection, user
association, precoding, and echo signal processing for multi-target position
and velocity estimation. In particular, we propose two novel heterogeneous
graph learning frameworks: a dynamic graph learning framework and a lightweight
mirror-based graph attention network (mirror-GAT) framework. The dynamic graph
learning framework employs structural and temporal attention mechanisms
integrated with a three-dimensional convolutional neural network (3D-CNN),
enabling superior performance and robustness in cell-free ISAC environments.
Conversely, the mirror-GAT framework significantly reduces computational
complexity and signaling overhead through a bi-level iterative structure with
share adjacency. Simulation results validate that both proposed
graph-learning-based frameworks achieve significant improvements in
multi-target position and velocity estimation accuracy compared to conventional
heuristic and optimization-based designs. Particularly, the mirror-GAT
framework demonstrates substantial reductions in computational time and
signaling overhead, underscoring its suitability for practical deployments.

</details>


### [3] [Wireless Energy Transfer Beamforming Optimization for Intelligent Transmitting Surface](https://arxiv.org/abs/2507.06805)
*Osmel Martínez Rosabal,Onel Alcaraz López,Victoria Dala Pegorara Souto,Richard Demo Souza,Samuel Montejo-Sánchez,Robert Schober,Hirley Alves*

Main category: eess.SP

TL;DR: 论文研究了利用智能传输表面（ITS）的无线能量传输技术，旨在最小化功率信标（PB）的功耗，并通过优化算法解决了非线性问题。


<details>
  <summary>Details</summary>
Motivation: 随着物联网设备的增长，高效无线能量传输技术需求迫切，智能传输表面为设计高效PB架构提供了新思路。

Method: 采用连续凸逼近（SCA）方法，联合优化数字预编码和相位配置，解决非线性非凸优化问题。

Result: 提出的ITS架构在功耗上优于数字和混合模拟-数字波束成形基准架构，且能适应不同场域的设备需求。

Conclusion: ITS-equipped PB架构在功耗和扩展性上表现优越，非均匀功率分布对波束成形有显著影响。

Abstract: Radio frequency (RF) wireless energy transfer (WET) is a promising technology
for powering the growing ecosystem of Internet of Things (IoT) devices using
power beacons (PBs). Recent research focuses on designing efficient PB
architectures that can support numerous antennas. In this context, PBs equipped
with intelligent surfaces present a promising approach, enabling physically
large, reconfigurable arrays. Motivated by these advantages, this work aims to
minimize the power consumption of a PB equipped with a passive intelligent
transmitting surface (ITS) and a collocated digital beamforming-based feeder to
charge multiple single-antenna devices. To model the PB's power consumption
accurately, we consider power amplifiers nonlinearities, ITS control power, and
feeder-to-ITS air interface losses. The resulting optimization problem is
highly nonlinear and nonconvex due to the high-power amplifier (HPA), the
received power constraints at the devices, and the unit-modulus constraint
imposed by the phase shifter configuration of the ITS. To tackle this issue, we
apply successive convex approximation (SCA) to iteratively solve convex
subproblems that jointly optimize the digital precoder and phase configuration.
Given SCA's sensitivity to initialization, we propose an algorithm that ensures
initialization feasibility while balancing convergence speed and solution
quality. We compare the proposed ITS-equipped PB's power consumption against
benchmark architectures featuring digital and hybrid analog-digital
beamforming. Results demonstrate that the proposed architecture efficiently
scales with the number of RF chains and ITS elements. We also show that
nonuniform ITS power distribution influences beamforming and can shift a device
between near- and far-field regions, even with a constant aperture.

</details>


### [4] [Enhancing Environment Generalizability for Deep Learning-Based CSI Feedback](https://arxiv.org/abs/2507.06833)
*Haoyu Wang,Shuangfeng Han,Xiaoyun Wang,Zhi Sun*

Main category: eess.SP

TL;DR: EG-CsiNet提出了一种新的CSI反馈学习框架，通过多路径解耦和细粒度对齐模块，提升了在未见环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的CSI反馈算法在未见环境中泛化能力有限，增加了部署成本。

Method: EG-CsiNet包含多路径解耦和细粒度对齐模块，以解决多路径结构和单一路径的分布偏移问题。

Result: EG-CsiNet在未见环境中表现出更强的泛化能力，尤其在单源环境的挑战性条件下。

Conclusion: EG-CsiNet显著提升了CSI反馈的环境泛化能力，优于现有方法。

Abstract: Accurate and low-overhead channel state information (CSI) feedback is
essential to boost the capacity of frequency division duplex (FDD) massive
multiple-input multiple-output (MIMO) systems. Deep learning-based CSI feedback
significantly outperforms conventional approaches. Nevertheless, current deep
learning-based CSI feedback algorithms exhibit limited generalizability to
unseen environments, which obviously increases the deployment cost. In this
paper, we first model the distribution shift of CSI across different
environments, which is composed of the distribution shift of multipath
structure and a single-path. Then, EG-CsiNet is proposed as a novel CSI
feedback learning framework to enhance environment-generalizability.
Explicitly, EG-CsiNet comprises the modules of multipath decoupling and
fine-grained alignment, which can address the distribution shift of multipath
structure and a single path. Based on extensive simulations, the proposed
EG-CsiNet can robustly enhance the generalizability in unseen environments
compared to the state-of-the-art, especially in challenging conditions with a
single source environment.

</details>


### [5] [OpenDPDv2: A Unified Learning and Optimization Framework for Neural Network Digital Predistortion](https://arxiv.org/abs/2507.06849)
*Yizhuo Wu,Ang Li,Chang Gao*

Main category: eess.SP

TL;DR: OpenDPDv2是一个用于功率放大器建模和数字预失真学习的统一框架，通过优化技术降低能耗，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络数字预失真（DPD）在宽带射频功率放大器中表现优异，但参数多且能耗高，需要优化。

Method: 提出TRes-DeltaGRU算法和两种节能方法，结合定点量化和动态时间稀疏性优化模型。

Result: 32位浮点模型ACPR为-59.4 dBc，EVM为-42.1 dBc；优化后能耗降低4.5倍，性能仍保持较高水平。

Conclusion: OpenDPDv2在降低能耗的同时保持了高性能，代码和数据集已开源。

Abstract: Neural network (NN)-based Digital Predistortion (DPD) stands out in improving
signal quality in wideband radio frequency (RF) power amplifiers (PAs)
employing complex modulation. However, NN DPDs usually rely on a large number
of parameters for effective linearization and can significantly contribute to
the energy consumption of the digital back-end in RF systems. This paper
presents OpenDPDv2, a unified framework for PA modeling, DPD learning, and
model optimization to reduce power consumption while maintaining high
linearization performance. The optimization techniques feature a novel DPD
algorithm, TRes-DeltaGRU, alongside two energy-efficient methods. The
top-performing 32-bit floating-point (FP32) TRes-DeltaGRU-DPD model achieves an
Adjacent Channel Power Ratio (ACPR) of -59.4 dBc and Error Vector Magnitude
(EVM) of -42.1 dBc. By exploiting fixed-point quantization and dynamic temporal
sparsity of input signals and hidden neurons, the inference energy of our model
can be reduced by 4.5X while still maintaining -50.3 dBc ACPR and -35.2 dB EVM
with 56% temporal sparsity. This was evaluated using a TM3.1a 200 MHz bandwidth
256-QAM OFDM signal applied to a 3.5 GHz GaN Doherty RF PA. OpenDPDv2 code,
datasets, and documentation are publicly accessible at:
https://github.com/lab-emi/OpenDPD.

</details>


### [6] [Joint Beamforming and Position Optimization for Fluid STAR-RIS-NOMA Assisted Wireless Communication Systems](https://arxiv.org/abs/2507.06904)
*Yu Liu,Qu Luo,Gaojie Chen,Pei Xiao,Ahmed Elzanaty,Mohsen Khalily,Rahim Tafazolli*

Main category: eess.SP

TL;DR: 本文提出了一种基于流体天线系统（FAS）的FSTAR-RIS辅助NOMA多用户通信系统，通过动态调整RIS元素位置增强空间自由度，并开发了优化算法提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 传统RIS在空间控制能力上存在局限，需要一种更灵活的系统来提升多用户通信的性能和适应性。

Method: 提出FSTAR-RIS系统，结合AO算法（含SCA、SDR和MM技术）优化基站波束成形、RIS传输/反射系数及元素位置。

Result: 仿真显示，相比传统STAR-RIS系统，总速率提升27%，且所需RIS元素减少50%。

Conclusion: FSTAR-RIS系统在提升性能和降低成本方面具有显著优势，适合大规模部署。

Abstract: To address the limitations of traditional reconfigurable intelligent surfaces
(RIS) in spatial control capability, this paper introduces the concept of the
fluid antenna system (FAS) and proposes a fluid simultaneously transmitting and
reflecting RIS (FSTAR-RIS) assisted non-orthogonal multiple access (NOMA)
multi-user communication system. In this system, each FSTAR-RIS element is
capable of flexible mobility and can dynamically adjust its position in
response to environmental variations, thereby enabling simultaneous service to
users in both the transmission and reflection zones. This significantly
enhances the system's spatial degrees of freedom (DoF) and service
adaptability. To maximize the system's weighted sum-rate, we formulate a
non-convex optimization problem that jointly optimizes the base station
beamforming, the transmission/reflection coefficients of the FSTAR-RIS, and the
element positions. An alternating optimization (AO) algorithm is developed,
incorporating successive convex approximation (SCA), semi-definite relaxation
(SDR), and majorization-minimization (MM) techniques. In particular, to address
the complex channel coupling introduced by the coexistence of direct and
FSTAR-RIS paths, the MM framework is employed in the element position
optimization subproblem, enabling an efficient iterative solution strategy.
Simulation results validate that the proposed system achieves up to a 27%
increase in total sum rate compared to traditional STAR-RIS systems and
requires approximately 50% fewer RIS elements to attain the same performance,
highlighting its effectiveness for cost-efficient large-scale deployment.

</details>


### [7] [Precise Representation Model of SAR Saturated Interference: Mechanism and Verification](https://arxiv.org/abs/2507.06932)
*Lunhao Duan,Xingyu Lu,Yushuang Liu,Jianchao Yang,Hong Gu*

Main category: eess.SP

TL;DR: 本文提出了一种基于贝塞尔函数的SAR接收机饱和干扰分析模型，解决了传统平滑函数近似方法的误差问题，并通过仿真验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: SAR接收机易受高功率射频干扰导致饱和，传统分析方法因饱和函数的非平滑特性难以准确分析，现有研究基于双曲正切函数的近似存在误差。

Method: 提出基于贝塞尔函数的饱和干扰分析模型，并通过仿真与传统平滑函数近似模型进行对比验证。

Result: 验证了所提模型在分析SAR接收机饱和干扰时的准确性。

Conclusion: 该模型为饱和干扰抑制等进一步工作提供了指导。

Abstract: Synthetic Aperture Radar (SAR) is highly susceptible to Radio Frequency
Interference (RFI). Due to the performance limitations of components such as
gain controllers and analog-to-digital converters in SAR receivers, high-power
interference can easily cause saturation of the SAR receiver, resulting in
nonlinear distortion of the interfered echoes, which are distorted in both the
time domain and frequency domain. Some scholars have analyzed the impact of SAR
receiver saturation on target echoes through simulations. However, the
saturation function has non-smooth characteristics, making it difficult to
conduct accurate analysis using traditional analytical methods. Current related
studies have approximated and analyzed the saturation function based on the
hyperbolic tangent function, but there are approximation errors. Therefore,
this paper proposes a saturation interference analysis model based on Bessel
functions, and verifies the accuracy of the proposed saturation interference
analysis model by simulating and comparing it with the traditional saturation
model based on smooth function approximation. This model can provide certain
guidance for further work such as saturation interference suppression.

</details>


### [8] [Federated Learning-based MARL for Strengthening Physical-Layer Security in B5G Networks](https://arxiv.org/abs/2507.06997)
*Deemah H. Tashman,Soumaya Cherkaoui,Walaa Hamouda*

Main category: eess.SP

TL;DR: 论文提出了一种基于联邦学习的多智能体强化学习（MARL）策略，用于提升超5G网络中多蜂窝网络的物理层安全性（PLS）。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决多蜂窝网络中合法用户在窃听者存在时的保密性问题。

Method: 方法包括使用深度强化学习（DRL）代理（基站）与环境交互，采用联邦学习共享网络参数而非用户数据，并比较了DQN和RDPG两种DRL方法。

Result: 结果表明RDPG比DQN收敛更快，且优于分布式DRL方法，同时揭示了安全性与复杂性的权衡。

Conclusion: 结论是提出的联邦学习MARL策略能有效提升物理层安全性，同时兼顾隐私保护。

Abstract: This paper explores the application of a federated learning-based multi-agent
reinforcement learning (MARL) strategy to enhance physical-layer security (PLS)
in a multi-cellular network within the context of beyond 5G networks. At each
cell, a base station (BS) operates as a deep reinforcement learning (DRL) agent
that interacts with the surrounding environment to maximize the secrecy rate of
legitimate users in the presence of an eavesdropper. This eavesdropper attempts
to intercept the confidential information shared between the BS and its
authorized users. The DRL agents are deemed to be federated since they only
share their network parameters with a central server and not the private data
of their legitimate users. Two DRL approaches, deep Q-network (DQN) and
Reinforce deep policy gradient (RDPG), are explored and compared. The results
demonstrate that RDPG converges more rapidly than DQN. In addition, we
demonstrate that the proposed method outperforms the distributed DRL approach.
Furthermore, the outcomes illustrate the trade-off between security and
complexity.

</details>


### [9] [How to Bridge the Sim-to-Real Gap in Digital Twin-Aided Telecommunication Networks](https://arxiv.org/abs/2507.07067)
*Clement Ruah,Houssem Sifaou,Osvaldo Simeone,Bashir M. Al-Hashimi*

Main category: eess.SP

TL;DR: 论文探讨了电信领域AI模型训练的挑战，提出通过数字孪生和模拟到现实（sim-to-real）策略解决数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 由于电信领域部署特定数据的稀缺性，训练有效的AI模型面临挑战，数字孪生提供了一种潜在解决方案。

Method: 论文回顾了两种互补策略：数字孪生的校准和模拟到现实差距感知的训练策略。

Result: 评估了两种方法：通过贝叶斯学习建模环境层面的差距，或通过预测驱动推理建模训练损失层面的差距。

Conclusion: 数字孪生和sim-to-real策略可以有效解决电信领域AI模型训练中的数据稀缺问题。

Abstract: Training effective artificial intelligence models for telecommunications is
challenging due to the scarcity of deployment-specific data. Real data
collection is expensive, and available datasets often fail to capture the
unique operational conditions and contextual variability of the network
environment. Digital twinning provides a potential solution to this problem, as
simulators tailored to the current network deployment can generate
site-specific data to augment the available training datasets. However, there
is a need to develop solutions to bridge the inherent simulation-to-reality
(sim-to-real) gap between synthetic and real-world data. This paper reviews
recent advances on two complementary strategies: 1) the calibration of digital
twins (DTs) through real-world measurements, and 2) the use of sim-to-real
gap-aware training strategies to robustly handle residual discrepancies between
digital twin-generated and real data. For the latter, we evaluate two
conceptually distinct methods that model the sim-to-real gap either at the
level of the environment via Bayesian learning or at the level of the training
loss via prediction-powered inference.

</details>


### [10] [Joint Target Acquisition and Refined Position Estimation in OFDM-based ISAC Networks](https://arxiv.org/abs/2507.07081)
*Lorenzo Pucci,Andrea Giorgetti*

Main category: eess.SP

TL;DR: 提出了一种两阶段框架，通过基站协作实现OFDM网络的联合目标获取与位置估计，显著提升检测性能和定位精度。


<details>
  <summary>Details</summary>
Motivation: 解决OFDM网络中基站协作下的目标检测与位置估计问题，利用空间多样性和协作ML估计器提升性能。

Method: 第一阶段：各基站计算距离-角度图检测目标并估计粗略位置；第二阶段：在共享全局参考系中，通过协作ML估计器在感兴趣区域内进行精细化定位。

Result: 数值结果表明，该方法通过基站协作提升了检测性能，并实现了厘米级定位精度。

Conclusion: 提出的两阶段框架在OFDM网络中有效实现了高精度的目标检测与定位。

Abstract: This paper addresses joint target acquisition and position estimation in an
OFDM-based integrated sensing and communication (ISAC) network with base
station (BS) cooperation via a fusion center. A two-stage framework is
proposed: in the first stage, each BS computes range-angle maps to detect
targets and estimate coarse positions, exploiting spatial diversity. In the
second stage, refined localization is performed using a cooperative maximum
likelihood (ML) estimator over predefined regions of interest (RoIs) within a
shared global reference frame. Numerical results demonstrate that the proposed
approach not only improves detection performance through BS cooperation but
also achieves centimeter-level localization accuracy, highlighting the
effectiveness of the refined estimation technique.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Neural Network-Based Parameter Estimation for Non-Autonomous Differential Equations with Discontinuous Signals](https://arxiv.org/abs/2507.06267)
*Hyeontae Jo,Krešimir Josić,Jae Kyoung Kim*

Main category: cs.LG

TL;DR: 提出了一种名为HADES-NN的新方法，通过神经网络平滑近似不连续信号，用于非自治微分方程的参数估计。


<details>
  <summary>Details</summary>
Motivation: 非自治微分方程在建模受外部信号影响的系统时很重要，但当信号突变时，拟合模型变得困难。

Method: HADES-NN分两阶段：1）用神经网络平滑近似不连续信号；2）用平滑信号估计模型参数。

Result: HADES-NN在多种应用中提供了高精度参数估计，如昼夜节律系统和酵母交配响应。

Conclusion: HADES-NN扩展了可拟合实际测量数据的模型系统范围。

Abstract: Non-autonomous differential equations are crucial for modeling systems
influenced by external signals, yet fitting these models to data becomes
particularly challenging when the signals change abruptly. To address this
problem, we propose a novel parameter estimation method utilizing functional
approximations with artificial neural networks. Our approach, termed Harmonic
Approximation of Discontinuous External Signals using Neural Networks
(HADES-NN), operates in two iterated stages. In the first stage, the algorithm
employs a neural network to approximate the discontinuous signal with a smooth
function. In the second stage, it uses this smooth approximate signal to
estimate model parameters. HADES-NN gives highly accurate and precise parameter
estimates across various applications, including circadian clock systems
regulated by external light inputs measured via wearable devices and the mating
response of yeast to external pheromone signals. HADES-NN greatly extends the
range of model systems that can be fit to real-world measurements.

</details>


### [12] [Sample-Efficient Reinforcement Learning Controller for Deep Brain Stimulation in Parkinson's Disease](https://arxiv.org/abs/2507.06326)
*Harsh Ravivarapu,Gaurav Bagwe,Xiaoyong Yuan,Chunxiu Yu,Lan Zhang*

Main category: cs.LG

TL;DR: SEA-DBS是一种基于强化学习的自适应深脑刺激框架，通过高效样本利用和稳定探索，解决了传统方法的不足，并在模拟中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统DBS缺乏适应性和个性化，且能耗高；aDBS虽提供闭环方案，但现有RL方法存在样本复杂度高、探索不稳定等问题。

Method: 提出SEA-DBS框架，结合预测奖励模型和Gumbel Softmax探索，优化样本效率和硬件兼容性。

Result: 在模拟中，SEA-DBS收敛更快，能更有效抑制病理beta波段活动，且适应低精度量化。

Conclusion: SEA-DBS为资源受限的实时神经调控提供了实用且高效的RL解决方案。

Abstract: Deep brain stimulation (DBS) is an established intervention for Parkinson's
disease (PD), but conventional open-loop systems lack adaptability, are
energy-inefficient due to continuous stimulation, and provide limited
personalization to individual neural dynamics. Adaptive DBS (aDBS) offers a
closed-loop alternative, using biomarkers such as beta-band oscillations to
dynamically modulate stimulation. While reinforcement learning (RL) holds
promise for personalized aDBS control, existing methods suffer from high sample
complexity, unstable exploration in binary action spaces, and limited
deployability on resource-constrained hardware.
  We propose SEA-DBS, a sample-efficient actor-critic framework that addresses
the core challenges of RL-based adaptive neurostimulation. SEA-DBS integrates a
predictive reward model to reduce reliance on real-time feedback and employs
Gumbel Softmax-based exploration for stable, differentiable policy updates in
binary action spaces. Together, these components improve sample efficiency,
exploration robustness, and compatibility with resource-constrained
neuromodulatory hardware. We evaluate SEA-DBS on a biologically realistic
simulation of Parkinsonian basal ganglia activity, demonstrating faster
convergence, stronger suppression of pathological beta-band power, and
resilience to post-training FP16 quantization. Our results show that SEA-DBS
offers a practical and effective RL-based aDBS framework for real-time,
resource-constrained neuromodulation.

</details>


### [13] [Transferable Parasitic Estimation via Graph Contrastive Learning and Label Rebalancing in AMS Circuits](https://arxiv.org/abs/2507.06535)
*Shan Shen,Shenglu Hua,Jiajun Zou,Jiawei Liu,Jianwang Zhai,Chuan Shi,Wenjian Yu*

Main category: cs.LG

TL;DR: CircuitGCL提出了一种新的图对比学习框架，通过表示散射和标签重平衡提升异构电路图的迁移性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决AMS电路中设计数据稀缺、标签分布不平衡和电路实现多样性带来的挑战。

Method: 采用自监督策略学习拓扑不变节点嵌入，结合平衡MSE和bsmCE损失优化标签分布。

Result: 在寄生电容估计和接地电容分类任务中，性能显著优于SOTA方法。

Conclusion: CircuitGCL为AMS电路提供了鲁棒且可迁移的表示学习方法。

Abstract: Graph representation learning on Analog-Mixed Signal (AMS) circuits is
crucial for various downstream tasks, e.g., parasitic estimation. However, the
scarcity of design data, the unbalanced distribution of labels, and the
inherent diversity of circuit implementations pose significant challenges to
learning robust and transferable circuit representations. To address these
limitations, we propose CircuitGCL, a novel graph contrastive learning
framework that integrates representation scattering and label rebalancing to
enhance transferability across heterogeneous circuit graphs. CircuitGCL employs
a self-supervised strategy to learn topology-invariant node embeddings through
hyperspherical representation scattering, eliminating dependency on large-scale
data. Simultaneously, balanced mean squared error (MSE) and softmax
cross-entropy (bsmCE) losses are introduced to mitigate label distribution
disparities between circuits, enabling robust and transferable parasitic
estimation. Evaluated on parasitic capacitance estimation (edge-level task) and
ground capacitance classification (node-level task) across TSMC 28nm AMS
designs, CircuitGCL outperforms all state-of-the-art (SOTA) methods, with the
$R^2$ improvement of $33.64\% \sim 44.20\%$ for edge regression and F1-score
gain of $0.9\times \sim 2.1\times$ for node classification. Our code is
available at
\href{https://anonymous.4open.science/r/CircuitGCL-099B/README.md}{here}.

</details>


### [14] [SymFlux: deep symbolic regression of Hamiltonian vector fields](https://arxiv.org/abs/2507.06342)
*M. A. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: SymFlux是一种新型深度学习框架，用于通过符号回归从标准辛平面上的向量场中识别哈密顿函数。


<details>
  <summary>Details</summary>
Motivation: 推动哈密顿力学中的自动化发现。

Method: 采用混合CNN-LSTM架构，从哈密顿向量场数据集中学习和输出符号数学表达式。

Result: 模型能准确恢复符号表达式。

Conclusion: SymFlux在哈密顿力学中具有显著的自动化发现潜力。

Abstract: We present SymFlux, a novel deep learning framework that performs symbolic
regression to identify Hamiltonian functions from their corresponding vector
fields on the standard symplectic plane. SymFlux models utilize hybrid CNN-LSTM
architectures to learn and output the symbolic mathematical expression of the
underlying Hamiltonian. Training and validation are conducted on newly
developed datasets of Hamiltonian vector fields, a key contribution of this
work. Our results demonstrate the model's effectiveness in accurately
recovering these symbolic expressions, advancing automated discovery in
Hamiltonian mechanics.

</details>


### [15] [Few-shot Learning on AMS Circuits and Its Application to Parasitic Capacitance Prediction](https://arxiv.org/abs/2507.06538)
*Shan Shen,Yibin Zhang,Hector Rodriguez Rodriguez,Wenjian Yu*

Main category: cs.LG

TL;DR: CircuitGPS是一种用于AMS电路中寄生效应预测的小样本学习方法，通过异构图表示电路网表，结合小跳采样和混合图Transformer，显著提高了预测精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 由于集成电路设计数据的稀缺性，训练深度学习模型用于AMS设计受到严重限制，因此需要一种高效的小样本学习方法。

Method: CircuitGPS将电路网表表示为异构图，利用小跳采样技术将链接或节点转换为子图，并通过混合图Transformer学习子图嵌入，同时整合低成本的位姿编码。

Result: CircuitGPS将耦合存在的准确性提高了至少20%，并将电容估计的MAE降低了至少0.067，同时展示了强大的零样本学习能力。

Conclusion: CircuitGPS为AMS电路设计提供了一种高效且可扩展的解决方案，并通过消融研究为图表示学习提供了有价值的见解。

Abstract: Graph representation learning is a powerful method to extract features from
graph-structured data, such as analog/mixed-signal (AMS) circuits. However,
training deep learning models for AMS designs is severely limited by the
scarcity of integrated circuit design data. In this work, we present
CircuitGPS, a few-shot learning method for parasitic effect prediction in AMS
circuits. The circuit netlist is represented as a heterogeneous graph, with the
coupling capacitance modeled as a link. CircuitGPS is pre-trained on link
prediction and fine-tuned on edge regression. The proposed method starts with a
small-hop sampling technique that converts a link or a node into a subgraph.
Then, the subgraph embeddings are learned with a hybrid graph Transformer.
Additionally, CircuitGPS integrates a low-cost positional encoding that
summarizes the positional and structural information of the sampled subgraph.
CircuitGPS improves the accuracy of coupling existence by at least 20\% and
reduces the MAE of capacitance estimation by at least 0.067 compared to
existing methods. Our method demonstrates strong inherent scalability, enabling
direct application to diverse AMS circuit designs through zero-shot learning.
Furthermore, the ablation studies provide valuable insights into graph models
for representation learning.

</details>


### [16] [DecoyDB: A Dataset for Graph Contrastive Learning in Protein-Ligand Binding Affinity Prediction](https://arxiv.org/abs/2507.06366)
*Yupu Zhang,Zelin Xu,Tingsong Xiao,Gustavo Seabra,Yanjun Li,Chenglong Li,Zhe Jiang*

Main category: cs.LG

TL;DR: 论文提出DecoyDB数据集和定制化GCL框架，用于蛋白质-配体复合物的自监督学习，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 蛋白质-配体结合亲和力预测在药物发现中至关重要，但缺乏大规模高质量标记数据限制了进展。

Method: 构建DecoyDB数据集（包含真实复合物和计算生成的诱饵结构），并设计定制化GCL框架进行预训练和微调。

Result: 实验表明，基于DecoyDB预训练的模型在准确性、标签效率和泛化性上表现优越。

Conclusion: DecoyDB和定制化GCL框架为蛋白质-配体复合物的自监督学习提供了有效解决方案。

Abstract: Predicting the binding affinity of protein-ligand complexes plays a vital
role in drug discovery. Unfortunately, progress has been hindered by the lack
of large-scale and high-quality binding affinity labels. The widely used
PDBbind dataset has fewer than 20K labeled complexes. Self-supervised learning,
especially graph contrastive learning (GCL), provides a unique opportunity to
break the barrier by pre-training graph neural network models based on vast
unlabeled complexes and fine-tuning the models on much fewer labeled complexes.
However, the problem faces unique challenges, including a lack of a
comprehensive unlabeled dataset with well-defined positive/negative complex
pairs and the need to design GCL algorithms that incorporate the unique
characteristics of such data. To fill the gap, we propose DecoyDB, a
large-scale, structure-aware dataset specifically designed for self-supervised
GCL on protein-ligand complexes. DecoyDB consists of high-resolution ground
truth complexes (less than 2.5 Angstrom) and diverse decoy structures with
computationally generated binding poses that range from realistic to suboptimal
(negative pairs). Each decoy is annotated with a Root Mean Squared Deviation
(RMSD) from the native pose. We further design a customized GCL framework to
pre-train graph neural networks based on DecoyDB and fine-tune the models with
labels from PDBbind. Extensive experiments confirm that models pre-trained with
DecoyDB achieve superior accuracy, label efficiency, and generalizability.

</details>


### [17] [Deep-Learning-Based Pre-Layout Parasitic Capacitance Prediction on SRAM Designs](https://arxiv.org/abs/2507.06549)
*Shan Shen,Dingcheng Yang,Yuyang Xie,Chunyan Pei,Wenjian Yu,Bei Yu*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的2阶段模型，用于在预布局阶段准确预测SRAM电路中的寄生效应，显著减少误差并加速仿真过程。


<details>
  <summary>Details</summary>
Motivation: SRAM在SoC中的定制化设计常因寄生效应导致预布局与后布局仿真差异大，设计迭代多，需一种方法在预布局阶段准确预测寄生效应。

Method: 结合图神经网络（GNN）分类器和多层感知机（MLP）回归器，采用Focal Loss处理类别不平衡，并整合子电路信息以抽象层次结构。

Result: 在4个实际SRAM设计中，模型误差最大减少19倍，仿真速度提升高达598倍。

Conclusion: 该方法显著提升了寄生效应预测的准确性和仿真效率，为SRAM设计提供了有力工具。

Abstract: To achieve higher system energy efficiency, SRAM in SoCs is often customized.
The parasitic effects cause notable discrepancies between pre-layout and
post-layout circuit simulations, leading to difficulty in converging design
parameters and excessive design iterations. Is it possible to well predict the
parasitics based on the pre-layout circuit, so as to perform parasitic-aware
pre-layout simulation? In this work, we propose a deep-learning-based 2-stage
model to accurately predict these parasitics in pre-layout stages. The model
combines a Graph Neural Network (GNN) classifier and Multi-Layer Perceptron
(MLP) regressors, effectively managing class imbalance of the net parasitics in
SRAM circuits. We also employ Focal Loss to mitigate the impact of abundant
internal net samples and integrate subcircuit information into the graph to
abstract the hierarchical structure of schematics. Experiments on 4 real SRAM
designs show that our approach not only surpasses the state-of-the-art model in
parasitic prediction by a maximum of 19X reduction of error but also
significantly boosts the simulation process by up to 598X speedup.

</details>


### [18] [Instance-Wise Monotonic Calibration by Constrained Transformation](https://arxiv.org/abs/2507.06516)
*Yunrui Zhang,Gustavo Batista,Salil S. Kanhere*

Main category: cs.LG

TL;DR: 提出了一种新的单调后校准方法，通过约束优化确保概率输出的排序，同时保持表达能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络常产生校准不良的概率估计，现有方法无法保证单调性或缺乏可解释性。

Method: 使用线性参数化的约束校准映射，通过约束优化问题确保单调性。

Result: 在多个数据集和模型上达到最先进性能，优于现有方法且高效。

Conclusion: 该方法在保持单调性的同时，兼具表达能力、鲁棒性和可解释性。

Abstract: Deep neural networks often produce miscalibrated probability estimates,
leading to overconfident predictions. A common approach for calibration is
fitting a post-hoc calibration map on unseen validation data that transforms
predicted probabilities. A key desirable property of the calibration map is
instance-wise monotonicity (i.e., preserving the ranking of probability
outputs). However, most existing post-hoc calibration methods do not guarantee
monotonicity. Previous monotonic approaches either use an under-parameterized
calibration map with limited expressive ability or rely on black-box neural
networks, which lack interpretability and robustness. In this paper, we propose
a family of novel monotonic post-hoc calibration methods, which employs a
constrained calibration map parameterized linearly with respect to the number
of classes. Our proposed approach ensures expressiveness, robustness, and
interpretability while preserving the relative ordering of the probability
output by formulating the proposed calibration map as a constrained
optimization problem. Our proposed methods achieve state-of-the-art performance
across datasets with different deep neural network models, outperforming
existing calibration methods while being data and computation-efficient. Our
code is available at
https://github.com/YunruiZhang/Calibration-by-Constrained-Transformation

</details>


### [19] [The Riemannian Geometry associated to Gradient Flows of Linear Convolutional Networks](https://arxiv.org/abs/2507.06367)
*El Mehdi Achour,Kathlén Kohn,Holger Rauhut*

Main category: cs.LG

TL;DR: 论文研究了深度线性卷积网络梯度流的几何性质，发现参数空间的梯度流可以表示为函数空间上的黎曼梯度流，且不受初始化条件限制。


<details>
  <summary>Details</summary>
Motivation: 探索深度线性卷积网络在梯度流下的几何特性，特别是参数空间与函数空间的关系。

Method: 通过分析线性卷积网络的梯度流，证明其可以表示为函数空间上的黎曼梯度流，适用于多维卷积和特定步长条件。

Result: 梯度流在参数空间可表示为函数空间的黎曼梯度流，且不受初始化条件限制，适用于D≥2维卷积或步长大于1的一维卷积。

Conclusion: 该研究为理解深度线性卷积网络的优化过程提供了新的几何视角，扩展了现有理论。

Abstract: We study geometric properties of the gradient flow for learning deep linear
convolutional networks. For linear fully connected networks, it has been shown
recently that the corresponding gradient flow on parameter space can be written
as a Riemannian gradient flow on function space (i.e., on the product of weight
matrices) if the initialization satisfies a so-called balancedness condition.
We establish that the gradient flow on parameter space for learning linear
convolutional networks can be written as a Riemannian gradient flow on function
space regardless of the initialization. This result holds for $D$-dimensional
convolutions with $D \geq 2$, and for $D =1$ it holds if all so-called strides
of the convolutions are greater than one. The corresponding Riemannian metric
depends on the initialization.

</details>


### [20] [Heterogeneous Graph Neural Networks for Short-term State Forecasting in Power Systems across Domains and Time Scales: A Hydroelectric Power Plant Case Study](https://arxiv.org/abs/2507.06694)
*Raffael Theiler,Olga Fink*

Main category: cs.LG

TL;DR: 论文提出了一种基于异构图注意力网络的方法，用于多领域电力系统状态预测，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现代电力系统因可再生能源和分布式能源的引入而变得复杂，需要可靠的短期状态预测以确保稳定运行。

Method: 使用异构图注意力网络（Heterogeneous Graph Attention Networks）建模同质和异质传感器关系，整合液压和电气领域数据。

Result: 实验表明，该方法在归一化均方根误差上平均优于传统基线35.5%。

Conclusion: 该方法在多领域、多速率电力系统状态预测中表现出色，解决了异质数据整合的挑战。

Abstract: Accurate short-term state forecasting is essential for efficient and stable
operation of modern power systems, especially in the context of increasing
variability introduced by renewable and distributed energy resources. As these
systems evolve rapidly, it becomes increasingly important to reliably predict
their states in the short term to ensure operational stability, support control
decisions, and enable interpretable monitoring of sensor and machine behavior.
Modern power systems often span multiple physical domains - including
electrical, mechanical, hydraulic, and thermal - posing significant challenges
for modeling and prediction. Graph Neural Networks (GNNs) have emerged as a
promising data-driven framework for system state estimation and state
forecasting in such settings. By leveraging the topological structure of sensor
networks, GNNs can implicitly learn inter-sensor relationships and propagate
information across the network. However, most existing GNN-based methods are
designed under the assumption of homogeneous sensor relationships and are
typically constrained to a single physical domain. This limitation restricts
their ability to integrate and reason over heterogeneous sensor data commonly
encountered in real-world energy systems, such as those used in energy
conversion infrastructure. In this work, we propose the use of Heterogeneous
Graph Attention Networks to address these limitations. Our approach models both
homogeneous intra-domain and heterogeneous inter-domain relationships among
sensor data from two distinct physical domains - hydraulic and electrical -
which exhibit fundamentally different temporal dynamics. Experimental results
demonstrate that our method significantly outperforms conventional baselines on
average by 35.5% in terms of normalized root mean square error, confirming its
effectiveness in multi-domain, multi-rate power system state forecasting.

</details>


### [21] [AdaDPIGU: Differentially Private SGD with Adaptive Clipping and Importance-Based Gradient Updates for Deep Neural Networks](https://arxiv.org/abs/2507.06525)
*Huiqi Zhang,Fang Xie*

Main category: cs.LG

TL;DR: 提出了一种新的差分隐私SGD框架AdaDPIGU，通过重要性梯度更新和自适应剪枝机制，在高维设置下提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私方法在高维设置中因噪声增加导致性能下降，需改进。

Method: 预训练阶段使用差分隐私高斯机制估计参数重要性，梯度更新阶段剪枝低重要性坐标并引入自适应裁剪机制。

Result: 在MNIST和CIFAR-10上表现优异，隐私预算下接近或超越非私有模型。

Conclusion: AdaDPIGU在保持隐私的同时提升了模型性能，证明了自适应稀疏化的有效性。

Abstract: Differential privacy has been proven effective for stochastic gradient
descent; however, existing methods often suffer from performance degradation in
high-dimensional settings, as the scale of injected noise increases with
dimensionality. To tackle this challenge, we propose AdaDPIGU--a new
differentially private SGD framework with importance-based gradient updates
tailored for deep neural networks. In the pretraining stage, we apply a
differentially private Gaussian mechanism to estimate the importance of each
parameter while preserving privacy. During the gradient update phase, we prune
low-importance coordinates and introduce a coordinate-wise adaptive clipping
mechanism, enabling sparse and noise-efficient gradient updates. Theoretically,
we prove that AdaDPIGU satisfies $(\varepsilon, \delta)$-differential privacy
and retains convergence guarantees. Extensive experiments on standard
benchmarks validate the effectiveness of AdaDPIGU. All results are reported
under a fixed retention ratio of 60%. On MNIST, our method achieves a test
accuracy of 99.12% under a privacy budget of $\epsilon = 8$, nearly matching
the non-private model. Remarkably, on CIFAR-10, it attains 73.21% accuracy at
$\epsilon = 4$, outperforming the non-private baseline of 71.12%, demonstrating
that adaptive sparsification can enhance both privacy and utility.

</details>


### [22] [Secure and Storage-Efficient Deep Learning Models for Edge AI Using Automatic Weight Generation](https://arxiv.org/abs/2507.06380)
*Habibur Rahaman,Atri Chatterjee,Swarup Bhunia*

Main category: cs.LG

TL;DR: WINGs框架通过动态生成权重和压缩存储，显著减少神经网络的内存需求，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决复杂神经网络因存储大量权重而占用过多内存的问题。

Method: 使用PCA降维和轻量级SVR模型预测权重，并结合敏感性分析压缩CNN低敏感性层权重。

Result: FC层压缩53倍，AlexNet在MNIST上压缩28倍，CIFAR-10上压缩18倍，精度损失1-2%。

Conclusion: WINGs显著降低内存需求，提升推理效率，适合资源受限的边缘应用。

Abstract: Complex neural networks require substantial memory to store a large number of
synaptic weights. This work introduces WINGs (Automatic Weight Generator for
Secure and Storage-Efficient Deep Learning Models), a novel framework that
dynamically generates layer weights in a fully connected neural network (FC)
and compresses the weights in convolutional neural networks (CNNs) during
inference, significantly reducing memory requirements without sacrificing
accuracy. WINGs framework uses principal component analysis (PCA) for
dimensionality reduction and lightweight support vector regression (SVR) models
to predict layer weights in the FC networks, removing the need for storing
full-weight matrices and achieving substantial memory savings. It also
preferentially compresses the weights in low-sensitivity layers of CNNs using
PCA and SVR with sensitivity analysis. The sensitivity-aware design also offers
an added level of security, as any bit-flip attack with weights in compressed
layers has an amplified and readily detectable effect on accuracy. WINGs
achieves 53x compression for the FC layers and 28x for AlexNet with MNIST
dataset, and 18x for Alexnet with CIFAR-10 dataset with 1-2% accuracy loss.
This significant reduction in memory results in higher throughput and lower
energy for DNN inference, making it attractive for resource-constrained edge
applications.

</details>


### [23] [A Single Merging Suffices: Recovering Server-based Learning Performance in Decentralized Learning](https://arxiv.org/abs/2507.06542)
*Tongtian Zhu,Tianyu Zhang,Mingze Wang,Zhanpeng Zhou,Can Wang*

Main category: cs.LG

TL;DR: 研究去中心化学习中通信调度对性能的影响，发现后期集中通信预算和最终全局合并能显著提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习因通信受限而性能受限，探索如何优化通信调度以提升性能。

Method: 通过实验和理论分析，研究通信时机和频率对训练的影响，提出后期集中通信和全局合并策略。

Result: 后期集中通信和最终全局合并能匹配服务器训练性能，低通信保持模型可合并性。

Conclusion: 去中心化学习在数据异构和通信受限下仍能表现优异，挑战了传统认知，为模型合并和损失景观提供新见解。

Abstract: Decentralized learning provides a scalable alternative to traditional
parameter-server-based training, yet its performance is often hindered by
limited peer-to-peer communication. In this paper, we study how communication
should be scheduled over time, including determining when and how frequently
devices synchronize. Our empirical results show that concentrating
communication budgets in the later stages of decentralized training markedly
improves global generalization. Surprisingly, we uncover that fully connected
communication at the final step, implemented by a single global merging, is
sufficient to match the performance of server-based training. We further show
that low communication in decentralized learning preserves the
\textit{mergeability} of local models throughout training. Our theoretical
contributions, which explains these phenomena, are first to establish that the
globally merged model of decentralized SGD can converge faster than centralized
mini-batch SGD. Technically, we novelly reinterpret part of the discrepancy
among local models, which were previously considered as detrimental noise, as
constructive components that accelerate convergence. This work challenges the
common belief that decentralized learning generalizes poorly under data
heterogeneity and limited communication, while offering new insights into model
merging and neural network loss landscapes.

</details>


### [24] [KPFlow: An Operator Perspective on Dynamic Collapse Under Gradient Descent Training of Recurrent Networks](https://arxiv.org/abs/2507.06381)
*James Hazelden,Laura Driscoll,Eli Shlizerman,Eric Shea-Brown*

Main category: cs.LG

TL;DR: 论文提出了一种梯度流分解方法，通过参数算子K和线性化流传播器P，揭示了梯度下降在非线性循环模型中学习动态的机制。


<details>
  <summary>Details</summary>
Motivation: 理解梯度下降在循环动态系统（如RNNs、Neural ODEs）中如何塑造学习表示，尤其是有限非线性模型中的机制。

Method: 将梯度流分解为参数算子K和线性化流传播器P的乘积，分析其相互作用。

Result: 揭示了低维潜在动态的成因，并展示了多任务训练中目标对齐的测量方法。

Conclusion: 为非线性循环模型的梯度下降学习提供了新的理论工具，推动了对其机制的深入理解。

Abstract: Gradient Descent (GD) and its variants are the primary tool for enabling
efficient training of recurrent dynamical systems such as Recurrent Neural
Networks (RNNs), Neural ODEs and Gated Recurrent units (GRUs). The dynamics
that are formed in these models exhibit features such as neural collapse and
emergence of latent representations that may support the remarkable
generalization properties of networks. In neuroscience, qualitative features of
these representations are used to compare learning in biological and artificial
systems. Despite recent progress, there remains a need for theoretical tools to
rigorously understand the mechanisms shaping learned representations,
especially in finite, non-linear models. Here, we show that the gradient flow,
which describes how the model's dynamics evolve over GD, can be decomposed into
a product that involves two operators: a Parameter Operator, K, and a
Linearized Flow Propagator, P. K mirrors the Neural Tangent Kernel in
feed-forward neural networks, while P appears in Lyapunov stability and optimal
control theory. We demonstrate two applications of our decomposition. First, we
show how their interplay gives rise to low-dimensional latent dynamics under
GD, and, specifically, how the collapse is a result of the network structure,
over and above the nature of the underlying task. Second, for multi-task
training, we show that the operators can be used to measure how objectives
relevant to individual sub-tasks align. We experimentally and theoretically
validate these findings, providing an efficient Pytorch package, \emph{KPFlow},
implementing robust analysis tools for general recurrent architectures. Taken
together, our work moves towards building a next stage of understanding of GD
learning in non-linear recurrent models.

</details>


### [25] [Steps Adaptive Decay DPSGD: Enhancing Performance on Imbalanced Datasets with Differential Privacy with HAM10000](https://arxiv.org/abs/2507.06619)
*Xiaobo Huang,Fang Xie*

Main category: cs.LG

TL;DR: 提出SAD-DPSGD方法，通过线性衰减机制优化噪声和裁剪阈值，解决医疗图像分类中的数据泄漏问题，性能优于Auto-DPSGD。


<details>
  <summary>Details</summary>
Motivation: 医疗图像分类中数据泄漏问题严重，传统方法在小规模不平衡数据集（如HAM10000）上效果不佳，导致模型陷入次优解。

Method: SAD-DPSGD采用线性衰减机制，初始阶段分配更多隐私预算和更高裁剪阈值，避免信息丢失。

Result: 在HAM10000数据集上，SAD-DPSGD比Auto-DPSGD准确率提高2.15%（ε=3.0，δ=10^-3）。

Conclusion: SAD-DPSGD有效解决了不平衡医疗数据集上的隐私保护问题，提升了模型性能。

Abstract: When applying machine learning to medical image classification, data leakage
is a critical issue. Previous methods, such as adding noise to gradients for
differential privacy, work well on large datasets like MNIST and CIFAR-100, but
fail on small, imbalanced medical datasets like HAM10000. This is because the
imbalanced distribution causes gradients from minority classes to be clipped
and lose crucial information, while majority classes dominate. This leads the
model to fall into suboptimal solutions early. To address this, we propose
SAD-DPSGD, which uses a linear decaying mechanism for noise and clipping
thresholds. By allocating more privacy budget and using higher clipping
thresholds in the initial training phases, the model avoids suboptimal
solutions and enhances performance. Experiments show that SAD-DPSGD outperforms
Auto-DPSGD on HAM10000, improving accuracy by 2.15% under $\epsilon = 3.0$ ,
$\delta = 10^{-3}$.

</details>


### [26] [Detection of Intelligent Tampering in Wireless Electrocardiogram Signals Using Hybrid Machine Learning](https://arxiv.org/abs/2507.06402)
*Siddhant Deshpande,Yalemzerf Getnet,Waltenegus Dargie*

Main category: cs.LG

TL;DR: 论文分析了CNN、ResNet和混合Transformer-CNN模型在ECG信号篡改检测中的性能，并评估了Siamese网络在ECG身份验证中的表现。实验结果显示，模型在多种篡改场景下表现优异，最高准确率超过99.5%。


<details>
  <summary>Details</summary>
Motivation: 随着无线ECG系统在健康监测和身份验证中的普及，保护信号完整性免受篡改变得至关重要。

Method: 使用连续小波变换（CWT）将一维ECG信号转换为时频域二维表示，并训练CNN、ResNet和混合Transformer-CNN模型进行篡改检测。同时，评估Siamese网络在身份验证中的性能。

Result: 在高度碎片化的篡改场景中，模型的准确率超过99.5%。对于细微篡改，FeatCNN-TranCNN模型平均准确率为98%。身份验证中，纯Transformer-Siamese网络平均准确率为98.30%，而混合CNN-Transformer Siamese模型达到100%准确率。

Conclusion: 混合Transformer-CNN模型和Siamese网络在ECG信号篡改检测和身份验证中表现出色，具有实际应用潜力。

Abstract: With the proliferation of wireless electrocardiogram (ECG) systems for health
monitoring and authentication, protecting signal integrity against tampering is
becoming increasingly important. This paper analyzes the performance of CNN,
ResNet, and hybrid Transformer-CNN models for tamper detection. It also
evaluates the performance of a Siamese network for ECG based identity
verification. Six tampering strategies, including structured segment
substitutions and random insertions, are emulated to mimic real world attacks.
The one-dimensional ECG signals are transformed into a two dimensional
representation in the time frequency domain using the continuous wavelet
transform (CWT). The models are trained and evaluated using ECG data from 54
subjects recorded in four sessions 2019 to 2025 outside of clinical settings
while the subjects performed seven different daily activities. Experimental
results show that in highly fragmented manipulation scenarios, CNN,
FeatCNN-TranCNN, FeatCNN-Tran and ResNet models achieved an accuracy exceeding
99.5 percent . Similarly, for subtle manipulations (for example, 50 percent
from A and 50 percent from B and, 75 percent from A and 25 percent from B
substitutions) our FeatCNN-TranCNN model demonstrated consistently reliable
performance, achieving an average accuracy of 98 percent . For identity
verification, the pure Transformer-Siamese network achieved an average accuracy
of 98.30 percent . In contrast, the hybrid CNN-Transformer Siamese model
delivered perfect verification performance with 100 percent accuracy.

</details>


### [27] [Mathematical artificial data for operator learning](https://arxiv.org/abs/2507.06752)
*Heng Wu,Benzhuo Lu*

Main category: cs.LG

TL;DR: MAD框架结合物理定律与数据驱动学习，通过生成物理嵌入的解析解和合成数据，解决了传统方法对标记数据或效率-准确性权衡的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统方法在求解微分方程时面临数据驱动方法需要昂贵标记数据或模型驱动方法效率与准确性难以兼顾的问题。

Method: 提出MAD框架，利用微分方程的数学结构生成物理嵌入的解析解和合成数据，实现大规模算子发现。

Result: 在2D参数化问题中展示了MAD的泛化能力和高效/准确性，适用于复杂参数空间。

Conclusion: MAD框架有望成为科学计算中物理信息机器智能的通用范式。

Abstract: Machine learning has emerged as a transformative tool for solving
differential equations (DEs), yet prevailing methodologies remain constrained
by dual limitations: data-driven methods demand costly labeled datasets while
model-driven techniques face efficiency-accuracy trade-offs. We present the
Mathematical Artificial Data (MAD) framework, a new paradigm that integrates
physical laws with data-driven learning to facilitate large-scale operator
discovery. By exploiting DEs' intrinsic mathematical structure to generate
physics-embedded analytical solutions and associated synthetic data, MAD
fundamentally eliminates dependence on experimental or simulated training data.
This enables computationally efficient operator learning across multi-parameter
systems while maintaining mathematical rigor. Through numerical demonstrations
spanning 2D parametric problems where both the boundary values and source term
are functions, we showcase MAD's generalizability and superior
efficiency/accuracy across various DE scenarios. This
physics-embedded-data-driven framework and its capacity to handle complex
parameter spaces gives it the potential to become a universal paradigm for
physics-informed machine intelligence in scientific computing.

</details>


### [28] [Bridging Data Gaps of Rare Conditions in ICU: A Multi-Disease Adaptation Approach for Clinical Prediction](https://arxiv.org/abs/2507.06432)
*Mingcheng Zhu,Yu Liu,Zhiyao Luo,Tingting Zhu*

Main category: cs.LG

TL;DR: KnowRare是一个基于领域适应的深度学习框架，用于预测ICU中罕见疾病的临床结果，通过自监督预训练和条件知识图谱解决数据稀缺和异质性问题，表现优于现有模型和ICU评分系统。


<details>
  <summary>Details</summary>
Motivation: ICU中罕见疾病因数据稀缺和异质性而服务不足，需要开发新方法填补这一空白。

Method: KnowRare通过自监督预训练学习条件无关表示，并利用条件知识图谱选择性适应临床相似条件的知识。

Result: 在五个临床预测任务中，KnowRare表现优于现有模型和ICU评分系统（如APACHE IV），并展示了适应性和泛化能力。

Conclusion: KnowRare是支持ICU罕见疾病临床决策的实用解决方案，具有潜力和灵活性。

Abstract: Artificial Intelligence has revolutionised critical care for common
conditions. Yet, rare conditions in the intensive care unit (ICU), including
recognised rare diseases and low-prevalence conditions in the ICU, remain
underserved due to data scarcity and intra-condition heterogeneity. To bridge
such gaps, we developed KnowRare, a domain adaptation-based deep learning
framework for predicting clinical outcomes for rare conditions in the ICU.
KnowRare mitigates data scarcity by initially learning condition-agnostic
representations from diverse electronic health records through self-supervised
pre-training. It addresses intra-condition heterogeneity by selectively
adapting knowledge from clinically similar conditions with a developed
condition knowledge graph. Evaluated on two ICU datasets across five clinical
prediction tasks (90-day mortality, 30-day readmission, ICU mortality,
remaining length of stay, and phenotyping), KnowRare consistently outperformed
existing state-of-the-art models. Additionally, KnowRare demonstrated superior
predictive performance compared to established ICU scoring systems, including
APACHE IV and IV-a. Case studies further demonstrated KnowRare's flexibility in
adapting its parameters to accommodate dataset-specific and task-specific
characteristics, its generalisation to common conditions under limited data
scenarios, and its rationality in selecting source conditions. These findings
highlight KnowRare's potential as a robust and practical solution for
supporting clinical decision-making and improving care for rare conditions in
the ICU.

</details>


### [29] [Mutual Information Free Topological Generalization Bounds via Stability](https://arxiv.org/abs/2507.06775)
*Mario Tuci,Lennart Bastian,Benjamin Dupuis,Nassir Navab,Tolga Birdal,Umut Şimşekli*

Main category: cs.LG

TL;DR: 论文提出了一种新的拓扑泛化边界框架，避免了复杂的信息论项，通过轨迹稳定性证明泛化误差的上界。


<details>
  <summary>Details</summary>
Motivation: 现有拓扑泛化边界依赖复杂的信息论项，难以应用于实际算法（如ADAM），因此需要更直观且可解释的边界。

Method: 引入基于算法稳定性的新框架，扩展假设集稳定性为轨迹稳定性，结合拓扑数据分析（TDA）量化优化器轨迹复杂度。

Result: 实验表明TDA项对泛化误差边界至关重要，尤其在训练样本增加时，解释了拓扑泛化边界的实证成功。

Conclusion: 新框架提供了无信息论项的拓扑泛化边界，通过轨迹稳定性和TDA量化，增强了理论解释性和实用性。

Abstract: Providing generalization guarantees for stochastic optimization algorithms is
a major challenge in modern learning theory. Recently, several studies
highlighted the impact of the geometry of training trajectories on the
generalization error, both theoretically and empirically. Among these works, a
series of topological generalization bounds have been proposed, relating the
generalization error to notions of topological complexity that stem from
topological data analysis (TDA). Despite their empirical success, these bounds
rely on intricate information-theoretic (IT) terms that can be bounded in
specific cases but remain intractable for practical algorithms (such as ADAM),
potentially reducing the relevance of the derived bounds. In this paper, we
seek to formulate comprehensive and interpretable topological generalization
bounds free of intractable mutual information terms. To this end, we introduce
a novel learning theoretic framework that departs from the existing strategies
via proof techniques rooted in algorithmic stability. By extending an existing
notion of \textit{hypothesis set stability}, to \textit{trajectory stability},
we prove that the generalization error of trajectory-stable algorithms can be
upper bounded in terms of (i) TDA quantities describing the complexity of the
trajectory of the optimizer in the parameter space, and (ii) the trajectory
stability parameter of the algorithm. Through a series of experimental
evaluations, we demonstrate that the TDA terms in the bound are of great
importance, especially as the number of training samples grows. This ultimately
forms an explanation of the empirical success of the topological generalization
bounds.

</details>


### [30] [eegFloss: A Python package for refining sleep EEG recordings using machine learning models](https://arxiv.org/abs/2507.06433)
*Niloy Sikder,Paul Zerr,Mahdad Jafarzadeh Esfahani,Martin Dresler,Matthias Krauledat*

Main category: cs.LG

TL;DR: 论文介绍了eegFloss，一个开源的Python工具包，用于检测睡眠EEG记录中的伪迹，提高睡眠研究的准确性。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪迹干扰，影响自动睡眠分期的准确性，eegFloss旨在解决这一问题。

Method: 开发了eegUsability模型，基于15名参与者127晚的标记数据训练，用于检测伪迹。

Result: 模型表现良好（F1-score约0.85，Cohen's kappa 0.78），识别可用数据的召回率达94%。

Conclusion: eegFloss提升了睡眠研究的分析精度和结果可靠性。

Abstract: Electroencephalography (EEG) allows monitoring of brain activity, providing
insights into the functional dynamics of various brain regions and their roles
in cognitive processes. EEG is a cornerstone in sleep research, serving as the
primary modality of polysomnography, the gold standard in the field. However,
EEG signals are prone to artifacts caused by both internal (device-specific)
factors and external (environmental) interferences. As sleep studies are
becoming larger, most rely on automatic sleep staging, a process highly
susceptible to artifacts, leading to erroneous sleep scores. This paper
addresses this challenge by introducing eegFloss, an open-source Python package
to utilize eegUsability, a novel machine learning (ML) model designed to detect
segments with artifacts in sleep EEG recordings. eegUsability has been trained
and evaluated on manually artifact-labeled EEG data collected from 15
participants over 127 nights using the Zmax headband. It demonstrates solid
overall classification performance (F1-score is approximately 0.85, Cohens
kappa is 0.78), achieving a high recall rate of approximately 94% in
identifying channel-wise usable EEG data, and extends beyond Zmax.
Additionally, eegFloss offers features such as automatic time-in-bed detection
using another ML model named eegMobility, filtering out certain artifacts, and
generating hypnograms and sleep statistics. By addressing a fundamental
challenge faced by most sleep studies, eegFloss can enhance the precision and
rigor of their analysis as well as the accuracy and reliability of their
outcomes.

</details>


### [31] [Scalable Gaussian Processes: Advances in Iterative Methods and Pathwise Conditioning](https://arxiv.org/abs/2507.06839)
*Jihao Andreas Lin*

Main category: cs.LG

TL;DR: 该论文提出了一种结合迭代方法和路径条件化的方法，以提高高斯过程在大规模数据下的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在经典框架下难以适应大规模数据和现代硬件并行计算的需求，因此需要改进其可扩展性。

Method: 通过结合迭代线性系统求解器和路径条件化，将高计算成本的问题转化为线性方程组求解，减少内存需求。

Result: 该方法显著降低了内存需求，适用于更大规模的数据，并充分利用现代硬件的矩阵乘法优势。

Conclusion: 结合迭代方法和路径条件化是提升高斯过程在大规模场景下性能的有效途径。

Abstract: Gaussian processes are a powerful framework for uncertainty-aware function
approximation and sequential decision-making. Unfortunately, their classical
formulation does not scale gracefully to large amounts of data and modern
hardware for massively-parallel computation, prompting many researchers to
develop techniques which improve their scalability. This dissertation focuses
on the powerful combination of iterative methods and pathwise conditioning to
develop methodological contributions which facilitate the use of Gaussian
processes in modern large-scale settings. By combining these two techniques
synergistically, expensive computations are expressed as solutions to systems
of linear equations and obtained by leveraging iterative linear system solvers.
This drastically reduces memory requirements, facilitating application to
significantly larger amounts of data, and introduces matrix multiplication as
the main computational operation, which is ideal for modern hardware.

</details>


### [32] [Can Interpretation Predict Behavior on Unseen Data?](https://arxiv.org/abs/2507.06445)
*Victoria R. Li,Jenny Kaufmann,Martin Wattenberg,David Alvarez-Melis,Naomi Saphra*

Main category: cs.LG

TL;DR: 论文探讨了可解释性研究如何预测模型在未见数据上的表现，发现注意力模式与OOD泛化能力相关，尤其是分层注意力模式能预测分层泛化行为。


<details>
  <summary>Details</summary>
Motivation: 当前可解释性研究多关注干预机制下的模型行为，但很少预测模型在未见数据（OOD）上的表现。本文旨在填补这一空白，探索可解释性工具在预测OOD行为中的潜力。

Method: 研究通过训练数百个Transformer模型在合成分类任务上，分析其注意力模式与OOD泛化能力的相关性，并利用可解释性工具进行预测。

Result: 研究发现，当模型在分布内数据中表现出分层注意力模式时，其在OOD数据上更可能实现分层泛化，即使其实现机制不依赖这些模式。

Conclusion: 研究为可解释性工具预测模型未见行为提供了概念验证，鼓励进一步探索可解释性在OOD预测中的应用。

Abstract: Interpretability research often aims to predict how a model will respond to
targeted interventions on specific mechanisms. However, it rarely predicts how
a model will respond to unseen input data. This paper explores the promises and
challenges of interpretability as a tool for predicting out-of-distribution
(OOD) model behavior. Specifically, we investigate the correspondence between
attention patterns and OOD generalization in hundreds of Transformer models
independently trained on a synthetic classification task. These models exhibit
several distinct systematic generalization rules OOD, forming a diverse
population for correlational analysis. In this setting, we find that simple
observational tools from interpretability can predict OOD performance. In
particular, when in-distribution attention exhibits hierarchical patterns, the
model is likely to generalize hierarchically on OOD data -- even when the
rule's implementation does not rely on these hierarchical patterns, according
to ablation tests. Our findings offer a proof-of-concept to motivate further
interpretability work on predicting unseen model behavior.

</details>


### [33] [DICE: Data Influence Cascade in Decentralized Learning](https://arxiv.org/abs/2507.06931)
*Tongtian Zhu,Wenhao Li,Can Wang,Fengxiang He*

Main category: cs.LG

TL;DR: 论文提出了一种名为DICE的方法，用于在去中心化网络中估计数据影响力传播，以解决激励机制中公平贡献分配的问题。


<details>
  <summary>Details</summary>
Motivation: 去中心化学习缺乏有效的激励机制，阻碍了参与积极性，公平贡献分配是关键挑战。

Method: 设计了DICE方法，通过理论推导近似计算数据影响力在多跳邻居中的传播，考虑数据、通信拓扑和损失曲率。

Result: DICE为选择合适合作者和识别恶意行为提供了理论基础。

Conclusion: DICE是首个在去中心化环境中估计数据影响力传播的方法，具有实际应用潜力。

Abstract: Decentralized learning offers a promising approach to crowdsource data
consumptions and computational workloads across geographically distributed
compute interconnected through peer-to-peer networks, accommodating the
exponentially increasing demands. However, proper incentives are still in
absence, considerably discouraging participation. Our vision is that a fair
incentive mechanism relies on fair attribution of contributions to
participating nodes, which faces non-trivial challenges arising from the
localized connections making influence ``cascade'' in a decentralized network.
To overcome this, we design the first method to estimate \textbf{D}ata
\textbf{I}nfluence \textbf{C}ascad\textbf{E} (DICE) in a decentralized
environment. Theoretically, the framework derives tractable approximations of
influence cascade over arbitrary neighbor hops, suggesting the influence
cascade is determined by an interplay of data, communication topology, and the
curvature of loss landscape. DICE also lays the foundations for applications
including selecting suitable collaborators and identifying malicious behaviors.
Project page is available at https://raiden-zhu.github.io/blog/2025/DICE/.

</details>


### [34] [FedPhD: Federated Pruning with Hierarchical Learning of Diffusion Models](https://arxiv.org/abs/2507.06449)
*Qianyu Long,Qiyuan Wang,Christos Anagnostopoulos,Daning Bi*

Main category: cs.LG

TL;DR: FedPhD是一种新颖的联邦学习方法，用于高效训练扩散模型（DMs），通过分层联邦学习和同质性感知模型聚合解决数据异质性和高通信成本问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）在联邦学习（FL）环境中训练时面临数据异质性和高通信成本的挑战，现有研究对此关注不足。

Method: FedPhD采用分层联邦学习，结合同质性感知模型聚合和选择策略，并通过分布式结构化剪枝提升计算效率和减少存储需求。

Result: 实验表明，FedPhD在FID分数上表现优异，通信成本降低88%，计算和通信资源仅需56%，FID改进至少34%。

Conclusion: FedPhD在联邦学习环境中高效训练扩散模型，显著提升性能并降低资源消耗。

Abstract: Federated Learning (FL), as a distributed learning paradigm, trains models
over distributed clients' data. FL is particularly beneficial for distributed
training of Diffusion Models (DMs), which are high-quality image generators
that require diverse data. However, challenges such as high communication costs
and data heterogeneity persist in training DMs similar to training Transformers
and Convolutional Neural Networks. Limited research has addressed these issues
in FL environments. To address this gap and challenges, we introduce a novel
approach, FedPhD, designed to efficiently train DMs in FL environments. FedPhD
leverages Hierarchical FL with homogeneity-aware model aggregation and
selection policy to tackle data heterogeneity while reducing communication
costs. The distributed structured pruning of FedPhD enhances computational
efficiency and reduces model storage requirements in clients. Our experiments
across multiple datasets demonstrate that FedPhD achieves high model
performance regarding Fr\'echet Inception Distance (FID) scores while reducing
communication costs by up to $88\%$. FedPhD outperforms baseline methods
achieving at least a $34\%$ improvement in FID, while utilizing only $56\%$ of
the total computation and communication resources.

</details>


### [35] [Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy](https://arxiv.org/abs/2507.06969)
*Bogdan Kulynych,Juan Felipe Gomez,Georgios Kaissis,Jamie Hayes,Borja Balle,Flavio du Pin Calmon,Jean Louis Raisaro*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Differentially private (DP) mechanisms are difficult to interpret and
calibrate because existing methods for mapping standard privacy parameters to
concrete privacy risks -- re-identification, attribute inference, and data
reconstruction -- are both overly pessimistic and inconsistent. In this work,
we use the hypothesis-testing interpretation of DP ($f$-DP), and determine that
bounds on attack success can take the same unified form across
re-identification, attribute inference, and data reconstruction risks. Our
unified bounds are (1) consistent across a multitude of attack settings, and
(2) tunable, enabling practitioners to evaluate risk with respect to arbitrary
(including worst-case) levels of baseline risk. Empirically, our results are
tighter than prior methods using $\varepsilon$-DP, R\'enyi DP, and concentrated
DP. As a result, calibrating noise using our bounds can reduce the required
noise by 20% at the same risk level, which yields, e.g., more than 15pp
accuracy increase in a text classification task. Overall, this unifying
perspective provides a principled framework for interpreting and calibrating
the degree of protection in DP against specific levels of re-identification,
attribute inference, or data reconstruction risk.

</details>


### [36] [Automated Neuron Labelling Enables Generative Steering and Interpretability in Protein Language Models](https://arxiv.org/abs/2507.06458)
*Arjun Banerjee,David Martinez,Camille Dang,Ethan Tam*

Main category: cs.LG

TL;DR: 本文提出了一种自动化框架，用于为蛋白质语言模型（PLM）中的每个神经元标注生物学基础的自然语言描述，并开发了一种基于神经元激活的引导方法，以生成具有目标特性的蛋白质。


<details>
  <summary>Details</summary>
Motivation: 理解PLM内部神经元表示的生物学意义，并利用这些信息指导蛋白质设计。

Method: 引入自动化框架标注神经元，开发神经元激活引导方法生成目标蛋白质。

Result: 揭示了神经元对多样生化特性的选择性敏感，实现了目标生化特性和结构基序的蛋白质生成。

Conclusion: 通过分析不同模型大小的神经元，揭示了PLM的缩放规律和神经元空间分布的结构化特征。

Abstract: Protein language models (PLMs) encode rich biological information, yet their
internal neuron representations are poorly understood. We introduce the first
automated framework for labeling every neuron in a PLM with biologically
grounded natural language descriptions. Unlike prior approaches relying on
sparse autoencoders or manual annotation, our method scales to hundreds of
thousands of neurons, revealing individual neurons are selectively sensitive to
diverse biochemical and structural properties. We then develop a novel neuron
activation-guided steering method to generate proteins with desired traits,
enabling convergence to target biochemical properties like molecular weight and
instability index as well as secondary and tertiary structural motifs,
including alpha helices and canonical Zinc Fingers. We finally show that
analysis of labeled neurons in different model sizes reveals PLM scaling laws
and a structured neuron space distribution.

</details>


### [37] [Energy-Efficient Supervised Learning with a Binary Stochastic Forward-Forward Algorithm](https://arxiv.org/abs/2507.06461)
*Risi Jaiswal,Supriyo Datta,Joseph G. Makin*

Main category: cs.LG

TL;DR: 论文提出了一种基于前向-前向算法的二进制随机单元训练方法，以减少能量消耗，并在硬件上高效执行。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习中，大规模神经网络的能量消耗问题日益突出，反向传播算法在硬件加速器上存在挑战。

Method: 采用前向-前向算法结合二进制随机单元，将矩阵乘法转化为索引操作，并利用p-bits实现快速采样。

Result: 在MNIST、Fashion-MNIST和CIFAR-10数据集上，性能接近实值前向-前向算法，能量消耗降低约一个数量级。

Conclusion: 该方法为减少机器学习能量消耗提供了有效途径，尤其在硬件实现上具有潜力。

Abstract: Reducing energy consumption has become a pressing need for modern machine
learning, which has achieved many of its most impressive results by scaling to
larger and more energy-consumptive neural networks. Unfortunately, the main
algorithm for training such networks, backpropagation, poses significant
challenges for custom hardware accelerators, due to both its serial
dependencies and the memory footprint needed to store forward activations for
the backward pass. Alternatives to backprop, although less effective, do exist;
here the main computational bottleneck becomes matrix multiplication. In this
study, we derive forward-forward algorithms for binary, stochastic units.
Binarization of the activations transforms matrix multiplications into indexing
operations, which can be executed efficiently in hardware. Stochasticity,
combined with tied weights across units with different biases, bypasses the
information bottleneck imposed by binary units. Furthermore, although slow and
expensive in traditional hardware, binary sampling that is very fast can be
implemented cheaply with p-bits (probabilistic bits), novel devices made up of
unstable magnets. We evaluate our proposed algorithms on the MNIST,
Fashion-MNIST, and CIFAR-10 datasets, showing that its performance is close to
real-valued forward-forward, but with an estimated energy savings of about one
order of magnitude.

</details>


### [38] [On-Device Training of PV Power Forecasting Models in a Smart Meter for Grid Edge Intelligence](https://arxiv.org/abs/2507.07016)
*Jian Huang,Yongli Zhu,Linna Xu,Zhe Zheng,Wenpeng Cui,Mingyang Sun*

Main category: cs.LG

TL;DR: 研究在资源有限的智能电表上进行边缘端模型训练的可行性，提出混合和降低精度训练方案，并通过光伏功率预测案例验证其经济性。


<details>
  <summary>Details</summary>
Motivation: 推动电网边缘智能化和设备端训练的概念，以适应资源受限的环境。

Method: 描述设备端训练的技术准备步骤，采用梯度提升树和循环神经网络模型，设计混合和降低精度训练方案。

Result: 实验证明通过现有高级计量基础设施经济地实现电网边缘智能是可行的。

Conclusion: 资源受限设备上的边缘端模型训练是可行的，为电网智能化提供了经济高效的解决方案。

Abstract: In this paper, an edge-side model training study is conducted on a
resource-limited smart meter. The motivation of grid-edge intelligence and the
concept of on-device training are introduced. Then, the technical preparation
steps for on-device training are described. A case study on the task of
photovoltaic power forecasting is presented, where two representative machine
learning models are investigated: a gradient boosting tree model and a
recurrent neural network model. To adapt to the resource-limited situation in
the smart meter, "mixed"- and "reduced"-precision training schemes are also
devised. Experiment results demonstrate the feasibility of economically
achieving grid-edge intelligence via the existing advanced metering
infrastructures.

</details>


### [39] [SoftSignSGD(S3): An Enhanced Optimizer for Practical DNN Training and Loss Spikes Minimization Beyond Adam](https://arxiv.org/abs/2507.06464)
*Hanyang Peng,Shuang Qin,Yue Yu,Fangqing Jiang,Hui Wang,Wen Gao*

Main category: cs.LG

TL;DR: 论文提出了一种名为SignSoftSGD（S3）的新型优化器，通过改进Adam的更新机制，显著提升了训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: Adam在训练深度神经网络中表现优异，但其成功机制和局限性尚未充分研究。本文旨在增强Adam的优势并解决其不稳定性问题。

Method: S3采用三方面创新：1）使用p阶动量替代传统二阶动量；2）统一分子分母的指数移动平均系数以减少损失尖峰；3）引入Nesterov加速梯度模块。

Result: S3在非凸随机优化中达到最优收敛率，实验显示其收敛更快、性能更优，且能避免损失尖峰。

Conclusion: S3在效率和最终任务性能上均优于AdamW，尤其在处理大学习率时表现突出。

Abstract: Adam has proven remarkable successful in training deep neural networks, but
the mechanisms underlying its empirical successes and limitations remain
underexplored. In this study, we demonstrate that the effectiveness of Adam
stems largely from its similarity to SignSGD in robustly handling large
gradient fluctuations, yet it is also vulnerable to destabilizing loss spikes
due to its uncontrolled update scaling. To enhance the advantage of Adam and
mitigate its limitation, we propose SignSoftSGD (S3), a novel optimizer with
three key innovations. \emph{First}, S3 generalizes the sign-like update by
employing a flexible $p$-th order momentum ($p \geq 1$) in the denominator,
departing from the conventional second-order momentum (variance)
preconditioning. This design enables enhanced performance while achieving
stable training even with aggressive learning rates. \emph{Second}, S3
minimizes the occurrences of loss spikes through unified exponential moving
average coefficients for numerator and denominator momenta, which inherently
bound updates to $[-1, 1]$ and simplify hyperparameter tuning. \emph{Third}, S3
incorporates an equivalent Nesterov's accelerated gradient(NAG) module,
accelerating convergence without memory overhead. Theoretically, we prove that
S3 achieves the optimal convergence rate of
$O\left(\frac{1}{T^{\sfrac{1}{4}}}\right)$ for general nonconvex stochastic
optimization under weak assumptions. Extensive experiments across a range of
vision and language tasks show that \textsf{\small S3} not only converges more
rapidly and improves performance but also rarely experiences loss spikes, even
with a \textbf{$\bm{10 \times}$} larger learning rate. In fact, S3 delivers
performance comparable to or better than AdamW with \textbf{$2 \times$} the
training steps, establishing its efficacy in both efficiency and final task
performance.

</details>


### [40] [Self-Supervised Learning at the Edge: The Cost of Labeling](https://arxiv.org/abs/2507.07033)
*Roberto Pereira,Fernanda Famá,Asal Rangrazi,Marco Miozzo,Charalampos Kalalas,Paolo Dini*

Main category: cs.LG

TL;DR: 对比学习（CL）和自监督学习（SSL）方法在资源受限的边缘设备上部署时面临挑战。本文研究了SSL技术在边缘学习中的可行性和效率，分析了不同SSL技术在有限资源下的表现，并通过实验证明定制化SSL策略能显著降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法需要大量标注数据，而CL和SSL方法虽能从无标签数据中学习，但对计算资源和数据量的需求较高，限制了其在边缘设备上的应用。

Method: 研究分析了不同SSL技术在有限计算、数据和能源预算下的适应性，评估了其在资源受限环境中学习鲁棒表征的效果，并探讨了半监督学习在降低CL模型训练能耗中的作用。

Result: 实验表明，定制化的SSL策略能在保持竞争力的性能同时，将资源消耗降低多达4倍。

Conclusion: SSL技术在边缘设备上具有高效学习的潜力，尤其是在资源受限的环境中，通过优化策略可以实现性能与能耗的平衡。

Abstract: Contrastive learning (CL) has recently emerged as an alternative to
traditional supervised machine learning solutions by enabling rich
representations from unstructured and unlabeled data. However, CL and, more
broadly, self-supervised learning (SSL) methods often demand a large amount of
data and computational resources, posing challenges for deployment on
resource-constrained edge devices. In this work, we explore the feasibility and
efficiency of SSL techniques for edge-based learning, focusing on trade-offs
between model performance and energy efficiency. In particular, we analyze how
different SSL techniques adapt to limited computational, data, and energy
budgets, evaluating their effectiveness in learning robust representations
under resource-constrained settings. Moreover, we also consider the energy
costs involved in labeling data and assess how semi-supervised learning may
assist in reducing the overall energy consumed to train CL models. Through
extensive experiments, we demonstrate that tailored SSL strategies can achieve
competitive performance while reducing resource consumption by up to 4X,
underscoring their potential for energy-efficient learning at the edge.

</details>


### [41] [Foundation Model Self-Play: Open-Ended Strategy Innovation via Foundation Models](https://arxiv.org/abs/2507.06466)
*Aaron Dharna,Cong Lu,Jeff Clune*

Main category: cs.LG

TL;DR: 论文提出了一种名为FMSP的新方法，利用基础模型的代码生成能力和广泛知识，通过三种变体（vFMSP、NSSP、QDSP）改进自博弈算法，解决传统自博弈的局限性，并在Car Tag和Gandalf实验中展示了其多样性和高质量策略发现能力。


<details>
  <summary>Details</summary>
Motivation: 传统自博弈算法（SP）容易陷入局部最优且缺乏多样性，因此需要一种新方法来克服这些限制。

Method: 提出了三种FMSP变体：vFMSP通过竞争自博弈优化策略；NSSP专注于策略多样性；QDSP结合多样性和高质量策略。

Result: 在Car Tag中，FMSP超越了人类设计的策略；在Gandalf中，FMSP成功突破了LLM的多层防御并自动修补漏洞。

Conclusion: FMSP为自博弈研究开辟了新方向，提供了更具创造性和开放性的策略发现途径。

Abstract: Multi-agent interactions have long fueled innovation, from natural
predator-prey dynamics to the space race. Self-play (SP) algorithms try to
harness these dynamics by pitting agents against ever-improving opponents,
thereby creating an implicit curriculum toward learning high-quality solutions.
However, SP often fails to produce diverse solutions and can get stuck in
locally optimal behaviors. We introduce Foundation-Model Self-Play (FMSP), a
new direction that leverages the code-generation capabilities and vast
knowledge of foundation models (FMs) to overcome these challenges by leaping
across local optima in policy space. We propose a family of approaches: (1)
\textbf{Vanilla Foundation-Model Self-Play (vFMSP)} continually refines agent
policies via competitive self-play; (2) \textbf{Novelty-Search Self-Play
(NSSP)} builds a diverse population of strategies, ignoring performance; and
(3) the most promising variant, \textbf{Quality-Diveristy Self-Play (QDSP)},
creates a diverse set of high-quality policies by combining the diversity of
NSSP and refinement of vFMSP. We evaluate FMSPs in Car Tag, a
continuous-control pursuer-evader setting, and in Gandalf, a simple AI safety
simulation in which an attacker tries to jailbreak an LLM's defenses. In Car
Tag, FMSPs explore a wide variety of reinforcement learning, tree search, and
heuristic-based methods, to name just a few. In terms of discovered policy
quality, \ouralgo and vFMSP surpass strong human-designed strategies. In
Gandalf, FMSPs can successfully automatically red-team an LLM, breaking through
and jailbreaking six different, progressively stronger levels of defense.
Furthermore, FMSPs can automatically proceed to patch the discovered
vulnerabilities. Overall, FMSPs represent a promising new research frontier of
improving self-play with foundation models, opening fresh paths toward more
creative and open-ended strategy discovery

</details>


### [42] [Mitigating Message Imbalance in Fraud Detection with Dual-View Graph Representation Learning](https://arxiv.org/abs/2507.06469)
*Yudan Song,Yuecen Wei,Yuhang Lu,Qingyun Sun,Minglai Shao,Li-e Wang,Chunming Hu,Xianxian Li,Xingcheng Fu*

Main category: cs.LG

TL;DR: 论文提出了一种双视图图表示学习方法（MimbFD），用于解决基于GNN的欺诈检测中因拓扑和类别不平衡导致的信息传播不均问题。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法在欺诈检测中因局部交互导致全局拓扑信息传播不均，且欺诈节点与良性节点不平衡会淹没节点特定信息。

Method: 设计拓扑消息可达性模块穿透欺诈伪装，并引入局部混淆去偏模块调整节点表示，平衡不同类别的影响。

Result: 在三个公开欺诈数据集上的实验表明，MimbFD在欺诈检测中表现优异。

Conclusion: MimbFD通过双视图方法有效缓解了信息不平衡问题，提升了欺诈检测性能。

Abstract: Graph representation learning has become a mainstream method for fraud
detection due to its strong expressive power, which focuses on enhancing node
representations through improved neighborhood knowledge capture. However, the
focus on local interactions leads to imbalanced transmission of global
topological information and increased risk of node-specific information being
overwhelmed during aggregation due to the imbalance between fraud and benign
nodes. In this paper, we first summarize the impact of topology and class
imbalance on downstream tasks in GNN-based fraud detection, as the problem of
imbalanced supervisory messages is caused by fraudsters' topological behavior
obfuscation and identity feature concealment. Based on statistical validation,
we propose a novel dual-view graph representation learning method to mitigate
Message imbalance in Fraud Detection(MimbFD). Specifically, we design a
topological message reachability module for high-quality node representation
learning to penetrate fraudsters' camouflage and alleviate insufficient
propagation. Then, we introduce a local confounding debiasing module to adjust
node representations, enhancing the stable association between node
representations and labels to balance the influence of different classes.
Finally, we conducted experiments on three public fraud datasets, and the
results demonstrate that MimbFD exhibits outstanding performance in fraud
detection.

</details>


### [43] [FedDifRC: Unlocking the Potential of Text-to-Image Diffusion Models in Heterogeneous Federated Learning](https://arxiv.org/abs/2507.06482)
*Huan Wang,Haoran Li,Huaming Chen,Jun Yan,Jiahua Shi,Jun Shen*

Main category: cs.LG

TL;DR: 论文提出FedDifRC，一种基于扩散模型的联邦学习方法，通过扩散表示协作解决数据异构性问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异构性影响模型收敛和性能，需有效方法解决。

Method: 引入扩散模型，构建文本驱动的对比学习和噪声驱动的一致性正则化，优化特征空间。

Result: 实验验证FedDifRC在不同场景下的有效性和关键组件效率。

Conclusion: FedDifRC通过扩散表示协作有效缓解数据异构性，提升联邦学习性能。

Abstract: Federated learning aims at training models collaboratively across
participants while protecting privacy. However, one major challenge for this
paradigm is the data heterogeneity issue, where biased data preferences across
multiple clients, harming the model's convergence and performance. In this
paper, we first introduce powerful diffusion models into the federated learning
paradigm and show that diffusion representations are effective steers during
federated training. To explore the possibility of using diffusion
representations in handling data heterogeneity, we propose a novel
diffusion-inspired Federated paradigm with Diffusion Representation
Collaboration, termed FedDifRC, leveraging meaningful guidance of diffusion
models to mitigate data heterogeneity. The key idea is to construct text-driven
diffusion contrasting and noise-driven diffusion regularization, aiming to
provide abundant class-related semantic information and consistent convergence
signals. On the one hand, we exploit the conditional feedback from the
diffusion model for different text prompts to build a text-driven contrastive
learning strategy. On the other hand, we introduce a noise-driven consistency
regularization to align local instances with diffusion denoising
representations, constraining the optimization region in the feature space. In
addition, FedDifRC can be extended to a self-supervised scheme without relying
on any labeled data. We also provide a theoretical analysis for FedDifRC to
ensure convergence under non-convex objectives. The experiments on different
scenarios validate the effectiveness of FedDifRC and the efficiency of crucial
components.

</details>


### [44] [MoFE-Time: Mixture of Frequency Domain Experts for Time-Series Forecasting Models](https://arxiv.org/abs/2507.06502)
*Yiwen Liu,Chenyu Zhang,Junjie Song,Siqi Chen,Sun Yin,Zihan Wang,Lingming Zeng,Yuji Cao,Junming Jiao*

Main category: cs.LG

TL;DR: MoFE-Time是一种创新的时间序列预测模型，结合时间和频域特征，通过混合专家网络（MoE）提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在预训练-微调范式中未能同时建模时间和频率特征，导致复杂时间序列预测性能不佳。

Method: 提出MoFE-Time模型，集成时间和频域特征，利用MoE路由机制构建多维稀疏表示。

Result: 在六个公共基准测试中，MoFE-Time表现最优，MSE和MAE分别降低6.95%和6.02%。

Conclusion: MoFE-Time在理论和实际应用中均表现出色，验证了其有效性。

Abstract: As a prominent data modality task, time series forecasting plays a pivotal
role in diverse applications. With the remarkable advancements in Large
Language Models (LLMs), the adoption of LLMs as the foundational architecture
for time series modeling has gained significant attention. Although existing
models achieve some success, they rarely both model time and frequency
characteristics in a pretraining-finetuning paradigm leading to suboptimal
performance in predictions of complex time series, which requires both modeling
periodicity and prior pattern knowledge of signals. We propose MoFE-Time, an
innovative time series forecasting model that integrates time and frequency
domain features within a Mixture of Experts (MoE) network. Moreover, we use the
pretraining-finetuning paradigm as our training framework to effectively
transfer prior pattern knowledge across pretraining and finetuning datasets
with different periodicity distributions. Our method introduces both frequency
and time cells as experts after attention modules and leverages the MoE routing
mechanism to construct multidimensional sparse representations of input
signals. In experiments on six public benchmarks, MoFE-Time has achieved new
state-of-the-art performance, reducing MSE and MAE by 6.95% and 6.02% compared
to the representative methods Time-MoE. Beyond the existing evaluation
benchmarks, we have developed a proprietary dataset, NEV-sales, derived from
real-world business scenarios. Our method achieves outstanding results on this
dataset, underscoring the effectiveness of the MoFE-Time model in practical
commercial applications.

</details>


### [45] [Direct Regret Optimization in Bayesian Optimization](https://arxiv.org/abs/2507.06529)
*Fengxue Zhang,Yuxin Chen*

Main category: cs.LG

TL;DR: 提出了一种新的贝叶斯优化方法，通过联合学习最优模型和非近视获取函数，直接优化多步遗憾。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化方法依赖手工设计的获取函数和代理模型，且通常是近视的。本文旨在解决这些问题。

Method: 使用高斯过程集合生成模拟轨迹，训练决策变换器直接选择查询点，采用密集训练-稀疏学习范式。

Result: 在合成和真实基准测试中表现优于基线，实现了更低的简单遗憾和更鲁棒的探索。

Conclusion: 该方法在贝叶斯优化中表现出色，尤其在高维或噪声环境下更具优势。

Abstract: Bayesian optimization (BO) is a powerful paradigm for optimizing expensive
black-box functions. Traditional BO methods typically rely on separate
hand-crafted acquisition functions and surrogate models for the underlying
function, and often operate in a myopic manner. In this paper, we propose a
novel direct regret optimization approach that jointly learns the optimal model
and non-myopic acquisition by distilling from a set of candidate models and
acquisitions, and explicitly targets minimizing the multi-step regret. Our
framework leverages an ensemble of Gaussian Processes (GPs) with varying
hyperparameters to generate simulated BO trajectories, each guided by an
acquisition function chosen from a pool of conventional choices, until a
Bayesian early stop criterion is met. These simulated trajectories, capturing
multi-step exploration strategies, are used to train an end-to-end decision
transformer that directly learns to select next query points aimed at improving
the ultimate objective. We further adopt a dense training--sparse learning
paradigm: The decision transformer is trained offline with abundant simulated
data sampled from ensemble GPs and acquisitions, while a limited number of real
evaluations refine the GPs online. Experimental results on synthetic and
real-world benchmarks suggest that our method consistently outperforms BO
baselines, achieving lower simple regret and demonstrating more robust
exploration in high-dimensional or noisy settings.

</details>


### [46] [The Primacy of Magnitude in Low-Rank Adaptation](https://arxiv.org/abs/2507.06558)
*Zicheng Zhang,Haoran Li,Yifeng Zhang,Guoqiang Gong,Jiaxing Wang,Pengzhang Liu,Qixia Jiang,Junxing Hu*

Main category: cs.LG

TL;DR: LoRAM是一种基于更新幅度的初始化方案，通过优化幅度调节提升LoRA性能，匹配谱初始化效果但更高效。


<details>
  <summary>Details</summary>
Motivation: 谱初始化方法虽然提升性能，但计算和存储开销大，LoRAM旨在解决这一问题。

Method: 提出LoRAM，通过幅度驱动的初始化策略，利用预训练权重幅度模拟谱增益。

Result: LoRAM在保持LoRA高效性的同时，性能匹配或优于谱初始化。

Conclusion: 幅度是LoRA性能的关键驱动因素，LoRAM提供了一种高效且性能优越的初始化方案。

Abstract: Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning
large models. While recent spectral initialization methods improve convergence
and performance over the naive "Noise & Zeros" scheme, their extra
computational and storage overhead undermines efficiency. In this paper, we
establish update magnitude as the fundamental driver of LoRA performance and
propose LoRAM, a magnitude-driven "Basis & Basis" initialization scheme that
matches spectral methods without their inefficiencies. Our key contributions
are threefold: (i) Magnitude of weight updates determines convergence. We prove
low-rank structures intrinsically bound update magnitudes, unifying
hyperparameter tuning in learning rate, scaling factor, and initialization as
mechanisms to optimize magnitude regulation. (ii) Spectral initialization
succeeds via magnitude amplification. We demystify that the presumed
knowledge-driven benefit of the spectral component essentially arises from the
boost in the weight update magnitude. (iii) A novel and compact initialization
strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight
magnitudes to simulate spectral gains. Extensive experiments show that LoRAM
serves as a strong baseline, retaining the full efficiency of LoRA while
matching or outperforming spectral initialization across benchmarks.

</details>


### [47] [SlimCaching: Edge Caching of Mixture-of-Experts for Distributed Inference](https://arxiv.org/abs/2507.06567)
*Qian Chen,Xianhao Chen,Kaibin Huang*

Main category: cs.LG

TL;DR: 论文提出了一种优化边缘网络中专家缓存的方法，以减少Mixture-of-Experts（MoE）模型的推理延迟。


<details>
  <summary>Details</summary>
Motivation: MoE模型通过激活少量专家网络提高扩展性，但大量专家网络在边缘设备上存储负担重，需分布式推理。

Method: 针对Top-K专家选择策略，设计了基于贪心和动态规划的算法，优化专家缓存以减少延迟。

Result: 仿真结果表明，该方法显著降低了推理延迟。

Conclusion: 提出的算法在边缘网络中有效优化了MoE模型的推理性能。

Abstract: Mixture-of-Experts (MoE) models improve the scalability of large language
models (LLMs) by activating only a small subset of relevant experts per input.
However, the sheer number of expert networks in an MoE model introduces a
significant storage burden for an edge device. To address this challenge, we
consider a scenario where experts are dispersed within an edge network for
distributed inference. Based on the popular Top-$K$ expert selection strategy,
we formulate a latency minimization problem by optimizing expert caching on
edge servers under storage constraints. When $K=1$, the problem reduces to a
monotone submodular maximization problem with knapsack constraints, for which
we design a greedy-based algorithm with a $(1 - 1/e)$-approximation guarantee.
For the general case where $K\geq1$, expert co-activation within the same MoE
layer introduces non-submodularity, causing greedy methods to be ineffective.
To tackle this issue, we propose a successive greedy decomposition method to
decompose the original problem into a series of subproblems, with each being
solved by a dynamic programming approach. Furthermore, we design an accelerated
algorithm based on the max-convolution technique to obtain the approximate
solution with a provable guarantee in polynomial time. Simulation results on
various MoE models demonstrate that our method significantly reduces inference
latency compared to existing baselines.

</details>


### [48] [From Data-Centric to Sample-Centric: Enhancing LLM Reasoning via Progressive Optimization](https://arxiv.org/abs/2507.06573)
*Xinjie Chen,Minpeng Liao,Guoxin Chen,Chengxi Li,Biao Fu,Kai Fan,Xinggao Liu*

Main category: cs.LG

TL;DR: 论文提出LPPO框架，通过前缀引导采样和学习进度加权优化小规模高质量示范数据的使用，提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究如何高效利用少量高质量示范数据，而非简单增加数据量。

Method: 提出前缀引导采样和学习进度加权两种技术，分别通过专家示范前缀引导策略和动态调整样本权重。

Result: 在数学推理基准测试中表现优于基线，收敛更快且性能上限更高。

Conclusion: LPPO框架通过样本中心视角优化RLVR，显著提升模型性能。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently advanced
the reasoning capabilities of large language models (LLMs). While prior work
has emphasized algorithmic design, data curation, and reward shaping, we
investigate RLVR from a sample-centric perspective and introduce LPPO
(Learning-Progress and Prefix-guided Optimization), a framework of progressive
optimization techniques. Our work addresses a critical question: how to best
leverage a small set of trusted, high-quality demonstrations, rather than
simply scaling up data volume. First, motivated by how hints aid human
problem-solving, we propose prefix-guided sampling, an online data augmentation
method that incorporates partial solution prefixes from expert demonstrations
to guide the policy, particularly for challenging instances. Second, inspired
by how humans focus on important questions aligned with their current
capabilities, we introduce learning-progress weighting, a dynamic strategy that
adjusts each training sample's influence based on model progression. We
estimate sample-level learning progress via an exponential moving average of
per-sample pass rates, promoting samples that foster learning and
de-emphasizing stagnant ones. Experiments on mathematical-reasoning benchmarks
demonstrate that our methods outperform strong baselines, yielding faster
convergence and a higher performance ceiling.

</details>


### [49] [Learning controllable dynamics through informative exploration](https://arxiv.org/abs/2507.06582)
*Peter N. Loxley,Friedrich T. Sommer*

Main category: cs.LG

TL;DR: 论文提出了一种基于“预测信息增益”的方法，用于指导环境探索，并通过强化学习找到高效的探索策略。


<details>
  <summary>Details</summary>
Motivation: 在缺乏显式模型的环境中，如何高效探索以学习可控动态是关键问题。

Method: 使用“预测信息增益”作为信息度量，结合强化学习方法，寻找高效的探索策略。

Result: 该方法优于几种短视探索方法，能可靠估计环境的可控动态。

Conclusion: 基于信息增益的探索策略在无显式模型的环境中具有潜力。

Abstract: Environments with controllable dynamics are usually understood in terms of
explicit models. However, such models are not always available, but may
sometimes be learned by exploring an environment. In this work, we investigate
using an information measure called "predicted information gain" to determine
the most informative regions of an environment to explore next. Applying
methods from reinforcement learning allows good suboptimal exploring policies
to be found, and leads to reliable estimates of the underlying controllable
dynamics. This approach is demonstrated by comparing with several myopic
exploration approaches.

</details>


### [50] [Generalization in Reinforcement Learning for Radio Access Networks](https://arxiv.org/abs/2507.06602)
*Burak Demirel,Yu Wang,Cristian Tatino,Pablo Soldati*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的通用框架，用于解决现代无线接入网络中的动态和异构环境问题，通过注意力图表示和域随机化提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现代无线接入网络（RAN）环境动态且异构，传统基于规则的算法表现不佳，而现有强化学习方法容易过拟合训练条件。

Method: 采用注意力图表示编码网络拓扑和节点属性，结合域随机化扩展训练分布，并通过分布式数据生成和集中式训练架构实现。

Result: 在5G基准测试中，该策略在吞吐量和频谱效率上优于基线方法，并在高移动性场景下表现显著提升。

Conclusion: 该框架为AI原生的6G RAN提供了一种通用且可扩展的解决方案。

Abstract: Modern RAN operate in highly dynamic and heterogeneous environments, where
hand-tuned, rule-based RRM algorithms often underperform. While RL can surpass
such heuristics in constrained settings, the diversity of deployments and
unpredictable radio conditions introduce major generalization challenges.
Data-driven policies frequently overfit to training conditions, degrading
performance in unseen scenarios. To address this, we propose a
generalization-centered RL framework for RAN control that: (i) encodes cell
topology and node attributes via attention-based graph representations; (ii)
applies domain randomization to broaden the training distribution; and (iii)
distributes data generation across multiple actors while centralizing training
in a cloud-compatible architecture aligned with O-RAN principles. Although
generalization increases computational and data-management complexity, our
distributed design mitigates this by scaling data collection and training
across diverse network conditions. Applied to downlink link adaptation in five
5G benchmarks, our policy improves average throughput and spectral efficiency
by ~10% over an OLLA baseline (10% BLER target) in full-buffer MIMO/mMIMO and
by >20% under high mobility. It matches specialized RL in full-buffer traffic
and achieves up to 4- and 2-fold gains in eMBB and mixed-traffic benchmarks,
respectively. In nine-cell deployments, GAT models offer 30% higher throughput
over MLP baselines. These results, combined with our scalable architecture,
offer a path toward AI-native 6G RAN using a single, generalizable RL agent.

</details>


### [51] [Denoising Multi-Beta VAE: Representation Learning for Disentanglement and Generation](https://arxiv.org/abs/2507.06613)
*Anshuk Uppal,Yuhta Takida,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 论文提出了一种新的生成模型框架，通过使用不同β值的VAE和扩散模型，平衡解耦性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决传统β-VAE在解耦性和重建质量之间的权衡问题。

Method: 训练单一VAE，结合新损失函数控制潜在表示信息，并引入非线性扩散模型平滑过渡不同β值的表示。

Result: 实现了高质量的解耦表示和几乎无损的重建，支持无输入图像的样本生成。

Conclusion: 新框架在解耦性和生成质量上表现优异，支持潜在空间的平滑过渡和输出一致性操作。

Abstract: Disentangled and interpretable latent representations in generative models
typically come at the cost of generation quality. The $\beta$-VAE framework
introduces a hyperparameter $\beta$ to balance disentanglement and
reconstruction quality, where setting $\beta > 1$ introduces an information
bottleneck that favors disentanglement over sharp, accurate reconstructions. To
address this trade-off, we propose a novel generative modeling framework that
leverages a range of $\beta$ values to learn multiple corresponding latent
representations. First, we obtain a slew of representations by training a
single variational autoencoder (VAE), with a new loss function that controls
the information retained in each latent representation such that the higher
$\beta$ value prioritize disentanglement over reconstruction fidelity. We then,
introduce a non-linear diffusion model that smoothly transitions latent
representations corresponding to different $\beta$ values. This model denoises
towards less disentangled and more informative representations, ultimately
leading to (almost) lossless representations, enabling sharp reconstructions.
Furthermore, our model supports sample generation without input images,
functioning as a standalone generative model. We evaluate our framework in
terms of both disentanglement and generation quality. Additionally, we observe
smooth transitions in the latent spaces with respect to changes in $\beta$,
facilitating consistent manipulation of generated outputs.

</details>


### [52] [Efficient Multi-Task Reinforcement Learning with Cross-Task Policy Guidance](https://arxiv.org/abs/2507.06615)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: 论文提出了一种名为CTPG的新框架，通过跨任务策略指导加速技能学习，并引入两种门控机制提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了利用已掌握任务的策略直接指导未掌握任务，以加速技能获取。

Method: CTPG为每个任务训练一个指导策略，从所有任务的控制策略中选择行为策略，并引入两种门控机制优化学习效率。

Result: 实验表明，CTPG显著提升了操作和运动基准测试的性能。

Conclusion: CTPG是一种通用框架，可适配现有参数共享方法，有效提升多任务强化学习性能。

Abstract: Multi-task reinforcement learning endeavors to efficiently leverage shared
information across various tasks, facilitating the simultaneous learning of
multiple tasks. Existing approaches primarily focus on parameter sharing with
carefully designed network structures or tailored optimization procedures.
However, they overlook a direct and complementary way to exploit cross-task
similarities: the control policies of tasks already proficient in some skills
can provide explicit guidance for unmastered tasks to accelerate skills
acquisition. To this end, we present a novel framework called Cross-Task Policy
Guidance (CTPG), which trains a guide policy for each task to select the
behavior policy interacting with the environment from all tasks' control
policies, generating better training trajectories. In addition, we propose two
gating mechanisms to improve the learning efficiency of CTPG: one gate filters
out control policies that are not beneficial for guidance, while the other gate
blocks tasks that do not necessitate guidance. CTPG is a general framework
adaptable to existing parameter sharing approaches. Empirical evaluations
demonstrate that incorporating CTPG with these approaches significantly
enhances performance in manipulation and locomotion benchmarks.

</details>


### [53] [UniOD: A Universal Model for Outlier Detection across Diverse Domains](https://arxiv.org/abs/2507.06624)
*Dazhi Fu,Jicong Fan*

Main category: cs.LG

TL;DR: UniOD是一个通用的异常检测框架，通过利用标记数据集训练单一模型，可跨领域检测异常，避免了繁琐的超参数调优和模型训练。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法需要针对不同数据集进行超参数调优和模型训练，过程繁琐且成本高。UniOD旨在解决这一问题，提高检测效率和准确性。

Method: UniOD将数据集转换为多个图，生成一致的节点特征，并将异常检测任务转化为节点分类问题，实现跨领域泛化。

Result: 在15个基准数据集上评估，UniOD优于15种现有方法，证明了其有效性。

Conclusion: UniOD简化了异常检测流程，降低了计算成本，提高了实际应用的便利性和准确性。

Abstract: Outlier detection (OD) seeks to distinguish inliers and outliers in
completely unlabeled datasets and plays a vital role in science and
engineering. Most existing OD methods require troublesome dataset-specific
hyperparameter tuning and costly model training before they can be deployed to
identify outliers. In this work, we propose UniOD, a universal OD framework
that leverages labeled datasets to train a single model capable of detecting
outliers of datasets from diverse domains. Specifically, UniOD converts each
dataset into multiple graphs, produces consistent node features, and frames
outlier detection as a node-classification task, and is able to generalize to
unseen domains. As a result, UniOD avoids effort on model selection and
hyperparameter tuning, reduces computational cost, and effectively utilizes the
knowledge from historical datasets, which improves the convenience and accuracy
in real applications. We evaluate UniOD on 15 benchmark OD datasets against 15
state-of-the-art baselines, demonstrating its effectiveness.

</details>


### [54] [Goal-Oriented Skill Abstraction for Offline Multi-Task Reinforcement Learning](https://arxiv.org/abs/2507.06628)
*Jinmin He,Kai Li,Yifan Zang,Haobo Fu,Qiang Fu,Junliang Xing,Jian Cheng*

Main category: cs.LG

TL;DR: GO-Skill是一种离线多任务强化学习方法，通过目标导向的技能提取和分层策略学习，提升知识共享和任务性能。


<details>
  <summary>Details</summary>
Motivation: 离线多任务强化学习面临跨任务知识共享的挑战，受人类学习启发，提出GO-Skill以提取可重用技能。

Method: 通过目标导向技能提取构建离散技能库，引入技能增强阶段优化技能，并采用分层策略学习动态协调技能。

Result: 在MetaWorld机器人操作任务中验证了GO-Skill的有效性和通用性。

Conclusion: GO-Skill通过技能抽象和分层策略学习，显著提升了离线多任务强化学习的性能。

Abstract: Offline multi-task reinforcement learning aims to learn a unified policy
capable of solving multiple tasks using only pre-collected task-mixed datasets,
without requiring any online interaction with the environment. However, it
faces significant challenges in effectively sharing knowledge across tasks.
Inspired by the efficient knowledge abstraction observed in human learning, we
propose Goal-Oriented Skill Abstraction (GO-Skill), a novel approach designed
to extract and utilize reusable skills to enhance knowledge transfer and task
performance. Our approach uncovers reusable skills through a goal-oriented
skill extraction process and leverages vector quantization to construct a
discrete skill library. To mitigate class imbalances between broadly applicable
and task-specific skills, we introduce a skill enhancement phase to refine the
extracted skills. Furthermore, we integrate these skills using hierarchical
policy learning, enabling the construction of a high-level policy that
dynamically orchestrates discrete skills to accomplish specific tasks.
Extensive experiments on diverse robotic manipulation tasks within the
MetaWorld benchmark demonstrate the effectiveness and versatility of GO-Skill.

</details>


### [55] [Prevention of Overfitting on Mesh-Structured Data Regressions with a Modified Laplace Operator](https://arxiv.org/abs/2507.06631)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出了一种基于网格数据结构的检测和防止过拟合的方法，通过拉普拉斯算子二阶导数计算熵损失，优化超参数以减少振荡。


<details>
  <summary>Details</summary>
Motivation: 解决回归问题中数据过拟合的问题，特别是在网格数据结构中。

Method: 利用网格结构计算拉普拉斯算子二阶导数，生成训练数据的熵标签，并通过交错网格识别振荡。

Result: 通过最小化熵损失优化超参数，有效减少了模型振荡。

Conclusion: 该方法无需分割训练数据，直接利用所有训练点进行训练，并通过交错网格的拉普拉斯算子作为测试指标。

Abstract: This document reports on a method for detecting and preventing overfitting on
data regressions, herein applied to mesh-like data structures. The mesh
structure allows for the straightforward computation of the Laplace-operator
second-order derivatives in a finite-difference fashion for noiseless data.
Derivatives of the training data are computed on the original training mesh to
serve as a true label of the entropy of the training data. Derivatives of the
trained data are computed on a staggered mesh to identify oscillations in the
interior of the original training mesh cells. The loss of the Laplace-operator
derivatives is used for hyperparameter optimisation, achieving a reduction of
unwanted oscillation through the minimisation of the entropy of the trained
model. In this setup, testing does not require the splitting of points from the
training data, and training is thus directly performed on all available
training points. The Laplace operator applied to the trained data on a
staggered mesh serves as a surrogate testing metric based on diffusion
properties.

</details>


### [56] [Deep Disentangled Representation Network for Treatment Effect Estimation](https://arxiv.org/abs/2507.06650)
*Hui Meng,Keping Yang,Xuyu Peng,Bo Zheng*

Main category: cs.LG

TL;DR: 提出了一种新的个体治疗效果估计算法，结合多头注意力机制和线性正交正则化器，软分解预处理变量，并通过重要性采样消除选择偏差，实验表明优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 从观测数据中估计个体治疗效果是因果推断的核心问题，现有方法难以确保精确分解协变量。

Method: 提出混合专家模型结合多头注意力和线性正交正则化器，软分解预处理变量，并使用重要性采样消除选择偏差。

Result: 在公开半合成和真实数据集上的实验表明，算法优于现有个体治疗效果估计方法。

Conclusion: 新算法通过软分解和消除选择偏差，显著提升了个体治疗效果估计的准确性。

Abstract: Estimating individual-level treatment effect from observational data is a
fundamental problem in causal inference and has attracted increasing attention
in the fields of education, healthcare, and public policy.In this work, we
concentrate on the study of disentangled representation methods that have shown
promising outcomes by decomposing observed covariates into instrumental,
confounding, and adjustment factors. However, most of the previous work has
primarily revolved around generative models or hard decomposition methods for
covariates, which often struggle to guarantee the attainment of precisely
disentangled factors. In order to effectively model different causal
relationships, we propose a novel treatment effect estimation algorithm that
incorporates a mixture of experts with multi-head attention and a linear
orthogonal regularizer to softly decompose the pre-treatment variables, and
simultaneously eliminates selection bias via importance sampling re-weighting
techniques. We conduct extensive experiments on both public semi-synthetic and
real-world production datasets. The experimental results clearly demonstrate
that our algorithm outperforms the state-of-the-art methods focused on
individual treatment effects.

</details>


### [57] [Federated Learning Inspired Fuzzy Systems: Decentralized Rule Updating for Privacy and Scalable Decision Making](https://arxiv.org/abs/2507.06652)
*Arthur Alexander Lim,Zhen Bin It,Jovan Bowen Heng,Tee Hui Teo*

Main category: cs.LG

TL;DR: 本文探讨如何通过机器学习和联邦学习改进模糊系统，以处理不确定性，并讨论了潜在的限制和改进空间。


<details>
  <summary>Details</summary>
Motivation: 模糊系统能处理不确定性，但仍有改进空间。机器学习和联邦学习的最新进展为模糊系统提供了新的优化方向。

Method: 提出将机器学习和联邦学习的技术（如更新模糊规则）应用于模糊系统，以提升其性能。

Result: 改进模糊系统的潜力存在，但具体效果需进一步研究。

Conclusion: 这些改进方案需要更多研究验证，但模糊系统的优化前景广阔。

Abstract: Fuzzy systems are a way to allow machines, systems and frameworks to deal
with uncertainty, which is not possible in binary systems that most computers
use. These systems have already been deployed for certain use cases, and fuzzy
systems could be further improved as proposed in this paper. Such technologies
to draw inspiration from include machine learning and federated learning.
Machine learning is one of the recent breakthroughs of technology and could be
applied to fuzzy systems to further improve the results it produces. Federated
learning is also one of the recent technologies that have huge potential, which
allows machine learning training to improve by reducing privacy risk, reducing
burden on networking infrastructure, and reducing latency of the latest model.
Aspects from federated learning could be used to improve federated learning,
such as applying the idea of updating the fuzzy rules that make up a key part
of fuzzy systems, to further improve it over time. This paper discusses how
these improvements would be implemented in fuzzy systems, and how it would
improve fuzzy systems. It also discusses certain limitations on the potential
improvements. It concludes that these proposed ideas and improvements require
further investigation to see how far the improvements are, but the potential is
there to improve fuzzy systems.

</details>


### [58] [Value from Observations: Towards Large-Scale Imitation Learning via Self-Improvement](https://arxiv.org/abs/2507.06701)
*Michael Bloesch,Markus Wulfmeier,Philemon Brakel,Todor Davchev,Martina Zambelli,Jost Tobias Springenberg,Abbas Abdolmaleki,William F Whitney,Nicolas Heess,Roland Hafner,Martin Riedmiller*

Main category: cs.LG

TL;DR: 该论文提出了一种从无动作标注的演示数据中学习行为的方法，通过改进现有模仿学习技术，使其适用于更复杂的数据分布。


<details>
  <summary>Details</summary>
Motivation: 模仿学习从观察（IfO）可以利用无动作标注的演示数据，避免了昂贵的动作标注或奖励函数需求，但现有研究多集中于理想化场景，限制了结果的实用性。

Method: 通过将基于强化学习的模仿学习技术适配到无动作演示数据，利用价值函数在专家与非专家数据间传递信息。

Result: 研究揭示了不同数据分布与算法适用性之间的关系，并指出了现有方法的局限性。

Conclusion: 该方法为开发更鲁棒且实用的IfO技术提供了重要见解，推动了可扩展行为学习的发展。

Abstract: Imitation Learning from Observation (IfO) offers a powerful way to learn
behaviors at large-scale: Unlike behavior cloning or offline reinforcement
learning, IfO can leverage action-free demonstrations and thus circumvents the
need for costly action-labeled demonstrations or reward functions. However,
current IfO research focuses on idealized scenarios with mostly bimodal-quality
data distributions, restricting the meaningfulness of the results. In contrast,
this paper investigates more nuanced distributions and introduces a method to
learn from such data, moving closer to a paradigm in which imitation learning
can be performed iteratively via self-improvement. Our method adapts RL-based
imitation learning to action-free demonstrations, using a value function to
transfer information between expert and non-expert data. Through comprehensive
evaluation, we delineate the relation between different data distributions and
the applicability of algorithms and highlight the limitations of established
methods. Our findings provide valuable insights for developing more robust and
practical IfO techniques on a path to scalable behaviour learning.

</details>


### [59] [PINN-Obs: Physics-Informed Neural Network-Based Observer for Nonlinear Dynamical Systems](https://arxiv.org/abs/2507.06712)
*Ayoub Farkane,Mohamed Boutayeb,Mustapha Oudani,Mounir Ghogho*

Main category: cs.LG

TL;DR: 本文提出了一种基于自适应物理信息神经网络的观测器（PINN-Obs），用于非线性系统的状态估计，具有高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 非线性动态系统的状态估计在控制和工程应用中至关重要，尤其是当只能获得部分噪声测量时。传统方法需要显式系统变换或线性化，限制了其适用性。

Method: 提出了一种直接整合系统动态和传感器数据的物理信息学习框架，自适应学习最优增益矩阵，确保估计状态收敛到真实状态。

Result: 理论分析证明了在温和可观测性条件下的收敛性，数值模拟验证了其在多种非线性系统中的有效性，包括感应电机模型和卫星运动系统。

Conclusion: PINN-Obs在精度、鲁棒性和适应性上优于现有观测器设计。

Abstract: State estimation for nonlinear dynamical systems is a critical challenge in
control and engineering applications, particularly when only partial and noisy
measurements are available. This paper introduces a novel Adaptive
Physics-Informed Neural Network-based Observer (PINN-Obs) for accurate state
estimation in nonlinear systems. Unlike traditional model-based observers,
which require explicit system transformations or linearization, the proposed
framework directly integrates system dynamics and sensor data into a
physics-informed learning process. The observer adaptively learns an optimal
gain matrix, ensuring convergence of the estimated states to the true system
states. A rigorous theoretical analysis establishes formal convergence
guarantees, demonstrating that the proposed approach achieves uniform error
minimization under mild observability conditions. The effectiveness of PINN-Obs
is validated through extensive numerical simulations on diverse nonlinear
systems, including an induction motor model, a satellite motion system, and
benchmark academic examples. Comparative experimental studies against existing
observer designs highlight its superior accuracy, robustness, and adaptability.

</details>


### [60] [Robust Deep Network Learning of Nonlinear Regression Tasks by Parametric Leaky Exponential Linear Units (LELUs) and a Diffusion Metric](https://arxiv.org/abs/2507.06765)
*Enda D. V. Bigarella*

Main category: cs.LG

TL;DR: 提出了一种新的参数化激活函数（Leaky Exponential Linear Unit）和改进的扩散损失度量，用于提升多维非线性数据回归的性能。


<details>
  <summary>Details</summary>
Motivation: 非线性激活函数对学习非线性数据集至关重要，但其平滑性和梯度特性会影响大型神经网络的性能（如过拟合和参数敏感性）。现有激活函数（如ELU、SiLU、RELU和Leaky-RELU）存在局限性。

Method: 提出了一种平滑且具有非零梯度的Leaky Exponential Linear Unit激活函数，并设计了一种新的扩散损失度量来评估模型过拟合。

Result: 新激活函数在性能上优于现有方法，扩散损失度量有效衡量了模型的过拟合程度。

Conclusion: 平滑且具有非零梯度的激活函数能显著提升模型性能，扩散损失度量为模型评估提供了新工具。

Abstract: This document proposes a parametric activation function (ac.f.) aimed at
improving multidimensional nonlinear data regression. It is a established
knowledge that nonlinear ac.f.'s are required for learning nonlinear datasets.
This work shows that smoothness and gradient properties of the ac.f. further
impact the performance of large neural networks in terms of overfitting and
sensitivity to model parameters. Smooth but vanishing-gradient ac.f.'s such as
ELU or SiLU have limited performance and non-smooth ac.f.'s such as RELU and
Leaky-RELU further impart discontinuity in the trained model. Improved
performance is demonstrated with a smooth "Leaky Exponential Linear Unit", with
non-zero gradient that can be trained. A novel diffusion-loss metric is also
proposed to gauge the performance of the trained models in terms of
overfitting.

</details>


### [61] [Learning safe, constrained policies via imitation learning: Connection to Probabilistic Inference and a Naive Algorithm](https://arxiv.org/abs/2507.06780)
*George Papadopoulos,George A. Vouros*

Main category: cs.LG

TL;DR: 提出了一种模仿学习方法，用于学习符合专家轨迹约束的最大熵策略。


<details>
  <summary>Details</summary>
Motivation: 通过连接性能与KL散度界限，结合强化学习目标与约束遵守目标，在熵最大化框架下实现约束遵守行为的学习。

Method: 利用KL散度连接性能与策略差异，结合强化学习与约束目标，采用对偶梯度下降优化学习目标。

Result: 实验表明，该方法能有效学习符合多种约束的策略模型，适应不同行为模态并具备泛化能力。

Conclusion: 该方法在约束遵守行为学习中表现出高效性和稳定性，适用于多约束和多样化行为模态的场景。

Abstract: This article introduces an imitation learning method for learning maximum
entropy policies that comply with constraints demonstrated by expert
trajectories executing a task. The formulation of the method takes advantage of
results connecting performance to bounds for the KL-divergence between
demonstrated and learned policies, and its objective is rigorously justified
through a connection to a probabilistic inference framework for reinforcement
learning, incorporating the reinforcement learning objective and the objective
to abide by constraints in an entropy maximization setting. The proposed
algorithm optimizes the learning objective with dual gradient descent,
supporting effective and stable training. Experiments show that the proposed
method can learn effective policy models for constraints-abiding behaviour, in
settings with multiple constraints of different types, accommodating different
modalities of demonstrated behaviour, and with abilities to generalize.

</details>


### [62] [Speech Tokenizer is Key to Consistent Representation](https://arxiv.org/abs/2507.06802)
*Wonjin Jung,Sungil Kang,Dong-Yeon Cho*

Main category: cs.LG

TL;DR: 本文提出了一种新型语音分词器，同时编码语言和声学信息，显著提升了语音表示的保真度，适用于多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于残差向量量化（RVQ）的方法虽引入语义元素，但常忽略关键声学特征，导致语音处理的局限性。

Method: 提出一种先进方法，同时编码语言和声学信息，保留韵律和情感内容。

Result: 实验证明该方法在语音编码、语音转换、情感识别和多模态语言建模中表现优异，无需额外训练。

Conclusion: 该方法具有广泛适用性，是推动AI语音处理的关键工具。

Abstract: Speech tokenization is crucial in digital speech processing, converting
continuous speech signals into discrete units for various computational tasks.
This paper introduces a novel speech tokenizer with broad applicability across
downstream tasks. While recent advances in residual vector quantization (RVQ)
have incorporated semantic elements, they often neglect critical acoustic
features. We propose an advanced approach that simultaneously encodes both
linguistic and acoustic information, preserving prosodic and emotional content.
Our method significantly enhances speech representation fidelity across diverse
applications. Empirical evaluations demonstrate its effectiveness in speech
coding, voice conversion, emotion recognition, and multimodal language
modeling, without requiring additional training. This versatility underscores
its potential as a key tool for advancing AI-driven speech processing.

</details>


### [63] [Intrinsic Training Signals for Federated Learning Aggregation](https://arxiv.org/abs/2507.06813)
*Cosimo Fiorini,Matteo Mosconi,Pietro Buzzega,Riccardo Salami,Simone Calderara*

Main category: cs.LG

TL;DR: LIVAR是一种无需架构修改或损失函数变化的联邦学习方法，通过利用训练信号实现高效模型聚合。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法需要修改架构或损失函数，而LIVAR旨在利用现有训练信号实现高效聚合。

Method: LIVAR采用方差加权的分类器聚合方案和基于SHAP分析的LoRA合并技术。

Result: LIVAR在多个基准测试中达到最优性能，且与现有FL方法无缝集成。

Conclusion: LIVAR证明仅通过现有训练信号即可实现高效模型聚合，为联邦学习提供了新范式。

Abstract: Federated Learning (FL) enables collaborative model training across
distributed clients while preserving data privacy. While existing approaches
for aggregating client-specific classification heads and adapted backbone
parameters require architectural modifications or loss function changes, our
method uniquely leverages intrinsic training signals already available during
standard optimization. We present LIVAR (Layer Importance and VARiance-based
merging), which introduces: i) a variance-weighted classifier aggregation
scheme using naturally emergent feature statistics, and ii) an
explainability-driven LoRA merging technique based on SHAP analysis of existing
update parameter patterns. Without any architectural overhead, LIVAR achieves
state-of-the-art performance on multiple benchmarks while maintaining seamless
integration with existing FL methods. This work demonstrates that effective
model merging can be achieved solely through existing training signals,
establishing a new paradigm for efficient federated model aggregation. The code
will be made publicly available upon acceptance.

</details>


### [64] [Comprehensive Evaluation of Prototype Neural Networks](https://arxiv.org/abs/2507.06819)
*Philipp Schlinge,Steffen Meinert,Martin Atzmueller*

Main category: cs.LG

TL;DR: 本文深入分析了几种原型模型（如ProtoPNet、ProtoPool和PIPNet），提出并应用了一套全面的评估指标，包括新提出的指标，以增强模型可解释性分析。实验涵盖了多样化的数据集，并开源了代码库。


<details>
  <summary>Details</summary>
Motivation: 原型模型是可解释人工智能（XAI）的重要方法，但缺乏全面的评估指标和对比分析。本文旨在填补这一空白。

Method: 应用多种原型模型（ProtoPNet、ProtoPool、PIPNet），提出新评估指标，并在多样化数据集（细粒度分类、非独立同分布、多标签分类）上进行实验。

Result: 通过实验对比了原型模型在不同数据集上的表现，并验证了新指标的有效性。

Conclusion: 本文提供了全面的原型模型分析框架和开源工具，为未来研究提供了便利。

Abstract: Prototype models are an important method for explainable artificial
intelligence (XAI) and interpretable machine learning. In this paper, we
perform an in-depth analysis of a set of prominent prototype models including
ProtoPNet, ProtoPool and PIPNet. For their assessment, we apply a comprehensive
set of metrics. In addition to applying standard metrics from literature, we
propose several new metrics to further complement the analysis of model
interpretability. In our experimentation, we apply the set of prototype models
on a diverse set of datasets including fine-grained classification, Non-IID
settings and multi-label classification to further contrast the performance.
Furthermore, we also provide our code as an open-source library, which
facilitates simple application of the metrics itself, as well as extensibility
- providing the option for easily adding new metrics and models.
https://github.com/uos-sis/quanproto

</details>


### [65] [HeLo: Heterogeneous Multi-Modal Fusion with Label Correlation for Emotion Distribution Learning](https://arxiv.org/abs/2507.06821)
*Chuhang Zheng,Chunwei Tian,Jie Wen,Daoqiang Zhang,Qi Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种名为HeLo的多模态情感分布学习框架，旨在挖掘多模态情感数据的异质性和互补信息，以及混合基本情感中的标签相关性。


<details>
  <summary>Details</summary>
Motivation: 多模态情感识别在人机交互中具有重要作用，但现有方法在挖掘多模态间的异质性和利用基本情感间的语义相关性方面存在不足。

Method: 采用交叉注意力融合生理数据，设计基于最优传输的异质性挖掘模块，并引入可学习的标签嵌入和标签相关性矩阵。

Result: 在两个公开数据集上的实验结果表明，该方法在情感分布学习中表现优越。

Conclusion: HeLo框架有效解决了多模态异质性挖掘和标签相关性学习的问题，提升了情感分布学习的准确性。

Abstract: Multi-modal emotion recognition has garnered increasing attention as it plays
a significant role in human-computer interaction (HCI) in recent years. Since
different discrete emotions may exist at the same time, compared with
single-class emotion recognition, emotion distribution learning (EDL) that
identifies a mixture of basic emotions has gradually emerged as a trend.
However, existing EDL methods face challenges in mining the heterogeneity among
multiple modalities. Besides, rich semantic correlations across arbitrary basic
emotions are not fully exploited. In this paper, we propose a multi-modal
emotion distribution learning framework, named HeLo, aimed at fully exploring
the heterogeneity and complementary information in multi-modal emotional data
and label correlation within mixed basic emotions. Specifically, we first adopt
cross-attention to effectively fuse the physiological data. Then, an optimal
transport (OT)-based heterogeneity mining module is devised to mine the
interaction and heterogeneity between the physiological and behavioral
representations. To facilitate label correlation learning, we introduce a
learnable label embedding optimized by correlation matrix alignment. Finally,
the learnable label embeddings and label correlation matrices are integrated
with the multi-modal representations through a novel label correlation-driven
cross-attention mechanism for accurate emotion distribution learning.
Experimental results on two publicly available datasets demonstrate the
superiority of our proposed method in emotion distribution learning.

</details>


### [66] [Artificial Generals Intelligence: Mastering Generals.io with Reinforcement Learning](https://arxiv.org/abs/2507.06825)
*Matej Straka,Martin Schmid*

Main category: cs.LG

TL;DR: 介绍了一个基于Generals.io的实时策略游戏环境，兼容Gymnasium和PettingZoo，支持高性能运行，训练出的智能体在短时间内达到人类顶尖水平。


<details>
  <summary>Details</summary>
Motivation: 为多智能体强化学习研究提供一个易用且具有挑战性的平台。

Method: 使用监督预训练和自对弈训练智能体，结合基于潜在奖励塑造和记忆特征加速学习。

Result: 智能体在36小时内达到1v1人类排行榜前0.003%。

Conclusion: 该环境和基准智能体为多智能体强化学习研究提供了高效平台。

Abstract: We introduce a real-time strategy game environment built on Generals.io, a
game that hosts thousands of active players each week across multiple game
formats. Our environment is fully compatible with Gymnasium and PettingZoo,
capable of running thousands of frames per second on commodity hardware. Our
reference agent -- trained with supervised pre-training and self-play -- hits
the top 0.003\% of the 1v1 human leaderboard after just 36 hours on a single
H100 GPU. To accelerate learning, we incorporate potential-based reward shaping
and memory features. Our contributions -- a modular RTS benchmark and a
competitive, state-of-the-art baseline agent -- provide an accessible yet
challenging platform for advancing multi-agent reinforcement learning research.

</details>


### [67] [DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models](https://arxiv.org/abs/2507.06853)
*Liang Wang,Yu Rong,Tingyang Xu,Zhenyi Zhong,Zhiyuan Liu,Pengju Wang,Deli Zhao,Qiang Liu,Shu Wu,Liang Wang*

Main category: cs.LG

TL;DR: DiffSpectra是一种基于扩散模型的生成框架，直接从多模态光谱数据推断2D和3D分子结构，解决了传统方法依赖专家和有限库的问题。


<details>
  <summary>Details</summary>
Motivation: 传统分子结构解析方法依赖专家且缺乏扩展性，现有机器学习方法依赖有限库，难以推广到新分子。生成模型如DiffSpectra提供了一种新思路。

Method: DiffSpectra采用扩散模型，结合SE(3)-等变架构的Denoising网络和基于Transformer的光谱编码器SpecFormer，实现多模态光谱数据的条件生成。

Result: DiffSpectra在结构解析中表现出高准确性，top-1准确率为16.01%，top-20为96.86%，3D几何建模和多模态条件显著提升效果。

Conclusion: DiffSpectra首次统一了多模态光谱推理和2D/3D生成建模，为分子结构解析提供了有效解决方案。

Abstract: Molecular structure elucidation from spectra is a foundational problem in
chemistry, with profound implications for compound identification, synthesis,
and drug development. Traditional methods rely heavily on expert interpretation
and lack scalability. Pioneering machine learning methods have introduced
retrieval-based strategies, but their reliance on finite libraries limits
generalization to novel molecules. Generative models offer a promising
alternative, yet most adopt autoregressive SMILES-based architectures that
overlook 3D geometry and struggle to integrate diverse spectral modalities. In
this work, we present DiffSpectra, a generative framework that directly infers
both 2D and 3D molecular structures from multi-modal spectral data using
diffusion models. DiffSpectra formulates structure elucidation as a conditional
generation process. Its denoising network is parameterized by Diffusion
Molecule Transformer, an SE(3)-equivariant architecture that integrates
topological and geometric information. Conditioning is provided by SpecFormer,
a transformer-based spectral encoder that captures intra- and inter-spectral
dependencies from multi-modal spectra. Extensive experiments demonstrate that
DiffSpectra achieves high accuracy in structure elucidation, recovering exact
structures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through
sampling. The model benefits significantly from 3D geometric modeling,
SpecFormer pre-training, and multi-modal conditioning. These results highlight
the effectiveness of spectrum-conditioned diffusion modeling in addressing the
challenge of molecular structure elucidation. To our knowledge, DiffSpectra is
the first framework to unify multi-modal spectral reasoning and joint 2D/3D
generative modeling for de novo molecular structure elucidation.

</details>


### [68] [Episodic Contextual Bandits with Knapsacks under Conversion Models](https://arxiv.org/abs/2507.06859)
*Zitian Li,Wang Chi Cheung*

Main category: cs.LG

TL;DR: 论文研究了在线情境下决策者（DM）与上下文带背包的赌博机（BwK）实例的交互问题，设计了实现次线性遗憾的算法。


<details>
  <summary>Details</summary>
Motivation: 解决动态定价和首次价格拍卖等应用中，资源量变化和上下文分布非平稳带来的挑战。

Method: 提出一种在线算法，假设存在一个置信边界预言机，能够实现次线性遗憾。

Result: 算法在特定情境下提供了改进的遗憾边界，尤其是在提供未标记特征数据时。

Conclusion: 该框架为上下文BwK文献提供了新的视角，尤其在处理无界状态空间时表现出色。

Abstract: We study an online setting, where a decision maker (DM) interacts with
contextual bandit-with-knapsack (BwK) instances in repeated episodes. These
episodes start with different resource amounts, and the contexts' probability
distributions are non-stationary in an episode. All episodes share the same
latent conversion model, which governs the random outcome contingent upon a
request's context and an allocation decision. Our model captures applications
such as dynamic pricing on perishable resources with episodic replenishment,
and first price auctions in repeated episodes with different starting budgets.
We design an online algorithm that achieves a regret sub-linear in $T$, the
number of episodes, assuming access to a \emph{confidence bound oracle} that
achieves an $o(T)$-regret. Such an oracle is readily available from existing
contextual bandit literature. We overcome the technical challenge with
arbitrarily many possible contexts, which leads to a reinforcement learning
problem with an unbounded state space. Our framework provides improved regret
bounds in certain settings when the DM is provided with unlabeled feature data,
which is novel to the contextual BwK literature.

</details>


### [69] [Horizontal and Vertical Federated Causal Structure Learning via Higher-order Cumulants](https://arxiv.org/abs/2507.06888)
*Wei Chen,Wanyang Gu,Linjun Peng,Ruichu Cai,Zhifeng Hao,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种在水平和垂直联邦设置下通过高阶累积量学习因果结构的方法，解决了现有方法在变量不完整时导致虚假因果关系的问题。


<details>
  <summary>Details</summary>
Motivation: 联邦因果发现在保护数据隐私的同时揭示实体间的因果关系具有重要应用价值，但现有方法主要针对水平联邦设置，而实际中不同客户可能包含不同变量，导致虚假因果关系。

Method: 通过聚合所有客户的高阶累积量信息构建全局累积量估计，用于递归源识别，生成全局因果强度矩阵，支持因果图重建和因果强度系数估计。

Result: 在合成数据和真实数据实验中，该方法表现出优越性能。

Conclusion: 该方法在水平和垂直联邦设置下均能有效学习因果结构，解决了变量不完整带来的问题。

Abstract: Federated causal discovery aims to uncover the causal relationships between
entities while protecting data privacy, which has significant importance and
numerous applications in real-world scenarios. Existing federated causal
structure learning methods primarily focus on horizontal federated settings.
However, in practical situations, different clients may not necessarily contain
data on the same variables. In a single client, the incomplete set of variables
can easily lead to spurious causal relationships, thereby affecting the
information transmitted to other clients. To address this issue, we
comprehensively consider causal structure learning methods under both
horizontal and vertical federated settings. We provide the identification
theories and methods for learning causal structure in the horizontal and
vertical federal setting via higher-order cumulants. Specifically, we first
aggregate higher-order cumulant information from all participating clients to
construct global cumulant estimates. These global estimates are then used for
recursive source identification, ultimately yielding a global causal strength
matrix. Our approach not only enables the reconstruction of causal graphs but
also facilitates the estimation of causal strength coefficients. Our algorithm
demonstrates superior performance in experiments conducted on both synthetic
data and real-world data.

</details>


### [70] [Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model](https://arxiv.org/abs/2507.06892)
*Jing Liang,Hongyao Tang,Yi Ma,Jinyi Liu,Yan Zheng,Shuyue Hu,Lei Bai,Jianye Hao*

Main category: cs.LG

TL;DR: 论文提出ReMix方法，通过混合策略近端策略梯度、KL凸约束和策略重生，显著降低强化学习微调的计算和时间成本，并在数学推理任务中实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习微调方法多为在线策略，数据利用率低，计算和时间成本高，限制了经济高效的扩展。

Method: ReMix结合混合策略近端策略梯度、KL凸约束和策略重生，提升数据利用率和训练效率。

Result: 在多个数学推理基准测试中，ReMix以显著降低的训练成本（30x至450x）达到SOTA性能。

Conclusion: ReMix为强化学习微调提供了一种高效、经济的方法，揭示了离线策略数据的潜在优势及其挑战。

Abstract: Reinforcement Learning (RL) has demonstrated its potential to improve the
reasoning ability of Large Language Models (LLMs). One major limitation of most
existing Reinforcement Finetuning (RFT) methods is that they are on-policy RL
in nature, i.e., data generated during the past learning process is not fully
utilized. This inevitably comes at a significant cost of compute and time,
posing a stringent bottleneck on continuing economic and efficient scaling. To
this end, we launch the renaissance of off-policy RL and propose Reincarnating
Mix-policy Proximal Policy Gradient (ReMix), a general approach to enable
on-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix
consists of three major components: (1) Mix-policy proximal policy gradient
with an increased Update-To-Data (UTD) ratio for efficient training; (2)
KL-Convex policy constraint to balance the trade-off between stability and
flexibility; (3) Policy reincarnation to achieve a seamless transition from
efficient early-stage learning to steady asymptotic improvement. In our
experiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base
models. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with
0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B
model) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math
reasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and
MATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level
performance with an over 30x to 450x reduction in training cost in terms of
rollout data volume. In addition, we reveal insightful findings via
multifaceted analysis, including the implicit preference for shorter responses
due to the Whipping Effect of off-policy discrepancy, the collapse mode of
self-reflection behavior under the presence of severe off-policyness, etc.

</details>


### [71] [Designing Adaptive Algorithms Based on Reinforcement Learning for Dynamic Optimization of Sliding Window Size in Multi-Dimensional Data Streams](https://arxiv.org/abs/2507.06901)
*Abolfazl Zarghani,Sadegh Abedi*

Main category: cs.LG

TL;DR: 提出了一种基于强化学习的动态滑动窗口优化方法（RL-Window），用于处理多维数据流，显著提升了分类准确性、鲁棒性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 多维数据流的高速度、无限性和复杂依赖性给处理带来挑战，固定窗口难以适应动态变化（如概念漂移或突发模式）。

Method: 将窗口大小选择建模为强化学习问题，利用Dueling DQN和优先经验回放处理非平稳性和高维性。

Result: 在多个基准数据集上，RL-Window在分类准确性、漂移鲁棒性和计算效率上优于现有方法（如ADWIN和CNN-Adaptive）。

Conclusion: RL-Window具有适应性和稳定性，适用于实时应用，并通过扩展指标（如能效、延迟）进一步验证其优势。

Abstract: Multi-dimensional data streams, prevalent in applications like IoT, financial
markets, and real-time analytics, pose significant challenges due to their high
velocity, unbounded nature, and complex inter-dimensional dependencies. Sliding
window techniques are critical for processing such streams, but fixed-size
windows struggle to adapt to dynamic changes like concept drift or bursty
patterns. This paper proposes a novel reinforcement learning (RL)-based
approach to dynamically optimize sliding window sizes for multi-dimensional
data streams. By formulating window size selection as an RL problem, we enable
an agent to learn an adaptive policy based on stream characteristics, such as
variance, correlations, and temporal trends. Our method, RL-Window, leverages a
Dueling Deep Q-Network (DQN) with prioritized experience replay to handle
non-stationarity and high-dimensionality. Evaluations on benchmark datasets
(UCI HAR, PAMAP2, Yahoo! Finance Stream) demonstrate that RL-Window outperforms
state-of-the-art methods like ADWIN and CNN-Adaptive in classification
accuracy, drift robustness, and computational efficiency. Additional
qualitative analyses, extended metrics (e.g., energy efficiency, latency), and
a comprehensive dataset characterization further highlight its adaptability and
stability, making it suitable for real-time applications.

</details>


### [72] [Robust and Safe Traffic Sign Recognition using N-version with Weighted Voting](https://arxiv.org/abs/2507.06907)
*Linyun Gao,Qiang Wen,Fumio Machida*

Main category: cs.LG

TL;DR: 论文提出了一种N版本机器学习（NVML）框架，通过安全感知加权软投票机制提升交通标志识别系统在对抗攻击下的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中交通标志识别易受对抗攻击影响，威胁驾驶安全，需提升其鲁棒性。

Method: 采用NVML框架，结合FMEA评估安全风险，动态分配安全感知权重，测试三版本NVML系统对抗FGSM和PGD攻击的效果。

Result: 实验表明，NVML显著提升了交通标志识别系统在对抗条件下的鲁棒性和安全性。

Conclusion: NVML框架为自动驾驶系统提供了一种有效的安全增强方法。

Abstract: Autonomous driving is rapidly advancing as a key application of machine
learning, yet ensuring the safety of these systems remains a critical
challenge. Traffic sign recognition, an essential component of autonomous
vehicles, is particularly vulnerable to adversarial attacks that can compromise
driving safety. In this paper, we propose an N-version machine learning (NVML)
framework that integrates a safety-aware weighted soft voting mechanism. Our
approach utilizes Failure Mode and Effects Analysis (FMEA) to assess potential
safety risks and assign dynamic, safety-aware weights to the ensemble outputs.
We evaluate the robustness of three-version NVML systems employing various
voting mechanisms against adversarial samples generated using the Fast Gradient
Sign Method (FGSM) and Projected Gradient Descent (PGD) attacks. Experimental
results demonstrate that our NVML approach significantly enhances the
robustness and safety of traffic sign recognition systems under adversarial
conditions.

</details>


### [73] [What Has a Foundation Model Found? Using Inductive Bias to Probe for World Models](https://arxiv.org/abs/2507.06952)
*Keyon Vafa,Peter G. Chang,Ashesh Rambachan,Sendhil Mullainathan*

Main category: cs.LG

TL;DR: 研究提出了一种评估基础模型的方法，通过合成数据集测试其是否真正掌握底层世界模型，发现模型在适应新任务时未能发展出与底层模型一致的归纳偏差。


<details>
  <summary>Details</summary>
Motivation: 评估基础模型是否真正通过序列预测捕捉到更深层次的领域理解，类似于开普勒行星运动预测后来导致牛顿力学的发现。

Method: 开发了一种称为“归纳偏差探针”的技术，通过生成合成数据集测试基础模型是否与假设的世界模型一致。

Result: 基础模型在训练任务上表现优异，但在适应新任务时未能发展出与底层世界模型一致的归纳偏差，尤其是在轨道轨迹任务中未能应用牛顿力学。

Conclusion: 基础模型可能仅发展出任务特定的启发式方法，而未能实现泛化能力。

Abstract: Foundation models are premised on the idea that sequence prediction can
uncover deeper domain understanding, much like how Kepler's predictions of
planetary motion later led to the discovery of Newtonian mechanics. However,
evaluating whether these models truly capture deeper structure remains a
challenge. We develop a technique for evaluating foundation models that
examines how they adapt to synthetic datasets generated from some postulated
world model. Our technique measures whether the foundation model's inductive
bias aligns with the world model, and so we refer to it as an inductive bias
probe. Across multiple domains, we find that foundation models can excel at
their training tasks yet fail to develop inductive biases towards the
underlying world model when adapted to new tasks. We particularly find that
foundation models trained on orbital trajectories consistently fail to apply
Newtonian mechanics when adapted to new physics tasks. Further analysis reveals
that these models behave as if they develop task-specific heuristics that fail
to generalize.

</details>


### [74] [Noisy PDE Training Requires Bigger PINNs](https://arxiv.org/abs/2507.06967)
*Sebastien Andre-Sloan,Anirbit Mukherjee,Matthew Colbrook*

Main category: cs.LG

TL;DR: 论文证明了在噪声监督数据下，物理信息神经网络（PINNs）达到低经验风险所需网络规模的下界，并验证了其在实际应用中的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究PINNs在高维偏微分方程（PDEs）噪声数据下的性能，明确其有效降低经验风险的条件。

Method: 通过理论分析证明网络规模与样本数量及噪声方差的关系，并以Hamilton–Jacobi–Bellman PDE为例进行实证验证。

Result: 证明了网络参数需满足$d_N\log d_N\gtrsim N_s \eta^2$才能实现经验风险低于噪声方差，实证结果支持理论。

Conclusion: 增加噪声监督样本数量无法单独降低经验风险，为噪声环境下训练PINNs提供了定量依据。

Abstract: Physics-Informed Neural Networks (PINNs) are increasingly used to approximate
solutions of partial differential equations (PDEs), especially in high
dimensions. In real-world applications, data samples are noisy, so it is
important to know when a predictor can still achieve low empirical risk.
However, little is known about the conditions under which a PINN can do so
effectively. We prove a lower bound on the size of neural networks required for
the supervised PINN empirical risk to fall below the variance of noisy
supervision labels. Specifically, if a predictor achieves an empirical risk
$O(\eta)$ below $\sigma^2$ (variance of supervision data), then necessarily
$d_N\log d_N\gtrsim N_s \eta^2$, where $N_s$ is the number of samples and $d_N$
is the number of trainable parameters of the PINN. A similar constraint applies
to the fully unsupervised PINN setting when boundary labels are sampled
noisily. Consequently, increasing the number of noisy supervision labels alone
does not provide a ``free lunch'' in reducing empirical risk. We also show
empirically that PINNs can indeed achieve empirical risks below $\sigma^2$
under such conditions. As a case study, we investigate PINNs applied to the
Hamilton--Jacobi--Bellman (HJB) PDE. Our findings lay the groundwork for
quantitatively understanding the parameter requirements for training PINNs in
the presence of noise.

</details>


### [75] [A Principled Framework for Multi-View Contrastive Learning](https://arxiv.org/abs/2507.06979)
*Panagiotis Koromilas,Efthymios Georgiou,Giorgos Bouritsas,Theodoros Giannakopoulos,Mihalis A. Nicolaou,Yannis Panagakis*

Main category: cs.LG

TL;DR: 论文提出两种新的损失函数（MV-InfoNCE和MV-DHEL），解决了多视图对比学习中存在的四个关键问题，并在实验中验证了其优越性和扩展性。


<details>
  <summary>Details</summary>
Motivation: 当前多视图对比学习方法存在四个主要问题（如目标冲突、视图交互不足等），限制了其性能提升。

Method: 提出MV-InfoNCE和MV-DHEL两种损失函数，分别通过整合所有视图交互和解耦对齐与均匀性来优化多视图学习。

Result: 在ImageNet1K等数据集上，新方法显著优于现有方法，并能扩展到多模态数据。

Conclusion: 新方法有效解决了多视图对比学习的局限性，并展示了在高视图数下的优势。

Abstract: Contrastive Learning (CL), a leading paradigm in Self-Supervised Learning
(SSL), typically relies on pairs of data views generated through augmentation.
While multiple augmentations per instance (more than two) improve
generalization in supervised learning, current CL methods handle additional
views suboptimally by simply aggregating different pairwise objectives. This
approach suffers from four critical limitations: (L1) it utilizes multiple
optimization terms per data point resulting to conflicting objectives, (L2) it
fails to model all interactions across views and data points, (L3) it inherits
fundamental limitations (e.g. alignment-uniformity coupling) from pairwise CL
losses, and (L4) it prevents fully realizing the benefits of increased view
multiplicity observed in supervised settings. We address these limitations
through two novel loss functions: MV-InfoNCE, which extends InfoNCE to
incorporate all possible view interactions simultaneously in one term per data
point, and MV-DHEL, which decouples alignment from uniformity across views
while scaling interaction complexity with view multiplicity. Both approaches
are theoretically grounded - we prove they asymptotically optimize for
alignment of all views and uniformity, providing principled extensions to
multi-view contrastive learning. Our empirical results on ImageNet1K and three
other datasets demonstrate that our methods consistently outperform existing
multi-view approaches and effectively scale with increasing view multiplicity.
We also apply our objectives to multimodal data and show that, in contrast to
other contrastive objectives, they can scale beyond just two modalities. Most
significantly, ablation studies reveal that MV-DHEL with five or more views
effectively mitigates dimensionality collapse by fully utilizing the embedding
space, thereby delivering multi-view benefits observed in supervised learning.

</details>


### [76] [Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing](https://arxiv.org/abs/2507.06996)
*Eunbyeol Cho,Jiyoun Kim,Minjae Lee,Sungjin Park,Edward Choi*

Main category: cs.LG

TL;DR: RawMed是一个生成多表时间序列电子健康记录（EHR）数据的框架，首次实现了接近原始EHR的合成数据生成，并通过新评估框架验证其性能。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和监管限制，真实EHR数据难以共享和利用，需要生成合成数据。现有方法通常仅生成专家选择的特征，无法捕捉原始EHR的复杂结构和时间动态。

Method: RawMed采用基于文本的表示和压缩技术，以最小预处理捕捉复杂结构和时间动态，并提出了多表时间序列合成EHR的新评估框架。

Result: 在两个开源EHR数据集上验证，RawMed在保真度和实用性上优于基线模型。

Conclusion: RawMed为合成EHR数据提供了更接近原始数据的解决方案，并提出了全面的评估标准。

Abstract: Electronic Health Records (EHR) are time-series relational databases that
record patient interactions and medical events over time, serving as a critical
resource for healthcare research and applications. However, privacy concerns
and regulatory restrictions limit the sharing and utilization of such sensitive
data, necessitating the generation of synthetic EHR datasets. Unlike previous
EHR synthesis methods, which typically generate medical records consisting of
expert-chosen features (e.g. a few vital signs or structured codes only), we
introduce RawMed, the first framework to synthesize multi-table, time-series
EHR data that closely resembles raw EHRs. Using text-based representation and
compression techniques, RawMed captures complex structures and temporal
dynamics with minimal preprocessing. We also propose a new evaluation framework
for multi-table time-series synthetic EHRs, assessing distributional
similarity, inter-table relationships, temporal dynamics, and privacy.
Validated on two open-source EHR datasets, RawMed outperforms baseline models
in fidelity and utility. The code is available at
https://github.com/eunbyeol-cho/RawMed.

</details>


### [77] [Exact Evaluation of the Accuracy of Diffusion Models for Inverse Problems with Gaussian Data Distributions](https://arxiv.org/abs/2507.07008)
*Emile Pierret,Bruno Galerne*

Main category: cs.LG

TL;DR: 本文研究了扩散模型在高斯数据分布去模糊任务中的准确性，通过计算Wasserstein距离比较理论解与扩散模型解之间的差异。


<details>
  <summary>Details</summary>
Motivation: 扩散模型作为贝叶斯逆问题的先验，虽然灵活且能生成多解，但其性能尚不明确，本文旨在评估其准确性。

Method: 在约束的高斯数据分布下，计算扩散模型采样器分布与理想解分布之间的Wasserstein距离。

Result: 研究发现可以精确分析理论解与扩散模型解之间的差异，并比较不同算法的性能。

Conclusion: 研究为扩散模型在逆问题中的应用提供了量化评估方法，有助于算法比较与改进。

Abstract: Used as priors for Bayesian inverse problems, diffusion models have recently
attracted considerable attention in the literature. Their flexibility and high
variance enable them to generate multiple solutions for a given task, such as
inpainting, super-resolution, and deblurring. However, several unresolved
questions remain about how well they perform. In this article, we investigate
the accuracy of these models when applied to a Gaussian data distribution for
deblurring. Within this constrained context, we are able to precisely analyze
the discrepancy between the theoretical resolution of inverse problems and
their resolution obtained using diffusion models by computing the exact
Wasserstein distance between the distribution of the diffusion model sampler
and the ideal distribution of solutions to the inverse problem. Our findings
allow for the comparison of different algorithms from the literature.

</details>


### [78] [PLAME: Leveraging Pretrained Language Models to Generate Enhanced Protein Multiple Sequence Alignments](https://arxiv.org/abs/2507.07032)
*Hanqun Cao,Xinyi Zhou,Zijun Gao,Chenyu Wang,Xin Gao,Zhi Zhang,Chunbin Gu,Ge Liu,Pheng-Ann Heng*

Main category: cs.LG

TL;DR: PLAME是一种新型MSA设计模型，利用预训练蛋白质语言模型的进化嵌入，提升低同源性和孤儿蛋白的结构预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有折叠模型依赖多序列比对（MSA），但在低同源性和孤儿蛋白中效果有限，PLAME旨在解决这一问题。

Method: PLAME引入预训练表示增强进化信息，采用保守性-多样性损失提升生成质量，并提出新的MSA筛选方法和序列质量评估指标。

Result: 在AlphaFold2和AlphaFold3基准测试中，PLAME在低同源性和孤儿蛋白上表现优异，提升折叠性能和序列质量评估。

Conclusion: PLAME不仅提升预测性能，还可作为适配器结合ESMFold实现高效推理，为蛋白质结构预测提供新思路。

Abstract: Protein structure prediction is essential for drug discovery and
understanding biological functions. While recent advancements like AlphaFold
have achieved remarkable accuracy, most folding models rely heavily on multiple
sequence alignments (MSAs) to boost prediction performance. This dependency
limits their effectiveness on low-homology proteins and orphan proteins, where
MSA information is sparse or unavailable. To address this limitation, we
propose PLAME, a novel MSA design model that leverages evolutionary embeddings
from pretrained protein language models. Unlike existing methods, PLAME
introduces pretrained representations to enhance evolutionary information and
employs a conservation-diversity loss to enhance generation quality.
Additionally, we propose a novel MSA selection method to effectively screen
high-quality MSAs and improve folding performance. We also propose a sequence
quality assessment metric that provides an orthogonal perspective to evaluate
MSA quality. On the AlphaFold2 benchmark of low-homology and orphan proteins,
PLAME achieves state-of-the-art performance in folding enhancement and sequence
quality assessment, with consistent improvements demonstrated on AlphaFold3.
Ablation studies validate the effectiveness of the MSA selection method, while
extensive case studies on various protein types provide insights into the
relationship between AlphaFold's prediction quality and MSA characteristics.
Furthermore, we demonstrate that PLAME can serve as an adapter achieving
AlphaFold2-level accuracy with the ESMFold's inference speed.

</details>


### [79] [An Ensemble Embedding Approach for Improving Semantic Caching Performance in LLM-based Systems](https://arxiv.org/abs/2507.07061)
*Shervin Ghaffari,Zohre Bahranifard,Mohammad Akbari*

Main category: cs.LG

TL;DR: 论文提出了一种集成嵌入方法，通过结合多个嵌入模型和训练元编码器，提升LLM缓存系统中的语义相似性检测能力，显著优于单模型方法。


<details>
  <summary>Details</summary>
Motivation: 现有语义缓存框架依赖单一嵌入模型表示查询，无法充分捕捉真实查询分布中的多样语义关系。

Method: 采用集成嵌入方法，结合多个嵌入模型并通过训练元编码器改进语义相似性检测。

Result: 在QQP数据集上，集成方法实现了92%的缓存命中率和85%的非等价查询拒绝准确率。

Conclusion: 集成嵌入方法显著提升LLM缓存系统的性能，减少计算开销。

Abstract: Semantic caching enhances the efficiency of large language model (LLM)
systems by identifying semantically similar queries, storing responses once,
and serving them for subsequent equivalent requests. However, existing semantic
caching frameworks rely on single embedding models for query representation,
which limits their ability to capture the diverse semantic relationships
present in real-world query distributions. This paper presents an ensemble
embedding approach that combines multiple embedding models through a trained
meta-encoder to improve semantic similarity detection in LLM caching systems.
We evaluate our method using the Quora Question Pairs (QQP) dataset, measuring
cache hit ratios, cache miss ratios, token savings, and response times. Our
ensemble approach achieves a 92\% cache hit ratio for semantically equivalent
queries while maintaining an 85\% accuracy in correctly rejecting
non-equivalent queries as cache misses. These results demonstrate that ensemble
embedding methods significantly outperform single-model approaches in
distinguishing between semantically similar and dissimilar queries, leading to
more effective caching performance and reduced computational overhead in
LLM-based systems.

</details>


### [80] [Addressing Imbalanced Domain-Incremental Learning through Dual-Balance Collaborative Experts](https://arxiv.org/abs/2507.07100)
*Lan Li,Da-Wei Zhou,Han-Jia Ye,De-Chuan Zhan*

Main category: cs.LG

TL;DR: DCE框架通过频率感知专家组和动态专家选择器，解决了DIL中的类内不平衡和跨域分布偏移问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: DIL在非平稳环境中面临类内不平衡和跨域分布偏移的挑战，导致模型性能下降。

Method: 提出DCE框架，包括频率感知专家组和动态专家选择器，通过高斯采样和伪特征合成平衡知识保留与新数据利用。

Result: 在四个基准数据集上，DCE表现出最先进的性能。

Conclusion: DCE有效解决了DIL中的关键挑战，显著提升了模型在非平稳环境中的适应能力。

Abstract: Domain-Incremental Learning (DIL) focuses on continual learning in
non-stationary environments, requiring models to adjust to evolving domains
while preserving historical knowledge. DIL faces two critical challenges in the
context of imbalanced data: intra-domain class imbalance and cross-domain class
distribution shifts. These challenges significantly hinder model performance,
as intra-domain imbalance leads to underfitting of few-shot classes, while
cross-domain shifts require maintaining well-learned many-shot classes and
transferring knowledge to improve few-shot class performance in old domains. To
overcome these challenges, we introduce the Dual-Balance Collaborative Experts
(DCE) framework. DCE employs a frequency-aware expert group, where each expert
is guided by specialized loss functions to learn features for specific
frequency groups, effectively addressing intra-domain class imbalance.
Subsequently, a dynamic expert selector is learned by synthesizing
pseudo-features through balanced Gaussian sampling from historical class
statistics. This mechanism navigates the trade-off between preserving many-shot
knowledge of previous domains and leveraging new data to improve few-shot class
performance in earlier tasks. Extensive experimental results on four benchmark
datasets demonstrate DCE's state-of-the-art performance.

</details>


### [81] [Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation Is Wasteful](https://arxiv.org/abs/2507.07101)
*Martin Marek,Sanae Lotfi,Aditya Somasundaram,Andrew Gordon Wilson,Micah Goldblum*

Main category: cs.LG

TL;DR: 论文探讨小批量训练语言模型的稳定性，提出Adam超参数调整规则，发现小批量训练更稳定、高效，并推荐避免梯度累积。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为小批量训练不稳定，需梯度累积，但作者重新审视小批量训练，探索其潜力。

Method: 研究小批量（低至批量1）训练，提出Adam超参数调整规则，并测试其效果。

Result: 小批量训练稳定、对超参数更鲁棒、性能优于大批量，且支持SGD训练。

Conclusion: 推荐小批量训练，避免梯度累积，除非多设备训练带宽受限。

Abstract: Conventional wisdom dictates that small batch sizes make language model
pretraining and fine-tuning unstable, motivating gradient accumulation, which
trades off the number of optimizer steps for a proportional increase in batch
size. While it is common to decrease the learning rate for smaller batch sizes,
other hyperparameters are often held fixed. In this work, we revisit small
batch sizes all the way down to batch size one, and we propose a rule for
scaling Adam hyperparameters to small batch sizes. We find that small batch
sizes (1) train stably, (2) are consistently more robust to hyperparameter
choices, (3) achieve equal or better per-FLOP performance than larger batch
sizes, and (4) notably enable stable language model training with vanilla SGD,
even without momentum, despite storing no optimizer state. Building on these
results, we provide practical recommendations for selecting a batch size and
setting optimizer hyperparameters. We further recommend against gradient
accumulation unless training on multiple devices with multiple model replicas,
bottlenecked by inter-device bandwidth.

</details>


### [82] [Does Data Scaling Lead to Visual Compositional Generalization?](https://arxiv.org/abs/2507.07102)
*Arnas Uselis,Andrea Dittadi,Seong Joon Oh*

Main category: cs.LG

TL;DR: 研究发现，组合泛化能力由数据多样性而非数据规模驱动，线性分解的表征结构是关键。


<details>
  <summary>Details</summary>
Motivation: 探究当代视觉模型是否具备组合理解能力，验证数据规模和多样性对组合泛化的影响。

Method: 通过控制实验系统变化数据规模、概念多样性和组合覆盖，分析模型表现。

Result: 组合泛化依赖数据多样性，线性分解表征结构能实现高效泛化。预训练模型表现部分符合。

Conclusion: 强调构建多样性数据集的重要性，关注支持高效组合学习的表征结构。

Abstract: Compositional understanding is crucial for human intelligence, yet it remains
unclear whether contemporary vision models exhibit it. The dominant machine
learning paradigm is built on the premise that scaling data and model sizes
will improve out-of-distribution performance, including compositional
generalization. We test this premise through controlled experiments that
systematically vary data scale, concept diversity, and combination coverage. We
find that compositional generalization is driven by data diversity, not mere
data scale. Increased combinatorial coverage forces models to discover a
linearly factored representational structure, where concepts decompose into
additive components. We prove this structure is key to efficiency, enabling
perfect generalization from few observed combinations. Evaluating pretrained
models (DINO, CLIP), we find above-random yet imperfect performance, suggesting
partial presence of this structure. Our work motivates stronger emphasis on
constructing diverse datasets for compositional generalization, and considering
the importance of representational structure that enables efficient
compositional learning. Code available at
https://github.com/oshapio/visual-compositional-generalization.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [83] [On the Hardness of Unsupervised Domain Adaptation: Optimal Learners and Information-Theoretic Perspective](https://arxiv.org/abs/2507.06552)
*Zhiyi Dong,Zixuan Liu,Yongyi Mao*

Main category: stat.ML

TL;DR: 该论文研究了协变量偏移下无监督域适应（UDA）的困难性，提出了一种基于后验目标标签不确定性（PTLU）的评估方法，用于量化UDA学习的难度。


<details>
  <summary>Details</summary>
Motivation: 传统的最坏情况分析过于悲观，忽略了真实UDA实例的分布特性。本文通过建模真实三元组（源分布、目标分布和分类器）的分布，提出了一种更合理的评估框架。

Method: 定义了UDA类的性能为平均目标域风险，并引入了PTLU及其经验估计EPTLU，用于量化目标域预测的不确定性。

Result: 证明了PTLU可以作为UDA学习难度的有效代理，并通过示例展示了其相对于现有方法的优势。

Conclusion: PTLU是一种有效的工具，可用于评估UDA学习的难度，并为未来的研究提供了新的方向。

Abstract: This paper studies the hardness of unsupervised domain adaptation (UDA) under
covariate shift. We model the uncertainty that the learner faces by a
distribution $\pi$ in the ground-truth triples $(p, q, f)$ -- which we call a
UDA class -- where $(p, q)$ is the source -- target distribution pair and $f$
is the classifier. We define the performance of a learner as the overall target
domain risk, averaged over the randomness of the ground-truth triple. This
formulation couples the source distribution, the target distribution and the
classifier in the ground truth, and deviates from the classical worst-case
analyses, which pessimistically emphasize the impact of hard but rare UDA
instances. In this formulation, we precisely characterize the optimal learner.
The performance of the optimal learner then allows us to define the learning
difficulty for the UDA class and for the observed sample. To quantify this
difficulty, we introduce an information-theoretic quantity -- Posterior Target
Label Uncertainty (PTLU) -- along with its empirical estimate (EPTLU) from the
sample , which capture the uncertainty in the prediction for the target domain.
Briefly, PTLU is the entropy of the predicted label in the target domain under
the posterior distribution of ground-truth classifier given the observed source
and target samples. By proving that such a quantity serves to lower-bound the
risk of any learner, we suggest that these quantities can be used as proxies
for evaluating the hardness of UDA learning. We provide several examples to
demonstrate the advantage of PTLU, relative to the existing measures, in
evaluating the difficulty of UDA learning.

</details>


### [84] [Semi-parametric Functional Classification via Path Signatures Logistic Regression](https://arxiv.org/abs/2507.06637)
*Pengcheng Zeng,Siyuan Jiang*

Main category: stat.ML

TL;DR: PSLR是一种半参数框架，用于分类带有标量协变量的向量值函数数据，通过路径签名克服传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统函数逻辑回归模型依赖线性假设和固定基展开，限制了灵活性且在非规则采样下性能下降。

Method: 利用截断路径签名构建有限维、无基表示，捕捉非线性和跨通道依赖，嵌入时间增强路径提取稳定特征。

Result: 实验显示PSLR在准确性、鲁棒性和可解释性上优于传统方法，特别是在非均匀采样下。

Conclusion: PSLR将粗糙路径理论融入现代函数数据分析，具有理论和实践优势。

Abstract: We propose Path Signatures Logistic Regression (PSLR), a semi-parametric
framework for classifying vector-valued functional data with scalar covariates.
Classical functional logistic regression models rely on linear assumptions and
fixed basis expansions, which limit flexibility and degrade performance under
irregular sampling. PSLR overcomes these issues by leveraging truncated path
signatures to construct a finite-dimensional, basis-free representation that
captures nonlinear and cross-channel dependencies. By embedding trajectories as
time-augmented paths, PSLR extracts stable, geometry-aware features that are
robust to sampling irregularity without requiring a common time grid, while
still preserving subject-specific timing patterns. We establish theoretical
guarantees for the existence and consistent estimation of the optimal
truncation order, along with non-asymptotic risk bounds. Experiments on
synthetic and real-world datasets show that PSLR outperforms traditional
functional classifiers in accuracy, robustness, and interpretability,
particularly under non-uniform sampling schemes. Our results highlight the
practical and theoretical benefits of integrating rough path theory into modern
functional data analysis.

</details>


### [85] [Fast Gaussian Processes under Monotonicity Constraints](https://arxiv.org/abs/2507.06677)
*Chao Zhang,Jasper M. Everink,Jakob Sauer Jørgensen*

Main category: stat.ML

TL;DR: 提出了一种基于虚拟点的新框架，结合单调性约束构建高斯过程模型，并通过RLRTO方法高效采样。改进了现有方法，使用NUTS替代Gibbs采样，提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 高斯过程在复杂函数建模中广泛应用，但高维问题中引入单调性约束的计算挑战较大，需改进方法以提高效率和准确性。

Method: 提出基于RLRTO的虚拟点框架，结合单调性约束；改进现有方法，用NUTS替代Gibbs采样。

Result: 在合成函数和微分方程系统建模中验证，所有方法预测性能相当，但RLRTO和NUTS方法显著提升了计算效率。

Conclusion: 新框架和NUTS改进方法在保持预测性能的同时，显著提高了计算效率，适用于广泛问题。

Abstract: Gaussian processes (GPs) are widely used as surrogate models for complicated
functions in scientific and engineering applications. In many cases, prior
knowledge about the function to be approximated, such as monotonicity, is
available and can be leveraged to improve model fidelity. Incorporating such
constraints into GP models enhances predictive accuracy and reduces
uncertainty, but remains a computationally challenging task for
high-dimensional problems. In this work, we present a novel virtual point-based
framework for building constrained GP models under monotonicity constraints,
based on regularized linear randomize-then-optimize (RLRTO), which enables
efficient sampling from a constrained posterior distribution by means of
solving randomized optimization problems. We also enhance two existing virtual
point-based approaches by replacing Gibbs sampling with the No U-Turn Sampler
(NUTS) for improved efficiency. A Python implementation of these methods is
provided and can be easily applied to a wide range of problems. This
implementation is then used to validate the approaches on approximating a range
of synthetic functions, demonstrating comparable predictive performance between
all considered methods and significant improvements in computational efficiency
with the two NUTS methods and especially with the RLRTO method. The framework
is further applied to construct surrogate models for systems of differential
equations.

</details>


### [86] [Adaptive collaboration for online personalized distributed learning with heterogeneous clients](https://arxiv.org/abs/2507.06844)
*Constantin Philippenko,Batiste Le Bars,Kevin Scaman,Laurent Massoulié*

Main category: stat.ML

TL;DR: 该论文研究了在线个性化去中心化学习问题，提出了一种基于梯度的协作标准，以动态选择梯度相似的客户端，从而减少梯度方差并缓解偏差。


<details>
  <summary>Details</summary>
Motivation: 解决在统计异构客户端协作加速本地训练时，如何选择相关协作者以减少梯度方差并缓解偏差的挑战。

Method: 引入基于梯度的协作标准，动态选择梯度相似的客户端，并提出了两种协作方法实例化该框架。

Result: 理论分析表明算法作为方差减少方法有效，实验验证了其在合成和真实数据集上的效果。

Conclusion: 提出的协作标准和方法在减少方差和保持最优性方面表现良好，实验验证了其有效性。

Abstract: We study the problem of online personalized decentralized learning with $N$
statistically heterogeneous clients collaborating to accelerate local training.
An important challenge in this setting is to select relevant collaborators to
reduce gradient variance while mitigating the introduced bias. To tackle this,
we introduce a gradient-based collaboration criterion, allowing each client to
dynamically select peers with similar gradients during the optimization
process. Our criterion is motivated by a refined and more general theoretical
analysis of the All-for-one algorithm, proved to be optimal in Even et al.
(2022) for an oracle collaboration scheme. We derive excess loss upper-bounds
for smooth objective functions, being either strongly convex, non-convex, or
satisfying the Polyak-Lojasiewicz condition; our analysis reveals that the
algorithm acts as a variance reduction method where the speed-up depends on a
sufficient variance. We put forward two collaboration methods instantiating the
proposed general schema; and we show that one variant preserves the optimality
of All-for-one. We validate our results with experiments on synthetic and real
datasets.

</details>


### [87] [Conformal Prediction for Long-Tailed Classification](https://arxiv.org/abs/2507.06867)
*Tiffany Ding,Jean-Baptiste Fermanian,Joseph Salmon*

Main category: stat.ML

TL;DR: 论文提出两种方法，用于解决长尾分布分类问题中的预测集覆盖率和大小问题，通过调整得分函数和加权方法，实现边际覆盖率和类条件覆盖率的平衡。


<details>
  <summary>Details</summary>
Motivation: 现实中的分类问题（如植物识别）常具有长尾分布，现有方法无法同时保证预测集的小规模和良好的类条件覆盖率。

Method: 提出两种方法：1) 基于宏覆盖率的得分函数（prevalence-adjusted softmax）；2) 标签加权方法，用于在边际和类条件预测之间插值。

Result: 在Pl@ntNet和iNaturalist数据集上验证了方法的有效性，分别包含1,081和8,142个类别。

Conclusion: 新方法在长尾分布分类问题中实现了预测集大小和类条件覆盖率的平衡，优于现有方法。

Abstract: Many real-world classification problems, such as plant identification, have
extremely long-tailed class distributions. In order for prediction sets to be
useful in such settings, they should (i) provide good class-conditional
coverage, ensuring that rare classes are not systematically omitted from the
prediction sets, and (ii) be a reasonable size, allowing users to easily verify
candidate labels. Unfortunately, existing conformal prediction methods, when
applied to the long-tailed setting, force practitioners to make a binary choice
between small sets with poor class-conditional coverage or sets with very good
class-conditional coverage but that are extremely large. We propose methods
with guaranteed marginal coverage that smoothly trade off between set size and
class-conditional coverage. First, we propose a conformal score function,
prevalence-adjusted softmax, that targets a relaxed notion of class-conditional
coverage called macro-coverage. Second, we propose a label-weighted conformal
prediction method that allows us to interpolate between marginal and
class-conditional conformal prediction. We demonstrate our methods on Pl@ntNet
and iNaturalist, two long-tailed image datasets with 1,081 and 8,142 classes,
respectively.

</details>


### [88] [Distribution-free inference for LightGBM and GLM with Tweedie loss](https://arxiv.org/abs/2507.06921)
*Alokesh Manna,Aditya Vikram Sett,Dipak K. Dey,Yuwen Gu,Elizabeth D. Schifano,Jichao He*

Main category: stat.ML

TL;DR: 论文提出了一种新的非一致性度量方法，用于GLMs和GBMs，以量化保险索赔预测中的不确定性，并通过模拟验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 保险行业中，准确评估索赔成本的不确定性对保费定价和风险管理至关重要。现有方法在复杂模型（如GBMs）中的不确定性量化仍有改进空间。

Method: 提出了针对GLMs和GBMs的非一致性度量方法，结合Tweedie GLM回归和LightGBM模型，并通过模拟验证其性能。

Result: 模拟结果显示，使用局部加权Pearson残差的LightGBM方法在保持名义覆盖范围的同时，区间宽度最小。

Conclusion: 该方法为保险索赔预测中的不确定性量化提供了有效工具，尤其适用于复杂模型。

Abstract: Prediction uncertainty quantification is a key research topic in recent years
scientific and business problems. In insurance industries
(\cite{parodi2023pricing}), assessing the range of possible claim costs for
individual drivers improves premium pricing accuracy. It also enables insurers
to manage risk more effectively by accounting for uncertainty in accident
likelihood and severity. In the presence of covariates, a variety of
regression-type models are often used for modeling insurance claims, ranging
from relatively simple generalized linear models (GLMs) to regularized GLMs to
gradient boosting models (GBMs). Conformal predictive inference has arisen as a
popular distribution-free approach for quantifying predictive uncertainty under
relatively weak assumptions of exchangeability, and has been well studied under
the classic linear regression setting. In this work, we propose new
non-conformity measures for GLMs and GBMs with GLM-type loss. Using regularized
Tweedie GLM regression and LightGBM with Tweedie loss, we demonstrate conformal
prediction performance with these non-conformity measures in insurance claims
data. Our simulation results favor the use of locally weighted Pearson
residuals for LightGBM over other methods considered, as the resulting
intervals maintained the nominal coverage with the smallest average width.

</details>


### [89] [Off-Policy Evaluation Under Nonignorable Missing Data](https://arxiv.org/abs/2507.06961)
*Han Wang,Yang Xu,Wenbin Lu,Rui Song*

Main category: stat.ML

TL;DR: 本文研究了在数据缺失情况下离策略评估（OPE）的理论影响，提出了一种逆概率加权估计器以提高估计的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，离线数据常存在缺失问题，但缺失数据如何影响OPE结果尚不明确。本文旨在填补这一理论空白。

Method: 在单调缺失和可忽略/不可忽略缺失条件下，提出逆概率加权值估计器，并进行统计推断。

Result: 理论证明可忽略缺失下估计无偏，但不可忽略缺失下可能有偏。实验验证所提估计器在缺失数据下更可靠。

Conclusion: 逆概率加权估计器能有效提升OPE在缺失数据下的估计一致性，为实际应用提供理论支持。

Abstract: Off-Policy Evaluation (OPE) aims to estimate the value of a target policy
using offline data collected from potentially different policies. In real-world
applications, however, logged data often suffers from missingness. While OPE
has been extensively studied in the literature, a theoretical understanding of
how missing data affects OPE results remains unclear. In this paper, we
investigate OPE in the presence of monotone missingness and theoretically
demonstrate that the value estimates remain unbiased under ignorable
missingness but can be biased under nonignorable (informative) missingness. To
retain the consistency of value estimation, we propose an inverse probability
weighted value estimator and conduct statistical inference to quantify the
uncertainty of the estimates. Through a series of numerical experiments, we
empirically demonstrate that our proposed estimator yields a more reliable
value inference under missing data.

</details>
