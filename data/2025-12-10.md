<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 19]
- [cs.LG](#cs.LG) [Total: 88]
- [stat.ML](#stat.ML) [Total: 4]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Signal and Noise Classification in Bio-Signals via unsupervised Machine Learning](https://arxiv.org/abs/2512.07851)
*Sansrit Paudel*

Main category: eess.SP

TL;DR: 使用K-means聚类对生物信号进行质量评估：首先二分类干净/噪声信号，然后对噪声类型（运动伪影、传感器故障等）进行分类，以筛选高质量信号段用于机器学习特征工程。


<details>
  <summary>Details</summary>
Motivation: 真实世界的生物信号常被各种噪声污染（如运动伪影、基线漂移），传统数字信号处理技术无法恢复严重退化的信号，需要自动识别和分类噪声以筛选高质量信号段。

Method: 采用K-means聚类算法，首先进行干净信号与噪声信号的二分类，然后对不同类型的噪声（运动伪影、传感器故障等）进行分类。

Result: 算法在区分干净信号段和噪声信号段方面表现最可靠，特别是在识别干净数据方面表现出色，相比各种噪声类别具有更强的性能。

Conclusion: 该方法能够筛选高质量生物信号段，为特征工程提供准确结果，可能提高基于生物信号的机器学习模型的精度。

Abstract: Real-world biosignal data is frequently corrupted by various types of noise, such as motion artifacts, and baseline wander. Although digital signal processing techniques exist to process such signals; however, heavily degraded signals cannot be recovered. In this study, we aim to classify two things: first, a binary classification of noisy and clean biosignals, and next, to categorize various kinds of noise such as motion artifacts, sensor failure, etc. We implemented K-means clustering, and our results indicate that the algorithm can most reliably group clean segments from noisy ones, particularly strong performance in identifying clean data compared to various categories of noise. This approach enables the selection of only high-quality bio-signal segments and provides accurate results for feature engineering that may enhance the precision of machine learning models trained on biosignals.

</details>


### [2] [Polarization-Diversity-Based Rotation Sensing Methodology Using COTS UHF RFID Tags](https://arxiv.org/abs/2512.08069)
*Florian Muralter,Fabian Muralter,Hugo Landaluce,Asier Perallos*

Main category: eess.SP

TL;DR: 提出一种基于极化分集的旋转传感方法，使用商用UHF RFID标签和软件定义无线电读取器，通过相位变化检测旋转速度


<details>
  <summary>Details</summary>
Motivation: 超高频RFID的相位传感在物联网中应用广泛，但需要开发更有效的旋转传感方法，利用商用设备和极化分集技术

Method: 使用软件定义无线电UHF RFID读取器和商用标签，通过完全相干解调后的标签-读取器消息计算反向散射负载调制状态的差分信号，分析相位随时间变化来测量旋转速度

Result: 实验验证了理论模型，评估了系统的性能和限制，证明了该方法的有效性

Conclusion: 提出的基于极化分集的旋转传感方法使用商用UHF RFID设备，能够有效检测旋转速度，为物联网传感应用提供了新方案

Abstract: Phase-based sensing using ultra-high frequency (UHF) radio-frequency identification (RFID) has, in recent years, yielded numerous additions to the Internet of Things (IoT). This work presents a polarization diversity-based rotation sensing methodology using common-off-the-shelf (COTS) UHF RFID tags identified with a software-defined radio (SDR) UHF RFID reader. The proposed methodology uses the tag-to-reader message after fully coherent demodulation to calculate a difference signal of the backscatter load modulation states. This sequence is then used to compute the rotation speed by evaluating its phase change over time. Experimental results are used to validate the theoretical model and to evaluate the performance and limitations of the proposed system.

</details>


### [3] [Millimeter-Wave True-Time Delay Array Beamforming with Robustness to Mobility](https://arxiv.org/abs/2512.08162)
*Benjamin W. Domae,Ibrahim Pehlivan,Danijela Cabric*

Main category: eess.SP

TL;DR: 提出使用频率相关的倾斜波束（通过真实时间延迟模拟阵列）在宽带多用户下行链路场景中实现鲁棒波束成形，以解决毫米波网络中移动用户的连续连接问题。


<details>
  <summary>Details</summary>
Motivation: 毫米波网络虽然能提供极高的数据速率，但在移动用户场景下难以维持连续连接。大型天线阵列在快速移动用户和实际模拟毫米波阵列架构下，以最小中断实现所需波束成形增益尤其具有挑战性。

Method: 采用真实时间延迟模拟阵列产生频率相关的倾斜波束，为不同用户和子带创建具有线性角度-频率关系的新型波束，在瞬时容量和角度覆盖之间取得平衡。

Result: 相比其他模拟阵列波束成形设计，倾斜波束对角度偏移具有更高的可靠性，并且对变化的用户移动统计特性具有更强的适应性。

Conclusion: 频率相关的倾斜波束是实现毫米波网络中移动用户鲁棒波束成形的有效解决方案，特别适用于超可靠低延迟应用场景。

Abstract: Ultra-reliable and low-latency connectivity is required for real-time and latency-sensitive applications, like wireless augmented and virtual reality streaming. Millimeter-wave (mmW) networks have enabled extremely high data rates through large available bandwidths but struggle to maintain continuous connectivity with mobile users. Achieving the required beamforming gain from large antenna arrays with minimal disruption is particularly challenging with fast-moving users and practical analog mmW array architectures. In this work, we propose frequency-dependent slanted beams from true-time delay (TTD) analog arrays to achieve robust beamforming in wideband, multi-user downlink scenarios. Novel beams with linear angle-frequency relationships for different users and sub-bands provide a trade-off between instantaneous capacity and angular coverage. Compared to alternative analog array beamforming designs, slanted beams provide higher reliability to angle offsets and greater adaptability to varied user movement statistics.

</details>


### [4] [Metasurfaces Enable Active-Like Passive Radar](https://arxiv.org/abs/2512.08208)
*Mingyi Li,Jiawen Xu,Hanting Zhao,Xu Zhao,Yan Jin Chen,Tie Jun Cui,Vincenzo Galdi,Lianlin Li*

Main category: eess.SP

TL;DR: 提出了一种基于超表面的被动雷达系统，通过编程超表面为环境无线波场添加时空标签，实现无需源控制的主动式感知能力


<details>
  <summary>Details</summary>
Motivation: 传统被动雷达需要非合作源波形先验知识，易受强干扰影响，依赖多普勒特征，限制了检测细微或慢速目标的能力。需要一种更灵活、抗干扰的被动感知方案

Method: 集成时空编码可编程超表面，为环境无线波场添加独特的时空标签，将被动雷达转变为类似主动的感知平台，实现干扰抑制和信号增强

Result: 在5.48 GHz的概念验证实现中，成功在干扰丰富的环境下实时成像和跟踪无人机，性能可与主动雷达系统相媲美

Conclusion: MEPR为可扩展、自适应、节能的下一代集成感知与通信系统奠定了坚实基础

Abstract: Passive radars (PRs) provide a low-cost and energy-efficient approach to object detection by reusing existing wireless transmissions instead of emitting dedicated probing signals. Yet, conventional passive systems require prior knowledge of non-cooperative source waveforms, are vulnerable to strong interference, and rely on Doppler signatures, limiting their ability to detect subtle or slow-moving targets. Here, we introduce a metasurface-enabled PR (MEPR) concept that integrates a space-time-coding programmable metasurface to imprint distinct spatiotemporal tags onto ambient wireless wavefields. This mechanism transforms a PR into an active-like sensing platform without the need for source control, enabling interference suppression, signal enhancement, and accurate target localization and tracking in cluttered environments. A proof-of-concept implementation operating at 5.48 GHz confirms real-time imaging and tracking of unmanned aerial vehicles under interference-rich conditions, with performance comparable to active radar systems. These results establish MEPR as a solid foundation for scalable, adaptive, and energy-efficient next-generation integrated sensing and communication systems.

</details>


### [5] [1024-Channel 0.8V 23.9-nW/Channel Event-based Compute In-memory Neural Spike Detector](https://arxiv.org/abs/2512.08244)
*Ye Ke,Zhengnan Fu,Junyi Yang,Hongyang Shang,Arindam Basu*

Main category: eess.SP

TL;DR: 提出事件驱动的尖峰检测算法和内存计算架构，用于高密度脑机接口中的压缩式事件前端，实现超低功耗的神经信号处理。


<details>
  <summary>Details</summary>
Motivation: 下一代脑机接口面临数据率激增的挑战，传统数字尖峰检测方法存在缓冲器尺寸增大和内存访问功耗高的问题，且现有尖峰增强器与事件前端不兼容。

Method: 提出事件驱动的尖峰检测算法，并设计10-T eDRAM-SRAM混合随机存取内存计算单元，在65nm工艺中实现1024通道的内存计算尖峰检测宏。

Result: 在合成数据集上达到96.06%的检测准确率，在Neuropixel记录上达到95.08%相似度和0.05放电模式MAE，每通道功耗23.9nW，面积375μm²。

Conclusion: 该工作为高密度脑机接口提供了与压缩式事件前端兼容的尖峰检测方案，通过内存计算架构在保持高精度的同时实现超低功耗。

Abstract: The increasing data rate has become a major issue confronting next-generation intracortical brain-machine interfaces (iBMIs). The scaling number of recording sites requires complex analog wiring and lead to huge digitization power consumption. Compressive event-based neural frontends have been used in high-density neural implants to support the simultaneous recording of more channels. Event-based frontends (EBF) convert recorded signals into asynchronous digital events via delta modulation and can inherently achieve considerable compression. But EBFs are prone to false events that do not correspond to neural spikes. Spike detection (SPD) is a key process in the iBMI pipeline to detect neural spikes and further reduce the data rate. However, conventional digital SPD suffers from the increasing buffer size and frequent memory access power, and conventional spike emphasizers are not compatible with EBFs. In this work we introduced an event-based spike detection (Ev-SPD) algorithm for scalable compressive EBFs. To implement the algorithm effectively, we proposed a novel low-power 10-T eDRAM-SRAM hybrid random-access memory in-memory computing bitcell for event processing. We fabricated the proposed 1024-channel IMC SPD macro in a 65nm process and tested the macro with both synthetic dataset and Neuropixel recordings. The proposed macro achieved a high spike detection accuracy of 96.06% on a synthetic dataset and 95.08% similarity and 0.05 firing pattern MAE on Neuropixel recordings. Our event-based IMC SPD macro achieved a high per channel spike detection energy efficiency of 23.9 nW per channel and an area efficiency of 375 um^2 per channel. Our work presented a SPD scheme compatible with compressive EBFs for high-density iBMIs, achieving ultra-low power consumption with an IMC architecture while maintaining considerable accuracy.

</details>


### [6] [Geometry-Aligned Differential Privacy for Location-Safe Federated Radio Map Construction](https://arxiv.org/abs/2512.08263)
*Jijia Tian,Wangqian Chen,Junting Chen,Pooi-Yuen Kam*

Main category: eess.SP

TL;DR: 提出一种几何对齐的差分隐私机制，用于保护无线电地图构建中的用户位置隐私，通过异质噪声混淆定位并覆盖梯度空间模式。


<details>
  <summary>Details</summary>
Motivation: 无线电地图构建需要用户的位置标记信号测量数据，这引发了位置隐私的担忧。即使原始数据保留在本地，共享的模型更新仍可能通过空间结构泄露用户位置，而简单的噪声注入要么无法隐藏这种泄露，要么会降低模型准确性。

Method: 分析虚拟环境无线电地图模型中梯度如何导致位置泄露，提出几何对齐的差分隐私机制，使用异质噪声来混淆定位并覆盖梯度空间模式。该方法有理论收敛保证，将隐私强度与学习精度联系起来。

Result: 数值实验显示，该方法将攻击者定位误差从30米提高到超过180米，而无线电地图构建误差仅比均匀噪声基线增加0.2 dB。

Conclusion: 提出的几何对齐差分隐私机制能有效保护无线电地图构建中的用户位置隐私，在保持模型精度的同时显著提高攻击者的定位难度。

Abstract: Radio maps that describe spatial variations in wireless signal strength are widely used to optimize networks and support aerial platforms. Their construction requires location-labeled signal measurements from distributed users, raising fundamental concerns about location privacy. Even when raw data are kept local, the shared model updates can reveal user locations through their spatial structure, while naive noise injection either fails to hide this leakage or degrades model accuracy. This work analyzes how location leakage arises from gradients in a virtual-environment radio map model and proposes a geometry-aligned differential privacy mechanism with heterogeneous noise tailored to both confuse localization and cover gradient spatial patterns. The approach is theoretically supported with a convergence guarantee linking privacy strength to learning accuracy. Numerical experiments show the approach increases attacker localization error from 30 m to over 180 m, with only 0.2 dB increase in radio map construction error compared to a uniform-noise baseline.

</details>


### [7] [Self-Alignment Resonant Beam Empowers Beamforming without Estimation and Control for 6G IoT](https://arxiv.org/abs/2512.08386)
*Yixuan Guo,Mingliang Xiong,Qingwen Liu*

Main category: eess.SP

TL;DR: RF-RBS是一种通过部署逆向天线阵列建立自持电磁循环的物理层范式，无需数字CSI处理即可实现自对准高增益波束成形，支持高效WPT、鲁棒通信和毫米级被动定位。


<details>
  <summary>Details</summary>
Motivation: 传统波束成形在动态环境中需要复杂的CSI估计和主动波束扫描，带来巨大开销，而6G智能物联网需要通信、感知和无线能量传输的集成，因此需要新的物理层解决方案。

Method: 部署逆向天线阵列建立自持循环电磁回路，通过正反馈机制实现自对准高增益波束成形，无需依赖数字CSI处理。

Result: RF-RBS能够支持高效无线能量传输、鲁棒通信和毫米级被动定位，适用于延迟敏感的6G场景如无人系统和工业自动化。

Conclusion: RF-RBS作为一种原生物理层范式，通过自对准机制规避了传统波束成形的限制，在6G智能物联网中具有重要的战略价值，但需要解决实施挑战。

Abstract: The integration of communication, sensing, and wireless power transfer (WPT) is a cornerstone of 6G intelligent IoT. However, relying on traditional beamforming imposes prohibitive overheads due to complex channel state information (CSI) estimation and active beam scanning, particularly in dynamic environments. This paper presents a comprehensive review of the radio frequency resonant beam system (RF-RBS), a native physical-layer paradigm that circumvents these limitations. By deploying retro-directive antenna arrays (RAA) at transceivers, RF-RBS establishes a self-sustaining cyclic electromagnetic loop. This mechanism inherently enables self-aligning, high-gain beamforming through positive feedback, eliminating the reliance on digital CSI processing. We analyze the system's architecture and its capability to support high-efficiency WPT, robust communication, and millimeter-level passive positioning. Finally, we evaluate the implementation challenges and strategic value of RF-RBS in latency-sensitive 6G scenarios, including unmanned systems and industrial automation.

</details>


### [8] [Hybrid Fuzzy Logic and Shading-Aware Particle Swarm Optimization for Dynamic Photovoltaic Shading Faults Mitigation](https://arxiv.org/abs/2512.08419)
*F. Philibert Andriniriniaimalaza,Nour Mohammad Murad,George Balan,Habachi Bilal,Nirilalaina Randriatefison,Abdel Khoodaruth,Charles Bernard Andrianirina,Blaise Ravelo*

Main category: eess.SP

TL;DR: 提出结合模糊逻辑控制与遮光感知粒子群优化的混合优化框架，用于光伏系统在遮光条件下的最大功率点跟踪，相比传统P&O算法提升11.8%功率输出并减少62%跟踪时间。


<details>
  <summary>Details</summary>
Motivation: 遮光故障是影响光伏系统效率的关键挑战，不仅降低发电量，还干扰最大功率点跟踪。需要开发能够适应部分遮光和完全遮光条件的智能控制方法。

Method: 提出混合优化框架：模糊逻辑控制器基于遮光模式提供快速决策支持，遮光感知粒子群优化加速搜索过程并防止陷入局部最小值。该方法能动态适应20%-80%部分遮光和完全遮光事件。

Result: 与传统P&O算法相比，混合模型实现11.8%的功率输出提升和62%的跟踪时间减少。能够可靠检测全局最大功率点。

Conclusion: 智能控制与遮光感知优化的结合能显著增强光伏系统在复杂实际条件下的抗干扰能力和能量产出，为解决遮光问题提供了有效方案。

Abstract: Shading faults remain one of the most critical challenges affecting photovoltaic (PV) system efficiency, as they not only reduce power generation but also disturb maximum power point tracking (MPPT). To address this issue, this study introduces a hybrid optimization framework that combines Fuzzy Logic Control (FLC) with a Shading-Aware Particle Swarm Optimization (SA-PSO) method. The proposed scheme is designed to adapt dynamically to both partial shading (20%-80%) and complete shading events, ensuring reliable global maximum power point (GMPP) detection. In this approach, the fuzzy controller provides rapid decision support based on shading patterns, while SA-PSO accelerates the search process and prevents the system from becoming trapped in local minima. A comparative performance assessment with the conventional Perturb and Observe (P\&O) algorithm highlights the advantages of the hybrid model, showing up to an 11.8% improvement in power output and a 62% reduction in tracking time. These results indicate that integrating intelligent control with shading-aware optimization can significantly enhance the resilience and energy yield of PV systems operating under complex real-world conditions.

</details>


### [9] [Aliasing in Near-Field Array Ambiguity Functions: a Spatial Frequency-Domain Framework](https://arxiv.org/abs/2512.08469)
*Gilles Monnoyer,Jérôme Louveaux,Laurence Defraigne,Baptiste Sambon,Luc vandendorpe*

Main category: eess.SP

TL;DR: 该论文提出了一个通用框架来分析近场超大阵列中栅瓣的几何行为和起源，为阵列设计提供无混叠区域指导。


<details>
  <summary>Details</summary>
Motivation: 超大阵列在近场操作时面临复杂的栅瓣问题，传统方法仅针对特定几何结构，需要通用框架来理解栅瓣的根本起源和几何行为，为阵列稀疏化设计提供指导。

Method: 采用局部空间频率分析导向信号，将近场栅瓣建模为混叠伪影，系统化量化其在模糊函数上的结构，推导出无混叠区域的设计准则。

Result: 建立了连接近场和远场原理的统一框架，为均匀线性阵列和均匀圆形阵列推导出无混叠区域的闭式表达式，提供了实用的阵列设计指导。

Conclusion: 该框架揭示了近场栅瓣的根本起源和几何行为，为超大阵列设计提供了系统化的混叠安全区域分析方法，有助于实现高性能的下一代通信和定位系统。

Abstract: Next-generation communication and localization systems increasingly rely on extremely large-scale arrays (XL-arrays), which promise unprecedented spatial resolution and new functionalities. These gains arise from their inherent operation in the near field (NF) regime, where the spherical nature of the wavefront can no longer be ignored; consequently, characterizing the ambiguity function -- which amounts to the matched beam pattern -- is considerably more challenging. Implementing very wide apertures with half-wavelength element spacing is costly and complex. This motivates thinning the array (removing elements), which introduces intricate aliasing structures, i.e., grating lobes. Whereas prior work has addressed this challenge using approximations tailored to specific array geometries, this paper develops a general framework that reveals the fundamental origins and geometric behavior of grating lobes in near-field ambiguity functions. Using a local spatial-frequency analysis of steering signals, we derive a systematic methodology to model NF grating lobes as aliasing artifacts, quantifying their structure on the AF, and providing design guidelines for XL-arrays that operate within aliasing-safe regions. We further connect our framework to established far-field principles. Finally, we demonstrate the practical value of the approach by deriving closed-form expressions for aliasing-free regions in canonical uniform linear arrays and uniform circular arrays.

</details>


### [10] [LoS+NLoS Holographic MIMO: Analysis and Application of Wavenumber-Division Multiplexing](https://arxiv.org/abs/2512.08509)
*Ashutosh Prajapati,Prathapasinghe Dharmawansa,Marco Di Renzo,Italo Atzeni*

Main category: eess.SP

TL;DR: 该论文为全息MIMO系统开发了统一的LoS+NLoS信道模型，扩展了波数分割复用框架，推导了各向同性和非各向同性散射的闭式表征，并证明NLoS分量能显著提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 全息MIMO系统在近场区域运行，需要能够联合捕捉视距和非视距分量的物理一致信道模型。现有研究通常分开处理这些分量或依赖环境特定的多径模型，缺乏统一的理论框架。

Method: 开发了统一的LoS+NLoS信道表示，结合了基于空间采样和基于扩展的公式。将原本仅适用于纯LoS信道的波数分割复用框架扩展到LoS+NLoS场景，对NLoS分量应用WDM得到其角度域表示，并通过功率谱因子和功率谱密度进行直接表征。

Result: 推导了各向同性和非各向同性散射的闭式表征，其中各向同性情况恢复了Jakes的各向同性模型。评估了系统自由度和遍历容量，结果表明加入NLoS分量相比纯LoS情况能显著提升性能。

Conclusion: 该研究为全息MIMO系统提供了统一的LoS+NLoS信道建模框架，扩展了WDM方法的应用范围，证明了NLoS分量对系统性能的重要贡献，为全息MIMO系统的设计和分析提供了理论基础。

Abstract: Holographic multiple-input multiple-output (MIMO) enables electrically large continuous apertures, overcoming the physical scaling limits of conventional MIMO architectures with half-wavelength spacing. Their near-field operating regime requires channel models that jointly capture line-of-sight (LoS) and non-line-of-sight (NLoS) components in a physically consistent manner. Existing studies typically treat these components separately or rely on environment-specific multipath models. In this work, we develop a unified LoS+NLoS channel representation for holographic lines that integrates spatial-sampling-based and expansion-based formulations. Building on this model, we extend the wavenumber-division multiplexing (WDM) framework, originally introduced for purely LoS channels, to the LoS+NLoS scenario. Applying WDM to the NLoS component yields its angular-domain representation, enabling direct characterization through the power spectral factor and power spectral density. We further derive closed-form characterizations for isotropic and non-isotropic scattering, with the former recovering Jakes' isotropic model. Lastly, we evaluate the resulting degrees of freedom and ergodic capacity, showing that incorporating the NLoS component substantially improves the performance relative to the purely LoS case.

</details>


### [11] [Beyond Diagonal RIS-assisted MIMO Transmission: Beamforming Gain and Capacity Optimization](https://arxiv.org/abs/2512.08516)
*Ainna Yue Moreno-Locubiche,Josep Vidal*

Main category: eess.SP

TL;DR: BD-RIS在毫米波MIMO下行系统中显著优于传统对角RIS，通过梯度优化实现更高的频谱效率和覆盖范围


<details>
  <summary>Details</summary>
Motivation: 传统对角RIS在无线通信中已有应用，但存在性能限制。BD-RIS作为对角RIS的泛化形式，有望在毫米波MIMO系统中提供更好的信号传播控制，特别是在LOS主导的传播环境下。

Method: 采用梯度优化方法，在毫米波MIMO下行系统中比较TxBF和MIMO容量传输（采用注水功率分配）。由于缺乏最优RIS元素的闭式解，该方法相比arXiv:2406.02170的解决方案具有更低的复杂度。

Result: 数值结果表明，BD-RIS在频谱效率和覆盖范围方面显著优于传统对角RIS。梯度优化方法有效解决了最优RIS元素配置问题。

Conclusion: BD-RIS是毫米波MIMO通信系统的有前途技术，能够提供比传统对角RIS更好的性能。梯度优化方法为实际部署提供了可行的解决方案。

Abstract: Reconfigurable Intelligent Surfaces (RIS) have emerged as a transformative technology in wireless communications, offering unprecedented control over signal propagation. This study focuses on passive beyond diagonal reconfigurable intelligent surface (BD-RIS), which has been proposed to generalize conventional diagonal RIS, in Multiple-Input Multiple-Output (MIMO) downlink (DL) communication systems. We compare the performance of transmit beamforming (TxBF) and MIMO capacity transmission with waterfilling power allocation in the millimeter wave (mmWave) band, where propagation primarily occurs under line-of-sight (LOS) conditions. In the lack of closed-form expressions for the optimal RIS elements in either case, our approach adopts a gradient-based optimization approach requiring lower complexity than the solution in arXiv:2406.02170. Numerical results reveal that BD-RIS significantly outperforms traditional diagonal RIS in terms of spectral efficiency and coverage

</details>


### [12] [Contextual Bandits and Reconfigurable Intelligent Surfaces for Predictive LTM Handover Decisions](https://arxiv.org/abs/2512.08556)
*Ainna Yue Moreno-Locubiche,Josep Vidal,Olga Muñoz-Medina,Margarita Cabrera-Bean*

Main category: eess.SP

TL;DR: 本文通过集成可重构智能表面(RIS)、预测接收信号功率和使用基于学习的决策来优化下一代无线网络中的切换(HO)过程。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络需要更高效的切换机制来减少不必要的切换、乒乓效应和信令开销，同时提高链路可靠性。

Method: 1) 增强传统反应式切换机制(LTM)通过线性预测来预测链路退化；2) 使用RIS缓解信号阻塞和扩展覆盖；3) 在线训练非线性上下文多臂老虎机(CMAB)代理基于上下文特征选择目标gNB。

Result: 在真实移动和信道条件下的广泛仿真显示：CMAB和RSRP预测能持续减少切换次数、乒乓率和小区准备，而RIS能提高链路可靠性。

Conclusion: 集成RIS、信号预测和基于学习的决策能有效优化下一代无线网络的切换性能，减少不必要的切换和信令开销，同时提高链路可靠性。

Abstract: This article addresses the challenge of optimizing handover (HO) in next-generation wireless networks by integrating Reconfigurable Intelligent Surfaces (RIS), predicting received signal power, and utilizing learning-based decision-making. A conventional reactive HO mechanism, such as lower-layer triggered mobility (LTM), is enhanced through linear prediction to anticipate link degradation. Additionally, the use of RIS helps to mitigate signal blockage and extend coverage. An online trained non-linear Contextual Multi-Armed Bandit (CMAB) agent selects target gNBs based on context features, which reduces unnecessary HO and signaling overhead. Extensive simulations evaluate eight combinations of these techniques under realistic mobility and channel conditions. Results show that CMAB and RSRP prediction consistently reduce the number of HO, ping-pong rate and cell preparations, while RIS improves link reliability.

</details>


### [13] [Applications of Singular Entropy to Signals and Singular Smoothness to Images](https://arxiv.org/abs/2512.08717)
*Oscar Romero,Néstor Thome*

Main category: eess.SP

TL;DR: 本文提出基于SVD和GSVD的信号与图像分析方法：在ECG分析中引入能量间隙变化和奇异能量改进母胎信号分离；在图像分析中提出奇异平滑度方法检测自然异常。


<details>
  <summary>Details</summary>
Motivation: SVD在ECG分析中已有应用，但现有方法在区分母体和胎儿信号成分方面仍有改进空间。同时，需要开发新的图像分析方法来检测自然异常（如山体裂缝和森林火灾区域）。

Method: 1. 信号分析：引入能量间隙变化(EGV)和奇异能量概念，结合GSVD增强判别能力，改进母胎ECG信号分离的阈值选择方法。
2. 图像分析：提出奇异平滑度方法，结合奇异熵和Frobenius范数评估信息密度，用于检测自然异常。

Result: 数值实验表明，提出的方法能更有效地分离母体和胎儿ECG信号，并在图像分析中成功检测出山体裂缝和森林火灾区域等自然异常。

Conclusion: SVD和GSVD为信号和图像分析提供了强大的数学框架。提出的EGV、奇异能量和奇异平滑度方法显著改进了现有技术，在医学信号分离和自然异常检测中表现出良好效果。

Abstract: This paper explores signal and image analysis by using the Singular Value Decomposition (SVD) and its extension, the Generalized Singular Value Decomposition (GSVD). A key strength of SVD lies in its ability to separate information into orthogonal subspaces. While SVD is a well-established tool in ECG analysis, particularly for source separation, this work proposes a refined method for selecting a threshold to distinguish between maternal and fetal components more effectively. In the first part of the paper, the focus is onmedical signal analysis,where the concepts of Energy Gap Variation (EGV) and Singular Energy are introduced to isolate fetal and maternal ECG signals, improving the known ones. Furthermore, the approach is significantly enhanced by the application of GSVD, which provides additional discriminative power for more accurate signal separation. The second part introduces a novel technique called Singular Smoothness, developed for image analysis. This method incorporates Singular Entropy and the Frobenius normto evaluate information density, and is applied to the detection of natural anomalies such asmountain fractures and burned forest regions. Numerical experiments are presented to demonstrate the effectiveness of the proposed approaches.

</details>


### [14] [RF sensing with dense IoT network graphs: An EM-informed analysis](https://arxiv.org/abs/2512.08746)
*Federica Fieramosca,Vittorio Rampa,Michele D'Amico,Stefano Savazzi*

Main category: eess.SP

TL;DR: 本文研究密集网络中RF感知系统的理论精度与分辨率界限，提出基于电磁模型和图神经网络的多人检测方法，并通过室内案例验证模型预测能力。


<details>
  <summary>Details</summary>
Motivation: RF感知在物联网应用中具有重要潜力，但需要理解其在密集网络中的理论性能界限，以指导网络部署前的系统设计。

Method: 采用电磁模型预测人体遮挡效应，构建基于接收信号强度的密集图结构，使用深度图神经网络检测人体运动，分析可区分人数的理论界限。

Result: 建立了RF感知系统的理论性能界限，这些界限依赖于无线链路数量、监控区域大小和人体尺寸等因素，室内案例研究验证了方法的有效性。

Conclusion: 提出的理论界限能够预测系统性能，指导网络预部署阶段的设计，电磁模型和图神经网络方法在RF感知中具有实际应用价值。

Abstract: Radio Frequency (RF) sensing is attracting interest in research, standardization, and industry, especially for its potential in Internet of Things (IoT) applications. By leveraging the properties of the ElectroMagnetic (EM) waves used in wireless networks, RF sensing captures environmental information such as the presence and movement of people and objects, enabling passive localization and vision applications. This paper investigates the theoretical bounds on accuracy and resolution for RF sensing systems within dense networks. It employs an EM model to predict the effects of body blockage in various scenarios. To detect human movements, the paper proposes a deep graph neural network, trained on Received Signal Strength (RSS) samples generated from the EM model. These samples are structured as dense graphs, with nodes representing antennas and edges as radio links. Focusing on the problem of identifying the number of human subjects co-present in a monitored area over time, the paper analyzes the theoretical limits on the number of distinguishable subjects, exploring how these limits depend on factors such as the number of radio links, the size of the monitored area and the subjects physical dimensions. These bounds enable the prediction of the system performance during network pre-deployment stages. The paper also presents the results of an indoor case study, which demonstrate the effectiveness of the approach and confirm the model's predictive potential in the network design stages.

</details>


### [15] [Evaluating the Deformation Measurement Accuracy Using Low-SNR Radars for Future InSAR Missions](https://arxiv.org/abs/2512.08779)
*Emre Havazli,Shadi Oveisgharan,Michael Denbina,Brian Hawkins*

Main category: eess.SP

TL;DR: 研究量化了低信噪比条件对InSAR位移测量的影响，通过模拟实验发现即使SNR低至-9~-10dB，仍可获得4mm精度的位移测量，通过多视处理可达到与高SNR系统相当的精度。


<details>
  <summary>Details</summary>
Motivation: InSAR在低后向散射区域常面临低信噪比问题，这会降低相位相干性和位移测量精度。研究旨在量化低SNR条件对InSAR位移测量的具体影响，为未来低成本SAR任务设计和InSAR处理优化提供依据。

Method: 使用L波段UAVSAR数据，通过将噪声等效Sigma零(NESZ)降低至-15dB来模拟低SNR条件，评估对干涉相干性、相位解缠和时间序列反演的影响。采用8x8窗口进行多视处理来改善相干性。

Result: 在信号去相干0.6且SNR为-9dB至-10dB条件下，单干涉图位移精度可达4mm。即使低SNR条件下，与高SNR相比仍可获得0.5cm/年的速度精度。多视处理显著改善相干性并消除偏差，使低SNR系统能达到与高SNR系统相当的精度。

Conclusion: 低SNR系统通过适当的多视处理可以在牺牲空间分辨率的代价下获得与高SNR系统相当的测量精度，这对未来低成本SAR任务（如SDC）的设计和在挑战性环境中的InSAR处理优化具有重要意义。

Abstract: Interferometric Synthetic Aperture Radar (InSAR) is a powerful tool for monitoring surface deformation with high precision. However, low Signal-to-Noise Ratio (SNR) conditions, common in regions with low backscatter, can degrade phase coherence and compromise displacement accuracy. In this study, we quantify the impact of low-SNR conditions on InSAR-derived displacement using L-band UAVSAR data collected over the San Andreas Fault and Greenland ice sheet. We simulate low-SNR conditions by degrading the Noise-Equivalent Sigma Zero (NESZ) to $-15~\mathrm{dB}$ and assess the resulting effects on interferometric coherence, phase unwrapping, and time series inversion. The displacement accuracy of 4mm in single interferogram can be achieved by taking looks for the signal decorrelation of 0.6 and SNR between -9dB to -10dB. Our findings indicate that even under low-SNR conditions, a velocity precision of $0.5~\mathrm{cm/yr}$ can be achieved in comparison to high-SNR conditions. By applying multilooking with an 8x8 window, we significantly improve coherence and eliminate this bias, demonstrating that low-SNR systems can achieve comparable precision to high-SNR systems at the expense of spatial resolution. These results have important implications for the design of future cost-effective SAR missions, such as Surface Deformation and Change (SDC), and the optimization of InSAR processing techniques in challenging environments.

</details>


### [16] [Delay-Oriented Distributed Scheduling with TransGNN](https://arxiv.org/abs/2512.08799)
*Boxuan Wen,Junyu Luo*

Main category: eess.SP

TL;DR: 提出基于Transformer GNN的分布式调度框架，通过注意力机制生成链路效用分数，结合本地贪婪求解器实现低延迟无线多跳网络调度


<details>
  <summary>Details</summary>
Motivation: 传统调度算法（如最大权重或基于队列长度的策略）主要优化吞吐量但延迟高，特别是在异构或动态拓扑中。现有GNN方法受限于局部聚合机制和无法建模冲突图中的长程依赖关系。

Method: 提出基于Transformer GNN的分布式调度框架：1）使用基于注意力的图编码器生成反映队列积压和干扰强度的自适应链路效用分数；2）本地贪婪求解器利用这些效用构建可行的独立链路集进行传输，确保分布式无冲突调度。

Result: 论文提出的方法能够更好地捕获空间干扰结构，解决传统GCN的局部聚合限制和长程依赖建模问题，实现低延迟的分布式调度。

Conclusion: 基于Transformer GNN的框架通过注意力机制有效建模干扰依赖关系，结合本地贪婪求解器实现了无线多跳网络中延迟优化的分布式调度。

Abstract: Minimizing transmission delay in wireless multi-hop networks is a fundamental yet challenging task due to the complex coupling among interference, queue dynamics, and distributed control. Traditional scheduling algorithms, such as max-weight or queue-length-based policies, primarily aim to optimize throughput but often suffer from high latency, especially in heterogeneous or dynamically changing topologies. Recent learning-based approaches, particularly those employing Graph Neural Networks (GNNs), have shown promise in capturing spatial interference structures. However, conventional Graph Convolutional Networks (GCNs) remain limited by their local aggregation mechanism and their inability to model long-range dependencies within the conflict graph. To address these challenges, this paper proposes a delay-oriented distributed scheduling framework based on Transformer GNN. The proposed model employs an attention-based graph encoder to generate adaptive per-link utility scores that reflect both queue backlog and interference intensity. A Local Greedy Solver (LGS) then utilizes these utilities to construct a feasible independent set of links for transmission, ensuring distributed and conflict-free scheduling.

</details>


### [17] [A Fast Broadband Beamspace Transformation](https://arxiv.org/abs/2512.08887)
*Nakul Singh,Coleman DeLude,Mark Davenport,Justin Romberg*

Main category: eess.SP

TL;DR: 提出一种计算高效的宽带多波束形成方法，通过快速波束空间变换，将计算复杂度从O(MB)降低到O(MlogN+BlogN)，显著优于传统的延迟求和法。


<details>
  <summary>Details</summary>
Motivation: 传统宽带波束形成器（如延迟求和法）的计算复杂度为O(MB)，其中M是传感器数量，B是波束数量。当B∼M时，计算量随M平方增长，限制了实时处理能力。窄带情况下可以使用空间快速傅里叶变换高效处理，但宽带设置需要同时处理多个阵列快照，缺乏高效的算法。

Method: 提出"快速波束空间变换"算法：1) 从M个传感器各取N个样本；2) 使用特殊的非均匀间隔傅里叶变换将传感器输出编码为一组系数；3) 通过求解具有Toeplitz结构的小型方程组形成每个波束。算法核心是利用傅里叶变换和Toeplitz矩阵特性实现高效计算。

Result: 算法总运行复杂度为O(MlogN+BlogN)操作/样本，与窄带情况基本保持相同的缩放比例，相比传统宽带波束形成器的O(MB)复杂度有显著提升。数值实验验证了算法良好的计算缩放性和高精度，并证明在波束空间中执行"离网"角度插值和干扰抑制等任务更加计算高效。

Conclusion: 该方法为宽带多波束形成提供了一种计算高效的解决方案，通过快速波束空间变换实现了与窄带情况相当的计算复杂度，显著优于传统方法，为实时宽带波束形成应用提供了实用工具。

Abstract: We present a new computationally efficient method for multi-beamforming in the broadband setting. Our "fast beamspace transformation" forms $B$ beams from $M$ sensor outputs using a number of operations per sample that scales linearly (to within logarithmic factors) with $M$ when $B\sim M$. While the narrowband version of this transformation can be performed efficiently with a spatial fast Fourier transform, the broadband setting requires coherent processing of multiple array snapshots simultaneously. Our algorithm works by taking $N$ samples off of each of $M$ sensors and encoding the sensor outputs into a set of coefficients using a special non-uniform spaced Fourier transform. From these coefficients, each beam is formed by solving a small system of equations that has Toeplitz structure. The total runtime complexity is $\mathcal{O}(M\log N+B\log N)$ operations per sample, exhibiting essentially the same scaling as in the narrowband case and vastly outperforming broadband beamformers based on delay and sum whose computations scale as $\mathcal{O}(MB)$. Alongside a careful mathematical formulation and analysis of our fast broadband beamspace transform, we provide a host of numerical experiments demonstrating the algorithm's favorable computational scaling and high accuracy. Finally, we demonstrate how tasks such as interpolating to ``off-grid" angles and nulling an interferer are more computationally efficient when performed directly in beamspace.

</details>


### [18] [Timing-Error Optimized Architecture for Current-Steering DACs](https://arxiv.org/abs/2512.08903)
*Ramin Babaee,Shahab Oveis Gharan,Martin Bouchard*

Main category: eess.SP

TL;DR: 提出一种新型DAC权重架构，通过统计方法最小化电流源随机时序失配引起的失真，并提供了三种不同计算复杂度的解码算法。


<details>
  <summary>Details</summary>
Motivation: 传统分段式DAC结构中，电流源之间的随机时序失配会导致信号失真，影响动态性能，需要一种能有效最小化这种失真的架构。

Method: 提出新型DAC权重架构，通过统计方法最小化时序失配失真；开发三种不同计算复杂度的解码算法，将DAC输入码字转换为相应的DAC开关控制信号。

Result: 通过高级Matlab仿真验证，相比传统分段结构，该架构在动态性能方面有显著改进。

Conclusion: 提出的DAC权重架构和算法能有效减少随机时序失配引起的失真，为高性能DAC设计提供了新方案。

Abstract: We propose a novel digital-to-analog converter (DAC) weighting architecture that statistically minimizes the distortion caused by random timing mismatches among current sources. To decode the DAC input codewords into corresponding DAC switches, we present three algorithms with varying computational complexities. We perform high-level Matlab simulations to illustrate the dynamic performance improvement over the segmented structure.

</details>


### [19] [Architecture Design for Rise/Fall Asymmetry Glitch Minimization in Current-Steering DACs](https://arxiv.org/abs/2512.08909)
*Ramin Babaee,Shahab Oveis Gharan,Martin Bouchard*

Main category: eess.SP

TL;DR: 该论文提出了一种针对电流舵DAC中开关不对称引起的输出毛刺问题的新权重方案，通过优化开关时序来提升动态性能。


<details>
  <summary>Details</summary>
Motivation: 电流舵DAC在高速度应用（如光通信）中广泛使用，但其输出毛刺与输入相关，会降低DAC的动态性能。论文旨在解决由DAC开关的上升/下降响应不对称引起的毛刺问题。

Method: 研究DAC开关不对称引起的毛刺，制定了一个定义整体DAC性能的毛刺度量标准，并基于此提出了一种新颖的DAC权重方案。

Result: 数值模拟表明，与分段结构相比，所提出的架构可能提供显著的性能优势。

Conclusion: 通过优化DAC权重方案来减少开关不对称引起的毛刺，可以显著提升电流舵DAC在高速度应用中的动态性能。

Abstract: Current-steering digital-to-analog converter (DAC) is a prominent architecture that is commonly used in high-speed applications such as optical communications. One of the shortcomings of this architecture is the output glitches that are input dependent and degrade the dynamic performance of the DAC. We investigate DAC glitches that arise from asymmetry in the fall/rise response of DAC switches. We formulate a glitch metric that defines the overall DAC performance, which is then used to find a novel DAC weighting scheme. Numerical simulations show that the proposed architecture can potentially provide a significant performance advantage compared to the segmented structure.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [20] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver是一个自适应并行推理框架，在保持与顺序推理模型相当准确率的同时，显著降低推理延迟，通过并行推理线程提高效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型推理时顺序解码导致高延迟，现有并行推理方法要么局限于监督行为克隆，要么相比顺序链式思维基线准确率显著下降，且需要定制化推理引擎，部署复杂。

Method: 1) 两阶段并行轨迹生成器产生大规模高质量并行标注的CoT数据用于监督微调；2) 基于Trie的训练-推理协同设计，可在现成的自回归推理引擎上实现并行推理；3) 并行感知的强化学习框架，教导模型平衡准确率和并行效率。

Result: 在六个数学推理基准测试中，基于Qwen3-8B的ThreadWeaver达到与先进顺序推理模型相当的准确率（平均71.9%，AIME24上79.9%），同时实现最高1.53倍的平均token延迟加速，建立了准确率与效率的新帕累托前沿。

Conclusion: ThreadWeaver成功实现了自适应并行推理，在保持准确率的同时显著提升推理效率，且无需修改现有推理引擎，具有实际部署价值。

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [21] [Space Alignment Matters: The Missing Piece for Inducing Neural Collapse in Long-Tailed Learning](https://arxiv.org/abs/2512.07844)
*Jinping Wang,Zhiqiang Gao,Zhiwu Xie*

Main category: cs.LG

TL;DR: 该论文针对长尾分布下神经网络崩溃(NC)现象缺失的问题，提出了特征与分类器权重空间对齐策略，显著提升了现有长尾方法的性能。


<details>
  <summary>Details</summary>
Motivation: 在类别平衡条件下，神经网络会出现"神经崩溃"现象，特征均值和分类器权重会自发对齐形成单纯形等角紧框架(ETF)，但在长尾分布中，严重的样本不平衡阻碍了NC现象的出现，导致泛化性能下降。现有方法主要关注恢复ETF几何结构，但忽视了特征空间与分类器权重空间之间的严重错位问题。

Method: 首先通过最优误差指数分析从理论上量化了特征与分类器权重空间错位的危害。基于这一洞察，提出了三种显式的对齐策略，这些策略可以即插即用地集成到现有的长尾方法中，无需改变网络架构。

Result: 在CIFAR-10-LT、CIFAR-100-LT和ImageNet-LT数据集上的大量实验表明，提出的对齐策略能够持续提升现有基线方法的性能，并达到了最先进的性能水平。

Conclusion: 特征与分类器权重空间的对齐对于长尾学习至关重要，提出的三种对齐策略能够有效解决空间错位问题，显著提升长尾分类的性能，且具有即插即用的优势。

Abstract: Recent studies on Neural Collapse (NC) reveal that, under class-balanced conditions, the class feature means and classifier weights spontaneously align into a simplex equiangular tight frame (ETF). In long-tailed regimes, however, severe sample imbalance tends to prevent the emergence of the NC phenomenon, resulting in poor generalization performance. Current efforts predominantly seek to recover the ETF geometry by imposing constraints on features or classifier weights, yet overlook a critical problem: There is a pronounced misalignment between the feature and the classifier weight spaces. In this paper, we theoretically quantify the harm of such misalignment through an optimal error exponent analysis. Built on this insight, we propose three explicit alignment strategies that plug-and-play into existing long-tail methods without architectural change. Extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets consistently boost examined baselines and achieve the state-of-the-art performances.

</details>


### [22] [CarBench: A Comprehensive Benchmark for Neural Surrogates on High-Fidelity 3D Car Aerodynamics](https://arxiv.org/abs/2512.07847)
*Mohamed Elrefaie,Dule Shu,Matt Klenk,Faez Ahmed*

Main category: cs.LG

TL;DR: CarBench是首个针对大规模3D汽车空气动力学仿真的标准化基准，在最大的公开汽车空气动力学数据集DrivAerNet++（包含8000+高保真仿真）上评估了11种最先进模型。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模CFD数据集为机器学习在空气动力学和工程设计中的应用提供了新机会，但工程设计中缺乏大规模数值仿真的标准化基准，阻碍了该领域的进展。

Method: 在DrivAerNet++数据集上评估11种架构，包括神经算子方法、几何深度学习、基于Transformer的神经求解器和隐式场网络。除了标准插值任务，还进行跨类别实验，并分析预测准确性、物理一致性、计算效率和统计不确定性。

Result: 建立了首个可重现的大规模高保真CFD仿真学习基础，开源了基准框架、训练流程、基于自助重采样的不确定性估计例程和预训练模型权重。

Conclusion: CarBench为数据驱动工程提供了首个标准化基准，将加速从高保真CFD仿真中进行大规模学习的进展，填补了工程设计中大规模数值仿真基准的空白。

Abstract: Benchmarking has been the cornerstone of progress in computer vision, natural language processing, and the broader deep learning domain, driving algorithmic innovation through standardized datasets and reproducible evaluation protocols. The growing availability of large-scale Computational Fluid Dynamics (CFD) datasets has opened new opportunities for applying machine learning to aerodynamic and engineering design. Yet, despite this progress, there exists no standardized benchmark for large-scale numerical simulations in engineering design. In this work, we introduce CarBench, the first comprehensive benchmark dedicated to large-scale 3D car aerodynamics, performing a large-scale evaluation of state-of-the-art models on DrivAerNet++, the largest public dataset for automotive aerodynamics, containing over 8,000 high-fidelity car simulations. We assess eleven architectures spanning neural operator methods (e.g., Fourier Neural Operator), geometric deep learning (PointNet, RegDGCNN, PointMAE, PointTransformer), transformer-based neural solvers (Transolver, Transolver++, AB-UPT), and implicit field networks (TripNet). Beyond standard interpolation tasks, we perform cross-category experiments in which transformer-based solvers trained on a single car archetype are evaluated on unseen categories. Our analysis covers predictive accuracy, physical consistency, computational efficiency, and statistical uncertainty. To accelerate progress in data-driven engineering, we open-source the benchmark framework, including training pipelines, uncertainty estimation routines based on bootstrap resampling, and pretrained model weights, establishing the first reproducible foundation for large-scale learning from high-fidelity CFD simulations, available at https://github.com/Mohamedelrefaie/CarBench.

</details>


### [23] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: RaX-Crash是一个资源高效、可解释的小模型管道，用于预测纽约市机动车碰撞事故的伤害严重程度，使用树集成模型在结构化数据上表现优于小型语言模型。


<details>
  <summary>Details</summary>
Motivation: 纽约市每年报告超过10万起机动车碰撞事故，造成严重的伤害和公共卫生负担。需要开发资源高效且可解释的模型来预测伤害严重程度，以支持城市规模的伤害分析。

Method: 整合三个关联表（数千万条记录），构建统一特征模式的分区存储，训练紧凑的树集成模型（随机森林和XGBoost）在工程化表格特征上，并与本地部署的小型语言模型（使用文本摘要提示）进行比较。

Result: 在时间保留测试集上，XGBoost和随机森林分别达到0.7828和0.7794的准确率，明显优于小型语言模型（0.594和0.496）。类别不平衡分析显示简单类别加权可提高致命事故召回率，SHAP归因显示人类脆弱性因素、时间和地点是预测严重程度的主要驱动因素。

Conclusion: 可解释的小模型集成仍然是城市规模伤害分析的强基线，而将表格预测器与小型语言模型生成的叙述配对的混合管道可以在不牺牲可扩展性的情况下改善沟通效果。

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [24] [Bayesian Optimization for Function-Valued Responses under Min-Max Criteria](https://arxiv.org/abs/2512.07868)
*Pouya Ahadi,Reza Marzban,Ali Adibi,Kamran Paynabar*

Main category: cs.LG

TL;DR: 提出MM-FBO框架，针对函数响应进行贝叶斯优化，直接最小化函数域上的最大误差，而非传统集成误差方法。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化主要针对标量响应，但许多科学工程应用中响应是函数形式的（随时间或波长变化）。现有方法最小化集成误差，忽略了最坏情况偏差。

Method: 使用函数主成分分析表示函数响应，为PC分数构建高斯过程代理模型。提出集成不确定性获取函数，平衡最坏情况期望误差的利用和函数域上的探索。

Result: 在合成基准测试和物理案例研究（电磁散射、气相渗透）中，MM-FBO始终优于现有基线方法，证明了显式建模函数不确定性的重要性。

Conclusion: MM-FBO框架有效解决了函数响应贝叶斯优化问题，提供了理论保证并在实际应用中表现出色，强调了考虑最坏情况误差的重要性。

Abstract: Bayesian optimization is widely used for optimizing expensive black box functions, but most existing approaches focus on scalar responses. In many scientific and engineering settings the response is functional, varying smoothly over an index such as time or wavelength, which makes classical formulations inadequate. Existing methods often minimize integrated error, which captures average performance but neglects worst case deviations. To address this limitation we propose min-max Functional Bayesian Optimization (MM-FBO), a framework that directly minimizes the maximum error across the functional domain. Functional responses are represented using functional principal component analysis, and Gaussian process surrogates are constructed for the principal component scores. Building on this representation, MM-FBO introduces an integrated uncertainty acquisition function that balances exploitation of worst case expected error with exploration across the functional domain. We provide two theoretical guarantees: a discretization bound for the worst case objective, and a consistency result showing that as the surrogate becomes accurate and uncertainty vanishes, the acquisition converges to the true min-max objective. We validate the method through experiments on synthetic benchmarks and physics inspired case studies involving electromagnetic scattering by metaphotonic devices and vapor phase infiltration. Results show that MM-FBO consistently outperforms existing baselines and highlights the importance of explicitly modeling functional uncertainty in Bayesian optimization.

</details>


### [25] [SABER: Small Actions, Big Errors -- Safeguarding Mutating Steps in LLM Agents](https://arxiv.org/abs/2512.07850)
*Alejandro Cuadron,Pengfei Yu,Yang Liu,Arpit Gupta*

Main category: cs.LG

TL;DR: 研究发现LLM代理在长时程工具使用任务中，突变性操作（改变环境状态）的偏差对成功率影响巨大，而非突变性操作偏差影响很小。作者提出了CM框架，通过突变门控验证、针对性反思和上下文清理来提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM代理发展迅速，但在长时程工具使用任务上的表现仍然脆弱。为了理解这种脆弱性，研究者探究了不同操作类型对任务失败的影响差异，特别是突变性操作与非突变性操作的区别。

Method: 1. 分析τ-Bench和SWE-Bench的执行轨迹，将操作分解为突变性和非突变性步骤；2. 形式化"决定性偏差"概念；3. 使用逻辑回归分析偏差影响；4. 提出CM框架：突变门控验证、针对性反思、块状上下文清理；5. 创建τ-Bench Verified以解决基准测试中的天花板效应问题。

Result: 研究发现每个突变性操作偏差会使成功率降低高达92%（Airline）和96%（Retail），而非突变性操作偏差影响很小。CM框架显著提升性能：Qwen3-Thinking在Airline上相对提升28%，Retail上11%，SWE-Bench上7%；Claude模型也有9%/7%的提升。

Conclusion: 研究强调了对操作级别分析、针对性安全保障和可靠评估的需求，这些是构建鲁棒多轮代理的前提条件。提出的CM框架和τ-Bench Verified基准为未来研究提供了重要工具。

Abstract: Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \emph{do all actions contribute equally to failure?} Analyzing execution traces on $τ$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \emph{mutating} (environment-changing) vs.\ non-mutating steps and formalize \emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\%$ on Airline and upto $96\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\% \emph{relative} on Airline, +11\% on Retail, and +7\% on SWE-Bench Verified; Claude: +9\%/+7\%. We further identify ceiling effects in $τ$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $τ$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.

</details>


### [26] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: S2KAN提出了一种软符号化的Kolmogorov-Arnold网络，通过将符号基元直接集成到训练中，使用可学习的门控机制稀疏化表示，实现可解释的机器学习。


<details>
  <summary>Details</summary>
Motivation: 传统KANs虽然提供了可解释机器学习的路径，但其训练后的激活函数往往缺乏符号保真度，学习到的分解与可解释形式没有有意义的对应关系。

Method: 提出S2KAN，将符号基元直接集成到训练中。每个激活函数从符号项和密集项字典中提取，使用可学习的门控稀疏化表示。稀疏化是可微分的，支持端到端优化，并通过最小描述长度目标进行指导。

Result: 在符号基准测试、动态系统预测和真实世界预测任务中，S2KAN表现出竞争性或更优的准确性，且模型规模显著更小。观察到即使在无正则化压力下也出现了自稀疏化现象。

Conclusion: S2KAN能够在符号项足够时发现可解释形式，不足时优雅地退化为密集样条，实现了符号可解释性和模型性能的良好平衡。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [27] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 提出一个预测多模态模型GPU峰值内存使用的框架，通过分解模型架构和分析训练行为来准确预测内存需求


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统中深度学习模型的规模和复杂性增加，GPU内存需求经常超过可用容量，导致内存不足错误，这会中断训练并浪费计算资源。现有研究仅关注单模态架构，无法推广到多模态模型，而多模态模型在智能体AI系统中很常见。

Method: 提出一个框架，通过分析多模态模型的架构和训练行为来预测GPU峰值内存使用。具体方法是将多模态模型分解为组成层，并应用因子化来估计每层的内存使用。

Result: 评估显示该框架实现了约8.7%的平均MAPE（平均绝对百分比误差），具有较高的预测准确性。

Conclusion: 该框架能够准确预测多模态模型的GPU内存使用，有助于防止内存不足错误，提高训练效率和资源利用率。

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [28] [Fourier-Enhanced Recurrent Neural Networks for Electrical Load Time Series Downscaling](https://arxiv.org/abs/2512.07876)
*Qi Chen,Mihai Anitescu*

Main category: cs.LG

TL;DR: 提出一种傅里叶增强的循环神经网络，用于电力负荷降尺度预测，结合循环主干、傅里叶季节性嵌入和自注意力机制，在PJM四个区域表现优于传统基准方法。


<details>
  <summary>Details</summary>
Motivation: 电力负荷预测需要从低分辨率输入生成高分辨率预测，传统方法在捕获复杂季节模式和跨时间依赖关系方面存在局限，需要更有效的降尺度方法。

Method: 1) 使用循环神经网络作为主干处理低分辨率输入；2) 在潜在空间中融合显式的傅里叶季节性嵌入；3) 添加自注意力层捕获每个周期内高分辨率分量间的依赖关系。

Result: 在PJM四个区域测试中，该方法相比传统Prophet基准（有无季节性和LAA）以及去除注意力或傅里叶特征的RNN消融模型，均获得更低的RMSE和更平坦的预测误差曲线。

Conclusion: 傅里叶增强的循环神经网络结合自注意力机制能有效提升电力负荷降尺度预测精度，显式季节性建模和跨时间依赖捕获对预测性能有重要贡献。

Abstract: We present a Fourier-enhanced recurrent neural network (RNN) for downscaling electrical loads. The model combines (i) a recurrent backbone driven by low-resolution inputs, (ii) explicit Fourier seasonal embeddings fused in latent space, and (iii) a self-attention layer that captures dependencies among high-resolution components within each period. Across four PJM territories, the approach yields RMSE lower and flatter horizon-wise than classical Prophet baselines (with and without seasonality/LAA) and than RNN ablations without attention or Fourier features.

</details>


### [29] [HSTMixer: A Hierarchical MLP-Mixer for Large-Scale Traffic Forecasting](https://arxiv.org/abs/2512.07854)
*Yongyao Wang,Jingyuan Wang,Xie Yu,Jiahao Ji,Chao Li*

Main category: cs.LG

TL;DR: 提出HSTMixer框架，使用全MLP架构进行高效的大规模交通预测，通过分层时空混合块和多分辨率特征提取，以及自适应区域混合器动态捕捉不同区域的时空模式。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测模型在处理大规模真实世界交通网络时存在二次计算复杂度问题，不适用于实际大规模场景，需要更高效的解决方案。

Method: 提出分层时空混合器(HSTMixer)框架，采用全MLP架构，包含分层时空混合块进行自底向上聚合和自顶向下传播的多分辨率特征提取，以及自适应区域混合器根据区域语义生成变换矩阵动态捕捉时空模式。

Result: 在四个大规模真实世界数据集上的实验表明，该方法不仅达到了最先进的性能，而且展现出具有竞争力的计算效率。

Conclusion: HSTMixer框架为大规模交通预测提供了高效有效的解决方案，通过分层结构和自适应机制成功解决了现有模型的计算复杂度问题。

Abstract: Traffic forecasting task is significant to modern urban management. Recently, there is growing attention on large-scale forecasting, as it better reflects the complexity of real-world traffic networks. However, existing models often exhibit quadratic computational complexity, making them impractical for large-scale real-world scenarios. In this paper, we propose a novel framework, Hierarchical Spatio-Temporal Mixer (HSTMixer), which leverages an all-MLP architecture for efficient and effective large-scale traffic forecasting. HSTMixer employs a hierarchical spatiotemporal mixing block to extract multi-resolution features through bottom-up aggregation and top-down propagation. Furthermore, an adaptive region mixer generates transformation matrices based on regional semantics, enabling our model to dynamically capture evolving spatiotemporal patterns for different regions. Extensive experiments conducted on four large-scale real-world datasets demonstrate that the proposed method not only achieves state-of-the-art performance but also exhibits competitive computational efficiency.

</details>


### [30] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: LUNA提出了一种可学习的核化线性注意力机制，在保持线性计算成本的同时，达到甚至超越传统二次注意力机制的精度。


<details>
  <summary>Details</summary>
Motivation: 传统softmax注意力存在O(n²)二次计算成本，限制了长序列应用。现有线性注意力机制虽然降低到O(n)，但依赖固定的随机特征映射，导致模型精度和计算效率之间存在根本性权衡。

Method: LUNA的核心思想是学习核特征映射而非固定先验。通过参数化核函数，学习针对特定数据和任务的特征基，实现正定核和流式形式，获得序列长度的线性时间和内存扩展。

Result: 在Long Range Arena上，LUNA在计算对等下实现了高效Transformer中的最先进平均精度。在BERT和ViT-B/16的后验转换中，通过微调恢复了大部分原始性能，显著优于固定线性化方法。

Conclusion: LUNA消除了线性注意力中精度与效率的权衡，通过可学习的核特征映射实现了线性计算成本下的高性能，为长序列应用提供了有效的解决方案。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [31] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: LAPA提出了一种基于对数域注意力预测的算法-架构协同设计，用于跨阶段稀疏加速Transformer模型，显著提升能效。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在不同阶段的计算瓶颈随输入序列变化而动态变化，需要跨阶段稀疏加速策略。现有稀疏Transformer方法多为单阶段设计，其稀疏预测机制在多阶段应用中会产生显著的功耗开销。

Method: 1) 设计非对称前导一计算(ALOC)方案消除昂贵乘法运算；2) 提出混合精度多轮移位累加(MRSA)机制减少累加开销；3) 设计数据特征依赖滤波器(DDF)与MRSA协同工作；4) 设计专用加速器将理论提升转化为实际硬件改进。

Result: LAPA相比现有最优工作Spatten、Sanger和FACT分别实现了3.52倍、3.24倍和2.79倍的能效提升。

Conclusion: LAPA通过算法-架构协同设计有效解决了Transformer跨阶段稀疏加速问题，显著提升了能效，为动态计算瓶颈的Transformer模型提供了高效的硬件加速方案。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [32] [Complexity of One-Dimensional ReLU DNNs](https://arxiv.org/abs/2512.08091)
*Jonathan Kogan,Hayden Jananthan,Jeremy Kepner*

Main category: cs.LG

TL;DR: 本文研究了1D ReLU深度神经网络的表达能力，通过分析其线性区域数量。证明了随机初始化网络在无限宽度极限下，期望线性区域数量随网络总神经元数线性增长，并提出了函数自适应稀疏性概念。


<details>
  <summary>Details</summary>
Motivation: 理解深度神经网络的表达能力是深度学习理论研究的核心问题。通过分析ReLU网络的线性区域数量，可以量化网络的表达能力，这对于理解神经网络如何近似复杂函数具有重要意义。

Method: 研究一维全连接ReLU网络，采用He初始化（带非零偏置）。在无限宽度极限下，分析网络的线性区域数量。提出了函数自适应稀疏性概念，比较网络使用的期望区域数量与近似目标函数所需的最小区域数量。

Result: 证明了随机初始化网络在无限宽度极限下，期望线性区域数量增长为∑_{i=1}^L n_i + o(∑_{i=1}^L n_i) + 1，其中n_ℓ是第ℓ隐藏层的神经元数。这意味着线性区域数量随网络总神经元数线性增长。

Conclusion: 1D ReLU深度神经网络的表达能力可以通过线性区域数量来量化，其增长与网络总神经元数线性相关。提出的函数自适应稀疏性概念为评估网络效率提供了新视角，有助于理解神经网络如何有效利用其表达能力。

Abstract: We study the expressivity of one-dimensional (1D) ReLU deep neural networks through the lens of their linear regions. For randomly initialized, fully connected 1D ReLU networks (He scaling with nonzero bias) in the infinite-width limit, we prove that the expected number of linear regions grows as $\sum_{i = 1}^L n_i + \mathop{o}\left(\sum_{i = 1}^L{n_i}\right) + 1$, where $n_\ell$ denotes the number of neurons in the $\ell$-th hidden layer. We also propose a function-adaptive notion of sparsity that compares the expected regions used by the network to the minimal number needed to approximate a target within a fixed tolerance.

</details>


### [33] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: CLDD是一种基于图的深度学习模型，通过利用疾病间关联和患者间相似性进行协同学习，无需依赖医学检测即可预测数千种疾病。


<details>
  <summary>Details</summary>
Motivation: 疾病检测对医疗至关重要，但全面医学检测成本高昂且不切实际。需要一种方法能够在无需大量检测的情况下，同时预测数百甚至数千种疾病。

Method: 提出CLDD（疾病检测协同学习）模型，将疾病检测构建为协同学习任务。模型整合患者-疾病交互数据和人口统计学特征，通过图神经网络自适应地利用疾病间关联和患者间相似性。

Result: 在MIMIC-IV数据集（61,191患者，2,000疾病）上，CLDD在召回率提升6.33%，精确率提升7.63%，优于基准方法。案例研究显示模型能成功恢复掩蔽疾病，具有可解释性和可靠性。

Conclusion: CLDD通过降低诊断成本和提高可及性，有望用于大规模疾病筛查和社会健康保障，为无需依赖医学检测的疾病预测提供了有效解决方案。

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [34] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 提出一种考虑标签依赖关系的多标签数据采样算法，通过估计多元伯努利分布参数和计算标签组合权重，实现目标分布特性的加权采样，应用于生物医学文献数据集以增强少数类别的代表性。


<details>
  <summary>Details</summary>
Motivation: 多标签数据集中，标签通常非互斥且频率差异巨大，导致稀缺标签样本不足难以进行有效推断。传统采样方法难以在已知偏离总体频率的情况下，同时考虑标签间的依赖关系。

Method: 采用多元伯努利分布作为多标签问题的底层分布模型，利用观测到的标签频率估计分布参数，计算每个标签组合的权重，设计考虑标签依赖关系的加权采样算法。

Result: 将方法应用于Web of Science的64个生物医学主题类别标注的研究文章样本，成功保持了类别频率顺序，减少了最常⻅和最不常⻅类别间的频率差异，并考虑了类别依赖关系，生成了更平衡的子样本。

Conclusion: 提出的采样算法能够有效处理多标签数据中的标签依赖性和频率不平衡问题，显著增强了少数类别的代表性，为多标签数据集的平衡采样提供了有效解决方案。

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [35] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: SA^2GFM是一个鲁棒的图基础模型框架，通过结构感知语义增强提升领域自适应表示能力，在节点和图分类任务中优于9个SOTA基线。


<details>
  <summary>Details</summary>
Motivation: 当前图基础模型在各种任务中取得进展，但其对领域噪声、结构扰动和对抗攻击的鲁棒性研究不足，关键限制在于对层次结构语义的建模不够充分。

Method: 1. 通过将基于熵的编码树转化为结构感知文本提示来编码层次结构先验；2. 使用自监督信息瓶颈机制通过结构引导压缩蒸馏鲁棒可迁移表示；3. 引入专家自适应路由机制（混合专家架构+空专家设计）解决跨领域适应的负迁移问题；4. 提出微调模块通过联合社区内和社区间结构学习优化层次结构。

Result: 大量实验表明，SA^2GFM在节点和图分类任务中，针对随机噪声和对抗扰动的有效性和鲁棒性方面，优于9个最先进的基线方法。

Conclusion: SA^2GFM通过结构感知语义增强和自适应机制，显著提升了图基础模型的鲁棒性和领域自适应能力，为解决图模型在噪声和扰动环境下的泛化问题提供了有效方案。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [36] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: 本文改进了FedProxGrad在非凸复合优化问题中的收敛性分析，提出了DS FedProxGrad框架，证明了在Robbins-Monro步长调度下算法能渐进达到平稳点，消除了方差引起的噪声底限依赖。


<details>
  <summary>Details</summary>
Motivation: 现有FedProxGrad方法在解决组公平联邦学习的非凸复合优化问题时，只能收敛到噪声主导的平稳邻域，且收敛性明确依赖于方差引起的噪声底限。需要改进分析以获得更好的收敛特性。

Method: 提出DS FedProxGrad（衰减步长FedProxGrad）分析框架，采用Robbins-Monro步长调度，允许局部近似解的不精确性，并包含显式公平正则化。在局部不精确性满足温和衰减条件下进行分析。

Result: 证明了liminf_{r→∞} E[‖∇F(x^r)‖^2] = 0，即算法渐进达到平稳点，收敛速率不依赖于方差引起的噪声底限，改进了原有FedProxGrad的收敛结果。

Conclusion: DS FedProxGrad框架在Robbins-Monro步长调度下能够实现渐进平稳收敛，消除了对噪声底限的依赖，为组公平联邦学习中的非凸复合优化问题提供了更好的理论保证。

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [37] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: FAIM是一个轻量级的频率感知交互式Mamba模型，用于时间序列分类任务，通过自适应滤波块和交互式Mamba块实现高效的多粒度信息交互，在多个基准测试中优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在时间序列分类中虽然能捕捉时间依赖性，但存在计算成本高、对噪声扰动敏感、在小规模数据集上容易过拟合等问题。需要开发轻量级且鲁棒的模型来解决这些挑战。

Method: 提出FAIM模型，包含两个核心组件：1) 自适应滤波块(AFB)：利用傅里叶变换提取频域特征，采用可学习的自适应阈值动态抑制噪声，通过全局和局部语义自适应滤波的元素级耦合深入建模不同频率分量间的协同作用；2) 交互式Mamba块(IMB)：促进高效的多粒度信息交互，平衡细粒度判别特征提取和全面的全局上下文信息获取。此外，还加入了自监督预训练机制以增强对复杂时间模式的理解。

Result: 在多个基准测试上的广泛实验表明，FAIM始终优于现有的最先进方法，在准确性和效率之间实现了优越的权衡，并表现出出色的性能。

Conclusion: FAIM通过结合频域分析和交互式Mamba架构，为时间序列分类任务提供了一个轻量级、鲁棒且高效的解决方案，能够有效处理噪声和小规模数据集，在多个领域表现出色。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [38] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 提出基于拓扑数据分析的损失函数，用于无监督选择核密度估计的最优带宽


<details>
  <summary>Details</summary>
Motivation: 核密度估计的关键超参数——带宽选择需要人工调优，影响偏差-方差权衡。拓扑数据分析能数学量化高维数据的拓扑特征，为自动化带宽选择提供新思路。

Method: 使用基于拓扑的损失函数进行无监督学习，自动选择最优带宽。该方法利用拓扑数据分析量化密度估计的拓扑特征（如连通分量、环、空洞等）。

Result: 提出的方法在不同维度上与经典技术进行基准测试，展示了其潜力。

Conclusion: 基于拓扑的损失函数为核密度估计的带宽选择提供了一种有效的无监督自动化方法。

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [39] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: SetAD将半监督异常检测重构为集合级任务，通过注意力集合编码器和分级学习目标直接建模异常定义中的群体交互，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督异常检测方法主要关注单个点或简单点对，忽略了异常本质上是相对于群体的偏离，且未能利用集合组合提供的丰富监督信号，限制了高阶交互的学习能力。

Method: 提出SetAD框架：1) 将半监督异常检测重构为集合级任务；2) 使用注意力机制集合编码器；3) 采用分级学习目标训练模型学习整个集合的异常程度；4) 引入上下文校准的异常评分机制，通过多个不同上下文集合中点的标准化偏离来评估异常分数。

Result: 在10个真实世界数据集上的实验表明，SetAD显著优于最先进模型。特别值得注意的是，模型性能随着集合大小的增加而持续提升，为基于集合的异常检测公式提供了强有力的实证支持。

Conclusion: SetAD通过集合级视角重新定义异常检测，直接建模异常定义中的群体交互，提供了一种更符合异常本质的框架，在性能和可扩展性方面都表现出显著优势。

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [40] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 本文提出一个无监督机器学习框架，用于分析海关数据以监测环境条约执行情况，通过聚类、异常检测和启发式标记识别可疑贸易模式，生成优先级评分供监管部门审查。


<details>
  <summary>Details</summary>
Motivation: 需要新方法来监测环境条约（如蒙特利尔议定书）的执行情况，通过分析大规模、复杂的海关数据集来识别可疑贸易活动。

Method: 结合多种无监督机器学习技术：K-Means聚类发现贸易原型；隔离森林和IQR异常检测识别"超大贸易"和价格异常；启发式标记检测模糊描述等策略；将这些层组合成优先级评分。

Result: 应用于10万条贸易记录，成功识别出1,351个价格异常值和1,288个高优先级货物供海关审查。高优先级商品显示出与普通商品不同的价值重量比。模型成功检测到2021年初"超大贸易"激增，与美国AIM法案的监管影响直接相关。

Conclusion: 该工作提供了一个可重复的无监督学习流程，将原始贸易数据转化为优先排序、可用的监管情报，验证了模糊描述和高价值是主要风险预测因素，为监管机构提供了实用的分析工具。

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [41] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 将瑞典人口登记数据转化为文本化生命轨迹，解决分类变量高基数性和编码不一致问题，使用NLP模型预测居住流动性，证明文本化登记数据能有效支持复杂纵向分析


<details>
  <summary>Details</summary>
Motivation: 解决数据分析中的两个长期挑战：分类变量的高基数性和时间上编码方案的不一致性。利用瑞典全面的人口登记数据，探索如何将结构化登记数据转化为语义丰富的文本，以支持纵向预测分析

Method: 将690万个体（2001-2013年）的登记数据转化为文本化生命轨迹，包含人口统计信息和年度居住、工作、教育、收入、家庭状况变化。使用多种NLP架构（LSTM、DistilBERT、BERT、Qwen）预测个体后续年份（2013-2017年）的居住流动性

Result: 序列化和基于Transformer的模型比基线模型更有效地捕捉时间和语义结构。文本化登记数据保留了关于个体路径的有意义信息，支持复杂、可扩展的建模。该数据集为开发和评估新的序列建模方法提供了严格的测试平台

Conclusion: 将语义丰富的登记数据与现代语言模型相结合，可以显著推进社会科学中的纵向分析。这种方法在少数拥有可比覆盖范围和精度的纵向微观数据的国家中具有独特优势

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [42] [Command & Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing](https://arxiv.org/abs/2512.07866)
*Maria Milena Araujo Felix*

Main category: cs.LG

TL;DR: 本文提出基于深度学习和NLP的DGA域名检测方法，使用LSTM网络达到97.2%准确率，优于传统熵分析方法


<details>
  <summary>Details</summary>
Motivation: 现代恶意软件使用DGA技术生成大量动态域名，使基于静态黑名单的传统防火墙防御失效，需要更智能的检测方法

Method: 收集包含5万合法和5万恶意域名的混合数据库，提取词汇特征，训练LSTM循环神经网络进行检测

Result: 神经网络方法在检测复杂DGA模式上表现优越，达到97.2%准确率，在模糊合法流量场景中降低误报率

Conclusion: 深度学习结合NLP技术能有效检测DGA域名，特别在处理复杂模式时优于传统统计熵分析方法

Abstract: The sophistication of modern malware, specifically regarding communication with Command and Control (C2) servers, has rendered static blacklist-based defenses obsolete. The use of Domain Generation Algorithms (DGA) allows attackers to generate thousands of dynamic addresses daily, hindering blocking by traditional firewalls. This paper aims to propose and evaluate a method for detecting DGA domains using Deep Learning and Natural Language Processing (NLP) techniques. The methodology consisted of collecting a hybrid database containing 50,000 legitimate and 50,000 malicious domains, followed by the extraction of lexical features and the training of a Recurrent Neural Network (LSTM). Results demonstrated that while statistical entropy analysis is effective for simple DGAs, the Neural Network approach presents superiority in detecting complex patterns, reaching 97.2% accuracy and reducing the false positive rate in ambiguous lawful traffic scenarios.

</details>


### [43] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 提出基于混合专家(MoE)的噪声估计器，用于医学时间序列信号的扩散模型重建，通过RFAMoE模块自适应选择感受野，并通过Fusion MoE模块在单次推理中并行生成多个噪声信号进行融合，显著提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列信号具有多变量、高时间变异性、高噪声和易受伪影影响等独特特性，使得基于深度学习的插补等任务仍然具有挑战性。现有扩散模型方法在医学时间序列领域尚未充分探索，且多推理平均方法虽然能减少重建误差但计算成本高。

Method: 1. 提出基于混合专家(MoE)的噪声估计器，在基于分数的扩散框架中工作；2. 设计RFAMoE模块，使每个通道在扩散过程中自适应选择所需感受野；3. 设计Fusion MoE模块，利用MoE特性并行生成K个噪声信号，通过路由机制融合，在单次推理步骤中完成信号重建。

Result: 提出的框架在不同任务和数据集上持续优于基于扩散的SOTA方法，不仅提高了性能，还消除了多推理过程带来的显著计算成本和延迟开销。

Conclusion: 该研究成功开发了一种高效的医学时间序列信号重建框架，通过创新的MoE-based噪声估计器和并行融合机制，在保持高性能的同时大幅降低了计算复杂度，为医学时间序列分析提供了有效的解决方案。

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [44] [Controllable risk scenario generation from human crash data for autonomous vehicle testing](https://arxiv.org/abs/2512.07874)
*Qiujing Lu,Xuanhan Wang,Runze Yuan,Wei Lu,Xinyi Gong,Shuo Feng*

Main category: cs.LG

TL;DR: CRAG框架统一建模自动驾驶测试中的常规与风险行为，通过解耦潜在空间实现可控风险场景生成，提升AV测试效率


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆需要同时在日常驾驶和罕见安全关键场景下进行严格测试。现有方法难以同时模拟真实交通中的常规行为和符合真实事故的风险行为，特别是如何有效利用有限的碰撞数据来生成多样化的风险场景。

Method: 提出可控风险代理生成（CRAG）框架，构建结构化潜在空间解耦正常行为和风险相关行为。结合风险感知潜在表示和基于优化的模式转换机制，使代理能够在长时间范围内平滑、可信地从安全状态过渡到风险状态，同时在两种状态下保持高保真度。

Result: 大量实验表明，CRAG相比现有基线方法提高了多样性，同时能够实现风险场景的可控生成，支持针对性和高效的自动驾驶鲁棒性评估。

Conclusion: CRAG框架成功统一了常规主导行为和罕见安全关键行为的建模，通过解耦潜在空间有效利用有限碰撞数据，为自动驾驶系统的针对性鲁棒性测试提供了有效工具。

Abstract: Ensuring the safety of autonomous vehicles (AV) requires rigorous testing under both everyday driving and rare, safety-critical conditions. A key challenge lies in simulating environment agents, including background vehicles (BVs) and vulnerable road users (VRUs), that behave realistically in nominal traffic while also exhibiting risk-prone behaviors consistent with real-world accidents. We introduce Controllable Risk Agent Generation (CRAG), a framework designed to unify the modeling of dominant nominal behaviors and rare safety-critical behaviors. CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, enabling efficient use of limited crash data. By combining risk-aware latent representations with optimization-based mode-transition mechanisms, the framework allows agents to shift smoothly and plausibly from safe to risk states over extended horizons, while maintaining high fidelity in both regimes. Extensive experiments show that CRAG improves diversity compared to existing baselines, while also enabling controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.

</details>


### [45] [Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural Network Architectures for Design](https://arxiv.org/abs/2512.07877)
*Amogh Anshu N,Harish BP*

Main category: cs.LG

TL;DR: 提出基于机器学习的NoC设计空间探索框架，使用BookSim仿真和反向神经网络模型，比较MLP、条件扩散模型和CVAE三种架构，条件扩散模型在预测精度和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统NoC设计空间探索方法速度慢，难以处理复杂的非线性参数交互，需要更高效自动化的解决方案来满足严格的吞吐量和延迟要求。

Method: 构建机器学习驱动框架，使用BookSim仿真生成超过150,000个数据点，比较三种神经网络架构（MLP、条件扩散模型、CVAE）来预测给定目标性能指标下的最优NoC参数。

Result: 条件扩散模型在未见数据上达到最低均方误差（0.463），预测精度最高；该框架将设计探索时间减少数个数量级，实现快速可扩展的NoC协同设计。

Conclusion: 基于条件扩散模型的机器学习框架为NoC设计空间探索提供了高效实用的解决方案，显著提升设计效率，支持快速可扩展的NoC协同设计。

Abstract: Network-on-Chip (NoC) design requires exploring a high-dimensional configuration space to satisfy stringent throughput requirements and latency constraints.Traditional design space exploration techniques are often slow and struggle to handle complex, non-linear parameter interactions.This work presents a machine learning-driven framework that automates NoC design space exploration using BookSim simulations and reverse neural network models.Specifically, we compare three architectures - a Multi-Layer Perceptron (MLP),a Conditional Diffusion Model, and a Conditional Variational Autoencoder (CVAE) to predict optimal NoC parameters given target performance metrics.Our pipeline generates over 150,000 simulation data points across varied mesh topologies.The Conditional Diffusion Model achieved the highest predictive accuracy, attaining a mean squared error (MSE) of 0.463 on unseen data.Furthermore, the proposed framework reduces design exploration time by several orders of magnitude, making it a practical solution for rapid and scalable NoC co-design.

</details>


### [46] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: 提出SpecMatch-CL损失函数，通过最小化图嵌入构建的图-图结构的归一化拉普拉斯矩阵差异来改进图对比学习，在多个基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如InfoNCE）在优化图嵌入的跨视图对齐时，缺乏对由这些嵌入构建的图-图结构全局结构的控制机制。

Method: 提出SpecMatch-CL损失函数，通过最小化视图特定图-图结构的归一化拉普拉斯矩阵差异来对齐这些结构。

Result: 在8个TU基准测试的无监督学习和低标签率半监督学习中达到新的SOTA，在PPI-306K和ZINC 2M数据集上的迁移学习中也获得一致提升。

Conclusion: SpecMatch-CL通过控制图-图结构的全局对齐，有效改进了图对比学习性能，在多个任务和数据集上表现出优越性。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [47] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 论文提出Cone Collapse算法，从几何视角重新审视NMF，通过收缩非负象限来恢复数据的最小生成锥，并在此基础上构建锥感知正交NMF模型CC-NMF，在聚类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有NMF算法主要从优化视角出发，未能充分利用NMF诱导的锥几何结构。数据点位于凸锥中，其极射线编码了基本方向或"主题"。从几何视角重新审视NMF，可以更好地理解数据结构并改进聚类性能。

Method: 1. 提出Cone Collapse算法：从完整的非负象限开始，迭代收缩锥体直至数据的最小生成锥。2. 在恢复的极射线上应用单正交NMF，构建锥感知正交NMF模型CC-NMF。3. 证明在温和数据假设下，Cone Collapse在有限步内终止并恢复最小生成锥。

Result: 在16个基准基因表达、文本和图像数据集上，CC-NMF在聚类纯度方面一致匹配或优于强基线方法，包括乘法更新、ANLS、投影NMF、ONMF和稀疏NMF。恢复数据锥能产生理论上有依据且经验上强大的基于NMF的聚类方法。

Conclusion: 从几何视角重新审视NMF，通过显式恢复数据锥，可以开发出理论上有依据且经验性能优越的NMF聚类方法。锥几何结构为理解NMF提供了新视角，并带来了性能改进。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [48] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 论文提出CLOP损失函数，通过促进类别嵌入的正交线性子空间来防止对比学习中的维度塌缩问题，在图像分类和目标检测任务中表现出更好的性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 对比学习在深度学习中表现出色，但在半监督和自监督设置中面临维度塌缩的挑战，即嵌入收敛到低维空间。作者发现存在一个关键学习率阈值，超过该阈值标准对比损失会收敛到塌缩解。

Method: 提出CLOP损失函数，这是一种新颖的半监督损失函数，通过促进类别嵌入之间形成正交线性子空间来防止维度塌缩。该方法基于对学习率阈值的分析，旨在提高训练的稳定性。

Result: 在真实和合成数据集上的广泛实验表明，CLOP在图像分类和目标检测任务中提高了性能，同时在不同学习率和批量大小下表现出更大的稳定性。

Conclusion: CLOP通过防止维度塌缩有效解决了对比学习中的关键问题，为半监督学习提供了更稳定和性能更好的解决方案，特别是在需要处理不同学习率和批量大小的实际应用中。

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [49] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: GSPN-2是GSPN的改进版本，通过算法-系统联合重设计解决了原实现中的GPU内核启动开销、内存传输和冗余计算问题，在保持精度的同时显著提升了效率。


<details>
  <summary>Details</summary>
Motivation: 现有GSPN实现虽然将自注意力计算复杂度从二次降低到接近线性，但仍存在GPU内核重复启动开销大、全局内存数据传输过多、以及每个通道维护独立传播权重导致的冗余计算等问题，限制了实际应用效率。

Method: 1. 系统优化：将数千个微内核启动合并为单个2D内核，为每个通道切片固定一个warp，将前一列的激活值暂存到共享内存；2. 算法改进：引入紧凑通道传播策略，替代每通道独立矩阵，减少参数，并与transformer注意力中的亲和力图自然对齐。

Result: 在图像分类和文本到图像合成任务中，GSPN-2能够匹配transformer级别的精度，同时显著降低计算成本，为视觉应用中的全局空间上下文建模建立了新的效率边界。

Conclusion: GSPN-2通过独特的结构化矩阵变换和GPU优化实现的结合，为高分辨率图像和长视频相关应用提供了高效的全局空间上下文建模方案，在保持精度的同时大幅提升了计算效率。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [50] [ByteStorm: a multi-step data-driven approach for Tropical Cyclones detection and tracking](https://arxiv.org/abs/2512.07885)
*Davide Donno,Donatello Elia,Gabriele Accarino,Marco De Carlo,Enrico Scoccimarro,Silvio Gualdi*

Main category: cs.LG

TL;DR: ByteStorm：基于深度学习和计算机视觉的热带气旋追踪框架，无需阈值调优，在东西北太平洋盆地表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统热带气旋追踪方法主要依赖主观阈值，这可能导致在不同地理区域应用时产生偏差。需要一种更准确、无需阈值调优的数据驱动方法

Method: 提出ByteStorm框架：1）使用深度学习网络（分类和定位）检测TC中心，仅需850mb相对涡度和平均海平面气压数据；2）通过BYTE算法将检测到的中心连接成TC轨迹

Result: 在东西北太平洋盆地评估显示：检测概率（ENP 85.05%，WNP 79.48%）、误报率（ENP 23.26%，WNP 16.14%）、年际变率相关性（ENP 0.75，WNP 0.69），均优于现有确定性追踪器

Conclusion: ByteStorm展示了深度学习和计算机视觉在快速准确TC追踪中的潜力，为传统方法提供了稳健的替代方案

Abstract: Accurate tropical cyclones (TCs) tracking represents a critical challenge in the context of weather and climate science. Traditional tracking schemes mainly rely on subjective thresholds, which may introduce biases in their skills on the geographical region of application. We present ByteStorm, an efficient data-driven framework for reconstructing TC tracks without threshold tuning. It leverages deep learning networks to detect TC centers (via classification and localization), using only relative vorticity (850 mb) and mean sea-level pressure. Then, detected centers are linked into TC tracks through the BYTE algorithm. ByteStorm is evaluated against state-of-the-art deterministic trackers in the East- and West-North Pacific basins (ENP and WNP). The proposed framework achieves superior performance in terms of Probability of Detection ($85.05\%$ ENP, $79.48\%$ WNP), False Alarm Rate ($23.26\%$ ENP, $16.14\%$ WNP), and high Inter-Annual Variability correlations ($0.75$ ENP and $0.69$ WNP). These results highlight the potential of integrating deep learning and computer vision for fast and accurate TC tracking, offering a robust alternative to traditional approaches.

</details>


### [51] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: Brush是一种将决策树分割算法与非线性常数优化相结合的符号回归方法，能够将基于规则的逻辑无缝集成到符号回归和分类模型中，在医疗决策中表现出色。


<details>
  <summary>Details</summary>
Motivation: 医疗决策经常使用结合风险方程和规则的算法，但传统符号回归仅限于连续函数形式，难以建模这种决策过程。然而，符号回归具有数据驱动和可解释性的优势，有望用于开发数据驱动的临床风险评分系统。

Method: Brush算法结合了决策树式的分割算法和非线性常数优化，允许将基于规则的逻辑无缝集成到符号回归和分类模型中。

Result: Brush在SRBench上实现了帕累托最优性能，成功复现了两个广泛使用的临床评分系统，获得了高准确性和可解释模型。与决策树、随机森林和其他符号回归方法相比，Brush实现了相当或更优的预测性能，同时产生更简单的模型。

Conclusion: Brush通过结合规则逻辑和符号回归，为医疗决策提供了数据驱动且可解释的建模方法，在保持高预测性能的同时生成更简单的模型，具有临床应用潜力。

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [52] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: CIP-Net：一种无需示例存储的自解释原型模型，用于持续学习，在保持高性能的同时提供可解释性并显著降低内存开销。


<details>
  <summary>Details</summary>
Motivation: 持续学习中存在灾难性遗忘问题，现有可解释方法大多使用事后解释或需要为每个新任务存储额外内存，导致可扩展性有限。需要一种既无需存储过去示例又能提供有用解释的实用可解释解决方案。

Method: 提出CIP-Net，一种无需示例的自解释原型模型。它避免存储过去示例，保持简单架构，同时提供有用解释。基于原型的方法在预测时生成解释，帮助保留知识。

Result: CIP-Net在任务增量学习和类别增量学习设置中，相比之前的无示例和自解释方法实现了最先进的性能，同时显著降低了内存相关开销。

Conclusion: CIP-Net为持续学习提供了一个实用且可解释的解决方案，既避免了示例存储，又保持了高性能和可解释性，在性能和内存效率之间取得了良好平衡。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [53] [HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability](https://arxiv.org/abs/2512.07988)
*Sudhanva Manjunath Athreya,Paul Rosen*

Main category: cs.LG

TL;DR: HOLE方法通过持续同调分析深度神经网络，利用拓扑特征和可视化工具解释模型表示，评估表示质量、层间可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在许多领域取得了显著成功，但其学习到的表示和决策过程仍然很大程度上是不透明且难以解释的。需要新的方法来分析和理解神经网络的内部工作机制。

Method: HOLE（潜在嵌入的同调观察）方法通过持续同调从神经激活中提取拓扑特征，并使用桑基图、热图、树状图和斑点图等可视化技术呈现这些特征，便于跨层检查表示结构和质量。

Result: 在标准数据集上使用多种判别模型进行评估，结果表明拓扑分析能够揭示与类别分离、特征解缠和模型鲁棒性相关的模式，为理解深度学习系统提供了补充视角。

Conclusion: HOLE方法通过拓扑分析为深度神经网络的可解释性提供了有价值的工具，能够揭示模型内部表示的结构特征，有助于理解和改进深度学习系统。

Abstract: Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces HOLE (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. HOLE extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate HOLE on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.

</details>


### [54] [Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis](https://arxiv.org/abs/2512.07992)
*Aaron D. Mullen,Daniel R. Harris,Svetla Slavova,V. K. Cody Bumgardner*

Main category: cs.LG

TL;DR: 开发了一个面向医疗研究者和临床医生的网络平台，简化时间序列预测流程，支持数据可视化、模型训练和结果解释，并集成大语言模型提供参数建议


<details>
  <summary>Details</summary>
Motivation: 时间序列预测在医疗等领域有广泛应用，但技术门槛高，需要专业知识分析数据、构建模型和解释结果，这限制了研究人员和临床医生使用这些技术

Method: 开发了一个网络平台，用户可上传数据、生成可视化图表，平台支持多种可高度定制的预测模型和训练技术，并集成大语言模型提供参数选择建议和结果解释

Result: 创建了一个使数据分析和预测建模过程对非技术用户可访问的平台，支持数据可视化、模型训练和结果解释，并计划整合到学习型医疗系统中

Conclusion: 该平台降低了时间序列预测的技术门槛，使研究人员和临床医生能够更轻松地应用预测技术，未来计划整合到学习型医疗系统中实现临床数据的持续收集和推理

Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.

</details>


### [55] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: 在ICU临床决策中，研究比较了多目标强化学习与单目标方法，发现基于决策变换器的PEDA DT算法在离线学习环境下提供更优的灵活性和适应性。


<details>
  <summary>Details</summary>
Motivation: ICU临床决策面临生存率最大化与资源利用最小化的冲突目标，传统单目标强化学习方法使用固定标量化奖励函数，导致策略僵化无法适应变化的临床优先级。

Method: 在MIMIC-IV数据集上，对三种离线多目标强化学习算法（CPQL、Adaptive CPQL、PEDA DT）与三种标量化单目标基线方法（BC、CQL、DDQN）进行基准测试，使用离线策略评估指标。

Result: PEDA DT算法相比静态标量化基线展现出更优的灵活性，同时验证了序列建模架构在扩展到多目标条件生成时仍保持鲁棒性和有效性。

Conclusion: 离线多目标强化学习是实现个性化、可调整重症监护决策的有前景框架，无需重新训练即可适应不同临床优先级。

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [56] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: CLARITY是一个医学世界模型，通过结构化潜在空间预测疾病演化，整合时间间隔和患者特定数据，生成个体化治疗计划，并在胶质瘤数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前肿瘤学临床决策需要预测动态疾病演化，但现有静态AI预测器无法完成此任务。现有医学世界模型方法通常忽略患者特定的时间和临床上下文，缺乏将预测与治疗决策连接的反馈机制。

Method: CLARITY在结构化潜在空间中直接预测疾病演化，明确整合时间间隔（时间上下文）和患者特定数据（临床上下文），将治疗条件进展建模为平滑、可解释的轨迹，并引入新颖的预测到决策框架，将潜在推演转化为透明、可操作的建议。

Result: CLARITY在治疗规划方面展示了最先进的性能。在MU-Glioma-Post数据集上，该方法比最近的MeWM提高了12%，并显著超越所有其他医学特定的大型语言模型。

Conclusion: CLARITY通过整合时间和临床上下文，在结构化潜在空间中生成生理上可信的个体化治疗计划，为肿瘤学临床决策提供了有效的动态疾病演化预测框架。

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [57] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 提出Deep Kernel Aalen-Johansen (DKAJ)模型，将经典Aalen-Johansen估计器推广到深度学习框架，通过自动学习的核函数衡量数据点相似性，提供可解释的竞争风险分析。


<details>
  <summary>Details</summary>
Motivation: 竞争风险分析中需要可解释的深度学习模型。经典Aalen-Johansen估计器虽然可解释但缺乏灵活性，而现有深度学习模型虽然灵活但缺乏可解释性。需要结合两者优势。

Method: 提出DKAJ模型，将每个数据点表示为多个簇的加权组合。权重通过自动学习的核函数计算，衡量数据点之间的相似性。当数据点仅对一个簇有非零权重时，其预测CIF对应于该簇的经典Aalen-Johansen估计。

Result: 在四个标准竞争风险数据集上，DKAJ与最先进的基线方法竞争性相当，同时能够提供可视化辅助模型解释。

Conclusion: DKAJ成功地将经典Aalen-Johansen估计器推广到深度学习框架，在保持预测性能的同时提供了更好的可解释性，通过可视化帮助理解模型决策过程。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [58] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 提出因果引导的多模态域泛化框架，通过对抗解耦和统一表示学习提升社交媒体危机分类在未见灾难类型上的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体危机分类方法在未见灾难类型上泛化能力差，原因包括：1) 未能解耦虚假特征和因果特征，导致域偏移时性能下降；2) 未能对齐异构模态表示，阻碍单模态域泛化技术向多模态场景的迁移

Method: 提出因果引导的多模态域泛化框架，结合对抗解耦和统一表示学习。对抗目标促使模型解耦并关注域不变的因果特征；统一表示将不同模态特征对齐到共享潜在空间，使单模态域泛化策略能无缝扩展到多模态学习

Result: 在不同数据集上的实验表明，该方法在未见灾难场景中取得了最佳性能

Conclusion: 通过因果引导的多模态域泛化框架，有效解决了社交媒体危机分类在未见灾难类型上的泛化挑战，提升了紧急响应中的情境感知能力

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [59] [Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders](https://arxiv.org/abs/2512.08077)
*Jaron Cohen,Alexander G. Hasson,Sara Tanovic*

Main category: cs.LG

TL;DR: 该研究将稀疏自编码器技术应用于化学语言模型，揭示了模型内部编码的丰富化学概念，包括结构基序、物理化学性质和药物类别等。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型在药物和材料发现等高风险应用中的使用，机器学习可解释性变得日益紧迫。虽然化学语言模型在分子性质预测和生成方面表现出色，但其内部如何表示化学知识仍不清楚。

Method: 将稀疏自编码器技术扩展到化学语言模型，应用于FM4M SMI-TED化学基础模型，提取语义上有意义的潜在特征，并分析其在多样化分子数据集上的激活模式。

Result: 发现这些模型编码了丰富的化学概念景观，识别出特定潜在特征与不同化学知识领域（包括结构基序、物理化学性质和药理学药物类别）之间的相关性。

Conclusion: 该方法为揭示化学AI系统中的潜在知识提供了通用框架，对基础理解和实际部署都有重要意义，有望加速计算化学研究。

Abstract: Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.

</details>


### [60] [Training LLMs for Honesty via Confessions](https://arxiv.org/abs/2512.08093)
*Manas Joglekar,Jeremy Chen,Gabriel Wu,Jason Yosinski,Jasmine Wang,Boaz Barak,Amelia Glaese*

Main category: cs.LG

TL;DR: 提出一种通过"忏悔"机制让大语言模型诚实报告自身缺陷的方法，在训练中单独奖励诚实的忏悔，而不影响主回答的奖励，从而激励模型诚实披露不当行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在报告自身行为和信念时可能存在不诚实问题，如夸大事实信心或掩盖不当行为。这种不诚实可能源于强化学习中的奖励塑造问题，训练过程可能无意中激励模型说谎或掩饰行为。

Method: 提出"忏悔"机制：在模型给出原始回答后，要求其提供忏悔输出，完整说明其遵守政策和指令的情况。训练中单独奖励诚实的忏悔，且不影响主回答的奖励。只要最大化忏悔奖励的最简单途径是披露不当行为而非掩盖，就能激励模型在忏悔中保持诚实。

Result: 在GPT-5-Thinking上训练忏悔机制，评估其在分布外场景中的诚实性，包括幻觉、指令遵循、阴谋和奖励攻击。发现当模型在主回答中说谎或隐瞒缺陷时，通常会在忏悔中诚实承认这些行为，且忏悔的诚实性随训练适度提升。忏悔机制支持多种推理时干预，包括监控、拒绝采样和向用户呈现问题。

Conclusion: 忏悔机制是促使大语言模型诚实报告自身缺陷的有效方法，通过单独奖励诚实忏悔而不影响主回答奖励，能够激励模型披露不当行为，为模型监控和干预提供了新途径。

Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.

</details>


### [61] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: MAC提出了一种基于模型的离线强化学习方法，通过动作块模型减少长期预测误差，结合拒绝采样防止模型利用，在复杂长时域任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决基于模型的强化学习在离线设置下处理复杂长时域任务时的挑战：传统基于模型的价值扩展方法在增加预测步长n时会减少价值引导的偏差，但会放大模型误差的累积，导致长期预测质量下降。

Method: 提出MAC方法：1）使用动作块模型预测动作序列（而非单个动作）对应的未来状态，减少复合误差；2）采用拒绝采样从表达性强的行为动作块策略中采样，防止模型利用分布外动作。

Result: 在包含高达1亿个转移的大规模数据集上的实验表明，MAC在离线基于模型的强化学习算法中表现最佳，特别是在具有挑战性的长时域任务上。

Conclusion: MAC通过动作块模型和拒绝采样的组合，为基于模型的强化学习提供了可扩展的解决方案，能够有效处理复杂的长时域离线强化学习任务。

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [62] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges -- Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 论文提出在评估大语言模型时，使用Balanced Accuracy（平衡准确率）来选择分类器（如LLM-as-a-judge或人工标注者），比传统指标如准确率、精确率、F1分数更可靠，能提供更稳健的模型比较。


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型时，依赖分类器（如LLM-as-a-judge或人工标注者）来估计期望/不期望行为的普遍性。传统评估指标（准确率、精确率、F1分数）对类别不平衡和正类选择敏感，可能导致选择那些扭曲普遍性估计的分类器，影响模型比较的可信度。

Method: 提出使用Youden's J统计量作为选择最佳分类器的理论依据，并证明Balanced Accuracy是J的等价线性变换。通过理论分析和实证研究（包括实际案例和模拟实验），验证了基于Balanced Accuracy选择分类器的优势。

Result: 研究表明，使用Balanced Accuracy选择分类器能获得更好、更稳健的分类器选择，从而提高大语言模型评估的可靠性。该方法对类别不平衡不敏感，避免了传统指标在模型比较中的偏差问题。

Conclusion: 在大语言模型评估中，应优先使用Balanced Accuracy（或等价的Youden's J统计量）来选择分类器，而不是传统的准确率、精确率、F1分数，以确保模型比较的可靠性和鲁棒性。

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [63] [Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach](https://arxiv.org/abs/2512.08124)
*Zijiang Yang*

Main category: cs.LG

TL;DR: 提出一种基于机器学习的加密货币组合管理方法，通过分析加密货币间的相对关系而非独立预测，利用神经网络预测未来收益排名并进行权重分配，在3.5年市场周期中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注单一加密货币（如比特币）的价格预测并进行交易，忽略了加密货币之间的相互关系。本文旨在通过分析加密货币间的相对关系来管理一组加密货币，以获取更好的投资组合表现。

Method: 在每个时间步，使用神经网络预测所管理加密货币的未来收益排名，并根据排名分配权重。这种方法利用了横截面信息，而不是独立预测每个加密货币的绝对价格走势。

Result: 在2020年5月至2023年11月的真实日频加密货币市场数据上进行回测，涵盖牛市、熊市和震荡市的完整周期。该方法在复杂市场条件下表现优异，夏普比率为1.01，年化收益率为64.26%，且对交易费用增加具有鲁棒性。

Conclusion: 通过分析加密货币间的相对关系而非独立预测，提出的机器学习组合管理方法在加密货币市场中表现优异，能够适应不同市场条件，为加密货币投资组合管理提供了新思路。

Abstract: This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.

</details>


### [64] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 提出CSO方法，通过抑制内在特征来增强后门检测的敏感性，解决传统方法在易区分类别和隐蔽后门攻击中的失效问题


<details>
  <summary>Details</summary>
Motivation: 传统后门检测方法依赖目标类表现出极端异常检测统计量，但在两类情况下会失效：1）某些非目标类本身容易与其他类区分，自然获得极端统计量；2）后门特征相对内在特征较弱时。需要更敏感的检测方法。

Method: 提出类子空间正交化（CSO）方法：利用少量干净样本，通过约束优化问题，在优化检测统计量的同时与类的内在特征正交化，从而抑制内在特征贡献，突出后门触发器的贡献。

Result: CSO方法能够有效检测具有挑战性的混合标签攻击和自适应攻击，相比传统方法具有更高的检测敏感性。

Conclusion: 通过抑制内在特征来增强后门检测统计量的方法（CSO）能够更敏感地检测隐蔽后门攻击，特别是在传统方法容易失效的场景下表现优异。

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [65] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 论文提出了首个生物威胁基准生成框架（BBG），专注于评估AI模型（特别是大语言模型）在细菌生物威胁方面的安全风险，采用分层结构和任务对齐查询方法。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型（尤其是大语言模型）的快速发展，需要量化并减轻这些模型可能被用于生物恐怖主义或获取生物武器的风险。当前缺乏能够全面评估模型生物安全风险的基准测试方法。

Method: 开发了生物威胁基准生成框架（BBG），采用分层结构（生物威胁类别、要素和任务），构建了细菌生物威胁模式（Bacterial Biothreat Schema），通过任务对齐查询来评估模型风险。

Result: 提出了BBG框架的第一个组件——细菌生物威胁模式，该框架能够可靠测量和评估AI模型的生物安全风险提升和潜在危害，考虑了不同行为者能力水平和操作风险因素。

Conclusion: BBG框架（包括细菌生物威胁模式）为评估大语言模型在细菌生物风险方面提供了稳健、可重用的结构，能够全面捕捉生物对手的技术和操作要求，并考虑广泛的能力范围。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [66] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: 该论文通过开放性和多智能体学习方法，开发能够泛化到新环境、分布外输入和与其他智能体交互的鲁棒AI代理，涵盖强化学习环境和大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各种应用中的普及，需要能够成功导航和适应不断变化的开放世界的智能代理。关键挑战是确保这些AI代理具有鲁棒性，不仅在训练期间熟悉的设置中表现出色，还能有效地泛化到以前未见过的多样化场景。

Method: 1. 引入MiniHack：基于NetHack游戏的沙盒框架，通过程序内容生成创建多样化环境；2. 提出Maestro：生成对抗性课程的方法，逐步增强RL代理在两人零和游戏中的鲁棒性和泛化能力；3. 利用质量多样性方法系统识别预训练RL策略在足球游戏领域的漏洞；4. 使用进化搜索生成多样化有效输入，诊断和增强LLM对抗对抗性提示的鲁棒性。

Result: 开发了能够创建多样化任务的MiniHack框架，提出了增强RL代理鲁棒性的Maestro方法，系统识别了多智能体领域中的策略漏洞，并建立了评估LLM对抗性鲁棒性的方法。

Conclusion: 这项工作为AI鲁棒性的未来发展铺平了道路，使得开发不仅能够适应不断变化的世界，而且能够在面对不可预见的挑战和交互时茁壮成长的智能代理成为可能。

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [67] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: PolyLingua是一个轻量级Transformer模型，用于语言识别和细粒度语言分类，通过两级对比学习框架实现高精度，在计算和延迟受限环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 语言识别是多语言系统的关键第一步，现有工具在音乐请求等关键场景中表现不佳，开源工具准确率低，大语言模型成本高且不适合低延迟/低资源环境。

Method: 采用轻量级Transformer架构，结合实例级分离和类级对齐的两级对比学习框架，使用自适应边界，为相近语言生成紧凑且分离良好的嵌入表示。

Result: 在Amazon Massive和Song数据集上分别达到99.25%和98.15%的F1分数，超越Sonnet 3.5，参数量减少10倍，适合计算和延迟受限环境。

Conclusion: PolyLingua在保持轻量级的同时实现了高精度语言识别，特别适合处理音乐请求等具有代码转换的复杂场景，为多语言系统提供了高效解决方案。

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases -- such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets -- Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching) -- PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [68] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO是一种新颖的强化学习框架，通过将去噪过程重构为搜索树，显著提高生成模型对齐的训练效率，实现2.4倍加速训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练对于将生成模型与人类偏好对齐至关重要，但其高昂的计算成本阻碍了广泛采用。现有方法在样本效率和信用分配方面存在局限性。

Method: 将去噪过程重构为搜索树，从共享的初始噪声样本出发，策略性地分支生成多个候选轨迹，同时高效重用它们的共同前缀。通过树结构实现奖励反向传播计算步骤特定优势，并通过多子分支实现每次前向传递多个策略更新。

Result: 在扩散和基于流的模型上实验表明，TreeGRPO实现2.4倍训练加速，在效率-奖励权衡空间中建立更优的帕累托前沿，在多个基准和奖励模型上一致优于GRPO基线。

Conclusion: TreeGRPO为基于强化学习的视觉生成模型对齐提供了可扩展且有效的途径，通过树结构方法显著提高了训练效率和性能。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [69] [LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks](https://arxiv.org/abs/2512.08160)
*Nanda K. Unnikrishnan,Keshab K. Parhi*

Main category: cs.LG

TL;DR: LayerPipe2 为神经网络的流水线训练提供了理论框架，通过延迟梯度适应和重定时技术，推导出各层所需的延迟量，并解决了历史权重存储瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 之前的LayerPipe方法虽然通过重叠前向和反向计算加速了训练，但缺乏对梯度延迟量的理论理解。本文旨在填补这一空白，为流水线训练提供理论基础。

Method: 使用变量延迟梯度适应和重定时技术，正式推导LayerPipe框架。分析网络结构中延迟的合法插入位置，开发流水线感知移动平均方法，通过重建而非存储历史状态来减少内存消耗。

Result: 建立了理论框架，能够预测各层延迟需求：内部层需要较少延迟，外部层需要较长延迟；当每层都流水线化时，延迟量仅取决于下游阶段数量；当层分组流水线化时，组内所有层共享相同的延迟分配。

Conclusion: LayerPipe2提供了一个原则性框架，指导如何构建LayerPipe架构、预测延迟需求并减轻存储负担，从而实现可扩展的流水线训练，并控制通信与计算的权衡。

Abstract: In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.

</details>


### [70] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: MobileFineTuner：首个开源框架，支持在普通手机上直接进行端到端LLM微调，通过参数分片、梯度累积等系统级优化解决移动设备内存和能耗限制。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公共数据接近枯竭，设备端微调成为利用私有用户数据同时保护隐私的重要途径。现有方法主要基于模拟或依赖IoT设备和PC，普通手机领域尚未充分探索，缺乏开源框架支持实际LLM微调。

Method: 提出MobileFineTuner统一开源框架，支持全参数微调和参数高效微调。引入系统级优化：参数分片、梯度累积、能量感知计算调度，以应对手机内存和能耗限制。

Result: 在真实手机上成功微调GPT-2、Gemma 3和Qwen 2.5模型。大量实验和消融研究验证了所提优化的有效性，证明MobileFineTuner可作为设备端LLM训练研究的可行基础。

Conclusion: MobileFineTuner填补了普通手机端LLM微调的开源框架空白，通过系统级优化解决了移动设备限制，为未来设备端LLM训练研究提供了实用基础。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [71] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 该论文挑战了传统AdamW中权重衰减与学习率γ成比例(∝γ)的假设，提出权重衰减应与γ²成比例(∝γ²)，基于稳态时权重范数稳定的理论推导，并验证了这种设置能改善训练动态和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统AdamW优化器中，解耦权重衰减(decoupled weight decay)一直被默认设置为与学习率γ成比例，但这一假设缺乏理论依据。作者旨在重新审视这一设置，探索更合理的权重衰减配置方式。

Method: 通过理论分析，作者推导出在稳态条件下，解耦权重衰减应与学习率的平方(γ²)成比例，而非传统的一阶关系。基于"更新在稳态时与权重无关"的假设，分析了权重范数的稳定性。同时研究了Scion优化器中minibatch的总更新贡献(TUC)特性。

Result: 理论推导表明权重衰减∝γ²能实现稳定的权重和梯度范数。实验验证了这种设置能更好地控制训练动态，提升模型性能。同时发现Scion优化器的TUC特性由动量相关的有效学习率决定，其最优值具有可迁移性。

Conclusion: 传统AdamW中权重衰减∝γ的假设需要修正，权重衰减∝γ²能提供更稳定的训练动态和更好的模型性能。这一发现为优化器设计提供了新的理论指导。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [72] [PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning](https://arxiv.org/abs/2512.08218)
*Ye Qin,Jingchao Wang,Yang Shi,Haiying Huang,Junxu Li,Weijian Liu,Tinghui Chen,Jinghui Qin*

Main category: cs.LG

TL;DR: PR-CapsNet将胶囊网络扩展到伪黎曼流形，通过自适应曲率路由解决复杂图结构表示问题，在节点和图分类任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统胶囊网络在固定曲率空间中建模复杂图结构存在局限性，而伪黎曼流形为图嵌入提供了更好的归纳偏置，但如何将其与胶囊网络结合尚未充分探索。

Method: 提出伪黎曼胶囊网络(PR-CapsNet)：1) 伪黎曼切空间路由将胶囊状态分解为球面-时间和欧几里得-空间子空间；2) 自适应曲率路由通过可学习曲率张量融合不同曲率空间特征；3) 保持几何特性的伪黎曼胶囊分类器使用曲率加权softmax进行分类。

Result: 在节点和图分类基准测试中，PR-CapsNet优于现有最先进模型，验证了其对复杂图结构的强大表示能力。

Conclusion: PR-CapsNet成功将胶囊网络扩展到伪黎曼流形，通过自适应曲率路由有效建模复杂图结构，为图表示学习提供了新的强大框架。

Abstract: Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\-world graphs poorly by fixed\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\-Euclidean pseudo\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\-Riemannian manifolds and derive a Pseudo\-Riemannian Capsule Network (PR\-CapsNet), which models data in pseudo\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\-CapsNet enhances the CapsNet with Adaptive Pseudo\-Riemannian Tangent Space Routing by utilizing pseudo\-Riemannian geometry. Unlike single\-curvature or subspace\-partitioning methods, PR\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\-Riemannian metric. It first deploys Pseudo\-Riemannian Tangent Space Routing to decompose capsule states into spherical\-temporal and Euclidean\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\-preserved Pseudo\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\-CapsNet outperforms SOTA models, validating PR\-CapsNet's strong representation power for complex graph structures.

</details>


### [73] [Persistent Topological Structures and Cohomological Flows as a Mathematical Framework for Brain-Inspired Representation Learning](https://arxiv.org/abs/2512.08241)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Shipra Prashant*

Main category: cs.LG

TL;DR: 提出基于持久拓扑结构与上同调流的脑启发表示学习数学框架，将神经计算重构为动态单纯复形上的上链映射演化，实现跨时空和功能状态的表示学习。


<details>
  <summary>Details</summary>
Motivation: 为脑启发表示学习建立严格的数学基础，通过拓扑结构捕捉大脑状态的不变量，解决现有方法在流形一致性和噪声鲁棒性方面的不足。

Method: 将代数拓扑与微分几何结合，构建上同调算子，在动态单纯复形上实现上链映射演化，使用持久同调、层上同调和谱拉普拉斯分析合成与真实神经数据。

Result: 模型在流形一致性和噪声鲁棒性方面优于图神经网络和基于流形的深度架构，验证了拓扑驱动表示学习的有效性。

Conclusion: 建立了拓扑驱动表示学习的连贯数学基础，为脑启发计算提供了严格的代数拓扑框架，展示了拓扑结构在表示学习中的核心作用。

Abstract: This paper presents a mathematically rigorous framework for brain-inspired representation learning founded on the interplay between persistent topological structures and cohomological flows. Neural computation is reformulated as the evolution of cochain maps over dynamic simplicial complexes, enabling representations that capture invariants across temporal, spatial, and functional brain states. The proposed architecture integrates algebraic topology with differential geometry to construct cohomological operators that generalize gradient-based learning within a homological landscape. Synthetic data with controlled topological signatures and real neural datasets are jointly analyzed using persistent homology, sheaf cohomology, and spectral Laplacians to quantify stability, continuity, and structural preservation. Empirical results demonstrate that the model achieves superior manifold consistency and noise resilience compared to graph neural and manifold-based deep architectures, establishing a coherent mathematical foundation for topology-driven representation learning.

</details>


### [74] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: SPROCKET是一种基于原型的新型时间序列特征工程方法，通过原型选择实现随机卷积核变换，在多数UCR/UEA数据集上性能与现有卷积算法相当，其MR-HY-SP集成模型超越了之前最佳的卷积集成HYDRA-MR。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类算法主要依赖特征工程策略，其中ROCKET通过随机核特征取得了优异性能。本文旨在探索基于原型的特征工程策略，以进一步提升时间序列分类的准确性和鲁棒性。

Method: 提出SPROCKET（Selected Prototype Random Convolutional Kernel Transform）方法，采用基于原型的特征工程策略，通过原型选择实现随机卷积核变换。还构建了MR-HY-SP集成模型（MultiROCKET-HYDRA-SPROCKET）。

Result: 在大多数UCR和UEA时间序列分类数据集上，SPROCKET性能与现有卷积算法相当。MR-HY-SP集成模型的平均准确率排名超过了之前最佳的卷积集成HYDRA-MR。

Conclusion: 基于原型的特征变换能够提升时间序列分类的准确性和鲁棒性，SPROCKET方法为时间序列分类提供了新的有效特征工程策略。

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [75] [Wavelet-Accelerated Physics-Informed Quantum Neural Network for Multiscale Partial Differential Equations](https://arxiv.org/abs/2512.08256)
*Deepak Gupta,Himanshu Pandey,Ratikanta Behera*

Main category: cs.LG

TL;DR: 提出基于小波的物理信息量子神经网络框架，用于高效求解具有尖锐梯度、刚度、快速局部变化和高频振荡的多尺度偏微分方程，相比传统方法参数更少、收敛更快。


<details>
  <summary>Details</summary>
Motivation: 传统物理信息神经网络(PINNs)及其量子版本(量子PINNs)在求解多尺度特征时面临挑战，且依赖自动微分构建损失函数导致计算开销大、训练时间长。

Method: 开发小波加速的物理信息量子神经网络，结合小波的多分辨率特性与量子神经网络架构，无需自动微分，显著降低计算复杂度，增强网络捕捉多尺度问题局部和全局特征的能力。

Result: 数值实验表明，该方法相比经典小波PINNs仅需不到5%的可训练参数，收敛更快；相比现有量子PINNs提速3-5倍，在求解挑战性多尺度振荡问题上表现出优越精度。

Conclusion: 提出的基于小波的物理信息量子神经网络框架能高效解决具有尖锐梯度、刚度、快速局部变化和高频振荡的多尺度偏微分方程，在参数效率和计算速度方面均有显著优势。

Abstract: This work proposes a wavelet-based physics-informed quantum neural network framework to efficiently address multiscale partial differential equations that involve sharp gradients, stiffness, rapid local variations, and highly oscillatory behavior. Traditional physics-informed neural networks (PINNs) have demonstrated substantial potential in solving differential equations, and their quantum counterparts, quantum-PINNs, exhibit enhanced representational capacity with fewer trainable parameters. However, both approaches face notable challenges in accurately solving multiscale features. Furthermore, their reliance on automatic differentiation for constructing loss functions introduces considerable computational overhead, resulting in longer training times. To overcome these challenges, we developed a wavelet-accelerated physics-informed quantum neural network that eliminates the need for automatic differentiation, significantly reducing computational complexity. The proposed framework incorporates the multiresolution property of wavelets within the quantum neural network architecture, thereby enhancing the network's ability to effectively capture both local and global features of multiscale problems. Numerical experiments demonstrate that our proposed method achieves superior accuracy while requiring less than five percent of the trainable parameters compared to classical wavelet-based PINNs, resulting in faster convergence. Moreover, it offers a speedup of three to five times compared to existing quantum PINNs, highlighting the potential of the proposed approach for efficiently solving challenging multiscale and oscillatory problems.

</details>


### [76] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 提出一个统一的几何-随机多模态深度学习框架，整合EEG、ECG、呼吸、SpO2、EMG和fMRI信号，用于建模癫痫猝死和卒中易感性。


<details>
  <summary>Details</summary>
Motivation: 癫痫猝死和急性缺血性卒中是危及生命的疾病，涉及皮层、脑干和自主神经系统的复杂相互作用。需要一种统一的多模态方法来早期检测和风险分层。

Method: 结合黎曼流形嵌入、李群不变特征表示、分数随机动力学、哈密顿能量流建模和跨模态注意力机制。使用分数流行病扩散在结构脑图上建模卒中传播。

Result: 在MULTI-CLARID数据集上展示了改进的预测准确性，并获得了可解释的生物标志物，包括流形曲率、分数记忆指数、注意力熵和扩散中心性。

Conclusion: 该框架为神经自主神经疾病的早期检测、风险分层和可解释多模态建模提供了数学原理基础。

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [77] [Mathematical Foundations of Neural Tangents and Infinite-Width Networks](https://arxiv.org/abs/2512.08264)
*Rachana Mysore,Preksha Girish,Kavitha Jayaram,Shrey Kumar,Preksha Girish,Shravan Sanjeev Bagal,Kavitha Jayaram,Shreya Aravind Shastry*

Main category: cs.LG

TL;DR: 提出NTK-ECRN架构，结合傅里叶特征嵌入、残差连接和随机深度，用于分析无限宽度神经网络训练中的核演化，建立理论框架连接无限宽度理论与实际深度学习架构。


<details>
  <summary>Details</summary>
Motivation: 研究无限宽度神经网络的理论基础，特别是神经正切核(NTK)的数学性质，旨在建立理论分析与实际深度学习架构之间的桥梁。

Method: 提出NTK-ECRN架构，整合傅里叶特征嵌入、带层间缩放的残差连接和随机深度，理论推导NTK动态边界，分析特征值演化，连接谱性质与泛化优化稳定性。

Result: 在合成和基准数据集上验证了预测的核行为，展示了改进的训练稳定性和泛化性能，为无限宽度理论与实际深度学习架构提供了综合分析框架。

Conclusion: 该工作提供了一个全面的理论框架，连接无限宽度神经网络理论与实际深度学习架构，通过NTK-ECRN架构实现了对训练过程中核演化的严格分析。

Abstract: We investigate the mathematical foundations of neural networks in the infinite-width regime through the Neural Tangent Kernel (NTK). We propose the NTK-Eigenvalue-Controlled Residual Network (NTK-ECRN), an architecture integrating Fourier feature embeddings, residual connections with layerwise scaling, and stochastic depth to enable rigorous analysis of kernel evolution during training. Our theoretical contributions include deriving bounds on NTK dynamics, characterizing eigenvalue evolution, and linking spectral properties to generalization and optimization stability. Empirical results on synthetic and benchmark datasets validate the predicted kernel behavior and demonstrate improved training stability and generalization. This work provides a comprehensive framework bridging infinite-width theory and practical deep-learning architectures.

</details>


### [78] [SOFA-FL: Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing](https://arxiv.org/abs/2512.08267)
*Yi Ni,Xinkun Wang,Han Zhang*

Main category: cs.LG

TL;DR: SOFA-FL是一个自组织分层联邦学习框架，通过动态聚类、拓扑重构和自适应数据共享来解决数据异构性和固定网络拓扑问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在动态环境中面临数据异构性和固定网络拓扑的挑战，现有方法缺乏适应性和灵活性。

Method: 提出SOFA-FL框架，包含三个核心机制：1) DMAC动态多分支凝聚聚类构建初始层次结构；2) SHAPE自组织层次自适应传播与演化，通过嫁接、剪枝、合并和净化操作动态重构拓扑；3) 自适应聚类数据共享，在客户端和集群节点间进行受控的部分数据交换。

Result: SOFA-FL能够有效捕捉客户端间的动态关系，增强个性化能力，无需依赖预定的集群结构。

Conclusion: SOFA-FL为动态环境下的联邦学习提供了一个自适应的分层框架，解决了数据异构性和拓扑僵化问题。

Abstract: Federated Learning (FL) faces significant challenges in evolving environments, particularly regarding data heterogeneity and the rigidity of fixed network topologies. To address these issues, this paper proposes \textbf{SOFA-FL} (Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing), a novel framework that enables hierarchical federated systems to self-organize and adapt over time.
  The framework is built upon three core mechanisms: (1) \textbf{Dynamic Multi-branch Agglomerative Clustering (DMAC)}, which constructs an initial efficient hierarchical structure; (2) \textbf{Self-organizing Hierarchical Adaptive Propagation and Evolution (SHAPE)}, which allows the system to dynamically restructure its topology through atomic operations -- grafting, pruning, consolidation, and purification -- to adapt to changes in data distribution; and (3) \textbf{Adaptive Clustered Data Sharing}, which mitigates data heterogeneity by enabling controlled partial data exchange between clients and cluster nodes.
  By integrating these mechanisms, SOFA-FL effectively captures dynamic relationships among clients and enhances personalization capabilities without relying on predetermined cluster structures.

</details>


### [79] [gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs](https://arxiv.org/abs/2512.08274)
*Humera Sabir,Fatima Farooq,Ashraf Aboulnaga*

Main category: cs.LG

TL;DR: gHAWK：通过预计算局部和全局结构特征来提升大规模知识图谱上GNN训练效率的框架


<details>
  <summary>Details</summary>
Motivation: 现有消息传递GNN在大规模知识图谱上扩展性差，因为迭代消息传递过程效率低，尤其在mini-batch训练中节点只能看到部分邻域视图

Method: 提出gHAWK框架，在GNN训练开始前预计算节点结构特征：1) Bloom过滤器编码局部邻域结构；2) TransE嵌入表示节点全局位置；然后将这些特征与领域特定特征融合，生成可用于任何GNN技术的节点特征向量

Result: 在Open Graph Benchmark大型数据集上的实验表明，gHAWK在节点属性预测和链接预测任务上达到最先进精度，训练时间更短，在三个图谱上位居OGB排行榜首位

Conclusion: 通过结构先验增强消息传递训练，gHAWK显著减少内存使用、加速收敛并提高模型精度，为大规模知识图谱上的GNN训练提供了高效解决方案

Abstract: Knowledge Graphs (KGs) are a rich source of structured, heterogeneous data, powering a wide range of applications. A common approach to leverage this data is to train a graph neural network (GNN) on the KG. However, existing message-passing GNNs struggle to scale to large KGs because they rely on the iterative message passing process to learn the graph structure, which is inefficient, especially under mini-batch training, where a node sees only a partial view of its neighborhood. In this paper, we address this problem and present gHAWK, a novel and scalable GNN training framework for large KGs. The key idea is to precompute structural features for each node that capture its local and global structure before GNN training even begins. Specifically, gHAWK introduces a preprocessing step that computes: (a)~Bloom filters to compactly encode local neighborhood structure, and (b)~TransE embeddings to represent each node's global position in the graph. These features are then fused with any domain-specific features (e.g., text embeddings), producing a node feature vector that can be incorporated into any GNN technique. By augmenting message-passing training with structural priors, gHAWK significantly reduces memory usage, accelerates convergence, and improves model accuracy. Extensive experiments on large datasets from the Open Graph Benchmark (OGB) demonstrate that gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs.

</details>


### [80] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: JARF通过计算随机森林预测的梯度，构建全局线性预条件器来旋转特征空间，使轴对齐决策树能处理旋转的决策边界，同时保持训练简单性。


<details>
  <summary>Details</summary>
Motivation: 轴对齐决策树在处理旋转或特征交互依赖的决策边界时表现不佳，而倾斜森林虽然能解决这个问题但计算成本高且实现复杂。需要一种既能处理复杂边界又能保持简单性的方法。

Method: 首先训练轴对齐森林来估计类别概率或回归输出，计算预测相对于每个特征的有限差分梯度，聚合这些梯度形成期望雅可比外积（扩展EGOP），将其作为全局线性预条件器旋转特征空间，然后在变换后的数据上重新训练标准轴对齐森林。

Result: 在表格分类和回归基准测试中，这种预条件处理方法持续改进了轴对齐森林的性能，通常匹配或超越倾斜基线方法，同时提高了训练时间。

Conclusion: 监督预条件处理能够恢复倾斜森林的大部分准确性，同时保持轴对齐树的简单性和鲁棒性，为处理复杂决策边界提供了一种高效实用的解决方案。

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [81] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 提出一种名为MAN的联邦学习正则化方法，通过最小化客户端模型各层激活范数来约束优化问题的平坦性，从而提高联邦学习模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习框架中，多个客户端在服务器协调下协作训练全局模型，但现有方法可能导致模型收敛到"尖锐最小值"，从而损害模型的泛化能力。

Method: 提出平坦性约束的联邦学习优化问题，通过约束训练损失Hessian矩阵的最大特征值来实现。进一步提出MAN正则化技术，在客户端侧最小化各层激活范数，理论上证明这能降低客户端损失函数的层间Hessian最大特征值。

Result: 将提出的平坦性约束优化应用于现有联邦学习技术，获得了显著改进，建立了新的最先进性能。

Conclusion: MAN正则化方法通过约束平坦性有效提高了联邦学习模型的泛化性能，理论分析和实验结果都验证了其有效性。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [82] [Fully Decentralized Certified Unlearning](https://arxiv.org/abs/2512.08443)
*Hithem Lamri,Michail Maniatakos*

Main category: cs.LG

TL;DR: RR-DU：一种用于去中心化网络的认证遗忘方法，通过随机游走结合投影梯度上升/下降、子采样高斯噪声和信任区域投影，提供网络遗忘认证和删除容量界限。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在从训练模型中移除特定数据的影响以响应隐私请求或数据中毒。虽然认证遗忘已在集中式和服务器协调的联邦设置中分析，但去中心化设置（无协调器的对等通信）仍未充分探索。

Method: 提出RR-DU：在去中心化网络中，遗忘客户端在遗忘集上执行一次投影梯度上升步骤，其他客户端在保留数据上执行几何分布的投影下降步骤，结合子采样高斯噪声和投影到原始模型周围的信任区域。

Result: 在凸情况下提供收敛保证，在非凸情况下提供平稳性保证；通过子采样高斯Rényi DP提供客户端视图的(ε,δ)网络遗忘认证；删除容量界限随遗忘与本地数据比率缩放。在MNIST和CIFAR-10上，RR-DU在给定(ε,δ)下比去中心化DP基线获得更高测试精度，并将遗忘准确率降至随机猜测水平（约10%）。

Conclusion: RR-DU是首个在去中心化网络中提供认证遗忘的方法，通过随机游走机制有效平衡隐私与效用，在保持模型性能的同时实现可靠的数据遗忘。

Abstract: Machine unlearning (MU) seeks to remove the influence of specified data from a trained model in response to privacy requests or data poisoning. While certified unlearning has been analyzed in centralized and server-orchestrated federated settings (via guarantees analogous to differential privacy, DP), the decentralized setting -- where peers communicate without a coordinator remains underexplored. We study certified unlearning in decentralized networks with fixed topologies and propose RR-DU, a random-walk procedure that performs one projected gradient ascent step on the forget set at the unlearning client and a geometrically distributed number of projected descent steps on the retained data elsewhere, combined with subsampled Gaussian noise and projection onto a trust region around the original model. We provide (i) convergence guarantees in the convex case and stationarity guarantees in the nonconvex case, (ii) $(\varepsilon,δ)$ network-unlearning certificates on client views via subsampled Gaussian Rényi DP (RDP) with segment-level subsampling, and (iii) deletion-capacity bounds that scale with the forget-to-local data ratio and quantify the effect of decentralization (network mixing and randomized subsampling) on the privacy-utility trade-off. Empirically, on image benchmarks (MNIST, CIFAR-10), RR-DU matches a given $(\varepsilon,δ)$ while achieving higher test accuracy than decentralized DP baselines and reducing forget accuracy to random guessing ($\approx 10\%$).

</details>


### [83] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process](https://arxiv.org/abs/2512.08451)
*Gary Ackerman,Zachary Kallenborn,Anna Wetzel,Hayley Peterson,Jenna LaTourette,Olivia Shoemaker,Brandon Behlendorf,Sheriff Almakki,Doug Clifford,Noah Sheinbaum*

Main category: cs.LG

TL;DR: 该论文介绍了细菌生物威胁基准（B3）数据集的构建，这是生物威胁基准生成（BBG）框架的第二部分，旨在评估AI模型在生物安全风险方面的表现。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型（特别是大语言模型）可能被用于生物恐怖主义或获取生物武器，这引起了政策、学术和公众的广泛关注。需要开发能够评估模型生物安全风险的基准测试。

Method: 采用三种互补方法生成基准：1）基于网络的提示生成，2）红队测试，3）挖掘现有基准语料库。通过去重、提升诊断性评估和质量控制，从7000多个候选基准中筛选出1010个最终基准。

Result: 成功构建了包含1010个基准的B3数据集，这些基准具有：a）提供提升的诊断性，b）与生物安全威胁直接相关，c）与更大的生物安全架构对齐，支持不同层次的分析。

Conclusion: B3数据集为评估AI模型的生物安全风险提供了系统化的基准测试工具，有助于模型开发者和政策制定者量化并减轻相关风险。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.

</details>


### [84] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: 论文介绍了细菌生物威胁基准（B3）数据集的试点实施，这是生物威胁基准生成（BBG）框架的第三部分，通过前沿AI模型测试和人工评估，验证了B3能有效评估LLM的生物安全风险。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型（特别是大语言模型）可能被用于生物恐怖主义或获取生物武器，这引发了政策、学术和公众的广泛担忧。模型开发者和政策制定者需要量化并减轻这些风险，因此需要开发能够评估模型生物安全风险的基准测试。

Method: 论文描述了细菌生物威胁基准（B3）数据集的试点实施，这是生物威胁基准生成（BBG）框架的第三部分。试点包括：1）在样本前沿AI模型上运行基准测试；2）人工评估模型响应；3）从多个维度对结果进行应用风险分析。

Result: 试点结果表明，B3数据集提供了一种可行且细致的方法，能够快速评估LLM的生物安全风险，识别风险的主要来源，并为优先缓解领域提供指导。

Conclusion: B3基准测试框架能够有效评估前沿AI模型的生物安全风险，为模型开发者和政策制定者提供了量化风险、识别关键风险源和确定优先缓解措施的重要工具。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [85] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: 提出结合fMRI数据和DICOM元数据的Transformer多模态框架，用于脑状态解码，提升准确性、可解释性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习方法未能充分利用DICOM元数据提供的丰富上下文信息，限制了fMRI脑状态解码的性能

Method: 采用基于Transformer的架构，整合fMRI数据和DICOM元数据作为多模态输入，利用注意力机制捕捉复杂的时空模式和上下文关系

Result: 提出的框架增强了模型的准确性、可解释性和鲁棒性，在临床诊断、认知神经科学和个性化医学中具有应用潜力

Conclusion: 该多模态Transformer框架有效利用了DICOM元数据的上下文信息，虽然存在元数据变异性和计算需求等限制，但为脑状态解码提供了新方向

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [86] [Solving Over-Smoothing in GNNs via Nonlocal Message Passing: Algebraic Smoothing and Depth Scalability](https://arxiv.org/abs/2512.08475)
*Weiqi Guan,Junlin He*

Main category: cs.LG

TL;DR: 提出一种新的层归一化方法，解决GNN中预LN架构的深度诅咒和后LN架构的过平滑问题，支持更深的网络（达256层）且无需额外参数。


<details>
  <summary>Details</summary>
Motivation: 层归一化（LN）放置位置与过平滑现象之间的关系尚未充分探索。预LN架构避免过平滑但遭受深度诅咒，而后LN架构绕过深度诅咒但经历过平滑，需要解决这一关键困境。

Method: 基于后LN架构提出新方法，诱导代数平滑，防止过平滑同时避免深度诅咒。该方法参数高效，无需额外参数。

Result: 在五个基准测试上的实证结果显示，该方法支持更深的网络（达256层）并提升性能，无需额外参数。

Conclusion: 提出的方法成功解决了LN放置的困境，通过诱导代数平滑同时避免了过平滑和深度诅咒问题，使GNN能够扩展到更深的架构。

Abstract: The relationship between Layer Normalization (LN) placement and the over-smoothing phenomenon remains underexplored. We identify a critical dilemma: Pre-LN architectures avoid over-smoothing but suffer from the curse of depth, while Post-LN architectures bypass the curse of depth but experience over-smoothing.
  To resolve this, we propose a new method based on Post-LN that induces algebraic smoothing, preventing over-smoothing without the curse of depth. Empirical results across five benchmarks demonstrate that our approach supports deeper networks (up to 256 layers) and improves performance, requiring no additional parameters.
  Key contributions:
  Theoretical Characterization: Analysis of LN dynamics and their impact on over-smoothing and the curse of depth.
  A Principled Solution: A parameter-efficient method that induces algebraic smoothing and avoids over-smoothing and the curse of depth.
  Empirical Validation: Extensive experiments showing the effectiveness of the method in deeper GNNs.

</details>


### [87] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 提出一种针对离线强化学习的全局预算分配攻击策略，通过TD误差分配扰动预算，相比均匀扰动更高效且隐蔽


<details>
  <summary>Details</summary>
Motivation: 现有离线RL数据投毒攻击采用局部均匀扰动，对所有样本不加区分，效率低下且缺乏隐蔽性，需要更智能的攻击策略

Method: 基于TD误差理论洞察，将攻击建模为全局资源分配问题，推导出在L2约束下扰动幅度与TD误差敏感性成正比的闭式解

Result: 在D4RL基准测试中显著优于基线策略，仅用最小扰动就能实现高达80%的性能下降，并能逃避最先进的统计和频谱防御检测

Conclusion: 提出的全局预算分配攻击策略比传统均匀扰动更高效、更隐蔽，揭示了离线RL系统在智能攻击下的脆弱性

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [88] [Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction](https://arxiv.org/abs/2512.08499)
*Waleed Razzaq,Yun-Bo Zhao*

Main category: cs.LG

TL;DR: 提出两种距离感知的不确定性方法PG-SNGP和PG-SNER，用于旋转机械轴承退化预测，通过谱归一化和物理引导神经网络提高OOD泛化能力和不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性方法在安全关键系统（如滚动轴承）的预测性维护中存在不足：缺乏置信度校准、计算成本高、不具备距离感知能力、在分布外数据下泛化能力差。

Method: 1) PG-SNGP：基于谱归一化高斯过程，在隐藏层应用谱归一化保持输入到潜在空间的距离，用高斯过程层替换最终密集层；2) PG-SNER：基于深度证据回归，输出正态逆伽马参数建模不确定性；3) 设计动态加权损失平衡数据保真度和物理一致性；4) 使用基于皮尔逊相关系数的新距离感知度量评估性能。

Result: 在PRONOSTIA轴承退化数据集上测试，PG-SNGP和PG-SNER相比蒙特卡洛和深度集成PGNNs提高了预测精度，在OOD条件下可靠泛化，对对抗攻击和噪声保持鲁棒性。

Conclusion: 提出的距离感知不确定性方法PG-SNGP和PG-SNER能够准确估计轴承退化，提供校准的不确定性，在分布外条件下可靠泛化，适用于安全关键系统的预测性维护。

Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.

</details>


### [89] [A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks](https://arxiv.org/abs/2512.08567)
*Nader Sadek,Mirette Moawad,Christina Naguib,Mariam Elzahaby*

Main category: cs.LG

TL;DR: 该论文提出了一种结合公司新闻和历史股价数据的多模态方法，使用图神经网络（GNN）预测股票市场走势，相比传统LSTM基线模型取得了更好的预测性能。


<details>
  <summary>Details</summary>
Motivation: 股票市场预测是金融领域的长期挑战，传统模型主要依赖历史价格数据，但金融新闻能提供有用的外部信号。研究旨在探索如何有效整合新闻文章和历史股价数据来改进预测性能。

Method: 采用多模态方法：1）使用LSTM编码每家公司的历史股价数据；2）使用语言模型嵌入新闻标题；3）构建包含文章、公司和行业节点的异质图；4）使用GraphSAGE模型捕捉节点间的交互关系。评估两个预测目标：二元方向变化标签和基于显著性的标签。

Result: 在美国股票和彭博数据集上的实验表明：1）GNN模型优于LSTM基线，在第一个目标上达到53%准确率，在第二个目标上获得4%精确度提升；2）拥有更多相关新闻的公司预测准确率更高；3）新闻标题比完整文章包含更强的预测信号。

Conclusion: 整合新闻文章和历史股价数据的多模态方法能有效提升股票市场预测性能，特别是使用图神经网络捕捉跨实体的复杂关系。简洁的新闻摘要在短期市场反应中扮演重要角色，为金融预测提供了新的研究方向。

Abstract: Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.

</details>


### [90] [Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset](https://arxiv.org/abs/2512.08591)
*Charles Rios,Longzhen Han,Almas Baimagambetov,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: 本文构建了一个覆盖2004-05至2024-25赛季的纵向NBA数据集，并提出了一个LSTM深度学习框架，利用长达8个赛季的9840场比赛序列来预测NBA比赛结果，取得了72.35%的准确率，优于传统机器学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有NBA比赛预测模型面临概念漂移、时间上下文有限和跨赛季不稳定的问题，需要更强大的预测方法来支持教练策略、球迷参与和体育博彩。

Method: 构建了覆盖20个赛季的纵向NBA数据集，提出了基于LSTM的深度学习框架，使用长达9840场比赛（相当于8个完整赛季）的序列来捕捉球队动态演变和赛季间依赖关系。

Result: LSTM模型在所有指标上表现最佳：准确率72.35%，精确率73.15%，AUC-ROC 76.13%，显著优于逻辑回归、随机森林、MLP和CNN等基线模型。

Conclusion: 长序列时间建模对篮球结果预测至关重要，新构建的多赛季数据集为开发稳健、可泛化的NBA预测系统提供了重要价值。

Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.

</details>


### [91] [An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals](https://arxiv.org/abs/2512.08699)
*Chenglong Duan,Dazhong Wu*

Main category: cs.LG

TL;DR: 提出基于动态时间规整和迁移学习的框架，用于增材制造零件认证，通过将低成本聚合物的应力-应变行为知识迁移到金属材料


<details>
  <summary>Details</summary>
Motivation: 增材制造零件认证需要准确预测复杂应力-应变行为，但金属材料数据获取成本高，而聚合物数据相对容易获得，因此希望通过迁移学习将聚合物知识迁移到金属材料

Method: 开发DTW-TL框架：使用动态时间规整选择与目标金属最相关的聚合物数据集作为源域，采用LSTM模型，利用四种聚合物（尼龙、PLA、CF-ABS、树脂）和三种金属（AlSi10Mg、Ti6Al4V、碳钢）验证框架有效性

Result: DTW-TL框架成功识别聚合物与金属之间的最佳匹配，选择单一聚合物作为源域；相比无迁移学习的LSTM模型和基于四种聚合物的迁移学习模型，DTW-TL模型在三种金属上获得最低平均绝对百分比误差（12.41%）和最高决定系数（0.96）

Conclusion: DTW-TL框架能有效利用低成本聚合物数据提升金属增材制造零件应力-应变行为预测精度，为增材制造零件认证提供高效解决方案

Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.

</details>


### [92] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: 提出Bias-Guided Prompt Search (BGPS)框架，自动生成能最大化文本到图像生成模型中偏见的提示词，发现现有去偏方法忽略的细微偏见。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型存在社会偏见，现有去偏方法依赖人工或LLM构建的提示词数据集，成本高且可能忽略未预料到的偏见触发提示。

Method: BGPS包含两个组件：1)生成属性中性提示词的LLM；2)作用于TTI内部表示的属性分类器，引导LLM解码过程向能放大目标图像属性的提示词空间区域。

Result: 在Stable Diffusion 1.5和先进去偏模型上发现大量细微且未记录的偏见，严重降低公平性指标，发现的提示词可解释且可被普通用户使用。

Conclusion: BGPS揭示了TTI模型的脆弱性，扩展了偏见搜索空间，可作为偏见缓解的新评估工具。

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [93] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 该论文提出使用神经常微分方程(NODEs)作为动态框架，从时间序列多组学数据中学习蛋白质组和代谢组之间的复杂相互作用，在预测生物系统行为方面相比传统方法有显著改进。


<details>
  <summary>Details</summary>
Motivation: 虽然高通量多组学数据日益丰富，但将这些数据转化为可操作的预测模型仍然是一个瓶颈。需要高容量、数据驱动的模拟系统来从观测数据中直接推断潜在相互作用，以支持个性化医疗和合成生物学中的下游干预效果预测。

Method: 引入神经常微分方程(NODEs)作为动态框架，应用于工程化大肠杆菌菌株的时间序列数据，模拟代谢通路的连续动态。

Result: NODE架构在捕捉系统动态方面表现优异，在柠檬烯和异戊烯醇通路数据集上，均方根误差相比基线改善超过90%（分别达94.38%和97.65%），推理时间加速1000倍。

Conclusion: NODE模型为下一代代谢工程和生物发现提供了可扩展、高保真的工具，能够有效预测复杂生物系统的行为。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [94] [Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning](https://arxiv.org/abs/2512.08763)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Yijie Li,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: 本文提出LEAP模型，通过强化学习选择节点并编辑提示，在保持通用图提示调优理论基础的同时追求更理想的提示效果。


<details>
  <summary>Details</summary>
Motivation: 现有选择性节点图提示调优方法会损害通用图提示调优的理论基础，需要一种既能保持理论基础又能获得更好提示效果的新方法。

Method: 提出LEAP模型：1) 构建基本通用图提示以保持理论基础；2) 使用actor-critic强化学习选择节点并编辑提示。

Result: 在多种预训练策略下的图级和节点级任务中，无论是全样本还是少样本场景，LEAP都持续优于微调和其他基于提示的方法。

Conclusion: 通过引入更严格的约束条件，证明了向所有节点添加提示是实现图提示通用性的必要条件，LEAP在保持理论基础的同时实现了更好的性能。

Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.

</details>


### [95] [De novo generation of functional terpene synthases using TpsGPT](https://arxiv.org/abs/2512.08772)
*Hamsini Ramanathan,Roman Bushuiev,Matouš Soldát,Jirí Kohout,Téo Hebra,Joshua David Smith,Josef Sivic,Tomáš Pluskal*

Main category: cs.LG

TL;DR: TpsGPT是一个通过微调ProtGPT2蛋白语言模型生成的萜烯合酶设计模型，能够从头设计功能性的TPS酶，实验验证显示至少两个生成的序列具有酶活性。


<details>
  <summary>Details</summary>
Motivation: 萜烯合酶是产生多种天然产物（包括紫杉醇等抗癌药物）的关键酶家族，但通过定向进化进行从头设计成本高、速度慢，需要更高效的设计方法。

Method: 通过微调蛋白语言模型ProtGPT2，使用从UniProt挖掘的79k个TPS序列构建TpsGPT模型，生成候选酶序列，并采用多种验证指标（EnzymeExplorer分类、ESMFold结构置信度、序列多样性、CLEAN分类、InterPro域检测、Foldseek结构比对）进行筛选。

Result: 从最初生成的28k个序列中，筛选出7个满足所有验证标准的候选TPS酶，实验验证确认其中至少两个序列具有TPS酶活性。

Conclusion: 在精心策划的酶类特定数据集上微调蛋白语言模型，结合严格的筛选策略，能够实现功能性、进化距离较远的酶的从头生成。

Abstract: Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.

</details>


### [96] [Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?](https://arxiv.org/abs/2512.08798)
*Jeongwhan Choi,Woosung Kang,Minseo Kim,Jongwoo Kim,Noseong Park*

Main category: cs.LG

TL;DR: TabPFN-GN将图节点分类问题转化为表格学习问题，通过特征工程提取节点属性、结构特征、位置编码等，在不进行图特定训练的情况下，在异质图上表现优于GNN。


<details>
  <summary>Details</summary>
Motivation: 基于TabPFN在表格数据和时序数据上的成功，研究是否可以将图节点分类问题有效地重新表述为表格学习问题，从而避免图特定训练或语言模型依赖。

Method: TabPFN-GN方法：将图数据转换为表格特征，提取节点属性、结构特征、位置编码，以及可选的平滑邻域特征，然后使用TabPFN进行直接节点分类。

Result: 在12个基准数据集上的实验表明，TabPFN-GN在同质图上与GNN竞争，在异质图上始终优于GNN。

Conclusion: 原则性的特征工程可以弥合表格和图领域之间的差距，为任务特定的GNN训练和LLM依赖的图基础模型提供实用替代方案。

Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.

</details>


### [97] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: 提出一种基于提升模型预测得分的反事实估计方法，通过拟合双变量Beta分布获得反事实结果的后验分布，无需额外因果假设


<details>
  <summary>Details</summary>
Motivation: 提升建模只能估计干预的因果效应（潜在结果差异），而反事实识别旨在恢复潜在结果的联合分布（如"如果给客户营销优惠，他们是否仍会流失？"）。反事实分布提供比提升更丰富的信息但更难估计，两者具有协同作用

Method: 提出一种反事实估计器，将双变量Beta分布拟合到预测的提升得分上，产生反事实结果的后验分布。该方法除了提升建模所需的因果假设外，无需额外假设

Result: 模拟实验显示该方法有效，可应用于电信客户流失等问题，揭示标准机器学习或单独提升模型无法获得的洞察

Conclusion: 利用提升模型进行反事实估计是可行的，提出的方法能够从提升得分中恢复反事实联合分布，为因果推断提供更丰富的信息

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [98] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: WAAPO框架生成针对AI天气预报模型的对抗性扰动，通过通道稀疏性、空间局部性和平滑性约束确保扰动有效且隐蔽，揭示了AI天气预报系统的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在天气预报中的日益依赖，评估其对对抗性扰动的脆弱性变得至关重要，需要研究如何生成既能操纵预测又难以检测的对抗性扰动。

Method: 提出Weather Adaptive Adversarial Perturbation Optimization (WAAPO)框架，通过通道稀疏性、空间局部性和平滑性约束生成对抗性扰动，确保扰动物理真实且难以察觉。使用ERA5数据集和FourCastNet模型进行实验验证。

Result: WAAPO能够生成与预定目标紧密对齐的对抗性轨迹，即使在约束条件下也能成功。实验表明，对初始条件的小扰动可导致预测天气模式的显著偏差，揭示了AI驱动预报模型的关键脆弱性。

Conclusion: AI天气预报模型容易受到对抗性攻击，初始条件的微小扰动可能导致预测结果的重大偏差，这强调了在业务预报系统中需要建立强大的安全防护措施来防止对抗性利用。

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [99] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: STD(λ)是一种改进的TD(λ)算法，通过训练函数逼近器关注状态相对值而非绝对值，解决了TD(λ)在某些情况下会收敛到次优策略的问题。


<details>
  <summary>Details</summary>
Motivation: 传统TD(λ)算法使用函数逼近时最小化状态值的平方误差，但策略性能更依赖于状态值的相对排序而非绝对值。作者发现TD(λ)在某些情况下会从最优策略收敛到次优策略，需要改进算法来更好地保持策略性能。

Method: 提出STD(λ)算法，在二元决策问题中训练函数逼近器关注状态相对值而非绝对值。算法基于相对状态值进行训练，理论上分析了其在两状态系统中的单调策略改进特性，并与Bertsekas的差分训练方法进行了比较。

Result: 在两状态系统和倒立摆问题的变体上成功演示了STD(λ)的有效性。理论分析证明STD(λ)在两状态系统中具有单调策略改进特性，解决了TD(λ)会收敛到次优策略的问题。

Conclusion: STD(λ)通过关注状态相对值而非绝对值，改进了TD(λ)在函数逼近下的策略性能，为强化学习中的值函数逼近提供了更有效的训练方法。

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [100] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 提出文本到IMU运动合成框架，通过加速度二阶损失微调扩散模型，提升IMU数据真实性和HAR性能


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动模型生成的IMU数据与真实IMU记录存在差异，特别是在加速度模式上，需要专门优化以提升IMU数据的真实性和下游任务性能

Method: 提出加速度二阶损失(L_acc)来增强扩散模型，强制生成运动在离散二阶时间差异上的一致性，将L_acc集成到现有扩散模型训练目标中，通过微调获得IMU特定的运动先验

Result: L_acc损失相对原始模型降低12.7%，高动态活动改进更显著，合成IMU数据分布更接近真实记录，HAR分类性能提升8.7%

Conclusion: 加速度感知的扩散模型优化有效对齐运动生成和IMU合成，展示了深度学习管道在将通用文本到运动先验专门化到传感器特定任务方面的灵活性

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [101] [Differentially Private Synthetic Data Generation Using Context-Aware GANs](https://arxiv.org/abs/2512.08869)
*Anantaa Kotal,Anupam Joshi*

Main category: cs.LG

TL;DR: ContextGAN：一种上下文感知的差分隐私生成对抗网络，通过约束矩阵整合领域特定规则，生成既保护隐私又符合领域约束的高质量合成数据。


<details>
  <summary>Details</summary>
Motivation: 大数据应用中的隐私保护需求与数据实用性之间存在矛盾，传统合成数据方法无法捕捉领域特定的复杂隐式规则（如医疗领域的处方指南、药物相互作用限制），导致生成的合成数据可能不现实或不适用。

Method: 提出ContextGAN框架，使用约束矩阵编码领域特定的显式和隐式知识，通过约束感知的判别器评估合成数据是否符合领域规则，同时结合差分隐私技术保护原始数据的敏感信息。

Result: 在医疗、安全和金融领域的验证表明，ContextGAN能够生成高质量合成数据，既尊重领域约束又保护隐私，相比传统方法显著提升了数据的真实性和实用性。

Conclusion: ContextGAN通过整合领域约束和隐私保护机制，解决了传统合成数据方法在复杂隐式规则处理上的不足，为需要在严格隐私保证下同时满足显式模式和隐式规则的应用提供了有效解决方案。

Abstract: The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.

</details>


### [102] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: Fed-SE：一个用于LLM智能体的联邦自进化框架，通过局部进化-全局聚合范式解决异构任务中的梯度冲突问题，在隐私约束下实现跨环境知识迁移


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂交互任务中广泛部署，但隐私约束限制了集中式优化和跨动态环境的协同进化。虽然联邦学习在静态数据集上有效，但在开放式的智能体自进化中应用不足，标准联邦学习面临异构任务和稀疏轨迹级奖励带来的梯度冲突问题

Method: 提出Fed-SE框架，采用局部进化-全局聚合范式：局部层面，智能体在过滤的高回报轨迹上进行参数高效微调；全局层面，在低秩子空间中聚合更新，解耦环境特定动态，减少客户端间的负迁移

Result: 在五个异构环境中的实验表明，Fed-SE相比联邦基线平均任务成功率提升约18%，验证了其在隐私约束部署中实现鲁棒跨环境知识迁移的有效性

Conclusion: Fed-SE成功解决了LLM智能体在联邦学习环境中的自进化挑战，通过创新的局部进化策略和全局聚合机制，有效缓解梯度冲突，为隐私约束下的智能体协同进化提供了可行方案

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


### [103] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: LLM生成的表格数据存在隐私泄露风险，通过数字序列的成员推理攻击可检测训练数据泄露，提出扰动数字的防御方法


<details>
  <summary>Details</summary>
Motivation: LLM在生成高质量表格合成数据方面表现出色，但现有方法（微调小模型或提示大模型）可能存在隐私风险，会泄露训练数据中的数字模式

Method: 提出LevAtt攻击方法，仅基于生成的合成数据，针对数字字符串序列进行成员推理攻击；同时提出防御方法，包括在生成过程中策略性扰动数字的采样策略

Result: 攻击揭示了广泛的隐私泄露，在某些情况下对最先进模型成为完美的成员分类器；提出的防御方法能以最小的保真度和效用损失抵御这些攻击

Conclusion: LLM合成数据生成存在独特的隐私漏洞，需要有效防御；提出的数字扰动策略能有效保护隐私同时保持数据质量

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [104] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: DAO-GP：一种完全自适应、无超参数、带衰减的稀疏非线性回归模型，专门用于处理在线学习中的概念漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常存在随时间变化的数据分布（概念漂移），忽略此现象会显著降低模型预测精度。传统在线高斯过程方法存在多个关键限制：缺乏漂移感知、依赖固定超参数、易受数据窥探影响、缺乏原则性衰减机制和内存效率低下。

Method: 提出DAO-GP（Drift-Aware Online Gaussian Process），一种新型完全自适应、无超参数、带衰减的稀疏非线性回归模型。该模型内置漂移检测和适应机制，能根据漂移严重程度动态调整模型行为。

Result: 广泛实证评估证实DAO-GP在平稳条件、多种漂移类型（突变、增量、渐进）和不同数据特征下都具有鲁棒性。分析显示其具有动态适应能力、高效的内存和衰减管理，以及演化的诱导点。与最先进的参数和非参数模型相比，DAO-GP始终实现优越或竞争性性能。

Conclusion: DAO-GP被确立为在线非线性回归的漂移弹性解决方案，能够有效处理概念漂移问题，在动态环境中保持预测准确性。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [105] [Explainable Anomaly Detection for Industrial IoT Data Streams](https://arxiv.org/abs/2512.08885)
*Ana Rita Paupério,Diogo Risca,Afonso Lourenço,Goreti Marreiros,Ricardo Martins*

Main category: cs.LG

TL;DR: 提出一个结合无监督异常检测与人在回路学习的协作式数据流挖掘框架，用于工业维护决策支持，采用在线隔离森林算法并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 工业维护在物联网和边缘计算转型中面临挑战：连续数据流需要实时自适应决策，但实际中地面真实标签往往延迟或缺失，现有数据流挖掘方法大多假设完全监督设置不切实际。

Method: 采用协作式数据流挖掘框架，整合无监督异常检测与交互式人在回路学习。使用在线隔离森林算法，通过增量部分依赖图和基于个体条件期望曲线偏离衰减平均的特征重要性评分增强可解释性，允许用户动态重新评估特征相关性和调整异常阈值。

Result: 在提花织机单元的故障检测中实现了实时实施并提供了初步结果。正在进行的工作旨在通过连续监测来预测和解释即将发生的轴承故障。

Conclusion: 该框架通过结合无监督异常检测与人在回路学习，解决了工业维护中标签稀缺的挑战，增强了决策支持系统的可解释性和适应性，为实时故障检测和预测提供了有效方案。

Abstract: Industrial maintenance is being transformed by the Internet of Things and edge computing, generating continuous data streams that demand real-time, adaptive decision-making under limited computational resources. While data stream mining (DSM) addresses this challenge, most methods assume fully supervised settings, yet in practice, ground-truth labels are often delayed or unavailable. This paper presents a collaborative DSM framework that integrates unsupervised anomaly detection with interactive, human-in-the-loop learning to support maintenance decisions. We employ an online Isolation Forest and enhance interpretability using incremental Partial Dependence Plots and a feature importance score, derived from deviations of Individual Conditional Expectation curves from a fading average, enabling users to dynamically reassess feature relevance and adjust anomaly thresholds. We describe the real-time implementation and provide initial results for fault detection in a Jacquard loom unit. Ongoing work targets continuous monitoring to predict and explain imminent bearing failures.

</details>


### [106] [Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training](https://arxiv.org/abs/2512.08894)
*Jakub Krajewski,Amitis Shidani,Dan Busbridge,Sam Wiseman,Jason Ramapuram*

Main category: cs.LG

TL;DR: 本文提出直接建模基准性能随训练预算的缩放规律，挑战了传统认为下游任务性能难以预测的观点，发现固定token-参数比时，简单幂律能准确描述多个下游任务的对数准确率缩放行为。


<details>
  <summary>Details</summary>
Motivation: 传统LLM缩放定律主要关注预训练损失等代理指标，而预测下游任务性能被认为不可靠。本文挑战这一观点，旨在建立从训练预算直接预测下游基准性能的框架。

Method: 提出直接框架建模基准性能随训练预算的缩放，发现固定token-参数比时，简单幂律能描述对数准确率缩放。引入跨token-参数比预测准确率的功能形式，并考虑重复采样下的推理计算。

Result: 直接方法比先前提出的两阶段程序外推效果更好（后者易产生复合误差）。在多达170亿参数、3500亿token的两个数据集混合上验证了发现，并发布了完整的预训练损失和下游评估结果。

Conclusion: 下游任务性能可以通过训练预算直接可靠预测，挑战了传统观点。直接缩放方法优于两阶段程序，为模型缩放提供了更准确的预测框架，支持可复现性和未来研究。

Abstract: While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.

</details>


### [107] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: Open Polymer Challenge (OPC) 发布了首个社区开发的聚合物信息学基准数据集，包含10K聚合物和5种性质，通过多任务预测竞赛推动可持续聚合物材料的机器学习发现。


<details>
  <summary>Details</summary>
Motivation: 机器学习在可持续聚合物材料发现中具有巨大潜力，但缺乏大规模、高质量、开放可访问的聚合物数据集限制了进展。需要建立标准化的基准来推动聚合物信息学发展。

Method: 发布包含10K聚合物和5种性质（热导率、回转半径、密度、自由体积分数、玻璃化转变温度）的基准数据集。举办多任务聚合物性质预测竞赛，参与者在数据量小、标签不平衡、模拟源异质等现实约束下开发模型，使用特征增强、迁移学习、自监督预训练、针对性集成等策略。

Result: 竞赛揭示了数据准备、分布偏移、跨组模拟一致性等方面的重要经验教训，为未来大规模聚合物数据集的最佳实践提供了指导。发布的模型、分析和数据为聚合物科学中的分子AI建立了新基础。

Conclusion: Open Polymer Challenge 通过发布基准数据集和竞赛，为聚合物信息学建立了新标准，有望加速可持续和节能材料的发展。同时发布了测试数据集和数据生成管道，支持更广泛的聚合物性质模拟。

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [108] [Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification](https://arxiv.org/abs/2512.07888)
*Fahad Mostafa,Hafiz Khan*

Main category: stat.ML

TL;DR: 提出FRF-ACS方法，通过自适应成本敏感分裂和混合采样策略解决不平衡功能数据分类问题，显著提升少数类召回率。


<details>
  <summary>Details</summary>
Motivation: 传统随机森林在处理功能数据（如曲线、轨迹）时存在两个主要问题：1）难以捕捉功能数据的固有结构；2）在严重类别不平衡情况下对少数类检测效果差。特别是在生物医学信号和传感器轨迹等领域，少数类检测至关重要。

Method: 提出FRF-ACS（功能随机森林-自适应成本敏感分裂）框架：1）使用基展开和功能主成分分析（FPCA）将曲线表示为低维功能特征；2）引入动态成本敏感分裂准则，在每个节点局部调整类别权重；3）结合功能SMOTE和加权自举的混合采样策略；4）用曲线特定相似性度量替代传统欧氏距离，在叶节点分配时保留功能特性。

Result: 在合成和真实数据集（包括生物医学信号和传感器轨迹）上的广泛实验表明，FRF-ACS相比现有功能分类器和不平衡处理技术，显著提高了少数类召回率和整体预测性能。

Conclusion: FRF-ACS为高维功能数据分析提供了一个可扩展、可解释的解决方案，特别适用于少数类检测至关重要的领域，有效解决了不平衡功能数据分类的挑战。

Abstract: Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical.

</details>


### [109] [Provable Diffusion Posterior Sampling for Bayesian Inversion](https://arxiv.org/abs/2512.08022)
*Jinyuan Chang,Chenguang Duan,Yuling Jiao,Ruoxuan Li,Jerry Zhijian Yang,Cheng Yuan*

Main category: stat.ML

TL;DR: 提出一种基于扩散的PnP后验采样方法，使用概率传输和预热启动策略，通过蒙特卡洛估计器近似后验分数，提供理论误差界并在多个逆问题上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有后验采样方法在处理复杂多模态分布时存在局限性，需要更有效的方法来近似后验分布，特别是在贝叶斯逆问题中。

Method: 在PnP框架下构建从易采样终端分布到目标后验的概率传输，采用预热启动策略初始化粒子，使用朗之万动力学生成粒子的蒙特卡洛估计器近似后验分数，通过学习数据中的分数来捕捉先验分布的结构特征。

Result: 提供了非渐近误差界，证明方法对复杂多模态目标后验的收敛性，明确量化了后验分数估计、预热启动初始化和后验采样过程的误差，阐明了先验分数匹配误差和贝叶斯逆问题条件数对性能的影响，数值实验验证了方法在多种逆问题中的有效性。

Conclusion: 提出了一种理论保证的扩散式PnP后验采样方法，能够有效处理复杂多模态分布，为贝叶斯逆问题提供了实用的后验采样解决方案。

Abstract: This paper proposes a novel diffusion-based posterior sampling method within a plug-and-play (PnP) framework. Our approach constructs a probability transport from an easy-to-sample terminal distribution to the target posterior, using a warm-start strategy to initialize the particles. To approximate the posterior score, we develop a Monte Carlo estimator in which particles are generated using Langevin dynamics, avoiding the heuristic approximations commonly used in prior work. The score governing the Langevin dynamics is learned from data, enabling the model to capture rich structural features of the underlying prior distribution. On the theoretical side, we provide non-asymptotic error bounds, showing that the method converges even for complex, multi-modal target posterior distributions. These bounds explicitly quantify the errors arising from posterior score estimation, the warm-start initialization, and the posterior sampling procedure. Our analysis further clarifies how the prior score-matching error and the condition number of the Bayesian inverse problem influence overall performance. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method across a range of inverse problems.

</details>


### [110] [Worst-case generation via minimax optimization in Wasserstein space](https://arxiv.org/abs/2512.08176)
*Xiuyuan Cheng,Yao Xie,Linglingzhi Zhu,Yunqin Zhu*

Main category: stat.ML

TL;DR: 提出一种基于Wasserstein空间的连续概率分布生成框架，用于生成最坏情况样本，通过Brenier定理将最不利分布表征为参考测度的推前映射，避免了传统离散分布鲁棒优化的可扩展性和泛化性问题。


<details>
  <summary>Details</summary>
Motivation: 传统离散分布鲁棒优化方法存在可扩展性差、泛化能力有限和最坏情况推断成本高等问题，需要开发一种连续、表达力强的风险诱导最坏情况生成框架，用于评估系统在分布偏移下的鲁棒性和压力测试。

Method: 基于Wasserstein空间中的min-max优化框架，利用Brenier定理将最不利分布表征为参考测度的推前映射；提出单循环梯度下降上升方案同时更新决策模型和传输映射；使用神经网络参数化传输映射，通过匹配传输的训练样本实现无模拟训练。

Result: 在温和正则性假设下建立了全局收敛保证，即使没有凸凹性；在合成数据和图像数据上的数值实验验证了该方法作为风险诱导最坏情况生成器的有效性。

Conclusion: 该框架提供了一种连续、表达力强的风险诱导最坏情况生成方法，克服了传统离散DRO的局限性，为机器学习和各种应用领域的鲁棒性评估提供了有效的工具。

Abstract: Worst-case generation plays a critical role in evaluating robustness and stress-testing systems under distribution shifts, in applications ranging from machine learning models to power grids and medical prediction systems. We develop a generative modeling framework for worst-case generation for a pre-specified risk, based on min-max optimization over continuous probability distributions, namely the Wasserstein space. Unlike traditional discrete distributionally robust optimization approaches, which often suffer from scalability issues, limited generalization, and costly worst-case inference, our framework exploits the Brenier theorem to characterize the least favorable (worst-case) distribution as the pushforward of a transport map from a continuous reference measure, enabling a continuous and expressive notion of risk-induced generation beyond classical discrete DRO formulations. Based on the min-max formulation, we propose a Gradient Descent Ascent (GDA)-type scheme that updates the decision model and the transport map in a single loop, establishing global convergence guarantees under mild regularity assumptions and possibly without convexity-concavity. We also propose to parameterize the transport map using a neural network that can be trained simultaneously with the GDA iterations by matching the transported training samples, thereby achieving a simulation-free approach. The efficiency of the proposed method as a risk-induced worst-case generator is validated by numerical experiments on synthetic and image data.

</details>


### [111] [Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis](https://arxiv.org/abs/2512.08601)
*Orit Davidovich,Shimrit Shtern,Segev Wasserkrug,Nimrod Megiddo*

Main category: stat.ML

TL;DR: 该论文提出了一个统一的马尔可夫决策过程框架来建模组合优化问题，并分析了强化学习方法的收敛性和最优性保证。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在组合优化问题上取得了经验成功，但缺乏理论基础。作者旨在填补这一空白，为RL在CO问题中的应用提供理论支撑。

Method: 建立统一的MDP框架将组合优化问题建模为无折扣马尔可夫决策过程，分析价值型强化学习方法的收敛条件，包括批量大小增长率和投影梯度下降步骤的要求。

Result: 提出了易于验证的假设条件，确保CO问题可等价转化为MDP并获得最优解。建立了RL方法收敛到近似解的保证，并量化了最优性差距与问题参数和RL精度的关系。

Conclusion: 该理论分析解释了深度Q学习在组合优化中的成功与局限，强调了状态空间嵌入选择的重要性，为RL在CO问题中的应用提供了理论基础。

Abstract: Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.

</details>
