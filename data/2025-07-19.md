<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 10]
- [cs.LG](#cs.LG) [Total: 63]
- [stat.ML](#stat.ML) [Total: 6]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Differential Communication in Channels with Mobility and Delay Spread using Zak-OTFS](https://arxiv.org/abs/2507.12593)
*Sandesh Rao Mattu,Nishant Mehrotra,Robert Calderbank*

Main category: eess.SP

TL;DR: 提出了一种Zak-OTFS系统的差分通信方案，无需周期性发送导频，利用检测数据作为导频进行信道估计，提高能量利用率和频谱效率。


<details>
  <summary>Details</summary>
Motivation: Zak-OTFS在延迟-多普勒（DD）域中信号处理具有可预测性，但仍需周期性导频。本文旨在减少导频需求，提高系统效率。

Method: 利用DD域信道的预测能力，将检测到的数据符号作为导频，通过前一时刻的信道估计检测下一时刻数据，实现差分通信。

Result: 方案提高了数据符号的能量利用率，实现了全频谱效率，且比现有方案具有更低的误码率和复杂度。

Conclusion: 差分通信方案在Zak-OTFS系统中有效减少了导频需求，提升了性能和效率。

Abstract: Zak-transform based orthogonal time frequency space (Zak-OTFS) is a
delay-Doppler (DD) domain modulation scheme in which the signal processing is
carried out in the DD domain. The channel when viewed in the DD domain is
predictable. However, even with Zak-OTFS, pilots need to be sent periodically,
albeit at a lower rate. In this paper, we propose a differential communication
scheme for Zak-OTFS systems that alleviates the need for periodic pilot
transmission. Towards this, we analytically show that the detected data can be
used as a pilot and that the channel estimate obtained from the detected data
can enable further detection enabling the "differential" aspect of the
communication. Specifically, we leverage the prediction capability of the DD
channel in Zak-OTFS to use the channel estimate (obtained from detected data
symbols treated as pilots) in the previous instant to detect data in the next
instant and propagate this forward. The advantages are two fold. First, it
allows the data symbols to enjoy higher energy since the energy that would
otherwise be required for pilot symbols can also be allocated to data symbols.
Second, it allows for full spectral efficiency compared to point or embedded
pilots. Comparison with the full spectral efficiency achieving spread pilot
scheme shows that the proposed method achieves better bit-error rate at lower
complexity.

</details>


### [2] [Achieving Robust Channel Estimation Neural Networks by Designed Training Data](https://arxiv.org/abs/2507.12630)
*Dianxin Luan,John Thompson*

Main category: eess.SP

TL;DR: 论文提出了一种离线训练的神经网络设计方法，用于在未知信道信息的情况下实现鲁棒的信道估计。


<details>
  <summary>Details</summary>
Motivation: 由于数据驱动方法在新数据上表现不佳且无法在线训练，需要设计无需实际信道信息的离线训练神经网络。

Method: 提出生成合成训练数据集的设计准则，确保神经网络在新信道上达到特定MSE。

Result: 神经网络在不同信道配置下表现出鲁棒性，且泛化能力与网络架构无关。

Conclusion: 所提方法在无需信道信息的情况下实现了鲁棒的信道估计，适用于实际无线通信场景。

Abstract: Channel estimation is crucial in cognitive communications, as it enables
intelligent spectrum sensing and adaptive transmission by providing accurate
information about the current channel state. However, in many papers neural
networks are frequently tested by training and testing on one example channel
or similar channels. This is because data-driven methods often degrade on new
data which they are not trained on, as they cannot extrapolate their training
knowledge. This is despite the fact physical channels are often assumed to be
time-variant. However, due to the low latency requirements and limited
computing resources, neural networks may not have enough time and computing
resources to execute online training to fine-tune the parameters. This
motivates us to design offline-trained neural networks that can perform
robustly over wireless channels, but without any actual channel information
being known at design time. In this paper, we propose design criteria to
generate synthetic training datasets for neural networks, which guarantee that
after training the resulting networks achieve a certain mean squared error
(MSE) on new and previously unseen channels. Therefore, neural network
solutions require no prior channel information or parameters update for
real-world implementations. Based on the proposed design criteria, we further
propose a benchmark design which ensures intelligent operation for different
channel profiles. To demonstrate general applicability, we use neural networks
with different levels of complexity to show that the generalization achieved
appears to be independent of neural network architecture. From simulations,
neural networks achieve robust generalization to wireless channels with both
fixed channel profiles and variable delay spreads.

</details>


### [3] [A Novel Data Augmentation Strategy for Robust Deep Learning Classification of Biomedical Time-Series Data: Application to ECG and EEG Analysis](https://arxiv.org/abs/2507.12645)
*Mohammed Guhdar,Ramadhan J. Mstafa,Abdulhakeem O. Mohammed*

Main category: eess.SP

TL;DR: 提出了一种统一的深度学习框架，结合ResNet和注意力机制，通过时间域拼接增强数据，解决生物信号分析和类别不平衡问题，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多传感器融合中统一处理不同生理信号的架构缺失问题，以及生物医学数据中的类别不平衡导致的性能偏差。

Method: 采用ResNet-based CNN与注意力机制结合，引入时间域拼接的数据增强策略，并使用Focal Loss和正则化技术。

Result: 在UCI Seizure EEG、MIT-BIH Arrhythmia和PTB Diagnostic ECG数据集上分别达到99.96%、99.78%和100%的准确率。

Conclusion: 该框架在性能和效率上均表现优异，适合部署在低端或可穿戴设备上。

Abstract: The increasing need for accurate and unified analysis of diverse biological
signals, such as ECG and EEG, is paramount for comprehensive patient
assessment, especially in synchronous monitoring. Despite advances in
multi-sensor fusion, a critical gap remains in developing unified architectures
that effectively process and extract features from fundamentally different
physiological signals. Another challenge is the inherent class imbalance in
many biomedical datasets, often causing biased performance in traditional
methods. This study addresses these issues by proposing a novel and unified
deep learning framework that achieves state-of-the-art performance across
different signal types. Our method integrates a ResNet-based CNN with an
attention mechanism, enhanced by a novel data augmentation strategy:
time-domain concatenation of multiple augmented variants of each signal to
generate richer representations. Unlike prior work, we scientifically increase
signal complexity to achieve future-reaching capabilities, which resulted in
the best predictions compared to the state of the art. Preprocessing steps
included wavelet denoising, baseline removal, and standardization. Class
imbalance was effectively managed through the combined use of this advanced
data augmentation and the Focal Loss function. Regularization techniques were
applied during training to ensure generalization. We rigorously evaluated the
proposed architecture on three benchmark datasets: UCI Seizure EEG, MIT-BIH
Arrhythmia, and PTB Diagnostic ECG. It achieved accuracies of 99.96%, 99.78%,
and 100%, respectively, demonstrating robustness across diverse signal types
and clinical contexts. Finally, the architecture requires ~130 MB of memory and
processes each sample in ~10 ms, suggesting suitability for deployment on
low-end or wearable devices.

</details>


### [4] [Enhancing Urban GNSS Positioning Reliability via Conservative Satellite Selection Using Unanimous Voting Across Multiple Machine Learning Classifiers](https://arxiv.org/abs/2507.12706)
*Sanghyun Kim,Jiwon Seo*

Main category: eess.SP

TL;DR: 该研究提出了一种基于多机器学习分类器一致投票的保守卫星选择策略，以增强城市环境中GNSS定位的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中，建筑物导致的信号遮挡和多径效应会显著增加GNSS定位误差，需改进现有方法。

Method: 使用随机森林（RF）、梯度提升决策树（GBDT）和支持向量机（SVM）三种模型进行LOS/NLOS分类，并通过一致投票和置信度阈值选择卫星。

Result: 实验表明，该方法显著提高了定位成功率和接收器包含率，尽管卫星数量减少导致位置边界略有增加。

Conclusion: 该方法在城市GNSS环境中有效提升了定位可靠性。

Abstract: In urban environments, global navigation satellite system (GNSS) positioning
is often compromised by signal blockages and multipath effects caused by
buildings, leading to significant positioning errors. To address this issue,
this study proposes a robust enhancement of zonotope shadow matching
(ZSM)-based positioning by employing a conservative satellite selection
strategy using unanimous voting across multiple machine learning classifiers.
Three distinct models - random forest (RF), gradient boosting decision tree
(GBDT), and support vector machine (SVM) - were trained to perform
line-of-sight (LOS) and non-line-of-sight (NLOS) classification based on global
positioning system (GPS) signal features. A satellite is selected for
positioning only when all classifiers unanimously agree on its classification
and their associated confidence scores exceed a threshold. Experiments with
real-world GPS data collected in dense urban areas demonstrate that the
proposed method significantly improves the positioning success rate and the
receiver containment rate, even with imperfect LOS/NLOS classification.
Although a slight increase in the position bound was observed due to the
reduced number of satellites used, overall positioning reliability was
substantially enhanced, indicating the effectiveness of the proposed approach
in urban GNSS environments.

</details>


### [5] [Beamforming Tradeoff for Sensing and Communication in Cell-Free MIMO](https://arxiv.org/abs/2507.12917)
*Xi Ding,Luca Kunz,E. Jorswieck*

Main category: eess.SP

TL;DR: 本文提出了一种基于SDR的全局最优波束成形框架，用于小规模无小区MIMO系统中的联合感知与通信优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法在联合感知与通信优化中缺乏全局最优性或需要额外步骤，本文旨在解决这一问题。

Method: 采用SDR优化框架，无需后处理即可保证全局最优解，并引入独立的波束成形策略作为基准。

Result: 提出的框架实现了全局最优且计算高效的波束成形设计。

Conclusion: 该框架为下一代无线网络的发展提供了有价值的参考。

Abstract: This paper studies optimal joint beamforming (BF) for joint sensing and
communication (JSAC) in small-scale cell-free MIMO (CF-MIMO) systems. While
prior works have explored JSAC optimization using methods such as successive
convex approximation (SCA) and semidefinite relaxation (SDR), many of these
approaches either lack global optimality or require additional rank-reduction
steps. In contrast, we propose an SDR-based optimization framework that
guarantees globally optimal solutions without post-processing. To benchmark its
performance, we introduce a standalone BF strategy that dedicates each access
point (AP) exclusively to either communication or sensing. The proposed
formulation builds upon a general multi-user system model, enabling future
extensions beyond the single-user setting. Overall, our framework offers a
globally optimal and computationally efficient BF design, providing valuable
insights for the development of next-generation wireless networks.

</details>


### [6] [Multiple-Mode Affine Frequency Division Multiplexing with Index Modulation](https://arxiv.org/abs/2507.13037)
*Guangyao Liu,Tianqi Mao,Yanqun Tang,Jingjing Zhao,Zhenyu Xiao*

Main category: eess.SP

TL;DR: 本文提出了一种基于AFDM的多模式索引调制方案（MM-AFDM-IM），旨在提高AFDM的频谱和能量效率。通过动态选择星座模式和激活子载波，无需额外能耗即可传输更多信息。


<details>
  <summary>Details</summary>
Motivation: 针对高移动性通信场景，AFDM是一种有效的多载波技术。为进一步提升其频谱和能量效率，本文提出了MM-AFDM-IM方案。

Method: 通过为不同子载波选择多星座模式，并结合星座模式选择和子载波激活的动态模式传输额外信息。讨论了模式选择策略，并推导了最大似然检测下的BER上限。

Result: 仿真结果表明，MM-AFDM-IM在性能上优于传统基准方案。

Conclusion: MM-AFDM-IM是一种高效的多载波调制方案，适用于高移动性通信场景。

Abstract: Affine frequency division multiplexing (AFDM), a promising multicarrier
technique utilizing chirp signals, has been envisioned as an effective solution
for high-mobility communication scenarios. In this paper, we develop a
multiple-mode index modulation scheme tailored for AFDM, termed as MM-AFDM-IM,
which aims to further improve the spectral and energy efficiencies of AFDM.
Specifically, multiple constellation alphabets are selected for different
chirp-based subcarriers (chirps). Aside from classical amplitude/phase
modulation, additional information bits can be conveyed by the dynamic patterns
of both constellation mode selection and chirp activation, without extra energy
consumption. Furthermore, we discuss the mode selection strategy and derive an
asymptotically tight upper bound on the bit error rate (BER) of the proposed
scheme under maximum-likelihood detection. Simulation results are provided to
demonstrate the superior performance of MM-AFDM-IM compared to conventional
benchmark schemes.

</details>


### [7] [Unmodulated Visible Light Positioning: A Deep Dive into Techniques, Studies, and Future Prospects](https://arxiv.org/abs/2507.13080)
*Morteza Alijani,Wout Joseph,David Plets*

Main category: eess.SP

TL;DR: 本文介绍了无调制可见光定位（uVLP）技术，作为传统VLP的低成本替代方案，利用现有照明设施实现高精度室内定位，并分类讨论了其技术方法和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统VLP技术因调制LED的高成本和操作复杂性限制了其广泛应用，而uVLP通过利用未调制光源解决了这些问题。

Method: 论文分析了uVLP的基本原理，将其与传统VLP对比，并根据接收器技术分类为基于强度和基于成像的方法，同时提出了一种分类框架。

Result: uVLP提供了一种低成本、高效的室内定位方案，但当前仍面临技术挑战。

Conclusion: uVLP具有广阔的应用前景，未来需进一步研究以解决其技术难题并推动实际部署。

Abstract: Visible Light Positioning (VLP) has emerged as a promising technology for
next-generation indoor positioning systems (IPS), particularly within the scope
of sixth-generation (6G) wireless networks. Its attractiveness stems from
leveraging existing lighting infrastructures equipped with light-emitting
diodes (LEDs), enabling cost-efficient deployments and achieving high-precision
positioning accuracy in the centimeter-todecimeter range. However, widespread
adoption of traditional VLP solutions faces significant barriers due to the
increased costs and operational complexity associated with modulating LEDs,
which consequently reduces illumination efficiency by lowering their radiant
flux. To address these limitations, recent research has introduced the concept
of unmodulated Visible Light Positioning (uVLP), which exploits Light Signals
of Opportunity (LSOOP) emitted by unmodulated illumination sources such as
conventional LEDs. This paradigm offers a cost-effective, lowinfrastructure
alternative for indoor positioning by eliminating the need for modulation
hardware and maintaining lighting efficiency. This paper delineates the
fundamental principles of uVLP, provides a comparative analysis of uVLP versus
conventional VLP methods, and classifies existing uVLP techniques according to
receiver technologies into intensity-based methods (e.g., photodiodes, solar
cells, etc.) and imaging-based methods. Additionally, we propose a
comprehensive taxonomy categorizing techniques into demultiplexed and
undemultiplexed approaches. Within this structured framework, we critically
review current advancements in uVLP, discuss prevailing challenges, and outline
promising research directions essential for developing robust, scalable, and
widely deployable uVLP solutions.

</details>


### [8] [Angle Estimation of a Single Source with Massive Uniform Circular Arrays](https://arxiv.org/abs/2507.13086)
*Mingyan Gong*

Main category: eess.SP

TL;DR: 提出了一种基于均匀圆形阵列（UCA）的简单二维DOA估计方法，适用于实时信号处理。


<details>
  <summary>Details</summary>
Motivation: 均匀线性阵列只能估计源方位角，而UCA能提供360度方位角和额外的仰角信息。

Method: 通过量化方位角并计算协方差，结合显式公式估计仰角。

Result: 数值结果表明，该方法能估计方位角和仰角，并可作为高精度多维搜索的起点。

Conclusion: 该方法计算简单，适用于实时处理，且在非均匀噪声下仍有效。

Abstract: Estimating the directions of arrival (DOAs) of incoming plane waves is an
essential topic in array signal processing. Widely adopted uniform linear
arrays can only provide estimates of source azimuth. Thus, uniform circular
arrays (UCAs) are attractive in that they can provide $360^{\circ}$ azimuthal
coverage and additional elevation angle information. Considering that with a
massive UCA, its polar angles of array sensors can approximately represent
azimuth angles over $360^{\circ}$ using angle quantization, a simple
two-dimensional DOA estimation method for a single source is proposed. In this
method, the quantized azimuth angle estimate is obtained by only calculating
and comparing a number of covariances, based on which the elevation angle
estimate is then obtained by an explicit formula. Thus, the proposed method is
computationally simple and suitable for real-time signal processing. Numerical
results verify that the proposed method can obtain azimuth as well as elevation
angle estimates and the estimates can be used as starting points of
multidimensional searches for methods with higher accuracy. Additionally, the
proposed method can still work in the presence of nonuniform noise.

</details>


### [9] [Multifrequency system model for multiport time-modulated scatterers](https://arxiv.org/abs/2507.13130)
*Aleksandr D. Kuznetsov,Jari Holopainen,Ville Viikari*

Main category: eess.SP

TL;DR: 提出了一种基于多端口S参数的多频散射模型，适用于非周期性、时变调制的散射结构，扩展了传统S矩阵模型的适用范围。


<details>
  <summary>Details</summary>
Motivation: 通信工程中散射体（如RIS和反向散射系统）需要物理一致的模型以准确预测性能，尤其是多频或产生谐波的系统。

Method: 扩展了多端口S参数模型，涵盖结构散射、互耦、非数字调制和非周期性配置，适用于时变调制结构。

Result: 模型验证了时空调制散射结构的实验数据，证明了其准确性和实用性。

Conclusion: 该模型为多频通信和传感系统提供了精确的分析和优化工具。

Abstract: Utilizing scatterers in communication engineering, such as reconfigurable
intelligent surfaces (RISs) and backscatter systems, requires physically
consistent models for accurate performance prediction. A multiport model, which
also accounts for structural scattering, has been developed for non-periodic
scatterers. However, many emerging systems operate at multiple frequencies or
generate intermodulation harmonics, particularly when incorporating space-time
modulation (STM) or dynamic load control. These functionalities demand advanced
modeling approaches capable of capturing scattering behavior across several
frequencies and directions simultaneously. This article extends a multiport
S-parameters-based model for predicting the scattering properties of
multifrequency operating structures. The model extends the applicability of
convenient S-matrix models to time-modulated multiport structures. Unlike known
approaches, this model incorporates structural scattering, mutual coupling, the
possibility of non-digital modulation, and non-periodic configurations,
enabling precise analysis and optimization for a broad range of communication
and sensing systems. Validation against experimental results for a space-time
modulated scattering structure demonstrates the accuracy and practical
applicability of the proposed model.

</details>


### [10] [Disentangling coincident cell events using deep transfer learning and compressive sensing](https://arxiv.org/abs/2507.13176)
*Moritz Leuthner,Rafael Vorländer,Oliver Hayden*

Main category: eess.SP

TL;DR: 提出了一种结合全卷积神经网络（FCN）和压缩感知（CS）的混合框架，用于解决单细胞分析中的信号重叠问题，显著提高了事件恢复率和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 单细胞分析在诊断和细胞治疗中至关重要，但信号重叠会严重影响数据准确性。

Method: 使用FCN估计重叠事件数量，并通过CS模块重建单个信号成分，结合可解释性工具确保透明度。

Result: 相比传统方法，恢复事件数量提升21%，分类准确率超过97%。

Conclusion: 该框架为非光学单细胞传感平台提供了自动化、通用化的解决方案，扩展了细胞计数在医学中的应用。

Abstract: Accurate single-cell analysis is critical for diagnostics, immunomonitoring,
and cell therapy, but coincident events - where multiple cells overlap in a
sensing zone - can severely compromise signal fidelity. We present a hybrid
framework combining a fully convolutional neural network (FCN) with compressive
sensing (CS) to disentangle such overlapping events in one-dimensional sensor
data. The FCN, trained on bead-derived datasets, accurately estimates
coincident event counts and generalizes to immunomagnetically labeled CD4+ and
CD14+ cells in whole blood without retraining. Using this count, the CS module
reconstructs individual signal components with high fidelity, enabling precise
recovery of single-cell features, including velocity, amplitude, and
hydrodynamic diameter. Benchmarking against conventional state-machine
algorithms shows superior performance - recovering up to 21% more events and
improving classification accuracy beyond 97%. Explinability via class
activation maps and parameterized Gaussian template fitting ensures
transparency and clinical interpretability. Demonstrated with magnetic flow
cytometry (MFC), the framework is compatible with other waveform-generating
modalities, including impedance cytometry, nanopore, and resistive pulse
sensing. This work lays the foundation for next-generation non-optical
single-cell sensing platforms that are automated, generalizable, and capable of
resolving overlapping events, broadening the utility of cytometry in
translational medicine and precision diagnostics, e.g. cell-interaction
studies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [11] [Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training](https://arxiv.org/abs/2507.12507)
*Mingjie Liu,Shizhe Diao,Jian Hu,Ximing Lu,Xin Dong,Hao Zhang,Alexander Bukharin,Shaokun Zhang,Jiaqi Zeng,Makesh Narsimhan Sreedhar,Gerald Shen,David Mosallanezhad,Di Zhang,Jonas Yang,June Yang,Oleksii Kuchaiev,Guilin Liu,Zhiding Yu,Pavlo Molchanov,Yejin Choi,Jan Kautz,Yi Dong*

Main category: cs.LG

TL;DR: 本文研究了在小型语言模型上进行长期强化学习训练，通过改进的GRPO算法和多种训练技术，在数学、编程和逻辑推理任务上取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 受OpenAI O1和DeepSeek-R1等推理型语言模型成功的启发，研究者希望探索在小型语言模型上应用长期强化学习训练的效果，特别是在多样化推理领域中的表现。

Method: 采用大规模强化学习方法，结合可验证的奖励信号；改进Group Relative Policy Optimization (GRPO)算法；引入受控KL正则化、裁剪比率和周期性参考策略重置等技术来提升训练稳定性和泛化能力。

Result: 模型在多个推理任务上取得显著改进：数学任务提升14.7%，编程任务提升13.9%，逻辑推理任务提升54.8%。研究还公开发布了训练好的模型以促进后续研究。

Conclusion: 长期强化学习训练可以显著提升小型语言模型在复杂推理任务上的性能，关键成功因素包括使用可验证奖励任务、改进的GRPO算法以及多种提升训练稳定性的实用技术。

Abstract: Recent advancements in reasoning-focused language models such as OpenAI's O1
and DeepSeek-R1 have shown that scaling test-time computation-through
chain-of-thought reasoning and iterative exploration-can yield substantial
improvements on complex tasks like mathematics and code generation. These
breakthroughs have been driven by large-scale reinforcement learning (RL),
particularly when combined with verifiable reward signals that provide
objective and grounded supervision. In this report, we investigate the effects
of prolonged reinforcement learning on a small language model across a diverse
set of reasoning domains. Our work identifies several key ingredients for
effective training, including the use of verifiable reward tasks, enhancements
to Group Relative Policy Optimization (GRPO), and practical techniques to
improve training stability and generalization. We introduce controlled KL
regularization, clipping ratio, and periodic reference policy resets as
critical components for unlocking long-term performance gains. Our model
achieves significant improvements over strong baselines, including +14.7% on
math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate
continued research, we release our model publicly.

</details>


### [12] [The Serial Scaling Hypothesis](https://arxiv.org/abs/2507.12549)
*Yuxi Liu,Konpat Preechakul,Kananart Kuwaranancharoen,Yutong Bai*

Main category: cs.LG

TL;DR: 论文指出机器学习中并行化的局限性，强调某些问题本质上是串行的，并呼吁重视串行计算的重要性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习依赖大规模并行化，但某些问题（如数学推理、物理模拟和顺序决策）本质上是串行的，无法并行化。

Method: 通过复杂性理论形式化区分并行与串行问题，并分析现有并行架构的局限性。

Result: 证明并行架构在串行问题上存在根本性限制，需重新思考计算方式。

Conclusion: 未来AI需同时重视串行计算的扩展，以应对复杂推理任务。

Abstract: While machine learning has advanced through massive parallelization, we
identify a critical blind spot: some problems are fundamentally sequential.
These "inherently serial" problems-from mathematical reasoning to physical
simulations to sequential decision-making-require dependent computational steps
that cannot be parallelized. Drawing from complexity theory, we formalize this
distinction and demonstrate that current parallel-centric architectures face
fundamental limitations on such tasks. We argue that recognizing the serial
nature of computation holds profound implications on machine learning, model
design, hardware development. As AI tackles increasingly complex reasoning,
deliberately scaling serial computation-not just parallel computation-is
essential for continued progress.

</details>


### [13] [Can Mental Imagery Improve the Thinking Capabilities of AI Systems?](https://arxiv.org/abs/2507.12555)
*Slimane Larabi*

Main category: cs.LG

TL;DR: 论文探讨了将心理意象整合到机器思维框架中，以增强自主推理能力，并提出了一个由认知思维单元和三个辅助单元组成的框架。


<details>
  <summary>Details</summary>
Motivation: 现有模型缺乏自主行动或独立推理能力，且输入数据通常为显式查询，而心理意象在人类思维中起关键作用。

Method: 提出一个机器思维框架，包括认知思维单元及三个辅助单元（输入数据单元、需求单元和心理意象单元），数据以自然语言或草图表示。

Result: 验证测试结果展示了框架的有效性。

Conclusion: 整合心理意象的机器思维框架有望提升机器的自主推理能力。

Abstract: Although existing models can interact with humans and provide satisfactory
responses, they lack the ability to act autonomously or engage in independent
reasoning. Furthermore, input data in these models is typically provided as
explicit queries, even when some sensory data is already acquired.
  In addition, AI agents, which are computational entities designed to perform
tasks and make decisions autonomously based on their programming, data inputs,
and learned knowledge, have shown significant progress. However, they struggle
with integrating knowledge across multiple domains, unlike humans.
  Mental imagery plays a fundamental role in the brain's thinking process,
which involves performing tasks based on internal multisensory data, planned
actions, needs, and reasoning capabilities. In this paper, we investigate how
to integrate mental imagery into a machine thinking framework and how this
could be beneficial in initiating the thinking process. Our proposed machine
thinking framework integrates a Cognitive thinking unit supported by three
auxiliary units: the Input Data Unit, the Needs Unit, and the Mental Imagery
Unit. Within this framework, data is represented as natural language sentences
or drawn sketches, serving both informative and decision-making purposes. We
conducted validation tests for this framework, and the results are presented
and discussed.

</details>


### [14] [IncA-DES: An incremental and adaptive dynamic ensemble selection approach using online K-d tree neighborhood search for data streams with concept drift](https://arxiv.org/abs/2507.12573)
*Eduardo V. L. Barboza,Paulo R. Lisboa de Almeida,Alceu de Souza Britto Jr.,Robert Sabourin,Rafael M. O. Cruz*

Main category: cs.LG

TL;DR: 提出了一种名为IncA-DES的新方法，用于处理数据流中的概念漂移问题，通过生成局部专家和融合概念漂移检测器来提高分类准确性，同时优化处理时间。


<details>
  <summary>Details</summary>
Motivation: 数据流中的概念漂移问题对机器学习提出了挑战，现有方法在处理连续到达的数据时效率不足。

Method: 采用训练策略生成局部专家，融合概念漂移检测器，并使用重叠分类过滤器和在线K-d树算法优化处理时间。

Result: 实验表明，该方法在准确性和处理时间上优于七种先进方法，且在线K-d树融合显著减少了处理时间。

Conclusion: IncA-DES框架有效解决了概念漂移问题，并在实际应用中表现出色，代码已开源。

Abstract: Data streams pose challenges not usually encountered in batch-based ML. One
of them is concept drift, which is characterized by the change in data
distribution over time. Among many approaches explored in literature, the
fusion of classifiers has been showing good results and is getting growing
attention. DS methods, due to the ensemble being instance-based, seem to be an
efficient choice under drifting scenarios. However, some attention must be paid
to adapting such methods for concept drift. The training must be done in order
to create local experts, and the commonly used neighborhood-search DS may
become prohibitive with the continuous arrival of data. In this work, we
propose IncA-DES, which employs a training strategy that promotes the
generation of local experts with the assumption that different regions of the
feature space become available with time. Additionally, the fusion of a concept
drift detector supports the maintenance of information and adaptation to a new
concept. An overlap-based classification filter is also employed in order to
avoid using the DS method when there is a consensus in the neighborhood, a
strategy that we argue every DS method should employ, as it was shown to make
them more applicable and quicker. Moreover, aiming to reduce the processing
time of the kNN, we propose an Online K-d tree algorithm, which can quickly
remove instances without becoming inconsistent and deals with unbalancing
concerns that may occur in data streams. Experimental results showed that the
proposed framework got the best average accuracy compared to seven
state-of-the-art methods considering different levels of label availability and
presented the smaller processing time between the most accurate methods.
Additionally, the fusion with the Online K-d tree has improved processing time
with a negligible loss in accuracy. We have made our framework available in an
online repository.

</details>


### [15] [Leveraging Asynchronous Cross-border Market Data for Improved Day-Ahead Electricity Price Forecasting in European Markets](https://arxiv.org/abs/2507.13250)
*Maria Margarida Mascarenhas,Jilles De Blauwe,Mikael Amelin,Hussain Kazmi*

Main category: cs.LG

TL;DR: 研究探讨了利用不同市场关闸时间（GCT）的异步电价数据提升电价预测准确性，结果显示在比利时和瑞典市场分别提高了22%和9%的预测精度。


<details>
  <summary>Details</summary>
Motivation: 短期电价预测对电力市场调度至关重要，但现有数据驱动方法依赖输入数据质量，本文探索异步电价数据的潜在价值。

Method: 采用先进模型集成方法，结合德国-卢森堡、奥地利和瑞士等市场的电价数据进行分析。

Result: 比利时和瑞典市场的预测精度分别提升22%和9%，但频繁模型校准带来高计算成本，且并非更多数据总能提升性能。

Conclusion: 研究为欧洲电力市场参与者提供了优化竞价策略的指导，强调了数据选择和模型校准的重要性。

Abstract: Accurate short-term electricity price forecasting is crucial for
strategically scheduling demand and generation bids in day-ahead markets. While
data-driven techniques have shown considerable prowess in achieving high
forecast accuracy in recent years, they rely heavily on the quality of input
covariates. In this paper, we investigate whether asynchronously published
prices as a result of differing gate closure times (GCTs) in some bidding zones
can improve forecasting accuracy in other markets with later GCTs. Using a
state-of-the-art ensemble of models, we show significant improvements of 22%
and 9% in forecast accuracy in the Belgian (BE) and Swedish bidding zones (SE3)
respectively, when including price data from interconnected markets with
earlier GCT (Germany-Luxembourg, Austria, and Switzerland). This improvement
holds for both general as well as extreme market conditions. Our analysis also
yields further important insights: frequent model recalibration is necessary
for maximum accuracy but comes at substantial additional computational costs,
and using data from more markets does not always lead to better performance - a
fact we delve deeper into with interpretability analysis of the forecast
models. Overall, these findings provide valuable guidance for market
participants and decision-makers aiming to optimize bidding strategies within
increasingly interconnected and volatile European energy markets.

</details>


### [16] [Assay2Mol: large language model-based drug design using BioAssay context](https://arxiv.org/abs/2507.12574)
*Yifan Deng,Spencer S. Ericksen,Anthony Gitter*

Main category: cs.LG

TL;DR: Assay2Mol利用大型语言模型从生化筛选数据中生成候选药物分子，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生化筛选数据中的非结构化文本未被充分利用，阻碍了新药发现。

Method: Assay2Mol通过检索类似目标的现有筛选数据，利用上下文学习生成候选分子。

Result: Assay2Mol在生成候选配体分子方面优于现有机器学习方法，且分子更易合成。

Conclusion: Assay2Mol为早期药物发现提供了高效工具，充分利用了现有数据。

Abstract: Scientific databases aggregate vast amounts of quantitative data alongside
descriptive text. In biochemistry, molecule screening assays evaluate the
functional responses of candidate molecules against disease targets.
Unstructured text that describes the biological mechanisms through which these
targets operate, experimental screening protocols, and other attributes of
assays offer rich information for new drug discovery campaigns but has been
untapped because of that unstructured format. We present Assay2Mol, a large
language model-based workflow that can capitalize on the vast existing
biochemical screening assays for early-stage drug discovery. Assay2Mol
retrieves existing assay records involving targets similar to the new target
and generates candidate molecules using in-context learning with the retrieved
assay screening data. Assay2Mol outperforms recent machine learning approaches
that generate candidate ligand molecules for target protein structures, while
also promoting more synthesizable molecule generation.

</details>


### [17] [Ranking Vectors Clustering: Theory and Applications](https://arxiv.org/abs/2507.12583)
*Ali Fattahi,Ali Eshragh,Babak Aslani,Meysam Rabiee*

Main category: cs.LG

TL;DR: 该论文研究了基于排序向量的聚类问题（KRC），提出了高效的近似算法KRCA和分支定界算法BnB，显著提升了聚类效果和计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究排序向量聚类的实际需求，特别是在个性化和大规模决策中的应用，解决传统k-means聚类（KMC）无法直接处理排序向量的问题。

Method: 提出KRCA算法，通过迭代优化KMC的初始解，并结合分支定界（BnB）算法加速聚类重构，同时平衡解的质量和计算效率。

Result: 理论证明了KRCA和BnB的误差界限，实验表明KRCA在合成和真实数据集上优于基线方法，计算速度快且解质量高。

Conclusion: KRC在个性化和大规模决策中具有实际意义，为未来研究提供了方法论基础和见解。

Abstract: We study the problem of clustering ranking vectors, where each vector
represents preferences as an ordered list of distinct integers. Specifically,
we focus on the k-centroids ranking vectors clustering problem (KRC), which
aims to partition a set of ranking vectors into k clusters and identify the
centroid of each cluster. Unlike classical k-means clustering (KMC), KRC
constrains both the observations and centroids to be ranking vectors. We
establish the NP-hardness of KRC and characterize its feasible set. For the
single-cluster case, we derive a closed-form analytical solution for the
optimal centroid, which can be computed in linear time. To address the
computational challenges of KRC, we develop an efficient approximation
algorithm, KRCA, which iteratively refines initial solutions from KMC, referred
to as the baseline solution. Additionally, we introduce a branch-and-bound
(BnB) algorithm for efficient cluster reconstruction within KRCA, leveraging a
decision tree framework to reduce computational time while incorporating a
controlling parameter to balance solution quality and efficiency. We establish
theoretical error bounds for KRCA and BnB. Through extensive numerical
experiments on synthetic and real-world datasets, we demonstrate that KRCA
consistently outperforms baseline solutions, delivering significant
improvements in solution quality with fast computational times. This work
highlights the practical significance of KRC for personalization and
large-scale decision making, offering methodological advancements and insights
that can be built upon in future studies.

</details>


### [18] [Second-Order Bounds for [0,1]-Valued Regression via Betting Loss](https://arxiv.org/abs/2507.12584)
*Yinan Li,Kwang-Sung Jun*

Main category: cs.LG

TL;DR: 论文研究了$[0,1]$值回归问题，提出了一种新的损失函数（betting loss），实现了无需方差知识的方差自适应边界。


<details>
  <summary>Details</summary>
Motivation: 探讨在$[0,1]$值回归问题中，是否存在一种损失函数能够实现优于对数损失的方差依赖边界。

Method: 提出了一种新的损失函数（betting loss），并通过理论分析验证其性能。

Result: 证明了betting loss能够实现方差自适应边界，且无需预先知道方差信息。

Conclusion: betting loss在回归问题中表现优异，提供了一种无需显式建模方差的高效方法。

Abstract: We consider the $[0,1]$-valued regression problem in the i.i.d. setting. In a
related problem called cost-sensitive classification, \citet{foster21efficient}
have shown that the log loss minimizer achieves an improved generalization
bound compared to that of the squared loss minimizer in the sense that the
bound scales with the cost of the best classifier, which can be arbitrarily
small depending on the problem at hand. Such a result is often called a
first-order bound. For $[0,1]$-valued regression, we first show that the log
loss minimizer leads to a similar first-order bound. We then ask if there
exists a loss function that achieves a variance-dependent bound (also known as
a second order bound), which is a strict improvement upon first-order bounds.
We answer this question in the affirmative by proposing a novel loss function
called the betting loss. Our result is ``variance-adaptive'' in the sense that
the bound is attained \textit{without any knowledge about the variance}, which
is in contrast to modeling label (or reward) variance or the label distribution
itself explicitly as part of the function class such as distributional
reinforcement learning.

</details>


### [19] [Are encoders able to learn landmarkers for warm-starting of Hyperparameter Optimization?](https://arxiv.org/abs/2507.12604)
*Antoni Zajko,Katarzyna Woźnica*

Main category: cs.LG

TL;DR: 论文提出两种针对元任务（贝叶斯超参数优化的热启动）的表格表示学习方法，分别基于深度度量学习和地标重建，但实验表明这些方法虽能学习到与地标对齐的表示，但未显著提升元任务性能。


<details>
  <summary>Details</summary>
Motivation: 异构表格数据集在元学习中的表示仍是一个开放问题，现有方法依赖通用表示，本文针对特定元任务提出定制化表示学习方法。

Method: 提出两种方法：1）基于深度度量学习的表示学习；2）基于地标重建的表示学习。评估时结合元任务性能和对特定要求的满足程度。

Result: 实验显示，提出的编码器能有效学习与地标对齐的表示，但未显著提升超参数优化热启动的性能。

Conclusion: 定制化表示学习方法虽能捕捉地标特性，但在目标元任务中的性能提升有限。

Abstract: Effectively representing heterogeneous tabular datasets for meta-learning
purposes is still an open problem. Previous approaches rely on representations
that are intended to be universal. This paper proposes two novel methods for
tabular representation learning tailored to a specific meta-task -
warm-starting Bayesian Hyperparameter Optimization. Both follow the specific
requirement formulated by ourselves that enforces representations to capture
the properties of landmarkers. The first approach involves deep metric
learning, while the second one is based on landmarkers reconstruction. We
evaluate the proposed encoders in two ways. Next to the gain in the target
meta-task, we also use the degree of fulfillment of the proposed requirement as
the evaluation metric. Experiments demonstrate that while the proposed encoders
can effectively learn representations aligned with landmarkers, they may not
directly translate to significant performance gains in the meta-task of HPO
warm-starting.

</details>


### [20] [Learning What Matters: Probabilistic Task Selection via Mutual Information for Model Finetuning](https://arxiv.org/abs/2507.12612)
*Prateek Chanda,Saral Sureka,Parth Pratim Chatterjee,Krishnateja Killamsetty,Nikhil Shivakumar Nayak,Ganesh Ramakrishnan*

Main category: cs.LG

TL;DR: TASKPGM是一个基于马尔可夫随机场（MRF）的框架，用于优化大型语言模型（LLM）微调时的任务混合比例，通过最小化能量函数实现任务间平衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM微调的任务混合选择依赖手动和启发式方法，缺乏理论支持，TASKPGM旨在提供一种原则性和可扩展的解决方案。

Method: 使用行为差异（如Jensen Shannon Divergence和Pointwise Mutual Information）建模任务关系，通过MRF框架优化任务比例，提供闭式解。

Result: 在Llama 2和Mistral上验证，TASKPGM在MMLU和BIGBench等评估中表现优于传统方法，并提供任务影响的解释性分析。

Conclusion: TASKPGM为LLM微调提供了一种高效、鲁棒且可解释的任务混合优化方法。

Abstract: The performance of finetuned large language models (LLMs) hinges critically
on the composition of the training mixture. However, selecting an optimal blend
of task datasets remains a largely manual, heuristic driven process, with
practitioners often relying on uniform or size based sampling strategies. We
introduce TASKPGM, a principled and scalable framework for mixture optimization
that selects continuous task proportions by minimizing an energy function over
a Markov Random Field (MRF). Task relationships are modeled using behavioral
divergences such as Jensen Shannon Divergence and Pointwise Mutual Information
computed from the predictive distributions of single task finetuned models. Our
method yields a closed form solution under simplex constraints and provably
balances representativeness and diversity among tasks. We provide theoretical
guarantees, including weak submodularity for budgeted variants, and demonstrate
consistent empirical improvements on Llama 2 and Mistral across evaluation
suites such as MMLU and BIGBench. Beyond performance, TASKPGM offers
interpretable insights into task influence and mixture composition, making it a
powerful tool for efficient and robust LLM finetuning.

</details>


### [21] [BootSeer: Analyzing and Mitigating Initialization Bottlenecks in Large-Scale LLM Training](https://arxiv.org/abs/2507.12619)
*Rui Li,Xiaoyun Zhi,Jinxin Chi,Menghan Yu,Lixin Huang,Jia Zhu,Weilun Zhang,Xing Ma,Wenjia Liu,Zhicheng Zhu,Daowen Luo,Zuquan Song,Xin Yin,Chao Xiang,Shuguang Wang,Wencong Xiao,Gene Cooperman*

Main category: cs.LG

TL;DR: 该论文研究了大型语言模型（LLM）训练中的启动开销问题，提出了优化框架Bootseer，显著减少了启动时间。


<details>
  <summary>Details</summary>
Motivation: 工业级LLM训练中，启动开销导致大量GPU时间浪费（3.5%），影响迭代效率。

Method: 通过分析真实生产数据，设计Bootseer框架，优化容器加载、依赖安装和模型恢复三个瓶颈。

Result: Bootseer部署后，启动开销减少50%。

Conclusion: Bootseer有效解决了LLM训练中的启动瓶颈，提升了工业级训练效率。

Abstract: Large Language Models (LLMs) have become a cornerstone of modern AI, driving
breakthroughs in natural language processing and expanding into multimodal jobs
involving images, audio, and video. As with most computational software, it is
important to distinguish between ordinary runtime performance and startup
overhead. Prior research has focused on runtime performance: improving training
efficiency and stability. This work focuses instead on the increasingly
critical issue of startup overhead in training: the delay before training jobs
begin execution. Startup overhead is particularly important in large,
industrial-scale LLMs, where failures occur more frequently and multiple teams
operate in iterative update-debug cycles. In one of our training clusters, more
than 3.5% of GPU time is wasted due to startup overhead alone.
  In this work, we present the first in-depth characterization of LLM training
startup overhead based on real production data. We analyze the components of
startup cost, quantify its direct impact, and examine how it scales with job
size. These insights motivate the design of Bootseer, a system-level
optimization framework that addresses three primary startup bottlenecks: (a)
container image loading, (b) runtime dependency installation, and (c) model
checkpoint resumption. To mitigate these bottlenecks, Bootseer introduces three
techniques: (a) hot block record-and-prefetch, (b) dependency snapshotting, and
(c) striped HDFS-FUSE. Bootseer has been deployed in a production environment
and evaluated on real LLM training workloads, demonstrating a 50% reduction in
startup overhead.

</details>


### [22] [Reasoning-Finetuning Repurposes Latent Representations in Base Models](https://arxiv.org/abs/2507.12638)
*Jake Ward,Chuqiao Lin,Constantin Venhoff,Neel Nanda*

Main category: cs.LG

TL;DR: 研究发现，推理微调模型中的回溯行为部分由基础模型中已有的方向驱动，而非全新学习。


<details>
  <summary>Details</summary>
Motivation: 探索推理微调模型中回溯行为的底层机制，揭示其是否依赖基础模型的已有表征。

Method: 通过识别基础模型（Llama-3.1-8B）残差流中的特定方向，并验证其在蒸馏推理模型（DeepSeek-R1-Distill-Llama-8B）中诱导回溯的效果。

Result: 发现该方向在基础模型中不引发回溯，但在推理微调模型中有效，表明微调过程重新利用了已有表征。

Conclusion: 推理微调模型通过重新利用基础模型的已有表征形成新行为回路，而非从头学习新能力。

Abstract: Backtracking, an emergent behavior elicited by reasoning fine-tuning, has
been shown to be a key mechanism in reasoning models' enhanced capabilities.
Prior work has succeeded in manipulating this behavior via steering vectors,
but the underlying mechanism remains poorly understood. In this work, we show
that the emergence of backtracking in DeepSeek-R1-Distill-Llama-8B is in part
driven by a repurposed direction already present in base model activations.
Specifically, we identify a direction in base Llama-3.1-8B's residual stream
which systematically induces backtracking when used to steer the distilled
reasoning model, and find that the effects of steering with this direction
cannot be trivially explained by token-level attributes. We further find that
this direction does not induce backtracking in the base model, suggesting that
the reasoning finetuning process repurposes pre-existing representations to
form new behavioral circuits. Additionally, we hypothesize that this direction
is one of several which may work together to mediate backtracking. Our findings
offer a compelling picture that reasoning-finetuned models repurpose
pre-existing base model representations, rather than learn new capabilities
from scratch.

</details>


### [23] [Improving physics-informed neural network extrapolation via transfer learning and adaptive activation functions](https://arxiv.org/abs/2507.12659)
*Athanasios Papastathopoulos-Katsaros,Alexandra Stavrianidi,Zhandong Liu*

Main category: cs.LG

TL;DR: 本文提出了一种结合迁移学习和自适应激活函数的PINN改进方法，显著提升了模型在训练域外的外推性能。


<details>
  <summary>Details</summary>
Motivation: 尽管PINNs在结合物理定律和数据驱动建模方面表现出色，但其外推性能较差且对激活函数选择敏感。

Method: 通过迁移学习在扩展训练域中使用少量选定的配点，并提出自适应激活函数（标准激活函数的线性组合）。

Result: 实验表明，外推域中相对L2误差平均减少40%，平均绝对误差减少50%，且计算成本未显著增加。

Conclusion: 该方法有效提升了PINNs的外推能力和鲁棒性，为复杂科学和工程问题提供了更可靠的解决方案。

Abstract: Physics-Informed Neural Networks (PINNs) are deep learning models that
incorporate the governing physical laws of a system into the learning process,
making them well-suited for solving complex scientific and engineering
problems. Recently, PINNs have gained widespread attention as a powerful
framework for combining physical principles with data-driven modeling to
improve prediction accuracy. Despite their successes, however, PINNs often
exhibit poor extrapolation performance outside the training domain and are
highly sensitive to the choice of activation functions (AFs). In this paper, we
introduce a transfer learning (TL) method to improve the extrapolation
capability of PINNs. Our approach applies transfer learning (TL) within an
extended training domain, using only a small number of carefully selected
collocation points. Additionally, we propose an adaptive AF that takes the form
of a linear combination of standard AFs, which improves both the robustness and
accuracy of the model. Through a series of experiments, we demonstrate that our
method achieves an average of 40% reduction in relative L2 error and an average
of 50% reduction in mean absolute error in the extrapolation domain, all
without a significant increase in computational cost. The code is available at
https://github.com/LiuzLab/PINN-extrapolation .

</details>


### [24] [Federated Learning in Open- and Closed-Loop EMG Decoding: A Privacy and Performance Perspective](https://arxiv.org/abs/2507.12652)
*Kai Malcolm,César Uribe,Momona Yamagami*

Main category: cs.LG

TL;DR: 论文探讨了联邦学习在神经接口中的应用，分析了其在开放和闭环场景下的性能与隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 神经信号包含敏感信息，数据共享存在隐私问题，联邦学习提供了一种隐私保护的解决方案。

Method: 引入基于联邦学习的神经解码方法，并在高维肌电信号中系统评估其性能与隐私。

Result: 开放环中联邦学习表现优于本地学习，但闭环中需调整方法，本地学习性能更好但隐私风险更高。

Conclusion: 研究揭示了实时自适应应用中性能与隐私的权衡，需开发专为单用户应用设计的联邦学习方法。

Abstract: Invasive and non-invasive neural interfaces hold promise as high-bandwidth
input devices for next-generation technologies. However, neural signals
inherently encode sensitive information about an individual's identity and
health, making data sharing for decoder training a critical privacy challenge.
Federated learning (FL), a distributed, privacy-preserving learning framework,
presents a promising solution, but it remains unexplored in closed-loop
adaptive neural interfaces. Here, we introduce FL-based neural decoding and
systematically evaluate its performance and privacy using high-dimensional
electromyography signals in both open- and closed-loop scenarios. In open-loop
simulations, FL significantly outperformed local learning baselines,
demonstrating its potential for high-performance, privacy-conscious neural
decoding. In contrast, closed-loop user studies required adapting FL methods to
accommodate single-user, real-time interactions, a scenario not supported by
standard FL. This modification resulted in local learning decoders surpassing
the adapted FL approach in closed-loop performance, yet local learning still
carried higher privacy risks. Our findings highlight a critical
performance-privacy tradeoff in real-time adaptive applications and indicate
the need for FL methods specifically designed for co-adaptive, single-user
applications.

</details>


### [25] [A Kernel Distribution Closeness Testing](https://arxiv.org/abs/2507.12843)
*Zhijian Zhou,Liuhua Peng,Xunye Tian,Feng Liu*

Main category: cs.LG

TL;DR: 论文提出了一种新的分布接近性测试方法NAMMD，通过结合RKHS范数改进MMD，提高了测试能力。


<details>
  <summary>Details</summary>
Motivation: 现有DCT方法主要针对离散一维空间，难以处理复杂数据（如图像），且MMD在评估多分布对时信息不足。

Method: 设计NAMMD，通过RKHS范数缩放MMD值，并基于其渐近分布提出NAMMD-based DCT。

Result: 理论证明NAMMD-based DCT比MMD-based DCT具有更高的测试能力，实验验证了其有效性。

Conclusion: NAMMD不仅适用于DCT，还在两样本测试中表现优于MMD，具有理论和实验支持。

Abstract: The distribution closeness testing (DCT) assesses whether the distance
between a distribution pair is at least $\epsilon$-far. Existing DCT methods
mainly measure discrepancies between a distribution pair defined on discrete
one-dimensional spaces (e.g., using total variation), which limits their
applications to complex data (e.g., images). To extend DCT to more types of
data, a natural idea is to introduce maximum mean discrepancy (MMD), a powerful
measurement of the distributional discrepancy between two complex
distributions, into DCT scenarios. However, we find that MMD's value can be the
same for many pairs of distributions that have different norms in the same
reproducing kernel Hilbert space (RKHS), making MMD less informative when
assessing the closeness levels for multiple distribution pairs. To mitigate the
issue, we design a new measurement of distributional discrepancy, norm-adaptive
MMD (NAMMD), which scales MMD's value using the RKHS norms of distributions.
Based on the asymptotic distribution of NAMMD, we finally propose the
NAMMD-based DCT to assess the closeness levels of a distribution pair.
Theoretically, we prove that NAMMD-based DCT has higher test power compared to
MMD-based DCT, with bounded type-I error, which is also validated by extensive
experiments on many types of data (e.g., synthetic noise, real images).
Furthermore, we also apply the proposed NAMMD for addressing the two-sample
testing problem and find NAMMD-based two-sample test has higher test power than
the MMD-based two-sample test in both theory and experiments.

</details>


### [26] [Data Transformation Strategies to Remove Heterogeneity](https://arxiv.org/abs/2507.12677)
*Sangbong Yoo,Jaeyoung Lee,Chanyoung Yoon,Geonyeong Son,Hyein Hong,Seongbum Seo,Soobin Yim,Chanyoung Jung,Jungsoo Park,Misuk Kim,Yun Jang*

Main category: cs.LG

TL;DR: 论文探讨了数据异构性问题及其根源，系统分类并提出了解决数据格式差异的策略，同时指出了每种策略的挑战。


<details>
  <summary>Details</summary>
Motivation: 数据异构性因多种冲突因素而普遍存在，其复杂性常需专家介入解决。现有方法多关注数据结构和模式冲突，忽视了数据转换的关键作用。随着AI应用扩展，对高效数据准备的需求增长，数据转换变得至关重要。

Method: 系统分类并分析解决数据格式差异的策略，探讨每种策略的挑战。

Result: 揭示了数据异构性的复杂性及其根源，提出了针对数据格式差异的解决策略。

Conclusion: 数据转换在AI应用中至关重要，需进一步研究以优化策略并应对挑战。

Abstract: Data heterogeneity is a prevalent issue, stemming from various conflicting
factors, making its utilization complex. This uncertainty, particularly
resulting from disparities in data formats, frequently necessitates the
involvement of experts to find resolutions. Current methodologies primarily
address conflicts related to data structures and schemas, often overlooking the
pivotal role played by data transformation. As the utilization of artificial
intelligence (AI) continues to expand, there is a growing demand for a more
streamlined data preparation process, and data transformation becomes
paramount. It customizes training data to enhance AI learning efficiency and
adapts input formats to suit diverse AI models. Selecting an appropriate
transformation technique is paramount in preserving crucial data details.
Despite the widespread integration of AI across various industries,
comprehensive reviews concerning contemporary data transformation approaches
are scarce. This survey explores the intricacies of data heterogeneity and its
underlying sources. It systematically categorizes and presents strategies to
address heterogeneity stemming from differences in data formats, shedding light
on the inherent challenges associated with each strategy.

</details>


### [27] [PinFM: Foundation Model for User Activity Sequences at a Billion-scale Visual Discovery Platform](https://arxiv.org/abs/2507.12704)
*Xiangyi Chen,Kousik Rajesh,Matthew Lawhon,Zelun Wang,Hanyu Li,Haomiao Li,Saurabh Vishwas Joshi,Pong Eksombatchai,Jaewon Yang,Yi-Ping Hsu,Jiajing Xu,Charles Rosenberg*

Main category: cs.LG

TL;DR: PinFM是一个基于用户活动序列的推荐系统基础模型，通过预训练和微调处理大规模数据，优化了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决工业推荐系统中预训练模型面临的扩展性、成本和延迟挑战，同时捕捉用户活动与其他特征的交互。

Method: 预训练20B+参数的Transformer模型，结合创新技术如DCAT，优化基础设施和算法。

Result: 吞吐量提升600%，新物品参与度增加20%，已部署服务于5亿用户。

Conclusion: PinFM成功应对工业推荐系统的挑战，显著提升用户体验和系统效率。

Abstract: User activity sequences have emerged as one of the most important signals in
recommender systems. We present a foundational model, PinFM, for understanding
user activity sequences across multiple applications at a billion-scale visual
discovery platform. We pretrain a transformer model with 20B+ parameters using
extensive user activity data, then fine-tune it for specific applications,
efficiently coupling it with existing models. While this
pretraining-and-fine-tuning approach has been popular in other domains, such as
Vision and NLP, its application in industrial recommender systems presents
numerous challenges. The foundational model must be scalable enough to score
millions of items every second while meeting tight cost and latency constraints
imposed by these systems. Additionally, it should capture the interactions
between user activities and other features and handle new items that were not
present during the pretraining stage.
  We developed innovative techniques to address these challenges. Our
infrastructure and algorithmic optimizations, such as the Deduplicated
Cross-Attention Transformer (DCAT), improved our throughput by 600% on
Pinterest internal data. We demonstrate that PinFM can learn interactions
between user sequences and candidate items by altering input sequences, leading
to a 20% increase in engagement with new items. PinFM is now deployed to help
improve the experience of more than a half billion users across various
applications.

</details>


### [28] [From SGD to Spectra: A Theory of Neural Network Weight Dynamics](https://arxiv.org/abs/2507.12709)
*Brian Richard Olsen,Sam Fatehmanesh,Frank Xiao,Adarsh Kumarappan,Anirudh Gajula*

Main category: cs.LG

TL;DR: 论文通过连续时间矩阵值随机微分方程（SDE）框架，将SGD的微观动力学与权重矩阵奇异值谱的宏观演化联系起来，解释了深度学习中的‘bulk+tail’谱结构。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的训练动力学理论尚不清晰，需要一种理论框架来解释其训练过程中的奇异值谱演化。

Method: 开发了一个连续时间矩阵值SDE框架，推导出奇异值平方遵循Dyson布朗运动，并描述了其稳态分布为伽马型密度。

Result: 理论预测与Transformer和MLP架构的实验结果一致，验证了SDE框架的准确性。

Conclusion: 该研究为理解深度学习为何有效提供了严格的理论基础。

Abstract: Deep neural networks have revolutionized machine learning, yet their training
dynamics remain theoretically unclear-we develop a continuous-time,
matrix-valued stochastic differential equation (SDE) framework that rigorously
connects the microscopic dynamics of SGD to the macroscopic evolution of
singular-value spectra in weight matrices. We derive exact SDEs showing that
squared singular values follow Dyson Brownian motion with eigenvalue repulsion,
and characterize stationary distributions as gamma-type densities with
power-law tails, providing the first theoretical explanation for the
empirically observed 'bulk+tail' spectral structure in trained networks.
Through controlled experiments on transformer and MLP architectures, we
validate our theoretical predictions and demonstrate quantitative agreement
between SDE-based forecasts and observed spectral evolution, providing a
rigorous foundation for understanding why deep learning works.

</details>


### [29] [Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient Data-Centric Learning](https://arxiv.org/abs/2507.12750)
*Suorong Yang,Peijia Li,Yujie Liu,Zhiming Xu,Peng Ye,Wanli Ouyang,Furao Shen,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 提出了一种动态数据集剪枝框架，通过任务驱动难度和跨模态语义一致性自适应选择训练样本，利用预训练多模态基础模型提升样本选择效果。


<details>
  <summary>Details</summary>
Motivation: 现有数据集剪枝方法依赖静态启发式或任务特定指标，缺乏跨领域的鲁棒性和通用性。

Method: 结合任务驱动难度和跨模态语义一致性，利用预训练多模态基础模型动态选择样本。

Result: 框架能有效过滤无信息样本，提升训练效率和模型性能。

Conclusion: 跨模态对齐的样本选择方法为数据为中心的学习提供了更高效和鲁棒的实践方向。

Abstract: Modern deep models are trained on large real-world datasets, where data
quality varies and redundancy is common. Data-centric approaches such as
dataset pruning have shown promise in improving training efficiency and model
performance. However, most existing methods rely on static heuristics or
task-specific metrics, limiting their robustness and generalizability across
domains. In this work, we introduce a dynamic dataset pruning framework that
adaptively selects training samples based on both task-driven difficulty and
cross-modality semantic consistency. By incorporating supervision from
pretrained multimodal foundation models, our approach captures training
dynamics while effectively filtering out uninformative samples. Our work
highlights the potential of integrating cross-modality alignment for robust
sample selection, advancing data-centric learning toward more efficient and
robust practices across application domains.

</details>


### [30] [Layer Separation Deep Learning Model with Auxiliary Variables for Partial Differential Equations](https://arxiv.org/abs/2507.12766)
*Yaru Liu,Yiqi Gu*

Main category: cs.LG

TL;DR: 提出了一种新的优化框架LySep模型，通过引入辅助变量分离深度神经网络的层，以解决深度学习中损失函数非凸导致的优化问题。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习损失函数的高度非凸性，现有优化算法容易陷入局部最优或梯度问题，影响性能。

Method: 通过辅助变量表示每层的输出及其导数，将深度架构分解为一系列浅层架构，并设计交替方向算法。

Result: 理论分析表明LySep模型与原深度模型一致，高维数值结果验证了其在损失最小化和减少解误差方面的优势。

Conclusion: LySep模型有效解决了深度学习优化问题，提升了性能。

Abstract: In this paper, we propose a new optimization framework, the layer separation
(LySep) model, to improve the deep learning-based methods in solving partial
differential equations. Due to the highly non-convex nature of the loss
function in deep learning, existing optimization algorithms often converge to
suboptimal local minima or suffer from gradient explosion or vanishing,
resulting in poor performance. To address these issues, we introduce auxiliary
variables to separate the layers of deep neural networks. Specifically, the
output and its derivatives of each layer are represented by auxiliary
variables, effectively decomposing the deep architecture into a series of
shallow architectures. New loss functions with auxiliary variables are
established, in which only variables from two neighboring layers are coupled.
Corresponding algorithms based on alternating directions are developed, where
many variables can be updated optimally in closed forms. Moreover, we provide
theoretical analyses demonstrating the consistency between the LySep model and
the original deep model. High-dimensional numerical results validate our theory
and demonstrate the advantages of LySep in minimizing loss and reducing
solution error.

</details>


### [31] [A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models](https://arxiv.org/abs/2507.12774)
*Weijieying Ren,Jingxi Zhu,Zehao Liu,Tianxiang Zhao,Vasant Honavar*

Main category: cs.LG

TL;DR: 该论文综述了深度学习、大语言模型与电子健康记录（EHR）建模的最新进展，提出了一个统一的分类法，涵盖五个关键设计维度，并探讨了未来趋势和开放挑战。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）数据的异质性、时间不规则性和领域特异性带来了独特挑战，需要专门的方法来推动AI在医疗领域的应用。

Method: 通过分类法分析五个设计维度：数据为中心的方法、神经网络架构设计、学习策略、多模态学习和基于LLM的建模系统，并回顾代表性方法。

Result: 总结了数据质量增强、结构和时间表示、自监督学习以及与临床知识整合等方面的进展，并指出基础模型、LLM驱动的临床代理等新兴趋势。

Conclusion: 论文为AI驱动的EHR建模和临床决策支持提供了结构化路线图，并讨论了基准测试、可解释性等开放挑战。

Abstract: Artificial intelligence (AI) has demonstrated significant potential in
transforming healthcare through the analysis and modeling of electronic health
records (EHRs). However, the inherent heterogeneity, temporal irregularity, and
domain-specific nature of EHR data present unique challenges that differ
fundamentally from those in vision and natural language tasks. This survey
offers a comprehensive overview of recent advancements at the intersection of
deep learning, large language models (LLMs), and EHR modeling. We introduce a
unified taxonomy that spans five key design dimensions: data-centric
approaches, neural architecture design, learning-focused strategies, multimodal
learning, and LLM-based modeling systems. Within each dimension, we review
representative methods addressing data quality enhancement, structural and
temporal representation, self-supervised learning, and integration with
clinical knowledge. We further highlight emerging trends such as foundation
models, LLM-driven clinical agents, and EHR-to-text translation for downstream
reasoning. Finally, we discuss open challenges in benchmarking, explainability,
clinical alignment, and generalization across diverse clinical settings. This
survey aims to provide a structured roadmap for advancing AI-driven EHR
modeling and clinical decision support. For a comprehensive list of EHR-related
methods, kindly refer to https://survey-on-tabular-data.github.io/.

</details>


### [32] [Multi-Channel Graph Neural Network for Financial Risk Prediction of NEEQ Enterprises](https://arxiv.org/abs/2507.12787)
*Jianyu Zhu*

Main category: cs.LG

TL;DR: 提出了一种多通道深度学习框架，结合财务指标、文本披露和企业关系数据，用于预测新三板企业的财务风险。


<details>
  <summary>Details</summary>
Motivation: 新三板企业因规模小、财务韧性低，面临较高的财务风险，需一种全面的预测方法。

Method: 设计了三通道图同构网络（GIN），分别处理数值、文本和图数据，并通过注意力机制和门控单元融合。

Result: 在7,731家新三板企业数据上，模型在AUC、精确率、召回率和F1分数上显著优于传统方法和单模态基线。

Conclusion: 该研究为中小企业风险建模提供了理论和实践支持，并为监管者和投资者提供了数据驱动工具。

Abstract: With the continuous evolution of China's multi-level capital market, the
National Equities Exchange and Quotations (NEEQ), also known as the "New Third
Board," has become a critical financing platform for small and medium-sized
enterprises (SMEs). However, due to their limited scale and financial
resilience, many NEEQ-listed companies face elevated risks of financial
distress. To address this issue, we propose a multi-channel deep learning
framework that integrates structured financial indicators, textual disclosures,
and enterprise relationship data for comprehensive financial risk prediction.
Specifically, we design a Triple-Channel Graph Isomorphism Network (GIN) that
processes numeric, textual, and graph-based inputs separately. These
modality-specific representations are fused using an attention-based mechanism
followed by a gating unit to enhance robustness and prediction accuracy.
Experimental results on data from 7,731 real-world NEEQ companies demonstrate
that our model significantly outperforms traditional machine learning methods
and single-modality baselines in terms of AUC, Precision, Recall, and F1 Score.
This work provides theoretical and practical insights into risk modeling for
SMEs and offers a data-driven tool to support financial regulators and
investors.

</details>


### [33] [FLDmamba: Integrating Fourier and Laplace Transform Decomposition with Mamba for Enhanced Time Series Prediction](https://arxiv.org/abs/2507.12803)
*Qianru Zhang,Chenglei Yu,Haixin Wang,Yudong Yan,Yuansheng Cao,Siu-Ming Yiu,Tailin Wu,Hongzhi Yin*

Main category: cs.LG

TL;DR: FLDmamba框架结合傅里叶和拉普拉斯变换，解决了时间序列预测中的多尺度周期性和瞬态动态问题，提升了模型对数据噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据具有非平稳性、多尺度周期性和瞬态动态等复杂性，现有方法（如Transformer和Mamba）在长期预测中效率不足或无法有效捕捉这些特性。

Method: 提出FLDmamba框架，利用傅里叶和拉普拉斯变换捕捉多尺度周期性和瞬态动态，并增强对数据噪声的鲁棒性。

Result: FLDmamba在时间序列预测基准测试中表现优异，优于基于Transformer和其他Mamba的架构。

Conclusion: FLDmamba为时间序列预测提供了一种高效且鲁棒的解决方案，代码和数据已开源以促进可重复性。

Abstract: Time series prediction, a crucial task across various domains, faces
significant challenges due to the inherent complexities of time series data,
including non-stationarity, multi-scale periodicity, and transient dynamics,
particularly when tackling long-term predictions. While Transformer-based
architectures have shown promise, their quadratic complexity with sequence
length hinders their efficiency for long-term predictions. Recent advancements
in State-Space Models, such as Mamba, offer a more efficient alternative for
long-term modeling, but they cannot capture multi-scale periodicity and
transient dynamics effectively. Meanwhile, they are susceptible to data noise
issues in time series. This paper proposes a novel framework, FLDmamba (Fourier
and Laplace Transform Decomposition Mamba), addressing these limitations.
FLDmamba leverages the strengths of both Fourier and Laplace transforms to
effectively capture both multi-scale periodicity, transient dynamics within
time series data, and improve the robustness of the model to the data noise
issue. Our extensive experiments demonstrate that FLDmamba achieves superior
performance on time series prediction benchmarks, outperforming both
Transformer-based and other Mamba-based architectures. To promote the
reproducibility of our method, we have made both the code and data accessible
via the following
URL:{\href{https://github.com/AI4Science-WestlakeU/FLDmamba}{https://github.com/AI4Science-WestlakeU/\model}.

</details>


### [34] [PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for Large-Scale Genomics Database](https://arxiv.org/abs/2507.12805)
*Hui Sun,Yanfeng Ding,Liping Yi,Huidong Ma,Gang Wang,Xiaoguang Liu,Cheng Zhong,Wentong Cai*

Main category: cs.LG

TL;DR: PMKLC是一种基于并行多知识学习的无损压缩器，通过四种关键设计解决了现有压缩器的不足，显著提升了压缩比、吞吐量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的无损压缩器在压缩比、吞吐量和鲁棒性方面表现不足，限制了其在基因组数据库中的应用。

Method: 提出PMKLC，包括自动化多知识学习框架、GPU加速编码器、并行加速机制和两种压缩模式（单GPU和多GPU）。

Result: 在15个真实数据集上测试，PMKLC-S/M的压缩比平均提升73.609%和73.480%，吞吐量提升3.036倍和10.710倍，且鲁棒性和内存成本表现优异。

Conclusion: PMKLC在压缩性能、吞吐量和鲁棒性方面显著优于现有方法，适用于资源受限和复杂场景。

Abstract: Learning-based lossless compressors play a crucial role in large-scale
genomic database backup, storage, transmission, and management. However, their
1) inadequate compression ratio, 2) low compression \& decompression
throughput, and 3) poor compression robustness limit their widespread adoption
and application in both industry and academia. To solve those challenges, we
propose a novel \underline{P}arallel \underline{M}ulti-\underline{K}nowledge
\underline{L}earning-based \underline{C}ompressor (PMKLC) with four crucial
designs: 1) We propose an automated multi-knowledge learning-based compression
framework as compressors' backbone to enhance compression ratio and robustness;
2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression
throughput and computing resource usage; 3) we introduce data block
partitioning and Step-wise Model Passing (SMP) mechanisms for parallel
acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet
the complex application scenarios, where the former runs on a
resource-constrained single GPU and the latter is multi-GPU accelerated. We
benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15
real-world datasets with different species and data sizes. Compared to
baselines on the testing datasets, PMKLC-S/M achieve the average compression
ratio improvement up to 73.609\% and 73.480\%, the average throughput
improvement up to 3.036$\times$ and 10.710$\times$, respectively. Besides,
PMKLC-S/M also achieve the best robustness and competitive memory cost,
indicating its greater stability against datasets with different probability
distribution perturbations, and its strong ability to run on memory-constrained
devices.

</details>


### [35] [RONOM: Reduced-Order Neural Operator Modeling](https://arxiv.org/abs/2507.12814)
*Sven Dummer,Dongwei Ye,Christoph Brune*

Main category: cs.LG

TL;DR: 论文提出了RONOM框架，结合了降阶建模和算子学习的优点，解决了传统方法在网格灵活性和误差量化上的限制。


<details>
  <summary>Details</summary>
Motivation: 解决传统降阶建模在网格灵活性上的不足，以及算子学习在误差量化上的缺失。

Method: 结合降阶建模和算子学习，提出RONOM框架，并建立离散化误差界限。

Result: RONOM在输入泛化、空间超分辨率和离散化鲁棒性上表现优异，并提供了时间超分辨率的新见解。

Conclusion: RONOM框架在解决偏微分方程时具有优越性能，同时提供了误差量化和灵活性。

Abstract: Time-dependent partial differential equations are ubiquitous in physics-based
modeling, but they remain computationally intensive in many-query scenarios,
such as real-time forecasting, optimal control, and uncertainty quantification.
Reduced-order modeling (ROM) addresses these challenges by constructing a
low-dimensional surrogate model but relies on a fixed discretization, which
limits flexibility across varying meshes during evaluation. Operator learning
approaches, such as neural operators, offer an alternative by parameterizing
mappings between infinite-dimensional function spaces, enabling adaptation to
data across different resolutions. Whereas ROM provides rigorous numerical
error estimates, neural operator learning largely focuses on discretization
convergence and invariance without quantifying the error between the
infinite-dimensional and the discretized operators. This work introduces the
reduced-order neural operator modeling (RONOM) framework, which bridges
concepts from ROM and operator learning. We establish a discretization error
bound analogous to those in ROM, and get insights into RONOM's discretization
convergence and discretization robustness. Moreover, two numerical examples are
presented that compare RONOM to existing neural operators for solving partial
differential equations. The results demonstrate that RONOM using standard
vector-to-vector neural networks achieves comparable performance in input
generalization and superior performance in both spatial super-resolution and
discretization robustness, while also offering novel insights into temporal
super-resolution scenarios.

</details>


### [36] [From Novelty to Imitation: Self-Distilled Rewards for Offline Reinforcement Learning](https://arxiv.org/abs/2507.12815)
*Gaurav Chaudhary,Laxmidhar Behera*

Main category: cs.LG

TL;DR: ReLOAD提出了一种基于随机网络蒸馏（RND）的离线强化学习奖励标注框架，无需人工标注奖励信号，通过专家演示生成内在奖励。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习依赖显式奖励标注，成本高且难以获取。ReLOAD旨在解决这一问题。

Method: 利用RND生成内在奖励：训练预测网络模仿目标网络的嵌入，预测误差作为奖励信号。

Result: 在D4RL基准测试中表现优异，性能接近传统奖励标注方法。

Conclusion: ReLOAD提供了一种无需人工标注的奖励生成方法，为离线强化学习提供了实用解决方案。

Abstract: Offline Reinforcement Learning (RL) aims to learn effective policies from a
static dataset without requiring further agent-environment interactions.
However, its practical adoption is often hindered by the need for explicit
reward annotations, which can be costly to engineer or difficult to obtain
retrospectively. To address this, we propose ReLOAD (Reinforcement Learning
with Offline Reward Annotation via Distillation), a novel reward annotation
framework for offline RL. Unlike existing methods that depend on complex
alignment procedures, our approach adapts Random Network Distillation (RND) to
generate intrinsic rewards from expert demonstrations using a simple yet
effective embedding discrepancy measure. First, we train a predictor network to
mimic a fixed target network's embeddings based on expert state transitions.
Later, the prediction error between these networks serves as a reward signal
for each transition in the static dataset. This mechanism provides a structured
reward signal without requiring handcrafted reward annotations. We provide a
formal theoretical construct that offers insights into how RND prediction
errors effectively serve as intrinsic rewards by distinguishing expert-like
transitions. Experiments on the D4RL benchmark demonstrate that ReLOAD enables
robust offline policy learning and achieves performance competitive with
traditional reward-annotated methods.

</details>


### [37] [Understanding the Evolution of the Neural Tangent Kernel at the Edge of Stability](https://arxiv.org/abs/2507.12837)
*Kaiqi Jiang,Jeremy Cohen,Yuanzhi Li*

Main category: cs.LG

TL;DR: 研究探讨了在Edge of Stability（EoS）现象中，NTK特征向量的动态变化，发现较大的学习率使最终NTK的特征向量与训练目标更对齐。


<details>
  <summary>Details</summary>
Motivation: 现有研究对EoS中NTK特征值行为有深入探讨，但对特征向量行为的理解仍不足。

Method: 通过实验观察不同架构下NTK特征向量的动态变化，并对两层线性网络进行理论分析。

Result: 较大的学习率导致最终NTK的特征向量与训练目标更对齐。

Conclusion: 研究加深了对深度学习中GD训练动态的理解。

Abstract: The study of Neural Tangent Kernels (NTKs) in deep learning has drawn
increasing attention in recent years. NTKs typically actively change during
training and are related to feature learning. In parallel, recent work on
Gradient Descent (GD) has found a phenomenon called Edge of Stability (EoS), in
which the largest eigenvalue of the NTK oscillates around a value inversely
proportional to the step size. However, although follow-up works have explored
the underlying mechanism of such eigenvalue behavior in depth, the
understanding of the behavior of the NTK eigenvectors during EoS is still
missing. This paper examines the dynamics of NTK eigenvectors during EoS in
detail. Across different architectures, we observe that larger learning rates
cause the leading eigenvectors of the final NTK, as well as the full NTK
matrix, to have greater alignment with the training target. We then study the
underlying mechanism of this phenomenon and provide a theoretical analysis for
a two-layer linear network. Our study enhances the understanding of GD training
dynamics in deep learning.

</details>


### [38] [Transformer-Based Person Identification via Wi-Fi CSI Amplitude and Phase Perturbations](https://arxiv.org/abs/2507.12854)
*Danilo Avola,Andrea Bernardini,Francesco Danese,Mario Lezoche,Maurizio Mancini,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 论文提出了一种基于Transformer的方法，通过Wi-Fi信号中的CSI（信道状态信息）实现静止状态下的人体识别，准确率达99.82%。


<details>
  <summary>Details</summary>
Motivation: 探索无线信号在无运动状态下的生物识别潜力，填补现有基于运动模式的无线识别方法的不足。

Method: 使用双分支Transformer架构分别处理CSI的幅度和相位信息，并通过预处理（异常值去除、平滑、相位校准）提升信号质量。

Result: 在控制室内环境中，使用低成本Wi-Fi硬件实现了99.82%的分类准确率，优于卷积和多层感知器基线。

Conclusion: 证明了CSI扰动具有生物特征编码能力，展示了低成本Wi-Fi硬件在无源、无设备人体识别中的实际应用潜力。

Abstract: Wi-Fi sensing is gaining momentum as a non-intrusive and privacy-preserving
alternative to vision-based systems for human identification. However, person
identification through wireless signals, particularly without user motion,
remains largely unexplored. Most prior wireless-based approaches rely on
movement patterns, such as walking gait, to extract biometric cues. In
contrast, we propose a transformer-based method that identifies individuals
from Channel State Information (CSI) recorded while the subject remains
stationary. CSI captures fine-grained amplitude and phase distortions induced
by the unique interaction between the human body and the radio signal. To
support evaluation, we introduce a dataset acquired with ESP32 devices in a
controlled indoor environment, featuring six participants observed across
multiple orientations. A tailored preprocessing pipeline, including outlier
removal, smoothing, and phase calibration, enhances signal quality. Our
dual-branch transformer architecture processes amplitude and phase modalities
separately and achieves 99.82\% classification accuracy, outperforming
convolutional and multilayer perceptron baselines. These results demonstrate
the discriminative potential of CSI perturbations, highlighting their capacity
to encode biometric traits in a consistent manner. They further confirm the
viability of passive, device-free person identification using low-cost
commodity Wi-Fi hardware in real-world settings.

</details>


### [39] [Supervised Fine Tuning on Curated Data is Reinforcement Learning (and can be improved)](https://arxiv.org/abs/2507.12856)
*Chongli Qin,Jost Tobias Springenberg*

Main category: cs.LG

TL;DR: 论文提出了一种改进的行为克隆方法iw-SFT，通过重要性加权优化RL目标，性能优于传统SFT。


<details>
  <summary>Details</summary>
Motivation: 探讨行为克隆（BC）与强化学习（RL）的联系，改进监督微调（SFT）以提升性能。

Method: 提出重要性加权监督微调（iw-SFT），优化更紧的RL目标下界，并支持质量评分数据训练。

Result: iw-SFT在语言模型和连续控制任务中表现优异，例如在AIME 2024数据集上达到66.7%。

Conclusion: iw-SFT是一种简单有效的改进方法，性能接近高级RL算法。

Abstract: Behavior Cloning (BC) on curated (or filtered) data is the predominant
paradigm for supervised fine-tuning (SFT) of large language models; as well as
for imitation learning of control policies. Here, we draw on a connection
between this successful strategy and the theory and practice of finding optimal
policies via Reinforcement Learning (RL). Building on existing literature, we
clarify that SFT can be understood as maximizing a lower bound on the RL
objective in a sparse reward setting. Giving support to its often observed good
performance. From this viewpoint, we realize that a small modification to SFT
leads to an importance weighted variant that behaves closer to training with RL
as it: i) optimizes a tighter bound to the RL objective and, ii) can improve
performance compared to SFT on curated data. We refer to this variant as
importance weighted supervised fine-tuning (iw-SFT). We show that it is easy to
implement and can be further generalized to training with quality scored data.
The resulting SFT variants are competitive with more advanced RL algorithms for
large language models and for training policies in continuous control tasks.
For example achieving 66.7% on the AIME 2024 dataset.

</details>


### [40] [An Investigation of Ear-EEG Signals for a Novel Biometric Authentication System](https://arxiv.org/abs/2507.12873)
*Danilo Avola,Giancarlo Crocetti,Gian Luca Foresti,Daniele Pannone,Claudio Piciarelli,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 研究探讨了利用耳部EEG信号进行生物识别的可行性，提出了一种基于深度神经网络的用户友好框架，实验结果显示平均准确率为82%。


<details>
  <summary>Details</summary>
Motivation: 传统头皮EEG生物识别系统因设备复杂导致可用性低，耳部EEG提供了一种更实用的替代方案。

Method: 从耳部EEG信号中提取时域和频域特征，通过全连接深度神经网络进行主体识别。

Result: 实验结果显示在主体识别场景中平均准确率为82%。

Conclusion: 耳部EEG有望成为下一代现实世界生物识别系统的可行方向。

Abstract: This work explores the feasibility of biometric authentication using EEG
signals acquired through in-ear devices, commonly referred to as ear-EEG.
Traditional EEG-based biometric systems, while secure, often suffer from low
usability due to cumbersome scalp-based electrode setups. In this study, we
propose a novel and practical framework leveraging ear-EEG signals as a
user-friendly alternative for everyday biometric authentication. The system
extracts an original combination of temporal and spectral features from ear-EEG
signals and feeds them into a fully connected deep neural network for subject
identification. Experimental results on the only currently available ear-EEG
dataset suitable for different purposes, including biometric authentication,
demonstrate promising performance, with an average accuracy of 82\% in a
subject identification scenario. These findings confirm the potential of
ear-EEG as a viable and deployable direction for next-generation real-world
biometric systems.

</details>


### [41] [Topology-Aware Activation Functions in Neural Networks](https://arxiv.org/abs/2507.12874)
*Pavel Snopov,Oleg R. Musin*

Main category: cs.LG

TL;DR: 论文提出两种新型激活函数SmoothSplit和ParametricSplit，用于增强神经网络在训练中对数据拓扑结构的处理能力，优于传统ReLU函数。


<details>
  <summary>Details</summary>
Motivation: 传统激活函数（如ReLU）在处理数据拓扑结构时存在局限性，特别是在低维数据场景中表现不足。

Method: 提出SmoothSplit和ParametricSplit两种激活函数，具备拓扑“切割”能力，能有效转换复杂数据流形。

Result: 实验表明，ParametricSplit在低维数据中表现优于传统激活函数，同时在高维数据中保持竞争力。

Conclusion: 拓扑感知激活函数在神经网络架构中有潜在应用价值。

Abstract: This study explores novel activation functions that enhance the ability of
neural networks to manipulate data topology during training. Building on the
limitations of traditional activation functions like $\mathrm{ReLU}$, we
propose $\mathrm{SmoothSplit}$ and $\mathrm{ParametricSplit}$, which introduce
topology "cutting" capabilities. These functions enable networks to transform
complex data manifolds effectively, improving performance in scenarios with
low-dimensional layers. Through experiments on synthetic and real-world
datasets, we demonstrate that $\mathrm{ParametricSplit}$ outperforms
traditional activations in low-dimensional settings while maintaining
competitive performance in higher-dimensional ones. Our findings highlight the
potential of topology-aware activation functions in advancing neural network
architectures. The code is available via
https://github.com/Snopoff/Topology-Aware-Activations.

</details>


### [42] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng,Hengkai Tan,Xinyi Mao,Guodong Liu,Shuhe Huang,Chendong Xiang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: VIDAR是一个两阶段框架，通过扩散模型预训练和掩码逆向动力学模型，显著减少数据需求并提升双手机器人操作的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作面临数据稀缺和异构性问题，需要一种更高效的方法来提升泛化能力。

Method: 结合大规模视频预训练和掩码逆向动力学模型，利用多视角视频数据预训练，并通过掩码提取动作相关信息。

Result: 仅需20分钟人类演示（1%的典型数据需求），VIDAR在未见过的任务和背景中表现优异，超越现有方法。

Conclusion: 视频基础模型与掩码动作预测结合，为双手机器人操作提供了可扩展和泛化的解决方案。

Abstract: Bimanual robotic manipulation, which involves the coordinated control of two
robotic arms, is foundational for solving challenging tasks. Despite recent
progress in general-purpose manipulation, data scarcity and embodiment
heterogeneity remain serious obstacles to further scaling up in bimanual
settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning
(VIDAR), a two-stage framework that leverages large-scale, diffusion-based
video pre-training and a novel masked inverse dynamics model for action
prediction. We pre-train the video diffusion model on 750K multi-view videos
from three real-world bimanual robot platforms, utilizing a unified observation
space that encodes robot, camera, task, and scene contexts. Our masked inverse
dynamics model learns masks to extract action-relevant information from
generated trajectories without requiring pixel-level labels, and the masks can
effectively generalize to unseen backgrounds. Our experiments demonstrate that
with only 20 minutes of human demonstrations on an unseen robot platform (only
1% of typical data requirements), VIDAR generalizes to unseen tasks and
backgrounds with strong semantic understanding, surpassing state-of-the-art
methods. Our findings highlight the potential of video foundation models,
coupled with masked action prediction, to enable scalable and generalizable
robotic manipulation in diverse real-world settings.

</details>


### [43] [Learning to Reject Low-Quality Explanations via User Feedback](https://arxiv.org/abs/2507.12900)
*Luca Stradiotti,Dario Pesenti,Stefano Teso,Jesse Davis*

Main category: cs.LG

TL;DR: 论文提出了一种拒绝低质量解释的框架（LtX），并引入了ULER（用户为中心的低质量解释拒绝器），通过学习人类评分和特征相关性判断来评估解释质量。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用中，机器学习预测器的解释质量参差不齐，影响用户信任和决策。

Method: 提出ULER框架，通过学习人类评分和特征相关性判断来设计拒绝器。

Result: ULER在多个基准测试和人类标注数据集上优于现有方法。

Conclusion: ULER能有效识别和拒绝低质量解释，提升用户信任和决策质量。

Abstract: Machine Learning predictors are increasingly being employed in high-stakes
applications such as credit scoring. Explanations help users unpack the reasons
behind their predictions, but are not always "high quality''. That is,
end-users may have difficulty interpreting or believing them, which can
complicate trust assessment and downstream decision-making. We argue that
classifiers should have the option to refuse handling inputs whose predictions
cannot be explained properly and introduce a framework for learning to reject
low-quality explanations (LtX) in which predictors are equipped with a rejector
that evaluates the quality of explanations. In this problem setting, the key
challenges are how to properly define and assess explanation quality and how to
design a suitable rejector. Focusing on popular attribution techniques, we
introduce ULER (User-centric Low-quality Explanation Rejector), which learns a
simple rejector from human ratings and per-feature relevance judgments to
mirror human judgments of explanation quality. Our experiments show that ULER
outperforms both state-of-the-art and explanation-aware learning to reject
strategies at LtX on eight classification and regression benchmarks and on a
new human-annotated dataset, which we will publicly release to support future
research.

</details>


### [44] [Fremer: Lightweight and Effective Frequency Transformer for Workload Forecasting in Cloud Services](https://arxiv.org/abs/2507.12908)
*Jiadong Chen,Hengyu Ye,Fuxin Jiang,Xiao He,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: Fremer是一种高效的工作负载预测模型，优于现有Transformer模型，并在准确性和效率上显著提升。


<details>
  <summary>Details</summary>
Motivation: 云服务中工作负载预测对自动扩展和调度至关重要，但现有Transformer模型在效率和规模上不足。

Method: 提出Fremer模型，基于频域处理复杂周期性模式，满足高效、准确和鲁棒性需求。

Result: Fremer在多个指标上优于SOTA模型，并在实际应用中显著降低延迟和资源消耗。

Conclusion: Fremer在云服务工作负载预测中表现出色，兼具高效性和实用性。

Abstract: Workload forecasting is pivotal in cloud service applications, such as
auto-scaling and scheduling, with profound implications for operational
efficiency. Although Transformer-based forecasting models have demonstrated
remarkable success in general tasks, their computational efficiency often falls
short of the stringent requirements in large-scale cloud environments. Given
that most workload series exhibit complicated periodic patterns, addressing
these challenges in the frequency domain offers substantial advantages. To this
end, we propose Fremer, an efficient and effective deep forecasting model.
Fremer fulfills three critical requirements: it demonstrates superior
efficiency, outperforming most Transformer-based forecasting models; it
achieves exceptional accuracy, surpassing all state-of-the-art (SOTA) models in
workload forecasting; and it exhibits robust performance for multi-period
series. Furthermore, we collect and open-source four high-quality, open-source
workload datasets derived from ByteDance's cloud services, encompassing
workload data from thousands of computing instances. Extensive experiments on
both our proprietary datasets and public benchmarks demonstrate that Fremer
consistently outperforms baseline models, achieving average improvements of
5.5% in MSE, 4.7% in MAE, and 8.6% in SMAPE over SOTA models, while
simultaneously reducing parameter scale and computational costs. Additionally,
in a proactive auto-scaling test based on Kubernetes, Fremer improves average
latency by 18.78% and reduces resource consumption by 2.35%, underscoring its
practical efficacy in real-world applications.

</details>


### [45] [Robust Explanations Through Uncertainty Decomposition: A Path to Trustworthier AI](https://arxiv.org/abs/2507.12913)
*Chenrui Zhu,Louenas Bounia,Vu Linh Nguyen,Sébastien Destercke,Arthur Hoarau*

Main category: cs.LG

TL;DR: 论文提出利用预测不确定性作为传统可解释性方法的补充，区分数据相关和模型相关的不确定性，以指导解释方法的选择。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习模型复杂度增加，可解释性降低，需要透明化的预测方法。

Method: 通过区分数据相关（aleatoric）和模型相关（epistemic）不确定性，选择适合的解释方法。

Result: 实验表明，这种不确定性感知方法提高了传统机器学习和深度学习场景中解释的鲁棒性和可实现性。

Conclusion: 不确定性量化与解耦驱动的可解释性框架能有效提升模型预测的透明性。

Abstract: Recent advancements in machine learning have emphasized the need for
transparency in model predictions, particularly as interpretability diminishes
when using increasingly complex architectures. In this paper, we propose
leveraging prediction uncertainty as a complementary approach to classical
explainability methods. Specifically, we distinguish between aleatoric
(data-related) and epistemic (model-related) uncertainty to guide the selection
of appropriate explanations. Epistemic uncertainty serves as a rejection
criterion for unreliable explanations and, in itself, provides insight into
insufficient training (a new form of explanation). Aleatoric uncertainty
informs the choice between feature-importance explanations and counterfactual
explanations. This leverages a framework of explainability methods driven by
uncertainty quantification and disentanglement. Our experiments demonstrate the
impact of this uncertainty-aware approach on the robustness and attainability
of explanations in both traditional machine learning and deep learning
scenarios.

</details>


### [46] [Trace Reconstruction with Language Models](https://arxiv.org/abs/2507.12927)
*Franziska Weindel,Michael Girsch,Reinhard Heckel*

Main category: cs.LG

TL;DR: TReconLM利用语言模型改进迹重建，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DNA数据存储中的错误需要通过算法校正，迹重建是数据检索的关键步骤。

Method: 预训练语言模型于合成数据，并在真实数据上微调以适应技术特定错误模式。

Result: TReconLM显著优于现有迹重建算法，包括深度学习方法，恢复更多无错误序列。

Conclusion: TReconLM通过语言模型有效提升迹重建性能，适用于DNA数据存储等应用。

Abstract: The general trace reconstruction problem seeks to recover an original
sequence from its noisy copies independently corrupted by deletions,
insertions, and substitutions. This problem arises in applications such as DNA
data storage, a promising storage medium due to its high information density
and longevity. However, errors introduced during DNA synthesis, storage, and
sequencing require correction through algorithms and codes, with trace
reconstruction often used as part of the data retrieval process. In this work,
we propose TReconLM, which leverages language models trained on next-token
prediction for trace reconstruction. We pretrain language models on synthetic
data and fine-tune on real-world data to adapt to technology-specific error
patterns. TReconLM outperforms state-of-the-art trace reconstruction
algorithms, including prior deep learning approaches, recovering a
substantially higher fraction of sequences without error.

</details>


### [47] [From a Mixed-Policy Perspective: Improving Differentiable Automatic Post-editing Optimization](https://arxiv.org/abs/2507.12931)
*Hongze Tan*

Main category: cs.LG

TL;DR: 论文提出了两种改进DAPO算法的混合策略方法，通过引入预训练策略和重用零奖励样本，提升了训练稳定性和样本效率。


<details>
  <summary>Details</summary>
Motivation: 标准策略梯度方法在稀疏奖励环境中存在不稳定性和样本效率低的问题，需要改进。

Method: 1. 引入预训练的稳定指导策略（$\piphi$）提供离策略经验；2. 重用零奖励样本作为专家策略指导的批次。

Result: 理论分析表明，两种方法的优化目标在强化学习框架下收敛到最优解，平衡了探索与利用。

Conclusion: 混合策略框架显著提升了策略优化的稳定性和效率。

Abstract: This paper introduces two novel modifications to the Differentiable Automatic
Post-editing Optimization (DAPO) algorithm, approached from a mixed-policy
perspective. Standard policy gradient methods can suffer from instability and
sample inefficiency, particularly in sparse reward settings. To address this,
we first propose a method that incorporates a pre-trained, stable guiding
policy ($\piphi$) to provide off-policy experience, thereby regularizing the
training of the target policy ($\pion$). This approach improves training
stability and convergence speed by adaptively adjusting the learning step size.
Secondly, we extend this idea to re-utilize zero-reward samples, which are
often discarded by dynamic sampling strategies like DAPO's. By treating these
samples as a distinct batch guided by the expert policy, we further enhance
sample efficiency. We provide a theoretical analysis for both methods,
demonstrating that their objective functions converge to the optimal solution
within the established theoretical framework of reinforcement learning. The
proposed mixed-policy framework effectively balances exploration and
exploitation, promising more stable and efficient policy optimization.

</details>


### [48] [MC$^2$A: Enabling Algorithm-Hardware Co-Design for Efficient Markov Chain Monte Carlo Acceleration](https://arxiv.org/abs/2507.12935)
*Shirui Zhao,Jun Yin,Lingyun Yao,Martin Andraud,Wannes Meert,Marian Verhelst*

Main category: cs.LG

TL;DR: MC²A是一种算法-硬件协同设计框架，用于高效灵活地加速MCMC算法，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: MCMC算法在机器学习的规划和推理中广泛应用，但高计算成本限制了其在大规模问题中的可行性。现有加速方案在硬件灵活性或系统级效率上存在不足。

Method: MC²A通过扩展处理器性能模型分析MCMC工作负载多样性，提出参数化硬件加速器架构，并引入新型Gumbel采样器。

Result: MC²A在CPU、GPU、TPU和现有MCMC加速器上分别实现了307.6倍、1.4倍、2.0倍和84.2倍的加速。

Conclusion: MC²A证明了通用硬件加速的可行性，有望推动MCMC在多样化应用中的普及。

Abstract: An increasing number of applications are exploiting sampling-based algorithms
for planning, optimization, and inference. The Markov Chain Monte Carlo (MCMC)
algorithms form the computational backbone of this emerging branch of machine
learning. Unfortunately, the high computational cost limits their feasibility
for large-scale problems and real-world applications, and the existing MCMC
acceleration solutions are either limited in hardware flexibility or fail to
maintain efficiency at the system level across a variety of end-to-end
applications. This paper introduces \textbf{MC$^2$A}, an algorithm-hardware
co-design framework, enabling efficient and flexible optimization for MCMC
acceleration. Firstly, \textbf{MC$^2$A} analyzes the MCMC workload diversity
through an extension of the processor performance roofline model with a 3rd
dimension to derive the optimal balance between the compute, sampling and
memory parameters. Secondly, \textbf{MC$^2$A} proposes a parametrized hardware
accelerator architecture with flexible and efficient support of MCMC kernels
with a pipeline of ISA-programmable tree-structured processing units,
reconfigurable samplers and a crossbar interconnect to support irregular
access. Thirdly, the core of \textbf{MC$^2$A} is powered by a novel Gumbel
sampler that eliminates exponential and normalization operations. In the
end-to-end case study, \textbf{MC$^2$A} achieves an overall {$307.6\times$,
$1.4\times$, $2.0\times$, $84.2\times$} speedup compared to the CPU, GPU, TPU
and state-of-the-art MCMC accelerator. Evaluated on various representative MCMC
workloads, this work demonstrates and exploits the feasibility of general
hardware acceleration to popularize MCMC-based solutions in diverse application
domains.

</details>


### [49] [Probabilistic Soundness Guarantees in LLM Reasoning Chains](https://arxiv.org/abs/2507.12948)
*Weiqiu You,Anton Xue,Shreya Havaldar,Delip Rao,Helen Jin,Chris Callison-Burch,Eric Wong*

Main category: cs.LG

TL;DR: 论文提出ARES框架，通过概率方法检测LLM推理链中的传播错误，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM错误检测方法未能有效处理推理链中早期错误的传播问题。

Method: 引入ARES框架，基于先前已验证的前提判断每一步，提供统计保证。

Result: 在四个基准测试中表现最佳（72.1% Macro-F1，提升8.2点），在长推理链中尤其稳健（90.3% F1，提升27.6点）。

Conclusion: ARES能有效检测传播错误，显著优于现有方法。

Abstract: In reasoning chains generated by large language models (LLMs), initial errors
often propagate and undermine the reliability of the final conclusion. Current
LLM-based error detection methods often fail to detect propagated errors
because they do not properly account for how earlier errors might corrupt
judgments of downstream reasoning. To better detect such propagated errors, we
introduce Autoregressive Reasoning Entailment Stability (ARES), a novel
probabilistic framework that prevents error propagation by judging each claim
based only on previously-assessed sound premises. This inductive method yields
a nuanced score for each step and provides certified statistical guarantees of
its soundness, rather than a brittle binary label. ARES achieves
state-of-the-art performance across four benchmarks (72.1% Macro-F1, +8.2
points) and demonstrates superior robustness on very long synthetic reasoning
chains, where it excels at detecting propagated errors (90.3% F1, +27.6
points).

</details>


### [50] [Insights into a radiology-specialised multimodal large language model with sparse autoencoders](https://arxiv.org/abs/2507.12950)
*Kenza Bouzid,Shruthi Bannur,Daniel Coelho de Castro,Anton Schwaighofer,Javier Alvarez-Valle,Stephanie L. Hyland*

Main category: cs.LG

TL;DR: 该研究通过稀疏自编码器（SAEs）对医学多模态大模型MAIRA-2进行内部表征的可解释性分析，识别出临床相关概念，并探索了这些特征对模型行为的影响。


<details>
  <summary>Details</summary>
Motivation: 提高AI模型在医疗应用中的安全性、透明度和信任度，尤其是在决策具有重大后果的场景。

Method: 使用Matryoshka-SAE对MAIRA-2的内部表征进行可解释性分析，并通过大规模自动化方法识别临床相关概念。

Result: 识别出多种临床相关概念（如医疗设备、病理特征等），并通过“steering”展示了这些特征对模型行为的影响。

Conclusion: 研究揭示了MAIRA-2内部学习的概念，为医学大模型的机制理解和可解释性提供了初步见解，并推动了模型透明度的提升。

Abstract: Interpretability can improve the safety, transparency and trust of AI models,
which is especially important in healthcare applications where decisions often
carry significant consequences. Mechanistic interpretability, particularly
through the use of sparse autoencoders (SAEs), offers a promising approach for
uncovering human-interpretable features within large transformer-based models.
In this study, we apply Matryoshka-SAE to the radiology-specialised multimodal
large language model, MAIRA-2, to interpret its internal representations. Using
large-scale automated interpretability of the SAE features, we identify a range
of clinically relevant concepts - including medical devices (e.g., line and
tube placements, pacemaker presence), pathologies such as pleural effusion and
cardiomegaly, longitudinal changes and textual features. We further examine the
influence of these features on model behaviour through steering, demonstrating
directional control over generations with mixed success. Our results reveal
practical and methodological challenges, yet they offer initial insights into
the internal concepts learned by MAIRA-2 - marking a step toward deeper
mechanistic understanding and interpretability of a radiology-adapted
multimodal large language model, and paving the way for improved model
transparency. We release the trained SAEs and interpretations:
https://huggingface.co/microsoft/maira-2-sae.

</details>


### [51] [A Spectral Interpretation of Redundancy in a Graph Reservoir](https://arxiv.org/abs/2507.12963)
*Anna Bison,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 论文提出了一种基于Fairing算法的MRGNN变体，用于解决图神经网络中的过平滑问题，并通过理论分析和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络中重复应用层操作会导致过平滑问题，即图信号向图拉普拉斯算子的低频分量收敛。本文旨在通过改进MRGNN中的储层定义来解决这一问题。

Method: 提出了一种基于Fairing算法的MRGNN变体，该算法通过拉普拉斯算子实现带通滤波，避免平滑过程中的收缩，并连接至图分类任务。

Result: 理论分析表明，调整谱系数可以调节冗余随机游走的贡献。初步实验验证了该方法的潜力。

Conclusion: 该方法为图神经网络中的过平滑问题提供了新的解决方案，并指出了未来研究方向。

Abstract: Reservoir computing has been successfully applied to graphs as a
preprocessing method to improve the training efficiency of Graph Neural
Networks (GNNs). However, a common issue that arises when repeatedly applying
layer operators on graphs is over-smoothing, which consists in the convergence
of graph signals toward low-frequency components of the graph Laplacian. This
work revisits the definition of the reservoir in the Multiresolution Reservoir
Graph Neural Network (MRGNN), a spectral reservoir model, and proposes a
variant based on a Fairing algorithm originally introduced in the field of
surface design in computer graphics. This algorithm provides a pass-band
spectral filter that allows smoothing without shrinkage, and it can be adapted
to the graph setting through the Laplacian operator. Given its spectral
formulation, this method naturally connects to GNN architectures for tasks
where smoothing, when properly controlled, can be beneficial,such as graph
classification. The core contribution of the paper lies in the theoretical
analysis of the algorithm from a random walks perspective. In particular, it
shows how tuning the spectral coefficients can be interpreted as modulating the
contribution of redundant random walks. Exploratory experiments based on the
MRGNN architecture illustrate the potential of this approach and suggest
promising directions for future research.

</details>


### [52] [WaveletInception Networks for Drive-by Vibration-Based Infrastructure Health Monitoring](https://arxiv.org/abs/2507.12969)
*Reza Riahi Samani,Alfredo Nunez,Bart De Schutter*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的框架WaveletInception-BiLSTM，用于通过振动信号监测基础设施健康状态，结合小波变换和LSTM提取特征，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基础设施健康监测方法在提取振动信号的频谱和时序信息方面存在不足，需要一种无需预处理且能高效分析多尺度特征的方法。

Method: 使用WaveletInception-BiLSTM网络，结合Learnable Wavelet Packet Transform提取频谱特征，1D Inception提取多尺度特征，LSTM整合时序信息，BiLSTM进行双向时序建模。

Result: 在铁路轨道刚度估计的案例中，模型显著优于现有方法，实现了高分辨率的局部健康评估。

Conclusion: 该方法为自动化、高精度的基础设施健康监测提供了有效解决方案，具有广泛应用潜力。

Abstract: This paper presents a novel deep learning-based framework for infrastructure
health monitoring using drive-by vibration response signals. Recognizing the
importance of spectral and temporal information, we introduce the
WaveletInception-BiLSTM network. The WaveletInception feature extractor
utilizes a Learnable Wavelet Packet Transform (LWPT) as the stem for extracting
vibration signal features, incorporating spectral information in the early
network layers. This is followed by 1D Inception networks that extract
multi-scale, high-level features at deeper layers. The extracted vibration
signal features are then integrated with operational conditions via a Long
Short-term Memory (LSTM) layer. The resulting feature extraction network
effectively analyzes drive-by vibration signals across various measurement
speeds without preprocessing and uses LSTM to capture interrelated temporal
dependencies among different modes of information and to create feature vectors
for health condition estimation. The estimator head is designed with a
sequential modeling architecture using bidirectional LSTM (BiLSTM) networks,
capturing bi-directional temporal relationships from drive-by measurements.
This architecture allows for a high-resolution, beam-level assessment of
infrastructure health conditions. A case study focusing on railway track
stiffness estimation with simulated drive-by vibration signals shows that the
model significantly outperforms state-of-the-art methods in estimating railway
ballast and railpad stiffness parameters. Results underscore the potential of
this approach for accurate, localized, and fully automated drive-by
infrastructure health monitoring.

</details>


### [53] [A Distributed Generative AI Approach for Heterogeneous Multi-Domain Environments under Data Sharing constraints](https://arxiv.org/abs/2507.12979)
*Youssef Tawfilis,Hossam Amer,Minar El-Aasser,Tallal Elshabrawy*

Main category: cs.LG

TL;DR: 提出了一种去中心化的GAN训练方法，结合KLD加权的集群联邦学习和异构U型分割学习，解决了数据异构和设备异构问题，同时保护数据隐私。


<details>
  <summary>Details</summary>
Motivation: 生成模型训练通常需要大量数据和计算资源，但在现实环境中难以获取，且隐私和版权问题限制了数据共享。

Method: 采用KLD加权的集群联邦学习处理数据异构和多域数据集，结合异构U型分割学习解决设备异构问题，确保不共享原始数据。

Result: 实验显示，该方法在图像生成和分类指标上显著优于基准方法，生成分数提高1.1-2.2倍，分类指标平均提升10%。

Conclusion: 该方法有效解决了去中心化环境中的关键挑战，同时保护了数据隐私，性能显著优于现有方法。

Abstract: Federated Learning has gained increasing attention for its ability to enable
multiple nodes to collaboratively train machine learning models without sharing
their raw data. At the same time, Generative AI -- particularly Generative
Adversarial Networks (GANs) -- have achieved remarkable success across a wide
range of domains, such as healthcare, security, and Image Generation. However,
training generative models typically requires large datasets and significant
computational resources, which are often unavailable in real-world settings.
Acquiring such resources can be costly and inefficient, especially when many
underutilized devices -- such as IoT devices and edge devices -- with varying
capabilities remain idle. Moreover, obtaining large datasets is challenging due
to privacy concerns and copyright restrictions, as most devices are unwilling
to share their data. To address these challenges, we propose a novel approach
for decentralized GAN training that enables the utilization of distributed data
and underutilized, low-capability devices while not sharing data in its raw
form. Our approach is designed to tackle key challenges in decentralized
environments, combining KLD-weighted Clustered Federated Learning to address
the issues of data heterogeneity and multi-domain datasets, with Heterogeneous
U-Shaped split learning to tackle the challenge of device heterogeneity under
strict data sharing constraints -- ensuring that no labels or raw data, whether
real or synthetic, are ever shared between nodes. Experimental results shows
that our approach demonstrates consistent and significant improvements across
key performance metrics, where it achieves 1.1x -- 2.2x higher image generation
scores, an average 10% boost in classification metrics (up to 50% in
multi-domain non-IID settings), in much lower latency compared to several
benchmarks. Find our code at https://github.com/youssefga28/HuSCF-GAN.

</details>


### [54] [FedGA: A Fair Federated Learning Framework Based on the Gini Coefficient](https://arxiv.org/abs/2507.12983)
*ShanBin Liu*

Main category: cs.LG

TL;DR: FedGA是一种公平感知的联邦学习算法，通过动态调整聚合权重和公平干预时机，减少客户端间的性能差异。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中数据异构性导致客户端性能差异显著，引发公平性问题。

Method: 使用基尼系数衡量性能差异，建立其与模型更新规模的关系，动态调整聚合权重。

Result: 实验表明FedGA显著改善了公平性指标（如方差和基尼系数），同时保持整体性能。

Conclusion: FedGA有效解决了联邦学习中的公平性问题，平衡了性能与公平性。

Abstract: Fairness has emerged as one of the key challenges in federated learning. In
horizontal federated settings, data heterogeneity often leads to substantial
performance disparities across clients, raising concerns about equitable model
behavior. To address this issue, we propose FedGA, a fairness-aware federated
learning algorithm. We first employ the Gini coefficient to measure the
performance disparity among clients. Based on this, we establish a relationship
between the Gini coefficient $G$ and the update scale of the global model
${U_s}$, and use this relationship to adaptively determine the timing of
fairness intervention. Subsequently, we dynamically adjust the aggregation
weights according to the system's real-time fairness status, enabling the
global model to better incorporate information from clients with relatively
poor performance.We conduct extensive experiments on the Office-Caltech-10,
CIFAR-10, and Synthetic datasets. The results show that FedGA effectively
improves fairness metrics such as variance and the Gini coefficient, while
maintaining strong overall performance, demonstrating the effectiveness of our
approach.

</details>


### [55] [Teach Old SAEs New Domain Tricks with Boosting](https://arxiv.org/abs/2507.12990)
*Nikita Koriagin,Yaroslav Aksenov,Daniil Laptev,Gleb Gerasimov,Nikita Balagansky,Daniil Gavrilov*

Main category: cs.LG

TL;DR: 论文提出了一种残差学习方法，通过训练次级稀疏自编码器（SAE）来捕捉主SAE在领域特定文本中遗漏的特征，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏自编码器（SAE）在领域特定特征捕捉上的不足，避免完全重新训练。

Method: 训练次级SAE建模主SAE在领域特定文本上的重构误差，推理时结合两者输出。

Result: 在多个专业领域中，模型在交叉熵和解释方差指标上均有显著提升。

Conclusion: 该方法能高效地将新领域知识融入现有SAE，同时保持通用任务性能，为LLM的针对性机制解释提供了新途径。

Abstract: Sparse Autoencoders have emerged as powerful tools for interpreting the
internal representations of Large Language Models, yet they often fail to
capture domain-specific features not prevalent in their training corpora. This
paper introduces a residual learning approach that addresses this feature
blindness without requiring complete retraining. We propose training a
secondary SAE specifically to model the reconstruction error of a pretrained
SAE on domain-specific texts, effectively capturing features missed by the
primary model. By summing the outputs of both models during inference, we
demonstrate significant improvements in both LLM cross-entropy and explained
variance metrics across multiple specialized domains. Our experiments show that
this method efficiently incorporates new domain knowledge into existing SAEs
while maintaining their performance on general tasks. This approach enables
researchers to selectively enhance SAE interpretability for specific domains of
interest, opening new possibilities for targeted mechanistic interpretability
of LLMs.

</details>


### [56] [SMART: Relation-Aware Learning of Geometric Representations for Knowledge Graphs](https://arxiv.org/abs/2507.13001)
*Kossi Amouzouvi,Bowen Song,Andrea Coletta,Luigi Bellomarini,Jens Lehmann,Sahar Vahdati*

Main category: cs.LG

TL;DR: 提出了一种基于几何变换的知识图谱嵌入框架，通过评估关系与几何变换的匹配度，为每个关系分配最佳变换或通过多数投票选择统一变换。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入模型使用单一或组合几何变换表示所有关系，未能充分考虑关系特异性。

Method: 提出框架评估关系与几何变换的匹配度，通过注意力机制学习低维向量空间中的关系特异性几何变换，并将其用于高维关系嵌入。

Result: 在三个基准知识图谱和一个真实金融知识图谱上验证了模型的有效性，性能与领先模型相当。

Conclusion: 该框架通过关系特异性几何变换提升了知识图谱嵌入的性能。

Abstract: Knowledge graph representation learning approaches provide a mapping between
symbolic knowledge in the form of triples in a knowledge graph (KG) and their
feature vectors. Knowledge graph embedding (KGE) models often represent
relations in a KG as geometric transformations. Most state-of-the-art (SOTA)
KGE models are derived from elementary geometric transformations (EGTs), such
as translation, scaling, rotation, and reflection, or their combinations. These
geometric transformations enable the models to effectively preserve specific
structural and relational patterns of the KG. However, the current use of EGTs
by KGEs remains insufficient without considering relation-specific
transformations. Although recent models attempted to address this problem by
ensembling SOTA baseline models in different ways, only a single or composite
version of geometric transformations are used by such baselines to represent
all the relations. In this paper, we propose a framework that evaluates how
well each relation fits with different geometric transformations. Based on this
ranking, the model can: (1) assign the best-matching transformation to each
relation, or (2) use majority voting to choose one transformation type to apply
across all relations. That is, the model learns a single relation-specific EGT
in low dimensional vector space through an attention mechanism. Furthermore, we
use the correlation between relations and EGTs, which are learned in a low
dimension, for relation embeddings in a high dimensional vector space. The
effectiveness of our models is demonstrated through comprehensive evaluations
on three benchmark KGs as well as a real-world financial KG, witnessing a
performance comparable to leading models

</details>


### [57] [Fault detection and diagnosis for the engine electrical system of a space launcher based on a temporal convolutional autoencoder and calibrated classifiers](https://arxiv.org/abs/2507.13022)
*Luis Basora,Louison Bocquet-Nouaille,Elinirina Robinson,Serge Le Gonidec*

Main category: cs.LG

TL;DR: 提出一种基于时间卷积自编码器的电气系统故障检测与诊断方法，满足置信度估计、异常检测和误报控制等需求。


<details>
  <summary>Details</summary>
Motivation: 为下一代可重复使用太空发射器的健康监测开发机载故障检测与诊断能力，满足更广泛的关键需求。

Method: 使用时间卷积自编码器提取特征，结合二元和多类分类器进行故障检测与诊断，采用简单技术处理异常数据和误报。

Result: 在模拟数据上验证了方法的有效性，但需进一步用真实数据测试以确保成熟度。

Conclusion: 该方法是一个有前景的初步解决方案，但需进一步验证以满足实际应用需求。

Abstract: In the context of the health monitoring for the next generation of reusable
space launchers, we outline a first step toward developing an onboard fault
detection and diagnostic capability for the electrical system that controls the
engine valves. Unlike existing approaches in the literature, our solution is
designed to meet a broader range of key requirements. This includes estimating
confidence levels for predictions, detecting out-of-distribution (OOD) cases,
and controlling false alarms. The proposed solution is based on a temporal
convolutional autoencoder to automatically extract low-dimensional features
from raw sensor data. Fault detection and diagnosis are respectively carried
out using a binary and a multiclass classifier trained on the autoencoder
latent and residual spaces. The classifiers are histogram-based gradient
boosting models calibrated to output probabilities that can be interpreted as
confidence levels. A relatively simple technique, based on inductive conformal
anomaly detection, is used to identify OOD data. We leverage other simple yet
effective techniques, such as cumulative sum control chart (CUSUM) to limit the
false alarms, and threshold moving to address class imbalance in fault
detection. The proposed framework is highly configurable and has been evaluated
on simulated data, covering both nominal and anomalous operational scenarios.
The results indicate that our solution is a promising first step, though
testing with real data will be necessary to ensure that it achieves the
required maturity level for operational use.

</details>


### [58] [Confidence-Filtered Relevance (CFR): An Interpretable and Uncertainty-Aware Machine Learning Framework for Naturalness Assessment in Satellite Imagery](https://arxiv.org/abs/2507.13034)
*Ahmed Emam,Ribana Roscher*

Main category: cs.LG

TL;DR: 论文提出了一种名为CFR的数据中心框架，结合LRP Attention Rollout和Deep Deterministic Uncertainty估计，分析模型不确定性对卫星图像自然性评估解释性的影响。


<details>
  <summary>Details</summary>
Motivation: 当前卫星图像和机器学习方法在监测自然保护区时缺乏可解释性和不确定性意识，且未探讨不确定性如何影响自然性评估。

Method: 提出Confidence-Filtered Relevance (CFR)框架，结合LRP Attention Rollout和DDU估计，根据不确定性阈值划分数据集，分析不确定性对解释性的影响。

Result: 在AnthroProtect数据集上，CFR对灌木丛、森林和湿地赋予更高相关性，且随着不确定性增加，解释性下降，熵值增加。

Conclusion: CFR提供了一种基于确定性的数据中心方法，用于评估卫星图像中模式对自然性的相关性。

Abstract: Protected natural areas play a vital role in ecological balance and ecosystem
services. Monitoring these regions at scale using satellite imagery and machine
learning is promising, but current methods often lack interpretability and
uncertainty-awareness, and do not address how uncertainty affects naturalness
assessment. In contrast, we propose Confidence-Filtered Relevance (CFR), a
data-centric framework that combines LRP Attention Rollout with Deep
Deterministic Uncertainty (DDU) estimation to analyze how model uncertainty
influences the interpretability of relevance heatmaps. CFR partitions the
dataset into subsets based on uncertainty thresholds, enabling systematic
analysis of how uncertainty shapes the explanations of naturalness in satellite
imagery. Applied to the AnthroProtect dataset, CFR assigned higher relevance to
shrublands, forests, and wetlands, aligning with other research on naturalness
assessment. Moreover, our analysis shows that as uncertainty increases, the
interpretability of these relevance heatmaps declines and their entropy grows,
indicating less selective and more ambiguous attributions. CFR provides a
data-centric approach to assess the relevance of patterns to naturalness in
satellite imagery based on their associated certainty.

</details>


### [59] [The Power of Architecture: Deep Dive into Transformer Architectures for Long-Term Time Series Forecasting](https://arxiv.org/abs/2507.13043)
*Lefei Shen,Mouxiang Chen,Han Fu,Xiaoxue Ren,Xiaoyun Joy Wang,Jianling Sun,Zhuo Li,Chenghao Liu*

Main category: cs.LG

TL;DR: 论文探讨了Transformer架构在长期时间序列预测（LTSF）中的最佳设计，提出了一种新的分类法以解耦时间序列特定设计，并通过实验验证了双向注意力、完整预测聚合和直接映射范式的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在LTSF任务中的架构设计差异较大，且常与时间序列特定设计紧密耦合，难以单独评估架构本身的影响。

Method: 提出一种新的分类法，解耦注意力机制、预测聚合、预测范式和归一化层等关键设计，并通过实验比较不同架构。

Result: 实验表明，双向注意力与联合注意力最有效，完整预测聚合提升性能，直接映射范式优于自回归方法。

Conclusion: 结合最优架构选择的模型表现优于现有模型，为未来LTSF中Transformer架构设计提供了指导。

Abstract: Transformer-based models have recently become dominant in Long-term Time
Series Forecasting (LTSF), yet the variations in their architecture, such as
encoder-only, encoder-decoder, and decoder-only designs, raise a crucial
question: What Transformer architecture works best for LTSF tasks? However,
existing models are often tightly coupled with various time-series-specific
designs, making it difficult to isolate the impact of the architecture itself.
To address this, we propose a novel taxonomy that disentangles these designs,
enabling clearer and more unified comparisons of Transformer architectures. Our
taxonomy considers key aspects such as attention mechanisms, forecasting
aggregations, forecasting paradigms, and normalization layers. Through
extensive experiments, we uncover several key insights: bi-directional
attention with joint-attention is most effective; more complete forecasting
aggregation improves performance; and the direct-mapping paradigm outperforms
autoregressive approaches. Furthermore, our combined model, utilizing optimal
architectural choices, consistently outperforms several existing models,
reinforcing the validity of our conclusions. We hope these findings offer
valuable guidance for future research on Transformer architectural designs in
LTSF. Our code is available at https://github.com/HALF111/TSF_architecture.

</details>


### [60] [On statistical learning of graphs](https://arxiv.org/abs/2507.13054)
*Vittorio Cipriani,Valentino Delle Rose,Luca San Mauro,Giovanni Solda*

Main category: cs.LG

TL;DR: 研究了基于无限图G的假设类的PAC和在线可学习性，其中每个副本通过顶点置换生成。主要结果表明，有限支持置换的PAC可学习性意味着G的完全同构类型的在线可学习性，且等价于自同构平凡性。还通过无限随机图的扩展性质松弛，刻画了不可学习的双顶点置换图类。最后，证明了对于所有G和k>2，k顶点置换的可学习性等价于2顶点置换的可学习性。


<details>
  <summary>Details</summary>
Motivation: 探索无限图在置换操作下的可学习性，特别是有限支持置换对学习性的影响，以及如何通过自同构性质刻画学习性。

Method: 通过分析图的置换操作（尤其是有限支持置换）对学习性的影响，结合自同构平凡性和无限随机图的扩展性质，对图进行分类和刻画。

Result: 1. 有限支持置换的PAC可学习性等价于完全同构类型的在线可学习性。2. 通过扩展性质松弛刻画了不可学习的双顶点置换图类。3. 对于k>2，k顶点置换的可学习性等价于2顶点置换的可学习性。

Conclusion: 无限图的可学习性可以通过有限支持置换和自同构性质来刻画，且k顶点置换的可学习性在k>2时简化为2顶点置换的可学习性。

Abstract: We study PAC and online learnability of hypothesis classes formed by copies
of a countably infinite graph G, where each copy is induced by permuting G's
vertices. This corresponds to learning a graph's labeling, knowing its
structure and label set. We consider classes where permutations move only
finitely many vertices. Our main result shows that PAC learnability of all such
finite-support copies implies online learnability of the full isomorphism type
of G, and is equivalent to the condition of automorphic triviality. We also
characterize graphs where copies induced by swapping two vertices are not
learnable, using a relaxation of the extension property of the infinite random
graph. Finally, we show that, for all G and k>2, learnability for k-vertex
permutations is equivalent to that for 2-vertex permutations, yielding a
four-class partition of infinite graphs, whose complexity we also determine
using tools coming from both descriptive set theory and computability theory.

</details>


### [61] [DASViT: Differentiable Architecture Search for Vision Transformer](https://arxiv.org/abs/2507.13079)
*Pengjin Wu,Ferrante Neri,Zhenhua Feng*

Main category: cs.LG

TL;DR: DASViT是一种可微分架构搜索方法，专为Vision Transformers设计，解决了传统方法在创新性和效率上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有NAS方法在探索Vision Transformers架构时面临创新性不足、计算资源消耗大和时间长的问题。

Method: 提出DASViT，一种可微分搜索方法，用于发现新颖的ViT架构。

Result: DASViT生成的架构突破了传统Transformer编码器设计，在多个数据集上优于ViT-B/16，且参数和计算量更少。

Conclusion: DASViT为Vision Transformers提供了一种高效且创新的架构搜索解决方案。

Abstract: Designing effective neural networks is a cornerstone of deep learning, and
Neural Architecture Search (NAS) has emerged as a powerful tool for automating
this process. Among the existing NAS approaches, Differentiable Architecture
Search (DARTS) has gained prominence for its efficiency and ease of use,
inspiring numerous advancements. Since the rise of Vision Transformers (ViT),
researchers have applied NAS to explore ViT architectures, often focusing on
macro-level search spaces and relying on discrete methods like evolutionary
algorithms. While these methods ensure reliability, they face challenges in
discovering innovative architectural designs, demand extensive computational
resources, and are time-intensive. To address these limitations, we introduce
Differentiable Architecture Search for Vision Transformer (DASViT), which
bridges the gap in differentiable search for ViTs and uncovers novel designs.
Experiments show that DASViT delivers architectures that break traditional
Transformer encoder designs, outperform ViT-B/16 on multiple datasets, and
achieve superior efficiency with fewer parameters and FLOPs.

</details>


### [62] [MUPAX: Multidimensional Problem Agnostic eXplainable AI](https://arxiv.org/abs/2507.13090)
*Vincenzo Dentamaro,Felice Franchini,Giuseppe Pirlo,Irina Voiculescu*

Main category: cs.LG

TL;DR: MUPAX是一种确定性、模型无关且保证收敛的XAI技术，通过结构化扰动分析提供特征重要性归因，适用于多维度任务，并能提升模型准确性。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法通常缺乏确定性、模型无关性或收敛保证，且可能在掩蔽时降低性能。MUPAX旨在解决这些问题，提供更可靠和高效的解释性。

Method: MUPAX基于测度论框架，通过结构化扰动分析发现输入的内在模式并消除虚假关系，适用于任意维度和损失函数。

Result: 在音频分类、图像分类、医学图像分析和解剖标志检测等任务中，MUPAX表现出维度无关的有效性，且能提升模型准确性。

Conclusion: MUPAX在生成精确、一致且可理解的解释方面优于现有XAI方法，是迈向可信AI系统的重要一步。

Abstract: Robust XAI techniques should ideally be simultaneously deterministic, model
agnostic, and guaranteed to converge. We propose MULTIDIMENSIONAL PROBLEM
AGNOSTIC EXPLAINABLE AI (MUPAX), a deterministic, model agnostic explainability
technique, with guaranteed convergency. MUPAX measure theoretic formulation
gives principled feature importance attribution through structured perturbation
analysis that discovers inherent input patterns and eliminates spurious
relationships. We evaluate MUPAX on an extensive range of data modalities and
tasks: audio classification (1D), image classification (2D), volumetric medical
image analysis (3D), and anatomical landmark detection, demonstrating dimension
agnostic effectiveness. The rigorous convergence guarantees extend to any loss
function and arbitrary dimensions, making MUPAX applicable to virtually any
problem context for AI. By contrast with other XAI methods that typically
decrease performance when masking, MUPAX not only preserves but actually
enhances model accuracy by capturing only the most important patterns of the
original data. Extensive benchmarking against the state of the XAI art
demonstrates MUPAX ability to generate precise, consistent and understandable
explanations, a crucial step towards explainable and trustworthy AI systems.
The source code will be released upon publication.

</details>


### [63] [Uncertainty-Aware Cross-Modal Knowledge Distillation with Prototype Learning for Multimodal Brain-Computer Interfaces](https://arxiv.org/abs/2507.13092)
*Hyo-Jeong Jang,Hye-Bin Shin,Seong-Whan Lee*

Main category: cs.LG

TL;DR: 本文提出了一种跨模态知识蒸馏框架，解决EEG学习中的模态差异和标签不一致问题，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受噪声和标签错误影响，导致模型性能下降。知识蒸馏虽能帮助，但面临模态差异和标签不一致的挑战。

Method: 提出原型相似性模块对齐特征语义，并设计任务特定的蒸馏头解决标签不一致问题。

Result: 实验表明，该方法在公共多模态数据集上优于单模态和多模态基线，提升了EEG情感回归和分类性能。

Conclusion: 该框架在BCI应用中具有潜力，有效解决了模态和标签不一致问题。

Abstract: Electroencephalography (EEG) is a fundamental modality for cognitive state
monitoring in brain-computer interfaces (BCIs). However, it is highly
susceptible to intrinsic signal errors and human-induced labeling errors, which
lead to label noise and ultimately degrade model performance. To enhance EEG
learning, multimodal knowledge distillation (KD) has been explored to transfer
knowledge from visual models with rich representations to EEG-based models.
Nevertheless, KD faces two key challenges: modality gap and soft label
misalignment. The former arises from the heterogeneous nature of EEG and visual
feature spaces, while the latter stems from label inconsistencies that create
discrepancies between ground truth labels and distillation targets. This paper
addresses semantic uncertainty caused by ambiguous features and weakly defined
labels. We propose a novel cross-modal knowledge distillation framework that
mitigates both modality and label inconsistencies. It aligns feature semantics
through a prototype-based similarity module and introduces a task-specific
distillation head to resolve label-induced inconsistency in supervision.
Experimental results demonstrate that our approach improves EEG-based emotion
regression and classification performance, outperforming both unimodal and
multimodal baselines on a public multimodal dataset. These findings highlight
the potential of our framework for BCI applications.

</details>


### [64] [NGTM: Substructure-based Neural Graph Topic Model for Interpretable Graph Generation](https://arxiv.org/abs/2507.13133)
*Yuanxin Zhuang,Dazhong Shen,Ying Sun*

Main category: cs.LG

TL;DR: NGTM提出了一种基于主题模型的图生成框架，通过隐式主题实现图的局部和全局可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有图生成方法缺乏可解释性，难以理解结构决策背后的逻辑。

Method: NGTM将图表示为隐式主题的混合，每个主题定义语义上有意义的子结构分布，并结合全局结构变量生成图。

Result: 实验表明NGTM在生成质量上具有竞争力，同时支持细粒度控制和可解释性。

Conclusion: NGTM通过主题级调整实现了图生成的透明性和可控性。

Abstract: Graph generation plays a pivotal role across numerous domains, including
molecular design and knowledge graph construction. Although existing methods
achieve considerable success in generating realistic graphs, their
interpretability remains limited, often obscuring the rationale behind
structural decisions. To address this challenge, we propose the Neural Graph
Topic Model (NGTM), a novel generative framework inspired by topic modeling in
natural language processing. NGTM represents graphs as mixtures of latent
topics, each defining a distribution over semantically meaningful
substructures, which facilitates explicit interpretability at both local and
global scales. The generation process transparently integrates these topic
distributions with a global structural variable, enabling clear semantic
tracing of each generated graph. Experiments demonstrate that NGTM achieves
competitive generation quality while uniquely enabling fine-grained control and
interpretability, allowing users to tune structural features or induce
biological properties through topic-level adjustments.

</details>


### [65] [NonverbalTTS: A Public English Corpus of Text-Aligned Nonverbal Vocalizations with Emotion Annotations for Text-to-Speech](https://arxiv.org/abs/2507.13155)
*Maksim Borisov,Egor Spirin,Daria Diatlova*

Main category: cs.LG

TL;DR: 论文提出了一个17小时的开源数据集NonverbalTTS（NVTTS），标注了10种非语言声音（NVs）和8种情感类别，用于改进表达性语音合成模型。


<details>
  <summary>Details</summary>
Motivation: 当前表达性语音合成模型受限于开源数据集中多样非语言声音（NVs）的缺乏。

Method: 数据集来自VoxCeleb和Expresso，通过自动检测和人工验证构建。提出一个集成ASR、NV标记、情感分类和融合算法的流程。

Result: 在NVTTS数据集上微调开源TTS模型，性能与闭源系统相当。

Conclusion: NVTTS解决了表达性TTS研究的关键瓶颈，数据集已开源。

Abstract: Current expressive speech synthesis models are constrained by the limited
availability of open-source datasets containing diverse nonverbal vocalizations
(NVs). In this work, we introduce NonverbalTTS (NVTTS), a 17-hour open-access
dataset annotated with 10 types of NVs (e.g., laughter, coughs) and 8 emotional
categories. The dataset is derived from popular sources, VoxCeleb and Expresso,
using automated detection followed by human validation. We propose a
comprehensive pipeline that integrates automatic speech recognition (ASR), NV
tagging, emotion classification, and a fusion algorithm to merge transcriptions
from multiple annotators. Fine-tuning open-source text-to-speech (TTS) models
on the NVTTS dataset achieves parity with closed-source systems such as
CosyVoice2, as measured by both human evaluation and automatic metrics,
including speaker similarity and NV fidelity. By releasing NVTTS and its
accompanying annotation guidelines, we address a key bottleneck in expressive
TTS research. The dataset is available at
https://huggingface.co/datasets/deepvk/NonverbalTTS.

</details>


### [66] [Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities](https://arxiv.org/abs/2507.13158)
*Hao Sun,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文综述了通过逆强化学习（IRL）在大语言模型（LLM）对齐中的最新进展，强调从人类数据构建神经奖励模型的必要性，并探讨了相关挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的普及，对齐问题成为提升模型可靠性、可控性和能力的关键挑战，而强化学习（RL）在此领域的应用备受关注。

Method: 通过逆强化学习（IRL）框架，分析LLM对齐中的RL技术，重点讨论神经奖励模型的构建及其实际影响。

Result: 总结了当前研究的关键挑战与机遇，提出了数据集、评估指标和高效训练技术等实际问题的解决方案。

Conclusion: 本文为LLM对齐领域提供了结构化综述，指出了未解决的问题，并展望了通过RL和IRL技术改进对齐的未来方向。

Abstract: In the era of Large Language Models (LLMs), alignment has emerged as a
fundamental yet challenging problem in the pursuit of more reliable,
controllable, and capable machine intelligence. The recent success of reasoning
models and conversational AI systems has underscored the critical role of
reinforcement learning (RL) in enhancing these systems, driving increased
research interest at the intersection of RL and LLM alignment. This paper
provides a comprehensive review of recent advances in LLM alignment through the
lens of inverse reinforcement learning (IRL), emphasizing the distinctions
between RL techniques employed in LLM alignment and those in conventional RL
tasks. In particular, we highlight the necessity of constructing neural reward
models from human data and discuss the formal and practical implications of
this paradigm shift. We begin by introducing fundamental concepts in RL to
provide a foundation for readers unfamiliar with the field. We then examine
recent advances in this research agenda, discussing key challenges and
opportunities in conducting IRL for LLM alignment. Beyond methodological
considerations, we explore practical aspects, including datasets, benchmarks,
evaluation metrics, infrastructure, and computationally efficient training and
inference techniques. Finally, we draw insights from the literature on
sparse-reward RL to identify open questions and potential research directions.
By synthesizing findings from diverse studies, we aim to provide a structured
and critical overview of the field, highlight unresolved challenges, and
outline promising future directions for improving LLM alignment through RL and
IRL techniques.

</details>


### [67] [Spectral Bellman Method: Unifying Representation and Exploration in RL](https://arxiv.org/abs/2507.13181)
*Ofir Nabati,Bo Dai,Shie Mannor,Guy Tennenholtz*

Main category: cs.LG

TL;DR: 论文提出了一种名为Spectral Bellman Representation的新框架，通过Inherent Bellman Error（IBE）条件与Bellman更新的基本结构对齐，直接面向基于值的强化学习。


<details>
  <summary>Details</summary>
Motivation: 现有表示学习方法主要从模型学习角度出发，与强化学习任务不匹配，因此需要一种更直接对齐Bellman更新的表示学习方法。

Method: 提出Spectral Bellman Representation框架，基于零IBE条件发现特征协方差结构与Bellman算子之间的谱关系，设计新的学习目标。

Result: 学习到的表示能够实现结构化探索，提升性能，尤其在困难探索和长期信用分配任务中表现优异。

Conclusion: Spectral Bellman Representation为基于值的强化学习提供了一种理论严谨且有效的表示学习方法。

Abstract: The effect of representation has been demonstrated in reinforcement learning,
from both theoretical and empirical successes. However, the existing
representation learning mainly induced from model learning aspects, misaligning
with our RL tasks. This work introduces Spectral Bellman Representation, a
novel framework derived from the Inherent Bellman Error (IBE) condition, which
aligns with the fundamental structure of Bellman updates across a space of
possible value functions, therefore, directly towards value-based RL. Our key
insight is the discovery of a fundamental spectral relationship: under the
zero-IBE condition, the transformation of a distribution of value functions by
the Bellman operator is intrinsically linked to the feature covariance
structure. This spectral connection yields a new, theoretically-grounded
objective for learning state-action features that inherently capture this
Bellman-aligned covariance. Our method requires a simple modification to
existing algorithms. We demonstrate that our learned representations enable
structured exploration, by aligning feature covariance with Bellman dynamics,
and improve overall performance, particularly in challenging hard-exploration
and long-horizon credit assignment tasks. Our framework naturally extends to
powerful multi-step Bellman operators, further broadening its impact. Spectral
Bellman Representation offers a principled and effective path toward learning
more powerful and structurally sound representations for value-based
reinforcement learning.

</details>


### [68] [GradNetOT: Learning Optimal Transport Maps with GradNets](https://arxiv.org/abs/2507.13191)
*Shreyas Chaudhari,Srinivasa Pranav,José M. F. Moura*

Main category: cs.LG

TL;DR: 论文提出了一种基于单调梯度网络（mGradNets）的方法，直接学习最优传输映射，并通过Monge-Ampère方程定义训练损失函数。


<details>
  <summary>Details</summary>
Motivation: 单调梯度函数在解决最优传输问题中起关键作用，尤其是在现代应用中如流体动力学和机器人群体控制。

Method: 利用mGradNets直接参数化单调梯度映射空间，并通过最小化Monge-Ampère方程定义的损失函数学习最优传输映射。

Result: 实验表明，mGradNets的结构偏置有助于学习最优传输映射，并成功应用于机器人群体控制问题。

Conclusion: mGradNets为学习最优传输映射提供了一种有效方法，并在实际应用中展示了潜力。

Abstract: Monotone gradient functions play a central role in solving the Monge
formulation of the optimal transport problem, which arises in modern
applications ranging from fluid dynamics to robot swarm control. When the
transport cost is the squared Euclidean distance, Brenier's theorem guarantees
that the unique optimal map is the gradient of a convex function, namely a
monotone gradient map, and it satisfies a Monge-Amp\`ere equation. In
[arXiv:2301.10862] [arXiv:2404.07361], we proposed Monotone Gradient Networks
(mGradNets), neural networks that directly parameterize the space of monotone
gradient maps. In this work, we leverage mGradNets to directly learn the
optimal transport mapping by minimizing a training loss function defined using
the Monge-Amp\`ere equation. We empirically show that the structural bias of
mGradNets facilitates the learning of optimal transport maps and employ our
method for a robot swarm control problem.

</details>


### [69] [MoTM: Towards a Foundation Model for Time Series Imputation based on Continuous Modeling](https://arxiv.org/abs/2507.13207)
*Etienne Le Naour,Tahar Nabil,Ghislain Agoua*

Main category: cs.LG

TL;DR: 提出了一种基于隐式神经表示（INRs）的时间序列缺失值填补方法MoTM，通过混合多个INRs模型提升跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型主要关注预测任务，而跨域缺失值填补任务研究不足。

Method: 利用INRs建模时间序列为连续函数，结合多个独立训练的INRs和岭回归器，适应不同缺失场景。

Result: 在多种缺失场景（如块缺失、点缺失、变采样率）下表现出稳健的域内和跨域泛化能力。

Conclusion: MoTM为适应性基础填补模型的发展提供了新方向。

Abstract: Recent years have witnessed a growing interest for time series foundation
models, with a strong emphasis on the forecasting task. Yet, the crucial task
of out-of-domain imputation of missing values remains largely underexplored. We
propose a first step to fill this gap by leveraging implicit neural
representations (INRs). INRs model time series as continuous functions and
naturally handle various missing data scenarios and sampling rates. While they
have shown strong performance within specific distributions, they struggle
under distribution shifts. To address this, we introduce MoTM (Mixture of
Timeflow Models), a step toward a foundation model for time series imputation.
Building on the idea that a new time series is a mixture of previously seen
patterns, MoTM combines a basis of INRs, each trained independently on a
distinct family of time series, with a ridge regressor that adapts to the
observed context at inference. We demonstrate robust in-domain and
out-of-domain generalization across diverse imputation scenarios (e.g., block
and pointwise missingness, variable sampling rates), paving the way for
adaptable foundation imputation models.

</details>


### [70] [Merge Kernel for Bayesian Optimization on Permutation Space](https://arxiv.org/abs/2507.13263)
*Zikai Xie,Linjiang Chen*

Main category: cs.LG

TL;DR: 提出了一种基于排序算法的新型框架，用于生成排列空间上的核函数，其中Mallows核被视为由冒泡排序衍生的特例。进一步提出了复杂度更低的Merge Kernel，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前排列空间的贝叶斯优化方法依赖于Mallows核，其复杂度为Ω(n²)，效率较低。受其与成对比较关系的启发，希望设计更高效的核函数。

Method: 提出基于排序算法的框架，将Mallows核视为冒泡排序的特例，并引入复杂度为Θ(n log n)的Merge Kernel。此外，结合了三种轻量级描述符以增强鲁棒性。

Result: 实验表明，Merge Kernel在多个排列优化基准测试中均优于Mallows核，提供了更紧凑且高效的解决方案。

Conclusion: Merge Kernel为排列空间的贝叶斯优化提供了更高效且紧凑的核函数，显著优于现有方法。

Abstract: Bayesian Optimization (BO) algorithm is a standard tool for black-box
optimization problems. The current state-of-the-art BO approach for permutation
spaces relies on the Mallows kernel-an $\Omega(n^2)$ representation that
explicitly enumerates every pairwise comparison. Inspired by the close
relationship between the Mallows kernel and pairwise comparison, we propose a
novel framework for generating kernel functions on permutation space based on
sorting algorithms. Within this framework, the Mallows kernel can be viewed as
a special instance derived from bubble sort. Further, we introduce the
\textbf{Merge Kernel} constructed from merge sort, which replaces the quadratic
complexity with $\Theta(n\log n)$ to achieve the lowest possible complexity.
The resulting feature vector is significantly shorter, can be computed in
linearithmic time, yet still efficiently captures meaningful permutation
distances. To boost robustness and right-invariance without sacrificing
compactness, we further incorporate three lightweight, task-agnostic
descriptors: (1) a shift histogram, which aggregates absolute element
displacements and supplies a global misplacement signal; (2) a split-pair line,
which encodes selected long-range comparisons by aligning elements across the
two halves of the whole permutation; and (3) sliding-window motifs, which
summarize local order patterns that influence near-neighbor objectives. Our
empirical evaluation demonstrates that the proposed kernel consistently
outperforms the state-of-the-art Mallows kernel across various permutation
optimization benchmarks. Results confirm that the Merge Kernel provides a more
compact yet more effective solution for Bayesian optimization in permutation
space.

</details>


### [71] [Boosting Team Modeling through Tempo-Relational Representation Learning](https://arxiv.org/abs/2507.13305)
*Vincenzo Marco De Luca,Giovanna Varni,Andrea Passerini*

Main category: cs.LG

TL;DR: TRENN是一种新型的时态关系架构，用于联合建模团队动态和关系，并通过多任务扩展（MT-TRENN）同时预测多个团队构念，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能满足实际需求，即联合建模团队动态和关系，并提供可解释的见解和可操作的建议。

Method: TRENN结合了自动时态图提取器、时态关系编码器、团队构念预测解码器和两个解释性模块；MT-TRENN进一步扩展为多任务头。

Result: 实验表明，TRENN和MT-TRENN在性能上显著优于仅使用时态或关系信息的方法，并能提供可解释的见解。

Conclusion: 该方法适用于人本AI应用，如高风险协作环境中的智能决策支持系统。

Abstract: Team modeling remains a fundamental challenge at the intersection of
Artificial Intelligence and the Social Sciences. Social Science research
emphasizes the need to jointly model dynamics and relations, while practical
applications demand unified models capable of inferring multiple team
constructs simultaneously, providing interpretable insights and actionable
recommendations to enhance team performance. However, existing works do not
meet these practical demands. To bridge this gap, we present TRENN, a novel
tempo-relational architecture that integrates: (i) an automatic temporal graph
extractor, (ii) a tempo-relational encoder, (iii) a decoder for team construct
prediction, and (iv) two complementary explainability modules. TRENN jointly
captures relational and temporal team dynamics, providing a solid foundation
for MT-TRENN, which extends TReNN by replacing the decoder with a multi-task
head, enabling the model to learn shared Social Embeddings and simultaneously
predict multiple team constructs, including Emergent Leadership, Leadership
Style, and Teamwork components. Experimental results demonstrate that our
approach significantly outperforms approaches that rely exclusively on temporal
or relational information. Additionally, experimental evaluation has shown that
the explainability modules integrated in MT-TRENN yield interpretable insights
and actionable suggestions to support team improvement. These capabilities make
our approach particularly well-suited for Human-Centered AI applications, such
as intelligent decision-support systems in high-stakes collaborative
environments.

</details>


### [72] [GeoReg: Weight-Constrained Few-Shot Regression for Socio-Economic Estimation using LLM](https://arxiv.org/abs/2507.13323)
*Kyeongjin Ahn,Sungwon Han,Seungeon Lee,Donghyun Ahn,Hyoshin Kim,Jungwon Kim,Jihee Kim,Sangyoon Park,Meeyoung Cha*

Main category: cs.LG

TL;DR: GeoReg模型结合卫星图像和地理空间数据，利用LLM解决标签数据稀缺问题，通过特征分类和交互提升社会经济指标预测效果。


<details>
  <summary>Details</summary>
Motivation: 社会经济指标对政策制定和可持续发展至关重要，但数据稀缺地区（如发展中国家）难以准确估计。

Method: 结合LLM提取特征，分类特征相关性，引入线性估计和非线性变换。

Result: 在三个不同发展阶段国家的实验中，模型优于基线方法。

Conclusion: GeoReg能有效估计数据稀缺地区的社会经济指标。

Abstract: Socio-economic indicators like regional GDP, population, and education
levels, are crucial to shaping policy decisions and fostering sustainable
development. This research introduces GeoReg a regression model that integrates
diverse data sources, including satellite imagery and web-based geospatial
information, to estimate these indicators even for data-scarce regions such as
developing countries. Our approach leverages the prior knowledge of large
language model (LLM) to address the scarcity of labeled data, with the LLM
functioning as a data engineer by extracting informative features to enable
effective estimation in few-shot settings. Specifically, our model obtains
contextual relationships between data features and the target indicator,
categorizing their correlations as positive, negative, mixed, or irrelevant.
These features are then fed into the linear estimator with tailored weight
constraints for each category. To capture nonlinear patterns, the model also
identifies meaningful feature interactions and integrates them, along with
nonlinear transformations. Experiments across three countries at different
stages of development demonstrate that our model outperforms baselines in
estimating socio-economic indicators, even for low-income countries with
limited data availability.

</details>


### [73] [Training Transformers with Enforced Lipschitz Constants](https://arxiv.org/abs/2507.13338)
*Laker Newhouse,R. Preston Hess,Franz Cesista,Andrii Zahorodnii,Jeremy Bernstein,Phillip Isola*

Main category: cs.LG

TL;DR: 论文研究了如何通过Lipschitz约束训练现代神经网络架构（如Transformer），提出新工具优化权重矩阵约束，并发现优化器选择对性能有显著影响。


<details>
  <summary>Details</summary>
Motivation: 神经网络对输入和权重扰动高度敏感，导致对抗样本、训练发散和过拟合等问题。现有Lipschitz约束方法尚未成功应用于现代架构（如Transformer）的全程训练。

Method: 开发高效工具约束权重矩阵范数，结合优化器Muon改进训练动态，设计新权重约束方法提升Lipschitz与性能的平衡。

Result: 成功训练2-Lipschitz Transformer（Shakespeare文本验证准确率60%）和10-Lipschitz Transformer（互联网文本准确率21%），但匹配基准性能需放宽Lipschitz约束。

Conclusion: Lipschitz约束可稳定训练Transformer，但性能与约束强度需权衡；优化器选择对结果至关重要。

Abstract: Neural networks are often highly sensitive to input and weight perturbations.
This sensitivity has been linked to pathologies such as vulnerability to
adversarial examples, divergent training, and overfitting. To combat these
problems, past research has looked at building neural networks entirely from
Lipschitz components. However, these techniques have not matured to the point
where researchers have trained a modern architecture such as a transformer with
a Lipschitz certificate enforced beyond initialization. To explore this gap, we
begin by developing and benchmarking novel, computationally-efficient tools for
maintaining norm-constrained weight matrices. Applying these tools, we are able
to train transformer models with Lipschitz bounds enforced throughout training.
We find that optimizer dynamics matter: switching from AdamW to Muon improves
standard methods -- weight decay and spectral normalization -- allowing models
to reach equal performance with a lower Lipschitz bound. Inspired by Muon's
update having a fixed spectral norm, we co-design a weight constraint method
that improves the Lipschitz vs. performance tradeoff on MLPs and 2M parameter
transformers. Our 2-Lipschitz transformer on Shakespeare text reaches
validation accuracy 60%. Scaling to 145M parameters, our 10-Lipschitz
transformer reaches 21% accuracy on internet text. However, to match the
NanoGPT baseline validation accuracy of 39.4%, our Lipschitz upper bound
increases to 10^264. Nonetheless, our Lipschitz transformers train without
stability measures such as layer norm, QK norm, and logit tanh softcapping.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [74] [Physics constrained learning of stochastic characteristics](https://arxiv.org/abs/2507.12661)
*Pardha Sai Krishna Ala,Ameya Salvi,Venkat Krovi,Matthias Schmid*

Main category: stat.ML

TL;DR: 论文提出了一种基于学习的方法，通过不同损失函数识别噪声特性，用于实时车辆状态估计。


<details>
  <summary>Details</summary>
Motivation: 准确的状态估计需要考虑过程和测量模型的不确定性，但噪声特性通常未知且难以建模，现有方法依赖优化算法，但效果有限。

Method: 采用学习框架，设计不同损失函数来识别噪声特性。

Result: 该方法能够有效识别噪声特性，并用于实时车辆状态估计。

Conclusion: 学习框架为噪声特性识别提供了新思路，提升了状态估计的准确性。

Abstract: Accurate state estimation requires careful consideration of uncertainty
surrounding the process and measurement models; these characteristics are
usually not well-known and need an experienced designer to select the
covariance matrices. An error in the selection of covariance matrices could
impact the accuracy of the estimation algorithm and may sometimes cause the
filter to diverge. Identifying noise characteristics has long been a
challenging problem due to uncertainty surrounding noise sources and
difficulties in systematic noise modeling. Most existing approaches try
identifying unknown covariance matrices through an optimization algorithm
involving innovation sequences. In recent years, learning approaches have been
utilized to determine the stochastic characteristics of process and measurement
models. We present a learning-based methodology with different loss functions
to identify noise characteristics and test these approaches' performance for
real-time vehicle state estimation

</details>


### [75] [Finite-Dimensional Gaussian Approximation for Deep Neural Networks: Universality in Random Weights](https://arxiv.org/abs/2507.12686)
*Krishnakumar Balasubramanian,Nathan Ross*

Main category: stat.ML

TL;DR: 研究了具有有限阶矩的随机初始化权重的深度神经网络的有限维分布（FDDs），并建立了其与高斯极限之间的Wasserstein-1范数近似界。


<details>
  <summary>Details</summary>
Motivation: 探索深度神经网络在随机初始化权重下的有限维分布行为，特别是其与高斯极限的近似程度。

Method: 假设激活函数为Lipschitz连续，允许层宽度以任意相对速率增长到无穷大，并推导高斯近似界。

Result: 在宽度与共同尺度参数n成比例且存在L-1个隐藏层的特殊情况下，得到了收敛速率为n^{-(1/6)^{L-1} + ε}。

Conclusion: 证明了深度神经网络的FDDs在高斯极限下的收敛性，并给出了具体的收敛速率。

Abstract: We study the Finite-Dimensional Distributions (FDDs) of deep neural networks
with randomly initialized weights that have finite-order moments. Specifically,
we establish Gaussian approximation bounds in the Wasserstein-$1$ norm between
the FDDs and their Gaussian limit assuming a Lipschitz activation function and
allowing the layer widths to grow to infinity at arbitrary relative rates. In
the special case where all widths are proportional to a common scale parameter
$n$ and there are $L-1$ hidden layers, we obtain convergence rates of order
$n^{-({1}/{6})^{L-1} + \epsilon}$, for any $\epsilon > 0$.

</details>


### [76] [Self Balancing Neural Network: A Novel Method to Estimate Average Treatment Effect](https://arxiv.org/abs/2507.12818)
*Atomsa Gemechu Abdisa,Yingchun Zhou,Yuqi Qiu*

Main category: stat.ML

TL;DR: 论文提出了一种名为“自平衡神经网络”（Sbnet）的新方法，用于解决观察性研究中因混杂变量和工具变量导致的平均处理效应估计偏差问题。该方法通过平衡网络自动获取伪倾向得分，并在一步中完成估计，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 观察性研究中，混杂变量和工具变量导致平均处理效应估计偏差，传统倾向得分方法存在模型误设风险，需要更稳健的解决方案。

Method: 提出自平衡神经网络（Sbnet），利用平衡网络生成伪倾向得分，并通过多伪倾向得分框架估计平均处理效应。

Result: 在模拟和真实数据集上，Sbnet性能优于现有方法。

Conclusion: Sbnet通过一步估计和伪倾向得分框架，有效解决了观察性研究中的偏差问题，具有实际应用潜力。

Abstract: In observational studies, confounding variables affect both treatment and
outcome. Moreover, instrumental variables also influence the treatment
assignment mechanism. This situation sets the study apart from a standard
randomized controlled trial, where the treatment assignment is random. Due to
this situation, the estimated average treatment effect becomes biased. To
address this issue, a standard approach is to incorporate the estimated
propensity score when estimating the average treatment effect. However, these
methods incur the risk of misspecification in propensity score models. To solve
this issue, a novel method called the "Self balancing neural network" (Sbnet),
which lets the model itself obtain its pseudo propensity score from the
balancing net, is proposed in this study. The proposed method estimates the
average treatment effect by using the balancing net as a key part of the
feedforward neural network. This formulation resolves the estimation of the
average treatment effect in one step. Moreover, the multi-pseudo propensity
score framework, which is estimated from the diversified balancing net and used
for the estimation of the average treatment effect, is presented. Finally, the
proposed methods are compared with state-of-the-art methods on three simulation
setups and real-world datasets. It has been shown that the proposed
self-balancing neural network shows better performance than state-of-the-art
methods.

</details>


### [77] [Bayesian Modeling and Estimation of Linear Time-Variant Systems using Neural Networks and Gaussian Processes](https://arxiv.org/abs/2507.12878)
*Yaniv Shulman*

Main category: stat.ML

TL;DR: 论文提出了一种统一的贝叶斯框架，用于识别线性时变系统的脉冲响应，并将其分解为后验均值和随机波动项，从而量化不确定性并定义新系统类别LTIE。


<details>
  <summary>Details</summary>
Motivation: 识别线性时变系统的脉冲响应是一个具有挑战性的不适定逆问题，需要一种能够量化不确定性的方法。

Method: 采用贝叶斯框架，结合贝叶斯神经网络和高斯过程，利用可扩展的变分推断进行推理。

Result: 实验表明，该框架能稳健地从单次噪声观测中推断LTI系统特性，在模拟环境噪声层析问题中数据效率优于经典方法，并能成功跟踪连续变化的LTV脉冲响应。

Conclusion: 该研究为动态环境中的不确定性感知系统识别提供了一种灵活且稳健的方法。

Abstract: The identification of Linear Time-Variant (LTV) systems from input-output
data is a fundamental yet challenging ill-posed inverse problem. This work
introduces a unified Bayesian framework that models the system's impulse
response, $h(t, \tau)$, as a stochastic process. We decompose the response into
a posterior mean and a random fluctuation term, a formulation that provides a
principled approach for quantifying uncertainty and naturally defines a new,
useful system class we term Linear Time-Invariant in Expectation (LTIE). To
perform inference, we leverage modern machine learning techniques, including
Bayesian neural networks and Gaussian Processes, using scalable variational
inference. We demonstrate through a series of experiments that our framework
can robustly infer the properties of an LTI system from a single noisy
observation, show superior data efficiency compared to classical methods in a
simulated ambient noise tomography problem, and successfully track a
continuously varying LTV impulse response by using a structured Gaussian
Process prior. This work provides a flexible and robust methodology for
uncertainty-aware system identification in dynamic environments.

</details>


### [78] [When Pattern-by-Pattern Works: Theoretical and Empirical Insights for Logistic Models with Missing Values](https://arxiv.org/abs/2507.13024)
*Christophe Muller,Erwan Scornet,Julie Josse*

Main category: stat.ML

TL;DR: 论文研究了逻辑回归模型中部分缺失输入的预测问题，提出了一种基于缺失模式的策略（PbP），并通过理论和实验验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 解决逻辑回归模型中部分缺失输入预测的挑战，填补现有研究在非线性模型中的空白。

Method: 提出Pattern-by-Pattern（PbP）策略，为每种缺失模式学习一个逻辑模型，并与多种方法（如均值插补、MICE.RF.Y等）进行对比。

Result: 理论证明PbP在多种缺失数据场景下能准确逼近贝叶斯概率；实验表明MICE.RF.Y在小样本下表现优异，PbP在大样本下对高斯混合数据效果最佳。

Conclusion: 推荐在小样本时使用MICE.RF.Y，大样本时使用PbP，为逻辑回归中缺失值处理提供了全面指导。

Abstract: Predicting a response with partially missing inputs remains a challenging
task even in parametric models, since parameter estimation in itself is not
sufficient to predict on partially observed inputs. Several works study
prediction in linear models. In this paper, we focus on logistic models, which
present their own difficulties. From a theoretical perspective, we prove that a
Pattern-by-Pattern strategy (PbP), which learns one logistic model per
missingness pattern, accurately approximates Bayes probabilities in various
missing data scenarios (MCAR, MAR and MNAR). Empirically, we thoroughly compare
various methods (constant and iterative imputations, complete case analysis,
PbP, and an EM algorithm) across classification, probability estimation,
calibration, and parameter inference. Our analysis provides a comprehensive
view on the logistic regression with missing values. It reveals that mean
imputation can be used as baseline for low sample sizes, and improved
performance is obtained via nonlinear multiple iterative imputation techniques
with the labels (MICE.RF.Y). For large sample sizes, PbP is the best method for
Gaussian mixtures, and we recommend MICE.RF.Y in presence of nonlinear
features.

</details>


### [79] [Relation-Aware Slicing in Cross-Domain Alignment](https://arxiv.org/abs/2507.13194)
*Dhruv Sarkar,Aprameyo Chakrabartty,Anish Chakrabarty,Swagatam Das*

Main category: stat.ML

TL;DR: 提出了一种优化自由的切片分布方法（RASD），通过关系感知投影方向（RAPD）改进SGW距离的计算效率和代表性。


<details>
  <summary>Details</summary>
Motivation: 解决SGW距离因非信息性投影方向导致的计算成本高和代表性不足的问题。

Method: 引入RAPD捕捉随机向量对的关联性，并基于此推导出RASD切片分布，提出RASGW距离及其变体（如IWRASGW）。

Result: 理论分析和实验验证表明，RASGW及其变体在多种对齐任务中表现优越。

Conclusion: RASGW方法有效提升了计算效率和距离的实用性，适用于复杂对齐任务。

Abstract: The Sliced Gromov-Wasserstein (SGW) distance, aiming to relieve the
computational cost of solving a non-convex quadratic program that is the
Gromov-Wasserstein distance, utilizes projecting directions sampled uniformly
from unit hyperspheres. This slicing mechanism incurs unnecessary computational
costs due to uninformative directions, which also affects the representative
power of the distance. However, finding a more appropriate distribution over
the projecting directions (slicing distribution) is often an optimization
problem in itself that comes with its own computational cost. In addition, with
more intricate distributions, the sampling itself may be expensive. As a
remedy, we propose an optimization-free slicing distribution that provides fast
sampling for the Monte Carlo approximation. We do so by introducing the
Relation-Aware Projecting Direction (RAPD), effectively capturing the pairwise
association of each of two pairs of random vectors, each following their
ambient law. This enables us to derive the Relation-Aware Slicing Distribution
(RASD), a location-scale law corresponding to sampled RAPDs. Finally, we
introduce the RASGW distance and its variants, e.g., IWRASGW (Importance
Weighted RASGW), which overcome the shortcomings experienced by SGW. We
theoretically analyze its properties and substantiate its empirical prowess
using extensive experiments on various alignment tasks.

</details>
