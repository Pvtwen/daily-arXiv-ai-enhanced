<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 29]
- [cs.LG](#cs.LG) [Total: 87]
- [stat.ML](#stat.ML) [Total: 22]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [COMET: Co-Optimization of a CNN Model using Efficient-Hardware OBC Techniques](https://arxiv.org/abs/2510.03516)
*Boyang Chen,Mohd Tasleem Khan,George Goussetis,Mathini Sellathurai,Yuan Ding,João F. C. Mota*

Main category: eess.SP

TL;DR: COMET框架通过偏移二进制编码技术优化CNN设计，实现性能与资源利用的协同优化，显著降低FPGA资源消耗且保持精度。


<details>
  <summary>Details</summary>
Motivation: CNN在计算机视觉任务中表现优异，但其计算密集性和对FPGA等硬件的依赖限制了在低功耗边缘设备上的部署。

Method: 采用偏移二进制编码分别表示输入和权重，利用位宽不对称性；修改移位累加操作，引入偏移项与预缩放偏置；提出四种新颖的LUT技术（并行、共享、拆分、混合）；基于im2col变换开发OBC通用矩阵乘法核心。

Result: FPGA评估显示，相比现有LeNet-5 CNN设计，该方法显著减少了资源利用，对精度影响极小。

Conclusion: COMET框架通过硬件友好的偏移二进制编码技术，成功实现了CNN在边缘设备上的高效部署，为低功耗应用提供了可行解决方案。

Abstract: Convolutional Neural Networks (CNNs) are highly effective for computer vision
and pattern recognition tasks; however, their computational intensity and
reliance on hardware such as FPGAs pose challenges for deployment on low-power
edge devices. In this work, we present COMET, a framework of CNN designs that
employ efficient hardware offset-binary coding (OBC) techniques to enable
co-optimization of performance and resource utilization. The approach
formulates CNN inference with OBC representations of inputs (Scheme A) and
weights (Scheme B) separately, enabling exploitation of bit-width asymmetry.
The shift-accumulate operation is modified by incorporating the offset term
with the pre-scaled bias. Leveraging inherent symmetries in Schemes A and B, we
introduce four novel look-up table (LUT) techniques -- parallel, shared, split,
and hybrid -- and analyze them to identify the most efficient options. Building
on this foundation, we develop an OBC-based general matrix multiplication core
using the im2col transformation, enabling efficient acceleration of a
fixed-point modified LeNet-5 model. FPGA evaluations demonstrate that the
proposed co-optimization approach significantly reduces resource utilization
compared to state-of-the-art LeNet-5 based CNN designs, with minimal impact on
accuracy.

</details>


### [2] [Variable Block-Correlation Modeling and Optimization for Secrecy Analysis in Fluid Antenna Systems](https://arxiv.org/abs/2510.03594)
*Tuo Wu,Kwai-Man Luk,Jie Tang,Kai-Kit Wong,Jianchao Zheng,Baiyang Liu,David Morales-Jimenez,Maged Elkashlan,Kin-Fai Tong,Chan-Byoung Chae,Fumiyuki Adachi,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 本文提出将可变块相关模型(VBCM)应用于流体天线系统(FAS)安全分析，开发了新的闭式表达式和优化算法，显著提升了安全性能分析的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统相关模型(如Jakes模型)在FAS安全分析中存在分析困难，而简化的恒定相关模型无法准确捕捉真实行为，需要更精确的建模方法。

Method: 采用VBCM模型进行FAS安全分析，推导了平均保密容量(ASC)和保密中断概率(SOP)的闭式表达式，并设计了网格搜索和梯度下降两种优化算法。

Result: VBCM框架实现了与仿真一致的准确性，相对误差始终低于5%，在高威胁场景下ASC提升超过120%，紧凑天线配置性能提升18-19%。

Conclusion: VBCM为FAS安全分析和优化提供了强大工具，对推进6G通信系统具有重要实践价值。

Abstract: Fluid antenna systems (FAS) are emerging as a transformative enabler for
sixth-generation (6G) wireless communications, providing unprecedented spatial
diversity through dynamic reconfiguration of antenna ports. However, the
inherent spatial correlation among ports poses significant challenges for
accurate analysis. Conventional models such as Jakes are analytically
intractable, while oversimplified constant-correlation models fail to capture
the true behavior. In this work, we address these challenges by applying the
variable block-correlation model (VBCM) -- originally proposed by
Ram\'{i}rez-Espinosa \textit{et al.} in 2024 -- to FAS security analysis, and
by developing comprehensive optimization methods to enhance analytical
accuracy. We derive new closed-form expressions for average secrecy capacity
(ASC) and secrecy outage probability (SOP), demonstrating that the VBCM
framework achieves simulation-aligned accuracy, with relative errors
consistently below $5\%$ (compared to $10$--$15\%$ for constant-correlation
models). To maximize ASC, we further design two algorithms: a grid search (GS)
method and a gradient descent (GD) method. Numerical results reveal that the
VBCM-based approach not only provides reliable insights into FAS security
performance, but also yields substantial gains -- ASC improvements exceeding
$120\%$ in high-threat scenarios and $18$--$19\%$ performance enhancements for
compact antenna configurations. These findings underscore the practical value
of integrating VBCM into FAS security analysis and optimization, establishing
it as a powerful tool for advancing 6G communication systems.

</details>


### [3] [On-Grid Equivalence of Continuous-Time Doubly Selective Channels: A Revisit of Bello's Models](https://arxiv.org/abs/2510.03626)
*Jun Tong*

Main category: eess.SP

TL;DR: 本文重新审视了实际物理信道的on-grid建模，研究了具有off-grid延迟和多普勒频移的连续时间双选择性信道的等效on-grid DD域表示，考虑了收发器端的实际时频域加窗处理。


<details>
  <summary>Details</summary>
Motivation: 现有的双选择性信道通信研究大多采用on-grid DD信道模型，但实际物理信道通常由传播环境决定且是off-grid的，导致实际物理信道与on-grid模型之间存在差距。

Method: 研究具有off-grid延迟和多普勒频移的连续时间双选择性信道的等效on-grid DD域表示，考虑收发器端的实际时频域加窗处理，获得在窗口具有有限支撑的温和假设下适用的通用模型。

Result: 获得了扩展Bello经典结果的通用模型，能够处理更一般的窗口函数，并讨论了等效on-grid模型的特征和意义。

Conclusion: 本文扩展了Bello的经典结果，为处理具有off-grid延迟和多普勒频移的实际物理信道提供了更通用的on-grid建模框架。

Abstract: Significant studies on communications over doubly selective channels have
utilized on-grid DD channel models, which are previously investigated in
Bello's seminar paper in 1963. The DD grid is typically specified by the
bandwidth and time duration of the transmission frames. However, the physical
channels are determined by the propagation environments and they are typically
off-grid. Hence, there is often a gap between an actual physical channel and
the on-grid model. This paper revisits the on-grid modeling of practical
physical channels. We study the associated on-grid DD-domain representations
for continuous-time, doubly selective channels with off-grid delay and Doppler
shifts, accounting for practical time/frequency-domain windowing at the
transceivers. The universal models obtained are applicable under the mild
assumption that the windows have finite supports, and they extend Bello's
classical results to account for more general windows. We also discuss the
features and implications of the equivalent on-grid models.

</details>


### [4] [Pinching Antenna Systems (PASS) for Cell-Free Communications](https://arxiv.org/abs/2510.03628)
*Haochen Li*

Main category: eess.SP

TL;DR: 提出了一种夹持天线系统辅助的无蜂窝通信系统，通过交替优化算法解决和速率最大化问题，在基站功率预算和PA部署约束下实现性能提升


<details>
  <summary>Details</summary>
Motivation: 传统无蜂窝系统面临性能瓶颈，需要新的天线架构来提升系统容量和覆盖性能

Method: 使用交替优化算法，其中数字波束成形采用WMMSE方法，夹持波束成形采用基于惩罚的方法结合逐元素优化

Result: 仿真结果表明：1）PASS辅助系统性能优于基准方案；2）增加每个波导的PA数量可提升系统优势；3）无蜂窝架构能缓解用户数增加带来的平均用户速率下降

Conclusion: PASS辅助的无蜂窝通信系统能有效提升系统性能，特别是在用户数量增加时仍能保持良好的平均用户速率

Abstract: A pinching antenna system (PASS) assisted cell-free communication system is
proposed. A sum rate maximization problem under the BS power budget constraint
and PA deployment constraint is formulated. To tackle the proposed non-convex
optimization problem, an alternating optimization (AO) algorithm is developed.
In particular, the digital beamforming sub-problem is solved using the weighted
minimum mean square error (WMMSE) method, whereas the pinching beamforming
sub-problem is handled via a penalty based approach combined with element-wise
optimization. Simulation results demonstrate that: 1) the PASS assisted
cell-free systems achieve superior performance over benchmark schemes; 2)
increasing the number of PAs per waveguides can improve the advantage of PASS
assisted cell-free systems; and 3) the cell-free architecture mitigates the
average user rate degradation as the number of users increases.

</details>


### [5] [Towards Secure ISAC Beamforming: How Many Dedicated Sensing Beams Are Required?](https://arxiv.org/abs/2510.03749)
*Fanghao Xia,Zesong Fei,Xinyi Wang,Nanchi Su,Zhaolin Wang,Yuanwei Liu,Jie Xu*

Main category: eess.SP

TL;DR: 研究多用户多窃听者的ISAC系统中的感知辅助安全通信，通过联合传输通信信号和专用感知信号来服务用户并感知空中窃听者，提出基于分数规划和交替优化的波束成形设计。


<details>
  <summary>Details</summary>
Motivation: 在集成感知与通信系统中，需要同时保证通信性能和感知能力，特别是在存在多个空中窃听者的情况下，需要设计有效的安全通信策略。

Method: 采用分数规划交替优化算法，结合逐次凸近似和半定松弛处理非凸约束，分析最小感知波束数量，并将设计扩展到混合模拟数字阵列架构。

Result: 仿真结果表明，少量感知波束即可同时实现感知和干扰窃听者，所提设计优于基线方法，并揭示了通信-感知权衡关系。

Conclusion: 该研究为ISAC系统中的安全通信提供了有效的波束成形设计方案，证明了感知辅助通信的可行性。

Abstract: In this paper, sensing-assisted secure communication in a multi-user
multi-eavesdropper integrated sensing and communication (ISAC) system is
investigated. Confidential communication signals and dedicated sensing signals
are jointly transmitted by a base station (BS) to simultaneously serve users
and sense aerial eavesdroppers (AEs). A sum rate maximization problem is
formulated under AEs' Signal-to-Interference-plus-Noise Ratio (SINR) and
sensing Signal-to-Clutter-plus-Noise Ratio (SCNR) constraints. A
fractional-programming-based alternating optimization algorithm is developed to
solve this problem for fully digital arrays, where successive convex
approximation (SCA) and semidefinite relaxation (SDR) are leveraged to handle
non-convex constraints. Furthermore, the minimum number of dedicated sensing
beams is analyzed via a worst-case rank bound, upon which the proposed
beamforming design is further extended to the hybrid analog-digital (HAD) array
architecture, where the unit-modulus constraint is addressed by manifold
optimization. Simulation results demonstrate that only a small number of
sensing beams are sufficient for both sensing and jamming AEs, and the proposed
designs consistently outperform strong baselines while also revealing the
communication-sensing trade-off.

</details>


### [6] [A Benchmark Study of Deep Learning Methods for Multi-Label Pediatric Electrocardiogram-Based Cardiovascular Disease Classification](https://arxiv.org/abs/2510.03780)
*Yiqiao Chen*

Main category: eess.SP

TL;DR: 本文首次对深度学习在儿科心血管疾病多标签分类方面进行基准研究，使用ZZU-pECG数据集（3716个记录，19种CVD类别），评估了ResNet-1D、BiLSTM、Transformer和Mamba 2四种模型在9导联和12导联配置下的性能。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是儿科主要健康负担，早期筛查至关重要。心电图作为无创且易得的工具非常适合此目的，但缺乏儿科CVD分类的深度学习基准研究。

Method: 使用ZZU-pECG数据集（3716个记录，19种CVD类别），系统评估ResNet-1D、BiLSTM、Transformer和Mamba 2四种深度学习模型在9导联和12导联配置下的性能。

Result: 所有模型都取得了良好结果，汉明损失低至0.0069，大多数设置下F1分数超过85%。ResNet-1D在12导联子集上达到94.67%的宏F1，BiLSTM和Transformer也表现出竞争力。但9导联子集中罕见疾病如肥厚型心肌病分类仍有挑战。

Conclusion: 该基准研究建立了可复用的基线，展示了不同范式的互补优势。未来需要更大规模、多中心验证、年龄分层分析和更广泛的疾病覆盖，以支持真实世界的儿科心电图应用。

Abstract: Cardiovascular disease (CVD) is a major pediatric health burden, and early
screening is of critical importance. Electrocardiography (ECG), as a
noninvasive and accessible tool, is well suited for this purpose. This paper
presents the first benchmark study of deep learning for multi-label pediatric
CVD classification on the recently released ZZU-pECG dataset, comprising 3716
recordings with 19 CVD categories. We systematically evaluate four
representative paradigms--ResNet-1D, BiLSTM, Transformer, and Mamba 2--under
both 9-lead and 12-lead configurations. All models achieved strong results,
with Hamming Loss as low as 0.0069 and F1-scores above 85% in most settings.
ResNet-1D reached a macro-F1 of 94.67% on the 12-lead subset, while BiLSTM and
Transformer also showed competitive performance. Per-class analysis indicated
challenges for rare conditions such as hypertrophic cardiomyopathy in the
9-lead subset, reflecting the effect of limited positive samples. This
benchmark establishes reusable baselines and highlights complementary strengths
across paradigms. It further points to the need for larger-scale, multi-center
validation, age-stratified analysis, and broader disease coverage to support
real-world pediatric ECG applications.

</details>


### [7] [Toward Multiband Sensing in FR3: Frequency Anisotropy Characterization and Non-Contiguous Bands Aggregation Algorithms](https://arxiv.org/abs/2510.03787)
*Jacopo Pegoraro,Gianmaria Ventura,Dario Tagliaferri,Marco Mezzavilla,Andrea Bedin,Michele Rossi,Joerg Widmer*

Main category: eess.SP

TL;DR: 本文首次研究了FR3频段（7-24 GHz）的相干多频段感知，提出了新的相位相干性指标和算法来解决频率各向异性和非连续频谱带来的挑战。


<details>
  <summary>Details</summary>
Motivation: FR3频段为6G网络提供了前所未有的多频段集成感知与通信机会，但频率各向异性和频谱非连续性对现有技术构成了关键挑战。

Method: 实验表征了目标的频率各向异性，提出了新的多频段处理相位相干性指标，并设计了新算法来减轻非连续频段导致的感知伪影。

Result: 所提出的算法优于现有技术，能够有效缓解非连续FR3频段带来的感知问题。

Conclusion: 这项研究是开发FR3多频段ISAC技术的重要第一步。

Abstract: Frequency Range 3 (FR3) in the 7-24 GHz band will be the new spectrum for 6G
wireless networks. The bandwidth availability and diversity of FR3 offer
unprecedented opportunities for coherent multiband Integrated Sensing and
Communications (ISAC), which aggregates the carrier phase information from
multiple frequency bands to increase the sensing resolution to the cm-level.
However, the frequency anisotropy of sensing targets over GHz-wide bands and
the non-contiguity of the 6G spectrum, pose critical challenges to the
application of existing multiband ISAC techniques. We present the first study
on coherent multiband sensing in FR3. We experimentally characterize the
frequency anisotropy of targets and propose new phase coherence metrics for
multiband processing. Then, we analyze the impact of non-contiguous FR3 bands
considered by 3GPP, and design a new algorithm to mitigate the resulting
sensing artifacts, outperforming existing techniques. Our results represent a
first step toward fully developing multiband ISAC for FR3.

</details>


### [8] [Source PAC Coding for Low-latency Secret Key Generation in Short Blocklength Regime](https://arxiv.org/abs/2510.03818)
*Lulu Song,Di Zhang,Tingting Zhang*

Main category: eess.SP

TL;DR: 提出了一种多级源极化调整卷积(PAC)编码框架，用于解决短块长下密钥生成率和协调可靠性的显著下降问题。


<details>
  <summary>Details</summary>
Motivation: 源极化编码是6G物联网中基于短块长的低延迟密钥生成的潜在解决方案，但现有方案在短块长下仍存在密钥生成率和协调可靠性的显著下降。

Method: 引入多级源极化调整卷积(PAC)编码框架，并提出一种新的码构造算法，联合利用极化效应和最大似然(ML)解码错误系数。

Result: 仿真表明，与传统和多级源极化编码方法相比，采用所提码构造的多级源PAC方案在短块长下实现了更优的密钥生成率。

Conclusion: 多级源PAC编码框架在短块长下能够显著提升密钥生成性能，满足密钥不一致性约束。

Abstract: Source polar coding is a potential solution for short blocklength-based
low-latency key generation with limited sources, which is a critical aspect of
six generation (6G) Internet of things. However, existing source coding schemes
still suffer from significant degradation in key generation rate and
reconciliation reliability in short blocklength regime. To address this issue,
we introduce a multilevel source polarization-adjusted convolutional (PAC)
coding framework. Furthermore, we propose a novel code construction algorithm
that jointly leverages polarization effects and the maximum likelihood (ML)
decoding error coefficient. Simulations demonstrate that the multilevel source
PAC scheme with the proposed code construction achieves superior key generation
rate under key disagreement constraints compared to conventional and multilevel
source polar coding methods even in short blocklength regimes.

</details>


### [9] [Multi-Frequency Resonating Based Magnetic Induction Underground Emergency Communications with Diverse Mediums](https://arxiv.org/abs/2510.03848)
*Jianyu Wang,Zhichao Li,Wenchi Cheng,Wei Zhang,Hailin Zhang*

Main category: eess.SP

TL;DR: 该论文提出了一个统计衰落信道模型来应对地下应急通信中多样介质的随机组合问题，并采用多频谐振补偿线圈实现多频带传输以减轻多样介质衰落的影响。


<details>
  <summary>Details</summary>
Motivation: 磁感应通信在地下应急通信中具有优势，但实际应用中传播介质的多样性和随机组合对通信性能构成挑战，需要解决多样介质衰落问题。

Method: 建立遵循对数正态分布的统计衰落信道模型，使用多频谐振补偿线圈实现多频带传输，分析多频带磁感应通信在多样介质衰落下的性能。

Result: 数值结果表明，基于多频谐振补偿的多频带传输方案能有效减少多样介质衰落的影响，提升通信性能。

Conclusion: 多频谐振补偿的多频带传输是应对地下应急通信中多样介质衰落的有效方法，能显著改善通信系统的性能指标。

Abstract: Magnetic induction (MI) communication is an effective underground emergency
communication technique after disasters such as landslides, mine collapses, and
earthquakes, due to its advantages in mediums such as soil, concrete, and
metals. However, the propagation mediums in practical MI based underground
emergency communications are usually diverse and composed randomly due to the
impact of disasters, which poses a challenge for MI communication in practical
applications. In this paper, we formulate a statistical fading channel model,
which reflects the random composition of diverse mediums and is shown to follow
a lognormal distribution. To mitigate the impact of diverse medium fading,
Multi-frequency Resonating Compensation (MuReC) based coils are used to achieve
multiband transmission. Then, we analyze the performance of MuReC based
multi-band MI communication with diverse medium fading and derive the
expressions of signal-to-noise ratio (SNR) probability density functions,
ergodic capacities, average bit error rates (BERs), and outage probabilities
for both multiplexing and diversity cases. Numerical results show that MuReC
based multiband transmission schemes can effectively reduce the impact of
diverse medium fading and enhance the performance.

</details>


### [10] [On the Exact Sum PDF and CDF of α-μ Variates](https://arxiv.org/abs/2510.03850)
*Fernando Darío Almeida García,Francisco Raimundo Albuquerque Parente,Michel Daoud Yacoub,Jose Cândido Silveira Santos Filho*

Main category: eess.SP

TL;DR: 本文提出了计算独立同分布α-μ随机变量和的新颖、简单且精确的PDF和CDF公式，解决了传统方法在变量数量增加时的计算问题。


<details>
  <summary>Details</summary>
Motivation: 在无线通信中，随机变量和的统计特性对系统性能分析至关重要，但现有方法在变量数量增加时存在计算稳定性、收敛性和精度问题。

Method: 推导了独立同分布α-μ随机变量和的精确PDF和CDF表达式，其计算复杂度与求和项数量无关。

Result: 新公式显著减少了计算时间，是迄今为止最高效且易处理的公式，并成功应用于L分支预检测等增益合并和最大比合并接收机的性能分析。

Conclusion: 提出的新方法解决了传统多变量积分和Fox H函数方法的局限性，为α-μ衰落环境下无线通信系统的精确性能分析提供了有效工具。

Abstract: The sum of random variables (RVs) appears extensively in wireless
communications, at large, both conventional and advanced, and has been subject
of longstanding research. The statistical characterization of the referred sum
is crucial to determine the performance of such communications systems.
Although efforts have been undertaken to unveil these sum statistics, e.g.,
probability density function (PDF) and cumulative distribution function (CDF),
no general efficient nor manageable solutions capable of evaluating the exact
sum PDF and CDF are available to date. The only formulations are given in terms
of either the multi-fold Brennan's integral or the multivariate Fox H-function.
Unfortunately, these methods are only feasible up to a certain number of RVs,
meaning that when the number of RVs in the sum increases, the computation of
the sum PDF and CDF is subject to stability problems, convergence issues, or
inaccurate results. In this paper, we derive new, simple, exact formulations
for the PDF and CDF of the sum of L independent and identically distributed
{\alpha}-{\mu} RVs. Unlike the available solutions, the computational
complexity of our analytical expressions is independent of the number of
summands. Capitalizing on our unprecedented findings, we analyze, in exact and
asymptotic manners, the performance of L-branch pre-detection equal-gain
combining and maximal-ratio combining receivers over {\alpha}-{\mu} fading
environments. The coding and diversity gains of the system for both receivers
are analyzed and quantified. Moreover, numerical simulations show that the
computation time reduces drastically when using our expressions, which are
arguably the most efficient and manageable formulations derived so far.

</details>


### [11] [Steady-State Spread Bounds for Graph Diffusion via Laplacian Regularisation](https://arxiv.org/abs/2510.04924)
*Ardavan Rahimian*

Main category: eess.SP

TL;DR: 本文研究了图扩散过程在拉普拉斯正则化设计的初始模式下能偏离多远，给出了稳态传播的闭式上界，并提出了简单的设计规则来控制传播范围。


<details>
  <summary>Details</summary>
Motivation: 研究图扩散过程在拉普拉斯正则化设计的初始模式下的稳定性，特别是在阵列波束成形等应用中，需要确保稳态偏差在可控范围内。

Method: 在无向、非负图的标准稳定性条件下，推导出稳态传播的闭式上界，该上界分离了由图最大节点度决定的不可约项和由正则化强度控制的设计项。

Result: 得到了一个非渐近、易于计算的保证，可以证明稳态偏差的程度，并提出了基于目标传播限制选择足够正则化强度的简单设计规则。

Conclusion: 该研究为拉普拉斯平滑后在图上演化线性扩散的任何场景提供了稳态偏差的量化保证，具有广泛的应用价值。

Abstract: We study how far a diffusion process on a graph can drift from a designed
starting pattern when that pattern is produced using Laplacian regularisation.
Under standard stability conditions for undirected, entrywise nonnegative
graphs, we give a closed-form, instance-specific upper bound on the
steady-state spread, measured as the relative change between the final and
initial profiles. The bound separates two effects: (i) an irreducible term
determined by the graph's maximum node degree, and (ii) a design-controlled
term that shrinks as the regularisation strength increases (following an
inverse square-root law). This leads to a simple design rule: given any target
limit on spread, one can choose a sufficient regularisation strength in closed
form. Although one motivating application is array beamforming, where the
initial pattern is the squared magnitude of the beamformer weights, the result
applies to any scenario that first enforces Laplacian smoothness and then
evolves by linear diffusion on a graph. Overall, the guarantee is
non-asymptotic, easy to compute, and certifies how much steady-state deviation
can occur.

</details>


### [12] [Robust Beamforming for Magnetic Induction Based Underground Emergency Communications](https://arxiv.org/abs/2510.03852)
*Jianyu Wang,Tianrui Hou,Wenchi Cheng,Hailin Zhang*

Main category: eess.SP

TL;DR: 提出了一种考虑信道估计误差的鲁棒波束成形方案，用于多用户磁感应地下应急通信，以最小化功耗并保证通信可靠性。


<details>
  <summary>Details</summary>
Motivation: 磁感应通信在地下应急通信中具有优势，但灾后复杂环境会导致信道估计误差，影响波束成形性能，需要设计鲁棒方案来应对这种不确定性。

Method: 基于最坏情况优化准则和S-过程，将考虑信道估计误差的非凸波束成形优化问题转化为凸问题求解。

Result: 数值结果表明，所提出的鲁棒波束成形方案在存在信道估计误差时能有效提高通信可靠性和有效吞吐量。

Conclusion: 该鲁棒波束成形方案能够在地下应急通信中有效应对信道估计误差，提升系统性能。

Abstract: Magnetic induction (MI) communication is an effective underground emergency
communication technique after disasters such as landslides, mine collapses, and
earthquakes, due to its advantages in mediums such as soil, concrete, and
metals. Based on channel state information (CSI), magnetic beamforming can
significantly improve the performance of MI communication. However, in
post-disaster underground communication, channel estimation may suffer from
errors due to factors such as complex environmental interferences. Taking
channel estimation error into account, we formulate a beamforming optimization
problem for multi-user MI underground emergency communications, which aims to
minimize the power consumption under the constraints of sum rate and signal to
interference plus noise ratio (SINR) of each user. Based on the worst-case
optimization criterion and the S-procedure, the non-convex optimization problem
is transformed into convex and solved. Numerical results show that the proposed
robust beamforming scheme can effectively enhance communication reliability and
effective throughput in the presence of channel estimation errors.

</details>


### [13] [On the Noise Robustness of Affine Frequency Division Multiplexing: Analysis and Applications](https://arxiv.org/abs/2510.03901)
*Vincent Savaux,Steve Sawadogo,Hyeon Seok Rou,Giuseppe Thadeu Freitas de Abreu*

Main category: eess.SP

TL;DR: 本文研究了AFDM和OTFS调制方案在非白高斯噪声下的鲁棒性，发现AFDM由于解调矩阵稀疏度较低而具有更好的性能，在大多数应用场景中比OTFS和OFDM有超过1dB的性能增益。


<details>
  <summary>Details</summary>
Motivation: 研究AFDM和OTFS调制方案对各种加性干扰源建模的非白高斯噪声的鲁棒性，分析不同波形在噪声环境下的性能差异。

Method: 通过分析解调矩阵的白化能力和稀疏度特性，比较AFDM、OTFS和OFDM的性能，并进行了仿真验证。

Result: AFDM在非白噪声环境下表现优于OTFS和OFDM，其解调矩阵通常比其他波形的矩阵稀疏度更低，在大多数应用场景中性能增益超过1dB。

Conclusion: AFDM调制方案在非白高斯噪声环境下具有最佳性能，适合在窄带信号或与OFDM信号共存等应用场景中使用。

Abstract: This paper investigates the robustness of affine frequency division
multiplexing (AFDM) and orthogonal time frequency space (OTFS) modulation
schemes against non-white Gaussian noise, which can model various sources of
additive disturbances to the received signal. The proposed approach
demonstrates that the performance of these waveforms depends on the ability of
the demodulation matrix to whiten the noise-a property that is, in turn,
related to the sparsity of the matrix. AFDM is shown to outperform OTFS and
orthogonal frequency division multiplexing (OFDM), as its demodulation matrix
is generally less sparse than those of the other waveforms. Based on this
analysis, several application examples and use cases are presented, such as the
use of AFDM and OTFS in narrowband signals or in coexistence with OFDM signals.
Finally, simulation results confirm that AFDM achieves better performance than
OTFS and OFDM in the presence of non-white noise, with gains exceeding 1 dB in
most application scenarios.

</details>


### [14] [Closed-form Solutions for Velocity and Acceleration of a Moving Vehicle Using Range, Range Rate, and Derivative of Range Rate](https://arxiv.org/abs/2510.04037)
*Mohammad Salman,Hadi Zayyani,Hasan Abu Hilal,Mostafa Rashdan*

Main category: eess.SP

TL;DR: 提出了一种基于距离测量的移动目标位置、速度和加速度估计新方法，通过使用距离变化率的导数来扩展传统框架，并推导了闭式最小二乘解。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多只关注位置和速度估计，缺乏对加速度的估计能力，需要扩展框架来更全面地描述移动目标运动学。

Method: 首先使用TOA技术估计位置，然后开发重构的最小二乘和加权最小二乘方法进行速度估计，最后利用距离变化率的导数结合先前的位置和速度估计来估计加速度。

Result: 仿真结果表明，该方法在估计移动目标运动学方面相比现有方法具有改进的性能。

Conclusion: 所提出的方法能够有效估计移动目标的位置、速度和加速度，为运动目标跟踪提供了更全面的解决方案。

Abstract: This letter presents a novel method for estimating the position, velocity,
and acceleration of a moving target using range-based measurements. Although
most existing studies focus on position and velocity estimation, the framework
of this letter is extended to include acceleration. To achieve this, we propose
using the derivative of the range rate, in addition to the range and range rate
measurements. The proposed method estimates the position at first using
Time-of-Arrival (TOA)-based techniques; then, develops a reformulated least
squares (LS) and weighted least squares (WLS) approaches for velocity
estimation; and finally, employs the derivative of the range rate to estimate
the acceleration using previous position and velocity estimates. On the other
hand, closed-form LS and WLS solutions are derived for both velocity and
acceleration. The simulation results show that the proposed approach provides
improved performance in estimating moving target kinematics compared to
existing methods.

</details>


### [15] [CLEAR: A Closed-Form Minimal-Sensor TDOA/FDOA Estimator for Moving-Source IoT Localization](https://arxiv.org/abs/2510.04160)
*Mohammad Kazzazi,Mohammad Morsali,Rouhollah Amiri*

Main category: eess.SP

TL;DR: CLEAR是一种使用最少传感器（N+1个）的闭式定位估计器，融合TDOA和FDOA测量，通过两阶段方法实现高效移动源定位，在噪声假设下达到CRLB级别的统计效率。


<details>
  <summary>Details</summary>
Motivation: 针对功率受限的分布式物联网应用（如无人机跟踪和智能交通），需要开发计算高效且使用最少传感器的定位方法，以降低系统复杂度和功耗。

Method: 两阶段估计器：第一阶段引入辅助参数构建伪线性方程，通过加权最小二乘求解，使用Sylvester结式代数消元简化为四次方程；第二阶段应用轻量级线性细化以减少残差偏差。

Result: 在2D和3D场景的蒙特卡洛模拟中，该方法达到CRLB级别的精度，性能优于代表性两阶段和迭代基线方法，位置和速度估计具有统计效率。

Conclusion: CLEAR方法实现了理论最小传感器数量要求，计算效率高，适合功率受限的物联网应用，为移动源定位提供了有效的闭式解决方案。

Abstract: This paper presents CLEAR -- a closed-form localization estimator with a
reduced sensor network. The proposed method is a computationally efficient,
two-stage estimator that fuses time-difference-of-arrival (TDOA) and
frequency-difference-of-arrival (FDOA) measurements with a minimal number of
sensors. CLEAR localizes a moving source in N-dimensional space using only N+1
sensors, achieving the theoretical minimum sensor count. The first stage
introduces auxiliary range and range-rate parameters to construct a set of
pseudo-linear equations, solved via weighted least squares. An algebraic
elimination using Sylvester's resultant then reduces the problem to a quartic
equation, yielding closed-form estimates for the nuisance variables. A second,
lightweight linear refinement stage is applied to mitigate residual bias. Under
mild Gaussian noise assumptions, the estimator's position and velocity
estimates are statistically efficient, closely approaching the Cramer-Rao lower
bound (CRLB). Extensive Monte Carlo simulations in 2-D and 3-D scenarios
demonstrate CRLB-level accuracy and consistent performance gains over
representative two-stage and iterative baselines, confirming the method's high
suitability for power-constrained, distributed Internet of Things (IoT)
applications such as UAV tracking and smart transportation.

</details>


### [16] [Integrating Phase-Coherent Multistatic Imaging in Downlink D-MIMO Networks](https://arxiv.org/abs/2510.04240)
*Dario Tagliaferri,Silvia Mura,Musa Furkan Keskin,Sauradeep Dey,Henk Wymeersch*

Main category: eess.SP

TL;DR: 提出了一种在分布式MIMO通信网络中集成多基地相干成像功能的D-ISAC系统，通过叠加专门设计的AP特定成像信号与传统通信信号，实现可调谐的权衡，并优化接收AP选择策略以最大化成像性能。


<details>
  <summary>Details</summary>
Motivation: 解决在相位相干分布式MIMO通信网络下行链路中集成多基地相干成像功能的挑战，传统通信需要AP联合预编码以最大化用户频谱效率，而成像需要部分AP作为接收器且发射AP发射AP特定的正交信号。

Method: 提出分布式集成感知与通信系统，将专门设计的AP特定成像信号叠加到传统通信信号上；详细设计符合扩展正交条件的成像波形和空频预编码器；提出优化的接收AP选择策略以在半双工约束下最大化成像性能。

Result: 广泛的数值结果证明了该方案的可行性和优势，实现了在实际D-MIMO部署中联合多基地成像和通信的潜力。

Conclusion: 该研究成功实现了在分布式MIMO网络中同时进行通信和成像的集成系统，通过创新的信号设计和优化策略，为实际部署提供了可行的解决方案。

Abstract: This paper addresses the challenge of integrating multistatic coherent
imaging functionalities in the downlink (DL) of a phase-coherent distributed
multiple input multiple output (D-MIMO) communication network. During DL, the
D-MIMO access points (APs) jointly precode the transmitted signals to maximize
the spectral efficiency (SE) at the users (UEs) locations. However, imaging
requires that \textit{(i)} a fraction of the APs work as receivers for sensing
and \textit{(ii)} the transmitting APs emit AP-specific and orthogonal signals
to illuminate the area to be imaged and allow multistatic operation. In these
settings, our contribution is twofold. We propose a novel distributed
integrated sensing and communication (D-ISAC) system that superposes a
purposely designed AP-specific signal for imaging to the legacy UE-specific
communication one, with a tunable trade-off factor. We detail both the imaging
waveform design according to the \textit{extended orthogonality condition} and
the space-frequency precoder design. Then, we propose an optimized selection
strategy for the receiving APs, in order to maximize imaging performance under
half-duplex constraints. Extensive numerical results prove the feasibility and
benefits of our proposal, materializing the potential of joint multistatic
imaging and communications in practical D-MIMO deployments.

</details>


### [17] [Terahertz Channel Measurement and Modeling for Short-Range Indoor Environments](https://arxiv.org/abs/2510.04258)
*Ziang Zhao,Weixi Liang,Kai Hu,Qun Zhang,Xiongbin Yu,Qiang Li*

Main category: eess.SP

TL;DR: 提出了一种基于物理的Rician衰落信道模型，用于6G室内太赫兹通信，通过联合确定性和随机性分量以及频率相关衰减，显著提高了信道建模精度。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理太赫兹通信中的严重频率选择性和多径效应方面存在困难，需要更准确的信道建模来实现6G室内网络的潜力。

Method: 采用物理基础的Rician衰落信道模型，结合确定性LOS和随机性NLOS分量，使用优化的alpha和beta指数表征频率相关衰减，集成双射线反射框架捕捉驻波现象，并采用宽带频谱平均来减轻频率选择性。

Result: 在208 GHz载波频率下，0.1-0.9米范围内的实测数据显示，模型实现了低至2.54 dB的RMSE，比自由空间路径损耗模型提升14.2%，带宽增加时RMSE降低73.3%。

Conclusion: 该方法为太赫兹系统设计提供了坚实基础，支持可靠的室内无线个域网、设备间通信和未来6G应用中的精确定位，强调了带宽在抑制振荡伪影和提高建模精度中的重要性。

Abstract: Accurate channel modeling is essential for realizing the potential of
terahertz (THz) communications in 6G indoor networks, where existing models
struggle with severe frequency selectivity and multipath effects. We propose a
physically grounded Rician fading channel model that jointly incorporates
deterministic line-of-sight (LOS) and stochastic non-line-of-sight (NLOS)
components, enhanced by frequency-dependent attenuation characterized by
optimized exponents alpha and beta. Unlike conventional approaches, our model
integrates a two-ray reflection framework to capture standing wave phenomena
and employs wideband spectral averaging to mitigate frequency selectivity over
bandwidths up to 15 GHz. Empirical measurements at a 208 GHz carrier, spanning
0.1-0.9 m, demonstrate that our model achieves root mean square errors (RMSE)
as low as 2.54 dB, outperforming free-space path loss (FSPL) by up to 14.2% and
reducing RMSE by 73.3% as bandwidth increases. These findings underscore the
importance of bandwidth in suppressing oscillatory artifacts and improving
modeling accuracy. Our approach provides a robust foundation for THz system
design, supporting reliable indoor wireless personal area networks (WPANs),
device-to-device (D2D) communications, and precise localization in future 6G
applications.

</details>


### [18] [Efficient Domain Generalization in Wireless Networks with Scarce Multi-Modal Data](https://arxiv.org/abs/2510.04359)
*Minsu Kim,Walid Saad,Dour Calin*

Main category: eess.SP

TL;DR: 提出一种用于6G无线网络的两阶段学习框架，通过物理知识引导和协作域适应，在稀缺多模态数据下实现鲁棒泛化


<details>
  <summary>Details</summary>
Motivation: 解决6G网络中多模态ML模型在域偏移下泛化能力差的问题，因为实际无线系统中经常出现信道统计变化、移动障碍物等导致的域偏移

Method: 两阶段框架：第一阶段使用基于物理的损失函数学习无线环境物理特性；第二阶段提出协作域适应，通过域相似性感知模型聚合来利用多个基站的无线环境知识

Result: 基于物理的训练仅需13%数据样本即可达到不使用物理训练的最先进基线性能；协作域适应仅需25%数据样本和20%FLOPs即可收敛

Conclusion: 该框架在稀缺多模态数据下显著提升了无线网络中的泛化性能，为6G网络中的情境感知决策提供了有效解决方案

Abstract: In 6G wireless networks, multi-modal ML models can be leveraged to enable
situation-aware network decisions in dynamic environments. However, trained ML
models often fail to generalize under domain shifts when training and test data
distributions are different because they often focus on modality-specific
spurious features. In practical wireless systems, domain shifts occur
frequently due to dynamic channel statistics, moving obstacles, or hardware
configuration. Thus, there is a need for learning frameworks that can achieve
robust generalization under scarce multi-modal data in wireless networks. In
this paper, a novel and data-efficient two-phase learning framework is proposed
to improve generalization performance in unseen and unfamiliar wireless
environments with minimal amount of multi-modal data. In the first stage, a
physics-based loss function is employed to enable each BS to learn the physics
underlying its wireless environment captured by multi-modal data. The
data-efficiency of the physics-based loss function is analytically
investigated. In the second stage, collaborative domain adaptation is proposed
to leverage the wireless environment knowledge of multiple BSs to guide
under-performing BSs under domain shift. Specifically, domain-similarity-aware
model aggregation is proposed to utilize the knowledge of BSs that experienced
similar domains. To validate the proposed framework, a new dataset generation
framework is developed by integrating CARLA and MATLAB-based mmWave channel
modeling to predict mmWave RSS. Simulation results show that the proposed
physics-based training requires only 13% of data samples to achieve the same
performance as a state-of-the-art baseline that does not use physics-based
training. Moreover, the proposed collaborative domain adaptation needs only 25%
of data samples and 20% of FLOPs to achieve the convergence compared to
baselines.

</details>


### [19] [Low-Rank-Based Approximate Computation with Memristors](https://arxiv.org/abs/2510.04402)
*Binyu Lu,Matthias Frey,Stark Draper,Jingge Zhu*

Main category: eess.SP

TL;DR: 提出基于低秩矩阵分解的忆阻器交叉阵列向量矩阵乘法方案，通过奇异值分解和两步串行计算来缓解写入误差，提高计算精度。


<details>
  <summary>Details</summary>
Motivation: 忆阻器交叉阵列难以精确写入电导值，导致向量矩阵乘法精度受限，需要改进方案来提高计算准确性。

Method: 使用奇异值分解获得目标矩阵的低秩近似，分解为两个较小矩阵，通过两步串行向量矩阵乘法和步进平均来减轻随机写入误差。

Result: 推导了计算误差的一般表达式，在规定的奇异值分布下进行渐近分析，显示误差如何随矩阵大小和秩缩放，分析和数值结果均证实方案优于基准方法。

Conclusion: 所提出的低秩矩阵分解方案能有效提高忆阻器交叉阵列的计算精度，在矩阵大小和秩的缩放关系上表现出优越性能。

Abstract: Memristor crossbars enable vector-matrix multiplication (VMM), and are
promising for low-power applications. However, it can be difficult to write the
memristor conductance values exactly. To improve the accuracy of VMM, we
propose a scheme based on low-rank matrix approximation. Specifically, singular
value decomposition (SVD) is first applied to obtain a low-rank approximation
of the target matrix, which is then factored into a pair of smaller matrices.
Subsequently, a two-step serial VMM is executed, where the stochastic write
errors are mitigated through step-wise averaging. To evaluate the performance
of the proposed scheme, we derive a general expression for the resulting
computation error and provide an asymptotic analysis under a prescribed
singular-value profile, which reveals how the error scales with matrix size and
rank. Both analytical and numerical results confirm the superiority of the
proposed scheme compared with the benchmark scheme.

</details>


### [20] [Effect of nearby Metals on Electro-Quasistatic Human Body Communication](https://arxiv.org/abs/2510.04409)
*Samyadip Sarkar,Arunashish Datta,David Yang,Mayukh Nath,Shovan Maity,Shreyas Sen*

Main category: eess.SP

TL;DR: 本研究系统分析了金属物体对人体通信信道的影响，发现近距离金属物体可降低传输损耗10dB，接地金属连接可提升信道增益至少20dB，接触面积影响高频信道特性。


<details>
  <summary>Details</summary>
Motivation: 虽然前人研究注意到寄生返回路径在电容耦合系统中的作用，但周围金属物体对这些路径的影响尚未充分探索，而这对EQS无线信号传输至关重要。

Method: 采用理论框架结合有限元方法模拟和可穿戴设备实验，系统研究各种导电物体（非接地金属、接地金属、封闭金属环境）对人体通信信道的影响。

Result: 距离设备20cm内的金属物体可减少传输损耗约10dB；设备接地连接到接地金属物体时，信道增益可增加至少20dB；触摸接地金属时的接触面积会产生接触阻抗依赖的高通信道特性。

Conclusion: 这些发现增进了对人体中心通信链路的理解，并为医疗保健、消费电子、国防和工业应用的设计提供了指导。

Abstract: In recent decades Human Body Communication has emerged as a promising
alternative to traditional radio wave communication, utilizing the body's
conductive properties for low-power connectivity among wearables. This method
harnesses the human body as an energy-efficient channel for data transmission
within the electro-quasistatic frequency range, enabling advancements in
human-machine interaction. While prior work has noted the role of parasitic
return paths in such capacitively coupled systems, the influence of surrounding
metallic objects on these paths, which are critical for EQS wireless signaling,
has not been fully explored. This paper fills that gap with a structured study
of how various conducting objects, from non-grounded (floating) metals and
grounded metals to enclosed metallic environments such as elevators and cars,
affect the body-communication channel. We present a theoretical framework
supported by finite element method simulations and experiments with wearable
devices. Results show that metallic objects within 20 cm of devices can reduce
transmission loss by about 10 dB. When a device ground connects to a grounded
metallic object, channel gain can increase by at least 20 dB. Contact area
during touch-based interactions with grounded metals produces contact-impedance
dependent high-pass channel characteristics. Proximity to metallic objects
introduces variability within a critical distance, with grounded metals
producing a larger overall effect than floating metals. These findings improve
understanding of body-centric communication links and inform design for
healthcare, consumer electronics, defense, and industrial applications.

</details>


### [21] [The Role of ISAC in 6G Networks: Enabling Next-Generation Wireless Systems](https://arxiv.org/abs/2510.04413)
*Muhammad Umar Farooq Qaisar,Weijie Yuan,Onur Günlü,Taneli Riihonen,Yuanhao Cui,Lin Zhang,Nuria Gonzalez-Prelcic,Marco Di Renzo,Zhu Han*

Main category: eess.SP

TL;DR: 本文是关于6G网络中集成感知与通信(ISAC)的教程，阐述了ISAC在6G中的核心作用、技术原理、实现方法和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要将通信与感知技术深度融合，以支持下一代应用。ISAC能够提高频谱效率、降低延迟，并为智慧城市、自主系统等多样化用例提供支持。

Method: 通过概述ISAC从5G到6G的演进历程，介绍其核心原理和系统变体，深入讨论促进其实际部署的使能技术，并分析当前研究方向。

Result: 提出了ISAC在6G网络中的设计见解和建议，解决了三个核心问题：ISAC对6G的必要性、带来的创新以及如何塑造无线通信的未来。

Conclusion: ISAC是6G网络发展的关键技术，通过统一的框架实现通信与感知的端到端支持，将深刻影响未来无线通信的发展方向。

Abstract: The commencement of the sixth-generation (6G) wireless networks represents a
fundamental shift in the integration of communication and sensing technologies
to support next-generation applications. Integrated sensing and communication
(ISAC) is a key concept in this evolution, enabling end-to-end support for both
communication and sensing within a unified framework. It enhances spectrum
efficiency, reduces latency, and supports diverse use cases, including smart
cities, autonomous systems, and perceptive environments. This tutorial provides
a comprehensive overview of ISAC's role in 6G networks, beginning with its
evolution since 5G and the technical drivers behind its adoption. Core
principles and system variations of ISAC are introduced, followed by an
in-depth discussion of the enabling technologies that facilitate its practical
deployment. The paper further analyzes current research directions to highlight
key challenges, open issues, and emerging trends. Design insights and
recommendations are also presented to support future development and
implementation. This work ultimately try to address three central questions:
Why is ISAC essential for 6G? What innovations does it bring? How will it shape
the future of wireless communication?

</details>


### [22] [Joint Probing and Scheduling for Cache-Aided Hybrid Satellite-Terrestrial Networks](https://arxiv.org/abs/2510.04492)
*Zhou Zhang,Yizhu Wang,Saman Atapattu,Sumei Sun*

Main category: eess.SP

TL;DR: 该论文提出了一种基于最优停止理论的联合探测与调度策略，通过LEO卫星探测卫星-地面链路和多个协作地面站的缓存状态，动态调度用户进行内容分发，显著提高了卫星-地面混合网络的系统吞吐量。


<details>
  <summary>Details</summary>
Motivation: 在带宽受限的卫星系统中，缓存对于降低延迟、优化吞吐量和提高数据可用性至关重要。需要战略性的MAC层来支持卫星-地面混合网络中的吞吐量优化。

Method: 采用最优停止理论方法，提出两级不完全信息的联合探测和调度策略。利用LEO卫星探测卫星-地面链路和协作地面站的缓存状态，实现动态用户调度和内容分发。

Result: 仿真结果验证了所提策略的有效性和实用性，通过利用协作缓存、卫星-地面链路传输和动态用户请求的时间分集，显著提高了平均系统吞吐量。

Conclusion: 基于阈值的最优停止策略能够有效优化探测和调度过程，在卫星-地面混合网络中实现吞吐量的大幅提升，证明了协作缓存策略的实用价值。

Abstract: Caching is crucial in hybrid satellite-terrestrial networks to reduce
latency, optimize throughput, and improve data availability by storing
frequently accessed content closer to users, especially in bandwidth-limited
satellite systems, requiring strategic Medium Access Control (MAC) layer. This
paper addresses throughput optimization in satellite-terrestrial integrated
networks through opportunistic cooperative caching. We propose a joint probing
and scheduling strategy to enhance content retrieval efficiency. The strategy
leverages the LEO satellite to probe satellite-to-ground links and cache states
of multiple cooperative terrestrial stations, enabling dynamic user scheduling
for content delivery. Using an optimal stopping theoretic approach with two
levels of incomplete information, we make real-time decisions on
satellite-terrestrial hybrid links and caching probing. Our threshold-based
strategy optimizes probing and scheduling, significantly improving average
system throughput by exploiting cooperative caching, satellite-terrestrial link
transmission, and time diversity from dynamic user requests. Simulation results
validate the effectiveness and practicality of the proposed strategies.

</details>


### [23] [Performance Analysis for Multi-User Holographic MIMO Downlink with Matched Filter Precoding](https://arxiv.org/abs/2510.04530)
*Gayathri Shekar,Saman Atapattu,Prathapasinghe Dharmawansa,Kandeepan Sithamparanathan*

Main category: eess.SP

TL;DR: 本文首次对多用户全息MIMO下行系统进行了分析性能研究，推导了匹配滤波预编码的闭式SINR表达式，并提出了在完整、部分和无CSI场景下的吞吐量近似方法。


<details>
  <summary>Details</summary>
Motivation: 虽然全息MIMO在电磁建模和仿真方面已有研究，但缺乏严谨的通信理论框架来分析其性能。

Method: 采用等效随机变量模型，结合多径传播、互耦合和元件激励，推导匹配滤波预编码的闭式SINR表达式，并利用双变量伽马分布开发可处理的吞吐量近似。

Result: 数值结果验证了所提框架的准确性，并显示匹配滤波预编码在低SINR和CSI不确定性下具有竞争性性能和强鲁棒性。

Conclusion: 匹配滤波预编码在全息MIMO系统中表现出良好的性能，特别是在具有挑战性的信道条件下。

Abstract: Holographic MIMO (HMIMO) has emerged as a promising solution for future
wireless systems by enabling ultra-dense, spatially continuous antenna
deployments. While prior studies have primarily focused on electromagnetic (EM)
modeling or simulation-based performance analysis, a rigorous
communication-theoretic framework remains largely unexplored. This paper
presents the first analytical performance study of a multi-user HMIMO downlink
system with matched filter (MF) precoding - a low-complexity baseline scheme.
By incorporating multipath propagation, mutual coupling, and element
excitation, we derive a novel closed-form expression for the MF
signal-to-interference-plus-noise ratio (SINR) using an equivalent random
variable model. Leveraging bivariate gamma distributions, we then develop
tractable throughput approximations under full, partial, and no channel state
information (CSI) scenarios. Additionally, we formulate a max-min beamforming
problem to benchmark optimal user fairness performance. Numerical results
validate the accuracy of the proposed framework and reveal that MF precoding
achieves competitive performance with strong robustness to low SINR and CSI
uncertainty.

</details>


### [24] [Coordinated Beamforming for Networked Integrated Communication and Multi-TMT Localization](https://arxiv.org/abs/2510.04600)
*Meidong Xia,Zhenyao He,Wei Xu,Yongming Huang,Derrick Wing Kwan Ng,Naofal Al-Dhahir*

Main category: eess.SP

TL;DR: 该论文研究了网络化集成感知与通信(ISAC)系统中的协调波束成形设计，重点关注基于到达时间(ToA)的多目标监测终端(TMT)定位。提出了两种优化问题(感知中心和通信中心)，并开发了相应的全局最优和次优高效算法。


<details>
  <summary>Details</summary>
Motivation: 网络化ISAC系统将感知信号接收委托给专用目标监测终端而非基站，可显著提升感知能力和部署灵活性。然而，针对网络化集成通信和基于ToA的多TMT定位的协调波束成形设计仍未被充分探索。

Method: 首先建立了通信和定位的信号模型，首次推导了定位性能的闭式克拉美-罗下界(CRLB)。利用CRLB制定了两种优化问题：感知中心和通信中心。针对感知中心问题，在基站天线数多于通信用户总数时开发了基于半定松弛(SDR)的全局最优算法；针对通信中心问题，在单基站情况下设计了基于二分搜索的全局最优算法。对于一般情况，提出了基于连续凸近似(SCA)的统一算法。

Result: 仿真结果表明所提算法的有效性，揭示了通信与定位之间的内在性能权衡，并进一步发现在网络化ISAC系统中部署更多TMT比部署更多基站更优。

Conclusion: 该研究填补了网络化集成通信和基于ToA的多TMT定位协调波束成形设计的空白，提出的算法和性能分析为网络化ISAC系统的实际部署提供了重要指导。

Abstract: Networked integrated sensing and communication (ISAC) has gained significant
attention as a promising technology for enabling next-generation wireless
systems. To further enhance networked ISAC, delegating the reception of sensing
signals to dedicated target monitoring terminals (TMTs) instead of base
stations (BSs) offers significant advantages in terms of sensing capability and
deployment flexibility. Despite its potential, the coordinated beamforming
design for networked integrated communication and time-of-arrival (ToA)-based
multi-TMT localization remains largely unexplored. In this paper, we present a
comprehensive study to fill this gap. Specifically, we first establish signal
models for both communication and localization, and, for the first time, derive
a closed-form Cram\'er-Rao lower bound (CRLB) to characterize the localization
performance. Subsequently, we exploit this CRLB to formulate two optimization
problems, focusing on sensing-centric and communication-centric criteria,
respectively. For the sensing-centric problem, we develop a globally optimal
algorithm based on semidefinite relaxation (SDR) when each BS is equipped with
more antennas than the total number of communication users. While for the
communication-centric problem, we design a globally optimal algorithm for the
single-BS case using bisection search. For the general case of both problems,
we propose a unified successive convex approximation (SCA)-based algorithm,
which is suboptimal yet efficient, and further extend it from single-target
scenarios to more practical multi-target scenarios. Finally, simulation results
demonstrate the effectiveness of our proposed algorithms, reveal the intrinsic
performance trade-offs between communication and localization, and further show
that deploying more TMTs is always preferable to deploying more BSs in
networked ISAC systems.

</details>


### [25] [Dimensionally-Efficient Transmission and Storage of Unitary Matrices](https://arxiv.org/abs/2510.04734)
*Juan Vidal Alegría*

Main category: eess.SP

TL;DR: 提出了一种维度高效参数化(DEP)方法，可将酉矩阵表示为实数序列，维度与酉群维度一致，有效将存储和传输需求减半。


<details>
  <summary>Details</summary>
Motivation: 酉矩阵在信号处理中广泛应用，但存储和传输成本高。需要找到高效表示方法来减少内存和吞吐量需求。

Method: 显式推导酉矩阵的维度高效参数化，建立酉矩阵与实数序列的对应关系，并给出逆映射方法。同时分析了参数的有界性和量化方法。

Result: 提出的DEP方法能将酉矩阵的维度减半，参数序列有界且易于量化，在无线通信等应用中展现出良好潜力。

Conclusion: 该方法为酉矩阵的高效存储和传输提供了有效解决方案，特别是在资源受限的应用场景中具有重要价值。

Abstract: Unitary matrices are the basis of a large number of signal processing
applications. In many of these applications, finding ways to efficiently store,
and even transmit these matrices, can significantly reduce memory and
throughput requirements. In this work, we study the problem of efficient
transmission and storage of unitary matrices. Specifically, we explicitly
derive a dimensionally-efficient parametrization (DEP) for unitary matrices
that allows identifying them with sequences of real numbers, where the
dimension coincides with the dimension of the unitary group where they lie. We
also characterize its inverse map that allows retrieving the original unitary
matrices from their DEP. The proposed approach effectively allows halving the
dimension with respect to naively considering all the entries of each unitary
matrix, thus reducing the resources required to store and transmit these
matrices. Furthermore, we show that the sequence of real numbers associated to
the proposed DEP is bounded, and we delimit the interval where these numbers
are contained, facilitating the implementation of quantization approaches with
limited distortion. On the other hand, we outline ways to further reduce the
dimension of the DEP when considering more restrictive constraints for matrices
that show up in certain applications. The numerical results showcase the
potential of the proposed approach in general settings, as well as in three
specific applications of current interest for wireless communications research.

</details>


### [26] [Multilayer Non-Terrestrial Networks with Spectrum Access aided by Beyond-Diagonal RIS](https://arxiv.org/abs/2510.04744)
*Wali Ullah Khan,Chandan Kumar Sheemar,Eva Lagunas,Xingwang Li,Symeon Chatzinotas,Petar Popovski,Zhu Han*

Main category: eess.SP

TL;DR: 本文研究了多层非地面网络(NTN)，其中卫星作为主网络，高空平台站(HAPS)作为认知无线电的次级网络。通过使用透射式BD-RIS天线前端，并联合优化BD-RIS相位响应和HAPS发射功率分配，在严格干扰温度约束下实现了显著的数据速率提升和干扰抑制。


<details>
  <summary>Details</summary>
Motivation: 传统天线阵列成本高、复杂度大、功耗高，需要更高效的解决方案来支持未来多层NTN的发展。BD-RIS技术能够提供更好的性能，同时降低系统复杂度和成本。

Method: 采用交替优化框架：功率分配子问题通过KKT条件得到闭式的水填充解，BD-RIS配置通过黎曼流形优化进行细化。

Result: 仿真结果显示，相比对角RIS辅助基准，在数据速率和干扰抑制方面取得了显著增益。

Conclusion: BD-RIS是未来多层NTN的有前景的使能技术，能够有效提升系统性能同时降低复杂度。

Abstract: In this work, we study a multi-user NTN in which a satellite serves as the
primary network and a high-altitude platform station (HAPS) operates as the
secondary network, acting as a cognitive radio. To reduce the cost, complexity,
and power consumption of conventional antenna arrays, we equip the HAPS with a
transmissive BD-RIS antenna front end. We then formulate a joint optimization
problem for the BD-RIS phase response and the HAPS transmit power allocation
under strict per-user interference temperature constraints. To tackle the
resulting highly nonconvex problem, we propose an alternating-optimization
framework: the power-allocation subproblem admits a closed-form,
water-filling-type solution derived from the Karush-Kuhn-Tucker (KKT)
conditions, while the BD-RIS configuration is refined via Riemannian manifold
optimization. Simulation results show significant gains in data rate and
interference suppression over diagonal RIS-assisted benchmarks, establishing
BD-RIS as a promising enabler for future multilayer NTNs.

</details>


### [27] [Interference Alignment for Multi-cluster Over-the-Air Computation](https://arxiv.org/abs/2510.04745)
*Lucas Sempéré,Yue Bi,Yue Wu,Pengwenlong Gu,Selma Boumerdassi*

Main category: eess.SP

TL;DR: 提出了一种针对上行链路AirComp系统的干扰对齐方案，能够在多簇网络中有效管理干扰，让每个簇可利用一半可用信道而非传统时间共享的1/K。


<details>
  <summary>Details</summary>
Motivation: 解决物联网网络中大量设备同时通信导致的干扰问题，特别是在多簇网络中，提升AirComp在密集干扰环境下的性能。

Method: 设计了一种新颖的干扰对齐方案，适用于任意数量K的集群，并针对相邻簇共享用户的场景开发了专门方案。

Result: 提出的方法使每个簇能够利用一半可用信道，相比传统时间共享方案（仅1/K）显著提高了信道利用率。

Conclusion: 该干扰对齐方案为密集多簇物联网网络中的AirComp系统提供了有效的干扰管理解决方案，显著提升了系统性能。

Abstract: One of the main challenges facing Internet of Things (IoT) networks is
managing interference caused by the large number of devices communicating
simultaneously, particularly in multi-cluster networks where multiple devices
simultaneously transmit to their respective receiver. Over-the-Air Computation
(AirComp) has emerged as a promising solution for efficient real-time data
aggregation, yet its performance suffers in dense, interference-limited
environments. To address this, we propose a novel Interference Alignment (IA)
scheme tailored for up-link AirComp systems. Unlike previous approaches, the
proposed method scales to an arbitrary number $\sf K$ of clusters and enables
each cluster to exploit half of the available channels, instead of only
$\tfrac{1}{\sf K}$ as in time-sharing. In addition, we develop schemes tailored
to scenarios where users are shared between adjacent clusters.

</details>


### [28] [The IEEE Signal Processing Society's Leading Role in Developing Standards for Computational Imaging and Sensing: Part II](https://arxiv.org/abs/2510.04913)
*Andreas Bathelt,Benjamin Deutschmann,Hyeon Seok Rou,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Peter Vouras*

Main category: eess.SP

TL;DR: 该论文讨论了如何通过利用额外自由度来克服成像和传感应用中硬件约束，并强调了IEEE信号处理协会在制定计算传感技术标准方面的重要作用。


<details>
  <summary>Details</summary>
Motivation: 在成像和传感应用中，物理硬件存在固有约束，需要通过额外自由度来扩展系统性能。随着这些技术从概念原型阶段走向商业化，标准制定变得至关重要。

Method: 通过同步分布式传感器合成更大孔径，以及通过波形设计和资源管理集成通信与传感功能。IEEE信号处理协会通过SASC委员会的工作组制定相关标准。

Result: 重点介绍了P3383 ISAC系统性能指标工作组和P3343分布式传感器合成孔径时空同步工作组的标准制定活动。

Conclusion: 标准确保不同厂商系统间的互操作性，并为行业定义最佳实践，对计算传感技术的商业化成功至关重要。

Abstract: In every imaging or sensing application, the physical hardware creates
constraints that must be overcome or they limit system performance. Techniques
that leverage additional degrees of freedom can effectively extend performance
beyond the inherent physical capabilities of the hardware. An example includes
synchronizing distributed sensors so as to synthesize a larger aperture for
remote sensing applications. An additional example is integrating the
communication and sensing functions in a wireless system through the clever
design of waveforms and optimized resource management. As these technologies
mature beyond the conceptual and prototype phase they will ultimately
transition to the commercial market. Here, standards play a critical role in
ensuring success. Standards ensure interoperability between systems
manufactured by different vendors and define industry best practices for
vendors and customers alike. The Signal Processing Society of the Institute for
Electrical and Electronics Engineers (IEEE) plays a leading role in developing
high-quality standards for computational sensing technologies through the
working groups of the Synthetic Aperture Standards Committee (SASC). In this
column we highlight the standards activities of the P3383 Performance Metrics
for Integrated Sensing and Communication (ISAC) Systems Working Group and the
P3343 Spatio-Temporal Synchronization of a Synthetic Aperture of Distributed
Sensors Working Group.

</details>


### [29] [My First Five Years of Faculty Career at the University of Delaware](https://arxiv.org/abs/2510.05000)
*Xiang-Gen Xia*

Main category: eess.SP

TL;DR: 作者总结了自己在美国大学前5年的研究成果，认为这是自己职业生涯中最好的成果，并希望这些经验能帮助年轻研究者。


<details>
  <summary>Details</summary>
Motivation: 分享个人在学术生涯初期的研究经验和成果，为年轻研究者提供参考和帮助。

Method: 通过个人总结和回顾的方式，整理前5年的研究成果。

Result: 作者认为这5年的研究成果是自己职业生涯中最喜欢的成果。

Conclusion: 作者希望自己的经验能够对年轻研究者有所帮助。

Abstract: In this short article, I would like to briefly summarize my research in the
first 5 years in my university academia life in USA. I think that my research
results obtained in these 5 years are the best in my career, at least which I
like the most by myself. I wish that my experience in my junior academia career
could be of some help to young researchers.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [30] [PARS: Low-Latency LLM Serving via Pairwise Learning-to-Rank](https://arxiv.org/abs/2510.03243)
*Yiheng Tao,Yihe Zhang,Matthew T. Dearing,Xin Wang,Yuping Fan,Zhiling Lan*

Main category: cs.LG

TL;DR: PARS是一个基于提示感知的LLM任务调度器，通过近似最短作业优先调度来提升推理效率，减少头部阻塞问题。


<details>
  <summary>Details</summary>
Motivation: 传统FCFS调度策略存在头部阻塞问题，长任务会延迟短任务，影响LLM推理的延迟和吞吐量。

Method: 使用成对排序和边界排序损失来预测基于响应长度的任务排序，近似SJF调度，并集成到vLLM系统中。

Result: 在多个LLM和真实推理数据集上的实验表明，PARS显著提升了性能，包括推理工作负载，且跨模型评估显示设计具有良好的泛化性。

Conclusion: PARS通过提示感知调度有效减少了LLM推理延迟，具有低开销和良好的跨模型泛化能力。

Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low
latency and high throughput, particularly with the growing use of
reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve
(FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks
delay shorter ones queued behind them. In this paper, we introduce PARS, a
prompt-aware LLM task scheduler that improves serving efficiency by
approximating shortest-job-first (SJF) scheduling through pairwise ranking with
margin ranking loss. PARS focuses on impactful scheduling decisions and is
seamlessly integrated into the state-of-the-art LLM serving system vLLM. It
effectively predicts response-length-based task ordering, reducing latency with
minimal overhead. Extensive experiments across multiple LLMs and real-world
inference datasets show that PARS significantly improves performance, including
for reasoning workloads. Furthermore, our cross-model evaluations demonstrate
that the design generalizes well, enabling effective scheduling even when
predictors are trained on different LLMs.

</details>


### [31] [Certifiable Safe RLHF: Fixed-Penalty Constraint Optimization for Safer Language Models](https://arxiv.org/abs/2510.03520)
*Kartik Pandit,Sourav Ganguly,Arnesh Banerjee,Shaahin Angizi,Arnob Ghosh*

Main category: cs.LG

TL;DR: CS-RLHF提出了一种可认证的安全RLHF方法，通过基于修正惩罚的公式替代拉格朗日方法，直接强制执行安全约束，无需双变量更新，显著提升了LLM的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于CMDP的方法存在两个主要问题：1）性能对评分机制高度敏感；2）双变量调优计算昂贵且无法提供可证明的安全保证。需要一种更有效的方法来平衡LLM的实用性和安全性。

Method: 引入了CS-RLHF方法，使用在大规模语料库上训练的成本模型分配语义基础的安全分数，采用基于修正惩罚的公式，利用约束优化中的精确惩罚函数理论直接强制执行安全约束。

Result: 实证评估表明，CS-RLHF在对抗普通和越狱提示方面，比最先进的LLM模型响应至少高效5倍。

Conclusion: CS-RLHF通过消除双变量更新的需求，提供了一种计算效率更高且具有可证明安全保证的方法，有效解决了现有安全RL方法的局限性。

Abstract: Ensuring safety is a foundational requirement for large language models
(LLMs). Achieving an appropriate balance between enhancing the utility of model
outputs and mitigating their potential for harm is a complex and persistent
challenge. Contemporary approaches frequently formalize this problem within the
framework of Constrained Markov Decision Processes (CMDPs) and employ
established CMDP optimization techniques. However, these methods exhibit two
notable limitations. First, their reliance on reward and cost functions renders
performance highly sensitive to the underlying scoring mechanism, which must
capture semantic meaning rather than being triggered by superficial keywords.
Second, CMDP-based training entails tuning dual-variable, a process that is
both computationally expensive and does not provide any provable safety
guarantee for a fixed dual variable that can be exploitable through adversarial
jailbreaks. To overcome these limitations, we introduce Certifiable Safe-RLHF
(CS-RLHF) that introduces a cost model trained on a large-scale corpus to
assign semantically grounded safety scores. In contrast to the lagrangian-based
approach, CS-RLHF adopts a rectified penalty-based formulation. This design
draws on the theory of exact penalty functions in constrained optimization,
wherein constraint satisfaction is enforced directly through a suitably chosen
penalty term. With an appropriately scaled penalty, feasibility of the safety
constraints can be guaranteed at the optimizer, eliminating the need for
dual-variable updates. Empirical evaluation demonstrates that CS-RLHF
outperforms state-of-the-art LLM model responses rendering at-least 5 times
efficient against nominal and jail-breaking prompts

</details>


### [32] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: VIFO是一个跨模态时间序列预测模型，通过将多元时间序列转换为图像，利用预训练的大型视觉模型提取通道间依赖关系，并与时间序列模态特征对齐融合，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型时间序列基础模型采用通道独立架构，忽略了关键的跨通道依赖关系；同时多模态方法未能充分利用大型视觉模型解释时空数据的能力；不同模态信息提取的优势尚未充分用于提升时间序列预测性能。

Method: 将多元时间序列渲染为图像，利用预训练的大型视觉模型提取复杂的跨通道模式，这些视觉特征与时间序列模态的表征进行对齐和融合，仅训练7.45%的参数。

Result: 在多个基准测试中取得了有竞争力的性能，提供了捕捉跨变量关系的有效解决方案。

Conclusion: VIFO通过跨模态方法有效解决了通道独立模型的局限性，为时间序列预测提供了高效且有效的解决方案。

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [33] [Generalization of Graph Neural Network Models for Distribution Grid Fault Detection](https://arxiv.org/abs/2510.03571)
*Burak Karabulut,Carlo Manna,Chris Develder*

Main category: cs.LG

TL;DR: 本文系统性地比较了不同图神经网络架构在配电网故障检测中的性能，发现RGATv2模型在不同拓扑变化下具有最佳泛化能力，而纯RNN模型表现最差。


<details>
  <summary>Details</summary>
Motivation: 配电网故障检测对系统可靠性至关重要，但现有方法需要适应不断变化的电网拓扑。当前基于RNN+GNN的方法中，GCN架构可能不是最优选择，需要系统评估更先进的GNN架构。

Method: 采用RNN+GNN流水线模型，首次将GraphSAGE、GAT和GATv2应用于故障诊断，并与RGCN和纯RNN模型（特别是GRU）进行系统性对比，重点评估它们在不同拓扑设置下的泛化能力。

Result: 在IEEE 123节点配电网上的实验表明，RGATv2具有最佳泛化性能，F1分数仅下降约12%；纯RNN模型表现最差，F1分数下降高达60%；其他RGNN变体性能下降约25%。

Conclusion: RGATv2在配电网故障检测中展现出卓越的泛化能力，能够有效应对电网拓扑变化，是实际部署的优选方案。

Abstract: Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.

</details>


### [34] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: 提出了一种可转移的频率感知对抗攻击方法，并基于此开发了频率感知模型参数探索器(FAMPE)来提升深度神经网络的可解释性，在插入得分上比现有最佳方法平均提升13.02%。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在真实世界噪声和有意扰动下的可靠性问题，现有归因方法效果不理想需要进一步改进。

Method: 提出可转移的频率感知对抗攻击，通过高低频分量进行频率感知探索，并基于此开发FAMPE归因方法。

Result: 相比当前最佳方法AttEXplore，FAMPE在插入得分上平均提升13.02%，通过消融研究验证了高低频分量在可解释性中的作用。

Conclusion: 频率感知方法能有效提升深度神经网络的可解释性，高低频分量在模型解释中都具有重要作用。

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [35] [Optimising Battery Energy Storage System Trading via Energy Market Operator Price Forecast](https://arxiv.org/abs/2510.03657)
*Aymeric Fabre*

Main category: cs.LG

TL;DR: 该研究探索如何利用澳大利亚能源市场运营商(AEMO)的电价预测数据开发可靠的电池储能系统(BESS)交易算法，通过分析预测准确性模式创建基于预测的交易模型，并与无预测的基本算法进行性能比较。


<details>
  <summary>Details</summary>
Motivation: 随着电网波动性增加，将电价预测转化为实际交易策略的需求日益迫切。虽然AEMO提供了丰富的预测数据，但这些数据在实际BESS交易决策中的应用价值尚未得到充分探索。

Method: 分析基于时间、预测周期和地区变化的预测准确性模式，创建基于预测的BESS交易模型，并与无预测的基本算法进行基准测试，同时探索机器学习技术来增强AEMO预测。

Result: 开发了一个新颖的基于预测的BESS交易模型，用于优化套利财务回报，并评估了预测驱动算法的性能表现。

Conclusion: 研究成果将为能源市场交易模型的未来改进提供信息，并促进BESS更有效地融入市场运营。

Abstract: In electricity markets around the world, the ability to anticipate price
movements with precision can be the difference between profit and loss,
especially for fast-acting assets like battery energy storage systems (BESS).
As grid volatility increases due to renewables and market decentralisation,
operators and forecasters alike face growing pressure to transform prediction
into strategy. Yet while forecast data is abundant, especially in advanced
markets like Australia's National Electricity Market (NEM), its practical value
in driving real-world BESS trading decisions remains largely unexplored. This
thesis dives into that gap. This work addresses a key research question: Can
the accuracy of the Australian Energy Market Operator (AEMO) energy price
forecasts be systematically leveraged to develop a reliable and profitable
battery energy storage system trading algorithm? Despite the availability of
AEMO price forecasts, no existing framework evaluates their reliability or
incorporates them into practical BESS trading strategies. By analysing patterns
in forecast accuracy based on time of day, forecast horizon, and regional
variations, this project creates a novel, forecast-informed BESS trading model
to optimise arbitrage financial returns. The performance of this
forecast-driven algorithm is benchmarked against a basic trading algorithm with
no knowledge of forecast data. The study further explores the potential of
machine learning techniques to predict future energy prices by enhancing AEMO
forecasts to govern a more advanced trading strategy. The research outcomes
will inform future improvements in energy market trading models and promote
more efficient BESS integration into market operations.

</details>


### [36] [StructPrune: Structured Global Pruning asymptotics with $\mathcal{O}(\sqrt{N})$ GPU Memory](https://arxiv.org/abs/2510.03246)
*Xinyuan Song,Guangji Bai,Liang Zhao*

Main category: cs.LG

TL;DR: 提出了STRUPRUNE框架，结合结构化剪枝的硬件效率和局部剪枝的内存效率，通过分治策略将全局剪枝问题分解为协调的子问题，在保持性能的同时大幅降低内存需求。


<details>
  <summary>Details</summary>
Motivation: 全局剪枝虽然性能好但内存需求高（O(N)），不适合十亿参数模型；局部剪枝内存效率高但忽略层间依赖，在高稀疏度下性能不佳；结构化剪枝硬件效率高但通常依赖全局剪枝。需要同时实现结构化剪枝和局部剪枝的内存效率。

Method: 采用分治策略将全局剪枝分解为跨模块的协调子问题；基于ADMM框架整合结构化稀疏性；推导结构化剪枝掩码的闭式解析解，提供层间稀疏度分配的显式规则；开发基于能量的渐进框架，产生softmax形式的分配方案。

Result: STRUPRUNE在匹配全局结构化剪枝困惑度的同时，将内存成本从O(N)降低到O(√N)，使得在十亿参数规模上的实际部署成为可能。

Conclusion: STRUPRUNE成功解决了结构化剪枝与内存效率之间的权衡问题，为大规模语言模型的实用部署提供了可行的剪枝方案。

Abstract: Pruning is critical for scaling large language models (LLMs). Global pruning
achieves strong performance but requires $\mathcal{O}(N)$ memory, which is
infeasible for billion-parameter models. Local pruning reduces GPU memory usage
to that of a single layer by pruning layers independently, but it neglects
inter-layer dependencies and often leads to suboptimal performance in
high-sparsity regimes. Unlike unstructured pruning, structured pruning produces
regular sparsity patterns that align well with GPU kernels and library
optimizations, making it more hardware-efficient. However, structured pruning
typically relies on global pruning, since structured patterns are more prone to
severe performance degradation under local optimization. To jointly achieve
structured pruning and the memory efficiency of local pruning, we propose a
divide-and-conquer strategy that decomposes the global pruning problem into
coordinated subproblems across different modules, each of which fits within
limited GPU memory. Building on this idea, we design \textbf{STRUPRUNE}, an
ADMM-based framework that integrates structured sparsity into the pruning
process, combining the memory efficiency of local pruning with the hardware
compatibility of structured methods. We derive a closed-form analytical
solution for structured pruning masks that provides an explicit rule for
layer-wise sparsity allocation, and further develop an energy-based asymptotic
framework yielding a softmax-form allocation scheme that simplifies
optimization while adapting to heterogeneous layer importance. Experiments
demonstrate that STRUPRUNE matches the perplexity of global structured pruning
while reducing memory cost from $\mathcal{O}(N)$ to $\mathcal{O}(\sqrt{N})$,
enabling practical deployment at the billion-parameter scale.

</details>


### [37] [HOFLON: Hybrid Offline Learning and Online Optimization for Process Start-Up and Grade-Transition Control](https://arxiv.org/abs/2510.03830)
*Alex Durkin,Jasper Stolte,Mehmet Mercangöz*

Main category: cs.LG

TL;DR: HOFLON（混合离线学习+在线优化）方法解决了连续过程工厂启动和产品等级转换中的自动化挑战，通过结合离线学习数据流形和Q值评估器，以及在线优化控制策略，超越了传统离线强化学习的局限。


<details>
  <summary>Details</summary>
Motivation: 传统工厂启动和产品等级转换依赖少数专家操作员的手动操作，但随着这些专家退休，工厂面临缺乏隐性知识的问题。离线强化学习虽然能挖掘历史数据，但面临分布偏移和价值高估的挑战。

Method: HOFLON采用混合方法：离线阶段学习数据流形和长时域Q值评估器；在线阶段通过单步优化最大化Q值，同时惩罚偏离学习流形和操纵变量变化率过大的行为。

Result: 在聚合反应器启动和造纸机等级转换两个工业案例中，HOFLON不仅超越了领先的离线RL算法IQL，平均累积奖励还超过了历史数据中观察到的最佳启动或等级转换。

Conclusion: HOFLON展示了超越当前专家能力的自动化过渡操作潜力，为连续过程工厂的启动和产品转换提供了有效的自动化解决方案。

Abstract: Start-ups and product grade-changes are critical steps in continuous-process
plant operation, because any misstep immediately affects product quality and
drives operational losses. These transitions have long relied on manual
operation by a handful of expert operators, but the progressive retirement of
that workforce is leaving plant owners without the tacit know-how needed to
execute them consistently. In the absence of a process model, offline
reinforcement learning (RL) promises to capture and even surpass human
expertise by mining historical start-up and grade-change logs, yet standard
offline RL struggles with distribution shift and value-overestimation whenever
a learned policy ventures outside the data envelope. We introduce HOFLON
(Hybrid Offline Learning + Online Optimization) to overcome those limitations.
Offline, HOFLON learns (i) a latent data manifold that represents the feasible
region spanned by past transitions and (ii) a long-horizon Q-critic that
predicts the cumulative reward from state-action pairs. Online, it solves a
one-step optimization problem that maximizes the Q-critic while penalizing
deviations from the learned manifold and excessive rates of change in the
manipulated variables. We test HOFLON on two industrial case studies: a
polymerization reactor start-up and a paper-machine grade-change problem, and
benchmark it against Implicit Q-Learning (IQL), a leading offline-RL algorithm.
In both plants HOFLON not only surpasses IQL but also delivers, on average,
better cumulative rewards than the best start-up or grade-change observed in
the historical data, demonstrating its potential to automate transition
operations beyond current expert capability.

</details>


### [38] [Towards Multimodal Active Learning: Efficient Learning with Limited Paired Data](https://arxiv.org/abs/2510.03247)
*Jiancheng Zhang,Yinglun Zhu*

Main category: cs.LG

TL;DR: 提出了首个针对未对齐多模态数据的主动学习框架，通过主动获取跨模态对齐而非标签来降低标注成本，在保持性能的同时减少高达40%的标注需求。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习算法主要关注单模态数据，忽视了多模态学习中标注对齐的高昂成本，特别是在CLIP和SigLIP等现代多模态管道中，单模态特征容易获取但高质量对齐成本很高。

Method: 开发了一种结合不确定性和多样性原则的模态感知算法，实现线性时间获取，可无缝应用于基于池和基于流的设置。

Result: 在基准数据集上的广泛实验表明，该方法能持续减少多模态标注成本同时保持性能，在ColorSwap数据集上可减少高达40%的标注需求而不损失准确性。

Conclusion: 该框架有效解决了多模态学习中的对齐标注瓶颈问题，为降低多模态标注成本提供了实用解决方案。

Abstract: Active learning (AL) is a principled strategy to reduce annotation cost in
data-hungry deep learning. However, existing AL algorithms focus almost
exclusively on unimodal data, overlooking the substantial annotation burden in
multimodal learning. We introduce the first framework for multimodal active
learning with unaligned data, where the learner must actively acquire
cross-modal alignments rather than labels on pre-aligned pairs. This setting
captures the practical bottleneck in modern multimodal pipelines such as CLIP
and SigLIP, where unimodal features are easy to obtain but high-quality
alignment is costly. We develop a new algorithm that combines uncertainty and
diversity principles in a modality-aware design, achieves linear-time
acquisition, and applies seamlessly to both pool-based and streaming-based
settings. Extensive experiments on benchmark datasets demonstrate that our
approach consistently reduces multimodal annotation cost while preserving
performance; for instance, on the ColorSwap dataset it cuts annotation
requirements by up to $40\%$ without loss in accuracy.

</details>


### [39] [Adaptive Federated Learning via Dynamical System Model](https://arxiv.org/abs/2510.04203)
*Aayushya Agarwal,Larry Pileggi,Gauri Joshi*

Main category: cs.LG

TL;DR: 提出了一种端到端的自适应联邦学习方法，通过将联邦学习建模为动态系统，自适应选择客户端和中央服务器的学习率和动量参数，无需手动调参即可实现快速稳定收敛。


<details>
  <summary>Details</summary>
Motivation: 解决异构联邦学习中超参数选择困难的问题，传统手动调参过程繁琐且计算成本高，特别是在客户端计算能力和数据分布非独立同分布的情况下。

Method: 将联邦学习建模为动态系统，借鉴数值模拟和物理设计原理，通过临界阻尼选择动量参数，根据数值模拟的精度要求自适应选择客户端和中央服务器的学习率，所有参数由单个全局超参数控制。

Result: 该方法能够处理异构联邦学习的关键挑战（如目标不一致性和客户端漂移），相比现有自适应方法具有更优越的收敛性能，且对全局超参数选择不敏感。

Conclusion: 提出的自适应联邦学习框架消除了客户端和服务器更新的超参数调优需求，适合快速原型设计和可扩展部署，在异构联邦学习中实现了快速收敛。

Abstract: Hyperparameter selection is critical for stable and efficient convergence of
heterogeneous federated learning, where clients differ in computational
capabilities, and data distributions are non-IID. Tuning hyperparameters is a
manual and computationally expensive process as the hyperparameter space grows
combinatorially with the number of clients. To address this, we introduce an
end-to-end adaptive federated learning method in which both clients and central
agents adaptively select their local learning rates and momentum parameters.
Our approach models federated learning as a dynamical system, allowing us to
draw on principles from numerical simulation and physical design. Through this
perspective, selecting momentum parameters equates to critically damping the
system for fast, stable convergence, while learning rates for clients and
central servers are adaptively selected to satisfy accuracy properties from
numerical simulation. The result is an adaptive, momentum-based federated
learning algorithm in which the learning rates for clients and servers are
dynamically adjusted and controlled by a single, global hyperparameter. By
designing a fully integrated solution for both adaptive client updates and
central agent aggregation, our method is capable of handling key challenges of
heterogeneous federated learning, including objective inconsistency and client
drift. Importantly, our approach achieves fast convergence while being
insensitive to the choice of the global hyperparameter, making it well-suited
for rapid prototyping and scalable deployment. Compared to state-of-the-art
adaptive methods, our framework is shown to deliver superior convergence for
heterogeneous federated learning while eliminating the need for hyperparameter
tuning both client and server updates.

</details>


### [40] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: 本研究评估了四种神经算子架构用于快速预测脑位移场，旨在实现创伤性脑损伤的实时建模。MG-FNO获得最高精度，F-FNO收敛最快，DeepONet推理速度最优，所有模型将计算时间从小时级降至毫秒级。


<details>
  <summary>Details</summary>
Motivation: 有限元模型虽然能高精度预测脑变形，但计算成本高昂，限制了其在临床快速决策中的应用。本研究旨在开发快速、患者特定的脑位移预测方法，以支持实时TBI建模。

Method: 将TBI建模构建为算子学习问题，使用四种神经算子架构（FNO、F-FNO、MG-FNO、DeepONet），在249个MRE数据集上进行训练和评估，输入包括解剖MRI、MRE刚度图和人口统计学特征。

Result: MG-FNO达到最高精度（MSE=0.0023，94.3%空间保真度），F-FNO收敛速度比标准FNO快2倍，DeepONet推理速度最快（14.5次迭代/秒），比MG-FNO快7倍。所有模型将计算时间从小时减少到毫秒级。

Conclusion: 神经算子提供了一种高效、分辨率不变的方法来预测脑变形，为实时患者特定TBI风险评估、临床分诊支持和防护设备优化打开了大门，展示了基于NO的人脑数字孪生的潜力。

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [41] [Benchmarking M-LTSF: Frequency and Noise-Based Evaluation of Multivariate Long Time Series Forecasting Models](https://arxiv.org/abs/2510.04900)
*Nick Janßen,Melanie Schaller,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 提出了一个基于模拟的评估框架，通过生成可参数化的合成数据集来系统评估多元长期时间序列预测模型的鲁棒性，揭示了不同模型在季节性模式、噪声类型和频率特性下的表现差异。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界数据集噪声特性未知，理解多元长期时间序列预测模型的鲁棒性具有挑战性，需要一种可控的评估方法来分析模型在不同信号和噪声条件下的表现。

Method: 开发了一个模拟评估框架，生成可配置的合成数据集，包含不同的信号组件、噪声类型、信噪比和频率特性，并对四种代表性架构（S-Mamba、iTransformer、R-Linear、Autoformer）进行基准测试。

Result: 所有模型在回看窗口无法捕捉完整季节性周期时性能严重下降；S-Mamba和Autoformer在锯齿波模式表现最佳，R-Linear和iTransformer在正弦信号表现更好；白噪声和布朗噪声普遍降低性能，S-Mamba对趋势噪声敏感，iTransformer对季节性噪声敏感；S-Mamba和iTransformer在频率重建方面表现优异。

Conclusion: 这种基于合成和原则驱动测试床的受控方法通过聚合MSE分数提供了对模型特定优势和局限性的深入洞察，为基于信号特性和噪声条件的模型选择提供了具体指导。

Abstract: Understanding the robustness of deep learning models for multivariate
long-term time series forecasting (M-LTSF) remains challenging, as evaluations
typically rely on real-world datasets with unknown noise properties. We propose
a simulation-based evaluation framework that generates parameterizable
synthetic datasets, where each dataset instance corresponds to a different
configuration of signal components, noise types, signal-to-noise ratios, and
frequency characteristics. These configurable components aim to model
real-world multivariate time series data without the ambiguity of unknown
noise. This framework enables fine-grained, systematic evaluation of M-LTSF
models under controlled and diverse scenarios. We benchmark four representative
architectures S-Mamba (state-space), iTransformer (transformer-based), R-Linear
(linear), and Autoformer (decomposition-based). Our analysis reveals that all
models degrade severely when lookback windows cannot capture complete periods
of seasonal patters in the data. S-Mamba and Autoformer perform best on
sawtooth patterns, while R-Linear and iTransformer favor sinusoidal signals.
White and Brownian noise universally degrade performance with lower
signal-to-noise ratio while S-Mamba shows specific trend-noise and iTransformer
shows seasonal-noise vulnerability. Further spectral analysis shows that
S-Mamba and iTransformer achieve superior frequency reconstruction. This
controlled approach, based on our synthetic and principle-driven testbed,
offers deeper insights into model-specific strengths and limitations through
the aggregation of MSE scores and provides concrete guidance for model
selection based on signal characteristics and noise conditions.

</details>


### [42] [Light Differentiable Logic Gate Networks](https://arxiv.org/abs/2510.03250)
*Lukas Rüttgers,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 论文提出了可微分逻辑门网络(DLGNs)的重新参数化方法，解决了梯度消失、离散化误差和高训练成本问题，同时减小了模型规模并加速了训练。


<details>
  <summary>Details</summary>
Motivation: 可微分逻辑门网络在推理时效率极高且保持竞争性准确率，但存在梯度消失、离散化误差和高训练成本问题，即使使用专门的参数初始化方案，增加深度仍会损害准确率。

Method: 提出对逻辑门神经元进行重新参数化，将每个门的参数规模按输入数量的对数缩小。对于二元输入，这已经将模型大小减少了4倍。

Result: 重新参数化使反向传播速度提升高达1.86倍，训练步数减少8.5倍，在CIFAR-100上的准确率保持稳定甚至优于原始参数化。

Conclusion: 重新参数化方法有效解决了DLGNs的缩放问题，同时减小了模型规模并加速了训练，而不损害准确率。

Abstract: Differentiable logic gate networks (DLGNs) exhibit extraordinary efficiency
at inference while sustaining competitive accuracy. But vanishing gradients,
discretization errors, and high training cost impede scaling these networks.
Even with dedicated parameter initialization schemes from subsequent works,
increasing depth still harms accuracy. We show that the root cause of these
issues lies in the underlying parametrization of logic gate neurons themselves.
To overcome this issue, we propose a reparametrization that also shrinks the
parameter size logarithmically in the number of inputs per gate. For binary
inputs, this already reduces the model size by 4x, speeds up the backward pass
by up to 1.86x, and converges in 8.5x fewer training steps. On top of that, we
show that the accuracy on CIFAR-100 remains stable and sometimes superior to
the original parametrization.

</details>


### [43] [Numerion: A Multi-Hypercomplex Model for Time Series Forecasting](https://arxiv.org/abs/2510.03251)
*Hanzhong Cao,Wenbo Yan,Ying Tan*

Main category: cs.LG

TL;DR: 提出Numerion模型，利用超复数空间自然分解时间序列，通过多维度RHR-MLP架构在不同维度空间中建模，并通过动态融合机制整合模式，在多个公开数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列预测方法通过复杂模型结构和先验知识进行分解，但受限于计算复杂性和假设的鲁棒性。研究发现超复数空间中时间序列的特征频率会自然降低。

Method: 提出Numerion模型，将线性层和激活函数推广到任意2的幂次维度的超复数空间，引入RHR-MLP架构，使用多个RHR-MLP将时间序列映射到不同维度的超复数空间进行自然分解和独立建模，并通过动态融合机制自适应融合不同空间中的潜在模式。

Result: 实验验证模型性能，在多个公开数据集上达到最先进结果。可视化和定量分析全面证明了多维RHR-MLP自然分解时间序列的能力，以及高维超复数空间倾向于捕获低频特征的趋势。

Conclusion: Numerion模型通过超复数空间实现了时间序列的自然分解和建模，提供了一种有效的时间序列预测方法，揭示了高维超复数空间在捕获低频特征方面的优势。

Abstract: Many methods aim to enhance time series forecasting by decomposing the series
through intricate model structures and prior knowledge, yet they are inevitably
limited by computational complexity and the robustness of the assumptions. Our
research uncovers that in the complex domain and higher-order hypercomplex
spaces, the characteristic frequencies of time series naturally decrease.
Leveraging this insight, we propose Numerion, a time series forecasting model
based on multiple hypercomplex spaces. Specifically, grounded in theoretical
support, we generalize linear layers and activation functions to hypercomplex
spaces of arbitrary power-of-two dimensions and introduce a novel
Real-Hypercomplex-Real Domain Multi-Layer Perceptron (RHR-MLP) architecture.
Numerion utilizes multiple RHR-MLPs to map time series into hypercomplex spaces
of varying dimensions, naturally decomposing and independently modeling the
series, and adaptively fuses the latent patterns exhibited in different spaces
through a dynamic fusion mechanism. Experiments validate the model`s
performance, achieving state-of-the-art results on multiple public datasets.
Visualizations and quantitative analyses comprehensively demonstrate the
ability of multi-dimensional RHR-MLPs to naturally decompose time series and
reveal the tendency of higher dimensional hypercomplex spaces to capture lower
frequency features.

</details>


### [44] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 提出了通用多域翻译(UMDT)框架，使用扩散路由器(DR)实现任意域对之间的翻译，仅需K-1个中心域配对数据集即可支持K个域的所有翻译任务。


<details>
  <summary>Details</summary>
Motivation: 现有多域翻译方法需要完全对齐的元组或只能处理训练中见过的域对，限制了实用性并排除了许多跨域映射。

Method: 提出扩散路由器(DR)，基于扩散的统一框架，使用单一噪声预测器建模所有中心域与非中心域之间的翻译，通过中心域路由实现间接非中心翻译，并引入变分边界目标和Tweedie精炼支持直接映射。

Result: 在三个大规模UMDT基准测试中取得最先进结果，同时降低采样成本，解锁了草图↔分割等新任务。

Conclusion: DR被证明是跨多个域进行通用翻译的可扩展且多功能的框架。

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [45] [Solving the Granularity Mismatch: Hierarchical Preference Learning for Long-Horizon LLM Agents](https://arxiv.org/abs/2510.03253)
*Heyang Gao,Zexu Sun,Erxue Min,Hengyi Cai,Shuaiqiang Wang,Dawei Yin,Xu Chen*

Main category: cs.LG

TL;DR: HPL是一个分层偏好学习框架，通过多粒度偏好信号优化LLM智能体，解决了轨迹级偏好学习信号过于粗糙而步级偏好学习过于短视的问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在复杂长程任务中的偏好学习粒度不匹配问题——轨迹级DPO信号过于粗糙，步级DPO过于短视，无法准确评估多步行为价值。

Method: 提出分层偏好学习(HPL)框架，包含轨迹级、步级和组级DPO，核心创新是双层级课程：将专家轨迹分解为语义连贯的动作组，生成对比次优组进行细粒度偏好学习，并通过课程调度器按组长度和样本难度组织学习过程。

Result: 在三个具有挑战性的智能体基准测试中，HPL优于现有最先进方法。分析表明分层DPO损失有效整合了多粒度偏好信号，双层级课程对解决从简单行为到复杂多步序列的广泛任务至关重要。

Conclusion: HPL通过分层偏好学习和双层级课程调度，成功解决了LLM智能体偏好学习的粒度不匹配问题，在复杂长程任务中表现出优越性能。

Abstract: Large Language Models (LLMs) as autonomous agents are increasingly tasked
with solving complex, long-horizon problems. Aligning these agents via
preference-based offline methods like Direct Preference Optimization (DPO) is a
promising direction, yet it faces a critical granularity mismatch.
Trajectory-level DPO provides a signal that is too coarse for precise credit
assignment, while step-level DPO is often too myopic to capture the value of
multi-step behaviors. To resolve this challenge, we introduce Hierarchical
Preference Learning (HPL), a hierarchical framework that optimizes LLM agents
by leveraging preference signals at multiple, synergistic granularities. While
HPL incorporates trajectory- and step-level DPO for global and local policy
stability, its core innovation lies in group-level preference optimization
guided by a dual-layer curriculum. Our approach first decomposes expert
trajectories into semantically coherent action groups and then generates
contrasting suboptimal groups to enable preference learning at a fine-grained,
sub-task level. Then, instead of treating all preference pairs equally, HPL
introduces a curriculum scheduler that organizes the learning process from
simple to complex. This curriculum is structured along two axes: the group
length, representing sub-task complexity, and the sample difficulty, defined by
the reward gap between preferred and dispreferred action groups. Experiments on
three challenging agent benchmarks show that HPL outperforms existing
state-of-the-art methods. Our analyses demonstrate that the hierarchical DPO
loss effectively integrates preference signals across multiple granularities,
while the dual-layer curriculum is crucial for enabling the agent to solve a
wide range of tasks, from simple behaviors to complex multi-step sequences.

</details>


### [46] [Adversarial training with restricted data manipulation](https://arxiv.org/abs/2510.03254)
*David Benfield,Stefano Coniglio,Phan Tu Vuong,Alain Zemkoho*

Main category: cs.LG

TL;DR: 提出了一种约束悲观双层优化模型，通过限制对手的移动来更好地反映现实，实验表明该模型平均性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有悲观双层优化方法中的对手不受限制，可能导致模型过于悲观和不现实，当对手数据变得无意义时，无法正确反映现实，导致在实际数据上分类器性能不佳。

Method: 构建约束悲观双层优化模型，限制对手的移动，识别更符合现实的解决方案。

Result: 通过实验证明，该模型在平均性能上优于现有方法。

Conclusion: 约束悲观双层优化能够更好地模拟现实对抗场景，提高分类器在实际数据上的性能表现。

Abstract: Adversarial machine learning concerns situations in which learners face
attacks from active adversaries. Such scenarios arise in applications such as
spam email filtering, malware detection and fake image generation, where
security methods must be actively updated to keep up with the everimproving
generation of malicious data. Pessimistic Bilevel optimisation has been shown
to be an effective method of training resilient classifiers against such
adversaries. By modelling these scenarios as a game between the learner and the
adversary, we anticipate how the adversary will modify their data and then
train a resilient classifier accordingly. However, since existing pessimistic
bilevel approaches feature an unrestricted adversary, the model is vulnerable
to becoming overly pessimistic and unrealistic. When finding the optimal
solution that defeats the classifier, it is possible that the adversary's data
becomes nonsensical and loses its intended nature. Such an adversary will not
properly reflect reality, and consequently, will lead to poor classifier
performance when implemented on real-world data. By constructing a constrained
pessimistic bilevel optimisation model, we restrict the adversary's movements
and identify a solution that better reflects reality. We demonstrate through
experiments that this model performs, on average, better than the existing
approach.

</details>


### [47] [SciTS: Scientific Time Series Understanding and Generation with LLMs](https://arxiv.org/abs/2510.03255)
*Wen Wu,Ziyang Zhang,Liwei Liu,Xuenan Xu,Junlin Liu,Ke Fan,Qitan Lv,Jimin Zhuang,Chen Zhang,Zheqi Yuan,Siyuan Hou,Tianyi Lin,Kai Chen,Bowen Zhou,Chao Zhang*

Main category: cs.LG

TL;DR: 提出了SciTS基准测试和TimeOmni框架，用于评估和改进大语言模型在科学时间序列数据上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型对时间序列数据的处理存在不足，要么将数值序列编码为文本，要么转换为图像，这无法满足科学时间序列的全面理解和生成需求。

Method: 引入SciTS基准测试，涵盖12个科学领域和43个任务，包含5万多个实例；提出TimeOmni框架，使大语言模型能够理解和生成时间序列数据。

Result: 基准测试17个模型发现，通用大语言模型比专门的时间序列模型具有更强的泛化能力，但将时间序列表示为文本或图像会因序列过长或数值精度损失而限制性能。

Conclusion: 这项工作填补了科学时间序列专用基准测试和建模框架的空白，为大语言模型理解和生成复杂时间科学数据铺平了道路。

Abstract: The scientific reasoning ability of large language models (LLMs) has recently
attracted significant attention. Time series, as a fundamental modality in
scientific data, presents unique challenges that are often overlooked in
current multimodal LLMs, which either encode numerical sequences as text or
convert them into images. Such approaches may be insufficient for comprehensive
scientific time series understanding and generation. Existing unified time
series models typically specialise in either forecasting or analysis, and their
effectiveness on non-periodic, heterogeneous scientific signals remains
unclear. To address these gaps, we introduce SciTS, a benchmark spanning 12
scientific domains and 43 tasks, with over 50k+ instances, both univariate and
multivariate signals ranging from $10^0$ to $10^7$ in length and up to 10~MHz
in frequency. We benchmark 17 models, including text-only LLMs, multimodal
LLMs, and unified time series models, and find that general-purpose LLMs
exhibit stronger generalisability than specialised time series models, while
representing time series as text or images limits their performance due to
excessively long sequences and loss of numerical precision, respectively. We
then introduce TimeOmni, a framework that equips LLMs with the ability to
understand and generate time series while remaining compatible with
general-purpose LLM training. This work fills a gap in both dedicated
benchmarks and modelling frameworks for scientific time series, paving the way
for LLMs to understand and generate complex temporal scientific data.

</details>


### [48] [Triple-BERT: Do We Really Need MARL for Order Dispatch on Ride-Sharing Platforms?](https://arxiv.org/abs/2510.03257)
*Zijian Zhao,Sen Li*

Main category: cs.LG

TL;DR: 提出了Triple-BERT方法，一种基于TD3的集中式单智能体强化学习算法，用于解决网约车平台大规模订单调度问题，通过动作分解和BERT网络处理高维动作和观测空间。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法在网约车订单调度中存在局限性：独立MARL无法捕捉全局信息和智能体协作，CTDE方法面临维度灾难问题。需要设计能够有效处理大规模司机和订单的集中式方法。

Method: 基于TD3变体构建集中式单智能体强化学习框架，采用动作分解策略将联合动作概率分解为单个司机动作概率，引入BERT网络通过参数复用和注意力机制处理大规模观测空间。

Result: 在曼哈顿真实网约车数据集上验证，相比现有最优方法提升11.95%，服务订单数增加4.26%，接驾时间减少22.25%。

Conclusion: Triple-BERT通过集中式单智能体强化学习框架，有效解决了大规模网约车订单调度中的高维动作和观测空间挑战，显著提升了调度性能。

Abstract: On-demand ride-sharing platforms, such as Uber and Lyft, face the intricate
real-time challenge of bundling and matching passengers-each with distinct
origins and destinations-to available vehicles, all while navigating
significant system uncertainties. Due to the extensive observation space
arising from the large number of drivers and orders, order dispatching, though
fundamentally a centralized task, is often addressed using Multi-Agent
Reinforcement Learning (MARL). However, independent MARL methods fail to
capture global information and exhibit poor cooperation among workers, while
Centralized Training Decentralized Execution (CTDE) MARL methods suffer from
the curse of dimensionality. To overcome these challenges, we propose
Triple-BERT, a centralized Single Agent Reinforcement Learning (MARL) method
designed specifically for large-scale order dispatching on ride-sharing
platforms. Built on a variant TD3, our approach addresses the vast action space
through an action decomposition strategy that breaks down the joint action
probability into individual driver action probabilities. To handle the
extensive observation space, we introduce a novel BERT-based network, where
parameter reuse mitigates parameter growth as the number of drivers and orders
increases, and the attention mechanism effectively captures the complex
relationships among the large pool of driver and orders. We validate our method
using a real-world ride-hailing dataset from Manhattan. Triple-BERT achieves
approximately an 11.95% improvement over current state-of-the-art methods, with
a 4.26% increase in served orders and a 22.25% reduction in pickup times. Our
code, trained model parameters, and processed data are publicly available at
the repository https://github.com/RS2002/Triple-BERT .

</details>


### [49] [POEM: Explore Unexplored Reliable Samples to Enhance Test-Time Adaptation](https://arxiv.org/abs/2510.03258)
*Chang'an Yi,Xiaohui Deng,Shuaicheng Niu,Yan Zhou*

Main category: cs.LG

TL;DR: 提出POEM方法，通过探索先前未被利用的可靠样本来改进测试时自适应(TTA)，解决现有方法依赖熵阈值导致可靠样本被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法依赖熵作为置信度指标，对预定义熵阈值敏感，导致许多潜在可靠的测试样本被忽视和未充分利用。这些样本虽然初始熵值略高于阈值，但在模型更新后可能变得可靠，能够提供稳定的监督信息和正常范围的梯度来指导模型适应。

Method: 提出POEM方法，探索先前未被利用的可靠样本进行TTA。引入额外的Adapt Branch网络，在提取领域无关表示和在目标数据上实现高性能之间取得平衡。

Result: 在多种架构上的综合实验表明，POEM在挑战性场景和真实世界领域偏移中始终优于现有TTA方法，同时保持计算效率。POEM的核心思想可作为增强策略提升现有TTA方法的性能。

Conclusion: POEM通过有效利用先前被忽视的可靠样本，显著提升了测试时自适应的性能，为TTA方法提供了新的改进方向。

Abstract: Test-time adaptation (TTA) aims to transfer knowledge from a source model to
unknown test data with potential distribution shifts in an online manner. Many
existing TTA methods rely on entropy as a confidence metric to optimize the
model. However, these approaches are sensitive to the predefined entropy
threshold, influencing which samples are chosen for model adaptation.
Consequently, potentially reliable target samples are often overlooked and
underutilized. For instance, a sample's entropy might slightly exceed the
threshold initially, but fall below it after the model is updated. Such samples
can provide stable supervised information and offer a normal range of gradients
to guide model adaptation. In this paper, we propose a general approach,
\underline{POEM}, to promote TTA via ex\underline{\textbf{p}}loring the
previously unexpl\underline{\textbf{o}}red reliabl\underline{\textbf{e}}
sa\underline{\textbf{m}}ples. Additionally, we introduce an extra Adapt Branch
network to strike a balance between extracting domain-agnostic representations
and achieving high performance on target data. Comprehensive experiments across
multiple architectures demonstrate that POEM consistently outperforms existing
TTA methods in both challenging scenarios and real-world domain shifts, while
remaining computationally efficient. The effectiveness of POEM is evaluated
through extensive analyses and thorough ablation studies. Moreover, the core
idea behind POEM can be employed as an augmentation strategy to boost the
performance of existing TTA approaches. The source code is publicly available
at \emph{https://github.com/ycarobot/POEM}

</details>


### [50] [Meta-Awareness Enhances Reasoning Models: Self-Alignment Reinforcement Learning](https://arxiv.org/abs/2510.03259)
*Yoonjeon Kim,Doohyuk Jang,Eunho Yang*

Main category: cs.LG

TL;DR: 提出MASA方法，通过自对齐增强语言模型的元认知能力，证明元认知对齐能显著提升推理性能，在数学和科学推理任务上取得显著效果


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型缺乏元认知能力，即知道自己如何思考的能力，这导致真实推理过程与元预测之间存在严重不对齐

Method: 设计MASA训练流程，通过自生成信号训练元认知，无需外部训练数据。方法包括：过滤零方差提示、截断不可能成功的冗长推理链

Result: 在领域内任务上显著提升准确率和训练效率，GRPO训练加速1.28倍，AIME25准确率提升19.3%，6个数学基准平均提升6.2%。在13个跨领域基准上获得2.08%平均提升

Conclusion: 元认知对齐能有效提升推理模型的性能和泛化能力，MASA方法证明通过自对齐增强元认知是可行且高效的途径

Abstract: Recent studies on reasoning models explore the meta-awareness of language
models, the ability to know how to think by itself. We argue that large
reasoning models lack this meta-awareness property by proving severe
misalignment between true rollouts and predicted meta information. We posit
that aligning meta-prediction with true rollouts will lead to significant
performance gains. To verify this hypothesis, we design a training pipeline
that boosts Meta-Awareness via Self-Alignment (MASA), and prove that enhanced
meta-awareness directly translates to improved accuracy. Unlike existing
meta-cognitive reasoning models, our method does not require external training
sources but leverages self-generated signals to train meta-awareness. Moreover,
our method enables efficient training by i) filtering out zero-variance prompts
that are either trivial or unsolvable and ii) cutting off lengthy rollouts when
they are unlikely to lead to correct answers. The results are inspiring: our
strategy yields significant improvements in both accuracy and training
efficiency on in-domain tasks and shows strong generalization to out-of-domain
benchmarks. More specifically, our method can speed up GRPO training by over
1.28x to reach the same performance, and achieve a 19.3% gain in accuracy on
AIME25, and a 6.2 % average gain over six mathematics benchmarks. Training with
meta-cognitive guidance enhances out-of-domain generalization, giving a 3.87 %
boost on GPQA-Diamond and a 2.08 % overall accuracy gain across 13 benchmarks
spanning logical, scientific, and coding domains.

</details>


### [51] [Semantic-Inductive Attribute Selection for Zero-Shot Learning](https://arxiv.org/abs/2510.03260)
*Juan Jose Herrera-Aranda,Guillermo Gomez-Trenado,Francisco Herrera,Isaac Triguero*

Main category: cs.LG

TL;DR: 提出了两种互补的特征选择方法来优化零样本学习中的语义空间，通过减少冗余属性来提高未见类的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 零样本学习中的语义空间通常包含噪声、冗余或不相关属性，这会阻碍模型性能。需要在不访问未见类语义信息的情况下评估属性相关性。

Method: 引入分区方案模拟未见条件，研究两种特征选择策略：基于嵌入的特征选择（RFS）和基于进化计算的特征选择（GA）。

Result: 在五个基准数据集上的实验表明，两种方法都能通过减少冗余持续提高未见类的准确率，RFS高效但依赖超参数，GA成本更高但搜索更广泛。

Conclusion: 语义空间本质上是冗余的，提出的分区方案是在归纳条件下精炼语义空间的有效工具。

Abstract: Zero-Shot Learning is an important paradigm within General-Purpose Artificial
Intelligence Systems, particularly in those that operate in open-world
scenarios where systems must adapt to new tasks dynamically. Semantic spaces
play a pivotal role as they bridge seen and unseen classes, but whether
human-annotated or generated by a machine learning model, they often contain
noisy, redundant, or irrelevant attributes that hinder performance. To address
this, we introduce a partitioning scheme that simulates unseen conditions in an
inductive setting (which is the most challenging), allowing attribute relevance
to be assessed without access to semantic information from unseen classes.
Within this framework, we study two complementary feature-selection strategies
and assess their generalisation. The first adapts embedded feature selection to
the particular demands of ZSL, turning model-driven rankings into meaningful
semantic pruning; the second leverages evolutionary computation to directly
explore the space of attribute subsets more broadly. Experiments on five
benchmark datasets (AWA2, CUB, SUN, aPY, FLO) show that both methods
consistently improve accuracy on unseen classes by reducing redundancy, but in
complementary ways: RFS is efficient and competitive though dependent on
critical hyperparameters, whereas GA is more costly yet explores the search
space more broadly and avoids such dependence. These results confirm that
semantic spaces are inherently redundant and highlight the proposed
partitioning scheme as an effective tool to refine them under inductive
conditions.

</details>


### [52] [Data-Driven Temperature Modelling of Machine Tools by Neural Networks: A Benchmark](https://arxiv.org/abs/2510.03261)
*C. Coelho,M. Hohmann,D. Fernández,L. Penter,S. Ihlenfeldt,O. Niggemann*

Main category: cs.LG

TL;DR: 提出了一种新的机器学习范式，通过神经网络预测机床内部的高保真温度和热通量场，为灵活通用的热误差校正奠定基础。


<details>
  <summary>Details</summary>
Motivation: 传统热误差校正方法依赖于特定误差类型、空间位置或机床配置，限制了通用性和适应性。需要一种更灵活通用的热误差补偿方法。

Method: 使用有限元方法获取数据训练神经网络预测温度和热通量场，采用基于相关性的测量点选择策略减少硬件需求，并比较了多种时间序列神经网络架构。

Result: 实现了对温度和热通量场的准确低成本预测，为机床环境中的灵活通用热误差校正提供了基础。

Conclusion: 该框架能够通过模块化下游组件计算和校正多种误差类型，提高了热误差补偿的通用性和适应性。

Abstract: Thermal errors in machine tools significantly impact machining precision and
productivity. Traditional thermal error correction/compensation methods rely on
measured temperature-deformation fields or on transfer functions. Most existing
data-driven compensation strategies employ neural networks (NNs) to directly
predict thermal errors or specific compensation values. While effective, these
approaches are tightly bound to particular error types, spatial locations, or
machine configurations, limiting their generality and adaptability. In this
work, we introduce a novel paradigm in which NNs are trained to predict
high-fidelity temperature and heat flux fields within the machine tool. The
proposed framework enables subsequent computation and correction of a wide
range of error types using modular, swappable downstream components. The NN is
trained using data obtained with the finite element method under varying
initial conditions and incorporates a correlation-based selection strategy that
identifies the most informative measurement points, minimising hardware
requirements during inference. We further benchmark state-of-the-art
time-series NN architectures, namely Recurrent NN, Gated Recurrent Unit,
Long-Short Term Memory (LSTM), Bidirectional LSTM, Transformer, and Temporal
Convolutional Network, by training both specialised models, tailored for
specific initial conditions, and general models, capable of extrapolating to
unseen scenarios. The results show accurate and low-cost prediction of
temperature and heat flux fields, laying the basis for enabling flexible and
generalisable thermal error correction in machine tool environments.

</details>


### [53] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出正交蒙特卡洛Dropout方法，在合并LoRA模块时强制正交性以避免语义向量干扰，但实证发现正交性本身不足以实现语义解耦和组合性。


<details>
  <summary>Details</summary>
Motivation: LoRA模块合并时语义向量会相互干扰，需要一种机制来保证合并后的正交性以避免直接干扰。

Method: 正交蒙特卡洛Dropout，在不增加时间复杂度的前提下强制稀疏语义向量组合时的严格正交性。

Result: 理论上和运行时都能保证合并LoRA的正交性，但实证分析显示正交性并不能带来语义解耦或组合性。

Conclusion: 仅靠LoRA间的正交性可能不足以实现真正的语义组合性，需要重新审视其在适配器合并中的作用。

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [54] [Memory Self-Regeneration: Uncovering Hidden Knowledge in Unlearned Models](https://arxiv.org/abs/2510.03263)
*Agnieszka Polowczyk,Alicja Polowczyk,Joanna Waczyńska,Piotr Borycki,Przemysław Spurek*

Main category: cs.LG

TL;DR: 本文探讨文本到图像模型的机器遗忘问题，提出记忆自我再生任务和MemoRa策略来恢复被遗忘的知识，并发现遗忘存在短期和长期两种模式。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像模型生成逼真图像的能力可能被滥用于创建有害内容，这推动了机器遗忘技术的发展，但实际遗忘特定概念非常困难，模型仍能通过对抗提示生成本应被遗忘的概念。

Method: 引入记忆自我再生任务，提出MemoRa策略作为再生方法支持有效恢复先前丢失的知识，并强调知识检索鲁棒性作为重要评估指标。

Result: 研究表明遗忘以两种不同方式发生：短期遗忘可以快速回忆，而长期遗忘恢复更具挑战性。

Conclusion: 知识检索鲁棒性是开发更强大有效遗忘技术的关键但未充分探索的评估维度，遗忘过程存在短期和长期两种模式。

Abstract: The impressive capability of modern text-to-image models to generate
realistic visuals has come with a serious drawback: they can be misused to
create harmful, deceptive or unlawful content. This has accelerated the push
for machine unlearning. This new field seeks to selectively remove specific
knowledge from a model's training data without causing a drop in its overall
performance. However, it turns out that actually forgetting a given concept is
an extremely difficult task. Models exposed to attacks using adversarial
prompts show the ability to generate so-called unlearned concepts, which can be
not only harmful but also illegal. In this paper, we present considerations
regarding the ability of models to forget and recall knowledge, introducing the
Memory Self-Regeneration task. Furthermore, we present MemoRa strategy, which
we consider to be a regenerative approach supporting the effective recovery of
previously lost knowledge. Moreover, we propose that robustness in knowledge
retrieval is a crucial yet underexplored evaluation measure for developing more
robust and effective unlearning techniques. Finally, we demonstrate that
forgetting occurs in two distinct ways: short-term, where concepts can be
quickly recalled, and long-term, where recovery is more challenging.

</details>


### [55] [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://arxiv.org/abs/2510.03264)
*Syeda Nahida Akter,Shrimai Prabhumoye,Eric Nyberg,Mostofa Patwary,Mohammad Shoeybi,Yejin Choi,Bryan Catanzaro*

Main category: cs.LG

TL;DR: 本研究首次系统分析了推理数据在预训练和后训练不同阶段的引入效果，发现预训练阶段引入推理数据至关重要（平均提升19%），能建立后期SFT无法完全复制的基础能力。研究揭示了数据分配的不对称原则：预训练最受益于推理模式的多样性，而SFT对数据质量更敏感。


<details>
  <summary>Details</summary>
Motivation: 当前增强LLM推理能力的主流方法是在后训练阶段使用高质量推理数据，但推理数据在预训练阶段的作用尚不明确。由于前沿模型的预训练语料不透明，不同阶段引入推理数据的效果在科学文献中较少报道。

Method: 系统研究推理数据（规模、多样性和质量变化）在不同训练阶段引入对LLM性能的影响，对比预训练阶段和后训练阶段引入推理数据的效果差异。

Result: 预训练阶段引入推理数据能带来19%的平均性能提升，建立的基础能力无法被后期SFT完全复制。预训练最受益于推理模式多样性（11%平均增益），SFT对数据质量更敏感（15%平均增益）。高质量预训练数据具有潜在效应，仅在SFT后被激活。

Conclusion: 研究结果挑战了语言建模与推理的传统分离，为在整个训练流程中战略性分配数据以构建更强大模型提供了原则性指导。预训练阶段引入推理数据对建立基础推理能力至关重要。

Abstract: The prevailing paradigm for enhancing the reasoning abilities of LLMs
revolves around post-training on high-quality, reasoning-intensive data. While
emerging literature suggests that reasoning data is increasingly incorporated
also during the mid-training stage-a practice that is relatively more
proprietary and less openly characterized-the role of such data in pretraining
remains unclear. In particular, due to the opaqueness of pretraining corpora in
most frontier models, the effect of reasoning data introduced at different
phases of pre- and/or post-training is relatively less reported in the
scientific literature. This raises several important questions: Is adding
reasoning data earlier during pretraining any better than introducing it during
post-training? Could earlier inclusion risk overfitting and harm
generalization, or instead establish durable foundations that later fine-tuning
cannot recover? We conduct the first systematic study of how reasoning
data-varying in scale, diversity, and quality-affects LLM performance when
introduced at different stages of training. We find that front-loading
reasoning data into pretraining is critical (19% avg gain), establishing
foundational capabilities that cannot be fully replicated by later-stage SFT,
even with more data. We uncover an asymmetric principle for optimal data
allocation: pretraining benefits most from broad diversity in reasoning
patterns (11% avg gain), while SFT is more sensitive to data quality (15% avg
gain). We show that high-quality pretraining data has latent effects, activated
only after SFT, and that naively scaling SFT data can be detrimental, washing
away the benefits of early reasoning injection. Our results challenge the
conventional separation of language modeling and reasoning, providing a
principled guide for strategically allocating data across the entire training
pipeline to build more capable models.

</details>


### [56] [MindCraft: How Concept Trees Take Shape In Deep Models](https://arxiv.org/abs/2510.03265)
*Bowei Tian,Yexiao He,Wanghao Ye,Ziyao Wang,Meng Liu,Ang Li*

Main category: cs.LG

TL;DR: 提出了MindCraft框架和概念树方法，通过谱分解和概念路径重建概念层次结构，揭示概念从共享表示到线性可分子空间的分化过程。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在语言、视觉和推理任务中表现出色，但其内部如何组织和稳定概念结构仍不清楚。

Method: 基于因果推断构建MindCraft框架，应用谱分解在每一层，将主方向连接成分支概念路径，形成概念树。

Result: 在医学诊断、物理推理和政治决策等多个领域的实证评估表明，概念树能够恢复语义层次、分离潜在概念，并具有广泛适用性。

Conclusion: 概念树建立了一个广泛适用且强大的框架，能够深入分析深度模型中的概念表示，是迈向可解释AI的重要一步。

Abstract: Large-scale foundation models demonstrate strong performance across language,
vision, and reasoning tasks. However, how they internally structure and
stabilize concepts remains elusive. Inspired by causal inference, we introduce
the MindCraft framework built upon Concept Trees. By applying spectral
decomposition at each layer and linking principal directions into branching
Concept Paths, Concept Trees reconstruct the hierarchical emergence of
concepts, revealing exactly when they diverge from shared representations into
linearly separable subspaces. Empirical evaluations across diverse scenarios
across disciplines, including medical diagnosis, physics reasoning, and
political decision-making, show that Concept Trees recover semantic
hierarchies, disentangle latent concepts, and can be widely applied across
multiple domains. The Concept Tree establishes a widely applicable and powerful
framework that enables in-depth analysis of conceptual representations in deep
models, marking a significant step forward in the foundation of interpretable
AI.

</details>


### [57] [Machine Learning Workflows in Climate Modeling: Design Patterns and Insights from Case Studies](https://arxiv.org/abs/2510.03305)
*Tian Zheng,Subashree Venkatasubramanian,Shuolin Li,Amy Braverman,Xinyi Ke,Zhewen Hou,Peter Jin,Samarth Sanjay Agrawal*

Main category: cs.LG

TL;DR: 本文分析机器学习在气候建模中的应用案例，重点关注工作流设计模式，包括替代建模、ML参数化、概率编程等，旨在为科学机器学习提供严谨框架。


<details>
  <summary>Details</summary>
Motivation: 解决气候建模中物理一致性、多尺度耦合、数据稀疏性等挑战，促进数据科学与气候建模的跨学科合作。

Method: 通过分析一系列应用机器学习研究案例，综合不同项目中的工作流设计模式，关注物理知识基础、模拟数据信息和观测集成。

Result: 提出了确保科学机器学习严谨性的框架，包括透明模型开发、关键评估、知情适应和可重复性。

Conclusion: 为机器学习在气候建模中的应用提供了系统化的工作流设计模式，有助于降低跨学科合作门槛，推动该领域的发展。

Abstract: Machine learning has been increasingly applied in climate modeling on system
emulation acceleration, data-driven parameter inference, forecasting, and
knowledge discovery, addressing challenges such as physical consistency,
multi-scale coupling, data sparsity, robust generalization, and integration
with scientific workflows. This paper analyzes a series of case studies from
applied machine learning research in climate modeling, with a focus on design
choices and workflow structure. Rather than reviewing technical details, we aim
to synthesize workflow design patterns across diverse projects in ML-enabled
climate modeling: from surrogate modeling, ML parameterization, probabilistic
programming, to simulation-based inference, and physics-informed transfer
learning. We unpack how these workflows are grounded in physical knowledge,
informed by simulation data, and designed to integrate observations. We aim to
offer a framework for ensuring rigor in scientific machine learning through
more transparent model development, critical evaluation, informed adaptation,
and reproducibility, and to contribute to lowering the barrier for
interdisciplinary collaboration at the interface of data science and climate
modeling.

</details>


### [58] [Variational Autoencoders-based Detection of Extremes in Plant Productivity in an Earth System Model](https://arxiv.org/abs/2510.03266)
*Bharat Sharma,Jitendra Kumar*

Main category: cs.LG

TL;DR: 本研究将变分自编码器(VAE)应用于检测美国大陆四个AR6区域的GPP极端事件，与传统奇异谱分析(SSA)方法进行比较，结果显示VAE在性能相当的同时具有计算优势和捕捉非线性时间依赖性的能力。


<details>
  <summary>Details</summary>
Motivation: 气候异常显著影响陆地碳循环动态，需要稳健的方法来检测和分析植物生产力的异常行为。

Method: 使用变分自编码器(VAE)架构，包含三个密集层和潜在空间，输入序列长度为12个月，通过重建误差识别GPP异常，并与传统SSA方法进行比较。

Result: VAE和SSA方法在极端事件频率的空间模式上表现出强区域一致性，但VAE产生更高的阈值。两种方法都显示到2050-80年负碳循环极端事件的幅度和频率增加，特别是在西部和中部北美地区。

Conclusion: VAE方法在性能上与成熟的SSA技术相当，同时提供计算优势并增强了对碳循环变异中非线性时间依赖性的捕捉能力，且无需预先定义数据的周期性。

Abstract: Climate anomalies significantly impact terrestrial carbon cycle dynamics,
necessitating robust methods for detecting and analyzing anomalous behavior in
plant productivity. This study presents a novel application of variational
autoencoders (VAE) for identifying extreme events in gross primary productivity
(GPP) from Community Earth System Model version 2 simulations across four AR6
regions in the Continental United States. We compare VAE-based anomaly
detection with traditional singular spectral analysis (SSA) methods across
three time periods: 1850-80, 1950-80, and 2050-80 under the SSP585 scenario.
The VAE architecture employs three dense layers and a latent space with an
input sequence length of 12 months, trained on a normalized GPP time series to
reconstruct the GPP and identifying anomalies based on reconstruction errors.
Extreme events are defined using 5th percentile thresholds applied to both VAE
and SSA anomalies. Results demonstrate strong regional agreement between VAE
and SSA methods in spatial patterns of extreme event frequencies, despite VAE
producing higher threshold values (179-756 GgC for VAE vs. 100-784 GgC for SSA
across regions and periods). Both methods reveal increasing magnitudes and
frequencies of negative carbon cycle extremes toward 2050-80, particularly in
Western and Central North America. The VAE approach shows comparable
performance to established SSA techniques, while offering computational
advantages and enhanced capability for capturing non-linear temporal
dependencies in carbon cycle variability. Unlike SSA, the VAE method does not
require one to define the periodicity of the signals in the data; it discovers
them from the data.

</details>


### [59] [Estimating link level traffic emissions: enhancing MOVES with open-source data](https://arxiv.org/abs/2510.03362)
*Lijiao Wang,Muhammad Usama,Haris N. Koutsopoulos,Zhengbing He*

Main category: cs.LG

TL;DR: 提出一个结合MOVES模型和开源数据的框架，用于估算车辆运行模式分布和交通排放，相比MOVES基准模型将关键污染物排放估算的RMSE降低了50%以上。


<details>
  <summary>Details</summary>
Motivation: 利用开源数据为城市区域的车辆活动和排放估算提供可扩展且透明的基础，实现低成本、可复制的排放估算。

Method: 整合MOVES模型、GPS轨迹数据、OpenStreetMap道路网络、区域交通数据集和卫星图像特征向量，训练神经网络模型预测MOVES定义的运行模式分布。

Result: 在波士顿大都会区45个城市应用该方法，相比MOVES基准模型，关键污染物（CO、NOx、CO2、PM2.5）的区域尺度交通排放RMSE降低了50%以上。

Conclusion: 证明了使用完全开源数据源进行低成本、可复制和数据驱动的排放估算的可行性。

Abstract: Open-source data offers a scalable and transparent foundation for estimating
vehicle activity and emissions in urban regions. In this study, we propose a
data-driven framework that integrates MOVES and open-source GPS trajectory
data, OpenStreetMap (OSM) road networks, regional traffic datasets and
satellite imagery-derived feature vectors to estimate the link level operating
mode distribution and traffic emissions. A neural network model is trained to
predict the distribution of MOVES-defined operating modes using only features
derived from readily available data. The proposed methodology was applied using
open-source data related to 45 municipalities in the Boston Metropolitan area.
The "ground truth" operating mode distribution was established using OSM
open-source GPS trajectories. Compared to the MOVES baseline, the proposed
model reduces RMSE by over 50% for regional scale traffic emissions of key
pollutants including CO, NOx, CO2, and PM2.5. This study demonstrates the
feasibility of low-cost, replicable, and data-driven emissions estimation using
fully open data sources.

</details>


### [60] [PT$^2$-LLM: Post-Training Ternarization for Large Language Models](https://arxiv.org/abs/2510.03267)
*Xianglong Yan,Chengzhu Bao,Zhiteng Li,Tianao Zhang,Kaicheng Yang,Haotong Qin,Ruobing Xie,Xingwu Sun,Yulun Zhang*

Main category: cs.LG

TL;DR: PT^2-LLM是一种面向大语言模型的后训练三值化框架，通过非对称三值量化器和两阶段优化流程，在保持竞争力的性能同时显著降低内存成本并加速推理。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在部署时面临巨大的内存和计算需求，三值化作为一种有前景的压缩技术，在后训练量化场景下的潜力尚未充分探索，主要挑战包括无需训练的参数量化和异常值/分散权重带来的量化困难。

Method: 提出PT^2-LLM框架，核心包括：1）迭代三值拟合(ITF)，交替进行最优三值网格构建和灵活舍入；2）激活感知网格对齐(AGA)，进一步优化三值网格；3）基于结构相似性的重排序(SSR)策略，利用列间结构相似性缓解异常值影响。

Result: 大量实验表明，PT^2-LLM在保持与最先进2位后训练量化方法竞争力的性能的同时，具有更低的内存成本，并能同时加速预填充和解码阶段，实现端到端加速。

Conclusion: PT^2-LLM为LLM部署提供了一种高效的后训练三值化解决方案，在性能、内存效率和推理速度方面取得了良好平衡。

Abstract: Large Language Models (LLMs) have shown impressive capabilities across
diverse tasks, but their large memory and compute demands hinder deployment.
Ternarization has gained attention as a promising compression technique,
delivering substantial size reduction and high computational efficiency.
However, its potential in the post-training quantization (PTQ) setting remains
underexplored, due to the challenge of training-free parameter optimization and
the quantization difficulty posed by outliers and dispersed weights. To address
these issues, we propose PT$^2$-LLM, a post-training ternarization framework
tailored for LLMs. At its core is an Asymmetric Ternary Quantizer equipped with
a two-stage refinement pipeline: (1) Iterative Ternary Fitting (ITF), which
alternates between optimal ternary grid construction and flexible rounding to
minimize quantization error, and (2) Activation-aware Grid Alignment (AGA),
which further refines the ternary grid to better match full-precision outputs.
In addition, we propose a plug-and-play Structural Similarity-based Reordering
(SSR) strategy that leverages inter-column structural similarity to ease
quantization and mitigate outlier effects, further enhancing overall
performance. Extensive experiments demonstrate that PT$^2$-LLM delivers
competitive performance against state-of-the-art (SOTA) 2-bit PTQ methods with
lower memory cost, while also accelerating both prefill and decoding to achieve
end-to-end speedup. The code and models will be available at
https://github.com/XIANGLONGYAN/PT2-LLM.

</details>


### [61] [Multi-task neural diffusion processes for uncertainty-quantified wind power prediction](https://arxiv.org/abs/2510.03419)
*Joseph Rawson,Domniki Ladopoulou,Petros Dellaportas*

Main category: cs.LG

TL;DR: 提出了一个多任务神经扩散过程（MT-NDP）框架，用于风电场功率预测，通过任务编码器捕获跨涡轮机相关性，并在真实SCADA数据上进行了首次NDP实证评估。


<details>
  <summary>Details</summary>
Motivation: 不确定性感知的风电功率预测对于电网集成和风电场可靠运行至关重要，需要能够提供校准且可扩展的预测方法。

Method: 扩展神经扩散过程（NDPs）到多任务框架MT-NDP，引入任务编码器来捕获跨涡轮机相关性，并支持对未见涡轮机的少样本适应。

Result: MT-NDP框架在点精度和校准方面优于单任务NDPs和GPs，特别是对于偏离机群平均行为的涡轮机，提供了更锐利但可信的预测区间。

Conclusion: 基于NDP的模型提供了适合运营部署的校准和可扩展预测，能够支持现代风电场的调度和维护决策。

Abstract: Uncertainty-aware wind power prediction is essential for grid integration and
reliable wind farm operation. We apply neural diffusion processes (NDPs)-a
recent class of models that learn distributions over functions-and extend them
to a multi-task NDP (MT-NDP) framework for wind power prediction. We provide
the first empirical evaluation of NDPs in real supervisory control and data
acquisition (SCADA) data. We introduce a task encoder within MT-NDPs to capture
cross-turbine correlations and enable few-shot adaptation to unseen turbines.
The proposed MT-NDP framework outperforms single-task NDPs and GPs in terms of
point accuracy and calibration, particularly for wind turbines whose behaviour
deviates from the fleet average. In general, NDP-based models deliver
calibrated and scalable predictions suitable for operational deployment,
offering sharper, yet trustworthy, predictive intervals that can support
dispatch and maintenance decisions in modern wind farms.

</details>


### [62] [Decrypt Modality Gap in Multimodal Contrastive Learning: From Convergent Representation to Pair Alignment](https://arxiv.org/abs/2510.03268)
*Lingjie Yi,Raphael Douady,Chao Chen*

Main category: cs.LG

TL;DR: 本文提出了首个分析多模态对比学习收敛最优表示和模态对齐的理论框架，揭示了维度塌陷是模态间隙的根本原因，并证明了通过超平面旋转和共享空间投影可以实现完美对齐。


<details>
  <summary>Details</summary>
Motivation: 现有研究发现多模态对比学习中不同模态的表示占据嵌入空间的不同区域（模态间隙），且关于模态间隙大小对下游性能影响的研究结果不一致，这引发了两个关键问题：什么导致了模态间隙？它如何影响下游任务？

Method: 建立理论框架分析多模态对比学习的收敛最优表示和模态对齐，在不同约束条件下（无约束、锥约束、子空间约束）证明模态间隙的收敛行为。

Result: 证明在无约束或锥约束下模态间隙收敛为零；在子空间约束下（由于维度塌陷），模态间隙收敛为两个超平面间的最小角度，识别维度塌陷是模态间隙的根本原因。

Conclusion: 模态间隙通过影响样本对之间的对齐来影响下游性能，但在子空间约束下仍可通过超平面旋转和共享空间投影实现两个模态的完美对齐。

Abstract: Multimodal contrastive learning (MCL) aims to embed data from different
modalities in a shared embedding space. However, empirical evidence shows that
representations from different modalities occupy completely separate regions of
embedding space, a phenomenon referred to as the modality gap. Moreover,
experimental findings on how the size of the modality gap influences downstream
performance are inconsistent. These observations raise two key questions: (1)
What causes the modality gap? (2) How does it affect downstream tasks? To
address these questions, this paper introduces the first theoretical framework
for analyzing the convergent optimal representations of MCL and the modality
alignment when training is optimized. Specifically, we prove that without any
constraint or under the cone constraint, the modality gap converges to zero.
Under the subspace constraint (i.e., representations of two modalities fall
into two distinct hyperplanes due to dimension collapse), the modality gap
converges to the smallest angle between the two hyperplanes. This result
identifies \emph{dimension collapse} as the fundamental origin of the modality
gap. Furthermore, our theorems demonstrate that paired samples cannot be
perfectly aligned under the subspace constraint. The modality gap influences
downstream performance by affecting the alignment between sample pairs. We
prove that, in this case, perfect alignment between two modalities can still be
achieved via two ways: hyperplane rotation and shared space projection.

</details>


### [63] [Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation](https://arxiv.org/abs/2510.03437)
*Jairo Diaz-Rodriguez,Mumin Jia*

Main category: cs.LG

TL;DR: 该论文研究了核变点检测在文本分割中的应用，证明了在m-依赖数据下的理论一致性，并通过LLM模拟和实证研究验证了该方法在现代文本嵌入中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有核变点检测理论主要基于独立性假设，但真实文本数据存在强依赖性，需要建立更符合实际的理论保证。

Method: 建立了m-依赖数据下的理论一致性证明，使用LLM生成合成m-依赖文本进行模拟验证，并在多个文本数据集上使用现代嵌入进行实证研究。

Result: 证明了检测变点数量的强一致性和位置估计的弱一致性，实证显示核变点检测在文本分割指标上优于基线方法。

Conclusion: 核变点检测不仅具有理论可靠性，在实际文本分割任务中也表现出色，特别是在使用现代文本嵌入时。

Abstract: Kernel change-point detection (KCPD) has become a widely used tool for
identifying structural changes in complex data. While existing theory
establishes consistency under independence assumptions, real-world sequential
data such as text exhibits strong dependencies. We establish new guarantees for
KCPD under $m$-dependent data: specifically, we prove consistency in the number
of detected change points and weak consistency in their locations under mild
additional assumptions. We perform an LLM-based simulation that generates
synthetic $m$-dependent text to validate the asymptotics. To complement these
results, we present the first comprehensive empirical study of KCPD for text
segmentation with modern embeddings. Across diverse text datasets, KCPD with
text embeddings outperforms baselines in standard text segmentation metrics. We
demonstrate through a case study on Taylor Swift's tweets that KCPD not only
provides strong theoretical and simulated reliability but also practical
effectiveness for text segmentation tasks.

</details>


### [64] [General Exploratory Bonus for Optimistic Exploration in RLHF](https://arxiv.org/abs/2510.03269)
*Wendi Li,Changdae Oh,Yixuan Li*

Main category: cs.LG

TL;DR: 提出了通用探索奖励(GEB)框架，解决现有KL和α-散度正则化方法在强化学习人类反馈中探索偏向保守区域的问题，通过参考相关奖励调节实现乐观探索。


<details>
  <summary>Details</summary>
Motivation: 现有基于KL或α-散度正则化的探索奖励方法无法实现真正的乐观探索，反而偏向参考模型的高概率区域，强化了保守行为而非发现不确定区域。

Method: 引入通用探索奖励(GEB)框架，通过参考相关奖励调节来抵消散度引起的偏差，统一了先前的启发式奖励方法，并自然扩展到完整的α-散度族。

Result: 在多个散度设置和大语言模型骨干上的对齐任务中，GEB始终优于基线方法。

Conclusion: GEB为RLHF中的乐观探索提供了既有理论依据又实用的解决方案。

Abstract: Optimistic exploration is central to improving sample efficiency in
reinforcement learning with human feedback, yet existing exploratory bonus
methods to incentivize exploration often fail to realize optimism. We provide a
theoretical analysis showing that current formulations, under KL or
$\alpha$-divergence regularization, unintentionally bias exploration toward
high-probability regions of the reference model, thereby reinforcing
conservative behavior instead of promoting discovery of uncertain regions. To
address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel
theoretical framework that provably satisfies the optimism principle. GEB
counteracts divergence-induced bias via reference-dependent reward regulation
and unifies prior heuristic bonuses as special cases, while extending naturally
across the full $\alpha$-divergence family. Empirically, GEB consistently
outperforms baselines on alignment tasks across multiple divergence settings
and large language model backbones. These results demonstrate that GEB offers
both a principled and practical solution for optimistic exploration in RLHF.

</details>


### [65] [On residual network depth](https://arxiv.org/abs/2510.03470)
*Benoit Dherin,Michael Munn*

Main category: cs.LG

TL;DR: 本文通过残差扩展定理证明了深度残差网络实际上等同于浅层模型的隐式集成，并解释了归一化层的历史必要性。


<details>
  <summary>Details</summary>
Motivation: 理解为什么深度残差网络如此有效，以及为什么需要归一化层来训练深层模型。

Method: 提出残差扩展定理，通过分析残差网络的组合路径来揭示其隐式集成结构。

Result: 发现网络深度的增加等价于隐式集成规模的扩大，组合路径的爆炸性增长导致输出信号激增，这解释了归一化层的必要性。

Conclusion: 残差模块的缩放提供了控制组合爆炸的原则性解决方案，同时也作为容量控制机制隐式正则化模型复杂度。

Abstract: Deep residual architectures, such as ResNet and the Transformer, have enabled
models of unprecedented depth, yet a formal understanding of why depth is so
effective remains an open question. A popular intuition, following Veit et al.
(2016), is that these residual networks behave like ensembles of many shallower
models. Our key finding is an explicit analytical formula that verifies this
ensemble perspective, proving that increasing network depth is mathematically
equivalent to expanding the size of this implicit ensemble. Furthermore, our
expansion reveals a hierarchical ensemble structure in which the combinatorial
growth of computation paths leads to an explosion in the output signal,
explaining the historical necessity of normalization layers in training deep
models. This insight offers a first principles explanation for the historical
dependence on normalization layers and sheds new light on a family of
successful normalization-free techniques like SkipInit and Fixup. However,
while these previous approaches infer scaling factors through optimizer
analysis or a heuristic analogy to Batch Normalization, our work offers the
first explanation derived directly from the network's inherent functional
structure. Specifically, our Residual Expansion Theorem reveals that scaling
each residual module provides a principled solution to taming the combinatorial
explosion inherent to these architectures. We further show that this scaling
acts as a capacity controls that also implicitly regularizes the model's
complexity.

</details>


### [66] [CoDA: Coding LM via Diffusion Adaptation](https://arxiv.org/abs/2510.03270)
*Haolin Chen,Shiyu Wang,Can Qin,Bo Pang,Zuxin Liu,Jielin Qiu,Jianguo Zhang,Yingbo Zhou,Zeyuan Chen,Ran Xu,Shelby Heinecke,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.LG

TL;DR: CoDA是一个1.7B参数的扩散语言模型，专为代码生成设计，通过大规模扩散预训练、代码中心的中期训练和指令调优，在保持推理延迟竞争力的同时，在多个代码基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型具有双向上下文和填充能力，但现有系统通常较重。作者希望开发一个轻量级但性能优异的扩散代码生成模型。

Method: 采用1.7B参数的扩散编码器，在TPU上进行训练，结合大规模扩散预训练、代码中心的中期训练和指令调优，使用置信度引导采样来保持推理效率。

Result: 在Humaneval、MBPP和EvalPlus基准测试中，CoDA-1.7B-Instruct匹配或超越了参数高达7B的扩散模型。

Conclusion: CoDA展示了轻量级扩散代码生成模型的可行性，并发布了模型检查点、评估工具和TPU训练流水线来促进相关研究。

Abstract: Diffusion language models promise bidirectional context and infilling
capabilities that autoregressive coders lack, yet practical systems remain
heavyweight. We introduce CoDA, a 1.7B-parameter diffusion coder trained on TPU
with a fully open-source training pipeline. CoDA pairs large-scale diffusion
pre-training with code-centric mid-training and instruction tuning, enabling
confidence-guided sampling that keeps inference latency competitive. On
Humaneval, MBPP, and EvalPlus, CoDA-1.7B-Instruct matches or surpasses
diffusion models up to 7B parameters. Our release includes model checkpoints,
evaluation harnesses, and TPU training pipelines to accelerate research on
lightweight diffusion-based coding assistants.

</details>


### [67] [MECKD: Deep Learning-Based Fall Detection in Multilayer Mobile Edge Computing With Knowledge Distillation](https://arxiv.org/abs/2510.03601)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Kai-Chun Liu,Yu Tsao*

Main category: cs.LG

TL;DR: 提出了一种多层移动边缘计算（MLMEC）框架，结合知识蒸馏（KD）技术来平衡跌倒检测系统的准确性和延迟问题，在多个数据集上实现了准确率提升和延迟降低。


<details>
  <summary>Details</summary>
Motivation: 随着老龄化人口增加，跌倒检测系统的重要性日益凸显。传统基于边缘设备或云计算的架构面临模型大小限制和数据传输延迟的挑战，需要寻找平衡准确性和延迟的解决方案。

Method: 采用多层移动边缘计算（MLMEC）框架，将架构分为多个站点，每个站点配备神经网络模型。前端设备无法可靠检测跌倒时，数据会传输到具有更强后端计算能力的站点。使用知识蒸馏（KD）方法，让高功率后端站点为前端提供额外学习经验。

Result: 知识蒸馏方法在SisFall数据集上提高准确率11.65%，在FallAllD数据集上提高2.78%。MLMEC+KD相比无KD的MLMEC，在FallAllD数据集上降低数据延迟率54.15%，在SisFall数据集上降低46.67%。

Conclusion: MLMEC跌倒检测系统在准确性和延迟方面都得到了显著改善，为实时跌倒检测提供了有效的解决方案。

Abstract: The rising aging population has increased the importance of fall detection
(FD) systems as an assistive technology, where deep learning techniques are
widely applied to enhance accuracy. FD systems typically use edge devices (EDs)
worn by individuals to collect real-time data, which are transmitted to a cloud
center (CC) or processed locally. However, this architecture faces challenges
such as a limited ED model size and data transmission latency to the CC. Mobile
edge computing (MEC), which allows computations at MEC servers deployed between
EDs and CC, has been explored to address these challenges. We propose a
multilayer MEC (MLMEC) framework to balance accuracy and latency. The MLMEC
splits the architecture into stations, each with a neural network model. If
front-end equipment cannot detect falls reliably, data are transmitted to a
station with more robust back-end computing. The knowledge distillation (KD)
approach was employed to improve front-end detection accuracy by allowing
high-power back-end stations to provide additional learning experiences,
enhancing precision while reducing latency and processing loads. Simulation
results demonstrate that the KD approach improved accuracy by 11.65% on the
SisFall dataset and 2.78% on the FallAllD dataset. The MLMEC with KD also
reduced the data latency rate by 54.15% on the FallAllD dataset and 46.67% on
the SisFall dataset compared to the MLMEC without KD. In summary, the MLMEC FD
system exhibits improved accuracy and reduced latency.

</details>


### [68] [Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^π$-Realizability and Concentrability](https://arxiv.org/abs/2510.03494)
*Volodymyr Tkachuk,Csaba Szepesvári,Xiaoqi Tan*

Main category: cs.LG

TL;DR: 本文研究了有限时域离线强化学习中的策略评估和策略优化问题，在轨迹数据、覆盖性和qπ可实现性假设下，提出了统计高效的策略评估学习器，并改进了策略优化的样本复杂度分析。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明，在仅有数据覆盖性和qπ可实现性假设下，策略评估和策略优化都无法实现统计高效学习。最近有工作证明了在轨迹数据假设下策略优化的可行性，本文旨在解决策略评估问题并改进策略优化的样本复杂度。

Method: 在轨迹数据、覆盖性和qπ可实现性假设下，开发了统计高效的策略评估学习器，并对现有策略优化方法的样本复杂度进行了更严格的分析。

Result: 成功提出了统计高效的策略评估学习器，并显著改进了策略优化的样本复杂度上界。

Conclusion: 本文证明了在轨迹数据、覆盖性和qπ可实现性假设下，离线强化学习中的策略评估和策略优化都可以实现统计高效学习，为离线RL的理论发展提供了重要贡献。

Abstract: We study finite-horizon offline reinforcement learning (RL) with function
approximation for both policy evaluation and policy optimization. Prior work
established that statistically efficient learning is impossible for either of
these problems when the only assumptions are that the data has good coverage
(concentrability) and the state-action value function of every policy is
linearly realizable ($q^\pi$-realizability) (Foster et al., 2021). Recently,
Tkachuk et al. (2024) gave a statistically efficient learner for policy
optimization, if in addition the data is assumed to be given as trajectories.
In this work we present a statistically efficient learner for policy evaluation
under the same assumptions. Further, we show that the sample complexity of the
learner used by Tkachuk et al. (2024) for policy optimization can be improved
by a tighter analysis.

</details>


### [69] [Decision Potential Surface: A Theoretical and Practical Approximation of LLM's Decision Boundary](https://arxiv.org/abs/2510.03271)
*Zi Liang,Zhiyao Wu,Haoyang Shang,Yulin Jin,Qingqing Ye,Huadi Zheng,Peizhao Hu,Haibo Hu*

Main category: cs.LG

TL;DR: 本文提出决策势能面(DPS)作为分析大语言模型决策边界的新概念，通过有限次序列采样来近似构建LLM的决策边界，解决了传统方法计算不可行的问题。


<details>
  <summary>Details</summary>
Motivation: 分析大语言模型的决策边界对于理解模型核心特性和解释行为至关重要，但由于词汇序列规模庞大和自回归特性，为主流LLMs构建决策边界在计算上不可行。

Method: 提出决策势能面(DPS)概念，定义在区分不同采样序列的置信度上，捕捉决策边界的潜力。开发K-DPS算法，仅需K次有限序列采样即可近似LLM的决策边界。

Result: 理论推导了K-DPS与理想DPS之间的绝对误差、期望误差和误差集中的上界，证明误差可通过采样次数进行权衡。通过多种LLM和语料库的实验验证了结果。

Conclusion: DPS为分析LLM决策边界提供了可行的新方法，K-DPS算法能够以可接受的误差有效近似决策边界，为理解大语言模型行为提供了新工具。

Abstract: Decision boundary, the subspace of inputs where a machine learning model
assigns equal classification probabilities to two classes, is pivotal in
revealing core model properties and interpreting behaviors. While analyzing the
decision boundary of large language models (LLMs) has raised increasing
attention recently, constructing it for mainstream LLMs remains computationally
infeasible due to the enormous vocabulary-sequence sizes and the
auto-regressive nature of LLMs. To address this issue, in this paper we propose
Decision Potential Surface (DPS), a new notion for analyzing LLM decision
boundary. DPS is defined on the confidences in distinguishing different
sampling sequences for each input, which naturally captures the potential of
decision boundary. We prove that the zero-height isohypse in DPS is equivalent
to the decision boundary of an LLM, with enclosed regions representing decision
regions. By leveraging DPS, for the first time in the literature, we propose an
approximate decision boundary construction algorithm, namely $K$-DPS, which
only requires K-finite times of sequence sampling to approximate an LLM's
decision boundary with negligible error. We theoretically derive the upper
bounds for the absolute error, expected error, and the error concentration
between K-DPS and the ideal DPS, demonstrating that such errors can be
trade-off with sampling times. Our results are empirically validated by
extensive experiments across various LLMs and corpora.

</details>


### [70] [Forecasting-Based Biomedical Time-series Data Synthesis for Open Data and Robust AI](https://arxiv.org/abs/2510.04622)
*Youngjoon Lee,Seongmin Cho,Yehhyun Jo,Jinu Gong,Hyunjoo Jenny Lee,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出基于先进预测模型的生物医学时间序列合成数据生成框架，能高保真复制EEG和EMG等复杂电生理信号，解决数据稀缺和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 严格的隐私法规和资源需求限制了生物医学时间序列AI发展，导致数据需求与可访问性之间存在关键差距。

Method: 基于先进预测模型的合成生物医学时间序列数据生成框架，准确复制复杂电生理信号。

Result: 合成数据保持了真实数据的关键时间和频谱特性，可作为真实数据的有效替代品，显著提升AI模型性能。

Conclusion: 该方法在保持关键生物医学特征的同时提供高可扩展性，无缝集成到开源存储库中，大幅扩展AI驱动生物医学研究的资源。

Abstract: The limited data availability due to strict privacy regulations and
significant resource demands severely constrains biomedical time-series AI
development, which creates a critical gap between data requirements and
accessibility. Synthetic data generation presents a promising solution by
producing artificial datasets that maintain the statistical properties of real
biomedical time-series data without compromising patient confidentiality. We
propose a framework for synthetic biomedical time-series data generation based
on advanced forecasting models that accurately replicates complex
electrophysiological signals such as EEG and EMG with high fidelity. These
synthetic datasets preserve essential temporal and spectral properties of real
data, which enables robust analysis while effectively addressing data scarcity
and privacy challenges. Our evaluations across multiple subjects demonstrate
that the generated synthetic data can serve as an effective substitute for real
data and also significantly boost AI model performance. The approach maintains
critical biomedical features while provides high scalability for various
applications and integrates seamlessly into open-source repositories,
substantially expanding resources for AI-driven biomedical research.

</details>


### [71] [Sequential decoder training for improved latent space dynamics identification](https://arxiv.org/abs/2510.03535)
*William Anderson,Seung Whan Chung,Youngsoo Choi*

Main category: cs.LG

TL;DR: 提出多阶段LaSDI框架，通过顺序学习额外解码器来修正残差误差，提高降阶模型的预测精度和训练效率


<details>
  <summary>Details</summary>
Motivation: 传统LaSDI在训练过程中强制实施潜在动力学可能会损害模型对仿真数据的重构精度，需要改进

Method: 多阶段LaSDI框架，顺序学习额外解码器来修正前阶段的残差误差

Result: 在1D-1V Vlasov方程上，mLaSDI始终优于标准LaSDI，实现更低的预测误差和更短的训练时间

Conclusion: 多阶段LaSDI框架能有效提高降阶模型的准确性和效率

Abstract: Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.

</details>


### [72] [PDE-Transformer: A Continuous Dynamical Systems Approach to Sequence Modeling](https://arxiv.org/abs/2510.03272)
*Yukun Zhang,Xueqing Zhou*

Main category: cs.LG

TL;DR: 将Transformer架构重新概念化为由主偏微分方程控制的连续时空动力系统，揭示了残差连接和层归一化是稳定该系统的必要数学稳定器。


<details>
  <summary>Details</summary>
Motivation: 虽然Transformer架构已经彻底改变了人工智能，但对其内部机制的理论理解仍然缺乏。本文旨在通过连续动力系统视角提供对Transformer设计的原理性解释。

Method: 引入一个分析框架，将Transformer的离散分层结构映射为连续时空动力系统，其中自注意力对应非局部相互作用，前馈网络对应局部反应，残差连接和层归一化对应稳定机制。

Result: 实验表明，没有残差连接会导致灾难性的表示漂移，没有层归一化会导致不稳定的爆炸性训练动态。这些组件是稳定系统的必要数学稳定器。

Conclusion: 这项工作为Transformer设计提供了第一性原理解释，并建立了通过连续动力学视角分析深度神经网络的新范式。

Abstract: The Transformer architecture has revolutionized artificial intelligence, yet
a principled theoretical understanding of its internal mechanisms remains
elusive. This paper introduces a novel analytical framework that
reconceptualizes the Transformer's discrete, layered structure as a continuous
spatiotemporal dynamical system governed by a master Partial Differential
Equation (PDE). Within this paradigm, we map core architectural components to
distinct mathematical operators: self-attention as a non-local interaction, the
feed-forward network as a local reaction, and, critically, residual connections
and layer normalization as indispensable stabilization mechanisms. We do not
propose a new model, but rather employ the PDE system as a theoretical probe to
analyze the mathematical necessity of these components. By comparing a standard
Transformer with a PDE simulator that lacks explicit stabilizers, our
experiments provide compelling empirical evidence for our central thesis. We
demonstrate that without residual connections, the system suffers from
catastrophic representational drift, while the absence of layer normalization
leads to unstable, explosive training dynamics. Our findings reveal that these
seemingly heuristic "tricks" are, in fact, fundamental mathematical stabilizers
required to tame an otherwise powerful but inherently unstable continuous
system. This work offers a first-principles explanation for the Transformer's
design and establishes a new paradigm for analyzing deep neural networks
through the lens of continuous dynamics.

</details>


### [73] [Federated Self-Supervised Learning for Automatic Modulation Classification under Non-IID and Class-Imbalanced Data](https://arxiv.org/abs/2510.04927)
*Usman Akram,Yiyue Chen,Haris Vikalo*

Main category: cs.LG

TL;DR: FedSSL-AMC：一种用于自动调制分类的联邦自监督学习方法，通过无标签I/Q序列训练因果时间膨胀CNN，结合客户端SVM分类器，解决隐私、通信开销和信道偏移问题。


<details>
  <summary>Details</summary>
Motivation: 集中式AMC训练存在隐私泄露、通信开销大和对信道偏移不鲁棒的问题，联邦学习虽然避免了数据集中但面临类别不平衡、非IID数据分布和标签样本有限等挑战。

Method: 提出FedSSL-AMC，在客户端使用无标签I/Q序列通过三元组损失自监督训练因果时间膨胀CNN，然后在每个客户端使用少量标签数据训练SVM分类器。

Result: 建立了联邦表示学习过程的收敛性和下游分类器在特征噪声下的可分离性保证。在合成和空中数据集上的实验显示，在异构SNR、载波频率偏移和非IID标签分布下，相比监督FL基线有持续提升。

Conclusion: FedSSL-AMC通过联邦自监督学习有效解决了AMC中的隐私、通信和鲁棒性问题，在异构信道条件下优于传统监督联邦学习方法。

Abstract: Training automatic modulation classification (AMC) models on centrally
aggregated data raises privacy concerns, incurs communication overhead, and
often fails to confer robustness to channel shifts. Federated learning (FL)
avoids central aggregation by training on distributed clients but remains
sensitive to class imbalance, non-IID client distributions, and limited labeled
samples. We propose FedSSL-AMC, which trains a causal, time-dilated CNN with
triplet-loss self-supervision on unlabeled I/Q sequences across clients,
followed by per-client SVMs on small labeled sets. We establish convergence of
the federated representation learning procedure and a separability guarantee
for the downstream classifier under feature noise. Experiments on synthetic and
over-the-air datasets show consistent gains over supervised FL baselines under
heterogeneous SNR, carrier-frequency offsets, and non-IID label partitions.

</details>


### [74] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: 提出IMMFM框架，通过多时间点联合学习连续随机动力学，使用分段二次插值路径作为流匹配的平滑目标，能够处理稀疏采样和高维轨迹问题。


<details>
  <summary>Details</summary>
Motivation: 生成模型在处理稀疏采样和高维轨迹时通常只能学习成对转移，无法充分利用多个观测时间点的信息。

Method: 采用分段二次插值路径作为流匹配的平滑目标，联合优化漂移项和数据驱动的扩散系数，并基于理论条件确保稳定学习。

Result: 在合成基准测试和真实神经影像数据集上，IMMFM在预测准确性和下游任务表现上优于现有方法。

Conclusion: IMMFM能够捕捉内在随机性、处理不规则稀疏采样，并生成特定对象的轨迹，为序列数据建模提供了有效解决方案。

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [75] [Learning without Global Backpropagation via Synergistic Information Distillation](https://arxiv.org/abs/2510.03273)
*Chenhao Ye,Ming Tang*

Main category: cs.LG

TL;DR: 提出Synergistic Information Distillation (SID)框架，通过将深度学习重构为局部协同精炼问题，解决反向传播的更新锁定和高内存消耗问题，实现并行训练并保持前向推理不变。


<details>
  <summary>Details</summary>
Motivation: 解决反向传播(BP)的两个关键可扩展性瓶颈：更新锁定（网络模块需等待整个反向传播完成）和高内存消耗（存储激活值用于梯度计算）。

Method: 将深度网络构建为模块管道，每个模块施加局部目标来精炼对真实目标的概率信念。该目标平衡对目标的保真度和与前一个模块信念的一致性，从而解耦模块间的反向依赖。

Result: 理论证明SID保证网络深度增加时性能单调提升。实验表明SID在分类准确率上匹配或超越BP，具有更好的可扩展性和对标签噪声的鲁棒性。

Conclusion: SID是一个通用的BP替代方案，消除了更新锁定，大幅降低内存需求，同时保持标准前向推理过程，具有优越的可扩展性和鲁棒性。

Abstract: Backpropagation (BP), while foundational to deep learning, imposes two
critical scalability bottlenecks: update locking, where network modules remain
idle until the entire backward pass completes, and high memory consumption due
to storing activations for gradient computation. To address these limitations,
we introduce Synergistic Information Distillation (SID), a novel training
framework that reframes deep learning as a cascade of local cooperative
refinement problems. In SID, a deep network is structured as a pipeline of
modules, each imposed with a local objective to refine a probabilistic belief
about the ground-truth target. This objective balances fidelity to the target
with consistency to the belief from its preceding module. By decoupling the
backward dependencies between modules, SID enables parallel training and hence
eliminates update locking and drastically reduces memory requirements.
Meanwhile, this design preserves the standard feed-forward inference pass,
making SID a versatile drop-in replacement for BP. We provide a theoretical
foundation, proving that SID guarantees monotonic performance improvement with
network depth. Empirically, SID consistently matches or surpasses the
classification accuracy of BP, exhibiting superior scalability and pronounced
robustness to label noise.Code is available at:
https://github.com/ychAlbert/sid-bp

</details>


### [76] [BEKAN: Boundary condition-guaranteed evolutionary Kolmogorov-Arnold networks with radial basis functions for solving PDE problems](https://arxiv.org/abs/2510.03576)
*Bongseok Kim,Jiahao Zhang,Guang Lin*

Main category: cs.LG

TL;DR: 提出了边界条件保证的进化Kolmogorov-Arnold网络(BEKAN)，通过三种可组合的方法精确处理Dirichlet、周期性和Neumann边界条件，在PDE求解中优于MLP和B样条KAN。


<details>
  <summary>Details</summary>
Motivation: 深度学习在求解PDE方面受到关注，但神经网络的"黑箱"特性阻碍了边界条件的精确执行。

Method: 使用高斯径向基函数构造单变量基函数近似解并在网络激活层编码边界信息；周期性边界使用正弦函数构造的周期层；Neumann边界采用最小二乘公式指导参数演化。

Result: 在Dirichlet、Neumann、周期性和混合边界值问题上进行了广泛数值实验，BEKAN在精度上优于多层感知机和B样条KAN。

Conclusion: 该方法增强了KAN在求解PDE问题同时满足边界条件的能力，促进了科学计算和工程应用的进步。

Abstract: Deep learning has gained attention for solving PDEs, but the black-box nature
of neural networks hinders precise enforcement of boundary conditions. To
address this, we propose a boundary condition-guaranteed evolutionary
Kolmogorov-Arnold Network (KAN) with radial basis functions (BEKAN). In BEKAN,
we propose three distinct and combinable approaches for incorporating
Dirichlet, periodic, and Neumann boundary conditions into the network. For
Dirichlet problem, we use smooth and global Gaussian RBFs to construct
univariate basis functions for approximating the solution and to encode
boundary information at the activation level of the network. To handle periodic
problems, we employ a periodic layer constructed from a set of sinusoidal
functions to enforce the boundary conditions exactly. For a Neumann problem, we
devise a least-squares formulation to guide the parameter evolution toward
satisfying the Neumann condition. By virtue of the boundary-embedded RBFs, the
periodic layer, and the evolutionary framework, we can perform accurate PDE
simulations while rigorously enforcing boundary conditions. For demonstration,
we conducted extensive numerical experiments on Dirichlet, Neumann, periodic,
and mixed boundary value problems. The results indicate that BEKAN outperforms
both multilayer perceptron (MLP) and B-splines KAN in terms of accuracy. In
conclusion, the proposed approach enhances the capability of KANs in solving
PDE problems while satisfying boundary conditions, thereby facilitating
advancements in scientific computing and engineering applications.

</details>


### [77] [Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](https://arxiv.org/abs/2510.03274)
*Tianao Zhang,Zhiteng Li,Xianglong Yan,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.LG

TL;DR: 提出了Quant-dLLM框架，专门针对扩散大语言模型进行超低位后训练量化，解决了标准PTQ方法在2位量化下性能不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型具有双向上下文和灵活掩码去噪生成的优点，但模型规模不断增长，需要权重压缩部署。标准后训练量化方法直接应用到dLLMs在2位量化时性能不理想。

Method: 1) 掩码校准模拟(MCS)对齐时间步相关的掩码校准；2) 数据感知任意顺序量化器(DAQ)通过优化算法学习超低位权重表示；3) 自适应块级混合精度(ABMP)基于敏感度分配位宽。

Result: 在严格的2位预算下，Quant-dLLM在dLLMs上始终比最先进的AR迁移PTQ方法获得更高的准确率。

Conclusion: Quant-dLLM是针对扩散大语言模型的有效超低位量化框架，显著提升了2位量化性能。

Abstract: Diffusion large language models (dLLMs), which offer bidirectional context
and flexible masked-denoising generation, are emerging as a compelling
alternative to autoregressive (AR) LLMs. However, like AR LLMs, their model
sizes continue to grow, motivating weight compression for deployment. Although
post-training quantization (PTQ) is effective for AR LLMs, directly
transferring it to dLLMs at 2-bit leads to unsatisfactory performance. To
tackle these challenges, we propose Quant-dLLM, an ultra-low-bit PTQ framework
tailored to dLLMs. Since masked-denoising activations in dLLMs differ from the
fully visible signals assumed by standard PTQ methods, we introduce Masked
Calibration Simulation (MCS) to align calibration with the timestep-dependent
masking, which yields more reliable calibrations. Moreover, we propose a
Data-aware Any-order Quantizer (DAQ) that learns ultra-low-bit weight
representations via an optimization algorithm. It performs iterative
approximation guided by our simulated calibration data. In addition, under a
strict 2-bit budget, we introduce Adaptive Blockwise Mixed Precision (ABMP), a
sensitivity-based precision allocation scheme that adaptively assigns bit width
across channel groups. When restricted to 2-bit precision, Quant-dLLM
consistently achieves higher accuracy than state-of-the-art (SOTA) AR-transfer
PTQ methods on dLLMs. The code and models will be available at:
https://github.com/ZTA2785/Quant-dLLM.

</details>


### [78] [Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning](https://arxiv.org/abs/2510.03578)
*Haoran Li,Chenhan Xiao,Muhao Guo,Yang Weng*

Main category: cs.LG

TL;DR: 提出Latent Mixture of Symmetries (Latent MoS)模型，通过捕捉复杂动态测量中的对称性混合来学习动力学，提高样本效率，并在插值和外推任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设单一全局对称群，并将对称性发现和动态学习作为独立任务，导致表达能力有限和误差累积。需要更有效的方法从有限测量中学习系统动力学。

Method: 提出Latent MoS模型，捕捉对称性主导的潜在因子混合，在局部可证明地保持基础对称变换。引入分层架构堆叠MoS块来捕获长期等变性。

Result: 在多种物理系统中的数值实验表明，Latent MoS在插值和外推任务中优于最先进的基线方法，并提供可解释的潜在表示。

Conclusion: Latent MoS通过对称性混合有效学习复杂系统动力学，提高样本效率，为未来几何和安全关键分析提供基础。

Abstract: Learning dynamics is essential for model-based control and Reinforcement
Learning in engineering systems, such as robotics and power systems. However,
limited system measurements, such as those from low-resolution sensors, demand
sample-efficient learning. Symmetry provides a powerful inductive bias by
characterizing equivariant relations in system states to improve sample
efficiency. While recent methods attempt to discover symmetries from data, they
typically assume a single global symmetry group and treat symmetry discovery
and dynamic learning as separate tasks, leading to limited expressiveness and
error accumulation. In this paper, we propose the Latent Mixture of Symmetries
(Latent MoS), an expressive model that captures a mixture of symmetry-governed
latent factors from complex dynamical measurements. Latent MoS focuses on
dynamic learning while locally and provably preserving the underlying symmetric
transformations. To further capture long-term equivariance, we introduce a
hierarchical architecture that stacks MoS blocks. Numerical experiments in
diverse physical systems demonstrate that Latent MoS outperforms
state-of-the-art baselines in interpolation and extrapolation tasks while
offering interpretable latent representations suitable for future geometric and
safety-critical analyses.

</details>


### [79] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: SDQ-LLM是一个用于1位LLM量化的新框架，通过Sigma-Delta量化器和过采样技术，将模型权重压缩到1位或1.58位表示，同时保持语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临显著的计算和内存挑战，需要极低位量化来实现高效部署。

Method: 使用上采样结合Sigma-Delta量化器对LLM权重进行二值化或三值化，采用Hadamard基权重平滑减少量化精度损失，并提出细粒度的MultiOSR分配策略。

Result: 在OPT和LLaMA模型系列上的广泛实验表明，SDQ-LLM在高度激进的低OSR设置下实现了更高效和高精度的性能。

Conclusion: SDQ-LLM框架能够实现极低位LLM量化，在模型大小和精度之间提供最佳权衡，显著提高推理效率。

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [80] [Explore the Loss space with Hill-ADAM](https://arxiv.org/abs/2510.03613)
*Meenakshi Manikandan,Leilani Gilpin*

Main category: cs.LG

TL;DR: Hill-ADAM是一种专注于逃离局部最小值以寻找全局最小值的优化器，通过确定性探索状态空间来避免随机梯度更新的不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决ADAM优化器在逃离局部最小值方面的局限性，提高找到全局最优解的能力。

Method: 通过分析ADAM优化器步长的解析近似，提出Hill-ADAM算法，在误差最小化和最大化之间交替进行，最大化用于逃离局部最小值，最小化用于收敛。

Result: 在5个损失函数和12个图像颜色校正实例上进行了测试，验证了算法的有效性。

Conclusion: Hill-ADAM通过交替最小化和最大化的策略，能够有效探索损失空间并找到全局最小值。

Abstract: This paper introduces Hill-ADAM. Hill-ADAM is an optimizer with its focus
towards escaping local minima in prescribed loss landscapes to find the global
minimum. Hill-ADAM escapes minima by deterministically exploring the state
space. This eliminates uncertainty from random gradient updates in stochastic
algorithms while seldom converging at the first minimum that visits. In the
paper we first derive an analytical approximation of the ADAM Optimizer step
size at a particular model state. From there define the primary condition
determining ADAM limitations in escaping local minima. The proposed optimizer
algorithm Hill-ADAM alternates between error minimization and maximization. It
maximizes to escape the local minimum and minimizes again afterward. This
alternation provides an overall exploration throughout the loss space. This
allows the deduction of the global minimum's state. Hill-ADAM was tested with 5
loss functions and 12 amber-saturated to cooler-shade image color correction
instances.

</details>


### [81] [QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks](https://arxiv.org/abs/2510.03276)
*Qian Chen,Linxin Yang,Akang Wang,Xiaodong Luo,Yin Zhang*

Main category: cs.LG

TL;DR: 提出了一种轻量级二次增强器，通过引入二次变换来增加神经网络的非线性，同时使用低秩、权重共享和稀疏化技术来减少参数和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 通过引入二次变换来增强现有神经网络架构的非线性能力，从而提高模型性能。

Method: 使用低秩性、权重共享和稀疏化技术设计轻量级二次增强器，在每层特征之间引入二次交互，仅增加少量额外参数和计算量。

Result: 在图像分类、文本分类和大语言模型微调三个任务上的概念验证实验均显示出明显且显著的性能提升。

Conclusion: 所提出的轻量级二次增强方法能够有效提升现有神经网络架构的性能，同时保持较低的参数和计算复杂度。

Abstract: The combination of linear transformations and non-linear activation functions
forms the foundation of most modern deep neural networks, enabling them to
approximate highly complex functions. This paper explores the introduction of
quadratic transformations to further increase nonlinearity in neural networks,
with the aim of enhancing the performance of existing architectures. To reduce
parameter complexity and computational complexity, we propose a lightweight
quadratic enhancer that uses low-rankness, weight sharing, and sparsification
techniques. For a fixed architecture, the proposed approach introduces
quadratic interactions between features at every layer, while only adding
negligible amounts of additional model parameters and forward computations. We
conduct a set of proof-of-concept experiments for the proposed method across
three tasks: image classification, text classification, and fine-tuning
large-language models. In all tasks, the proposed approach demonstrates clear
and substantial performance gains.

</details>


### [82] [Neural Bayesian Filtering](https://arxiv.org/abs/2510.03614)
*Christopher Solinas,Radovan Haluska,David Sychrovsky,Finbarr Timbers,Nolan Bard,Michael Buro,Martin Schmid,Nathan R. Sturtevant,Michael Bowling*

Main category: cs.LG

TL;DR: 提出神经贝叶斯滤波算法，在部分可观测系统中维护隐藏状态的分布，结合经典滤波器的计算效率和深度生成模型的表达能力


<details>
  <summary>Details</summary>
Motivation: 解决部分可观测系统中信念跟踪的问题，传统滤波器表达能力有限，而粒子滤波器容易遭遇粒子贫乏问题

Method: 训练算法找到任务诱导信念的良好潜在表示，将信念映射为固定长度的嵌入向量，在嵌入空间中使用粒子式更新计算后验分布

Result: 在三个部分可观测环境的状态估计任务中验证了NBF的有效性

Conclusion: NBF能够跟踪快速变化的多模态信念，同时减轻粒子贫乏的风险

Abstract: We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.

</details>


### [83] [Quantifying constraint hierarchies in Bayesian PINNs via per-constraint Hessian decomposition](https://arxiv.org/abs/2510.03278)
*Filip Landgren*

Main category: cs.LG

TL;DR: 提出了一个可扩展的矩阵自由拉普拉斯框架，用于分解贝叶斯物理信息神经网络中物理约束对后验Hessian的贡献，量化各约束对损失景观的相对影响。


<details>
  <summary>Details</summary>
Motivation: 需要澄清单个物理约束如何影响贝叶斯物理信息神经网络，因为物理约束可能导致网络过度自信，但这种过度自信可能反映了约束带来的合理精度而非校准错误。

Method: 引入可扩展的矩阵自由拉普拉斯框架，将后验Hessian分解为每个约束的贡献，并提供量化指标来衡量它们在损失景观中的相对影响力。

Result: 应用于Van der Pol方程时，该方法能够追踪约束如何塑造网络的几何结构，并通过Hessian显示单个损失权重的变化如何非平凡地重新分布曲率和有效主导地位。

Conclusion: 该方法为理解物理约束在贝叶斯物理信息神经网络中的影响提供了新的分析工具，能够揭示约束对网络几何形状和不确定性的具体作用机制。

Abstract: Bayesian physics-informed neural networks (B-PINNs) merge data with governing
equations to solve differential equations under uncertainty. However,
interpreting uncertainty and overconfidence in B-PINNs requires care due to the
poorly understood effects the physical constraints have on the network;
overconfidence could reflect warranted precision, enforced by the constraints,
rather than miscalibration. Motivated by the need to further clarify how
individual physical constraints shape these networks, we introduce a scalable,
matrix-free Laplace framework that decomposes the posterior Hessian into
contributions from each constraint and provides metrics to quantify their
relative influence on the loss landscape. Applied to the Van der Pol equation,
our method tracks how constraints sculpt the network's geometry and shows,
directly through the Hessian, how changing a single loss weight non-trivially
redistributes curvature and effective dominance across the others.

</details>


### [84] [Implicit Models: Expressive Power Scales with Test-Time Compute](https://arxiv.org/abs/2510.03638)
*Jialin Liu,Lisang Ding,Stanley Osher,Wotao Yin*

Main category: cs.LG

TL;DR: 隐式模型通过单参数块迭代到固定点计算输出，实现无限深度、权重绑定的网络，训练时内存需求低。研究表明这些紧凑模型通过增加测试时计算量，可以匹配甚至超越更大的显式网络。


<details>
  <summary>Details</summary>
Motivation: 理解隐式模型如何通过增加测试时计算来提升表达能力的内在机制，目前这方面研究还很缺乏。

Method: 通过非参数化分析研究表达能力，证明简单的隐式算子通过迭代可以逐步表达更复杂的映射，使模型表达能力随测试时计算量扩展。

Result: 理论分析表明隐式模型的表达能力可以随测试时迭代次数增加而扩展，最终匹配更丰富的函数类。在图像重建、科学计算和运筹学三个领域的实验验证了这一发现。

Conclusion: 隐式模型通过测试时迭代实现了表达能力与计算量的可扩展性，在保持紧凑架构的同时能够表达复杂映射，同时提高解的质量和稳定性。

Abstract: Implicit models, an emerging model class, compute outputs by iterating a
single parameter block to a fixed point. This architecture realizes an
infinite-depth, weight-tied network that trains with constant memory,
significantly reducing memory needs for the same level of performance compared
to explicit models. While it is empirically known that these compact models can
often match or even exceed larger explicit networks by allocating more
test-time compute, the underlying mechanism remains poorly understood.
  We study this gap through a nonparametric analysis of expressive power. We
provide a strict mathematical characterization, showing that a simple and
regular implicit operator can, through iteration, progressively express more
complex mappings. We prove that for a broad class of implicit models, this
process lets the model's expressive power scale with test-time compute,
ultimately matching a much richer function class. The theory is validated
across three domains: image reconstruction, scientific computing, and
operations research, demonstrating that as test-time iterations increase, the
complexity of the learned mapping rises, while the solution quality
simultaneously improves and stabilizes.

</details>


### [85] [MemMamba: Rethinking Memory Patterns in State Space Model](https://arxiv.org/abs/2510.03279)
*Youjin Wang,Yangjingyi Chen,Jiahao Yan,Jiaxuan Lu,Xiao Sun*

Main category: cs.LG

TL;DR: MemMamba通过状态总结机制和跨层跨token注意力，解决了Mamba模型的长程记忆衰减问题，在保持线性复杂度的同时显著提升了长序列建模性能。


<details>
  <summary>Details</summary>
Motivation: 现有长序列建模方法存在效率与内存的权衡：RNN有梯度问题，Transformer有二次复杂度，Mamba虽然高效但长程记忆呈指数衰减。需要解决Mamba的长程遗忘问题。

Method: 提出MemMamba框架，集成状态总结机制以及跨层和跨token注意力，灵感来自人类阅读长文档时提炼和保留关键信息的方式。

Result: 在PG19和Passkey Retrieval等长序列基准测试中显著优于现有Mamba变体和Transformer，推理效率提升48%。

Conclusion: MemMamba在复杂度-内存权衡上取得突破，为超长序列建模提供了新范式。

Abstract: With the explosive growth of data, long-sequence modeling has become
increasingly important in tasks such as natural language processing and
bioinformatics. However, existing methods face inherent trade-offs between
efficiency and memory. Recurrent neural networks suffer from gradient vanishing
and explosion, making them hard to scale. Transformers can model global
dependencies but are constrained by quadratic complexity. Recently, selective
state-space models such as Mamba have demonstrated high efficiency with O(n)
time and O(1) recurrent inference, yet their long-range memory decays
exponentially. In this work, we conduct mathematical derivations and
information-theoretic analysis to systematically uncover the memory decay
mechanism of Mamba, answering a fundamental question: what is the nature of
Mamba's long-range memory and how does it retain information? To quantify key
information loss, we further introduce horizontal-vertical memory fidelity
metrics that capture degradation both within and across layers. Inspired by how
humans distill and retain salient information when reading long documents, we
propose MemMamba, a novel architectural framework that integrates state
summarization mechanism together with cross-layer and cross-token attention,
which alleviates long-range forgetting while preserving linear complexity.
MemMamba achieves significant improvements over existing Mamba variants and
Transformers on long-sequence benchmarks such as PG19 and Passkey Retrieval,
while delivering a 48% speedup in inference efficiency. Both theoretical
analysis and empirical results demonstrate that MemMamba achieves a
breakthrough in the complexity-memory trade-off, offering a new paradigm for
ultra-long sequence modeling.

</details>


### [86] [Does higher interpretability imply better utility? A Pairwise Analysis on Sparse Autoencoders](https://arxiv.org/abs/2510.03659)
*Xu Wang,Yan Hu,Benyou Wang,Difan Zou*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器(SAE)的可解释性与模型行为控制效用之间仅存在弱正相关，提出新的特征选择标准Delta Token Confidence，显著提升了控制性能，并揭示最有效的控制特征的可解释性与效用之间存在分歧。


<details>
  <summary>Details</summary>
Motivation: 验证稀疏自编码器(SAE)的可解释性是否确实意味着更好的模型行为控制效用，因为这一基本假设尚未得到充分验证。

Method: 在三个大语言模型上训练90个SAE，涵盖5种架构和6种稀疏度水平，使用SAEBench和AxBench分别评估可解释性和控制效用，并通过Kendall秩相关系数进行秩一致性分析。提出Delta Token Confidence特征选择标准来衡量特征放大对下一个token分布的影响。

Result: 可解释性与控制效用之间仅存在弱正相关(tau b ≈ 0.298)；使用Delta Token Confidence选择特征后，控制性能比当前最佳输出分数标准提升了52.52%；选择高Delta Token Confidence特征后，可解释性与效用之间的相关性消失甚至变为负相关。

Conclusion: 可解释性不能作为控制性能的充分代理指标，最有效的控制特征的可解释性与效用之间存在分歧，Delta Token Confidence是更有效的特征选择标准。

Abstract: Sparse Autoencoders (SAEs) are widely used to steer large language models
(LLMs), based on the assumption that their interpretable features naturally
enable effective model behavior steering. Yet, a fundamental question remains
unanswered: does higher interpretability indeed imply better steering utility?
To answer this question, we train 90 SAEs across three LLMs (Gemma-2-2B,
Qwen-2.5-3B, Gemma-2-9B), spanning five architectures and six sparsity levels,
and evaluate their interpretability and steering utility based on SAEBench
(arXiv:2501.12345) and AxBench (arXiv:2502.23456) respectively, and perform a
rank-agreement analysis via Kendall's rank coefficients (tau b). Our analysis
reveals only a relatively weak positive association (tau b approx 0.298),
indicating that interpretability is an insufficient proxy for steering
performance. We conjecture the interpretability utility gap may stem from the
selection of SAE features, as not all of them are equally effective for
steering. To further find features that truly steer the behavior of LLMs, we
propose a novel selection criterion called Delta Token Confidence, which
measures how much amplifying a feature changes the next token distribution. We
show that our method improves the steering performance of three LLMs by 52.52
percent compared to the current best output score based criterion
(arXiv:2503.34567). Strikingly, after selecting features with high Delta Token
Confidence, the correlation between interpretability and utility vanishes (tau
b approx 0), and can even become negative. This further highlights the
divergence between interpretability and utility for the most effective steering
features.

</details>


### [87] [Training Optimal Large Diffusion Language Models](https://arxiv.org/abs/2510.03280)
*Jinjie Ni,Qian Liu,Chao Du,Longxu Dou,Hang Yan,Zili Wang,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: Quokka是首个针对扩散语言模型的系统化缩放定律，涵盖计算受限和数据受限两种场景，研究关键建模和优化设计。


<details>
  <summary>Details</summary>
Motivation: 为扩散语言模型训练提供短期实践指导，并为整个AI社区带来长期启发。

Method: 建立系统化缩放定律，分析计算受限和数据受限情况下的关键建模和优化设计。

Result: 开发出Quokka缩放定律，作为Chinchilla的补充，提供更广泛的研究范围。

Conclusion: Quokka为扩散语言模型训练提供了实用的缩放指导，有望推动该领域的发展。

Abstract: We introduce Quokka, the first systematic scaling law for diffusion language
models (DLMs), encompassing both compute-constrained and data-constrained
regimes, and studying the key modeling and optimization designs. Quokka is a
good friend of Chinchilla and provides wider scopes. We hope the results would
bring short-term practical guidance in DLMs training and long-term inspirations
for the whole AI community.

</details>


### [88] [Towards Sampling Data Structures for Tensor Products in Turnstile Streams](https://arxiv.org/abs/2510.03678)
*Zhao Song,Shenghao Xie,Samson Zhou*

Main category: cs.LG

TL;DR: 提出注意力采样器定义，利用流式设置中的重要性采样方法显著降低传统注意力机制的计算负担，并分析其理论有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模注意力模型在人工智能中的计算挑战，受经典ℓ2采样器和LLM中注意力方案进展的启发。

Method: 提出注意力采样器定义，在流式设置中使用重要性采样方法，分析空间和更新时间等理论特性。

Result: 显著降低传统注意力机制的计算负担，框架具有可扩展性和跨模型架构及领域的广泛适用性。

Conclusion: 注意力采样器为大规模注意力模型提供有效的计算优化方案，具有理论保证和实际应用价值。

Abstract: This paper studies the computational challenges of large-scale
attention-based models in artificial intelligence by utilizing importance
sampling methods in the streaming setting. Inspired by the classical definition
of the $\ell_2$ sampler and the recent progress of the attention scheme in
Large Language Models (LLMs), we propose the definition of the attention
sampler. Our approach significantly reduces the computational burden of
traditional attention mechanisms. We analyze the effectiveness of the attention
sampler from a theoretical perspective, including space and update time.
Additionally, our framework exhibits scalability and broad applicability across
various model architectures and domains.

</details>


### [89] [Discovering Transformer Circuits via a Hybrid Attribution and Pruning Framework](https://arxiv.org/abs/2510.03282)
*Hao Gu,Vibhas Nair,Amrithaa Ashok Kumar,Jayvart Sharma,Ryan Lagasse*

Main category: cs.LG

TL;DR: 提出混合归因和剪枝框架，在保持电路忠实度的同时提升46%的计算效率


<details>
  <summary>Details</summary>
Motivation: 现有电路发现算法面临基本权衡：归因修补速度快但不忠实，边剪枝忠实但计算昂贵

Method: 使用归因修补识别高潜力子图，然后应用边剪枝从中提取忠实电路

Result: 比基线算法快46%且不牺牲电路忠实度，在间接对象识别任务中保留合作电路组件

Conclusion: HAP是提高机械可解释性研究扩展到更大模型的有效方法

Abstract: Interpreting language models often involves circuit analysis, which aims to
identify sparse subnetworks, or circuits, that accomplish specific tasks.
Existing circuit discovery algorithms face a fundamental trade-off: attribution
patching is fast but unfaithful to the full model, while edge pruning is
faithful but computationally expensive. This research proposes a hybrid
attribution and pruning (HAP) framework that uses attribution patching to
identify a high-potential subgraph, then applies edge pruning to extract a
faithful circuit from it. We show that HAP is 46\% faster than baseline
algorithms without sacrificing circuit faithfulness. Furthermore, we present a
case study on the Indirect Object Identification task, showing that our method
preserves cooperative circuit components (e.g. S-inhibition heads) that
attribution patching methods prune at high sparsity. Our results show that HAP
could be an effective approach for improving the scalability of mechanistic
interpretability research to larger models. Our code is available at
https://anonymous.4open.science/r/HAP-circuit-discovery.

</details>


### [90] [Group Policy Gradient](https://arxiv.org/abs/2510.03679)
*Junhua Chen,Zixi Zhang,Hantao Zhong,Rika Antonova*

Main category: cs.LG

TL;DR: GPG是一种无评论家的策略梯度估计器家族，用基于组的蒙特卡洛优势估计器替代学习价值函数，在保持PPO结构的同时消除评论家训练成本。


<details>
  <summary>Details</summary>
Motivation: 受GRPO在RLHF中成功的启发，旨在消除训练评论家所需的内存、计算和超参数成本，同时保持PPO的裁剪目标结构。

Method: 使用基于组的蒙特卡洛优势估计器替代学习价值函数，保留PPO的裁剪目标结构，无需训练评论家网络。

Result: 经验证明GPG在标准基准测试中匹配或优于PPO，能更好利用并行模拟，计算资源使用效率更高。

Conclusion: GPG提供了一种高效的无评论家策略梯度方法，在保持性能的同时显著降低了计算成本。

Abstract: We introduce Group Policy Gradient (GPG), a family of critic-free
policy-gradient estimators for general MDPs. Inspired by the success of GRPO's
approach in Reinforcement Learning from Human Feedback (RLHF), GPG replaces a
learned value function with a group-based Monte Carlo advantage estimator,
removing the memory, compute, and hyperparameter costs of training a critic
while preserving PPO's clipped-objective structure. We prove the consistency of
the GPG estimator, analyze the bias-variance tradeoffs, and demonstrate
empirically that GPG matches or outperforms PPO on standard benchmarks. GPG
makes better use of parallel simulations, which, together with its critic-free
design, results in more efficient use of computational resources than PPO.

</details>


### [91] [MACE: A Hybrid LLM Serving System with Colocated SLO-aware Continuous Retraining Alignment](https://arxiv.org/abs/2510.03283)
*Yufei Li,Yu Fu,Yue Dong,Cong Liu*

Main category: cs.LG

TL;DR: MACE是一个混合LLM系统，通过在边缘服务器上协同调度推理和微调任务，实现迭代级别的资源分配，在保证推理延迟的同时提升模型更新频率。


<details>
  <summary>Details</summary>
Motivation: 边缘服务器上的LLM应用需要频繁重训练以适应非稳态用户数据，但现有方法在推理延迟和模型准确性之间存在矛盾，要么延迟模型更新，要么过度分配资源给重训练。

Method: 提出MACE系统，将推理（预填充、解码）和微调任务协同部署，采用智能内存管理，根据模型更新对输出对齐的影响程度分配GPU周期，实现迭代级别的混合调度。

Result: MACE在保持吞吐量的同时将推理延迟降低高达63%，在NVIDIA AGX Orin上维持GPU利用率超过85%，优于连续重训练和周期性重训练方法。

Conclusion: 迭代级别的混合调度是在边缘平台上部署具有持续学习能力的LLM的有前景方向，能够平衡吞吐量、延迟和更新新鲜度。

Abstract: Large language models (LLMs) deployed on edge servers are increasingly used
in latency-sensitive applications such as personalized assistants,
recommendation, and content moderation. However, the non-stationary nature of
user data necessitates frequent retraining, which introduces a fundamental
tension between inference latency and model accuracy under constrained GPU
resources. Existing retraining strategies either delay model updates,
over-commit resources to retraining, or overlook iteration-level retraining
granularity. In this paper, we identify that iteration-level scheduling is
crucial for adapting retraining frequency to model drift without violating
service-level objectives (SLOs). We propose MACE, a hybrid LLM system that
colocates concurrent inference (prefill, decode) and fine-tuning, with
intelligent memory management to maximize task performance while promising
inference throughput. MACE leverages the insight that not all model updates
equally affect output alignment and allocates GPU cycles accordingly to balance
throughput, latency, and update freshness. Our trace-driven evaluation shows
that MACE matches or exceeds continuous retraining while reducing inference
latency by up to 63% and maintaining throughput under resource constraints.
Compared to periodic retraining, MACE improves latency breakdown across
prefill, decode, and finetune stages, and sustains GPU utilization above 85% in
NVIDIA AGX Orin. These results demonstrate that iteration-level hybrid
scheduling is a promising direction for deploying LLMs with continual learning
capabilities on edge platforms.

</details>


### [92] [From Moments to Models: Graphon Mixture-Aware Mixup and Contrastive Learning](https://arxiv.org/abs/2510.03690)
*Ali Azizpour,Reza Ramezanpour,Ashutosh Sabharwal,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出了一个统一框架，显式建模图数据为图混合模型，通过图矩聚类识别不同生成机制，改进了图对比学习和数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界图数据集通常包含来自多个不同分布的混合群体，但现有图表示学习方法往往忽略这种混合结构。

Method: 利用图矩（模体密度）聚类来自同一模型的图，提出图混合感知的Mixup增强方法和模型自适应图对比学习框架。

Result: 在无监督学习中达到SOTA，在8个数据集中平均排名第一；在有监督学习中，在7个数据集中的6个获得新的SOTA准确率。

Conclusion: 显式建模图数据的混合结构能够有效改进图表示学习性能，为图对比学习和数据增强提供了理论基础和实践指导。

Abstract: Real-world graph datasets often consist of mixtures of populations, where
graphs are generated from multiple distinct underlying distributions. However,
modern representation learning approaches, such as graph contrastive learning
(GCL) and augmentation methods like Mixup, typically overlook this mixture
structure. In this work, we propose a unified framework that explicitly models
data as a mixture of underlying probabilistic graph generative models
represented by graphons. To characterize these graphons, we leverage graph
moments (motif densities) to cluster graphs arising from the same model. This
enables us to disentangle the mixture components and identify their distinct
generative mechanisms. This model-aware partitioning benefits two key graph
learning tasks: 1) It enables a graphon-mixture-aware mixup (GMAM), a data
augmentation technique that interpolates in a semantically valid space guided
by the estimated graphons, instead of assuming a single graphon per class. 2)
For GCL, it enables model-adaptive and principled augmentations. Additionally,
by introducing a new model-aware objective, our proposed approach (termed MGCL)
improves negative sampling by restricting negatives to graphs from other
models. We establish a key theoretical guarantee: a novel, tighter bound
showing that graphs sampled from graphons with small cut distance will have
similar motif densities with high probability. Extensive experiments on
benchmark datasets demonstrate strong empirical performance. In unsupervised
learning, MGCL achieves state-of-the-art results, obtaining the top average
rank across eight datasets. In supervised learning, GMAM consistently
outperforms existing strategies, achieving new state-of-the-art accuracy in 6
out of 7 datasets.

</details>


### [93] [Edge-FIT: Federated Instruction Tuning of Quantized LLMs for Privacy-Preserving Smart Home Environments](https://arxiv.org/abs/2510.03284)
*Vinay Venkatesh,Vamsidhar R Kamanuru,Lav Kumar,Nikita Kothari*

Main category: cs.LG

TL;DR: Edge-FIT是一个用于在边缘设备上联邦指令调优LLM的可扩展框架，通过结合联邦学习和4位量化低秩适应来解决传统联邦学习方法在LLM上的通信和计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习方法如FedAvg在面对LLM的巨大参数量时会失效，需要解决通信和计算开销问题以实现边缘设备上的LLM部署。

Method: 结合联邦学习和4位量化低秩适应(QLORA)，使用过滤后的Databricks Dolly 15k数据集进行IoT领域的指令调优。

Result: Edge-FIT调优的Llama 2(7B)模型达到F1分数0.89，并在3.8B Phi-3-mini模型上验证了可行的权衡。

Conclusion: Edge-FIT是一个可扩展的框架，适用于在家庭计算网关上实现去中心化的LLM部署。

Abstract: This paper proposes Edge-FIT (Federated Instruction Tuning on the Edge), a
scalable framework for Federated Instruction Tuning (FIT) of Large Language
Models (LLMs). Traditional Federated Learning (TFL) methods, like FedAvg, fail
when confronted with the massive parameter size of LLMs [3], [6]. Our Edge-FIT
framework combines federated learning with 4-bit Quantized Low-Rank Adaptation
(QLORA), mitigating the core issues of communication and computational
overhead. We demonstrate this by filtering the general-purpose Databricks Dolly
15k dataset for the IoT domain. Experimental results show the Edge-FIT tuned
Llama 2(7B) achieves an F1-Score of 0.89. We also demonstrate a viable
trade-off using the 3.8B Phi-3-mini model, validating Edge-FIT as a scalable
framework for decentralized LLM deployment on home compute gateways.

</details>


### [94] [Balancing Interpretability and Performance in Reinforcement Learning: An Adaptive Spectral Based Linear Approach](https://arxiv.org/abs/2510.03722)
*Qianxin Yi,Shao-Bo Lin,Jun Fan,Yao Wang*

Main category: cs.LG

TL;DR: 提出了一种基于谱滤波的线性强化学习方法，通过自适应正则化参数选择策略，在保持可解释性的同时提升决策性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注性能，依赖事后解释来提供可解释性。本文旨在设计一种以可解释性为导向但性能增强的RL方法。

Method: 扩展基于岭回归的方法，通过谱滤波函数设计自适应正则化参数选择策略，基于偏差-方差权衡原则。

Result: 理论分析建立了参数估计和泛化误差的近似最优界。在快手和淘宝的真实数据集上，该方法在决策质量上优于或匹配现有基线方法。

Conclusion: 该方法有潜力弥合RL理论与实际决策之间的差距，在管理场景中提供可解释性、准确性和适应性。

Abstract: Reinforcement learning (RL) has been widely applied to sequential decision
making, where interpretability and performance are both critical for practical
adoption. Current approaches typically focus on performance and rely on post
hoc explanations to account for interpretability. Different from these
approaches, we focus on designing an interpretability-oriented yet
performance-enhanced RL approach. Specifically, we propose a spectral based
linear RL method that extends the ridge regression-based approach through a
spectral filter function. The proposed method clarifies the role of
regularization in controlling estimation error and further enables the design
of an adaptive regularization parameter selection strategy guided by the
bias-variance trade-off principle. Theoretical analysis establishes
near-optimal bounds for both parameter estimation and generalization error.
Extensive experiments on simulated environments and real-world datasets from
Kuaishou and Taobao demonstrate that our method either outperforms or matches
existing baselines in decision quality. We also conduct interpretability
analyses to illustrate how the learned policies make decisions, thereby
enhancing user trust. These results highlight the potential of our approach to
bridge the gap between RL theory and practical decision making, providing
interpretability, accuracy, and adaptability in management contexts.

</details>


### [95] [LogAction: Consistent Cross-system Anomaly Detection through Logs via Active Domain](https://arxiv.org/abs/2510.03288)
*Chiming Duan,Minghua He,Pei Xiao,Tong Jia,Xin Zhang,Zhewei Zhong,Xiang Luo,Yan Niu,Lingzhe Zhang,Yifan Wu,Siyu Yu,Weijie Hong,Ying Li,Gang Huang*

Main category: cs.LG

TL;DR: LogAction是一个基于主动领域自适应的日志异常检测模型，结合迁移学习和主动学习，仅需2%人工标注就能达到93.01%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有日志异常检测方法严重依赖标注，但大规模日志标注成本高昂。迁移学习和主动学习方法存在源-目标系统数据分布差异和冷启动问题。

Method: 使用成熟系统的标注数据训练基础模型解决冷启动问题，采用基于自由能和不确定性的采样方法选择分布边界日志进行人工标注，以最小化标注成本解决数据分布差异。

Result: 在六个不同数据集组合上的实验显示，LogAction仅用2%人工标注就达到平均93.01%的F1分数，比现有最优方法提升26.28%。

Conclusion: LogAction通过主动领域自适应有效解决了日志异常检测中的标注成本和数据分布差异问题，实现了高性能的异常检测。

Abstract: Log-based anomaly detection is a essential task for ensuring the reliability
and performance of software systems. However, the performance of existing
anomaly detection methods heavily relies on labeling, while labeling a large
volume of logs is highly challenging. To address this issue, many approaches
based on transfer learning and active learning have been proposed.
Nevertheless, their effectiveness is hindered by issues such as the gap between
source and target system data distributions and cold-start problems. In this
paper, we propose LogAction, a novel log-based anomaly detection model based on
active domain adaptation. LogAction integrates transfer learning and active
learning techniques. On one hand, it uses labeled data from a mature system to
train a base model, mitigating the cold-start issue in active learning. On the
other hand, LogAction utilize free energy-based sampling and uncertainty-based
sampling to select logs located at the distribution boundaries for manual
labeling, thus addresses the data distribution gap in transfer learning with
minimal human labeling efforts. Experimental results on six different
combinations of datasets demonstrate that LogAction achieves an average 93.01%
F1 score with only 2% of manual labels, outperforming some state-of-the-art
methods by 26.28%. Website: https://logaction.github.io

</details>


### [96] [Cost Efficient Fairness Audit Under Partial Feedback](https://arxiv.org/abs/2510.03734)
*Nirjhar Das,Mohit Sharma,Praharsh Nanavati,Kirankumar Shiragur,Amit Deshpande*

Main category: cs.LG

TL;DR: 提出了一种在部分反馈下（仅观察正分类个体的真实标签）进行公平性审计的新方法，引入更符合现实成本的成本模型，设计了在两种设置下的最优审计算法，显著降低了审计成本。


<details>
  <summary>Details</summary>
Motivation: 现实世界中很多场景下只能观察到正分类个体的真实标签（如贷款申请中只有获批者的还款结果），而传统公平性审计需要完整标签数据，这在实际中成本高昂。

Method: 在两种设置下设计算法：黑盒模型（无数据分布假设）和混合模型（特征和标签遵循指数族分布混合）。利用截断样本学习和最大后验概率预言机，将球面高斯混合结果扩展到指数族混合。

Result: 在黑盒设置下提出接近最优的审计算法，证明自然基线方法严格次优；在混合模型设置下设计的算法审计成本显著低于黑盒情况。在真实数据集上比自然基线节省约50%的审计成本。

Conclusion: 所提出的公平性审计算法在部分反馈场景下显著提高了成本效益，适用于多种公平性指标，为现实世界中的公平性评估提供了实用解决方案。

Abstract: We study the problem of auditing the fairness of a given classifier under
partial feedback, where true labels are available only for positively
classified individuals, (e.g., loan repayment outcomes are observed only for
approved applicants). We introduce a novel cost model for acquiring additional
labeled data, designed to more accurately reflect real-world costs such as
credit assessment, loan processing, and potential defaults. Our goal is to find
optimal fairness audit algorithms that are more cost-effective than random
exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no
assumptions on the data distribution, and a mixture model, where features and
true labels follow a mixture of exponential family distributions. In the
black-box setting, we propose a near-optimal auditing algorithm under mild
assumptions and show that a natural baseline can be strictly suboptimal. In the
mixture model setting, we design a novel algorithm that achieves significantly
lower audit cost than the black-box case. Our approach leverages prior work on
learning from truncated samples and maximum-a-posteriori oracles, and extends
known results on spherical Gaussian mixtures to handle exponential family
mixtures, which may be of independent interest. Moreover, our algorithms apply
to popular fairness metrics including demographic parity, equal opportunity,
and equalized odds. Empirically, we demonstrate strong performance of our
algorithms on real-world fair classification datasets like Adult Income and Law
School, consistently outperforming natural baselines by around 50% in terms of
audit cost.

</details>


### [97] [Why mask diffusion does not work](https://arxiv.org/abs/2510.03289)
*Haocheng Sun,Cynthia Xin Wen,Edward Hong Wang*

Main category: cs.LG

TL;DR: 本文分析了掩码扩散语言模型在实现并行生成和双向注意力方面的固有困难，并提出了最有效的训练和推理策略。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型相比自回归模型具有并行生成和双向注意力的优势，但现有的开源掩码扩散模型主要基于吸收扩散变体，存在实现这些优势的困难。

Method: 分析了掩码扩散模型在并行生成和双向注意力方面的固有困难，并提出了改进的训练和推理策略。

Result: 揭示了掩码扩散模型在实现并行生成和双向注意力方面的局限性。

Conclusion: 掩码扩散模型在实现并行生成和双向注意力方面存在固有困难，需要采用更有效的训练和推理策略来克服这些限制。

Abstract: The main advantages of diffusion language models over autoregressive (AR)
models lie in their ability to support parallel generation and bidirectional
attention, enabling a more controllable generation process. In recent years,
open-source mask diffusion language models have emerged, most of which are
based on a variant known as absorbing diffusion. However, this paper
demonstrates why mask diffusion faces inherent difficulties in achieving
parallel generation and bidirectional attention. We also propose the most
effective training and inference strategies for mask diffusion.

</details>


### [98] [Allocation of Parameters in Transformers](https://arxiv.org/abs/2510.03784)
*Ruoxi Yu,Haotian Jiang,Jingpu Cheng,Penghao Yu,Qianxiao Li,Zhong Li*

Main category: cs.LG

TL;DR: 该论文从理论角度分析了Transformer模型中注意力头和头维度在不同层间的分配策略，揭示了softmax激活的饱和行为，并提出了平衡表达能力和效率的参数分配原则。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在各种应用中取得了显著成功，但其模型效率的理论基础仍未被充分探索。本文旨在研究模型参数（主要是注意力头和头维度）如何在各层间分配以平衡表达能力和效率。

Method: 首先从近似角度对早期层在信息提取中的作用进行数学分析，理论刻画了在固定参数预算下注意力头数量和头维度之间的权衡关系。此外，发现并证明了softmax激活的饱和行为：持续增加头维度会导致学习误差的收益递减，尤其对于长序列。

Result: 理论和实验都支持饱和模式的存在，表明后续层可以用更少的参数更高效地运行。基于这些见解，提出了在Transformer各层间分配注意力头和维度的原则性策略。

Conclusion: 本文揭示了Transformer架构的理论基础模型效率，为设计更高效的Transformer模型提供了理论指导，特别是在参数分配策略方面。

Abstract: Transformers have achieved remarkable successes across a wide range of
applications, yet the theoretical foundation of their model efficiency remains
underexplored. In this work, we investigate how the model parameters -- mainly
attention heads and head dimensions -- should be allocated across layers to
balance expressivity and efficiency. We first provide mathematical analysis on
the role of early layers in information extraction from an approximation
perspective, with a theoretical characterization on the trade-off between the
number of heads and head dimension under a fixed parameter budget. In addition,
we uncover and prove the \emph{saturation} behavior of softmax activations:
Continuously increasing head dimensions can lead to diminishing returns in
learning errors, particularly for long sequences. Supported by both theory and
experiments, this saturation pattern suggests that later layers can operate
more efficiently with reduced parameters. Combining these insights, we propose
principled strategies for allocating attention heads and dimensions across
Transformers' layers, shedding light on theoretically-grounded model efficiency
of Transformer-based architectures.

</details>


### [99] [Single-Core Superscalar Optimization of Clifford Neural Layers](https://arxiv.org/abs/2510.03290)
*X. Angelo Huang,Ruben Ciranni,Giovanni Spadaccini,Carla J. López Zurita*

Main category: cs.LG

TL;DR: 本文分析了Clifford卷积层的内部计算结构，提出了多项优化方法来加速推理过程，同时保持正确性，最终实现了平均21.35倍的加速效果。


<details>
  <summary>Details</summary>
Motivation: 随着物理科学中对具有等变性特性的网络兴趣日益增长，Clifford神经网络层作为实现E(n)和O(n)等变性的一种方法备受关注。本文旨在优化其计算效率。

Method: 首先分析Clifford代数的理论基础以消除冗余矩阵分配和计算，然后系统性地应用已建立的优化技术来进一步提升性能。

Result: 在11个函数上实现了平均21.35倍的加速，在6个案例中运行时间与原始PyTorch实现相当或更快，其余案例中性能与原始库保持同一数量级。

Conclusion: 通过理论分析和系统优化，成功显著提升了Clifford卷积层的推理效率，同时保持了计算正确性。

Abstract: Within the growing interest in the physical sciences in developing networks
with equivariance properties, Clifford neural layers shine as one approach that
delivers $E(n)$ and $O(n)$ equivariances given specific group actions. In this
paper, we analyze the inner structure of the computation within Clifford
convolutional layers and propose and implement several optimizations to speed
up the inference process while maintaining correctness. In particular, we begin
by analyzing the theoretical foundations of Clifford algebras to eliminate
redundant matrix allocations and computations, then systematically apply
established optimization techniques to enhance performance further. We report a
final average speedup of 21.35x over the baseline implementation of eleven
functions and runtimes comparable to and faster than the original PyTorch
implementation in six cases. In the remaining cases, we achieve performance in
the same order of magnitude as the original library.

</details>


### [100] [Robust Batched Bandits](https://arxiv.org/abs/2510.03798)
*Yunwen Guo,Yunlun Shu,Gongyi Zhuo,Tianyu Wang*

Main category: cs.LG

TL;DR: 本文针对重尾奖励分布的批处理多臂老虎机问题，提出了鲁棒算法，发现在实例无关和Lipschitz设置中，重尾奖励需要更少的批次，而在实例相关设置中批次需求不变。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要假设轻尾奖励分布，但许多现实场景（如临床试验结果）表现出重尾特征，需要填补这一研究空白。

Method: 提出了适用于重尾奖励的鲁棒批处理老虎机算法，涵盖有限臂和Lipschitz连续设置。

Result: 揭示了一个令人惊讶的现象：在实例无关和Lipschitz设置中，重尾奖励需要更少的批次来实现接近最优的遗憾；而在实例相关设置中，所需批次数量不随尾部厚重程度变化。

Conclusion: 重尾奖励分布对批处理老虎机算法的影响因设置而异，在实例无关和Lipschitz设置中可减少批次需求，但在实例相关设置中保持稳定。

Abstract: The batched multi-armed bandit (MAB) problem, in which rewards are collected
in batches, is crucial for applications such as clinical trials. Existing
research predominantly assumes light-tailed reward distributions, yet many
real-world scenarios, including clinical outcomes, exhibit heavy-tailed
characteristics. This paper bridges this gap by proposing robust batched bandit
algorithms designed for heavy-tailed rewards, within both finite-arm and
Lipschitz-continuous settings. We reveal a surprising phenomenon: in the
instance-independent regime, as well as in the Lipschitz setting,
heavier-tailed rewards necessitate a smaller number of batches to achieve
near-optimal regret. In stark contrast, for the instance-dependent setting, the
required number of batches to attain near-optimal regret remains invariant with
respect to tail heaviness.

</details>


### [101] [UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs](https://arxiv.org/abs/2510.03291)
*Yizhuo Ding,Wanying Qu,Jiawei Geng,Wenqi Shao,Yanwei Fu*

Main category: cs.LG

TL;DR: UniPruning是一个统一的后训练剪枝框架，结合了局部显著性度量的速度和全局协调的稳定性，通过镜像下降优化实现，无需更新模型权重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临高昂的计算和内存成本，现有剪枝方法难以平衡效率和鲁棒性：局部方法在高稀疏度下容易崩溃，全局方法需要昂贵的权重更新或限制半结构化格式。

Method: 使用快速层间评分和轻量级全局控制器分配单一稀疏度预算，支持非结构化和半结构化N:M剪枝，通过镜像下降优化实现全局协调。

Result: 在多个预训练LLM家族和标准基准测试中，UniPruning持续提供有竞争力或更优的困惑度和零样本准确率。

Conclusion: UniPruning为大规模LLM稀疏化提供了一个高效、原则性和可扩展的解决方案。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks
but face prohibitive computational and memory costs. Pruning offers a promising
path by inducing sparsity while preserving architectural flexibility. However,
existing methods struggle to balance efficiency and robustness: local metric
approaches prune layer by layer but often collapse under high sparsity, whereas
global feedback methods enforce consistency at the cost of expensive weight
updates or restrictive semi-structured formats. We present UniPruning, a
unified post-training pruning framework that combines the speed of local
saliency metrics with the stability of global coordination, enabled by a mirror
descent based optimization, all without updating model weights. UniPruning
leverages fast layer-wise scoring and a lightweight global controller to
allocate a single sparsity budget, supporting both unstructured and
semi-structured N :M pruning within one framework. After a brief calibration,
it can generate pruning masks for arbitrary sparsity levels in one shot, and
adapts seamlessly to hardware-aware constraints. Extensive experiments on
multiple pretrained LLM families and standard benchmarks show that UniPruning
consistently delivers competitive or superior perplexity and zero-shot
accuracy. Ablation studies further highlight the importance of mirror descent
and local saliency anchoring. Overall, UniPruning provides an efficient,
principled, and scalable solution for sparsifying large-scale LLMs. Our code is
available at: https://github.com/RainbowQTT/UniPruning.

</details>


### [102] [TROLL: Trust Regions improve Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2510.03817)
*Philipp Becker,Niklas Freymuth,Serge Thilges,Fabian Otto,Gerhard Neumann*

Main category: cs.LG

TL;DR: TROLL方法用离散可微信任域投影替代PPO的剪裁机制，为LLM奖励微调提供原则性的token级KL约束，在训练速度、稳定性和成功率方面均优于PPO剪裁。


<details>
  <summary>Details</summary>
Motivation: PPO剪裁机制作为KL信任域的粗略近似，常导致不稳定更新和次优性能，需要更原则性的替代方案。

Method: 提出离散可微信任域投影，在模型最重要的token logits稀疏子集上操作，平衡计算成本和投影效果，作为PPO剪裁的直接替代。

Result: 在不同数据集、模型家族和优势估计方法下，TROLL在训练速度、稳定性和最终成功率方面始终优于PPO剪裁。

Conclusion: TROLL可作为PPO剪裁的直接替代，在不改变模型推理行为的前提下，提供更稳定高效的LLM奖励微调方法。

Abstract: On-policy Reinforcement Learning (RL) with PPO-like clip objectives has
become the standard choice for reward-based fine-tuning of large language
models (LLMs). Although recent work has explored improved estimators of
advantages and normalization, the clipping mechanism itself has remained
untouched. Originally introduced as a proxy for principled KL-based trust
regions, clipping is a crude approximation that often causes unstable updates
and suboptimal performance. We replace the clip objective with a novel discrete
differentiable trust region projection, which provides principled token-level
KL constraints. The projection operates on a sparse subset of the model's most
important token logits to balance computational cost and projection
effectiveness. Our approach, Trust Region Optimization for Large Language
Models (TROLL), serves as a direct replacement for PPO-like clipping during
training and does not alter the model's inference behavior. Across datasets,
model families, and advantage-estimation methods, TROLL consistently
outperforms PPO-like clipping in terms of training speed, stability, and final
success rates.

</details>


### [103] [From Score Distributions to Balance: Plug-and-Play Mixture-of-Experts Routing](https://arxiv.org/abs/2510.03293)
*Rana Shahout,Colin Cai,Yilun Du,Minlan Yu,Michael Mitzenmacher*

Main category: cs.LG

TL;DR: LASER是一种即插即用的推理时路由算法，通过平衡专家负载来提升MoE模型的推理性能，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: MoE模型通过条件路由减少训练成本，但推理时专家参数和激活占用内存，且令牌路由导致专家负载不均衡，影响系统延迟、吞吐量和成本。

Method: LASER根据门控分数分布自适应路由：当分数显示明显偏好时路由到最强专家；当分数更均匀时扩展到可行专家集并路由到负载最轻的专家。无需重新训练或微调。

Result: 在Mixtral-8x7B和DeepSeek-MoE-16b-chat上的评估显示，LASER改善了负载平衡，降低了延迟，提高了吞吐量，同时准确率变化可忽略不计。

Conclusion: LASER是一种有效的推理时路由算法，能够在不影响准确性的前提下显著提升MoE模型的推理性能。

Abstract: Mixture-of-Experts (MoE) models can scale parameter capacity by routing each
token to a subset of experts through a learned gate function. While conditional
routing reduces training costs, it shifts the burden on inference memory:
expert parameters and activations consume memory, limiting the number of
experts per device. As tokens are routed, some experts become overloaded while
others are underutilized. Because experts are mapped to GPUs, this imbalance
translates directly into degraded system performance in terms of latency,
throughput, and cost. We present LASER, a plug-and-play, inference-time routing
algorithm that balances load while preserving accuracy. LASER adapts to the
shape of the gate's score distribution. When scores provide a clear preference,
it routes to the strongest experts; when scores are more uniform, it broadens
the set of viable experts and routes to the least-loaded among them. Because
LASER relies only on gate scores from a trained model, it integrates directly
into existing MoE inference pipelines without retraining or finetuning. We
evaluate LASER on Mixtral-8x7B and DeepSeek-MoE-16b-chat across four datasets
(ARC-Easy, ARC-Challenge, MMLU, and GSM8K). LASER improves load balancing,
translating into lower latency and higher throughput, while keeping the
accuracy changes negligible.

</details>


### [104] [Proximal Diffusion Neural Sampler](https://arxiv.org/abs/2510.03824)
*Wei Guo,Jaemoo Choi,Yuchen Zhu,Molei Tao,Yongxin Chen*

Main category: cs.LG

TL;DR: 提出PDNS框架，通过路径测度上的近端点方法解决多模态分布采样中的模式崩溃问题，将学习过程分解为一系列逐步逼近目标分布的简单子问题。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的神经采样器在多模态分布中容易发生模式崩溃，特别是当模式之间存在显著障碍时，训练变得困难。

Method: 使用路径测度上的近端点方法，将随机最优控制问题分解为一系列简单子问题，每个近端步骤使用加权去噪交叉熵目标实现。

Result: 在连续和离散采样任务中，包括分子动力学和统计物理中的挑战性场景，PDNS表现出有效性和鲁棒性。

Conclusion: PDNS框架通过渐进式路径构建解决了多模态分布采样中的模式崩溃问题，促进了跨模式的充分探索。

Abstract: The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.

</details>


### [105] [CAFL-L: Constraint-Aware Federated Learning with Lagrangian Dual Optimization for On-Device Language Models](https://arxiv.org/abs/2510.03298)
*Dongqi Zheng,Wenjin Fu*

Main category: cs.LG

TL;DR: CAFL-L是一个基于拉格朗日对偶优化的联邦学习方法，在FedAvg基础上显式地整合设备级资源约束（能量、通信、内存、热预算），通过动态调整训练超参数来满足约束条件。


<details>
  <summary>Details</summary>
Motivation: 解决标准联邦学习在资源受限边缘设备上部署时面临的设备级资源约束问题，如能量、通信、内存和热预算限制。

Method: 采用拉格朗日对偶优化动态调整训练超参数（冻结深度、本地步数、批量大小、通信压缩），通过梯度累积保持训练稳定性。

Result: 在字符级语言模型实验中，相比标准FedAvg，内存使用减少20%，通信量减少95%，同时保持竞争力的验证性能。

Conclusion: CAFL-L在满足资源约束方面表现优异，适合在资源受限的边缘设备上实际部署。

Abstract: We introduce Constraint-Aware Federated Learning with Lagrangian Dual
Optimization (CAFL-L), a principled extension of FedAvg that explicitly
incorporates device-level resource constraints including energy, communication,
memory, and thermal budgets. CAFL-L employs Lagrangian dual optimization to
dynamically adapt training hyperparameters -- freezing depth, local steps,
batch size, and communication compression -- while preserving training
stability through token-budget preservation via gradient accumulation.
Experiments on a character-level language model demonstrate that CAFL-L
achieves superior constraint satisfaction compared to standard FedAvg (reducing
memory usage by 20% and communication by 95%) while maintaining competitive
validation performance, making it practical for deployment on
resource-constrained edge devices.

</details>


### [106] [Dynamic Meta-Learning for Adaptive XGBoost-Neural Ensembles](https://arxiv.org/abs/2510.03301)
*Arthur Sedek*

Main category: cs.LG

TL;DR: 提出了一种新颖的自适应集成框架，通过元学习协同结合XGBoost和神经网络，利用不确定性量化和特征重要性来动态选择与组合模型。


<details>
  <summary>Details</summary>
Motivation: 开发更智能和灵活的机器学习系统，通过结合不同模型的优势来提升预测性能和可解释性。

Method: 使用元学习技术将XGBoost和神经网络进行协同集成，结合先进的不确定性量化方法和特征重要性集成，实现动态的模型选择和组合。

Result: 实验结果表明，该方法在多个数据集上展现出优越的预测性能和增强的可解释性。

Conclusion: 该自适应集成框架为开发更智能、灵活的机器学习系统做出了贡献，展示了在预测性能和可解释性方面的显著提升。

Abstract: This paper introduces a novel adaptive ensemble framework that
synergistically combines XGBoost and neural networks through sophisticated
meta-learning. The proposed method leverages advanced uncertainty
quantification techniques and feature importance integration to dynamically
orchestrate model selection and combination. Experimental results demonstrate
superior predictive performance and enhanced interpretability across diverse
datasets, contributing to the development of more intelligent and flexible
machine learning systems.

</details>


### [107] [Technical note on Fisher Information for Robust Federated Cross-Validation](https://arxiv.org/abs/2510.03838)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: 提出FIRE方法，通过Fisher信息来估计和补偿联邦学习中由数据碎片化引起的协变量偏移，提升模型在偏移验证集上的性能。


<details>
  <summary>Details</summary>
Motivation: 当训练数据在不同批次或地理位置间碎片化时，模型会出现性能下降，这主要是由于协变量偏移导致各数据片段的分布与全局训练分布和验证分布存在差异。

Method: FIRE方法通过近似Fisher信息来累积碎片化引起的协变量偏移差异，并将其作为每个数据片段的损失惩罚项，实现可扩展的分布对齐。

Result: FIRE在偏移验证集上比重要性加权基准方法最多提升5.1%，比联邦学习基准方法最多提升5.3%。

Conclusion: FIRE方法能有效应对联邦学习中的数据碎片化问题，通过Fisher信息估计协变量偏移，显著提升模型在分布偏移场景下的性能。

Abstract: When training data are fragmented across batches or federated-learned across
different geographic locations, trained models manifest performance
degradation. That degradation partly owes to covariate shift induced by data
having been fragmented across time and space and producing dissimilar empirical
training distributions. Each fragment's distribution is slightly different to a
hypothetical unfragmented training distribution of covariates, and to the
single validation distribution. To address this problem, we propose Fisher
Information for Robust fEderated validation (\textbf{FIRE}). This method
accumulates fragmentation-induced covariate shift divergences from the global
training distribution via an approximate Fisher information. That term, which
we prove to be a more computationally-tractable estimate, is then used as a
per-fragment loss penalty, enabling scalable distribution alignment. FIRE
outperforms importance weighting benchmarks by $5.1\%$ at maximum and federated
learning (FL) benchmarks by up to $5.3\%$ on shifted validation sets.

</details>


### [108] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 本文揭示了概念擦除技术在扩散模型中只是制造了"遗忘"的假象，实际上是通过偏置采样轨迹来避开目标概念，而非真正移除概念。作者提出了RevAm框架，能够在不修改模型权重的情况下通过轨迹优化复活被擦除的概念。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型架构演进到下一代（如Flux），现有的概念擦除方法（如ESD、UCE、AC）效果下降。研究发现这些方法并非真正移除概念，而是通过偏置采样轨迹制造遗忘假象，这种擦除本质上是可逆的。

Method: 提出了RevAm框架，基于强化学习的轨迹优化方法。通过将Group Relative Policy Optimization (GRPO)适配到扩散模型中，探索多样化的恢复轨迹，利用轨迹级奖励克服局部最优问题，动态引导去噪过程而不修改模型权重。

Result: 大量实验表明，RevAm在概念复活保真度上表现优异，同时将计算时间减少了10倍，暴露了当前安全机制的关键漏洞。

Conclusion: 当前基于轨迹操纵的安全机制存在根本性脆弱性，需要开发超越轨迹操纵的更鲁棒的概念擦除技术。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [109] [Technical note on Sequential Test-Time Adaptation via Martingale-Driven Fisher Prompting](https://arxiv.org/abs/2510.03839)
*Behraj Khan,Tahir Qasim Syed*

Main category: cs.LG

TL;DR: M-FISHER是一个用于流数据中序列分布漂移检测和稳定适应的理论框架，通过指数鞅和Ville不等式实现时间均匀的误报控制，并基于Fisher信息进行自然梯度下降以实现稳定适应。


<details>
  <summary>Details</summary>
Motivation: 解决流数据中序列分布漂移的检测和适应问题，确保在任意停止时间都具有统计有效性，同时实现几何稳定的参数更新。

Method: 使用非符合性分数构建指数鞅，应用Ville不等式进行漂移检测；采用Fisher预条件化的提示参数更新实现自然梯度下降。

Result: 检测延迟为O(log(1/δ)/Γ)，其中Γ反映后漂移信息增益；适应过程在分布流形上实现局部最优更新，最小化KL散度。

Conclusion: M-FISHER为协变量漂移下的序列决策提供了原则性的、鲁棒的检测和几何稳定适应方法。

Abstract: We present a theoretical framework for M-FISHER, a method for sequential
distribution shift detection and stable adaptation in streaming data. For
detection, we construct an exponential martingale from non-conformity scores
and apply Ville's inequality to obtain time-uniform guarantees on false alarm
control, ensuring statistical validity at any stopping time. Under sustained
shifts, we further bound the expected detection delay as
$\mathcal{O}(\log(1/\delta)/\Gamma)$, where $\Gamma$ reflects the post-shift
information gain, thereby linking detection efficiency to distributional
divergence. For adaptation, we show that Fisher-preconditioned updates of
prompt parameters implement natural gradient descent on the distributional
manifold, yielding locally optimal updates that minimize KL divergence while
preserving stability and parameterization invariance. Together, these results
establish M-FISHER as a principled approach for robust, anytime-valid detection
and geometrically stable adaptation in sequential decision-making under
covariate shift.

</details>


### [110] [On Provable Benefits of Muon in Federated Learning](https://arxiv.org/abs/2510.03866)
*Xinwen Zhang,Hongchang Gao*

Main category: cs.LG

TL;DR: 本文研究了Muon优化器在联邦学习中的应用，提出了FedMuon算法并证明了其在非凸问题上的收敛性，实验验证了算法的有效性。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在多种应用中表现出色，但其在联邦学习中的有效性尚未被探索，本文旨在填补这一空白。

Method: 提出了FedMuon算法，利用正交化更新方向，使其学习率独立于问题特定参数，并能自然处理重尾噪声。

Result: 理论分析表明FedMuon具有多个有利特性，大量实验验证了该算法在各种神经网络架构上的有效性。

Conclusion: FedMuon算法在联邦学习环境中表现优异，具有良好的收敛性和对噪声的鲁棒性。

Abstract: The recently introduced optimizer, Muon, has gained increasing attention due
to its superior performance across a wide range of applications. However, its
effectiveness in federated learning remains unexplored. To address this gap,
this paper investigates the performance of Muon in the federated learning
setting. Specifically, we propose a new algorithm, FedMuon, and establish its
convergence rate for nonconvex problems. Our theoretical analysis reveals
multiple favorable properties of FedMuon. In particular, due to its
orthonormalized update direction, the learning rate of FedMuon is independent
of problem-specific parameters, and, importantly, it can naturally accommodate
heavy-tailed noise. The extensive experiments on a variety of neural network
architectures validate the effectiveness of the proposed algorithm.

</details>


### [111] [Thin Bridges for Drug Text Alignment: Lightweight Contrastive Learning for Target Specific Drug Retrieval](https://arxiv.org/abs/2510.03309)
*Mallikarjuna Tupakula*

Main category: cs.LG

TL;DR: 本文提出了一种轻量级的对比桥接方法，通过冻结的单模态编码器和简单的线性投影头，实现化学分子指纹与生物医学文本嵌入的跨模态对齐，无需大规模多模态预训练。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型在药物发现中具有潜力，但现有方法依赖繁重的预训练或大规模多模态语料库。本文研究是否可以通过轻量级投影头在冻结的单模态编码器上实现化学和文本表示的对齐。

Method: 使用ChEMBL中的配对机制，通过对比目标训练的双线性投影将ECFP4分子指纹与生物医学句子嵌入对齐。引入硬负样本加权和边界损失来处理共享相同治疗靶点的药物。

Result: 在基于支架的分割评估中，该方法实现了非平凡的跨模态对齐，并显著提高了目标内区分能力，相比冻结基线有显著改进。

Conclusion: 薄桥接方法为大规模多模态预训练提供了计算效率高的替代方案，能够在精准医学中实现支架感知的药物文本对齐和靶点特异性检索。

Abstract: Multimodal foundation models hold promise for drug discovery and biomedical
applications, but most existing approaches rely on heavy pretraining or large
scale multimodal corpora. We investigate whether thin contrastive bridges,
lightweight projection heads over frozen unimodal encoders can align chemical
and textual representations without training a full multimodal model. Using
paired mechanisms from ChEMBL, we align ECFP4 molecular fingerprints with
biomedical sentence embeddings through dual linear projections trained with a
contrastive objective. To better handle drugs sharing the same therapeutic
target, we incorporate hard negative weighting and a margin loss. Evaluation
under scaffold based splits, which require generalization across disjoint
chemical cores, demonstrates that our approach achieves non-trivial cross modal
alignment and substantially improves within target discrimination compared to
frozen baselines. These results suggest that thin bridges offer a compute
efficient alternative to large scale multimodal pretraining, enabling scaffold
aware drug text alignment and target specific retrieval in precision medicine.

</details>


### [112] [Optimal Scaling Needs Optimal Norm](https://arxiv.org/abs/2510.03871)
*Oleg Filatov,Jiangtao Wang,Jan Ebert,Stefan Kesselheim*

Main category: cs.LG

TL;DR: 研究发现模型和数据集规模联合优化缩放由输出层的算子范数这一单一不变量控制，称为范数转移现象。


<details>
  <summary>Details</summary>
Motivation: 尽管在模型和数据集缩放下的最优超参数转移方面取得了进展，但尚未建立统一的解释原则。

Method: 使用Scion优化器，在不同规模模型和数据集上测试学习率和批次大小的最优组合，并测量算子范数。

Result: 在1.3B参数模型和138B tokens数据集上，最优学习率/批次大小组合具有相同的算子范数值；输出层对学习率最敏感，隐藏层受益于较低学习率。

Conclusion: 提供了基于范数指导的最优缩放实用见解，并发布了分布式Scion实现和两千多次运行的日志以支持大规模LLM训练动态研究。

Abstract: Despite recent progress in optimal hyperparameter transfer under model and
dataset scaling, no unifying explanatory principle has been established. Using
the Scion optimizer, we discover that joint optimal scaling across model and
dataset sizes is governed by a single invariant: the operator norm of the
output layer. Across models with up to 1.3B parameters trained on up to 138B
tokens, the optimal learning rate/batch size pair $(\eta^{\ast}, B^{\ast})$
consistently has the same operator norm value - a phenomenon we term norm
transfer. This constant norm condition is necessary but not sufficient: while
for each dataset size, multiple $(\eta, B)$ reach the optimal norm, only a
unique $(\eta^{\ast}, B^{\ast})$ achieves the best loss. As a sufficient
condition, we provide the first measurement of $(\eta^{\ast}, B^{\ast})$
scaling with dataset size for Scion, and find that the scaling rules are
consistent with those of the Adam optimizer. Tuning per-layer-group learning
rates also improves model performance, with the output layer being the most
sensitive and hidden layers benefiting from lower learning rates. We provide
practical insights on norm-guided optimal scaling and release our Distributed
Scion (Disco) implementation with logs from over two thousand runs to support
research on LLM training dynamics at scale.

</details>


### [113] [Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management](https://arxiv.org/abs/2510.03310)
*Runze Zhang,Xiaowei Zhang,Mingyang Zhao*

Main category: cs.LG

TL;DR: 评估LLMs在运营管理中模拟人类行为的能力，发现虽然能复现大部分假设检验结果，但响应分布与人类数据存在差异，轻量级干预措施可改善对齐效果。


<details>
  <summary>Details</summary>
Motivation: LLMs作为模拟人类行为的低成本工具在商业、经济和社会科学中兴起，需要评估其在运营管理领域复制人类行为的准确性。

Method: 使用9个已发表的行为运营实验，通过假设检验结果复现和Wasserstein距离分布对齐两个标准来评估LLMs，并测试思维链提示和超参数调优两种干预措施。

Result: LLMs能复现大部分假设级效应，捕捉关键决策偏差，但响应分布与人类数据存在差异，包括强商业模型。轻量级干预可减少不对齐，有时让小型或开源模型达到或超越大型系统。

Conclusion: LLMs在运营管理中能有效模拟人类行为的假设检验结果，但分布对齐需要改进，轻量级干预措施有助于提升模拟准确性。

Abstract: LLMs are emerging tools for simulating human behavior in business, economics,
and social science, offering a lower-cost complement to laboratory experiments,
field studies, and surveys. This paper evaluates how well LLMs replicate human
behavior in operations management. Using nine published experiments in
behavioral operations, we assess two criteria: replication of hypothesis-test
outcomes and distributional alignment via Wasserstein distance. LLMs reproduce
most hypothesis-level effects, capturing key decision biases, but their
response distributions diverge from human data, including for strong commercial
models. We also test two lightweight interventions -- chain-of-thought
prompting and hyperparameter tuning -- which reduce misalignment and can
sometimes let smaller or open-source models match or surpass larger systems.

</details>


### [114] [Slow-Fast Policy Optimization: Reposition-Before-Update for LLM Reasoning](https://arxiv.org/abs/2510.04072)
*Ziyan Wang,Zheng Wang,Jie Fu,Xingwei Qu,Qi Cheng,Shengpu Tang,Minjia Zhang,Xiaoming Huo*

Main category: cs.LG

TL;DR: SFPO通过慢-快策略优化框架解决强化学习训练早期的不稳定问题，在数学推理基准上比GRPO提升2.80分，减少4.93倍rollouts，加速4.19倍收敛。


<details>
  <summary>Details</summary>
Motivation: 现有策略优化算法如GRPO在早期训练中因低质量rollouts产生的噪声梯度导致不稳定更新和低效探索。

Method: 提出SFPO框架，将每个训练步骤分解为三个阶段：在相同批次上的短快速轨迹、控制离策略漂移的重定位机制、以及最终的慢速校正。

Result: SFPO在数学推理基准上比GRPO平均提升2.80分，减少4.93倍rollouts，加速4.19倍收敛时间。

Conclusion: SFPO通过重定位-更新设计有效提升强化学习训练的稳定性和效率，与现有策略梯度流程兼容。

Abstract: Reinforcement learning (RL) has become central to enhancing reasoning in
large language models (LLMs). Yet on-policy algorithms such as Group Relative
Policy Optimization (GRPO) often suffer in early training: noisy gradients from
low-quality rollouts lead to unstable updates and inefficient exploration. We
introduce Slow-Fast Policy Optimization (SFPO), a simple yet efficient
framework to address these limitations via decomposing each step into three
stages: a short fast trajectory of inner steps on the same batch, a reposition
mechanism to control off-policy drift, and a final slow correction. This
reposition-before-update design preserves the objective and rollout process
unchanged, making SFPO plug-compatible with existing policy-gradient pipelines.
Extensive experiments demonstrate that SFPO consistently improves stability,
reduces rollouts, and accelerates convergence of reasoning RL training.
Specifically, it outperforms GRPO by up to 2.80 points in average on math
reasoning benchmarks. It also achieves up to 4.93\texttimes{} fewer rollouts
and a 4.19\texttimes{} reduction in wall-clock time to match GRPO's best
accuracy.

</details>


### [115] [Scaling Laws Revisited: Modeling the Role of Data Quality in Language Model Pretraining](https://arxiv.org/abs/2510.03313)
*Anirudh Subramanyam,Yuxin Chen,Robert L. Grossman*

Main category: cs.LG

TL;DR: 本文提出了一个包含数据质量参数的扩展缩放定律，将Chinchilla框架扩展到同时考虑模型大小、数据量和数据质量，通过有效样本量和信息论视角形式化数据质量对语言模型训练的影响。


<details>
  <summary>Details</summary>
Motivation: 传统缩放定律主要关注模型大小和数据量，但缺乏对数据质量的形式化分析。本文旨在建立一个包含数据质量参数的缩放定律，为大规模预训练中数据筛选和模型规模的平衡提供理论指导。

Method: 引入无量纲数据质量参数Q，提出质量感知缩放定律，通过有效样本量和信息论视角分析噪声或冗余语料库。开发了两种Q的实用估计器：腐败率代理和缺陷度量，并在神经机器翻译和自回归建模中进行合成实验，通过噪声注入和覆盖变化系统控制数据质量。

Result: 实验表明损失随数据质量可预测地缩放，高质量数据可显著减小模型规模和计算需求。结果展示了有效数据随质量的亚线性衰减和对适度数据腐败的鲁棒性，样本外评估进一步验证了定律的预测形式。

Conclusion: 与先前的实证分析不同，本研究为数据质量建立了一个明确、可推广的定律，为大规模预训练中数据筛选工作和模型规模的平衡提供了具体指导。

Abstract: Scaling laws for language model training traditionally characterize how
performance scales with model size and dataset volume. Prior work has explored
architecture variants and data treatments such as dataset filtering and noise
injection in language model pretraining; however, these studies have not
formalized data quality within a principled scaling law. We introduce a
dimensionless data-quality parameter Q, and propose a quality-aware scaling law
extending the Chinchilla framework to predict loss as a joint function of model
size, data volume, and data quality. The law is motivated by an
effective-sample-size and information-theoretic view of noisy or redundant
corpora, and it admits two practical estimators for Q: (i) a corruption rate
proxy and (ii) a deficiency measure. Through synthetic experiments in neural
machine translation and autoregressive modeling -- where we systematically
control data quality via multiple levels of noise injection and coverage
variation -- we show that loss scales predictably with data quality and that
higher-quality data can substantially reduce model size and hence compute
requirements. Our results demonstrate a sublinear decay of effective data with
quality and robustness to moderate data corruption; out-of-sample evaluations
further validate the predictive form of the law. Unlike prior empirical
analyses, our work establishes an explicit, generalizable law for data quality,
offering concrete guidance for balancing data curation effort and model scale
in large-scale pretraining.

</details>


### [116] [Offline Reinforcement Learning in Large State Spaces: Algorithms and Guarantees](https://arxiv.org/abs/2510.04088)
*Nan Jiang,Tengyang Xie*

Main category: cs.LG

TL;DR: 该论文介绍了离线强化学习在大状态空间中的理论框架，探讨了从历史数据中学习策略而不需要与环境在线交互的方法。


<details>
  <summary>Details</summary>
Motivation: 研究离线强化学习的理论基础，解决在大状态空间中仅使用历史数据学习有效策略的挑战，避免在线交互的成本和风险。

Method: 基于函数逼近的表达性假设（如Bellman完备性与可实现性）和数据覆盖假设（如全策略覆盖与单策略覆盖），构建算法和理论分析框架。

Result: 建立了丰富的算法和结果体系，展示了在不同假设条件下可实现的样本和计算复杂度保证。

Conclusion: 离线强化学习在大状态空间中具有重要理论价值，但仍有开放性问题需要解决，并与相关领域存在密切联系。

Abstract: This article introduces the theory of offline reinforcement learning in large
state spaces, where good policies are learned from historical data without
online interactions with the environment. Key concepts introduced include
expressivity assumptions on function approximation (e.g., Bellman completeness
vs. realizability) and data coverage (e.g., all-policy vs. single-policy
coverage). A rich landscape of algorithms and results is described, depending
on the assumptions one is willing to make and the sample and computational
complexity guarantees one wishes to achieve. We also discuss open questions and
connections to adjacent areas.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [117] [Quantile-Scaled Bayesian Optimization Using Rank-Only Feedback](https://arxiv.org/abs/2510.03277)
*Tunde Fahd Egunjobi*

Main category: stat.ML

TL;DR: 提出QS-BO方法，将排名反馈转换为异方差高斯目标，使贝叶斯优化能在仅有相对排名信息的情况下工作，显著优于随机搜索。


<details>
  <summary>Details</summary>
Motivation: 标准贝叶斯优化需要精确的目标函数值，但在实际应用中可能只能获得相对排名反馈，需要开发适用于排名反馈的优化方法。

Method: 通过分位数缩放流程将排名转换为异方差高斯目标，使用高斯过程代理模型和标准采集函数，无需显式度量分数。

Result: 在合成基准函数上的测试表明，QS-BO始终获得更低的目标值，运行更稳定，统计检验显示在1%显著性水平上显著优于随机搜索。

Conclusion: QS-BO是贝叶斯优化在仅有排名反馈情况下的实用有效扩展，在偏好学习、推荐系统和人机协同优化中具有应用前景。

Abstract: Bayesian Optimization (BO) is widely used for optimizing expensive black-box
functions, particularly in hyperparameter tuning. However, standard BO assumes
access to precise objective values, which may be unavailable, noisy, or
unreliable in real-world settings where only relative or rank-based feedback
can be obtained. In this study, we propose Quantile-Scaled Bayesian
Optimization (QS-BO), a principled rank-based optimization framework. QS-BO
converts ranks into heteroscedastic Gaussian targets through a quantile-scaling
pipeline, enabling the use of Gaussian process surrogates and standard
acquisition functions without requiring explicit metric scores. We evaluate
QS-BO on synthetic benchmark functions, including one- and two-dimensional
nonlinear functions and the Branin function, and compare its performance
against Random Search. Results demonstrate that QS-BO consistently achieves
lower objective values and exhibits greater stability across runs. Statistical
tests further confirm that QS-BO significantly outperforms Random Search at the
1\% significance level. These findings establish QS-BO as a practical and
effective extension of Bayesian Optimization for rank-only feedback, with
promising applications in preference learning, recommendation, and
human-in-the-loop optimization where absolute metric values are unavailable or
unreliable.

</details>


### [118] [Mathematically rigorous proofs for Shapley explanations](https://arxiv.org/abs/2510.03281)
*David van Batenburg*

Main category: stat.ML

TL;DR: 本文从数学严谨角度重新证明了Lundberg和Lee关于Shapley值的两个主要结果，包括Shapley值的公理化特征及其作为加权线性回归问题的唯一解。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型决策过程的可解释性日益重要，Shapley值是理解模型决策的流行方法。原论文缺乏完整的数学证明，需要从数学严谨角度重新证明这些结果。

Method: 使用Young的公理系统对Shapley值进行公理化特征化，并通过维度约简证明Shapley值可作为加权线性回归问题的唯一解。

Result: 证明了Shapley值是满足局部准确性、缺失性、对称性和一致性的唯一解释；证明了对称性公理是必需的；证明了Shapley值可表示为加权线性回归问题的唯一解。

Conclusion: 从数学严谨角度完整证明了Shapley值的两个关键性质，纠正了原论文中关于对称性公理可省略的错误观点，为机器学习模型可解释性提供了坚实的理论基础。

Abstract: Machine Learning is becoming increasingly more important in today's world. It
is therefore very important to provide understanding of the decision-making
process of machine-learning models. A popular way to do this is by looking at
the Shapley-Values of these models as introduced by Lundberg and Lee.
  In this thesis, we discuss the two main results by Lundberg and Lee from a
mathematically rigorous standpoint and provide full proofs, which are not
available from the original material.
  The first result of this thesis is an axiomatic characterization of the
Shapley values in machine learning based on axioms by Young. We show that the
Shapley values are the unique explanation to satisfy local accuracy,
missingness, symmetry and consistency. Lundberg and Lee claim that the symmetry
axiom is redundant for explanations. However, we provide a counterexample that
shows the symmetry axiom is in fact essential.
  The second result shows that we can write the Shapley values as the unique
solution to a weighted linear regression problem. This result is proven with
the use of dimensionality reduction.

</details>


### [119] [Transformed $\ell_1$ Regularizations for Robust Principal Component Analysis: Toward a Fine-Grained Understanding](https://arxiv.org/abs/2510.03624)
*Kun Zhao,Haoke Zhang,Jiayi Wang,Yifei Lou*

Main category: stat.ML

TL;DR: 本文提出使用非凸正则化方法（变换L1范数，TL1）来改进鲁棒主成分分析（RPCA），通过调整TL1的内部参数，可以渐进逼近L0或L1范数，从而更好地近似矩阵的秩和稀疏性。


<details>
  <summary>Details</summary>
Motivation: 传统RPCA模型依赖凸松弛方法（如核范数和L1范数）来近似矩阵的秩和稀疏性，但这些近似可能不够精确。本文旨在通过非凸正则化方法提高近似精度。

Method: 采用变换L1（TL1）正则化方法，通过调整其内部参数来控制逼近L0或L1范数的程度。将TL1应用于奇异值来近似矩阵的秩或核范数，并进行理论收敛率分析。

Result: 理论分析表明该方法在Frobenius范数下的统计收敛率与经典凸模型相当，在TL1逼近L0范数的机制下，对低秩分量和稀疏分量的估计秩和基数建立了常数阶上界。数值实验显示该方法在非均匀采样下比经典凸模型精度更高。

Conclusion: 提出的TL1非凸正则化方法在鲁棒主成分分析中表现出优于经典凸模型的性能，特别是在非均匀采样场景下，能够更准确地恢复低秩结构和稀疏异常值。

Abstract: Robust Principal Component Analysis (RPCA) aims to recover a low-rank
structure from noisy, partially observed data that is also corrupted by sparse,
potentially large-magnitude outliers. Traditional RPCA models rely on convex
relaxations, such as nuclear norm and $\ell_1$ norm, to approximate the rank of
a matrix and the $\ell_0$ functional (the number of non-zero elements) of
another. In this work, we advocate a nonconvex regularization method, referred
to as transformed $\ell_1$ (TL1), to improve both approximations. The rationale
is that by varying the internal parameter of TL1, its behavior asymptotically
approaches either $\ell_0$ or $\ell_1$. Since the rank is equal to the number
of non-zero singular values and the nuclear norm is defined as their sum,
applying TL1 to the singular values can approximate either the rank or the
nuclear norm, depending on its internal parameter. We conduct a fine-grained
theoretical analysis of statistical convergence rates, measured in the
Frobenius norm, for both the low-rank and sparse components under general
sampling schemes. These rates are comparable to those of the classical RPCA
model based on the nuclear norm and $\ell_1$ norm. Moreover, we establish
constant-order upper bounds on the estimated rank of the low-rank component and
the cardinality of the sparse component in the regime where TL1 behaves like
$\ell_0$, assuming that the respective matrices are exactly low-rank and
exactly sparse. Extensive numerical experiments on synthetic data and
real-world applications demonstrate that the proposed approach achieves higher
accuracy than the classic convex model, especially under non-uniform sampling
schemes.

</details>


### [120] [The analogy theorem in Hoare logic](https://arxiv.org/abs/2510.03685)
*Nikitin Nikita*

Main category: stat.ML

TL;DR: 该论文提出了一种基于一阶逻辑和霍尔逻辑的形式化类比方法，为机器学习模型在不同数据域之间的知识迁移提供数学保证。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在不同数据域间的迁移缺乏严格的数学理论基础，无法保证模型在目标域上的性能保持。

Method: 使用一阶逻辑和霍尔逻辑形式化数据集和模型之间的类比概念，提出并证明类比定理，并在模型数据和MNIST、USPS数据集上进行验证。

Result: 在卷积神经网络和随机森林上分别达到0.84和0.88的F1分数，验证了类比定理的有效性。

Conclusion: 该工作通过程序逻辑层面的严格形式化，为知识迁移的正确性提供了可验证的保证，为机器学习模型在之前不可达领域的应用开辟了新机会。

Abstract: The introduction of machine learning methods has led to significant advances
in automation, optimization, and discoveries in various fields of science and
technology. However, their widespread application faces a fundamental
limitation: the transfer of models between data domains generally lacks a
rigorous mathematical justification. The key problem is the lack of formal
criteria to guarantee that a model trained on one type of data will retain its
properties on another.This paper proposes a solution to this problem by
formalizing the concept of analogy between data sets and models using
first-order logic and Hoare logic.We formulate and rigorously prove a theorem
that sets out the necessary and sufficient conditions for analogy in the task
of knowledge transfer between machine learning models. Practical verification
of the analogy theorem on model data obtained using the Monte Carlo method, as
well as on MNIST and USPS data, allows us to achieving F1 scores of 0.84 and
0.88 for convolutional neural networks and random forests, respectively.The
proposed approach not only allows us to justify the correctness of transfer
between domains but also provides tools for comparing the applicability of
models to different types of data.The main contribution of the work is a
rigorous formalization of analogy at the level of program logic, providing
verifiable guarantees of the correctness of knowledge transfer, which opens new
opportunities for both theoretical research and the practical use of machine
learning models in previously inaccessible areas.

</details>


### [121] [Spectral Thresholds for Identifiability and Stability:Finite-Sample Phase Transitions in High-Dimensional Learning](https://arxiv.org/abs/2510.03809)
*William Hao-Cheng Huang*

Main category: stat.ML

TL;DR: 论文提出了Fisher阈值定理，揭示了高维学习中模型稳定性与最小Fisher特征值之间的临界关系：当最小Fisher特征值低于O(√(d/n))界限时，模型会突然崩溃。


<details>
  <summary>Details</summary>
Motivation: 高维学习中模型会突然崩溃的现象不是算法特定的，而是几何机制导致的，需要理解这种不稳定性的根本原因。

Method: 引入Fisher阈值定理证明稳定性需要最小Fisher特征值超过显式的O(√(d/n))界限，并提出Fisher floor作为可验证的谱正则化方法。

Result: 在高斯混合和逻辑模型上的合成实验证实了预测的相变，与d/n缩放一致。

Conclusion: 该阈值将经典特征值条件锐化为非渐近定律，定义了谱样本复杂度前沿，为稳健高维推断提供了理论与诊断工具。

Abstract: In high-dimensional learning, models remain stable until they collapse
abruptly once the sample size falls below a critical level. This instability is
not algorithm-specific but a geometric mechanism: when the weakest Fisher
eigendirection falls beneath sample-level fluctuations, identifiability fails.
Our Fisher Threshold Theorem formalizes this by proving that stability requires
the minimal Fisher eigenvalue to exceed an explicit $O(\sqrt{d/n})$ bound.
Unlike prior asymptotic or model-specific criteria, this threshold is
finite-sample and necessary, marking a sharp phase transition between reliable
concentration and inevitable failure. To make the principle constructive, we
introduce the Fisher floor, a verifiable spectral regularization robust to
smoothing and preconditioning. Synthetic experiments on Gaussian mixtures and
logistic models confirm the predicted transition, consistent with $d/n$
scaling. Statistically, the threshold sharpens classical eigenvalue conditions
into a non-asymptotic law; learning-theoretically, it defines a spectral
sample-complexity frontier, bridging theory with diagnostics for robust
high-dimensional inference.

</details>


### [122] [Self-Speculative Masked Diffusions](https://arxiv.org/abs/2510.03929)
*Andrew Campbell,Valentin De Bortoli,Jiaxin Shi,Arnaud Doucet*

Main category: stat.ML

TL;DR: 提出自推测掩码扩散模型，通过修改注意力掩码实现并行验证，将离散数据生成的网络前向传播次数减少约2倍


<details>
  <summary>Details</summary>
Motivation: 标准掩码扩散模型由于因子化近似，需要多次采样步骤才能保证样本质量，导致计算开销大

Method: 修改transformer注意力掩码为非因果到因果，实现草稿token生成和并行验证，在单次前向传播中获得非因子化的预测分布

Result: 在GPT2规模文本建模和蛋白质序列生成中，相比标准掩码扩散模型减少了约2倍网络前向传播次数

Conclusion: 自推测掩码扩散模型能显著降低离散数据生成的计算成本，同时保持高质量样本生成

Abstract: We present self-speculative masked diffusions, a new class of masked
diffusion generative models for discrete data that require significantly fewer
function evaluations to generate samples. Standard masked diffusion models
predict factorized logits over currently masked positions. A number of masked
positions are then sampled, however, the factorization approximation means that
sampling too many positions in one go leads to poor sample quality. As a
result, many simulation steps and therefore neural network function evaluations
are required to generate high-quality data. We reduce the computational burden
by generating non-factorized predictions over masked positions. This is
achieved by modifying the final transformer attention mask from non-causal to
causal, enabling draft token generation and parallel validation via a novel,
model-integrated speculative sampling mechanism. This results in a
non-factorized predictive distribution over masked positions in a single
forward pass. We apply our method to GPT2 scale text modelling and protein
sequences generation, finding that we can achieve a ~2x reduction in the
required number of network forward passes relative to standard masked diffusion
models.

</details>


### [123] [Simulation-based inference via telescoping ratio estimation for trawl processes](https://arxiv.org/abs/2510.04042)
*Dan Leonte,Raphaël Huser,Almut E. D. Veraart*

Main category: stat.ML

TL;DR: 提出了一种快速、准确且样本高效的模拟推理框架，用于处理难处理的随机过程的后验推断，通过参数维度分解和切比雪夫多项式近似实现高效采样。


<details>
  <summary>Details</summary>
Motivation: 大型复杂数据集的增长增加了对能够捕捉边际偏度、非高斯尾部、长记忆甚至非马尔可夫动力学等风格化事实的时序随机过程的需求。虽然这些模型易于模拟，但参数估计仍然具有挑战性。

Method: 采用两步法：首先通过参数维度分解顺序学习后验密度，然后使用切比雪夫多项式近似高效生成独立后验样本。还开发了诊断工具和后验校准技术。

Result: 该方法在拖网过程（一类灵活的无限可分模型）上得到验证，应用于能源需求数据，展示了方法的有效性。

Conclusion: 提出的框架能够实现快速准确的摊销后验推断，即使在马尔可夫链蒙特卡洛方法混合不良时也能进行准确推断，并且能够重用训练好的推理工具处理不同长度的时间序列。

Abstract: The growing availability of large and complex datasets has increased interest
in temporal stochastic processes that can capture stylized facts such as
marginal skewness, non-Gaussian tails, long memory, and even non-Markovian
dynamics. While such models are often easy to simulate from, parameter
estimation remains challenging. Simulation-based inference (SBI) offers a
promising way forward, but existing methods typically require large training
datasets or complex architectures and frequently yield confidence (credible)
regions that fail to attain their nominal values, raising doubts on the
reliability of estimates for the very features that motivate the use of these
models. To address these challenges, we propose a fast and accurate,
sample-efficient SBI framework for amortized posterior inference applicable to
intractable stochastic processes. The proposed approach relies on two main
steps: first, we learn the posterior density by decomposing it sequentially
across parameter dimensions. Then, we use Chebyshev polynomial approximations
to efficiently generate independent posterior samples, enabling accurate
inference even when Markov chain Monte Carlo methods mix poorly. We further
develop novel diagnostic tools for SBI in this context, as well as post-hoc
calibration techniques; the latter not only lead to performance improvements of
the learned inferential tool, but also to the ability to reuse it directly with
new time series of varying lengths, thus amortizing the training cost. We
demonstrate the method's effectiveness on trawl processes, a class of flexible
infinitely divisible models that generalize univariate Gaussian processes,
applied to energy demand data.

</details>


### [124] [Scalable Causal Discovery from Recursive Nonlinear Data via Truncated Basis Function Scores and Tests](https://arxiv.org/abs/2510.04276)
*Joseph Ramsey,Bryan Andrews*

Main category: stat.ML

TL;DR: 提出了两种基于基函数扩展的可扩展因果发现工具：BF-BIC评分和BF-LRT条件独立性检验，用于从非线性、连续或混合数据中学习图条件独立结构。


<details>
  <summary>Details</summary>
Motivation: 现有的许多方法难以扩展到数千个样本或数百个变量的规模，需要开发更高效的因果发现方法。

Method: 使用截断加性展开来近似非线性依赖关系，BF-BIC在加性模型下具有理论一致性，通过可逆重参数化扩展到后非线性模型，并通过退化高斯嵌入支持混合数据。BF-LRT提供近似条件独立性检验。

Result: 在完全非线性神经因果模型模拟中，BF-BIC在准确性和运行时间上优于基于核和约束的方法。BF-LRT比核检验快得多，同时保持竞争性准确性。

Conclusion: 基于基函数的方法在混合搜索中能够实现可解释和可扩展的因果发现，并在加拿大野火风险的实际应用中验证了有效性。

Abstract: Learning graphical conditional independence structures from nonlinear,
continuous or mixed data is a central challenge in machine learning and the
sciences, and many existing methods struggle to scale to thousands of samples
or hundreds of variables. We introduce two basis-expansion tools for scalable
causal discovery. First, the Basis Function BIC (BF-BIC) score uses truncated
additive expansions to approximate nonlinear dependencies. BF-BIC is
theoretically consistent under additive models and extends to post-nonlinear
(PNL) models via an invertible reparameterization. It remains robust under
moderate interactions and supports mixed data through a degenerate-Gaussian
embedding for discrete variables. In simulations with fully nonlinear neural
causal models (NCMs), BF-BIC outperforms kernel- and constraint-based methods
(e.g., KCI, RFCI) in both accuracy and runtime. Second, the Basis Function
Likelihood Ratio Test (BF-LRT) provides an approximate conditional independence
test that is substantially faster than kernel tests while retaining competitive
accuracy. Extensive simulations and a real-data application to Canadian
wildfire risk show that, when integrated into hybrid searches, BF-based methods
enable interpretable and scalable causal discovery. Implementations are
available in Python, R, and Java.

</details>


### [125] [Relative Information Gain and Gaussian Process Regression](https://arxiv.org/abs/2510.04277)
*Hamish Flynn*

Main category: stat.ML

TL;DR: 提出了相对信息增益的概念，它平滑地插值于有效维度和信息增益之间，并具有与有效维度相同的增长率。基于此，为高斯过程回归建立了新的PAC-Bayesian超额风险界，结合核的光谱特性获得了极小极大最优收敛率。


<details>
  <summary>Details</summary>
Motivation: 在再生核希尔伯特空间中估计或最大化未知函数的样本复杂度与核的有效维度和信息增益相关。信息增益具有信息论解释，但有效维度通常能获得更好的收敛率。需要一个新的量来连接这两个概念。

Method: 引入相对信息增益的概念，测量信息增益对观测噪声的敏感性。证明相对信息增益平滑插值于有效维度和信息增益之间。建立高斯过程回归的PAC-Bayesian超额风险界，其中相对信息增益自然出现在复杂度项中。

Result: 相对信息增益具有与有效维度相同的增长率。结合核的光谱特性，获得了相对信息增益的上界。当这些上界与超额风险界结合时，得到了极小极大最优的收敛率。

Conclusion: 相对信息增益是连接有效维度和信息增益的有用概念，通过PAC-Bayesian框架为高斯过程回归提供了理论保证，并实现了最优收敛率。

Abstract: The sample complexity of estimating or maximising an unknown function in a
reproducing kernel Hilbert space is known to be linked to both the effective
dimension and the information gain associated with the kernel. While the
information gain has an attractive information-theoretic interpretation, the
effective dimension typically results in better rates. We introduce a new
quantity called the relative information gain, which measures the sensitivity
of the information gain with respect to the observation noise. We show that the
relative information gain smoothly interpolates between the effective dimension
and the information gain, and that the relative information gain has the same
growth rate as the effective dimension. In the second half of the paper, we
prove a new PAC-Bayesian excess risk bound for Gaussian process regression. The
relative information gain arises naturally from the complexity term in this
PAC-Bayesian bound. We prove bounds on the relative information gain that
depend on the spectral properties of the kernel. When these upper bounds are
combined with our excess risk bound, we obtain minimax-optimal rates of
convergence.

</details>


### [126] [Adaptive Coverage Policies in Conformal Prediction](https://arxiv.org/abs/2510.04318)
*Etienne Gauthier,Francis Bach,Michael I. Jordan*

Main category: stat.ML

TL;DR: 提出一种自适应覆盖策略的共形预测方法，通过训练神经网络优化数据依赖的覆盖水平，使预测集大小能根据每个样本的难度动态调整。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测方法的固定覆盖水平会导致预测集不具信息性：覆盖水平过高产生过于保守的集合，过低则产生空集，且无法适应不同样本的特性。

Method: 利用e值和事后共形推断的最新进展，在验证集上通过留一法训练神经网络来优化自适应覆盖策略，使覆盖水平和预测集大小随样本难度变化。

Result: 该方法具有理论覆盖保证，并通过一系列实验证明了其实际优势。

Conclusion: 提出的自适应覆盖策略能够有效提高共形预测的灵活性和效率，同时保持有效的统计保证。

Abstract: Traditional conformal prediction methods construct prediction sets such that
the true label falls within the set with a user-specified coverage level.
However, poorly chosen coverage levels can result in uninformative predictions,
either producing overly conservative sets when the coverage level is too high,
or empty sets when it is too low. Moreover, the fixed coverage level cannot
adapt to the specific characteristics of each individual example, limiting the
flexibility and efficiency of these methods. In this work, we leverage recent
advances in e-values and post-hoc conformal inference, which allow the use of
data-dependent coverage levels while maintaining valid statistical guarantees.
We propose to optimize an adaptive coverage policy by training a neural network
using a leave-one-out procedure on the calibration set, allowing the coverage
level and the resulting prediction set size to vary with the difficulty of each
individual example. We support our approach with theoretical coverage
guarantees and demonstrate its practical benefits through a series of
experiments.

</details>


### [127] [Modular and Adaptive Conformal Prediction for Sequential Models via Residual Decomposition](https://arxiv.org/abs/2510.04406)
*William Zhang,Saurabh Amin,Georgia Perakis*

Main category: stat.ML

TL;DR: 提出了一个用于两阶段顺序模型的保形预测框架，通过分解预测残差为阶段特定组件，实现不确定性到特定管道阶段的归因，并开发了风险控制参数选择程序。


<details>
  <summary>Details</summary>
Motivation: 现有保形预测方法将整个建模过程视为黑盒，忽略了利用模块化结构的机会，无法将不确定性归因到特定管道阶段。

Method: 引入两阶段顺序模型的保形预测框架，将总体预测残差分解为阶段特定组件，使用族错误率控制来校准阶段级缩放参数，并提出自适应扩展用于非平稳设置。

Result: 在合成分布偏移以及真实供应链和股票市场数据上的实验表明，该方法在标准保形方法性能下降的条件下仍能保持覆盖，同时提供可解释的阶段级不确定性归因。

Conclusion: 该框架提供了标准保形方法所缺乏的诊断优势和稳健覆盖保证。

Abstract: Conformal prediction offers finite-sample coverage guarantees under minimal
assumptions. However, existing methods treat the entire modeling process as a
black box, overlooking opportunities to exploit modular structure. We introduce
a conformal prediction framework for two-stage sequential models, where an
upstream predictor generates intermediate representations for a downstream
model. By decomposing the overall prediction residual into stage-specific
components, our method enables practitioners to attribute uncertainty to
specific pipeline stages. We develop a risk-controlled parameter selection
procedure using family-wise error rate (FWER) control to calibrate stage-wise
scaling parameters, and propose an adaptive extension for non-stationary
settings that preserves long-run coverage guarantees. Experiments on synthetic
distribution shifts, as well as real-world supply chain and stock market data,
demonstrate that our approach maintains coverage under conditions that degrade
standard conformal methods, while providing interpretable stage-wise
uncertainty attribution. This framework offers diagnostic advantages and robust
coverage that standard conformal methods lack.

</details>


### [128] [Learning Survival Models with Right-Censored Reporting Delays](https://arxiv.org/abs/2510.04421)
*Yuta Shikuri,Hironori Fujisawa*

Main category: stat.ML

TL;DR: 提出了一种联合建模事件发生和报告时间的生存分析方法，解决了保险行业中报告延迟导致的右删失问题，特别针对新入组队列的有限随访数据。


<details>
  <summary>Details</summary>
Motivation: 保险行业中报告延迟导致事件发生时间无法观测，特别是在新入组队列中由于行政删失导致随访时间有限，这使得危险率估计面临重大挑战。

Method: 联合建模事件发生和报告时间的参数化危险函数，通过对潜在事件发生状态进行边际化处理，构建生存模型估计器并开发期望最大化算法进行计算。

Result: 实验结果表明，该方法有效提高了对新入组队列风险评估的及时性，建立的估计器具有渐近一致性。

Conclusion: 提出的两阶段估计程序基于参数比例危险模型，能够有效处理行政删失下的观测数据，为保险行业的风险评价提供了更及时的方法。

Abstract: Survival analysis is a statistical technique used to estimate the time until
an event occurs. Although it is applied across a wide range of fields,
adjusting for reporting delays under practical constraints remains a
significant challenge in the insurance industry. Such delays render event
occurrences unobservable when their reports are subject to right censoring.
This issue becomes particularly critical when estimating hazard rates for newly
enrolled cohorts with limited follow-up due to administrative censoring. Our
study addresses this challenge by jointly modeling the parametric hazard
functions of event occurrences and report timings. The joint probability
distribution is marginalized over the latent event occurrence status. We
construct an estimator for the proposed survival model and establish its
asymptotic consistency. Furthermore, we develop an expectation-maximization
algorithm to compute its estimates. Using these findings, we propose a
two-stage estimation procedure based on a parametric proportional hazards model
to evaluate observations subject to administrative censoring. Experimental
results demonstrate that our method effectively improves the timeliness of risk
evaluation for newly enrolled cohorts.

</details>


### [129] [Divergence Phase Index: A Riesz-Transform Framework for Multidimensional Phase Difference Analysis](https://arxiv.org/abs/2510.04426)
*Magaly Catanzariti,Hugo Aimar,Diego M. Mateos*

Main category: stat.ML

TL;DR: 提出了发散相位指数（DPI）框架，基于Riesz变换的谐波分析，用于量化一维和多维信号的相位差异。该几何感知度量对强度缩放不变，对结构变化敏感，适用于癫痫脑电、显微图像和绘画等多种数据。


<details>
  <summary>Details</summary>
Motivation: 扩展经典希尔伯特变换相位测量到高维，开发对强度缩放不变且对结构变化敏感的几何感知相位差异度量方法。

Method: 基于Riesz变换的谐波分析，构建发散相位指数框架，应用于一维和多维信号分析。在合成和真实数据集上进行验证，包括癫痫iEEG记录、高分辨率显微图像和绘画作品。

Result: 在一维情况下，DPI能稳健检测与全身性癫痫相关的超同步化；在二维情况下，能揭示图像和艺术品中细微不可察觉的变化；还能检测高度各向同性显微图像中的旋转变化。

Conclusion: DPI对幅度变化具有鲁棒性，且跨领域适应性强，可在非线性动力学、复杂系统分析和多维信号处理等多样化应用中发挥作用。

Abstract: We introduce the Divergence Phase Index (DPI), a novel framework for
quantifying phase differences in one and multidimensional signals, grounded in
harmonic analysis via the Riesz transform. Based on classical Hilbert Transform
phase measures, the DPI extends these principles to higher dimensions, offering
a geometry-aware metric that is invariant to intensity scaling and sensitive to
structural changes. We applied this method on both synthetic and real-world
datasets, including intracranial EEG (iEEG) recordings during epileptic
seizures, high-resolution microscopy images, and paintings. In the 1D case, the
DPI robustly detects hypersynchronization associated with generalized epilepsy,
while in 2D, it reveals subtle, imperceptible changes in images and artworks.
Additionally, it can detect rotational variations in highly isotropic
microscopy images. The DPI's robustness to amplitude variations and its
adaptability across domains enable its use in diverse applications from
nonlinear dynamics, complex systems analysis, to multidimensional signal
processing.

</details>


### [130] [Gini-based Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing](https://arxiv.org/abs/2510.04556)
*Alexej Brauer,Paul Menzel*

Main category: stat.ML

TL;DR: 本文首次系统研究非寿险定价中的概念漂移问题，区分了虚拟漂移和概念漂移，推导了基尼系数的渐近分布，并提出了标准化的监控程序来指导模型重构时机。


<details>
  <summary>Details</summary>
Motivation: 在动态变化的保险环境中，保持定价模型的准确性至关重要。目前缺乏对非寿险定价中概念漂移的系统研究，需要建立有效的监控和重构机制。

Method: 通过文献综述和方法论梳理，区分虚拟漂移与概念漂移；形式化性能度量指标；推导基尼系数的渐近分布；设计标准化监控程序。

Result: 建立了完整的监控框架，能够有效识别概念漂移并确定模型重构时机，通过修改的真实世界投资组合验证了方法的有效性。

Conclusion: 提出的框架为保险定价模型提供了系统化的概念漂移监控方案，有助于维持长期模型性能，并讨论了实际应用中的注意事项和陷阱。

Abstract: In a dynamic landscape where portfolios and environments evolve, maintaining
the accuracy of pricing models is critical. To the best of our knowledge, this
is the first study to systematically examine concept drift in non-life
insurance pricing. We (i) provide an overview of the relevant literature and
commonly used methodologies, clarify the distinction between virtual drift and
concept drift, and explain their implications for long-run model performance;
(ii) review and formalize common performance measures, including the Gini index
and deviance loss, and articulate their interpretation; (iii) derive the
asymptotic distribution of the Gini index, enabling valid inference and
hypothesis testing; and (iv) present a standardized monitoring procedure that
indicates when refitting is warranted. We illustrate the framework using a
modified real-world portfolio with induced concept drift and discuss practical
considerations and pitfalls.

</details>


### [131] [Computing Wasserstein Barycenters through Gradient Flows](https://arxiv.org/abs/2510.04602)
*Eduardo Fernandes Montesuma,Yassir Bendou,Mike Gartrell*

Main category: stat.ML

TL;DR: 提出了一种基于Wasserstein空间中梯度流的可扩展Wasserstein重心计算方法，通过小批量采样解决了现有离散方法的可扩展性问题，并在玩具数据集和领域自适应基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的离散Wasserstein重心计算方法可扩展性差，需要访问输入度量的完整样本集，这限制了其在大规模应用中的使用。

Method: 将原始重心问题重新构造为Wasserstein空间中的梯度流，通过从输入度量中采样小批量来实现可扩展性，并引入概率度量上的泛函来正则化重心问题。

Result: 在玩具数据集和领域自适应基准测试中，该方法在计算Wasserstein重心方面优于先前的离散和基于神经网络的方法。

Conclusion: 提出的基于梯度流的方法为计算Wasserstein重心提供了一种可扩展且有效的解决方案，特别适用于大规模数据集。

Abstract: Wasserstein barycenters provide a powerful tool for aggregating probability
measures, while leveraging the geometry of their ambient space. Existing
discrete methods suffer from poor scalability, as they require access to the
complete set of samples from input measures. We address this issue by recasting
the original barycenter problem as a gradient flow in the Wasserstein space.
Our approach offers two advantages. First, we achieve scalability by sampling
mini-batches from the input measures. Second, we incorporate functionals over
probability measures, which regularize the barycenter problem through internal,
potential, and interaction energies. We present two algorithms for empirical
and Gaussian mixture measures, providing convergence guarantees under the
Polyak-{\L}ojasiewicz inequality. Experimental validation on toy datasets and
domain adaptation benchmarks show that our methods outperform previous discrete
and neural net-based methods for computing Wasserstein barycenters.

</details>


### [132] [Fisher-Bingham-like normalizing flows on the sphere](https://arxiv.org/abs/2510.04762)
*Thorsten Glüsenkamp*

Main category: stat.ML

TL;DR: 提出了ZLP-Fisher流，这是一种新的归一化流方法，用于在球面上建模类似Fisher-Bingham分布的复杂分布，特别适用于尺度变化很大的条件密度估计问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Fisher-Bingham和角高斯分布虽然是在球面上的基本分布，但除了两个特殊情况外，无法直接表示为归一化流。需要一种能够处理尺度变化很大条件密度估计的通用方法。

Method: 通过"缩放-线性-投影"(ZLP)操作构建归一化流，可以逐步增加复杂度，并自然处理尺度变化很大的条件分布。

Result: 开发了ZLP-Fisher流系列，其中Kent类比方法可以廉价地提升现有流的性能，特别在天文学应用中表现良好。

Conclusion: ZLP-Fisher流提供了一种灵活的方法来建模球面上的复杂分布，能够有效处理尺度变化很大的条件密度估计问题，优于现有方法。

Abstract: A generic D-dimensional Gaussian can be conditioned or projected onto the D-1
unit sphere, thereby leading to the well-known Fisher-Bingham (FB) or Angular
Gaussian (AG) distribution families, respectively. These are some of the most
fundamental distributions on the sphere, yet cannot straightforwardly be
written as a normalizing flow except in two special cases: the von-Mises Fisher
in D=3 and the central angular Gaussian in any D. In this paper, we describe
how to generalize these special cases to a family of normalizing flows that
behave similarly to the full FB or AG family in any D. We call them
"zoom-linear-project" (ZLP)-Fisher flows. Unlike a normal Fisher-Bingham
distribution, their composition allows to gradually add complexity as needed.
Furthermore, they can naturally handle conditional density estimation with
target distributions that vary by orders of magnitude in scale - a setting that
is important in astronomical applications but that existing flows often
struggle with. A particularly useful member of the new family is the Kent
analogue that can cheaply upgrade any flow in this situation to yield better
performance.

</details>


### [133] [Kernel ridge regression under power-law data: spectrum and generalization](https://arxiv.org/abs/2510.04780)
*Arie Wortsman,Bruno Loureiro*

Main category: stat.ML

TL;DR: 该论文研究了高维核岭回归在具有各向异性幂律协方差的高斯数据上的表现，推导了多项式内积核的核谱特征，并分析了在这种谱行为下的超额风险，表明样本复杂度由数据有效维度而非环境维度决定。


<details>
  <summary>Details</summary>
Motivation: 研究高维核岭回归在具有各向异性幂律协方差的数据上的表现，这与传统KRR的源和容量条件不同，传统方法通常对核特征谱本身施加幂律假设。

Method: 推导多项式内积核的核谱显式特征化，并对具有这种谱行为的特定核进行超额风险的渐近分析。

Result: 核特征谱继承了数据的衰减特性，样本复杂度由数据有效维度而非环境维度决定，这确立了在幂律各向异性数据上学习相对于各向同性数据的根本优势。

Conclusion: 这是对幂律数据下非线性KRR的首次严格处理，揭示了在具有各向异性幂律协方差的数据上学习的基本优势。

Abstract: In this work, we investigate high-dimensional kernel ridge regression (KRR)
on i.i.d. Gaussian data with anisotropic power-law covariance. This setting
differs fundamentally from the classical source & capacity conditions for KRR,
where power-law assumptions are typically imposed on the kernel eigen-spectrum
itself. Our contributions are twofold. First, we derive an explicit
characterization of the kernel spectrum for polynomial inner-product kernels,
giving a precise description of how the kernel eigen-spectrum inherits the data
decay. Second, we provide an asymptotic analysis of the excess risk in the
high-dimensional regime for a particular kernel with this spectral behavior,
showing that the sample complexity is governed by the effective dimension of
the data rather than the ambient dimension. These results establish a
fundamental advantage of learning with power-law anisotropic data over
isotropic data. To our knowledge, this is the first rigorous treatment of
non-linear KRR under power-law data.

</details>


### [134] [A Noise Resilient Approach for Robust Hurst Exponent Estimation](https://arxiv.org/abs/2510.04811)
*Malith Premarathna,Fabrizio Ruggeri,Dixon Vimalajeewa*

Main category: stat.ML

TL;DR: 提出了NC-ALPHEE方法，通过噪声抑制和神经网络结合多个水平对估计值，改进了基于小波的Hurst指数估计，在噪声环境下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界测量中的加性噪声会降低基于小波的Hurst指数估计精度，需要开发能够有效处理噪声的鲁棒方法。

Method: 在ALPHEE基础上加入噪声抑制，从信号能量对生成多个水平对估计值，使用神经网络替代传统平均方法进行组合。

Result: 无噪声数据中NC-ALPHEE与ALPHEE精度相当；有噪声时传统平均方法性能下降且需要不切实际的水平限制，而NC-ALPHEE始终优于现有技术。

Conclusion: NC-ALPHEE提供了一种鲁棒的自适应Hurst指数估计方法，显著提高了小波方法在噪声环境中的可靠性。

Abstract: Understanding signal behavior across scales is vital in areas such as natural
phenomena analysis and financial modeling. A key property is self-similarity,
quantified by the Hurst exponent (H), which reveals long-term dependencies.
Wavelet-based methods are effective for estimating H due to their multi-scale
analysis capability, but additive noise in real-world measurements often
degrades accuracy. We propose Noise-Controlled ALPHEE (NC-ALPHEE), an
enhancement of the Average Level-Pairwise Hurst Exponent Estimator (ALPHEE),
incorporating noise mitigation and generating multiple level-pairwise estimates
from signal energy pairs. A neural network (NN) combines these estimates,
replacing traditional averaging. This adaptive learning maintains ALPHEE's
behavior in noise-free cases while improving performance in noisy conditions.
Extensive simulations show that in noise-free data, NC-ALPHEE matches ALPHEE's
accuracy using both averaging and NN-based methods. Under noise, however,
traditional averaging deteriorates and requires impractical level restrictions,
while NC-ALPHEE consistently outperforms existing techniques without such
constraints. NC-ALPHEE offers a robust, adaptive approach for H estimation,
significantly enhancing the reliability of wavelet-based methods in noisy
environments.

</details>


### [135] [Set to Be Fair: Demographic Parity Constraints for Set-Valued Classification](https://arxiv.org/abs/2510.04926)
*Eyal Cohen,Christophe Denis,Mohamed Hebiri*

Main category: stat.ML

TL;DR: 该论文提出了在人口统计均等和期望大小约束下的集合值分类方法，包括基于oracle的方法和计算高效的代理方法。


<details>
  <summary>Details</summary>
Motivation: 集合值分类在多类设置中可能放大歧视性偏见，因此需要在公平约束下开发集合值方法。

Method: 提出了两种策略：基于oracle的方法（最小化分类风险同时满足约束）和计算高效的代理方法（优先满足约束）。

Result: 推导了公平集合值分类器的闭式表达式，建立了分布无关的收敛率，并提供了基于oracle方法的超额风险界限。

Conclusion: 实证结果证明了两种策略的有效性，并突出了代理方法的高效性。

Abstract: Set-valued classification is used in multiclass settings where confusion
between classes can occur and lead to misleading predictions. However, its
application may amplify discriminatory bias motivating the development of
set-valued approaches under fairness constraints. In this paper, we address the
problem of set-valued classification under demographic parity and expected size
constraints. We propose two complementary strategies: an oracle-based method
that minimizes classification risk while satisfying both constraints, and a
computationally efficient proxy that prioritizes constraint satisfaction. For
both strategies, we derive closed-form expressions for the (optimal) fair
set-valued classifiers and use these to build plug-in, data-driven procedures
for empirical predictions. We establish distribution-free convergence rates for
violations of the size and fairness constraints for both methods, and under
mild assumptions we also provide excess-risk bounds for the oracle-based
approach. Empirical results demonstrate the effectiveness of both strategies
and highlight the efficiency of our proxy method.

</details>


### [136] [Embracing Discrete Search: A Reasonable Approach to Causal Structure Learning](https://arxiv.org/abs/2510.04970)
*Marcel Wienöbst,Leonard Henckel,Sebastian Weichwald*

Main category: stat.ML

TL;DR: FLOP是一种基于分数的因果发现算法，通过快速父节点选择和迭代Cholesky分数更新，显著减少运行时间，使离散搜索变得可行，在基准测试中实现高精度结构恢复。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现算法运行时间较长，限制了离散搜索的应用。FLOP旨在通过优化算法效率，使离散搜索成为因果发现的可行方法。

Method: 采用快速父节点选择与迭代Cholesky分数更新相结合的方法，支持迭代局部搜索和基于原则的顺序初始化，以找到接近全局最优的图结构。

Result: 在标准设置下实现近乎完美的结构恢复，在多个基准测试中表现出高准确性，运行时间显著优于先前算法。

Conclusion: FLOP的成功表明离散搜索在因果发现中是一个合理且有效的方法，值得重新审视其在领域中的应用价值。

Abstract: We present FLOP (Fast Learning of Order and Parents), a score-based causal
discovery algorithm for linear models. It pairs fast parent selection with
iterative Cholesky-based score updates, cutting run-times over prior
algorithms. This makes it feasible to fully embrace discrete search, enabling
iterated local search with principled order initialization to find graphs with
scores at or close to the global optimum. The resulting structures are highly
accurate across benchmarks, with near-perfect recovery in standard settings.
This performance calls for revisiting discrete search over graphs as a
reasonable approach to causal discovery.

</details>


### [137] [Curiosity-Driven Co-Development of Action and Language in Robots Through Self-Exploration](https://arxiv.org/abs/2510.05013)
*Theodore Jerome Tinker,Kenji Doya,Jun Tani*

Main category: stat.ML

TL;DR: 该研究通过机器人模拟实验探索人类婴儿高效语言和动作协同发展的机制，结合主动推理和强化学习实现好奇心驱动的发育学习，发现好奇心探索、动作发展顺序、组合泛化等关键机制。


<details>
  <summary>Details</summary>
Motivation: 探究人类婴儿为何能从少量学习样本中获得卓越的泛化能力，而大型语言模型需要数十亿训练标记才能达到类似效果，揭示高效发育学习的潜在机制。

Method: 使用机器人进行模拟实验，通过自主探索学习执行命令句对应的动作，整合主动推理框架和强化学习实现好奇心驱动的发育学习。

Result: 发现好奇心驱动探索显著优于无好奇心学习；简单先决动作先发展，复杂动作后发展；机械配对先于组合泛化出现；组合元素越多泛化能力越强。

Conclusion: 研究揭示了婴儿高效协同学习可能的计算机制，为发育心理学发现提供了计算层面的平行证据，展示了好奇心驱动探索在语言动作协同发展中的关键作用。

Abstract: Human infants acquire language and action co-developmentally, achieving
remarkable generalization capabilities from only a minimal number of learning
examples. In contrast, recent large language models require exposure to
billions of training tokens to achieve such generalization. What mechanisms
underlie such efficient developmental learning in humans? This study addresses
this question through simulation experiments in which robots learn to perform
various actions corresponding to imperative sentences (e.g., \textit{push red
cube}) via trials of self-guided exploration. Our approach integrates the
active inference framework with reinforcement learning, enabling
curiosity-driven developmental learning. The simulations yielded several
nontrivial findings: i) Curiosity-driven exploration combined with motor noise
substantially outperforms learning without curiosity. ii) Simpler,
prerequisite-like actions emerge earlier in development, while more complex
actions involving these prerequisites develop later. iii) Rote pairing of
sentences and actions occurs before the emergence of compositional
generalization. iv) Generalization is drastically improved as the number of
compositional elements increases. These results shed light into possible
mechanisms underlying efficient co-developmental learning in infants and
provide computational parallels to findings in developmental psychology.

</details>


### [138] [Causal Abstractions, Categorically Unified](https://arxiv.org/abs/2510.05033)
*Markus Englberger,Devendra Singh Dhami*

Main category: stat.ML

TL;DR: 提出了一个用于关联不同抽象层次因果模型的范畴框架，通过马尔可夫函子的自然变换定义因果抽象，统一并推广了现有因果抽象概念。


<details>
  <summary>Details</summary>
Motivation: 现有因果抽象方法缺乏统一的数学框架，需要建立能够整合各种因果抽象性质并证明现有结果的系统化理论。

Method: 使用范畴论工具，定义因果抽象为马尔可夫函子间的自然变换，利用弦图工具描述干预下的一致性抽象图。

Result: 统一并推广了现有因果抽象概念，获得了现有结果的范畴证明，展示了机械可解释性方法在框架中的适用性，并推广了do-演算在高阶抽象图上的有效性。

Conclusion: 该范畴框架比现有框架更适合建模因果抽象，能够恢复τ一致性和构造性τ抽象等概念，为因果抽象研究提供了更合适的理论基础。

Abstract: We present a categorical framework for relating causal models that represent
the same system at different levels of abstraction. We define a causal
abstraction as natural transformations between appropriate Markov functors,
which concisely consolidate desirable properties a causal abstraction should
exhibit. Our approach unifies and generalizes previously considered causal
abstractions, and we obtain categorical proofs and generalizations of existing
results on causal abstractions. Using string diagrammatical tools, we can
explicitly describe the graphs that serve as consistent abstractions of a
low-level graph under interventions. We discuss how methods from mechanistic
interpretability, such as circuit analysis and sparse autoencoders, fit within
our categorical framework. We also show how applying do-calculus on a
high-level graphical abstraction of an acyclic-directed mixed graph (ADMG),
when unobserved confounders are present, gives valid results on the low-level
graph, thus generalizing an earlier statement by Anand et al. (2023). We argue
that our framework is more suitable for modeling causal abstractions compared
to existing categorical frameworks. Finally, we discuss how notions such as
$\tau$-consistency and constructive $\tau$-abstractions can be recovered with
our framework.

</details>
