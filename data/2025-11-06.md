<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 24]
- [cs.LG](#cs.LG) [Total: 72]
- [stat.ML](#stat.ML) [Total: 8]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [AI-Enhanced Wi-Fi Sensing Through Single Transceiver Pair](https://arxiv.org/abs/2511.02845)
*Yuxuan Liu,Chiya Zhang,Yifeng Yuan,Chunlong He,Weizheng Zhang,Gaojie Chen*

Main category: eess.SP

TL;DR: AI驱动的Wi-Fi感知系统在硬件受限条件下通过先验信息和时间相关性突破传统雷达理论的分辨率限制，实现高精度人体姿态估计和室内定位。


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入探讨AI技术如何突破传统雷达理论分辨率限制的理论基础，特别是在Wi-Fi感知系统中AI性能提升的来源机制。

Method: 开发基于AI的Wi-Fi感知系统，使用单收发对进行实验，重点验证人体姿态估计和室内定位任务中的先验信息和时间相关性作用。

Result: 实验证实时间相关性有助于降低感知误差上限，先验信息使AI能够基于模糊输入生成合理细节，两者共同贡献性能提升。

Conclusion: 在硬件受限的Wi-Fi感知系统中，AI的性能增益主要来源于先验信息和时间相关性，这为理解AI突破传统分辨率限制提供了理论解释。

Abstract: The advancement of next-generation Wi-Fi technology heavily relies on sensing
capabilities, which play a pivotal role in enabling sophisticated applications.
In response to the growing demand for large-scale deployments, contemporary
Wi-Fi sensing systems strive to achieve high-precision perception while
maintaining minimal bandwidth consumption and antenna count requirements.
Remarkably, various AI-driven perception technologies have demonstrated the
ability to surpass the traditional resolution limitations imposed by radar
theory. However, the theoretical underpinnings of this phenomenon have not been
thoroughly investigated in existing research. In this study, we found that
under hardware-constrained conditions, the performance gains brought by AI to
Wi-Fi sensing systems primarily originate from two aspects: prior information
and temporal correlation. Prior information enables the AI to generate
plausible details based on vague input, while temporal correlation helps reduce
the upper bound of sensing error. We developed an AI-based Wi-Fi sensing system
using a single transceiver pair and designed experiments focusing on human pose
estimation and indoor localization to validate the theoretical claims. The
results confirm the performance gains contributed by temporal correlation and
prior information.

</details>


### [2] [Spatio-Temporal Attention Network for Epileptic Seizure Prediction](https://arxiv.org/abs/2511.02846)
*Zan Li,Kyongmin Yeo,Wesley Gifford,Lara Marcuse,Madeline Fields,Bülent Yener*

Main category: eess.SP

TL;DR: 提出了一种基于时空注意力网络（STAN）的深度学习框架，用于癫痫患者癫痫发作的准确预测，通过建模脑电信号的复杂时空相关性并利用对抗鉴别器区分发作前和发作间期的注意力模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖特征工程和/或假设固定的发作前持续时间，无法有效捕捉脑电信号的复杂时空相关性，需要一种能够同时建模时空相关性并实现患者特异性学习的方法。

Method: 使用时空注意力网络（STAN）同时建模时空相关性，并采用对抗鉴别器来区分发作前和发作间期的注意力模式，实现患者特异性学习。

Result: 在CHB-MIT数据集上达到96.6%的敏感度和0.011/h的误检率，在MSSM数据集上达到94.2%的敏感度和0.063/h的误检率，显著优于现有最先进方法。能够可靠地在发作前至少15分钟检测到发作前状态，患者特异性窗口可延长至45分钟。

Conclusion: 该框架能够准确预测癫痫发作，提供足够的干预时间，在临床应用中具有重要价值。

Abstract: In this study, we present a deep learning framework that learns complex
spatio-temporal correlation structures of EEG signals through a Spatio-Temporal
Attention Network (STAN) for accurate predictions of onset of seizures for
Epilepsy patients. Unlike existing methods, which rely on feature engineering
and/or assume fixed preictal durations, our approach simultaneously models
spatio-temporal correlations through STAN and employs an adversarial
discriminator to distinguish preictal from interictal attention patterns,
enabling patient-specific learning. Evaluation on CHB-MIT and MSSM datasets
demonstrates 96.6\% sensitivity with 0.011/h false detection rate on CHB-MIT,
and 94.2% sensitivity with 0.063/h FDR on MSSM, significantly outperforming
state-of-the-art methods. The framework reliably detects preictal states at
least 15 minutes before an onset, with patient-specific windows extending to 45
minutes, providing sufficient intervention time for clinical applications.

</details>


### [3] [EEGReXferNet: A Lightweight Gen-AI Framework for EEG Subspace Reconstruction via Cross-Subject Transfer Learning and Channel-Aware Embedding](https://arxiv.org/abs/2511.02848)
*Shantanu Sarkar,Piotr Nabrzyski,Saurabh Prasad,Jose Luis Contreras-Vidal*

Main category: eess.SP

TL;DR: EEGReXferNet是一个轻量级生成AI框架，通过跨受试者迁移学习进行EEG子空间重建，解决了传统EEG信号去噪方法的局限性，提高了时空频谱分辨率并减少计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统EEG信号去噪方法需要人工干预或在滤波/重建过程中可能抑制关键神经特征，而现有的生成模型方法缺乏集成的时空频谱敏感性且计算量大，不适合脑机接口等实时应用。

Method: EEGReXferNet采用模块化架构，利用相邻通道的容积传导、频带特定卷积编码和滑动窗口的动态潜在特征提取，结合基于参考的缩放确保窗口间连续性。

Result: 该框架显著提高了时空频谱分辨率（平均PSD相关性≥0.95；平均频谱图RV系数≥0.85），总权重减少约45%以减少过拟合，保持计算效率。

Conclusion: EEGReXferNet为神经生理学和脑机接口应用提供了稳健、实时的EEG预处理解决方案，在保持高性能的同时显著降低了计算复杂度。

Abstract: Electroencephalography (EEG) is a widely used non-invasive technique for
monitoring brain activity, but low signal-to-noise ratios (SNR) due to various
artifacts often compromise its utility. Conventional artifact removal methods
require manual intervention or risk suppressing critical neural features during
filtering/reconstruction. Recent advances in generative models, including
Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs),
have shown promise for EEG reconstruction; however, these approaches often lack
integrated temporal-spectral-spatial sensitivity and are computationally
intensive, limiting their suitability for real-time applications like
brain-computer interfaces (BCIs). To overcome these challenges, we introduce
EEGReXferNet, a lightweight Gen-AI framework for EEG subspace reconstruction
via cross-subject transfer learning - developed using Keras TensorFlow
(v2.15.1). EEGReXferNet employs a modular architecture that leverages volume
conduction across neighboring channels, band-specific convolution encoding, and
dynamic latent feature extraction through sliding windows. By integrating
reference-based scaling, the framework ensures continuity across successive
windows and generalizes effectively across subjects. This design improves
spatial-temporal-spectral resolution (mean PSD correlation >= 0.95; mean
spectrogram RV-Coefficient >= 0.85), reduces total weights by ~45% to mitigate
overfitting, and maintains computational efficiency for robust, real-time EEG
preprocessing in neurophysiological and BCI applications.

</details>


### [4] [Benchmarking ResNet for Short-Term Hypoglycemia Classification with DiaData](https://arxiv.org/abs/2511.02849)
*Beyza Cinar,Maria Maleshkova*

Main category: eess.SP

TL;DR: 该研究改进了T1D数据集DiaData的数据质量，通过异常值处理、缺失值插补等方法，并分析了血糖与心率的相关性，构建了低血糖预测模型。


<details>
  <summary>Details</summary>
Motivation: T1D个体化治疗需要高质量数据，但现有数据存在异常值、噪声和小样本问题，影响分析的可靠性。

Method: 使用IQR方法识别异常值并替换为缺失值；小间隔（≤25分钟）用线性插值，大间隔（30-120分钟）用Stineman插值；分析血糖与心率相关性；用ResNet模型进行低血糖分类预测。

Result: Stineman插值比线性插值提供更真实的血糖估计；血糖与心率在低血糖前15-60分钟呈中等相关；使用更多数据提升性能7%，使用质量优化数据比原始数据提升2-3%。

Conclusion: 数据质量改进对T1D分析至关重要，高质量数据能提升预测模型性能，为个体化治疗提供更可靠的基础。

Abstract: Individualized therapy is driven forward by medical data analysis, which
provides insight into the patient's context. In particular, for Type 1 Diabetes
(T1D), which is an autoimmune disease, relationships between demographics,
sensor data, and context can be analyzed. However, outliers, noisy data, and
small data volumes cannot provide a reliable analysis. Hence, the research
domain requires large volumes of high-quality data. Moreover, missing values
can lead to information loss. To address this limitation, this study improves
the data quality of DiaData, an integration of 15 separate datasets containing
glucose values from 2510 subjects with T1D. Notably, we make the following
contributions: 1) Outliers are identified with the interquartile range (IQR)
approach and treated by replacing them with missing values. 2) Small gaps
($\le$ 25 min) are imputed with linear interpolation and larger gaps ($\ge$ 30
and $<$ 120 min) with Stineman interpolation. Based on a visual comparison,
Stineman interpolation provides more realistic glucose estimates than linear
interpolation for larger gaps. 3) After data cleaning, the correlation between
glucose and heart rate is analyzed, yielding a moderate relation between 15 and
60 minutes before hypoglycemia ($\le$ 70 mg/dL). 4) Finally, a benchmark for
hypoglycemia classification is provided with a state-of-the-art ResNet model.
The model is trained with the Maindatabase and Subdatabase II of DiaData to
classify hypoglycemia onset up to 2 hours in advance. Training with more data
improves performance by 7% while using quality-refined data yields a 2-3% gain
compared to raw data.

</details>


### [5] [ECGXtract: Deep Learning-based ECG Feature Extraction for Automated CVD Diagnosis](https://arxiv.org/abs/2511.02850)
*Youssif Abuzied,Hassan AbdEltawab,Abdelrhman Gaber,Tamer ElBatt*

Main category: eess.SP

TL;DR: ECGXtract是一种基于深度学习的可解释ECG特征提取方法，通过CNN模型提取与临床验证真值强相关的时域和形态特征，在多个实验设置下表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统信号处理和黑盒机器学习方法在ECG特征提取中的局限性，开发可解释且准确的深度学习方法。

Method: 使用卷积神经网络模型，先训练单特征提取模型确保精确性和可解释性，然后探索多特征同时提取的可行性，包括语义分组等方法。

Result: ECGXtract在全局特征上平均相关系数为0.80，导联II表现最佳；导联特定特征平均相关系数为0.822；在90%的特征上优于ECGdeli；语义分组对全局特征有效。

Conclusion: 结构化分组策略能在计算效率和模型精度之间取得平衡，为资源受限环境下开发可扩展且临床可解释的ECG特征提取系统铺平道路。

Abstract: This paper presents ECGXtract, a deep learning-based approach for
interpretable ECG feature extraction, addressing the limitations of traditional
signal processing and black-box machine learning methods. In particular, we
develop convolutional neural network models capable of extracting both temporal
and morphological features with strong correlations to a clinically validated
ground truth. Initially, each model is trained to extract a single feature,
ensuring precise and interpretable outputs. A series of experiments is then
carried out to evaluate the proposed method across multiple setups, including
global versus lead-specific features, different sampling frequencies, and
comparisons with other approaches such as ECGdeli. Our findings show that
ECGXtract achieves robust performance across most features with a mean
correlation score of 0.80 with the ground truth for global features, with lead
II consistently providing the best results. For lead-specific features,
ECGXtract achieves a mean correlation score of 0.822. Moreover, ECGXtract
achieves superior results to the state-of-the-art open source ECGdeli as it got
a higher correlation score with the ground truth in 90% of the features.
Furthermore, we explore the feasibility of extracting multiple features
simultaneously utilizing a single model. Semantic grouping is proved to be
effective for global features, while large-scale grouping and lead-specific
multi-output models show notable performance drops. These results highlight the
potential of structured grouping strategies to balance the computational
efficiency vs. model accuracy, paving the way for more scalable and clinically
interpretable ECG feature extraction systems in limited resource settings.

</details>


### [6] [Approaching Low-Cost Cardiac Intelligence with Semi-Supervised Knowledge Distillation](https://arxiv.org/abs/2511.02851)
*Rushuang Zhou,Yuan-Ting Zhang,M. Jamal Deen,Yining Dong*

Main category: eess.SP

TL;DR: LiteHeart是一个半监督知识蒸馏框架，通过区域感知蒸馏和跨层互信息模块，显著缩小了低成本心脏智能系统与高成本系统之间的诊断性能差距。


<details>
  <summary>Details</summary>
Motivation: 先进心脏AI在日常监测中面临医疗数据依赖和计算资源需求高的挑战，低成本心脏智能系统虽然使用可穿戴设备数据，但与高成本系统存在显著性能差距。

Method: 提出LiteHeart框架，包含区域感知蒸馏模块（模拟医生关注诊断相关ECG区域）和跨层互信息模块（对齐LCCI和HCCI决策过程），采用半监督训练策略提升模型鲁棒性。

Result: 在覆盖38种心血管疾病的5个数据集上评估，LiteHeart将LCCI与HCCI的性能差距显著缩小，在宏观F1分数上比现有方法提升4.27%至7.10%。

Conclusion: LiteHeart显著增强了低成本心脏智能系统的诊断能力，为使用可穿戴技术实现可扩展、经济实惠且准确的日常心脏医疗保健铺平了道路。

Abstract: Deploying advanced cardiac artificial intelligence for daily cardiac
monitoring is hindered by its reliance on extensive medical data and high
computational resources. Low-cost cardiac intelligence (LCCI) offers a
promising alternative by using wearable device data, such as 1-lead
electrocardiogram (ECG), but it suffers from a significant diagnostic
performance gap compared to high-cost cardiac intelligence (HCCI). To bridge
this gap, we propose LiteHeart, a semi-supervised knowledge distillation
framework. LiteHeart introduces a region-aware distillation module to mimic how
cardiologists focus on diagnostically relevant ECG regions and a cross-layer
mutual information module to align the decision processes of LCCI and HCCI
systems. Using a semi-supervised training strategy, LiteHeart further improves
model robustness under limited supervision. Evaluated on five datasets covering
over 38 cardiovascular diseases, LiteHeart substantially reduces the
performance gap between LCCI and HCCI, outperforming existing methods by 4.27%
to 7.10% in macro F1 score. These results demonstrate that LiteHeart
significantly enhances the diagnostic capabilities of low-cost cardiac
intelligence systems, paving the way for scalable, affordable, and accurate
daily cardiac healthcare using wearable technologies.

</details>


### [7] [Real-Time Interactive Hybrid Ocean: Spectrum-Consistent Wave Particle-FFT Coupling](https://arxiv.org/abs/2511.02852)
*Shengze Xue,Yu Ren,Jiacheng Hong,Run Ni,Shuangjiu Xiao,Deli Dong*

Main category: eess.SP

TL;DR: 提出了一种实时交互式混合海洋模拟方法，结合全局FFT背景和局部波粒子区域，在统一频谱参数下实现大尺度真实性和细粒度交互性。


<details>
  <summary>Details</summary>
Motivation: 传统FFT方法假设全局平稳性和空间均匀性，难以表示非均匀海洋和近场交互；而波粒子方法能捕捉局部波浪但计算成本高且难以匹配全局频谱统计。

Method: 使用全局FFT背景与局部波粒子区域耦合，在统一频谱参数和色散关系下驱动；在边界处根据相同方向频谱注入粒子，保持频率-方向分布与背景一致。

Result: 实现了大尺度频谱一致性的同时支持局部波浪和涟漪，通过基于频率桶的粒子采样和GPU并行合成方案保持实时交互性能。

Conclusion: 该混合方法提供了一个统一框架，在实时条件下同时实现了大尺度频谱真实性和细粒度交互性。

Abstract: Fast Fourier Transform-based (FFT) spectral oceans are widely adopted for
their efficiency and large-scale realism, but they assume global stationarity
and spatial homogeneity, making it difficult to represent non-uniform seas and
near-field interactions (e.g., ships and floaters). In contrast, wave particles
capture local wakes and ripples, yet are costly to maintain at scale and hard
to match global spectral statistics.We present a real-time interactive hybrid
ocean: a global FFT background coupled with local wave-particle (WP) patch
regions around interactive objects, jointly driven under a unified set of
spectral parameters and dispersion. At patch boundaries, particles are injected
according to the same directional spectrum as the FFT, aligning the local
frequency-direction distribution with the background and matching energy
density, without disturbing the far field.Our approach introduces two main
innovations: (1) Hybrid ocean representation. We couple a global FFT background
with local WP patches under a unified spectrum, achieving large-scale spectral
consistency while supporting localized wakes and ripples.(2) Frequency-bucketed
implementation. We design a particle sampling and GPU-parallel synthesis scheme
based on frequency buckets, which preserves spectral energy consistency and
sustains real-time interactive performance.Together, these innovations enable a
unified framework that delivers both large-scale spectral realism and
fine-grained interactivity in real time.

</details>


### [8] [Consciousness-ECG Transformer for Conscious State Estimation System with Real-Time Monitoring](https://arxiv.org/abs/2511.02853)
*Young-Seok Kweon,Gi-Hwan Shin,Ji-Yong Kim,Bokyeong Ryu,Seong-Whan Lee*

Main category: eess.SP

TL;DR: 提出了基于心电图的意识状态估计系统，使用解耦查询注意力的transformer模型，在睡眠分期和麻醉监测任务上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统意识状态监测主要依赖脑电图，但易受噪声干扰且需要受控环境。心电图提供了一种非侵入性、可靠的替代方案。

Method: 使用transformer模型配合解耦查询注意力机制，有效捕捉心率变异性特征来区分意识和无意识状态，并实现实时监测系统。

Result: 在睡眠分期任务上准确率0.877，麻醉监测准确率0.880；AUC值分别为0.786和0.895，均优于基线模型。

Conclusion: 基于心电图的意识监测系统为临床动态环境提供了实用且稳健的替代方案，有望提升患者安全并增进对意识状态的理解。

Abstract: Conscious state estimation is important in various medical settings,
including sleep staging and anesthesia management, to ensure patient safety and
optimize health outcomes. Traditional methods predominantly utilize
electroencephalography (EEG), which faces challenges such as high sensitivity
to noise and the requirement for controlled environments. In this study, we
propose the consciousness-ECG transformer that leverages electrocardiography
(ECG) signals for non-invasive and reliable conscious state estimation. Our
approach employs a transformer with decoupled query attention to effectively
capture heart rate variability features that distinguish between conscious and
unconscious states. We implemented the conscious state estimation system with
real-time monitoring and validated our system on datasets involving sleep
staging and anesthesia level monitoring during surgeries. Experimental results
demonstrate that our model outperforms baseline models, achieving accuracies of
0.877 on sleep staging and 0.880 on anesthesia level monitoring. Moreover, our
model achieves the highest area under curve values of 0.786 and 0.895 on sleep
staging and anesthesia level monitoring, respectively. The proposed system
offers a practical and robust alternative to EEG-based methods, particularly
suited for dynamic clinical environments. Our results highlight the potential
of ECG-based consciousness monitoring to enhance patient safety and advance our
understanding of conscious states.

</details>


### [9] [NEF-NET+: Adapting Electrocardio panorama in the wild](https://arxiv.org/abs/2511.02880)
*Zehui Zhan,Yaojun Hu,Jiajing Zhan,Wanchen Lian,Wanqing Wu,Jintai Chen*

Main category: eess.SP

TL;DR: NEF-NET+是一个增强的心电图全景合成框架，能够从任意视角生成任意长度的心电信号，克服了传统心电图系统的固定视角限制，并在真实场景下比前代模型Nef-Net提升了约6dB的PSNR。


<details>
  <summary>Details</summary>
Motivation: 传统多导联心电图系统只能从固定解剖视角捕获心脏信号，但某些心脏疾病需要非标准视角才能显示诊断关键模式。前代模型Nef-Net存在理想化假设，在真实场景下面临长时程建模、设备特定伪影和电极放置偏差等挑战。

Method: 设计了新的模型架构执行直接视角变换，包含离线预训练、设备校准调优步骤以及用于患者特定适应的实时校准步骤。构建了包含5367个记录、每个受试者48个视角的Panobench基准数据集。

Result: 实验结果显示NEF-NET+相比Nef-Net在真实场景下PSNR提升了约6dB，能够支持任意长度信号合成，跨设备泛化，并补偿电极放置偏差。

Conclusion: NEF-NET+为现实全景心电图合成提供了增强框架，显著提升了在真实环境下的性能，为心脏疾病的更准确诊断提供了技术支持。

Abstract: Conventional multi-lead electrocardiogram (ECG) systems capture cardiac
signals from a fixed set of anatomical viewpoints defined by lead placement.
However, certain cardiac conditions (e.g., Brugada syndrome) require
additional, non-standard viewpoints to reveal diagnostically critical patterns
that may be absent in standard leads. To systematically overcome this
limitation, Nef-Net was recently introduced to reconstruct a continuous
electrocardiac field, enabling virtual observation of ECG signals from
arbitrary views (termed Electrocardio Panorama). Despite its promise, Nef-Net
operates under idealized assumptions and faces in-the-wild challenges, such as
long-duration ECG modeling, robustness to device-specific signal artifacts, and
suboptimal lead placement calibration. This paper presents NEF-NET+, an
enhanced framework for realistic panoramic ECG synthesis that supports
arbitrary-length signal synthesis from any desired view, generalizes across ECG
devices, and compensates for operator-induced deviations in electrode
placement. These capabilities are enabled by a newly designed model
architecture that performs direct view transformation, incorporating a workflow
comprising offline pretraining, device calibration tuning steps as well as an
on-the-fly calibration step for patient-specific adaptation. To rigorously
evaluate panoramic ECG synthesis, we construct a new Electrocardio Panorama
benchmark, called Panobench, comprising 5367 recordings with 48-view per
subject, capturing the full spatial variability of cardiac electrical activity.
Experimental results show that NEF-NET+ delivers substantial improvements over
Nef-Net, yielding an increase of around 6 dB in PSNR in real-world setting. The
code and Panobench will be released in a subsequent publication.

</details>


### [10] [Adaptive Internal Calibration for Temperature-Robust mmWave FMCW Radars](https://arxiv.org/abs/2511.02884)
*Dariush Salami,Nima Bahmani,Hüseyin Yiğitler,Stephan Sigg*

Main category: eess.SP

TL;DR: 提出一种毫米波FMCW雷达内部校准框架，通过温度补偿模型减少温度漂移对测量精度的影响，在密集无线网络中确保稳健性能。


<details>
  <summary>Details</summary>
Motivation: 解决毫米波FMCW雷达在内部温度变化下的性能漂移问题，特别是在密集无线网络部署中需要保持可靠性和准确性。

Method: 利用内部传感器数据和信号处理技术构建温度补偿模型，通过处理中频信号来校正温度引起的硬件漂移。

Result: 实验结果显示，该方法将中频信号幅度与内部温度漂移的皮尔逊相关性降低了84%，显著减轻了温度漂移影响，且计算开销小。

Conclusion: 该框架有效提升了毫米波雷达在温度变化环境下的鲁棒性，适用于密集网络部署，并遵循伦理设计原则避免使用敏感外部数据。

Abstract: We present a novel internal calibration framework for Millimeter- Wave
(mmWave) Frequency-Modulated Continuous-Wave (FMCW) radars to ensure robust
performance under internal temperature variations, tailored for deployment in
dense wireless networks. Our approach mitigates the impact of
temperature-induced drifts in radar hardware, enhancing reliability. We propose
a temperature compensation model that leverages internal sensor data and signal
processing techniques to maintain measurement accuracy. Experimental results
demonstrate improved robustness across a range of internal temperature
conditions, with minimal computational overhead, ensuring scalability in dense
network environments. The framework also incorporates ethical design
principles, avoiding reliance on sensitive external data. The proposed scheme
reduces the Pearson correlation between the amplitude of the Intermediate
Frequency (IF) signal and internal temperature drift up to 84%, significantly
mitigating the temperature drift.

</details>


### [11] [From Narrow to Wide: Autoencoding Transformers for Ultrasound Bandwidth Recovery](https://arxiv.org/abs/2511.02938)
*Sepideh KhakzadGharamaleki,Hassan Rivaz,Brandon Helfield*

Main category: eess.SP

TL;DR: 使用深度学习将窄带超声信号转换为宽带信号，提升图像分辨率而不需要硬件升级


<details>
  <summary>Details</summary>
Motivation: 传统脉冲回波超声在使用低成本窄带探头时，脉冲会被拉长，高频细节会丢失，限制了图像分辨率

Method: 训练基于Tiny Vision Transformer自编码器的数据驱动映射，从窄带RF线谱图映射到宽带谱图，使用课程加权损失在仿真数据上训练

Result: 在异质斑点囊肿体模上，图像域MSE降低90%，PSNR提升6.7 dB，SSIM达到0.965；在未见过的分辨率体模上也能锐化点目标行，显示强泛化能力

Conclusion: 纯软件升级可以让已安装的窄带探头获得类似宽带探头的性能，有望在资源受限环境中推广高分辨率超声

Abstract: Conventional pulse-echo ultrasound suffers when low-cost probes deliver only
narrow fractional bandwidths, elongating pulses and erasing high-frequency
detail. We address this limitation by learning a data-driven mapping from
band-limited to broadband spectrogram of radio-frequency (RF) lines. To this
end, a variation of Tiny Vision Transform (ViT) auto-encoder is trained on
simulation data using a curriculum-weighted loss. On heterogeneous speckle-cyst
phantoms, the network reduces image-domain MSE by 90 percent, boosts PSNR by
6.7 dB, and raises SSIM to 0.965 compared with the narrow-band input. It also
sharpens point-target rows in a completely unseen resolution phantom,
demonstrating strong out-of-distribution generalisation without sacrificing
frame rate or phase information. These results indicate that a purely software
upgrade can endow installed narrow-band probes with broadband-like performance,
potentially widening access to high-resolution ultrasound in
resource-constrained settings.

</details>


### [12] [Consensus Tracking of an Underwater Vehicle Using Weighted Harmonic Mean Density](https://arxiv.org/abs/2511.03130)
*Ved Prakash Dubey,Shovan Bhaumik*

Main category: eess.SP

TL;DR: 提出基于加权调和均值密度的水下目标跟踪方法，通过最小化KL散度分配最优权重，在分布式跟踪中优于现有融合方法


<details>
  <summary>Details</summary>
Motivation: 解决水下目标跟踪问题，大量声纳浮标部署在监控区域，需要实现分布式跟踪器之间的共识融合

Method: 使用加权调和均值密度进行跟踪共识，通过最小化Kullback-Leibler散度度量来分配最优权重，提供高斯密度融合解决方案

Result: 仿真结果显示，优化的HMD融合方法在均方根误差、轨迹发散百分比和归一化估计误差平方方面优于现有融合方法

Conclusion: 提出的加权调和均值密度融合方法在分布式跟踪中表现出优越性能，能够有效实现跟踪器间的共识

Abstract: This paper addresses an underwater target tracking problem in which a large
number of sonobuoy sensors are deployed on a surveillance region. The region is
divided into several sub-regions, where a single tracker, capable of generating
track is installed. Each sonobuoy can measure the direction of arrival of
acoustic signals (known as bearing angles) and communicate the measurements
with the local tracker. Further, each local tracker can communicate with all
other trackers, where each of them can exchange their estimate and finally a
consensus is reached. We propose a weighted harmonic mean density (HMD) based
tracking to reach a consensus and provide a solution for the fusion of Gaussian
densities. In this approach, optimal weights are assigned by minimizing the
Kullback-Leibler divergence measure. Performance of the proposed method is
measured using root mean square error, percentage of track divergence, and
normalized estimation error squared. Simulation results demonstrate that the
optimized HMD-based fusion outperforms existing fusion methods during a
distributed tracking.

</details>


### [13] [Analysis and Algorithm for Multi IRS Collaborative Localization via Hybrid Time Angle Estimation](https://arxiv.org/abs/2511.03133)
*Ziheng Zhang,Wen Chen,Qingqing Wu,Haoran Qin,Zhendong Li,Qiong Wu*

Main category: eess.SP

TL;DR: 提出一种多智能反射面协同混合定位系统，通过联合时延和角度估计实现目标定位，设计了高效的角度和位置估计算法，在低信噪比条件下具有优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统定位系统在复杂环境中性能受限，需要开发更精确的定位技术来满足现代通信和感知需求。

Method: 使用多个IRS部署在目标区域附近，通过联合处理所有反射元件的回波信号来估计时延和角度参数，设计了基于原子范数最小化和ADMM的角度估计算法，以及三阶段定位算法。

Result: 数值仿真验证了系统的优越性，特别是在低信噪比条件下，系统协作、混合定位和分布式部署提供了显著优势。

Conclusion: 所提出的多IRS协同混合定位系统能够实现精确的目标定位，特别是在挑战性环境中表现出色。

Abstract: This paper proposes a novel multiple intelligent reflecting surfaces (IRSs)
collaborative hybrid localization system, which involves deploying multiple
IRSs near the target area and achieving target localization through joint time
delay and angle estimation. Specifically, echo signals from all reflective
elements are received by each sensor and jointly processed to estimate the time
delay and angle parameters. Based on the above model, we derive the Fisher
Information Matrix (FIM) for cascaded delay, Angle of Arrival (AOA), and Angle
of Departure (AOD) estimation in semi passive passive models, along with the
corresponding Cramer Rao Bound (CRB). To achieve precise estimation close to
the CRB, we design efficient algorithms for angle and location estimation. For
angle estimation, reflective signals are categorized into three cases based on
their rank, with different signal preprocessing. By constructing an atomic norm
set and minimizing the atomic norm, the joint angle estimation problem is
transformed into a convex optimization problem, and low-complexity estimation
of multiple AOA and AOD pairs is achieved using the Alternating Direction
Method of Multipliers (ADMM). For location estimation, we propose a three-stage
localization algorithm that combines weighted least squares, total least
squares, and quadratic correction to handle errors in the coefficient matrix
and observation vector, thus improving accuracy. Numerical simulations validate
the superiority of the proposed system, demonstrating that the system's
collaboration, hybrid localization, and distributed deployment provide
substantial benefits, as well as the accuracy of the proposed estimation
algorithms, particularly in low signal to noise ratio (SNR) condition.

</details>


### [14] [Multimodal-Wireless: A Large-Scale Dataset for Sensing and Communication](https://arxiv.org/abs/2511.03220)
*Tianhao Mao,Le Liang,Jie Yang,Hao Ye,Shi Jin,Geoffrey Ye Li*

Main category: eess.SP

TL;DR: Multimodal-Wireless是一个用于无线通信研究的开源多模态感知数据集，包含约16万帧数据，涵盖通信信道、激光雷达、RGB和深度相机、IMU和雷达等多种传感模式。


<details>
  <summary>Details</summary>
Motivation: 为无线通信研究提供综合的多模态感知数据集，支持通信和协同感知等研究应用。

Method: 基于CARLA模拟器和Sionna框架构建集成化可定制数据采集管道，在四个虚拟城镇、十六个通信场景和三种天气条件下收集数据。

Result: 成功创建包含约160,000帧多模态数据的开放数据集，涵盖多种传感模式，并展示了在波束预测等应用中的潜力。

Conclusion: Multimodal-Wireless数据集为无线通信研究提供了宝贵的资源，支持多模态学习在通信和感知任务中的应用探索。

Abstract: This paper presents Multimodal-Wireless, an open-source multimodal sensing
dataset designed for wireless communication research. The dataset is generated
through an integrated and customizable data pipeline built upon the CARLA
simulator and Sionna framework. It contains approximately 160,000 frames
collected across four virtual towns, sixteen communication scenarios, and three
weather conditions, encompassing multiple sensing modalities--communication
channel, light detection and ranging, RGB and depth cameras, inertial
measurement unit, and radar. This paper provides a comprehensive overview of
the dataset, outlining its key features, overall framework, and technical
implementation details. In addition, it explores potential research
applications concerning communication and collaborative perception, exemplified
by beam prediction using a multimodal large language model. The dataset is open
in https://le-liang.github.io/mmw/.

</details>


### [15] [Integrated Sensing and Communication with UAV Swarms via Decentralized Consensus ADMM](https://arxiv.org/abs/2511.03283)
*Zhiyuan Zhai,Wei Ni,Xin Wang,Dusit Niyato,Ekram Hossain*

Main category: eess.SP

TL;DR: 提出了一种去中心化的无人机群优化框架，通过共识ADMM算法让无人机并行决策位置，达成全局最优的ISAC几何配置，显著提升了通信和感知性能。


<details>
  <summary>Details</summary>
Motivation: 无人机群可以形成虚拟天线阵列来增强集成感知与通信(ISAC)，但由于群集的分布式特性和单个无人机缺乏全局视野，位置优化具有挑战性。

Method: 开发了去中心化共识ADMM算法，将全局目标分解为局部投影更新、代理辅助的共识协调和轻量级对偶更新，确保群集的可扩展性和一致性。

Result: 仿真表明共识ADMM算法具有快速收敛性和强可扩展性，无人机群在通信和感知性能上显著优于固定阵列基线。

Conclusion: 该去中心化优化框架能够有效解决无人机群位置优化问题，为ISAC应用提供了可扩展的解决方案。

Abstract: UAV swarms can form virtual antenna arrays to exploit additional spatial
degrees of freedom and enhance integrated sensing and communication (ISAC). The
optimization of UAV positions is challenging due to the distributed nature of
swarms and the lack of a global view at individual UAVs.
  This paper presents a new decentralized optimization framework that allows
UAVs to decide their locations in parallel and reach consensus on a globally
optimal swarm geometry for ISAC.
  Specifically, we derive the achievable uplink rate and Cram\'er-Rao Bound
(CRB) as tractable metrics for communication and sensing, respectively.
  The UAV positions are optimized to balance maximizing the communication rate
and minimizing the CRB.
  To solve this non-convex problem with coupled variables, we develop a
decentralized consensus alternating direction method of multipliers (ADMM)
algorithm, which enables the UAVs to iteratively align their local updates and
reach consensus.
  The algorithm decomposes the global objective into local projection updates,
proxy-assisted consensus coordination, and lightweight dual updates, ensuring
scalability and consistency throughout the swarm.
  Simulations demonstrate that the proposed consensus ADMM algorithm converges
rapidly with strong scalability, and that the UAV swarm significantly
outperforms fixed-array baselines in both communication and sensing
performance.

</details>


### [16] [Decentralized Federated Learning with Distributed Aggregation Weight Optimization](https://arxiv.org/abs/2511.03284)
*Zhiyuan Zhai,Xiaojun Yuan,Xin Wang,Geoffrey Ye Li*

Main category: eess.SP

TL;DR: 本文提出了一种分布式聚合权重优化算法，用于去中心化联邦学习(DFL)，通过本地设备间通信优化聚合权重以最小化收敛界，实现完全分布式的DFL系统。


<details>
  <summary>Details</summary>
Motivation: 传统DFL依赖中央实体收集信息并优化聚合权重，这与DFL的去中心化本质不符。需要开发分布式算法让设备仅通过本地通信就能获得最优聚合权重。

Method: 分析聚合权重对收敛的影响，将学习性能优化问题转化为特征值优化问题，提出基于次梯度的分布式算法，设备仅需本地信息和D2D通信即可获得最优权重。

Result: 数值结果表明所提算法在实际DFL部署中具有优越性，实现了真正分布式的DFL系统。

Conclusion: 提出的分布式聚合权重优化算法与DFL的去中心化特性一致，使优化、通信和学习过程都能以分布式方式进行。

Abstract: Decentralized federated learning (DFL) is an emerging paradigm to enable edge
devices collaboratively training a learning model using a device-to-device
(D2D) communication manner without the coordination of a parameter server (PS).
Aggregation weights, also known as mixing weights, are crucial in DFL process,
and impact the learning efficiency and accuracy. Conventional design relies on
a so-called central entity to collect all local information and conduct system
optimization to obtain appropriate weights. In this paper, we develop a
distributed aggregation weight optimization algorithm to align with the
decentralized nature of DFL. We analyze convergence by quantitatively capturing
the impact of the aggregation weights over decentralized communication
networks. Based on the analysis, we then formulate a learning performance
optimization problem by designing the aggregation weights to minimize the
derived convergence bound. The optimization problem is further transformed as
an eigenvalue optimization problem and solved by our proposed subgradient-based
algorithm in a distributed fashion. In our algorithm, edge devices only need
local information to obtain the optimal aggregation weights through local (D2D)
communications, just like the learning itself. Therefore, the optimization,
communication, and learning process can be all conducted in a distributed
fashion, which leads to a genuinely distributed DFL system. Numerical results
demonstrate the superiority of the proposed algorithm in practical DFL
deployment.

</details>


### [17] [Diffusion-Driven Terahertz Air-Ground Communications under Dynamic Atmospheric Turbulence](https://arxiv.org/abs/2511.03290)
*Jinhao Yi,Weijun Gao,Chong Han*

Main category: eess.SP

TL;DR: 提出AI赋能的太赫兹空-地通信框架，通过流体动力学建模飞机湍流引起的衰减，并采用扩散算法联合优化发射功率和飞行姿态以最大化链路容量。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往忽略飞机高速移动引起的强烈湍流对太赫兹通信的额外传播损耗，需要建立考虑湍流衰减的通信优化框架。

Method: 建立流体动力学启发的衰减模型表征飞机湍流，提出联合功率-姿态优化问题，使用扩散算法学习飞行配置与湍流衰减的非线性关系。

Result: 湍流引起的衰减在-10°到10°攻角、0.7马赫下达到18-28dB；所提框架平均容量达11.241bps/Hz，比现有策略提升22.8%和66.5%，接近理论容量极限的98%。

Conclusion: 飞机诱导湍流对太赫兹传播有显著影响，所提AI赋能的优化框架能有效提升空-地太赫兹通信性能。

Abstract: The ever-increasing demand for ultra-high data rates in space-air-ground
integrated networks (SAGINs) has rendered terahertz THz communications a
promising technology owing to its exceptionally broad and continuous spectrum
resources. Nevertheless, in air-ground (AG) scenarios, the high mobility of
aircraft induces intense and rapidly fluctuating turbulence, leading to
additional propagation loss that is often overlooked in existing studies. To
bridge this gap, this paper presents an AI-empowered THz AG communication
framework that explicitly models turbulence-induced attenuation through fluid
dynamics and integrates it into an adaptive optimization paradigm for
communication performance enhancement. Specifically, a fluid-dynamics-informed
attenuation model is established to characterize aircraft-generated turbulence
and quantify its impact on THz signal propagation. Building upon this model, a
joint power-attitude optimization problem is formulated to adaptively allocate
transmit power and adjust aircraft attitude for maximizing link capacity. The
optimization problem is efficiently solved using a diffusion-based algorithm
that learns the nonlinear relationship between flight configuration and
turbulence-induced attenuation. Comprehensive numerical evaluations demonstrate
that the turbulence-induced attenuation ranges from 18 to 28 dB under attacking
angles between -10 degree and 10 degree at 0.7 Mach, verifying the pronounced
impact of aircraft-induced turbulence on THz propagation. Furthermore, the
proposed framework attains an average capacity of 11.241 bps/Hz, substantially
outperforming existing strategies by 22.8% and 66.5%, and approaching
approximately 98% of the theoretical capacity limit.

</details>


### [18] [Spectral-Convergent Decentralized Machine Learning: Theory and Application in Space Networks](https://arxiv.org/abs/2511.03291)
*Zhiyuan Zhai,Shuyan Hu,Wei Ni,Xiaojun Yuan,Xin Wang*

Main category: eess.SP

TL;DR: 该论文研究了不可靠通信对去中心化机器学习收敛性的影响，建立了混合过程谱特性与全局性能的直接联系，并提出了一种谱优化方法来提高概率链路故障下的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 去中心化机器学习在大规模网络中支持无中心服务器的协作训练，但对设备间通信质量敏感，导致时变和随机拓扑。需要研究不可靠通信对收敛的影响。

Method: 通过谱优化最小化期望二阶混合矩阵的谱半径，设计了一种高效的基于次梯度的分布式算法，结合切比雪夫加速特征向量估计与本地更新和聚合权重调整。

Result: 在真实的低地球轨道卫星星座实验中，该方法相比现有基线显著提高了分类准确性和收敛效率。

Conclusion: 该方法在卫星和其他去中心化系统中具有适用性，验证了其在不可靠通信环境下的有效性和可行性。

Abstract: Decentralized machine learning (DML) supports collaborative training in
large-scale networks with no central server. It is sensitive to the quality and
reliability of inter-device communications that result in time-varying and
stochastic topologies. This paper studies the impact of unreliable
communication on the convergence of DML and establishes a direct connection
between the spectral properties of the mixing process and the global
performance. We provide rigorous convergence guarantees under random topologies
and derive bounds that characterize the impact of the expected mixing matrix's
spectral properties on learning. We formulate a spectral optimization problem
that minimizes the spectral radius of the expected second-order mixing matrix
to enhance the convergence rate under probabilistic link failures. To solve
this non-smooth spectral problem in a fully decentralized manner, we design an
efficient subgradient-based algorithm that integrates Chebyshev-accelerated
eigenvector estimation with local update and aggregation weight adjustment,
while ensuring symmetry and stochasticity constraints without central
coordination. Experiments on a realistic low Earth orbit (LEO) satellite
constellation with time-varying inter-satellite link models and real-world
remote sensing data demonstrate the feasibility and effectiveness of the
proposed method. The method significantly improves classification accuracy and
convergence efficiency compared to existing baselines, validating its
applicability in satellite and other decentralized systems.

</details>


### [19] [UAV SAR Imaging with 5G NR OFDM Signals in NLOS Environments](https://arxiv.org/abs/2511.03292)
*Qiuyuan Yang,Cunhua Pan,Ruidong Li,Zhenkun Zhang,Hong Ren,Changhong Wang,Jiangzhou Wang*

Main category: eess.SP

TL;DR: 提出了一个基于OFDM通信信号的协作式感知通信一体化SAR成像框架，采用两阶段CS-SAGE方案在NLOS环境下实现高精度散射体定位。


<details>
  <summary>Details</summary>
Motivation: 解决在非视距环境下使用感知通信一体化进行SAR成像时出现的严重成像退化问题，提高频谱利用效率并支持新型应用场景。

Method: 开发了两阶段压缩感知-空间交替广义期望最大化方案：第一阶段使用正交匹配追踪进行粗估计，第二阶段使用SAGE算法进行精细参数提取。

Result: 仿真结果验证了所提协作式ISAC框架的有效性，为实际系统设计提供了有价值的见解。

Conclusion: 提出的协作式ISAC框架在NLOS环境下能够有效实现高精度SAR成像，为未来无线系统提供了可行的解决方案。

Abstract: The integration of sensing and communication (ISAC) has significant potential
for future wireless systems, enabling efficient spectrum utilization and novel
application scenarios. In this paper, we propose a cooperative ISAC framework
for synthetic aperture radar (SAR) imaging by leveraging orthogonal frequency
division multiplexing (OFDM) communication signals. We address the challenge of
severe imaging degradation in non-line-of-sight (NLOS) environments under the
QUAsi Deterministic RadIo channel GenerAtor (QuaDRiGa). To detect weak signals
and eliminate false points, we develop a two-stage compressed sensing-space
alternating generalized expectation maximization (CS-SAGE) scheme for
high-precision scatterer localization. In stage I, orthogonal matching pursuit
(OMP) is employed for coarse estimation to identify the approximate locations
of dominant scatterers. Then, the SAGE algorithm in stage II performs fine
estimation to accurately extract scatterer parameters. Simulation results
validate the effectiveness of the proposed cooperative ISAC framework, and
provide valuable insights for practical system design.

</details>


### [20] [C-RAN Advanced: From a Network Cooperation Perspective](https://arxiv.org/abs/2511.03302)
*Xiaoyun Wang,Yutong Zhang,Sen Wang,Sun Qi,Hanning Wang,Qixing Wang,Jing Jin,Jiwei He,Nan Li*

Main category: eess.SP

TL;DR: 本文提出了一种新型的CIS-RAN架构，将传统的协作通信扩展到协作感知和协作AI，通过全过程的网络协作提升6G网络性能。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要从传统通信服务向全面信息服务转变，推动无线接入网络架构向增强协作、智能化和服务化方向演进。

Method: 基于C-RAN概念，提出CIS-RAN架构，整合协作通信、协作感知和协作AI，在获取、传输和处理全过程中增强网络协作。

Result: 通过网络协作MIMO案例研究，数值结果显示CIS-RAN架构相比传统架构具有优越性能。

Conclusion: CIS-RAN架构为6G网络提供了有前景的方向，未来研究应继续探索和推进网络协作的增强。

Abstract: Future mobile networks in the sixth generation (6G) are poised for a paradigm
shift from conventional communication services toward comprehensive information
services, driving the evolution of radio access network (RAN) architectures
toward enhanced cooperation, intelligence, and service orientation. Building
upon the concept of centralized, collaborative, cloud, and clean RAN (C-RAN),
this article proposes a novel cooperative, intelligent, and service-based RAN
(CIS-RAN) architecture. Focusing on cooperation, CIS-RAN extends the
traditional cooperative communication paradigm by further integrating
cooperative sensing and cooperative artificial intelligence (AI). To improve
both performance and effectiveness across diverse application scenarios,
CIS-RAN enhances network cooperation throughout the entire process of
acquisition, transmission, and processing, thereby enabling efficient
information acquisition, diverse cooperative interactions, and intelligent
fusion decision-making. Key technologies are discussed, with network
cooperative multiple-input multiple-output (MIMO) examined as a case study,
demonstrating superior performance over traditional architectures, as
demonstrated by numerical results. Future research directions are outlined,
emphasizing the continued exploration and advancement of the CIS-RAN
architecture, particularly in enhancing network cooperation.

</details>


### [21] [Performance Analysis of Wireless-Powered Pinching Antenna Systems](https://arxiv.org/abs/2511.03401)
*Kunrui Cao,Jingyu Chen,Panagiotis D. Diamantoulakis,Lei Zhou,Xingwang Li,Yuanwei Liu,George K. Karagiannidis*

Main category: eess.SP

TL;DR: 本文提出无线供电的夹持天线系统（PAS），通过灵活调整夹持天线位置建立强视距链路来减少自由空间路径损耗，研究了该系统在无线供电通信中的可靠性，并推导了中断概率和遍历速率的闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 探索夹持天线在改善无线供电通信系统性能方面的优势，通过建立强视距链路来减少自由空间路径损耗，提高系统可靠性。

Method: 推导了实际有损波导和理想无损波导情况下的中断概率和遍历速率的闭式表达式，分析了波导和用户的最优部署策略。

Result: 吸收系数和用户区域尺寸的增加会导致波导内和自由空间传播损耗增加，从而提高中断概率并降低遍历速率。在高吸收系数和长波导条件下，无线供电PAS的中断概率甚至比传统WPC系统更差，但遍历速率更好。

Conclusion: 无线供电PAS存在最优时间分配因子和PS与AP之间的最优距离，能够最小化中断概率或最大化遍历速率。在最优距离下分离PS和AP的系统性能优于将它们集成到混合接入点的方案。

Abstract: Pinching antenna system (PAS) serves as a groundbreaking paradigm that
enhances wireless communications by flexibly adjusting the position of pinching
antenna (PA) and establishing a strong line-of-sight (LoS) link, thereby
reducing the free-space path loss. This paper introduces the concept of
wireless-powered PAS, and investigates the reliability of wireless-powered PAS
to explore the advantages of PA in improving the performance of
wireless-powered communication (WPC) system. In addition, we derive the
closed-form expressions of outage probability and ergodic rate for the
practical lossy waveguide case and ideal lossless waveguide case, respectively,
and analyze the optimal deployment of waveguides and user to provide valuable
insights for guiding their deployments. The results show that an increase in
the absorption coefficient and in the dimensions of the user area leads to
higher in-waveguide and free-space propagation losses, respectively, which in
turn increase the outage probability and reduce the ergodic rate of the
wireless-powered PAS. However, the performance of wireless-powered PAS is
severely affected by the absorption coefficient and the waveguide length, e.g.,
under conditions of high absorption coefficient and long waveguide, the outage
probability of wireless-powered PAS is even worse than that of traditional WPC
system. While the ergodic rate of wireless-powered PAS is better than that of
traditional WPC system under conditions of high absorption coefficient and long
waveguide. Interestingly, the wireless-powered PAS has the optimal time
allocation factor and optimal distance between power station (PS) and access
point (AP) to minimize the outage probability or maximize the ergodic rate.
Moreover, the system performance of PS and AP separated at the optimal distance
between PS and AP is superior to that of PS and AP integrated into a hybrid
access point.

</details>


### [22] [A Modified Pulse and Design Framework to Halve the Complexity of OFDM Spectral Shaping Techniques](https://arxiv.org/abs/2511.03465)
*Javier Giménez,José A. Cortés,Francisco Javier Cañete,Eduardo Martos-Naya,Luis Díez*

Main category: eess.SP

TL;DR: 提出一种改进的OFDM波形，可降低频谱整形技术50%的优化系数和实现乘积数量


<details>
  <summary>Details</summary>
Motivation: OFDM调制存在高带外发射问题，现有频谱整形技术虽然有效但优化过程和实时实现成本较高

Method: 对传统OFDM波形进行修改，为频谱整形技术建立降低成本的框架

Result: 该方法可将优化涉及的系数数量和实现乘积数量减少高达50%

Conclusion: 该方法为未来研究工作提供了降低频谱整形技术成本的框架

Abstract: Orthogonal frequency division multiplexing (OFDM) is a widespread modulation
but suffers from high out-of-band emissions (OOBE). Spectral shaping strategies
such as precoding, active interference cancellation (AIC) and time-domain
methods are effective at reducing the OOBE but entail optimization procedures
and real-time implementation costs which might be considerable. This letter
proposes a modification of the conventional OFDM waveform aimed at reducing the
cost associated to many of the state-of-theart spectral shaping techniques and
sets a framework for future works that want to benefit from the same reduction.
This approach may reduce both the number of coefficients involved in the
optimization and the number of products of its implementation by up to 50%.

</details>


### [23] [A Novel Multi-Reference-Point Modeling Framework for Monostatic Background Channel: Toward 3GPP ISAC Standardization](https://arxiv.org/abs/2511.03487)
*Yameng Liu,Jianhua Zhang,Yuxiang Zhang,Zhiqiang Yuan,Chuangxin Jiang,Junchen Liu,Wei Hong,Yingyang Li,Yan Li,Guangyi Liu*

Main category: eess.SP

TL;DR: 本文提出了一种用于6G集成感知与通信(ISAC)单静态背景信道的新型随机模型，通过将信道建模为单静态收发器与多个参考点之间子信道的叠加，解决了现有通信标准在单静态模式下的建模挑战。


<details>
  <summary>Details</summary>
Motivation: 3GPP将ISAC信道定义为目标信道和背景信道的组合，但现有通信标准主要针对分离的收发器建模，而单静态模式（收发器共址）的背景信道建模仍是一个紧迫挑战，特别是在复杂环境中对感知性能评估至关重要。

Method: 首先在28GHz频段进行室内ISAC单静态背景信道测量，提取真实信道参数；然后提出一种随机模型，将单静态背景信道建模为单静态收发器与多个通信接收器样参考点之间子信道的叠加；最后提出基于遗传算法的多参考点数量和位置优化方法。

Result: 测量结果显示信道具有明显的单跳传播和离散多径分布特性。通过比较实测和仿真信道参数验证了所提模型的有效性，证明该模型能够准确捕捉单静态背景信道特性。

Conclusion: 所提出的模型有效解决了ISAC信道建模中的关键空白，支持6G标准化，为ISAC系统设计提供了标准兼容的信道建模框架。

Abstract: Integrated Sensing and Communication (ISAC) has been identified as a key 6G
application by ITU and 3GPP. A realistic, standard-compatible channel model is
essential for ISAC system design. To characterize the impact of Sensing Targets
(STs), 3GPP defines ISAC channel as a combination of target and background
channels, comprising multipath components related to STs and those originating
solely from the environment, respectively. Although the background channel does
not carry direct ST information, its accurate modeling is critical for
evaluating sensing performance, especially in complex environments. Existing
communication standards characterize propagation between separated transmitter
(Tx) and receiver (Rx). However, modeling background channels in the ISAC
monostatic mode, where the Tx and Rx are co-located, remains a pressing
challenge. In this paper, we firstly conduct ISAC monostatic background channel
measurements for an indoor scenario at 28 GHz. Realistic channel parameters are
extracted, revealing pronounced single-hop propagation and discrete multipath
distribution. Inspired by these properties, a novel stochastic model is
proposed to characterizing the ISAC monostatic background channel as the
superposition of sub-channels between the monostatic Tx&Rx and multiple
communication Rx-like Reference Points (RPs). This model is compatible with
standardizations, and a 3GPP-extended implementation framework is introduced.
Finally, a genetic algorithm-based method is proposed to extract the optimal
number and placement of multi-RPs. The optimization approach and modeling
framework are validated by comparing measured and simulated channel parameters.
Results demonstrate that the proposed model effectively captures monostatic
background channel characteristics, addresses a critical gap in ISAC channel
modeling, and supports 6G standardization.

</details>


### [24] [3D Cooperative User Tracking for Distributed Integrated Sensing and Communication](https://arxiv.org/abs/2511.03612)
*Yingjie Xu,Xuesong Cai,Michiel Sandra,Sara Willhammar,Fredrik Tufvesson*

Main category: eess.SP

TL;DR: 本文提出了一个分布式ISAC系统中的协同用户跟踪框架，通过全局PHD滤波器和视场感知AP管理策略，实现了厘米级精度的用户轨迹跟踪，并证明了无需始终保持所有AP活跃即可维持高跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 随着ISAC成为6G网络的重要组成部分，分布式ISAC通过其去中心化架构有望提升感知和通信性能，但需要解决协同用户跟踪的挑战。

Method: 采用全局概率假设密度滤波器和视场感知接入点管理策略，结合分布式MIMO信道测量来评估框架有效性。

Result: 实现了厘米级的均方根轨迹误差，并发现无需始终保持所有AP活跃即可维持高跟踪精度。

Conclusion: 这些发现为DISAC系统中协同用户跟踪技术的实际部署和进一步发展提供了有价值的见解。

Abstract: As integrated sensing and communication (ISAC) becomes an integral part of 6G
networks, distributed ISAC (DISAC) is expected to enhance both sensing and
communication performance through its decentralized architecture. This paper
presents a complete framework to address the challenge of cooperative user
tracking in DISAC systems. By incorporating a global probability hypothesis
density (PHD) filter and a field-of-view-aware access point (AP) management
strategy, the framework enables accurate user tracking using radio signals
while optimizing AP scheduling. In addition, a real-world distributed MIMO
channel measurement campaign is performed to evaluate the effectiveness of the
framework. The results demonstrate that a centimeter-level root mean-square
trajectory error can be achieved. Furthermore, the results show that it is not
necessary to keep APs active at all times to maintain high tracking accuracy,
indicating the need for robust and efficient AP management. These findings
provide valuable insight into practical deployments and further development of
cooperative user tracking techniques in DISAC systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [25] [Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks](https://arxiv.org/abs/2511.02957)
*Mohsin Mahmud Topu,Mahfuz Ahmed Anik,Azmine Toushik Wasi,Md Manjurul Ahsan*

Main category: cs.LG

TL;DR: 提出了一种结合数字孪生和图神经网络的道路健康监测框架，通过图结构建模道路段和空间关系，实现预测性维护。


<details>
  <summary>Details</summary>
Motivation: 传统路面管理系统反应滞后，缺乏实时智能来预防故障和优化维护计划，需要更主动的解决方案。

Method: 将路面段和空间关系建模为图节点和边，使用感应式图神经网络从图结构输入中学习退化模式，结合无人机、传感器和激光雷达实时数据。

Result: 在真实世界数据集上训练，模型R2达到0.3798，优于基线回归器，能有效捕捉非线性退化。

Conclusion: 数字孪生与图神经网络的集成提高了预测精度，建立了持续改进的闭环反馈，为主动、智能和可持续的路面管理奠定了基础。

Abstract: Pavement infrastructure monitoring is challenged by complex spatial
dependencies, changing environmental conditions, and non-linear deterioration
across road networks. Traditional Pavement Management Systems (PMS) remain
largely reactive, lacking real-time intelligence for failure prevention and
optimal maintenance planning. To address this, we propose a unified Digital
Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven
pavement health monitoring and predictive maintenance. Pavement segments and
spatial relations are modeled as graph nodes and edges, while real-time UAV,
sensor, and LiDAR data stream into the DT. The inductive GNN learns
deterioration patterns from graph-structured inputs to forecast distress and
enable proactive interventions. Trained on a real-world-inspired dataset with
segment attributes and dynamic connectivity, our model achieves an R2 of
0.3798, outperforming baseline regressors and effectively capturing non-linear
degradation. We also develop an interactive dashboard and reinforcement
learning module for simulation, visualization, and adaptive maintenance
planning. This DT-GNN integration enhances forecasting precision and
establishes a closed feedback loop for continuous improvement, positioning the
approach as a foundation for proactive, intelligent, and sustainable pavement
management, with future extensions toward real-world deployment, multi-agent
coordination, and smart-city integration.

</details>


### [26] [FATE: A Formal Benchmark Series for Frontier Algebra of Multiple Difficulty Levels](https://arxiv.org/abs/2511.02872)
*Jiedong Jiang,Wanyi He,Yuefeng Wang,Guoxiong Gao,Yongle Hu,Jingting Wang,Nailing Guan,Peihao Wu,Chunbo Dai,Liang Xiao,Bin Dong*

Main category: cs.LG

TL;DR: FATE是一个新的形式代数定理评估基准系列，包含FATE-H和FATE-X两个组件，各100个抽象代数和交换代数问题，难度从本科练习到超过博士资格考试水平。评估显示当前最佳LLM在FATE-H上仅3%准确率，FATE-X上为0%，揭示了模型在形式化推理能力上的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基于竞赛的数学基准（如IMO）无法反映现代数学研究的深度、广度和抽象性，需要创建更接近研究水平的数学推理基准。

Method: 引入FATE基准系列，包含FATE-H和FATE-X两个组件，各100个代数问题，涵盖从本科到博士资格考试的难度范围。采用两阶段评估方法，比较模型的自然语言推理能力和形式化推理能力。

Result: 最佳模型在FATE-H上仅达到3%（pass@64）准确率，在FATE-X上为0%。模型的自然语言推理能力明显优于形式化能力。专门化证明器在自然语言阶段的反思效果不如通用模型。

Conclusion: FATE提供了一个稳健且具有挑战性的基准，为通向研究级形式数学推理的道路建立了重要检查点，揭示了当前LLM在高级数学推理方面的局限性。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
capabilities in formal theorem proving, particularly on contest-based
mathematical benchmarks like the IMO. However, these contests do not reflect
the depth, breadth, and abstraction of modern mathematical research. To bridge
this gap, we introduce FATE (Formal Algebra Theorem Evaluation), a new
benchmark series in formal algebra designed to chart a course toward advanced
mathematical reasoning. We present two new components, FATE-H and FATE-X, each
with 100 problems in abstract and commutative algebra. The FATE series spans a
difficulty spectrum from undergraduate exercises to problems exceeding PhD
qualifying exams. Notably, FATE-X is the first formal benchmark to surpass both
PhD-level exam difficulty and the coverage of the Mathlib library. Our
evaluations of state-of-the-art LLM provers on this new benchmark reveal a
stark performance gap compared to contest math: the best model achieves only 3%
(pass@64) accuracy on FATE-H and 0% on FATE-X. Our two-stage evaluation reveals
that models' natural-language reasoning is notably more accurate than their
ability to formalize this reasoning. We systematically classify the common
errors that arise during this formalization process. Furthermore, a comparative
study shows that a specialized prover can exhibit less effective reflection
than general-purpose models, reducing its accuracy at the natural-language
stage. We believe FATE provides a robust and challenging benchmark that
establishes essential checkpoints on the path toward research-level formal
mathematical reasoning.

</details>


### [27] [Tensor-Efficient High-Dimensional Q-learning](https://arxiv.org/abs/2511.03595)
*Junyi Wu,Dan Li*

Main category: cs.LG

TL;DR: 提出了Tensor-Efficient Q-Learning (TEQL)，一种基于低秩张量分解的高维强化学习方法，通过改进的块坐标下降、新颖的探索策略和正则化机制，在样本效率和总奖励方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 高维强化学习面临计算复杂和样本效率低的问题，特别是在大规模状态-动作空间中，Q-learning算法受到维度诅咒的严重影响。

Method: 在离散化状态-动作空间上使用改进的块坐标下降进行低秩张量分解，结合近似误差和访问计数上置信界的探索策略，并在目标函数中加入基于频率的惩罚项。

Result: 在经典控制任务上的实验结果表明，TEQL在样本效率和总奖励方面优于传统矩阵方法和深度强化学习方法。

Conclusion: TEQL特别适合采样成本高的资源受限应用场景，如太空和医疗领域。

Abstract: High-dimensional reinforcement learning faces challenges with complex
calculations and low sample efficiency in large state-action spaces. Q-learning
algorithms struggle particularly with the curse of dimensionality, where the
number of state-action pairs grows exponentially with problem size. While
neural network-based approaches like Deep Q-Networks have shown success, recent
tensor-based methods using low-rank decomposition offer more
parameter-efficient alternatives. Building upon existing tensor-based methods,
we propose Tensor-Efficient Q-Learning (TEQL), which enhances low-rank tensor
decomposition via improved block coordinate descent on discretized state-action
spaces, incorporating novel exploration and regularization mechanisms. The key
innovation is an exploration strategy that combines approximation error with
visit count-based upper confidence bound to prioritize actions with high
uncertainty, avoiding wasteful random exploration. Additionally, we incorporate
a frequency-based penalty term in the objective function to encourage
exploration of less-visited state-action pairs and reduce overfitting to
frequently visited regions. Empirical results on classic control tasks
demonstrate that TEQL outperforms conventional matrix-based methods and deep RL
approaches in both sample efficiency and total rewards, making it suitable for
resource-constrained applications, such as space and healthcare where sampling
costs are high.

</details>


### [28] [Stochastic Deep Graph Clustering for Practical Group Formation](https://arxiv.org/abs/2511.02879)
*Junhyung Park,Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.LG

TL;DR: DeepForm是一个用于动态群组推荐的框架，通过随机深度图聚类解决群组形成问题，支持高阶用户信息整合、实时群组形成和动态群组数量调整。


<details>
  <summary>Details</summary>
Motivation: 现有的群组推荐系统大多假设群组是静态或预定义的，不适用于动态现实场景，需要解决群组形成这一核心挑战。

Method: 采用轻量级GCN架构捕捉高阶结构信号，结合随机聚类学习实现无需重新训练的自适应群组重构，并使用对比学习在动态条件下优化群组。

Result: 在多个数据集上的实验表明，DeepForm在群组形成质量、效率和推荐准确性方面均优于各种基线方法。

Conclusion: DeepForm成功解决了动态群组推荐中的群组形成问题，为实际应用提供了有效的解决方案。

Abstract: While prior work on group recommender systems (GRSs) has primarily focused on
improving recommendation accuracy, most approaches assume static or predefined
groups, making them unsuitable for dynamic, real-world scenarios. We reframe
group formation as a core challenge in GRSs and propose DeepForm (Stochastic
Deep Graph Clustering for Practical Group Formation), a framework designed to
meet three key operational requirements: (1) the incorporation of high-order
user information, (2) real-time group formation, and (3) dynamic adjustment of
the number of groups. DeepForm employs a lightweight GCN architecture that
effectively captures high-order structural signals. Stochastic cluster learning
enables adaptive group reconfiguration without retraining, while contrastive
learning refines groups under dynamic conditions. Experiments on multiple
datasets demonstrate that DeepForm achieves superior group formation quality,
efficiency, and recommendation accuracy compared with various baselines.

</details>


### [29] [Test-time Adaptation of Tiny Recursive Models](https://arxiv.org/abs/2511.02886)
*Ronan Killian McGovern*

Main category: cs.LG

TL;DR: 通过从在公开ARC任务上预训练的微小递归模型出发，可以在比赛允许的计算限制内高效微调，在竞赛任务上达到6.67%的分数。


<details>
  <summary>Details</summary>
Motivation: 解决之前领先的开源方法TRM虽然能在ARC任务上获得约7.8%的分数，但计算需求远超比赛限制的问题。

Method: 首先在1,280个公开任务上预训练7M参数的递归神经网络，然后在竞赛任务上进行完整的微调（非LoRA或仅任务嵌入微调）。

Result: 预训练模型在公开评估集上获得约10%的分数，经过仅12,500步梯度更新的竞赛后训练，在半私有评估任务上达到6.67%的分数。

Conclusion: 从预训练的微小递归模型出发进行完整微调，可以在有限计算资源下有效提升ARC任务的性能。

Abstract: Prior to the close of the 2025 ARC Prize competition, the leading open source
approach - known as TRM, or Tiny Recursive Models - involved training a 7M
parameter recursive neural network on augmented variants of ARC tasks. That
approach scored approximately 7.8% on the public ARC AGI II evaluation set, but
required a level of compute far in excess of what is allowed during the
competition. This paper shows that, by starting from a tiny recursive model
that has been pre-trained on public ARC tasks, one can efficiently fine-tune on
competition tasks within the allowed compute limits. Specifically, a model was
pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on
4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model
was then post-trained in just 12,500 gradient steps during the competition to
reach a score of 6.67% on semi-private evaluation tasks. Notably, such
post-training performance is achieved by full-fine tuning of the tiny model,
not LoRA fine-tuning or fine-tuning of task embeddings alone.

</details>


### [30] [Predicting Weekly Fishing Concentration Zones through Deep Learning Integration of Heterogeneous Environmental Spatial Datasets](https://arxiv.org/abs/2511.02887)
*Chaitanya Rele,Aditya Rathod,Kaustubh Natu,Saurabh Kulkarni,Ajay Koli,Swapnali Makdey*

Main category: cs.LG

TL;DR: 提出了一个基于AI的框架，利用海面温度和叶绿素浓度等海洋参数来预测北印度洋的潜在渔区，帮助渔民减少搜索时间和燃料消耗。


<details>
  <summary>Details</summary>
Motivation: 北印度洋（包括阿拉伯海和孟加拉湾）是沿海社区的重要生计来源，但渔民在寻找高产渔区时经常面临不确定性，需要更准确的方法来识别潜在渔区。

Method: 使用AI辅助框架，结合海面温度和叶绿素浓度等海洋参数来预测潜在渔区，旨在提高渔区识别的准确性并提供区域特定的可持续捕鱼实践见解。

Result: 初步结果表明，该框架能够支持渔民减少搜索时间、降低燃料消耗，并促进高效的资源利用。

Conclusion: 该AI辅助框架为北印度洋地区的渔民提供了有效的潜在渔区预测工具，有助于实现可持续的渔业管理。

Abstract: The North Indian Ocean, including the Arabian Sea and the Bay of Bengal,
represents a vital source of livelihood for coastal communities, yet fishermen
often face uncertainty in locating productive fishing grounds. To address this
challenge, we present an AI-assisted framework for predicting Potential Fishing
Zones (PFZs) using oceanographic parameters such as sea surface temperature and
chlorophyll concentration. The approach is designed to enhance the accuracy of
PFZ identification and provide region-specific insights for sustainable fishing
practices. Preliminary results indicate that the framework can support
fishermen by reducing search time, lowering fuel consumption, and promoting
efficient resource utilization.

</details>


### [31] [Adaptive and Robust Data Poisoning Detection and Sanitization in Wearable IoT Systems using Large Language Models](https://arxiv.org/abs/2511.02894)
*W. K. M Mithsara,Ning Yang,Ahmed Imteaj,Hussein Zangoti,Abdur R. Shahid*

Main category: cs.LG

TL;DR: 提出基于大语言模型(LLMs)的零样本、少样本学习框架，用于检测和清理可穿戴物联网系统中的人体活动识别(HAR)数据中毒攻击，减少对大规模标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 可穿戴设备在物联网中的广泛应用需要可靠的人体活动识别技术，但机器学习模型容易受到数据中毒攻击。传统防御方法需要大量标注数据和特定任务训练，难以适应动态物联网环境。

Method: 使用大语言模型进行中毒检测和清理，采用角色扮演提示和逐步推理策略，让LLM扮演专家角色分析传感器异常并推断中毒指标和干净数据替代方案。

Result: 通过广泛评估框架的检测准确率、清理质量、延迟和通信成本，证明LLMs在提高可穿戴物联网系统安全性和可靠性方面的实用性和有效性。

Conclusion: LLMs能够为可穿戴物联网系统提供强大、自适应的防御机制，减少对大规模数据集的依赖，在实时环境中实现可靠的数据中毒防护。

Abstract: The widespread integration of wearable sensing devices in Internet of Things
(IoT) ecosystems, particularly in healthcare, smart homes, and industrial
applications, has required robust human activity recognition (HAR) techniques
to improve functionality and user experience. Although machine learning models
have advanced HAR, they are increasingly susceptible to data poisoning attacks
that compromise the data integrity and reliability of these systems.
Conventional approaches to defending against such attacks often require
extensive task-specific training with large, labeled datasets, which limits
adaptability in dynamic IoT environments. This work proposes a novel framework
that uses large language models (LLMs) to perform poisoning detection and
sanitization in HAR systems, utilizing zero-shot, one-shot, and few-shot
learning paradigms. Our approach incorporates \textit{role play} prompting,
whereby the LLM assumes the role of expert to contextualize and evaluate sensor
anomalies, and \textit{think step-by-step} reasoning, guiding the LLM to infer
poisoning indicators in the raw sensor data and plausible clean alternatives.
These strategies minimize reliance on curation of extensive datasets and enable
robust, adaptable defense mechanisms in real-time. We perform an extensive
evaluation of the framework, quantifying detection accuracy, sanitization
quality, latency, and communication cost, thus demonstrating the practicality
and effectiveness of LLMs in improving the security and reliability of wearable
IoT systems.

</details>


### [32] [Zero-shot data citation function classification using transformer-based large language models (LLMs)](https://arxiv.org/abs/2511.02936)
*Neil Byers,Ali Zaidi,Valerie Skye,Chris Beecroft,Kjiersten Fagnan*

Main category: cs.LG

TL;DR: 使用Llama 3.1-405B大语言模型对引用特定基因组数据集的科学文献进行数据使用案例的自动标注，无需人工标注或训练传统机器学习系统。


<details>
  <summary>Details</summary>
Motivation: 近年来需要识别数据集与引用它们的科学文献之间的关联，了解数据如何被使用，但人工标注成本高昂且需要开发训练数据集。

Method: 应用开源LLM Llama 3.1-405B对已知引用特定基因组数据集的文献生成结构化数据使用案例标签，并引入新的评估框架。

Result: 在零样本数据引用分类任务中，基础模型达到F1分数0.674，无需预定义类别。

Conclusion: 结果有前景，但受到数据可用性、提示过拟合、计算基础设施和负责任性能评估所需成本等障碍的限制。

Abstract: Efforts have increased in recent years to identify associations between
specific datasets and the scientific literature that incorporates them. Knowing
that a given publication cites a given dataset, the next logical step is to
explore how or why that data was used. Advances in recent years with
pretrained, transformer-based large language models (LLMs) offer potential
means for scaling the description of data use cases in the published
literature. This avoids expensive manual labeling and the development of
training datasets for classical machine-learning (ML) systems. In this work we
apply an open-source LLM, Llama 3.1-405B, to generate structured data use case
labels for publications known to incorporate specific genomic datasets. We also
introduce a novel evaluation framework for determining the efficacy of our
methods. Our results demonstrate that the stock model can achieve an F1 score
of .674 on a zero-shot data citation classification task with no previously
defined categories. While promising, our results are qualified by barriers
related to data availability, prompt overfitting, computational infrastructure,
and the expense required to conduct responsible performance evaluation.

</details>


### [33] [Power Constrained Nonstationary Bandits with Habituation and Recovery Dynamics](https://arxiv.org/abs/2511.02944)
*Fengxu Li,Stephanie M. Carpenter,Matthew P. Buman,Yonatan Mintz*

Main category: cs.LG

TL;DR: 本文针对ROGUE多臂老虎机框架开发了ROGUE-TS算法，通过概率裁剪平衡个性化推荐与群体效应学习，在微随机试验中实现低遗憾和高统计功效。


<details>
  <summary>Details</summary>
Motivation: 现有算法在ROGUE框架下过度强调利用而探索不足，限制了群体效应估计能力，这在需要平衡个性化推荐与群体统计有效性的微随机试验中尤为重要。

Method: 开发了ROGUE-TS汤普森采样算法，并引入概率裁剪程序来平衡个性化与群体学习，量化了遗憾与最小探索概率之间的权衡。

Result: 在两个微随机试验数据集上的验证表明，该方法比现有方法获得更低遗憾，通过裁剪程序保持高统计功效且不显著增加遗憾。

Conclusion: 该框架为设计微随机试验的研究人员提供了平衡个性化与统计有效性的实用指导，能够可靠检测治疗效果同时考虑个体行为动态。

Abstract: A common challenge for decision makers is selecting actions whose rewards are
unknown and evolve over time based on prior policies. For instance, repeated
use may reduce an action's effectiveness (habituation), while inactivity may
restore it (recovery). These nonstationarities are captured by the Reducing or
Gaining Unknown Efficacy (ROGUE) bandit framework, which models real-world
settings such as behavioral health interventions. While existing algorithms can
compute sublinear regret policies to optimize these settings, they may not
provide sufficient exploration due to overemphasis on exploitation, limiting
the ability to estimate population-level effects. This is a challenge of
particular interest in micro-randomized trials (MRTs) that aid researchers in
developing just-in-time adaptive interventions that have population-level
effects while still providing personalized recommendations to individuals. In
this paper, we first develop ROGUE-TS, a Thompson Sampling algorithm tailored
to the ROGUE framework, and provide theoretical guarantees of sublinear regret.
We then introduce a probability clipping procedure to balance personalization
and population-level learning, with quantified trade-off that balances regret
and minimum exploration probability. Validation on two MRT datasets concerning
physical activity promotion and bipolar disorder treatment shows that our
methods both achieve lower regret than existing approaches and maintain high
statistical power through the clipping procedure without significantly
increasing regret. This enables reliable detection of treatment effects while
accounting for individual behavioral dynamics. For researchers designing MRTs,
our framework offers practical guidance on balancing personalization with
statistical validity.

</details>


### [34] [Inference-Time Personalized Alignment with a Few User Preference Queries](https://arxiv.org/abs/2511.02966)
*Victor-Alexandru Pădurean,Parameswaran Kamalaruban,Nachiket Kotalwar,Alkis Gotovos,Adish Singla*

Main category: cs.LG

TL;DR: UserAlign是一种推理时个性化对齐方法，通过少量成对响应比较来获取用户偏好，基于逻辑赌博机中的最佳臂识别理论框架，从固定响应池中选择个性化响应。


<details>
  <summary>Details</summary>
Motivation: 现有个性化对齐方法要么需要大量用户偏好查询，要么要求偏好以文本形式明确指定，这在实际应用中存在限制。

Method: 基于逻辑赌博机的最佳臂识别理论框架，将用户反馈视为一致且无噪声的，通过少量成对响应比较快速识别最佳响应。

Result: 在个性化文本和图像生成等多个任务上的实验结果表明，UserAlign在实现个性化对齐方面具有有效性。

Conclusion: UserAlign提供了一种高效的推理时个性化对齐方法，能够以少量用户查询实现与用户偏好的有效对齐。

Abstract: We study the problem of aligning a generative model's response with a user's
preferences. Recent works have proposed several different formulations for
personalized alignment; however, they either require a large amount of user
preference queries or require that the preference be explicitly specified as a
text input. In this paper, we propose a novel inference-time personalized
alignment method, UserAlign, that elicits the user's preferences with a few
queries as pairwise response comparisons. In particular, UserAlign builds on
the theoretical framework of best-arm identification in logistic bandits and
selects a personalized response from a fixed pool of the model's generated
responses. The key idea is to consider the user's feedback consistent and
noise-free, and incorporate it into the theoretical framework to identify the
best response quickly. Experimental results across several tasks, involving
personalized text and image generation, showcase the effectiveness of UserAlign
in achieving personalized alignment.

</details>


### [35] [Discrete Bayesian Sample Inference for Graph Generation](https://arxiv.org/abs/2511.03015)
*Ole Petersen,Marcel Kollovieh,Marten Lienen,Stephan Günnemann*

Main category: cs.LG

TL;DR: GraphBSI是一种基于贝叶斯样本推理(BSI)的一步式图生成模型，通过在分布参数的连续空间中迭代优化图结构的信念来处理离散图结构，在分子和合成图生成任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 图结构数据在分子生成、知识图谱和网络分析中很重要，但其离散、无序的特性使传统生成模型难以处理，因此需要开发新的离散扩散和流匹配模型。

Method: 提出GraphBSI模型，基于贝叶斯样本推理(BSI)，在分布参数的连续空间中迭代优化图结构的信念，而非直接演化样本。将BSI表述为随机微分方程(SDE)，并推导出通过分数函数近似保持边缘分布的噪声控制SDE族。

Result: 在分子和合成图生成的标准基准测试Moses和GuacaMol上，GraphBSI表现优于现有的一步式图生成模型，达到最先进性能。

Conclusion: GraphBSI通过贝叶斯样本推理方法有效处理离散图结构生成问题，理论分析揭示了其与贝叶斯流网络和扩散模型的联系，在多个基准测试中展示了优越性能。

Abstract: Generating graph-structured data is crucial in applications such as molecular
generation, knowledge graphs, and network analysis. However, their discrete,
unordered nature makes them difficult for traditional generative models,
leading to the rise of discrete diffusion and flow matching models. In this
work, we introduce GraphBSI, a novel one-shot graph generative model based on
Bayesian Sample Inference (BSI). Instead of evolving samples directly, GraphBSI
iteratively refines a belief over graphs in the continuous space of
distribution parameters, naturally handling discrete structures. Further, we
state BSI as a stochastic differential equation (SDE) and derive a
noise-controlled family of SDEs that preserves the marginal distributions via
an approximation of the score function. Our theoretical analysis further
reveals the connection to Bayesian Flow Networks and Diffusion models. Finally,
in our empirical evaluation, we demonstrate state-of-the-art performance on
molecular and synthetic graph generation, outperforming existing one-shot graph
generative models on the standard benchmarks Moses and GuacaMol.

</details>


### [36] [Value of Information-Enhanced Exploration in Bootstrapped DQN](https://arxiv.org/abs/2511.02969)
*Stergios Plataniotis,Charilaos Akasiadis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 将期望信息价值(EVOI)整合到Bootstrapped DQN框架中，通过测量不同网络头之间的意见分歧来指导探索，在稀疏奖励的复杂环境中实现更高效的探索。


<details>
  <summary>Details</summary>
Motivation: 传统基于随机局部策略噪声的探索策略（如ε-greedy和Boltzmann方法）在高维状态和稀疏奖励环境中难以有效平衡探索与利用。

Method: 开发了两种新算法，将期望信息价值整合到Bootstrapped DQN中，利用信息价值估计来测量不同网络头之间的意见分歧，并引导探索到最有潜力的区域。

Result: 在复杂稀疏奖励的Atari游戏中表现出更高的性能，更好地利用了随机网络初始化产生的固有不确定性，且没有引入额外超参数。

Conclusion: 基于期望信息价值的探索方法能够有效增强深度强化学习中的探索能力，特别是在高维稀疏奖励环境中。

Abstract: Efficient exploration in deep reinforcement learning remains a fundamental
challenge, especially in environments characterized by high-dimensional states
and sparse rewards. Traditional exploration strategies that rely on random
local policy noise, such as $\epsilon$-greedy and Boltzmann exploration
methods, often struggle to efficiently balance exploration and exploitation. In
this paper, we integrate the notion of (expected) value of information (EVOI)
within the well-known Bootstrapped DQN algorithmic framework, to enhance the
algorithm's deep exploration ability. Specifically, we develop two novel
algorithms that incorporate the expected gain from learning the value of
information into Bootstrapped DQN. Our methods use value of information
estimates to measure the discrepancies of opinions among distinct network
heads, and drive exploration towards areas with the most potential. We evaluate
our algorithms with respect to performance and their ability to exploit
inherent uncertainty arising from random network initialization. Our
experiments in complex, sparse-reward Atari games demonstrate increased
performance, all the while making better use of uncertainty, and, importantly,
without introducing extra hyperparameters.

</details>


### [37] [Cross-Modal Alignment via Variational Copula Modelling](https://arxiv.org/abs/2511.03196)
*Feng Wu,Tsai Hor Chan,Fuying Wang,Guosheng Yin,Lequan Yu*

Main category: cs.LG

TL;DR: 提出了一种基于copula的多模态学习框架，通过建模不同模态间的复杂交互来学习联合分布，能够有效处理缺失模态问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中存在多种数据模态，现有方法主要依赖拼接或Kronecker积，过于简化模态间交互结构，需要建模更复杂的交互关系。

Method: 使用copula统计结构建模多模态联合分布，假设每个模态服从高斯混合分布，通过copula模型对齐各模态的边际分布。

Result: 在公开MIMIC数据集上的实验表明，该模型优于其他竞争方法，能够为缺失模态生成准确表示。

Conclusion: copula驱动的多模态学习框架能够有效捕获模态间复杂交互，在缺失模态场景下表现优异。

Abstract: Various data modalities are common in real-world applications (e.g.,
electronic health records, medical images and clinical notes in healthcare). It
is essential to develop multimodal learning methods to aggregate various
information from multiple modalities. The main challenge is how to
appropriately align and fuse the representations of different modalities into a
joint distribution. Existing methods mainly rely on concatenation or the
Kronecker product, oversimplifying the interaction structure between modalities
and indicating a need to model more complex interactions. Additionally, the
joint distribution of latent representations with higher-order interactions is
underexplored. Copula is a powerful statistical structure for modelling the
interactions among variables, as it naturally bridges the joint distribution
and marginal distributions of multiple variables. We propose a novel
copula-driven multimodal learning framework, which focuses on learning the
joint distribution of various modalities to capture the complex interactions
among them. The key idea is to interpret the copula model as a tool to align
the marginal distributions of the modalities efficiently. By assuming a
Gaussian mixture distribution for each modality and a copula model on the joint
distribution, our model can generate accurate representations for missing
modalities. Extensive experiments on public MIMIC datasets demonstrate the
superior performance of our model over other competitors. The code is available
at https://github.com/HKU-MedAI/CMCM.

</details>


### [38] [Heterogeneous Metamaterials Design via Multiscale Neural Implicit Representation](https://arxiv.org/abs/2511.03012)
*Hongrui Chen,Liwei Wang,Levent Burak Kara*

Main category: cs.LG

TL;DR: 提出基于神经网络的双尺度表示框架，用于设计具有空间变化特性的异质超材料，解决了传统方法中的设计空间大和单元间兼容性问题。


<details>
  <summary>Details</summary>
Motivation: 传统异质超材料设计方法面临巨大设计空间和单元间兼容性挑战，数据驱动方法受限于固定微结构库且需要额外后处理。

Method: 使用多尺度神经表示，神经网络同时输入全局和局部坐标，输出表示多尺度结构的隐式场，通过兼容性损失项确保相邻单元连接性。

Result: 框架能够生成任意高分辨率的超材料设计，支持无限上采样，在力学超材料设计、负泊松比和力学隐身问题上验证了有效性。

Conclusion: 该神经网络框架成功解决了异质超材料设计中的关键挑战，在机器人、生物工程和航空航天领域具有应用潜力。

Abstract: Metamaterials are engineered materials composed of specially designed unit
cells that exhibit extraordinary properties beyond those of natural materials.
Complex engineering tasks often require heterogeneous unit cells to accommodate
spatially varying property requirements. However, designing heterogeneous
metamaterials poses significant challenges due to the enormous design space and
strict compatibility requirements between neighboring cells. Traditional
concurrent multiscale design methods require solving an expensive optimization
problem for each unit cell and often suffer from discontinuities at cell
boundaries. On the other hand, data-driven approaches that assemble structures
from a fixed library of microstructures are limited by the dataset and require
additional post-processing to ensure seamless connections. In this work, we
propose a neural network-based metamaterial design framework that learns a
continuous two-scale representation of the structure, thereby jointly
addressing these challenges. Central to our framework is a multiscale neural
representation in which the neural network takes both global (macroscale) and
local (microscale) coordinates as inputs, outputting an implicit field that
represents multiscale structures with compatible unit cell geometries across
the domain, without the need for a predefined dataset. We use a compatibility
loss term during training to enforce connectivity between adjacent unit cells.
Once trained, the network can produce metamaterial designs at arbitrarily high
resolution, hence enabling infinite upsampling for fabrication or simulation.
We demonstrate the effectiveness of the proposed approach on mechanical
metamaterial design, negative Poisson's ratio, and mechanical cloaking problems
with potential applications in robotics, bioengineering, and aerospace.

</details>


### [39] [Decoupled Entropy Minimization](https://arxiv.org/abs/2511.03256)
*Jing Ma,Hanlin Li,Xiang Xiang*

Main category: cs.LG

TL;DR: 该论文重新分析了经典熵最小化的内部机制，将其解耦为两个相反作用的部分，并提出了自适应解耦熵最小化方法来解决经典方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 经典熵最小化在机器学习中虽然有益于减少类别重叠、缩小域差距和限制不确定性，但其潜力有限。作者希望研究熵最小化的内部机制，揭示其局限性并改进方法。

Method: 将经典熵最小化解耦为聚类聚合驱动因子和梯度缓解校准器，然后提出自适应解耦熵最小化，通过归一化奖励和使用边际熵校准器来替代梯度缓解校准器。

Result: AdaDEM方法在噪声和动态环境中的各种不完善监督学习任务中表现优于DEM*（经典熵最小化的上界变体），实现了更优越的性能。

Conclusion: 通过解耦熵最小化并引入自适应机制，可以有效解决经典方法的奖励崩溃和简单类别偏差问题，在不完善监督学习任务中取得更好的效果。

Abstract: Entropy Minimization (EM) is beneficial to reducing class overlap, bridging
domain gap, and restricting uncertainty for various tasks in machine learning,
yet its potential is limited. To study the internal mechanism of EM, we
reformulate and decouple the classical EM into two parts with opposite effects:
cluster aggregation driving factor (CADF) rewards dominant classes and prompts
a peaked output distribution, while gradient mitigation calibrator (GMC)
penalizes high-confidence classes based on predicted probabilities.
Furthermore, we reveal the limitations of classical EM caused by its coupled
formulation: 1) reward collapse impedes the contribution of high-certainty
samples in the learning process, and 2) easy-class bias induces misalignment
between output distribution and label distribution. To address these issues, we
propose Adaptive Decoupled Entropy Minimization (AdaDEM), which normalizes the
reward brought from CADF and employs a marginal entropy calibrator (MEC) to
replace GMC. AdaDEM outperforms DEM*, an upper-bound variant of classical EM,
and achieves superior performance across various imperfectly supervised
learning tasks in noisy and dynamic environments.

</details>


### [40] [Why Less is More (Sometimes): A Theory of Data Curation](https://arxiv.org/abs/2511.03492)
*Elvis Dohmatob,Mohammad Pezeshki,Reyhane Askari-Hemmat*

Main category: cs.LG

TL;DR: 该论文提出了一个理论框架来解决机器学习中的核心悖论：何时使用更少数据反而更好。研究发现，在某些条件下，精心筛选的小数据集可以超越完整数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现代机器学习中的核心矛盾：当经典扩展定律建议"越多越好"时，为什么像LIMO和s1这样的方法通过使用少量精心筛选的数据集反而能获得更好的性能。

Method: 研究数据筛选策略，其中不完美的oracle根据样本的难度和正确性来选择训练样本。推导了在标签无关和标签感知筛选规则下测试误差的精确扩展定律曲线。

Result: 证明了在某些条件下，小型筛选数据集可以超越完整数据集的性能，并提供了与数据大小和质量相关的精确相变曲线。在ImageNet上的实证结果验证了理论预测。

Conclusion: 该框架为最近在LLM数学推理中观察到的矛盾筛选策略提供了原则性解释，揭示了何时以及为什么仅保留数据子集可以改善泛化性能。

Abstract: This paper introduces a theoretical framework to resolve a central paradox in
modern machine learning: When is it better to use less data? This question has
become critical as classical scaling laws suggesting ``more is more'' (Sun et
al., 2025) are challenged by methods like LIMO (``less is more'') and s1 (Ye et
al., 2025; Muenighoff et al., 2025), which achieve superior performance with
small, aggressively curated datasets. Here, we study data curation strategies
where an imperfect oracle selects the training examples according to their
difficulty and correctness. Our results provide exact scaling law curves for
test error under both label-agnostic and label-aware curation rules, revealing
when and why keeping only a subset of data can improve generalization. In
contrast to classical scaling laws, we show that under certain conditions,
small curated datasets can outperform full datasets, and we provide analytical
conditions for this by deriving precise phase transition curves tied to data
size and quality. We validate these theoretical claims with empirical results
on ImageNet, confirming our predictions about when curation improves accuracy
and can even mitigate model collapse. Furthermore, our framework provides a
principled explanation for the contradictory curation strategies recently
observed in LLM mathematical reasoning.

</details>


### [41] [Adaptive-Sensorless Monitoring of Shipping Containers](https://arxiv.org/abs/2511.03022)
*Lingqing Shen,Chi Heem Wong,Misaki Mito,Arnab Chakrabarti*

Main category: cs.LG

TL;DR: 提出了一种自适应无传感器监测方法，通过残差校正框架修正无传感器模型的系统性偏差，在集装箱温湿度监测中显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统无传感器监测方法无法整合遥测信息和修正系统性误差，导致预测结果与实时数据差异显著，给用户造成困惑。

Method: 引入残差校正方法，这是一个通用框架，在观察到实时遥测数据后修正无传感器模型的系统性偏差，称为"自适应无传感器"监测。

Result: 在348万数据点上评估，自适应无传感器模型相比基线无传感器模型持续改进：温度MAE从2.43°C降至2.24-2.31°C，湿度MAE从7.99%降至5.72-7.09%；温度RMSE从3.38°C降至3.19-3.26°C，湿度RMSE从10.0%降至7.70-9.12%。

Conclusion: 自适应无传感器模型能够实现更准确的货物监测、早期风险检测，并减少全球航运中对完全连接的依赖。

Abstract: Monitoring the internal temperature and humidity of shipping containers is
essential to preventing quality degradation during cargo transportation.
Sensorless monitoring -- machine learning models that predict the internal
conditions of the containers using exogenous factors -- shows promise as an
alternative to monitoring using sensors. However, it does not incorporate
telemetry information and correct for systematic errors, causing the
predictions to differ significantly from the live data and confusing the users.
In this paper, we introduce the residual correction method, a general framework
for correcting for systematic biases in sensorless models after observing live
telemetry data. We call this class of models ``adaptive-sensorless''
monitoring. We train and evaluate adaptive-sensorless models on the 3.48
million data points -- the largest dataset of container sensor readings ever
used in academic research -- and show that they produce consistent improvements
over the baseline sensorless models. When evaluated on the holdout set of the
simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$
2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$
7.09% for relative humidity (vs 7.99% by sensorless) and average root
mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs
3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs
10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo
monitoring, early risk detection, and less dependence on full connectivity in
global shipping.

</details>


### [42] [Towards Formalizing Reinforcement Learning Theory](https://arxiv.org/abs/2511.03618)
*Shangtong Zhang*

Main category: cs.LG

TL;DR: 使用Lean 4定理证明器基于Mathlib库形式化验证了Q学习和线性TD学习在马尔可夫样本下的几乎必然收敛性


<details>
  <summary>Details</summary>
Motivation: Q学习和线性TD学习是最早且最有影响力的强化学习算法，研究其收敛性不仅是RL领域早期发展的主要研究课题，如今也受到越来越多的关注

Method: 基于Robbins-Siegmund定理的统一框架，使用Lean 4定理证明器进行形式化验证

Result: 成功形式化验证了Q学习和线性TD学习在马尔可夫样本下的几乎必然收敛性

Conclusion: 这项工作为完全形式化收敛性RL结果迈出了重要一步，所开发的框架可以轻松扩展到收敛速率和其他收敛模式

Abstract: In this paper, we formalize the almost sure convergence of $Q$-learning and
linear temporal difference (TD) learning with Markovian samples using the Lean
4 theorem prover based on the Mathlib library. $Q$-learning and linear TD are
among the earliest and most influential reinforcement learning (RL) algorithms.
The investigation of their convergence properties is not only a major research
topic during the early development of the RL field but also receives increasing
attention nowadays. This paper formally verifies their almost sure convergence
in a unified framework based on the Robbins-Siegmund theorem. The framework
developed in this work can be easily extended to convergence rates and other
modes of convergence. This work thus makes an important step towards fully
formalizing convergent RL results. The code is available at
https://github.com/ShangtongZhang/rl-theory-in-lean.

</details>


### [43] [Leveraging Discrete Function Decomposability for Scientific Design](https://arxiv.org/abs/2511.03032)
*James C. Bowden,Sergey Levine,Jennifer Listgarten*

Main category: cs.LG

TL;DR: 提出了DADO算法，一种能够利用设计变量可分解性的分布优化方法，通过图消息传递协调优化过程，提高离散设计空间中的优化效率。


<details>
  <summary>Details</summary>
Motivation: 在AI驱动的科学工程中，需要根据用户指定属性设计离散对象（如蛋白质、电路组件等）。现有分布优化算法无法利用设计变量的可分解性结构，限制了优化效率。

Method: 使用软因子化的"搜索分布"作为学习生成模型，通过图消息传递协调链接因子间的优化，利用junction tree定义的可分解性结构。

Result: DADO算法能够有效利用设计变量的可分解性，在离散设计空间中进行更高效的优化。

Conclusion: DADO为利用可分解性结构的分布优化提供了新方法，有望在蛋白质设计、电路优化等科学应用中提升设计效率。

Abstract: In the era of AI-driven science and engineering, we often want to design
discrete objects in silico according to user-specified properties. For example,
we may wish to design a protein to bind its target, arrange components within a
circuit to minimize latency, or find materials with certain properties. Given a
property predictive model, in silico design typically involves training a
generative model over the design space (e.g., protein sequence space) to
concentrate on designs with the desired properties. Distributional optimization
-- which can be formalized as an estimation of distribution algorithm or as
reinforcement learning policy optimization -- finds the generative model that
maximizes an objective function in expectation. Optimizing a distribution over
discrete-valued designs is in general challenging because of the combinatorial
nature of the design space. However, many property predictors in scientific
applications are decomposable in the sense that they can be factorized over
design variables in a way that could in principle enable more effective
optimization. For example, amino acids at a catalytic site of a protein may
only loosely interact with amino acids of the rest of the protein to achieve
maximal catalytic activity. Current distributional optimization algorithms are
unable to make use of such decomposability structure. Herein, we propose and
demonstrate use of a new distributional optimization algorithm,
Decomposition-Aware Distributional Optimization (DADO), that can leverage any
decomposability defined by a junction tree on the design variables, to make
optimization more efficient. At its core, DADO employs a soft-factorized
"search distribution" -- a learned generative model -- for efficient navigation
of the search space, invoking graph message-passing to coordinate optimization
across linked factors.

</details>


### [44] [Data-Efficient Realized Volatility Forecasting with Vision Transformers](https://arxiv.org/abs/2511.03046)
*Emi Soroka,Artem Arzyn*

Main category: cs.LG

TL;DR: 使用Vision Transformer (ViT)架构从隐含波动率表面预测资产未来30天的已实现波动率，探索将Transformer模型应用于期权数据的可行性。


<details>
  <summary>Details</summary>
Motivation: 金融机器学习研究表明深度学习能够学习高度非线性关系，在金融预测中表现优于简单方法。虽然Transformer架构在金融时间序列预测中显示出潜力，但在期权数据中的应用仍未被充分探索。

Method: 训练Vision Transformer (ViT)架构，从单日的隐含波动率表面（增强日期信息）预测资产未来30天的已实现波动率。

Result: ViT能够从隐含波动率表面学习季节性模式和非线性特征，表明这是一个有前景的模型开发方向。

Conclusion: 初步研究表明，将Transformer模型应用于期权数据具有潜力，ViT能够有效学习隐含波动率表面的复杂模式，为期权数据建模提供了新的方向。

Abstract: Recent work in financial machine learning has shown the virtue of complexity:
the phenomenon by which deep learning methods capable of learning highly
nonlinear relationships outperform simpler approaches in financial forecasting.
While transformer architectures like Informer have shown promise for financial
time series forecasting, the application of transformer models for options data
remains largely unexplored. We conduct preliminary studies towards the
development of a transformer model for options data by training the Vision
Transformer (ViT) architecture, typically used in modern image recognition and
classification systems, to predict the realized volatility of an asset over the
next 30 days from its implied volatility surface (augmented with date
information) for a single day. We show that the ViT can learn seasonal patterns
and nonlinear features from the IV surface, suggesting a promising direction
for model development.

</details>


### [45] [Unsupervised Evaluation of Multi-Turn Objective-Driven Interactions](https://arxiv.org/abs/2511.03047)
*Emi Soroka,Tanmay Chopra,Krish Desai,Sanjay Lall*

Main category: cs.LG

TL;DR: 提出了首个针对目标驱动交互的无监督评估指标，利用未标注交互数据的统计特性，通过微调LLM适应分布变化，无需人工标注即可评估用户目标、目标完成度和LLM不确定性。


<details>
  <summary>Details</summary>
Motivation: 企业应用中LLM驱动的AI代理与人类进行目标驱动交互时，现有评估方法面临数据复杂未标注、人工标注不可扩展、定制指标无法检测未知错误、LLM评估不可靠等挑战。

Method: 基于未标注交互数据的统计特性，使用微调LLM适应分布变化，开发了用户目标标注、目标完成度测量和LLM不确定性量化的无监督指标。

Result: 在开放领域和任务特定交互数据上验证了方法的有效性，能够在不依赖人工生成理想响应的情况下进行可靠评估。

Conclusion: 该无监督评估框架为LLM驱动的目标驱动交互系统提供了可扩展且可靠的评估解决方案，解决了现有方法的局限性。

Abstract: Large language models (LLMs) have seen increasing popularity in enterprise
applications where AI agents and humans engage in objective-driven
interactions. However, these systems are difficult to evaluate: data may be
complex and unlabeled; human annotation is often impractical at scale; custom
metrics can monitor for specific errors, but not previously-undetected ones;
and LLM judges can produce unreliable results. We introduce the first set of
unsupervised metrics for objective-driven interactions, leveraging statistical
properties of unlabeled interaction data and using fine-tuned LLMs to adapt to
distributional shifts. We develop metrics for labeling user goals, measuring
goal completion, and quantifying LLM uncertainty without grounding evaluations
in human-generated ideal responses. Our approach is validated on open-domain
and task-specific interaction data.

</details>


### [46] [The Curved Spacetime of Transformer Architectures](https://arxiv.org/abs/2511.03060)
*Riccardo Di Sipio,Jairo Diaz-Rodriguez,Luis Serrano*

Main category: cs.LG

TL;DR: 该论文提出了一个将Transformer语言模型与广义相对论类比的理论框架，将注意力机制视为离散连接，在表示空间中实现并行传输，并通过实验验证了嵌入空间曲率的存在和影响。


<details>
  <summary>Details</summary>
Motivation: 为理解Transformer语言模型提供一个几何视角，将复杂的神经网络机制与物理学的广义相对论进行类比，从而揭示语言表示的内在几何结构。

Method: 设计几何框架将查询和键视为诱导表示空间有效度规，注意力作为离散连接实现值向量的并行传输，并通过三个实验验证曲率存在：(i)可视化曲率景观，(ii)模拟分析角度分布，(iii)受爱因斯坦日食实验启发的上下文编辑实验。

Result: 实验证实了嵌入空间曲率的存在：可视化显示局部转向角在不同token和层间变化；模拟表明尖锐/平坦角的超额计数无法用维度或偶然性解释；上下文编辑实验显示嵌入轨迹出现可测量的、意义一致的弯曲。

Conclusion: Transformer语言模型确实在弯曲的表示空间中运行，注意力机制诱导的曲率影响token嵌入的演化轨迹，为理解语言模型的几何性质提供了新视角。

Abstract: We present a geometric framework for understanding Transformer-based language
models, drawing an explicit analogy to General Relativity. Queries and keys
induce an effective metric on representation space, and attention acts as a
discrete connection that implements parallel transport of value vectors across
tokens. Stacked layers provide discrete time-slices through which token
representations evolve on this curved manifold, while backpropagation plays the
role of a least-action principle that shapes loss-minimizing trajectories in
parameter space. If this analogy is correct, token embeddings should not
traverse straight paths in feature space; instead, their layer-wise steps
should bend and reorient as interactions mediated by embedding space curvature.
To test this prediction, we design experiments that expose both the presence
and the consequences of curvature: (i) we visualize a curvature landscape for a
full paragraph, revealing how local turning angles vary across tokens and
layers; (ii) we show through simulations that excess counts of sharp/flat
angles and longer length-to-chord ratios are not explainable by dimensionality
or chance; and (iii) inspired by Einstein's eclipse experiment, we probe
deflection under controlled context edits, demonstrating measurable,
meaning-consistent bends in embedding trajectories that confirm
attention-induced curvature.

</details>


### [47] [Homomorphism distortion: A metric to distinguish them all and in the latent space bind them](https://arxiv.org/abs/2511.03068)
*Martin Carrasco,Olga Zaghen,Erik Bekkers,Bastian Rieck*

Main category: cs.LG

TL;DR: 提出了一种基于图同态失真的图相似性度量方法，能够完全表征图结构，并可通过采样高效计算，在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 长期以来，图神经网络的表达能力仅通过组合特性来衡量，本文旨在提供一种基于图同态原理的图相似性度量新方法。

Method: 定义图同态失真作为图相似性度量，通过采样方法高效计算该度量，避免图规范化问题。

Result: 图同态失真能够完全区分BREC数据集中4-WL无法区分的图，在ZINC-12k数据集上优于先前基于同态的方法。

Conclusion: 该方法为图表征开辟了新途径，将图论传统扩展到新前沿。

Abstract: For far too long, expressivity of graph neural networks has been measured
\emph{only} in terms of combinatorial properties. In this work we stray away
from this tradition and provide a principled way to measure similarity between
vertex attributed graphs. We denote this measure as the \emph{graph
homomorphism distortion}. We show it can \emph{completely characterize} graphs
and thus is also a \emph{complete graph embedding}. However, somewhere along
the road, we run into the graph canonization problem. To circumvent this
obstacle, we devise to efficiently compute this measure via sampling, which in
expectation ensures \emph{completeness}. Additionally, we also discovered that
we can obtain a metric from this measure. We validate our claims empirically
and find that the \emph{graph homomorphism distortion}: (1.) fully
distinguishes the \texttt{BREC} dataset with up to $4$-WL non-distinguishable
graphs, and (2.) \emph{outperforms} previous methods inspired in homomorphisms
under the \texttt{ZINC-12k} dataset.
  These theoretical results, (and their empirical validation), pave the way for
future characterization of graphs, extending the graph theoretic tradition to
new frontiers.

</details>


### [48] [Online Learning to Rank under Corruption: A Robust Cascading Bandits Approach](https://arxiv.org/abs/2511.03074)
*Fatemeh Ghaffari,Siddarth Sitaraman,Xutong Liu,Xuchuang Wang,Mohammad Hajiesmaili*

Main category: cs.LG

TL;DR: 提出了MSUCB算法，这是一种针对在线学习排序中点击欺诈和操纵问题的鲁棒算法，使用新颖的中位数均值估计器，在无污染时达到最优对数遗憾，在有污染时遗憾仅随总污染量线性增加。


<details>
  <summary>Details</summary>
Motivation: 在线学习排序系统容易受到点击欺诈和其他操纵的影响，这些污染反馈会误导学习过程并降低用户体验。

Method: 提出了MSUCB算法，首次将中位数均值估计器应用于带污染的赌博机设置，在没有污染时表现如标准均值，在污染时通过中位数步骤过滤异常值和污染样本。

Result: 在真实世界数据集上的综合实验表明，该方法始终优于先前方法，同时保持强大的鲁棒性，相比两种最先进方法分别实现了97.35%和91.60%的遗憾改进。

Conclusion: MSUCB算法在无污染时实现最优对数遗憾，在有污染时遗憾仅随总污染量线性增加，表现出优雅的退化特性。

Abstract: Online learning to rank (OLTR) studies how to recommend a short ranked list
of items from a large pool and improves future rankings based on user clicks.
This setting is commonly modeled as cascading bandits, where the objective is
to maximize the likelihood that the user clicks on at least one of the
presented items across as many timesteps as possible. However, such systems are
vulnerable to click fraud and other manipulations (i.e., corruption), where
bots or paid click farms inject corrupted feedback that misleads the learning
process and degrades user experience. In this paper, we propose MSUCB, a robust
algorithm that incorporates a novel mean-of-medians estimator, which to our
knowledge is applied to bandits with corruption setting for the first time.
This estimator behaves like a standard mean in the absence of corruption, so no
cost is paid for robustness. Under corruption, the median step filters out
outliers and corrupted samples, keeping the estimate close to its true value.
Updating this estimate at every round further accelerates empirical convergence
in experiments. Hence, MSUCB achieves optimal logarithmic regret in the absence
of corruption and degrades gracefully under corruptions, with regret increasing
only by an additive term tied to the total corruption. Comprehensive and
extensive experiments on real-world datasets further demonstrate that our
approach consistently outperforms prior methods while maintaining strong
robustness. In particular, it achieves a \(97.35\%\) and a \(91.60\%\) regret
improvement over two state-of-the-art methods.

</details>


### [49] [Sparse, self-organizing ensembles of local kernels detect rare statistical anomalies](https://arxiv.org/abs/2511.03095)
*Gaia Grosso,Sai Sumedh R. Hindupur,Thomas Fel,Samuel Bright-Thonney,Philip Harris,Demba Ba*

Main category: cs.LG

TL;DR: 提出了SparKer方法，使用稀疏高斯核集合在Neyman-Pearson框架下进行异常检测，通过稀疏性、局部性和竞争性三个原则实现高效可解释的异常识别。


<details>
  <summary>Details</summary>
Motivation: 现代AI表示学习虽然强大，但其统计特性难以控制，导致传统异常检测方法在弱信号或罕见信号面前失效，需要新的检测框架。

Method: 基于稀疏性、局部性和竞争性三个原则，提出自组织局部核方法SparKer，使用稀疏高斯核集合在半监督Neyman-Pearson框架下建模似然比。

Result: 在数千维表示空间中，仅需少量核即可识别统计显著的异常位置，在科学发现、开放世界新颖性检测、入侵检测和生成模型验证等任务中表现优异。

Conclusion: SparKer方法具有可解释性、高效性和可扩展性，能够有效检测高维表示空间中的统计异常。

Abstract: Modern artificial intelligence has revolutionized our ability to extract rich
and versatile data representations across scientific disciplines. Yet, the
statistical properties of these representations remain poorly controlled,
causing misspecified anomaly detection (AD) methods to falter. Weak or rare
signals can remain hidden within the apparent regularity of normal data,
creating a gap in our ability to detect and interpret anomalies. We examine
this gap and identify a set of structural desiderata for detection methods
operating under minimal prior information: sparsity, to enforce parsimony;
locality, to preserve geometric sensitivity; and competition, to promote
efficient allocation of model capacity. These principles define a class of
self-organizing local kernels that adaptively partition the representation
space around regions of statistical imbalance. As an instantiation of these
principles, we introduce SparKer, a sparse ensemble of Gaussian kernels trained
within a semi-supervised Neyman--Pearson framework to locally model the
likelihood ratio between a sample that may contain anomalies and a nominal,
anomaly-free reference. We provide theoretical insights into the mechanisms
that drive detection and self-organization in the proposed model, and
demonstrate the effectiveness of this approach on realistic high-dimensional
problems of scientific discovery, open-world novelty detection, intrusion
detection, and generative-model validation. Our applications span both the
natural- and computer-science domains. We demonstrate that ensembles containing
only a handful of kernels can identify statistically significant anomalous
locations within representation spaces of thousands of dimensions, underscoring
both the interpretability, efficiency and scalability of the proposed approach.

</details>


### [50] [Scaling Multi-Agent Environment Co-Design with Diffusion Models](https://arxiv.org/abs/2511.03100)
*Hao Xiang Li,Michael Amir,Amanda Prorok*

Main category: cs.LG

TL;DR: 提出了DiCoDe框架，通过投影通用引导和评论家蒸馏机制，解决了多智能体环境协同设计中的可扩展性和样本效率问题，在仓库自动化等任务中显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前协同设计方法难以处理高维环境设计空间，且在联合优化中面临移动目标导致的样本效率低下问题，限制了在实际应用中的部署。

Method: 开发了DiCoDe框架，包含两个核心创新：投影通用引导(PUG)采样技术，在满足硬约束条件下探索奖励最大化环境分布；评论家蒸馏机制，通过共享强化学习评论家知识确保扩散模型适应演化中的智能体策略。

Result: 在仓库自动化、多智能体路径规划和风电场优化等基准测试中，方法持续超越现有技术，如在仓库设置中获得39%更高的奖励且减少66%的模拟样本。

Conclusion: DiCoDe为智能体-环境协同设计设立了新标准，是实现现实世界协同设计应用的重要进展。

Abstract: The agent-environment co-design paradigm jointly optimises agent policies and
environment configurations in search of improved system performance. With
application domains ranging from warehouse logistics to windfarm management,
co-design promises to fundamentally change how we deploy multi-agent systems.
However, current co-design methods struggle to scale. They collapse under
high-dimensional environment design spaces and suffer from sample inefficiency
when addressing moving targets inherent to joint optimisation. We address these
challenges by developing Diffusion Co-Design (DiCoDe), a scalable and
sample-efficient co-design framework pushing co-design towards practically
relevant settings. DiCoDe incorporates two core innovations. First, we
introduce Projected Universal Guidance (PUG), a sampling technique that enables
DiCoDe to explore a distribution of reward-maximising environments while
satisfying hard constraints such as spatial separation between obstacles.
Second, we devise a critic distillation mechanism to share knowledge from the
reinforcement learning critic, ensuring that the guided diffusion model adapts
to evolving agent policies using a dense and up-to-date learning signal.
Together, these improvements lead to superior environment-policy pairs when
validated on challenging multi-agent environment co-design benchmarks including
warehouse automation, multi-agent pathfinding and wind farm optimisation. Our
method consistently exceeds the state-of-the-art, achieving, for example, 39%
higher rewards in the warehouse setting with 66% fewer simulation samples. This
sets a new standard in agent-environment co-design, and is a stepping stone
towards reaping the rewards of co-design in real world domains.

</details>


### [51] [An Efficient Classification Model for Cyber Text](https://arxiv.org/abs/2511.03107)
*Md Sakhawat Hossen,Md. Zashid Iqbal Borshon,A. S. M. Badrudduza*

Main category: cs.LG

TL;DR: 提出改进的CTF-IDF算法和快速IRLBA降维方法，在文本分析中替代深度学习，显著降低计算复杂度和碳足迹，同时保持较高准确率。


<details>
  <summary>Details</summary>
Motivation: 深度学习在文本分析中的广泛应用导致计算资源需求和碳排放急剧增加，需要寻找更环保高效的替代方案。

Method: 改进传统TF-IDF算法为CTF-IDF，结合IRLBA快速降维算法，构建经典机器学习文本分析流程。

Result: 相比深度学习方法，显著降低了时间复杂度和计算资源消耗，碳足迹大幅减少，模型准确率仅有轻微下降。

Conclusion: 经典机器学习方法结合CTF-IDF和IRLBA在文本分析中能提供更高效环保的解决方案，是深度学习的有力替代方案。

Abstract: The uprising of deep learning methodology and practice in recent years has
brought about a severe consequence of increasing carbon footprint due to the
insatiable demand for computational resources and power. The field of text
analytics also experienced a massive transformation in this trend of
monopolizing methodology. In this paper, the original TF-IDF algorithm has been
modified, and Clement Term Frequency-Inverse Document Frequency (CTF-IDF) has
been proposed for data preprocessing. This paper primarily discusses the
effectiveness of classical machine learning techniques in text analytics with
CTF-IDF and a faster IRLBA algorithm for dimensionality reduction. The
introduction of both of these techniques in the conventional text analytics
pipeline ensures a more efficient, faster, and less computationally intensive
application when compared with deep learning methodology regarding carbon
footprint, with minor compromise in accuracy. The experimental results also
exhibit a manifold of reduction in time complexity and improvement of model
accuracy for the classical machine learning methods discussed further in this
paper.

</details>


### [52] [Towards Scalable Backpropagation-Free Gradient Estimation](https://arxiv.org/abs/2511.03110)
*Daniel Wang,Evan Markou,Dylan Campbell*

Main category: cs.LG

TL;DR: 提出了一种新的梯度估计方法，通过操纵上游雅可比矩阵来减少偏差和方差，在更宽的网络中表现更好


<details>
  <summary>Details</summary>
Motivation: 反向传播需要两次前向传播和存储中间激活值，而现有的前向模式自动微分方法由于高方差难以扩展到大型网络，且现有方法引入了显著偏差

Method: 通过操纵上游雅可比矩阵来计算猜测方向，减少梯度估计的偏差和方差

Result: 该方法显示出有希望的结果，随着网络宽度的增加性能更好

Conclusion: 该方法通过分析偏差和方差及其与神经网络梯度低维结构的联系来理解，具有扩展到更大网络的潜力

Abstract: While backpropagation--reverse-mode automatic differentiation--has been
extraordinarily successful in deep learning, it requires two passes (forward
and backward) through the neural network and the storage of intermediate
activations. Existing gradient estimation methods that instead use forward-mode
automatic differentiation struggle to scale beyond small networks due to the
high variance of the estimates. Efforts to mitigate this have so far introduced
significant bias to the estimates, reducing their utility. We introduce a
gradient estimation approach that reduces both bias and variance by
manipulating upstream Jacobian matrices when computing guess directions. It
shows promising results and has the potential to scale to larger networks,
indeed performing better as the network width is increased. Our understanding
of this method is facilitated by analyses of bias and variance, and their
connection to the low-dimensional structure of neural network gradients.

</details>


### [53] [FP-AbDiff: Improving Score-based Antibody Design by Capturing Nonequilibrium Dynamics through the Underlying Fokker-Planck Equation](https://arxiv.org/abs/2511.03113)
*Jiameng Chen,Yida Xiong,Kun Li,Hongzhi Zhang,Xiantao Cai,Wenbin Hu,Jia Wu*

Main category: cs.LG

TL;DR: FP-AbDiff是首个在整个生成轨迹中强制执行Fokker-Planck方程物理学的抗体生成器，通过FPE残差损失确保物理一致性，在RAbD基准测试中创下新记录。


<details>
  <summary>Details</summary>
Motivation: 现有抗体生成模型存在两个核心挑战：(i)缺乏动力学一致性，产生物理上不可行的结构；(ii)由于数据稀缺和结构偏差导致泛化能力差。

Method: 在SE(3)-等变扩散框架中引入FPE物理约束，在CDR几何混合流形(R^3 x SO(3))上最小化FPE残差损失，将局部学习的去噪分数组装成全局一致的概率流。

Result: 在de novo CDR-H3设计中，可变区RMSD达到0.99Å，比之前最佳模型AbX提升25%；接触氨基酸恢复率39.91%；六CDR协同设计中，全链RMSD降低约15%，CDR-H3环氨基酸恢复率达45.67%。

Conclusion: 通过将生成动力学与物理定律对齐，FP-AbDiff增强了鲁棒性和泛化能力，为物理忠实且功能可行的抗体设计建立了原则性方法。

Abstract: Computational antibody design holds immense promise for therapeutic
discovery, yet existing generative models are fundamentally limited by two core
challenges: (i) a lack of dynamical consistency, which yields physically
implausible structures, and (ii) poor generalization due to data scarcity and
structural bias. We introduce FP-AbDiff, the first antibody generator to
enforce Fokker-Planck Equation (FPE) physics along the entire generative
trajectory. Our method minimizes a novel FPE residual loss over the mixed
manifold of CDR geometries (R^3 x SO(3)), compelling locally-learned denoising
scores to assemble into a globally coherent probability flow. This
physics-informed regularizer is synergistically integrated with deep biological
priors within a state-of-the-art SE(3)-equivariant diffusion framework.
Rigorous evaluation on the RAbD benchmark confirms that FP-AbDiff establishes a
new state-of-the-art. In de novo CDR-H3 design, it achieves a mean Root Mean
Square Deviation of 0.99 {\AA} when superposing on the variable region, a 25%
improvement over the previous state-of-the-art model, AbX, and the highest
reported Contact Amino Acid Recovery of 39.91%. This superiority is underscored
in the more challenging six-CDR co-design task, where our model delivers
consistently superior geometric precision, cutting the average full-chain Root
Mean Square Deviation by ~15%, and crucially, achieves the highest full-chain
Amino Acid Recovery on the functionally dominant CDR-H3 loop (45.67%). By
aligning generative dynamics with physical laws, FP-AbDiff enhances robustness
and generalizability, establishing a principled approach for physically
faithful and functionally viable antibody design.

</details>


### [54] [An Augmentation Overlap Theory of Contrastive Learning](https://arxiv.org/abs/2511.03114)
*Qi Zhang,Yifei Wang,Yisen Wang*

Main category: cs.LG

TL;DR: 本文提出了增强重叠理论来解释对比学习的工作机制，推导了基于增强重叠假设的渐近紧界，并开发了一种无监督的表示评估指标。


<details>
  <summary>Details</summary>
Motivation: 自监督对比学习在各种任务上取得了巨大成功，但其工作机制尚不清楚。本文旨在通过更实际的增强重叠假设来揭示对比学习的工作机制。

Method: 首先基于条件独立性假设提供最紧边界，然后放松该假设到更实用的增强重叠假设，推导下游性能的渐近闭边界。基于增强重叠理论开发无监督表示评估指标。

Result: 提出的增强重叠理论表明，在激进数据增强下，同类样本的支持区域会变得更加重叠，因此简单对齐正样本（同一样本的增强视图）就能使对比学习将同类样本聚类在一起。

Conclusion: 从增强重叠的新视角开发的无监督表示评估指标与下游性能高度一致，几乎不需要依赖额外模块。

Abstract: Recently, self-supervised contrastive learning has achieved great success on
various tasks. However, its underlying working mechanism is yet unclear. In
this paper, we first provide the tightest bounds based on the widely adopted
assumption of conditional independence. Further, we relax the conditional
independence assumption to a more practical assumption of augmentation overlap
and derive the asymptotically closed bounds for the downstream performance. Our
proposed augmentation overlap theory hinges on the insight that the support of
different intra-class samples will become more overlapped under aggressive data
augmentations, thus simply aligning the positive samples (augmented views of
the same sample) could make contrastive learning cluster intra-class samples
together. Moreover, from the newly derived augmentation overlap perspective, we
develop an unsupervised metric for the representation evaluation of contrastive
learning, which aligns well with the downstream performance almost without
relying on additional modules. Code is available at
https://github.com/PKU-ML/GARC.

</details>


### [55] [From Insight to Exploit: Leveraging LLM Collaboration for Adaptive Adversarial Text Generation](https://arxiv.org/abs/2511.03128)
*Najrin Sultana,Md Rafi Ur Rashid,Kang Gu,Shagufta Mehnaz*

Main category: cs.LG

TL;DR: 提出了两种新型对抗攻击框架StaDec和DyDec，通过利用LLM的理解能力生成动态自适应的对抗样本，用于系统评估LLM的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当将LLMs应用于敏感任务时，需要全面评估其对对抗性输入的鲁棒性，但目前缺乏系统性的评估方法。

Method: 开发了静态欺骗器(StaDec)和动态欺骗器(DyDec)两种攻击框架，利用LLM驱动的自动化流水线生成语义相似但能有效欺骗目标LLM的自然对抗样本。

Result: 攻击方法随着LLM的进步而演化，并在攻击者未知的模型上表现出强大的可迁移性。

Conclusion: 这项工作为LLM的鲁棒性自我评估提供了系统性方法，有助于提高LLM在敏感任务中的安全性。

Abstract: LLMs can provide substantial zero-shot performance on diverse tasks using a
simple task prompt, eliminating the need for training or fine-tuning. However,
when applying these models to sensitive tasks, it is crucial to thoroughly
assess their robustness against adversarial inputs. In this work, we introduce
Static Deceptor (StaDec) and Dynamic Deceptor (DyDec), two innovative attack
frameworks designed to systematically generate dynamic and adaptive adversarial
examples by leveraging the understanding of the LLMs. We produce subtle and
natural-looking adversarial inputs that preserve semantic similarity to the
original text while effectively deceiving the target LLM. By utilizing an
automated, LLM-driven pipeline, we eliminate the dependence on external
heuristics. Our attacks evolve with the advancements in LLMs and demonstrate
strong transferability across models unknown to the attacker. Overall, this
work provides a systematic approach for the self-assessment of an LLM's
robustness. We release our code and data at
https://github.com/Shukti042/AdversarialExample.

</details>


### [56] [Test Time Adaptation Using Adaptive Quantile Recalibration](https://arxiv.org/abs/2511.03148)
*Paria Mehrbod,Pedro Vianna,Geraldin Nanfack,Guy Wolf,Eugene Belilovsky*

Main category: cs.LG

TL;DR: 提出了一种名为自适应分位数重校准（AQR）的测试时适应技术，通过通道级别的分位数对齐来修改预激活分布，无需重新训练模型即可实现无监督域适应。


<details>
  <summary>Details</summary>
Motivation: 解决传统域适应方法需要目标域先验知识或模型重新训练的问题，以及现有测试时适应方法无法捕捉复杂激活分布且局限于特定归一化层的局限性。

Method: AQR通过通道级别的分位数对齐来修改预激活分布，采用鲁棒的尾部校准策略来处理不同批次大小下的分布尾部估计问题，利用训练时计算的源域统计量。

Result: 在CIFAR-10-C、CIFAR-100-C和ImageNet-C数据集上的实验表明，AQR在多种架构下都能实现鲁棒的适应性能，优于现有的测试时适应基线方法。

Conclusion: AQR在动态和不可预测数据分布的真实场景中具有部署潜力，能够有效提升深度学习模型的泛化能力。

Abstract: Domain adaptation is a key strategy for enhancing the generalizability of
deep learning models in real-world scenarios, where test distributions often
diverge significantly from the training domain. However, conventional
approaches typically rely on prior knowledge of the target domain or require
model retraining, limiting their practicality in dynamic or
resource-constrained environments. Recent test-time adaptation methods based on
batch normalization statistic updates allow for unsupervised adaptation, but
they often fail to capture complex activation distributions and are constrained
to specific normalization layers. We propose Adaptive Quantile Recalibration
(AQR), a test-time adaptation technique that modifies pre-activation
distributions by aligning quantiles on a channel-wise basis. AQR captures the
full shape of activation distributions and generalizes across architectures
employing BatchNorm, GroupNorm, or LayerNorm. To address the challenge of
estimating distribution tails under varying batch sizes, AQR incorporates a
robust tail calibration strategy that improves stability and precision. Our
method leverages source-domain statistics computed at training time, enabling
unsupervised adaptation without retraining models. Experiments on CIFAR-10-C,
CIFAR-100-C, and ImageNet-C across multiple architectures demonstrate that AQR
achieves robust adaptation across diverse settings, outperforming existing
test-time adaptation baselines. These results highlight AQR's potential for
deployment in real-world scenarios with dynamic and unpredictable data
distributions.

</details>


### [57] [Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction](https://arxiv.org/abs/2511.03149)
*Atif Hassan,Tarun Kumar,Ashish Mishra,Sergey Serebryakov,Satish Kumar Mopur,Phanidhar Koganti,Murthy Chelankuri,Ramanagopal Vogety,Suparna Bhattacharya,Martin Foltin*

Main category: cs.LG

TL;DR: Forecast2Anomaly (F2A) 是一个新颖框架，通过联合预测-异常损失和检索增强生成模块，赋予时间序列基础模型异常预测能力，在16个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有异常预测方法局限于特定系统，无法适应随时间演变的异常模式，而预训练的时间序列基础模型虽然具有强大的泛化能力，但尚未用于异常预测任务。

Method: 提出联合预测-异常损失来微调模型，使其即使在异常时间点也能准确预测未来信号；引入检索增强生成模块，检索历史相关时间窗口并基于它们进行预测，动态适应推理时的分布变化。

Result: 在16个多样化数据集和多个时间序列基础模型骨干上的广泛实验表明，F2A始终优于最先进的方法。

Conclusion: F2A通过目标微调和动态检索的结合，弥合了强大的时间序列基础模型零样本预测与零样本异常预测之间的差距，为实际应用提供了可扩展的零样本异常预测解决方案。

Abstract: Forecasting anomalies (anomaly prediction) in multivariate time series from
different real-world, dynamic, and complex systems is vital for preempting
critical failures, leading to a substantial minimization in operational costs
and human labor. Yet, existing methods are limited to specific systems while
failing to generalize to evolving anomaly patterns over time. In contrast,
pretrained Time Series Foundation Models (TSFMs) have recently demonstrated
strong generalization and zero-shot forecasting capabilities. However, their
potential remains untapped for anomaly prediction, a task fundamentally
different from forecasting normal behavior. Thus, we present Forecast2Anomaly
(F2A), a novel framework that empowers TSFMs with anomaly prediction abilities
through two key innovations. First, we propose a joint forecast-anomaly loss
that fine-tunes TSFMs to accurately forecast future signals even at anomalous
time points. Second, we introduce a Retrieval-Augmented Generation (RAG) module
that retrieves historically relevant horizons and conditions predictions on
them. This component dynamically adapts to distributional shifts at inference
time, enabling F2A to track evolving anomalies without requiring model updates.
By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gap
between robust TSFM zero-shot forecasting and zero-shot anomaly prediction.
Extensive experiments across 16 diverse datasets and multiple TSFM backbones
show that F2A consistently outperforms state-of-the-art methods, offering a
scalable, zero-shot anomaly prediction solution for real-world applications.

</details>


### [58] [UnCLe: Towards Scalable Dynamic Causal Discovery in Non-linear Temporal Systems](https://arxiv.org/abs/2511.03168)
*Tingzhu Bi,Yicheng Pan,Xinrui Jiang,Huize Sun,Meng Ma,Ping Wang*

Main category: cs.LG

TL;DR: UnCLe是一种新颖的深度学习动态因果发现方法，通过解耦器和重耦合器网络分离时间序列，利用自回归依赖矩阵学习变量间依赖关系，能够准确捕捉动态因果关系。


<details>
  <summary>Details</summary>
Motivation: 现实世界系统通常表现出动态因果关系，现有方法大多推断静态因果图，无法准确捕捉随时间演化的关系，因此需要时间分辨的因果图。

Method: 使用解耦器和重耦合器网络将输入时间序列分解为语义表示，通过自回归依赖矩阵学习变量间依赖关系，通过分析时间扰动引起的数据点预测误差来估计动态因果影响。

Result: 在静态因果发现基准测试中优于最先进基线方法，更重要的是在合成和真实世界动态系统（如人体运动）中能够准确捕捉和表示演化的时间因果关系。

Conclusion: UnCLe为揭示复杂现象中潜在的、随时间变化的机制提供了一种有前景的方法。

Abstract: Uncovering cause-effect relationships from observational time series is
fundamental to understanding complex systems. While many methods infer static
causal graphs, real-world systems often exhibit dynamic causality-where
relationships evolve over time. Accurately capturing these temporal dynamics
requires time-resolved causal graphs. We propose UnCLe, a novel deep learning
method for scalable dynamic causal discovery. UnCLe employs a pair of Uncoupler
and Recoupler networks to disentangle input time series into semantic
representations and learns inter-variable dependencies via auto-regressive
Dependency Matrices. It estimates dynamic causal influences by analyzing
datapoint-wise prediction errors induced by temporal perturbations. Extensive
experiments demonstrate that UnCLe not only outperforms state-of-the-art
baselines on static causal discovery benchmarks but, more importantly, exhibits
a unique capability to accurately capture and represent evolving temporal
causality in both synthetic and real-world dynamic systems (e.g., human
motion). UnCLe offers a promising approach for revealing the underlying,
time-varying mechanisms of complex phenomena.

</details>


### [59] [Periodic Skill Discovery](https://arxiv.org/abs/2511.03187)
*Jonghae Park,Daesol Cho,Jusuk Lee,Dongseok Shim,Inkyu Jang,H. Jin Kim*

Main category: cs.LG

TL;DR: 提出了PSD框架，用于在无监督强化学习中自动发现周期性技能，通过将状态映射到圆形潜在空间来编码周期性，能够学习具有不同周期的多样化技能。


<details>
  <summary>Details</summary>
Motivation: 现有无监督技能发现方法往往忽略技能的周期性特征，而许多机器人任务（特别是运动控制）需要在不同时间尺度上执行周期性行为，因此发现多样化周期性技能至关重要。

Method: 训练编码器将状态映射到圆形潜在空间，在潜在表示中自然编码周期性，通过捕捉时间距离来学习具有不同周期的技能。

Result: PSD能够在复杂机器人任务中有效学习具有不同周期的技能，即使基于像素观测也能工作，在跨栏等下游任务中表现优异，与现有方法结合可提供更多样化的行为。

Conclusion: PSD框架成功解决了无监督周期性技能发现问题，为机器人学习提供了更丰富的技能库，在复杂任务中展现出良好性能。

Abstract: Unsupervised skill discovery in reinforcement learning (RL) aims to learn
diverse behaviors without relying on external rewards. However, current methods
often overlook the periodic nature of learned skills, focusing instead on
increasing the mutual dependence between states and skills or maximizing the
distance traveled in latent space. Considering that many robotic tasks --
particularly those involving locomotion -- require periodic behaviors across
varying timescales, the ability to discover diverse periodic skills is
essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a
framework that discovers periodic behaviors in an unsupervised manner. The key
idea of PSD is to train an encoder that maps states to a circular latent space,
thereby naturally encoding periodicity in the latent representation. By
capturing temporal distance, PSD can effectively learn skills with diverse
periods in complex robotic tasks, even with pixel-based observations. We
further show that these learned skills achieve high performance on downstream
tasks such as hurdling. Moreover, integrating PSD with an existing skill
discovery method offers more diverse behaviors, thus broadening the agent's
repertoire. Our code and demos are available at
https://jonghaepark.github.io/psd/

</details>


### [60] [Efficient Linear Attention for Multivariate Time Series Modeling via Entropy Equality](https://arxiv.org/abs/2511.03190)
*Mingtao Zhang,Guoli Yang,Zhanxing Zhu,Mengzhu Wang,Xiaoying Bai*

Main category: cs.LG

TL;DR: 提出了一种基于熵的线性注意力机制，通过熵值相似性来近似传统注意力，实现线性复杂度，在时空时间序列预测中达到竞争性能并显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制因二次计算复杂度难以扩展到长序列，需要开发高效的线性注意力替代方案。

Method: 基于熵的严格凹性理论，开发线性复杂度算法计算点积分布熵值，构建基于熵等性的线性注意力机制。

Result: 在四个时空数据集上的实验表明，该方法在预测性能上具有竞争力或更优，同时大幅减少内存使用和计算时间。

Conclusion: 注意力的有效性可能主要源于适中的权重分布平衡而非softmax非线性，基于熵的线性注意力是高效可行的替代方案。

Abstract: Attention mechanisms have been extensively employed in various applications,
including time series modeling, owing to their capacity to capture intricate
dependencies; however, their utility is often constrained by quadratic
computational complexity, which impedes scalability for long sequences. In this
work, we propose a novel linear attention mechanism designed to overcome these
limitations. Our approach is grounded in a theoretical demonstration that
entropy, as a strictly concave function on the probability simplex, implies
that distributions with aligned probability rankings and similar entropy values
exhibit structural resemblance. Building on this insight, we develop an
efficient approximation algorithm that computes the entropy of
dot-product-derived distributions with only linear complexity, enabling the
implementation of a linear attention mechanism based on entropy equality.
Through rigorous analysis, we reveal that the effectiveness of attention in
spatio-temporal time series modeling may not primarily stem from the
non-linearity of softmax but rather from the attainment of a moderate and
well-balanced weight distribution. Extensive experiments on four
spatio-temporal datasets validate our method, demonstrating competitive or
superior forecasting performance while achieving substantial reductions in both
memory usage and computational time.

</details>


### [61] [A Probabilistic U-Net Approach to Downscaling Climate Simulations](https://arxiv.org/abs/2511.03197)
*Maryam Alipourhajiagha,Pierre-Louis Lemaire,Youssef Diouane,Julie Carreau*

Main category: cs.LG

TL;DR: 本文采用概率U-Net进行统计降尺度，结合确定性U-Net主干和变分潜空间来捕捉随机不确定性，评估了四种训练目标在降尺度降水和温度数据中的表现。


<details>
  <summary>Details</summary>
Motivation: 气候模型受限于计算成本，通常产生粗空间分辨率的输出，而许多气候变化影响研究需要更精细的尺度，统计降尺度可以弥合这一差距。

Method: 使用概率U-Net架构，结合确定性U-Net主干和变分潜空间，评估了afCRPS和WMSE-MS-SSIM两种训练目标的四种设置，用于将降水和温度数据从16倍粗分辨率降尺度。

Result: WMSE-MS-SSIM在某些设置下对极端值表现良好，而afCRPS在不同尺度上能更好地捕捉空间变异性。

Conclusion: 不同训练目标在统计降尺度任务中各有优势，需要根据具体应用需求选择合适的损失函数。

Abstract: Climate models are limited by heavy computational costs, often producing
outputs at coarse spatial resolutions, while many climate change impact studies
require finer scales. Statistical downscaling bridges this gap, and we adapt
the probabilistic U-Net for this task, combining a deterministic U-Net backbone
with a variational latent space to capture aleatoric uncertainty. We evaluate
four training objectives, afCRPS and WMSE-MS-SSIM with three settings for
downscaling precipitation and temperature from $16\times$ coarser resolution.
Our main finding is that WMSE-MS-SSIM performs well for extremes under certain
settings, whereas afCRPS better captures spatial variability across scales.

</details>


### [62] [A Quantized VAE-MLP Botnet Detection Model: A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies](https://arxiv.org/abs/2511.03201)
*Hassan Wasswa,Hussein Abbass,Timothy Lynar*

Main category: cs.LG

TL;DR: 该研究提出了一种VAE-MLP框架，通过变分自编码器将高维数据压缩为8维潜在向量，然后系统评估了两种量化策略（QAT和PTQ）在IoT僵尸网络检测中的性能影响。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习检测方法计算量大，难以部署在资源受限的IoT设备上，需要轻量级检测模型来解决这一挑战。

Method: 使用预训练VAE的编码器将高维训练数据转换为8维潜在向量，然后训练MLP分类器，并系统评估QAT和PTQ两种量化策略。

Result: PTQ在检测精度上仅有轻微下降，而QAT下降更明显。PTQ实现了6倍加速和21倍压缩，QAT实现了3倍加速和24倍压缩。

Conclusion: 量化技术为设备级IoT僵尸网络检测提供了实用解决方案，其中PTQ在精度保持和性能提升方面表现更优。

Abstract: In an effort to counter the increasing IoT botnet-based attacks,
state-of-the-art deep learning methods have been proposed and have achieved
impressive detection accuracy. However, their computational intensity restricts
deployment on resource-constrained IoT devices, creating a critical need for
lightweight detection models. A common solution to this challenge is model
compression via quantization. This study proposes a VAE-MLP model framework
where an MLP-based classifier is trained on 8-dimensional latent vectors
derived from the high-dimensional train data using the encoder component of a
pretrained variational autoencoder (VAE). Two widely used quantization
strategies--Quantization-Aware Training (QAT) and Post-Training Quantization
(PTQ)--are then systematically evaluated in terms of their impact on detection
performance, storage efficiency, and inference latency using two benchmark IoT
botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with
respect to detection accuracy, the QAT strategy experienced a more noticeable
decline,whereas PTQ incurred only a marginal reduction compared to the original
unquantized model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in
size, while QAT achieved a 3x speedup and 24x compression, demonstrating the
practicality of quantization for device-level IoT botnet detection.

</details>


### [63] [Incorporating Quality of Life in Climate Adaptation Planning via Reinforcement Learning](https://arxiv.org/abs/2511.03238)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 使用强化学习优化城市洪水适应策略，通过集成评估模型结合降雨预测、洪水模拟、交通可达性和生活质量指数，寻找长期提升城市生活质量的适应路径。


<details>
  <summary>Details</summary>
Motivation: 气候变化导致城市洪水频率和严重性增加，影响城市生活质量，而政策制定者需要应对气候变化的不确定性和城市洪水的复杂性。

Method: 采用强化学习结合集成评估模型，整合降雨预测模型、洪水模型、交通可达性模型和生活质量指数来识别最优适应路径。

Result: 初步结果表明该方法能够学习到最优适应措施，并且优于其他现实和真实世界的规划策略。

Conclusion: 强化学习框架能够有效识别提升长期城市生活质量的洪水适应策略，为政策制定提供科学依据。

Abstract: Urban flooding is expected to increase in frequency and severity as a
consequence of climate change, causing wide-ranging impacts that include a
decrease in urban Quality of Life (QoL). Meanwhile, policymakers must devise
adaptation strategies that can cope with the uncertain nature of climate change
and the complex and dynamic nature of urban flooding. Reinforcement Learning
(RL) holds significant promise in tackling such complex, dynamic, and uncertain
problems. Because of this, we use RL to identify which climate adaptation
pathways lead to a higher QoL in the long term. We do this using an Integrated
Assessment Model (IAM) which combines a rainfall projection model, a flood
model, a transport accessibility model, and a quality of life index. Our
preliminary results suggest that this approach can be used to learn optimal
adaptation measures and it outperforms other realistic and real-world planning
strategies. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [64] [A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams](https://arxiv.org/abs/2511.03239)
*Philipp Reis,Philipp Rigoll,Christian Steinhauser,Jacob Langner,Eric Sax*

Main category: cs.LG

TL;DR: FCDC将数据收集建模为闭环控制问题，通过在线概率模型和反馈信号动态调节样本保留，在减少39.8%存储的同时提升数据集平衡性25.9%


<details>
  <summary>Details</summary>
Motivation: 现代AI系统主要受限于数据质量和多样性，传统开环数据收集方式会积累冗余样本，导致存储效率低、标注成本高和泛化能力有限

Method: FCDC使用在线概率模型近似已收集数据分布状态，基于似然和马氏距离等反馈信号自适应调节样本保留，动态平衡探索与利用

Result: 在真实数据流实验中，FCDC减少39.8%数据存储的同时提升数据集平衡性25.9%

Conclusion: 数据收集本身可以主动控制，将其从被动流水线阶段转变为数据驱动AI核心的自调节、反馈驱动过程

Abstract: Modern AI systems are increasingly constrained not by model capacity but by
the quality and diversity of their data. Despite growing emphasis on
data-centric AI, most datasets are still gathered in an open-loop manner which
accumulates redundant samples without feedback from the current coverage. This
results in inefficient storage, costly labeling, and limited generalization. To
address this, this paper introduces \ac{FCDC}, a paradigm that formulates data
collection as a closed-loop control problem. \ac{FCDC} continuously
approximates the state of the collected data distribution using an online
probabilistic model and adaptively regulates sample retention using based on
feedback signals such as likelihood and Mahalanobis distance. Through this
feedback mechanism, the system dynamically balances exploration and
exploitation, maintains dataset diversity, and prevents redundancy from
accumulating over time. Besides showcasing the controllability of \ac{FCDC} on
a synthetic dataset, experiments on a real data stream show that \ac{FCDC}
produces more balanced datasets by $\SI{25.9}{\percent}$ while reducing data
storage by $\SI{39.8}{\percent}$. These results demonstrate that data
collection itself can be actively controlled, transforming collection from a
passive pipeline stage into a self-regulating, feedback-driven process at the
core of data-centric AI.

</details>


### [65] [A unified physics-informed generative operator framework for general inverse problems](https://arxiv.org/abs/2511.03241)
*Gang Bao,Yaohua Zang*

Main category: cs.LG

TL;DR: 提出IGNO生成神经算子框架，统一解决点测量和算子值数据的逆问题，无需标记训练对，通过物理约束和潜在空间优化实现准确稳定的反演


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法需要大量标记数据或局限于特定测量类型，在稀疏、噪声、高维或不连续系数情况下容易失败，限制了实际应用

Method: 将高维系数场编码到低维潜在空间，通过神经算子解码器重建系数和PDE解，仅依赖物理约束训练，在潜在空间进行梯度优化，并用归一化流模型加速

Result: 在多种挑战性逆问题中（包括不连续系数恢复和EIT问题），IGNO实现了准确、稳定、可扩展的反演，在严重噪声下表现优异，优于现有方法

Conclusion: IGNO为计算科学领域的挑战性逆问题提供了一个统一而强大的框架

Abstract: Solving inverse problems governed by partial differential equations (PDEs) is
central to science and engineering, yet remains challenging when measurements
are sparse, noisy, or when the underlying coefficients are high-dimensional or
discontinuous. Existing deep learning approaches either require extensive
labeled datasets or are limited to specific measurement types, often leading to
failure in such regimes and restricting their practical applicability. Here, a
novel generative neural operator framework, IGNO, is introduced to overcome
these limitations. IGNO unifies the solution of inverse problems from both
point measurements and operator-valued data without labeled training pairs.
This framework encodes high-dimensional, potentially discontinuous coefficient
fields into a low-dimensional latent space, which drives neural operator
decoders to reconstruct both coefficients and PDE solutions. Training relies
purely on physics constraints through PDE residuals, while inversion proceeds
via efficient gradient-based optimization in latent space, accelerated by an a
priori normalizing flow model. Across a diverse set of challenging inverse
problems, including recovery of discontinuous coefficients from solution-based
measurements and the EIT problem with operator-based measurements, IGNO
consistently achieves accurate, stable, and scalable inversion even under
severe noise. It consistently outperforms the state-of-the-art method under
varying noise levels and demonstrates strong generalization to
out-of-distribution targets. These results establish IGNO as a unified and
powerful framework for tackling challenging inverse problems across
computational science domains.

</details>


### [66] [Climate Adaptation with Reinforcement Learning: Economic vs. Quality of Life Adaptation Pathways](https://arxiv.org/abs/2511.03243)
*Miguel Costa,Arthur Vandervoort,Martin Drews,Karyn Morrissey,Francisco C. Pereira*

Main category: cs.LG

TL;DR: 本文提出使用强化学习来识别气候变化下的适应路径，并明确比较不同适应优先级（如经济vs生活质量）对政策的影响。


<details>
  <summary>Details</summary>
Motivation: 气候变化将增加洪水事件的频率和严重性，需要有效的适应政策制定。但政策设计面临长期气候影响的不确定性，且包含重要的规范性选择往往未被明确说明。

Method: 使用强化学习（RL）结合综合评估模型（IAM），将降雨和洪水模型联系起来，计算洪水对生活质量、交通和基础设施的影响。

Result: 结果显示，优先考虑生活质量而非经济影响的模型会导致更多的适应支出，并在研究区域内更均匀地分配支出。

Conclusion: 强化学习是识别不确定条件下适应路径的有用工具，同时允许明确建模和比较不同的适应优先级，规范性假设会显著改变适应政策。

Abstract: Climate change will cause an increase in the frequency and severity of flood
events, prompting the need for cohesive adaptation policymaking. Designing
effective adaptation policies, however, depends on managing the uncertainty of
long-term climate impacts. Meanwhile, such policies can feature important
normative choices that are not always made explicit. We propose that
Reinforcement Learning (RL) can be a useful tool to both identify adaptation
pathways under uncertain conditions while it also allows for the explicit
modelling (and consequent comparison) of different adaptation priorities (e.g.
economic vs. wellbeing). We use an Integrated Assessment Model (IAM) to link
together a rainfall and flood model, and compute the impacts of flooding in
terms of quality of life (QoL), transportation, and infrastructure damage. Our
results show that models prioritising QoL over economic impacts results in more
adaptation spending as well as a more even distribution of spending over the
study area, highlighting the extent to which such normative assumptions can
alter adaptation policy. Our framework is publicly available:
https://github.com/MLSM-at-DTU/maat_qol_framework.

</details>


### [67] [GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models](https://arxiv.org/abs/2511.03251)
*Zhibin Wang,Zhixing Zhang,Shuqi Wang,Xuanting Xie,Zhao Kang*

Main category: cs.LG

TL;DR: GMoPE是一个将混合专家架构与基于提示的图学习相结合的新框架，通过专家特定提示向量和结构感知路由实现跨领域泛化，显著降低了适应成本。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络在跨领域泛化方面存在局限性，面临负迁移、可扩展性问题和适应成本高等挑战。

Method: 提出GMoPE框架，集成混合专家架构与提示学习，使用专家特定提示向量、结构感知路由和软正交约束来促进专家多样性，采用仅提示微调策略降低复杂度。

Result: 在各种预训练策略和下游任务中，GMoPE持续优于最先进基线，性能可与全参数微调相媲美，同时仅需少量适应开销。

Conclusion: GMoPE为推进通用且高效的图基础模型提供了一个原则性和可扩展的框架。

Abstract: Graph Neural Networks (GNNs) have demonstrated impressive performance on
task-specific benchmarks, yet their ability to generalize across diverse
domains and tasks remains limited. Existing approaches often struggle with
negative transfer, scalability issues, and high adaptation costs. To address
these challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novel
framework that seamlessly integrates the Mixture-of-Experts (MoE) architecture
with prompt-based learning for graphs. GMoPE leverages expert-specific prompt
vectors and structure-aware MoE routing to enable each expert to specialize in
distinct subdomains and dynamically contribute to predictions. To promote
diversity and prevent expert collapse, we introduce a soft orthogonality
constraint across prompt vectors, encouraging expert specialization and
facilitating a more balanced expert utilization. Additionally, we adopt a
prompt-only fine-tuning strategy that significantly reduces spatiotemporal
complexity during transfer. We validate GMoPE through extensive experiments
under various pretraining strategies and multiple downstream tasks. Results
show that GMoPE consistently outperforms state-of-the-art baselines and
achieves performance comparable to full parameter fine-tuning-while requiring
only a fraction of the adaptation overhead. Our work provides a principled and
scalable framework for advancing generalizable and efficient graph foundation
models.

</details>


### [68] [Diffusion Language Models are Super Data Learners](https://arxiv.org/abs/2511.03276)
*Jinjie Ni,Qian Liu,Longxu Dou,Chao Du,Zili Wang,Hang Yan,Tianyu Pang,Michael Qizhe Shieh*

Main category: cs.LG

TL;DR: 在数据受限情况下，扩散语言模型(DLMs)通过更多训练轮次超越自回归模型(AR)，这种优势源于多阶建模、迭代双向去噪和内置数据增强。


<details>
  <summary>Details</summary>
Motivation: 研究在严格控制预训练条件下，当独特数据有限时，扩散语言模型与自回归模型的性能比较，探索数据约束下的模型训练规律。

Method: 在严格匹配的预训练设置下，比较扩散语言模型和自回归模型在不同数据量、模型大小和架构下的表现，分析影响性能的关键因素。

Result: 1.7B参数的DLM在10B独特Python token上训练，计算预算约1.5T token时超越AR模型；1B参数的DLM仅用1B token就达到HellaSwag >56%和MMLU >33%准确率。

Conclusion: 扩散语言模型在数据受限场景下具有显著优势，验证交叉熵上升不一定意味着下游性能下降，为数据高效训练提供了新思路。

Abstract: Under strictly controlled pre-training settings, we observe a Crossover: when
unique data is limited, diffusion language models (DLMs) consistently surpass
autoregressive (AR) models by training for more epochs. The crossover shifts
later with more or higher-quality data, earlier with larger models, and
persists across dense and sparse architectures. We attribute the gains to three
compounding factors: (1) any-order modeling, (2) super-dense compute from
iterative bidirectional denoising, and (3) built-in Monte Carlo augmentation;
input or parameter noise improves AR under data constraint but cannot close the
gap. At scale, a 1.7B DLM trained with a ~1.5T-token compute budget on 10B
unique Python tokens overtakes an AR coder trained with strictly matched
settings. In addition, a 1B-parameter DLM achieves > 56% accuracy on HellaSwag
and > 33% on MMLU using only 1B tokens, without any special tricks, just by
repeating standard pre-training data. We also show that rising validation
cross-entropy does not imply degraded downstream performance in this regime.

</details>


### [69] [Multi-Objective Adaptive Rate Limiting in Microservices Using Deep Reinforcement Learning](https://arxiv.org/abs/2511.03279)
*Ning Lyu,Yuxi Wang,Ziyu Cheng,Qingyuan Zhang,Feng Chen*

Main category: cs.LG

TL;DR: 提出基于深度强化学习的自适应API限流策略，在Kubernetes环境中相比传统固定阈值方法提升23.7%吞吐量，降低31.4% P99延迟


<details>
  <summary>Details</summary>
Motivation: 传统限流算法难以适应动态流量模式和变化的系统负载，需要更智能的自适应限流机制来确保系统稳定性和服务质量

Method: 设计结合DQN和A3C算法的混合架构，将限流决策过程建模为马尔可夫决策过程，通过环境交互学习最优限流策略

Result: 高负载场景下吞吐量提升23.7%，P99延迟降低31.4%；90天生产部署处理5亿日请求，服务降级事件减少82%，人工干预减少68%

Conclusion: 基于深度强化学习的自适应限流策略能有效平衡系统吞吐量和服务延迟，在动态云环境中具有显著优势

Abstract: As cloud computing and microservice architectures become increasingly
prevalent, API rate limiting has emerged as a critical mechanism for ensuring
system stability and service quality. Traditional rate limiting algorithms,
such as token bucket and sliding window, while widely adopted, struggle to
adapt to dynamic traffic patterns and varying system loads. This paper proposes
an adaptive rate limiting strategy based on deep reinforcement learning that
dynamically balances system throughput and service latency. We design a hybrid
architecture combining Deep Q-Network (DQN) and Asynchronous Advantage
Actor-Critic (A3C) algorithms, modeling the rate limiting decision process as a
Markov Decision Process. The system continuously monitors microservice states
and learns optimal rate limiting policies through environmental interaction.
Extensive experiments conducted in a Kubernetes cluster environment demonstrate
that our approach achieves 23.7% throughput improvement and 31.4% P99 latency
reduction compared to traditional fixed-threshold strategies under high-load
scenarios. Results from a 90-day production deployment handling 500 million
daily requests validate the practical effectiveness of the proposed method,
with 82% reduction in service degradation incidents and 68% decrease in manual
interventions.

</details>


### [70] [A Probabilistic Approach to Pose Synchronization for Multi-Reference Alignment with Applications to MIMO Wireless Communication Systems](https://arxiv.org/abs/2511.03280)
*Rob Romijnders,Gabriele Cesa,Christos Louizos,Kumar Pratik,Arash Behboodi*

Main category: cs.LG

TL;DR: 提出了一种新的多参考对齐算法，通过概率建模和相对位姿作为干扰变量进行边缘化，消除全局对称性，实现更直接的解决方案和更好的收敛性。


<details>
  <summary>Details</summary>
Motivation: 从分子成像到无线通信，从多个未对齐观测中重建信号对系统性能至关重要。多参考对齐问题在冷冻电镜、计算机视觉和无线通信系统等实际应用中广泛存在。

Method: 采用概率方法建模多参考对齐问题，使用相对位姿作为干扰变量进行边缘化，通过循环一致性避免集中式方法的立方复杂度，实现计算效率提升。

Result: 所提出的算法在各种实验设置下实现了更低的重建误差，同时通过去中心化方法显著节省了计算成本。

Conclusion: 该概率方法成功解决了多参考对齐问题中的全局对称性挑战，提供了更有效和可扩展的解决方案。

Abstract: From molecular imaging to wireless communications, the ability to align and
reconstruct signals from multiple misaligned observations is crucial for system
performance. We study the problem of multi-reference alignment (MRA), which
arises in many real-world problems, such as cryo-EM, computer vision, and, in
particular, wireless communication systems. Using a probabilistic approach to
model MRA, we find a new algorithm that uses relative poses as nuisance
variables to marginalize out -- thereby removing the global symmetries of the
problem and allowing for more direct solutions and improved convergence. The
decentralization of this approach enables significant computational savings by
avoiding the cubic scaling of centralized methods through cycle consistency.
Both proposed algorithms achieve lower reconstruction error across experimental
settings.

</details>


### [71] [Graph Neural AI with Temporal Dynamics for Comprehensive Anomaly Detection in Microservices](https://arxiv.org/abs/2511.03285)
*Qingyuan Zhang,Ning Lyu,Le Liu,Yuxi Wang,Ziyu Cheng,Cancan Hua*

Main category: cs.LG

TL;DR: 提出了一种结合图神经网络和时间建模的统一框架，用于微服务架构中的异常检测和根因溯源，在动态拓扑和复杂环境下表现出高准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决微服务架构中异常检测和根因溯源的问题，为分布式系统的智能运维提供技术支撑。

Method: 将微服务调用链抽象为有向图，使用图卷积聚合节点特征和建模依赖关系，引入门控循环单元建模时间演化，定义节点和路径级别的异常评分函数实现从局部异常检测到全局调用链溯源。

Result: 在AUC、ACC、Recall和F1-Score等关键指标上优于基线方法，在动态拓扑和复杂环境下保持高准确性和稳定性。

Conclusion: 为微服务异常检测提供了新的技术路径，为分布式系统智能运维奠定了方法论基础。

Abstract: This study addresses the problem of anomaly detection and root cause tracing
in microservice architectures and proposes a unified framework that combines
graph neural networks with temporal modeling. The microservice call chain is
abstracted as a directed graph, where multidimensional features of nodes and
edges are used to construct a service topology representation, and graph
convolution is applied to aggregate features across nodes and model
dependencies, capturing complex structural relationships among services. On
this basis, gated recurrent units are introduced to model the temporal
evolution of call chains, and multi-layer stacking and concatenation operations
are used to jointly obtain structural and temporal representations, improving
the ability to identify anomaly patterns. Furthermore, anomaly scoring
functions at both the node and path levels are defined to achieve unified
modeling from local anomaly detection to global call chain tracing, which
enables the identification of abnormal service nodes and the reconstruction of
potential anomaly propagation paths. Sensitivity experiments are then designed
from multiple dimensions, including hyperparameters, environmental
disturbances, and data distribution, to evaluate the framework, and results
show that it outperforms baseline methods in key metrics such as AUC, ACC,
Recall, and F1-Score, maintaining high accuracy and stability under dynamic
topologies and complex environments. This research not only provides a new
technical path for anomaly detection in microservices but also lays a
methodological foundation for intelligent operations in distributed systems.

</details>


### [72] [Extending Fair Null-Space Projections for Continuous Attributes to Kernel Methods](https://arxiv.org/abs/2511.03304)
*Felix Störck,Fabian Hinder,Barbara Hammer*

Main category: cs.LG

TL;DR: 本文提出了一种用于连续公平性的核方法，通过零空间投影技术扩展了公平性评估在连续保护属性和回归任务中的应用范围。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在日常社会生活中的广泛应用，公平性成为重要议题。现有研究主要关注离散属性，而连续属性特别是与回归任务结合的连续公平性研究相对匮乏。

Method: 提出了一种模型和公平性评分无关的核嵌入方法，通过迭代零空间投影技术扩展到核方法，支持连续保护属性。

Result: 与支持向量回归(SVR)结合使用时，该方法在多个数据集上表现出竞争性或改进的性能。

Conclusion: 该方法显著扩展了连续公平性评估的应用范围，为处理连续保护属性提供了有效的核方法解决方案。

Abstract: With the on-going integration of machine learning systems into the everyday
social life of millions the notion of fairness becomes an ever increasing
priority in their development. Fairness notions commonly rely on protected
attributes to assess potential biases. Here, the majority of literature focuses
on discrete setups regarding both target and protected attributes. The
literature on continuous attributes especially in conjunction with regression
-- we refer to this as \emph{continuous fairness} -- is scarce. A common
strategy is iterative null-space projection which as of now has only been
explored for linear models or embeddings such as obtained by a non-linear
encoder. We improve on this by generalizing to kernel methods, significantly
extending the scope. This yields a model and fairness-score agnostic method for
kernel embeddings applicable to continuous protected attributes. We demonstrate
that our novel approach in conjunction with Support Vector Regression (SVR)
provides competitive or improved performance across multiple datasets in
comparisons to other contemporary methods.

</details>


### [73] [SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration](https://arxiv.org/abs/2511.03344)
*Elif Arslan,Jacobus G. M. van der Linden,Serge Hoogendoorn,Marco Rinaldi,Emir Demirović*

Main category: cs.LG

TL;DR: SORTD框架可高效枚举Rashomon集合中的决策树，按目标值排序，比现有方法快两个数量级，支持可分离和全序目标函数。


<details>
  <summary>Details</summary>
Motivation: Rashomon集合包含性能相似但结构不同的决策树，可增强变量重要性分析、丰富解释性，并允许用户根据偏好选择更简单或满足公平性等标准的树，而无需将这些标准硬编码到目标函数中。

Method: 提出SORTD框架，改进可扩展性，按目标值顺序枚举Rashomon集合中的树，实现随时终止的特性。

Result: 实验表明SORTD相比现有技术将运行时间减少多达两个数量级，支持任何可分离和全序目标，并支持使用其他可分离目标对集合进行后评估。

Conclusion: 这些进展使得在现实应用中探索Rashomon集合更加实用。

Abstract: Sparse decision tree learning provides accurate and interpretable predictive
models that are ideal for high-stakes applications by finding the single most
accurate tree within a (soft) size limit. Rather than relying on a single
"best" tree, Rashomon sets-trees with similar performance but varying
structures-can be used to enhance variable importance analysis, enrich
explanations, and enable users to choose simpler trees or those that satisfy
stakeholder preferences (e.g., fairness) without hard-coding such criteria into
the objective function. However, because finding the optimal tree is NP-hard,
enumerating the Rashomon set is inherently challenging. Therefore, we introduce
SORTD, a novel framework that improves scalability and enumerates trees in the
Rashomon set in order of the objective value, thus offering anytime behavior.
Our experiments show that SORTD reduces runtime by up to two orders of
magnitude compared with the state of the art. Moreover, SORTD can compute
Rashomon sets for any separable and totally ordered objective and supports
post-evaluating the set using other separable (and partially ordered)
objectives. Together, these advances make exploring Rashomon sets more
practical in real-world applications.

</details>


### [74] [A Modular, Data-Free Pipeline for Multi-Label Intention Recognition in Transportation Agentic AI Applications](https://arxiv.org/abs/2511.03363)
*Xiaocai Zhang,Hur Lim,Ke Wang,Zhe Xiao,Jing Wang,Kelvin Lee,Xiuju Fu,Zheng Qin*

Main category: cs.LG

TL;DR: 提出了一种无需数据的模块化多标签意图识别管道DMTC，通过提示工程生成合成查询、Sentence-T5编码和在线焦点对比损失训练，在交通领域实现95.92%的AUC性能。


<details>
  <summary>Details</summary>
Motivation: 传统意图识别系统依赖大规模标注语料库，难以进行细粒度多标签识别，且数据收集成本高昂。

Method: 三阶段管道：1) 提示工程引导LLM生成多样化合成查询；2) Sentence-T5编码获取语义嵌入；3) 使用在线焦点对比损失训练轻量级分类器。

Result: 在海上交通应用中，DMTC实现5.35%的汉明损失和95.92%的AUC，优于现有多标签分类器和LLM基线。Sentence-T5嵌入比替代编码器提升至少3.29%的子集准确率，OFC损失比标准对比目标额外提升0.98%。

Conclusion: 该系统可将用户查询无缝路由到特定任务模块，为无需昂贵人工标注的完全自主意图感知代理奠定基础。

Abstract: In this study, a modular, data-free pipeline for multi-label intention
recognition is proposed for agentic AI applications in transportation. Unlike
traditional intent recognition systems that depend on large, annotated corpora
and often struggle with fine-grained, multi-label discrimination, our approach
eliminates the need for costly data collection while enhancing the accuracy of
multi-label intention understanding. Specifically, the overall pipeline, named
DMTC, consists of three steps: 1) using prompt engineering to guide large
language models (LLMs) to generate diverse synthetic queries in different
transport scenarios; 2) encoding each textual query with a Sentence-T5 model to
obtain compact semantic embeddings; 3) training a lightweight classifier using
a novel online focal-contrastive (OFC) loss that emphasizes hard samples and
maximizes inter-class separability. The applicability of the proposed pipeline
is demonstrated in an agentic AI application in the maritime transportation
context. Extensive experiments show that DMTC achieves a Hamming loss of 5.35%
and an AUC of 95.92%, outperforming state-of-the-art multi-label classifiers
and recent end-to-end SOTA LLM-based baselines. Further analysis reveals that
Sentence-T5 embeddings improve subset accuracy by at least 3.29% over
alternative encoders, and integrating the OFC loss yields an additional 0.98%
gain compared to standard contrastive objectives. In conclusion, our system
seamlessly routes user queries to task-specific modules (e.g., ETA information,
traffic risk evaluation, and other typical scenarios in the transportation
domain), laying the groundwork for fully autonomous, intention-aware agents
without costly manual labelling.

</details>


### [75] [TripleWin: Fixed-Point Equilibrium Pricing for Data-Model Coupled Markets](https://arxiv.org/abs/2511.03368)
*Hongrun Ren,Yun Xiong,Lei You,Yingying Wang,Haixu Xiong,Yangyong Zhu*

Main category: cs.LG

TL;DR: 提出统一的数据-模型耦合市场，将数据集和模型交易作为单一系统处理，通过双向映射机制实现均衡定价


<details>
  <summary>Details</summary>
Motivation: 机器学习模型经济兴起，但现有定价方法仍分离数据和模型交易，或依赖偏向一方的中介管道，缺乏同时对称的机制

Method: 构建统一数据-模型耦合市场，包含供应端映射（将数据集支付转化为买家可见的模型报价）和需求端映射（通过Shapley分配将买家价格传播回数据集），形成闭环系统

Result: 证明联合算子为标准干扰函数，保证均衡价格的存在性、唯一性和全局收敛性。实验显示高效收敛和相比中介中心化及单边基线的公平性提升

Conclusion: 提出的统一市场机制能够有效连接数据卖家、模型生产者和模型买家，实现三方共赢的定价策略

Abstract: The rise of the machine learning (ML) model economy has intertwined markets
for training datasets and pre-trained models. However, most pricing approaches
still separate data and model transactions or rely on broker-centric pipelines
that favor one side. Recent studies of data markets with externalities capture
buyer interactions but do not yield a simultaneous and symmetric mechanism
across data sellers, model producers, and model buyers. We propose a unified
data-model coupled market that treats dataset and model trading as a single
system. A supply-side mapping transforms dataset payments into buyer-visible
model quotations, while a demand-side mapping propagates buyer prices back to
datasets through Shapley-based allocation. Together, they form a closed loop
that links four interactions: supply-demand propagation in both directions and
mutual coupling among buyers and among sellers. We prove that the joint
operator is a standard interference function (SIF), guaranteeing existence,
uniqueness, and global convergence of equilibrium prices. Experiments
demonstrate efficient convergence and improved fairness compared with
broker-centric and one-sided baselines. The code is available on
https://github.com/HongrunRen1109/Triple-Win-Pricing.

</details>


### [76] [Adaptable Hindsight Experience Replay for Search-Based Learning](https://arxiv.org/abs/2511.03405)
*Alexandros Vazaios,Jannis Brugger,Cedric Derstroff,Kristian Kersting,Mira Mezini*

Main category: cs.LG

TL;DR: 提出Adaptable HER框架，将Hindsight Experience Replay与AlphaZero结合，解决稀疏奖励环境下神经网络训练困难的问题，在方程发现等任务中表现优于纯监督或强化学习方法。


<details>
  <summary>Details</summary>
Motivation: AlphaZero在稀疏奖励环境早期阶段无法提供有效指导，因为神经网络尚未得到充分训练。Hindsight Experience Replay通过重新标记失败轨迹作为监督学习信号来解决这个问题。

Method: 提出Adaptable HER框架，灵活整合HER与AlphaZero，允许调整HER的重新标记目标、策略目标和轨迹选择等属性。

Result: 实验表明，修改HER的可能性是有益的，在方程发现等任务中超越了纯监督学习或强化学习的性能。

Conclusion: Adaptable HER框架成功解决了稀疏奖励环境下的训练挑战，为AlphaZero类系统在经典搜索问题中的应用提供了有效解决方案。

Abstract: AlphaZero-like Monte Carlo Tree Search systems, originally introduced for
two-player games, dynamically balance exploration and exploitation using neural
network guidance. This combination makes them also suitable for classical
search problems. However, the original method of training the network with
simulation results is limited in sparse reward settings, especially in the
early stages, where the network cannot yet give guidance. Hindsight Experience
Replay (HER) addresses this issue by relabeling unsuccessful trajectories from
the search tree as supervised learning signals. We introduce Adaptable HER
(\ours{}), a flexible framework that integrates HER with AlphaZero, allowing
easy adjustments to HER properties such as relabeled goals, policy targets, and
trajectory selection. Our experiments, including equation discovery, show that
the possibility of modifying HER is beneficial and surpasses the performance of
pure supervised or reinforcement learning.

</details>


### [77] [POEMS: Product of Experts for Interpretable Multi-omic Integration using Sparse Decoding](https://arxiv.org/abs/2511.03464)
*Mihriban Kocak Balik,Pekka Marttinen,Negar Safinianaini*

Main category: cs.LG

TL;DR: POEMS是一个用于多组学数据整合的无监督概率框架，通过稀疏解码在保持预测性能的同时提供可解释性，解决了深度生成模型中性能与可解释性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型在整合多组学数据时存在性能与可解释性的权衡：要么牺牲可解释性追求预测性能，要么通过线性化解码器来强制可解释性但削弱了网络的非线性表达能力。

Method: POEMS采用专家乘积模型，通过1）稀疏连接将特征映射到潜在因子，实现生物标志物发现；2）共享潜在空间实现跨组学关联；3）门控网络自适应计算各组学在表示学习中的贡献。

Result: 在癌症亚型分型案例研究中，POEMS实现了具有竞争力的聚类和分类性能，同时提供了新颖的解释能力。

Conclusion: POEMS证明了在多组学表示学习中，基于生物标志物的洞察力和预测准确性可以共存，无需线性化网络任何部分即可提供可解释性。

Abstract: Integrating different molecular layers, i.e., multiomics data, is crucial for
unraveling the complexity of diseases; yet, most deep generative models either
prioritize predictive performance at the expense of interpretability or enforce
interpretability by linearizing the decoder, thereby weakening the network's
nonlinear expressiveness. To overcome this tradeoff, we introduce POEMS:
Product Of Experts for Interpretable Multiomics Integration using Sparse
Decoding, an unsupervised probabilistic framework that preserves predictive
performance while providing interpretability. POEMS provides interpretability
without linearizing any part of the network by 1) mapping features to latent
factors using sparse connections, which directly translates to biomarker
discovery, 2) allowing for cross-omic associations through a shared latent
space using product of experts model, and 3) reporting contributions of each
omic by a gating network that adaptively computes their influence in the
representation learning. Additionally, we present an efficient sparse decoder.
In a cancer subtyping case study, POEMS achieves competitive clustering and
classification performance while offering our novel set of interpretations,
demonstrating that biomarker based insight and predictive accuracy can coexist
in multiomics representation learning.

</details>


### [78] [Reinforcement Learning Using known Invariances](https://arxiv.org/abs/2511.03473)
*Alexandru Cioba,Aya Kayal,Laura Toni,Sattar Vakili,Alberto Bernacchia*

Main category: cs.LG

TL;DR: 提出了一个将已知群对称性融入基于核的强化学习的理论和算法框架，通过不变核编码奖励和转移动态的不变性，显著提高了样本效率。


<details>
  <summary>Details</summary>
Motivation: 许多现实世界强化学习问题中的环境存在固有对称性，可以利用这些对称性来提高学习效率。

Method: 提出了对称感知的乐观最小二乘值迭代(LSVI)变体，使用不变核来编码奖励和转移动态的不变性。

Result: 在自定义Frozen Lake环境和2D布局设计问题上的实证结果表明，对称感知强化学习比标准核方法性能显著更好。

Conclusion: 这些发现强调了结构先验在设计更样本高效的强化学习算法中的价值。

Abstract: In many real-world reinforcement learning (RL) problems, the environment
exhibits inherent symmetries that can be exploited to improve learning
efficiency. This paper develops a theoretical and algorithmic framework for
incorporating known group symmetries into kernel-based RL. We propose a
symmetry-aware variant of optimistic least-squares value iteration (LSVI),
which leverages invariant kernels to encode invariance in both rewards and
transition dynamics. Our analysis establishes new bounds on the maximum
information gain and covering numbers for invariant RKHSs, explicitly
quantifying the sample efficiency gains from symmetry. Empirical results on a
customized Frozen Lake environment and a 2D placement design problem confirm
the theoretical improvements, demonstrating that symmetry-aware RL achieves
significantly better performance than their standard kernel counterparts. These
findings highlight the value of structural priors in designing more
sample-efficient reinforcement learning algorithms.

</details>


### [79] [RAGBoost: Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse](https://arxiv.org/abs/2511.03475)
*Yinsicheng Jiang,Yeqi Huang,Liang Cheng,Cheng Deng,Xuan Sun,Luo Mai*

Main category: cs.LG

TL;DR: RAGBoost是一个高效的检索增强生成系统，通过准确性保持的上下文重用实现高缓存复用率，在不牺牲准确性的情况下将预填充性能提升1.5-3倍。


<details>
  <summary>Details</summary>
Motivation: 现有的检索增强生成系统在处理更长更复杂的输入时预填充性能下降，现有缓存技术要么保持准确性但缓存复用率低，要么提高复用率但牺牲推理质量。

Method: 通过检测并发会话和多轮交互中的重叠检索项，使用高效的上下文索引、排序和去重来最大化复用，同时通过轻量级上下文提示保持推理保真度。

Result: 与现有最先进方法相比，预填充性能提升1.5-3倍，同时在多样化RAG和智能AI工作负载中保持甚至提高了推理准确性。

Conclusion: RAGBoost实现了高缓存复用率而不牺牲准确性，可无缝集成到现有LLM推理引擎中，显著提升RAG系统性能。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs)
with retrieved context but often suffers from downgraded prefill performance as
modern applications demand longer and more complex inputs. Existing caching
techniques either preserve accuracy with low cache reuse or improve reuse at
the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG
system that achieves high cache reuse without sacrificing accuracy through
accuracy-preserving context reuse. RAGBoost detects overlapping retrieved items
across concurrent sessions and multi-turn interactions, using efficient context
indexing, ordering, and de-duplication to maximize reuse, while lightweight
contextual hints maintain reasoning fidelity. It integrates seamlessly with
existing LLM inference engines and improves their prefill performance by 1.5-3X
over state-of-the-art methods, while preserving or even enhancing reasoning
accuracy across diverse RAG and agentic AI workloads. Our code is released at:
https://github.com/Edinburgh-AgenticAI/RAGBoost.

</details>


### [80] [NAP: Attention-Based Late Fusion for Automatic Sleep Staging](https://arxiv.org/abs/2511.03488)
*Alvise Dei Rossi,Julia van der Meer,Markus H. Schmidt,Claudio L. A. Bassetti,Luigi Fiorillo,Francesca Faraci*

Main category: cs.LG

TL;DR: 提出了NAP模型，通过三轴注意力机制聚合多通道预测，解决多导睡眠图信号异质性问题，实现零样本泛化


<details>
  <summary>Details</summary>
Motivation: 多导睡眠图信号存在模态组成、通道可用性和采集协议的异质性，现有模型依赖固定子集无法充分利用其多模态特性

Method: 基于注意力的NAP模型，使用三轴注意力机制捕捉时间、空间和预测器级依赖关系，通过聚合冻结预训练单通道模型的输出来适应不同输入维度

Result: NAP持续优于单个预测器和简单集成方法，在多个数据集上实现最先进的零样本泛化性能

Conclusion: 该方法可扩展到其他多模态生理应用，为处理异质多模态数据提供了有效解决方案

Abstract: Polysomnography signals are highly heterogeneous, varying in modality
composition (e.g., EEG, EOG, ECG), channel availability (e.g., frontal,
occipital EEG), and acquisition protocols across datasets and clinical sites.
Most existing models that process polysomnography data rely on a fixed subset
of modalities or channels and therefore neglect to fully exploit its inherently
multimodal nature. We address this limitation by introducing NAP (Neural
Aggregator of Predictions), an attention-based model which learns to combine
multiple prediction streams using a tri-axial attention mechanism that captures
temporal, spatial, and predictor-level dependencies. NAP is trained to adapt to
different input dimensions. By aggregating outputs from frozen, pretrained
single-channel models, NAP consistently outperforms individual predictors and
simple ensembles, achieving state-of-the-art zero-shot generalization across
multiple datasets. While demonstrated in the context of automated sleep staging
from polysomnography, the proposed approach could be extended to other
multimodal physiological applications.

</details>


### [81] [Learning Without Critics? Revisiting GRPO in Classical Reinforcement Learning Environments](https://arxiv.org/abs/2511.03527)
*Bryan L. M. de Oliveira,Felipe V. Frujeri,Marcos P. C. M. Queiroz,Luana G. B. Martins,Telma W. de L. Soares,Luckeciano C. Melo*

Main category: cs.LG

TL;DR: GRPO作为PPO的可扩展替代方案，通过轨迹组间比较估计优势值，无需学习评论家。研究发现：学习评论家在长时域任务中仍必不可少；GRPO需要高折扣因子；小分组规模表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究GRPO这种无需学习评论家的策略梯度方法是否真的能替代传统PPO，探究学习基线在策略梯度方法中的必要性。

Method: 在经典单任务强化学习环境中系统研究GRPO，通过控制实验分离基线、折扣和分组采样的影响，涵盖离散和连续控制任务。

Result: 1) 学习评论家在长时域任务中仍至关重要；2) GRPO需要高折扣因子(γ=0.99)；3) 小分组规模优于大分组。

Conclusion: 揭示了无评论家方法在经典控制中的局限性，以及它们在特定条件下作为学习价值函数替代方案的可行性。

Abstract: Group Relative Policy Optimization (GRPO) has emerged as a scalable
alternative to Proximal Policy Optimization (PPO) by eliminating the learned
critic and instead estimating advantages through group-relative comparisons of
trajectories. This simplification raises fundamental questions about the
necessity of learned baselines in policy-gradient methods. We present the first
systematic study of GRPO in classical single-task reinforcement learning
environments, spanning discrete and continuous control tasks. Through
controlled ablations isolating baselines, discounting, and group sampling, we
reveal three key findings: (1) learned critics remain essential for
long-horizon tasks: all critic-free baselines underperform PPO except in
short-horizon environments like CartPole where episodic returns can be
effective; (2) GRPO benefits from high discount factors (gamma = 0.99) except
in HalfCheetah, where lack of early termination favors moderate discounting
(gamma = 0.9); (3) smaller group sizes outperform larger ones, suggesting
limitations in batch-based grouping strategies that mix unrelated episodes.
These results reveal both the limitations of critic-free methods in classical
control and the specific conditions where they remain viable alternatives to
learned value functions.

</details>


### [82] [Byzantine-Robust Federated Learning with Learnable Aggregation Weights](https://arxiv.org/abs/2511.03529)
*Javad Parsa,Amir Hossein Daghestani,André M. H. Teixeira,Mikael Johansson*

Main category: cs.LG

TL;DR: 提出了一种新的拜占庭鲁棒联邦学习优化问题，将聚合权重作为可学习参数与全局模型参数联合优化，开发了具有强收敛保证的交替最小化算法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中恶意客户端的存在对系统鲁棒性构成重大挑战，特别是在客户端数据分布异构的情况下，需要更有效的拜占庭攻击防御机制。

Method: 将聚合权重视为可学习参数，与全局模型参数联合优化，开发了交替最小化算法来解决这一优化问题。

Result: 在各种数据集和攻击场景下的实验表明，该方法在高度异构数据和大量恶意客户端的情况下始终优于现有方法。

Conclusion: 所提出的自适应加权方法显著提升了联邦学习在拜占庭攻击下的鲁棒性，特别是在数据异构和恶意客户端比例高的场景中表现优异。

Abstract: Federated Learning (FL) enables clients to collaboratively train a global
model without sharing their private data. However, the presence of malicious
(Byzantine) clients poses significant challenges to the robustness of FL,
particularly when data distributions across clients are heterogeneous. In this
paper, we propose a novel Byzantine-robust FL optimization problem that
incorporates adaptive weighting into the aggregation process. Unlike
conventional approaches, our formulation treats aggregation weights as
learnable parameters, jointly optimizing them alongside the global model
parameters. To solve this optimization problem, we develop an alternating
minimization algorithm with strong convergence guarantees under adversarial
attack. We analyze the Byzantine resilience of the proposed objective. We
evaluate the performance of our algorithm against state-of-the-art
Byzantine-robust FL approaches across various datasets and attack scenarios.
Experimental results demonstrate that our method consistently outperforms
existing approaches, particularly in settings with highly heterogeneous data
and a large proportion of malicious clients.

</details>


### [83] [Efficient Neural Networks with Discrete Cosine Transform Activations](https://arxiv.org/abs/2511.03531)
*Marc Martinez-Gost,Sara Pepe,Ana Pérez-Neira,Miguel Ángel Lagunas*

Main category: cs.LG

TL;DR: 本文扩展了基于离散余弦变换(DCT)参数化的表达性神经网络(ENN)，重点研究其效率、可解释性和剪枝能力。DCT参数化提供了结构化表示，可直接识别冗余组件，实现高效剪枝。


<details>
  <summary>Details</summary>
Motivation: 在之前证明ENN具有强大表达能力的基础上，进一步探索其效率、可解释性和剪枝能力，将信号处理概念系统性地整合到神经网络设计中。

Method: 使用DCT参数化自适应激活函数的多层感知机，提出基于DCT系数剪枝的高效剪枝策略，利用DCT的正交性和有界性特性。

Result: 在分类和隐式神经表示任务中达到最先进精度，同时保持较少的参数数量。DCT激活系数可安全剪枝高达40%而性能损失可忽略。

Conclusion: ENN框架在信号处理概念与神经网络设计之间实现了原则性整合，在表达能力、紧凑性和可解释性之间达到了平衡的权衡。

Abstract: In this paper, we extend our previous work on the Expressive Neural Network
(ENN), a multilayer perceptron with adaptive activation functions parametrized
using the Discrete Cosine Transform (DCT). Building upon previous work that
demonstrated the strong expressiveness of ENNs with compact architectures, we
now emphasize their efficiency, interpretability and pruning capabilities. The
DCT-based parameterization provides a structured and decorrelated
representation that reveals the functional role of each neuron and allows
direct identification of redundant components. Leveraging this property, we
propose an efficient pruning strategy that removes unnecessary DCT coefficients
with negligible or no loss in performance. Experimental results across
classification and implicit neural representation tasks confirm that ENNs
achieve state-of-the-art accuracy while maintaining a low number of parameters.
Furthermore, up to 40% of the activation coefficients can be safely pruned,
thanks to the orthogonality and bounded nature of the DCT basis. Overall, these
findings demonstrate that the ENN framework offers a principled integration of
signal processing concepts into neural network design, achieving a balanced
trade-off between expressiveness, compactness, and interpretability.

</details>


### [84] [Flat Minima and Generalization: Insights from Stochastic Convex Optimization](https://arxiv.org/abs/2511.03548)
*Matan Schliserman,Shira Vansover-Hager,Tomer Koren*

Main category: cs.LG

TL;DR: 该研究在随机凸优化框架下分析了平坦最小值与泛化性能的关系，发现平坦最小值可能产生较差的泛化性能，而尖锐最小值反而能实现最优泛化。同时验证了两种锐度感知算法(SA-GD和SAM)在泛化方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 理解学习算法的泛化行为是学习理论的核心目标。最近有观点认为学习算法成功是因为收敛到平坦最小值，这与改善泛化性能相关。本研究旨在在随机凸优化这一基础设置下检验平坦最小值与泛化之间的关联。

Method: 在非负β-平滑目标的随机凸优化框架下，分析平坦最小值与泛化性能的关系。研究两种锐度感知算法：SA-GD（在预定义邻域内对最大损失执行梯度步骤）和SAM（基于归一化上升步骤的SA-GD计算高效近似）。使用算法稳定性技术建立种群风险上界。

Result: 1) 平坦经验最小值可能产生Ω(1)的平凡种群风险，而尖锐最小值能实现最优泛化；2) SA-GD虽然快速收敛到平坦最小值，但种群风险仍可达Ω(1)；3) SAM虽然最小化经验损失，但可能收敛到尖锐最小值并产生Ω(1)种群风险。

Conclusion: 即使在基础的随机凸优化设置中，平坦最小值与良好泛化之间也没有必然联系。锐度感知算法(SA-GD和SAM)在泛化方面存在局限性，平坦最小值本身不能保证良好的泛化性能。

Abstract: Understanding the generalization behavior of learning algorithms is a central
goal of learning theory. A recently emerging explanation is that learning
algorithms are successful in practice because they converge to flat minima,
which have been consistently associated with improved generalization
performance. In this work, we study the link between flat minima and
generalization in the canonical setting of stochastic convex optimization with
a non-negative, $\beta$-smooth objective. Our first finding is that, even in
this fundamental and well-studied setting, flat empirical minima may incur
trivial $\Omega(1)$ population risk while sharp minima generalizes optimally.
Then, we show that this poor generalization behavior extends to two natural
''sharpness-aware'' algorithms originally proposed by Foret et al. (2021),
designed to bias optimization toward flat solutions: Sharpness-Aware Gradient
Descent (SA-GD) and Sharpness-Aware Minimization (SAM). For SA-GD, which
performs gradient steps on the maximal loss in a predefined neighborhood, we
prove that while it successfully converges to a flat minimum at a fast rate,
the population risk of the solution can still be as large as $\Omega(1)$,
indicating that even flat minima found algorithmically using a sharpness-aware
gradient method might generalize poorly. For SAM, a computationally efficient
approximation of SA-GD based on normalized ascent steps, we show that although
it minimizes the empirical loss, it may converge to a sharp minimum and also
incur population risk $\Omega(1)$. Finally, we establish population risk upper
bounds for both SA-GD and SAM using algorithmic stability techniques.

</details>


### [85] [Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances](https://arxiv.org/abs/2511.03565)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 这篇论文是关于模仿学习(IL)最新进展的综述，提出了新的分类法来反映当前研究现状和发展趋势。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，模仿学习在各个领域的能力和可扩展性显著提升，需要系统梳理最新进展、方法创新和实际应用。

Method: 作者回顾了模仿学习的最新研究进展，提出了一个与现有分类不同的新分类法，批判性地分析了代表性工作的优缺点和评估实践。

Result: 论文系统总结了模仿学习的最新趋势、方法创新和应用，并提出了新的分类框架来更好地反映当前研究格局。

Conclusion: 论文概述了模仿学习领域的关键挑战和未来研究方向，为后续研究提供了重要参考。

Abstract: Imitation learning (IL) enables agents to acquire skills by observing and
replicating the behavior of one or multiple experts. In recent years, advances
in deep learning have significantly expanded the capabilities and scalability
of imitation learning across a range of domains, where expert data can range
from full state-action trajectories to partial observations or unlabeled
sequences. Alongside this growth, novel approaches have emerged, with new
methodologies being developed to address longstanding challenges such as
generalization, covariate shift, and demonstration quality. In this survey, we
review the latest advances in imitation learning research, highlighting recent
trends, methodological innovations, and practical applications. We propose a
novel taxonomy that is distinct from existing categorizations to better reflect
the current state of the IL research stratum and its trends. Throughout the
survey, we critically examine the strengths, limitations, and evaluation
practices of representative works, and we outline key challenges and open
directions for future research.

</details>


### [86] [TabGemma: Text-Based Tabular ICL via LLM using Continued Pretraining and Retrieval](https://arxiv.org/abs/2511.03570)
*Günther Schindler,Maximilian Schambach,Michael Medek,Sam Thelin*

Main category: cs.LG

TL;DR: TabGemma是一个用于表格预测的LLM模型，通过科学记数法处理数值、目标插补预训练和n-gram检索选择示例，在语义丰富的分类任务上达到SOTA，但在回归任务中数据量大时落后传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决将预训练LLMs应用于表格预测时的两个实际问题：不稳定的数值标记化和有限的上下文大小。

Method: 使用带符号科学记数法规范化数值，在12B Gemma 3模型上继续预训练，采用目标插补目标，使用大规模真实数据集，推理时使用紧凑的n-gram检索选择信息性示例。

Result: 在语义丰富的基准测试中，TabGemma在分类任务上达到新的SOTA，在低数据和高数据情况下都表现优异，且随着上下文行数增加而单调改进；在回归任务中，小样本时具有竞争力，但数据量大时落后传统方法。

Conclusion: LLMs在配备专用数值处理和上下文检索时，可以成为有效的表格上下文学习器，特别是在高度语义任务上，但需要在数值建模和长上下文扩展方面进一步改进。

Abstract: We study LLMs for tabular prediction with mixed text, numeric, and
categorical fields. We introduce TabGemma, a schema-agnostic in-context learner
that treats rows as sequences and tackles two practical hurdles when adapting
pretrained LLMs for tabular predictions: unstable numeric tokenization and
limited context size. We propose to canonicalize numbers via signed scientific
notation and continue pretraining of a 12B Gemma 3 model with a target
imputation objective using a large-scale real world dataset. For inference, we
use a compact n-gram-based retrieval to select informative exemplars that fit
within a 128k-token window.
  On semantically rich benchmarks, TabGemma establishes a new state of the art
on classification across low- and high-data regimes and improves monotonically
with more context rows. For regression, it is competitive at small sample sizes
but trails conventional approaches as data grows. Our results show that LLMs
can be effective tabular in-context learners on highly semantic tasks when
paired with dedicated numeric handling and context retrieval, while motivating
further advances in numeric modeling and long-context scaling.

</details>


### [87] [Learning Under Laws: A Constraint-Projected Neural PDE Solver that Eliminates Hallucinations](https://arxiv.org/abs/2511.03578)
*Mainak Singha*

Main category: cs.LG

TL;DR: CPL框架通过约束投影学习确保神经网络在求解偏微分方程时严格遵守物理定律，包括守恒、熵增、正性等约束，消除了物理违规现象。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络求解偏微分方程时经常违反物理定律（如质量不守恒、冲击波漂移、违反熵增原理等），需要一种能确保物理合规性的训练方法。

Method: 使用约束投影学习(CPL)框架，将网络输出投影到物理约束集的交集上；结合总变差阻尼(TVD)抑制小振荡，以及滚动课程学习确保长期预测一致性。

Result: 在Burgers和Euler系统上，CPL产生了稳定且物理合规的解，守恒律达到机器精度，总变差增长消失，熵和误差保持有界。

Conclusion: CPL使物理合规性成为学习过程的内在属性，而非依赖神经网络自行学习遵守物理定律。

Abstract: Neural networks can approximate solutions to partial differential equations,
but they often break the very laws they are meant to model-creating mass from
nowhere, drifting shocks, or violating conservation and entropy. We address
this by training within the laws of physics rather than beside them. Our
framework, called Constraint-Projected Learning (CPL), keeps every update
physically admissible by projecting network outputs onto the intersection of
constraint sets defined by conservation, Rankine-Hugoniot balance, entropy, and
positivity. The projection is differentiable and adds only about 10%
computational overhead, making it fully compatible with back-propagation. We
further stabilize training with total-variation damping (TVD) to suppress small
oscillations and a rollout curriculum that enforces consistency over long
prediction horizons. Together, these mechanisms eliminate both hard and soft
violations: conservation holds at machine precision, total-variation growth
vanishes, and entropy and error remain bounded. On Burgers and Euler systems,
CPL produces stable, physically lawful solutions without loss of accuracy.
Instead of hoping neural solvers will respect physics, CPL makes that behavior
an intrinsic property of the learning process.

</details>


### [88] [Going Beyond Expert Performance via Deep Implicit Imitation Reinforcement Learning](https://arxiv.org/abs/2511.03616)
*Iason Chrysomallis,Georgios Chalkiadakis*

Main category: cs.LG

TL;DR: 提出深度隐式模仿Q网络(DIIQN)框架，结合深度强化学习和仅从观察数据进行的隐式模仿学习，解决了传统模仿学习需要完整状态-动作对和最优专家的限制。


<details>
  <summary>Details</summary>
Motivation: 传统模仿学习需要完整的状态-动作演示和最优专家，这严重限制了实际应用，因为许多现实场景只提供状态观察而没有对应动作，且专家表现往往次优。

Method: DIIQN采用动作推断机制通过在线探索重建专家动作，并集成动态置信机制自适应平衡专家引导和自主学习。HA-DIIQN算法进一步处理专家和智能体具有不同动作集的情况，引入不可行性检测和桥接程序。

Result: DIIQN相比标准DQN获得高达130%的更高回合回报，持续优于现有无法超越专家性能的隐式模仿方法。在异构动作设置中，HA-DIIQN学习速度比基线快64%。

Conclusion: 该框架能够利用专家指导加速训练，同时保持超越次优专家性能的能力，在仅观察数据集和异构动作场景中表现出色。

Abstract: Imitation learning traditionally requires complete state-action
demonstrations from optimal or near-optimal experts. These requirements
severely limit practical applicability, as many real-world scenarios provide
only state observations without corresponding actions and expert performance is
often suboptimal. In this paper we introduce a deep implicit imitation
reinforcement learning framework that addresses both limitations by combining
deep reinforcement learning with implicit imitation learning from
observation-only datasets. Our main algorithm, Deep Implicit Imitation
Q-Network (DIIQN), employs an action inference mechanism that reconstructs
expert actions through online exploration and integrates a dynamic confidence
mechanism that adaptively balances expert-guided and self-directed learning.
This enables the agent to leverage expert guidance for accelerated training
while maintaining capacity to surpass suboptimal expert performance. We further
extend our framework with a Heterogeneous Actions DIIQN (HA-DIIQN) algorithm to
tackle scenarios where expert and agent possess different action sets, a
challenge previously unaddressed in the implicit imitation learning literature.
HA-DIIQN introduces an infeasibility detection mechanism and a bridging
procedure identifying alternative pathways connecting agent capabilities to
expert guidance when direct action replication is impossible. Our experimental
results demonstrate that DIIQN achieves up to 130% higher episodic returns
compared to standard DQN, while consistently outperforming existing implicit
imitation methods that cannot exceed expert performance. In heterogeneous
action settings, HA-DIIQN learns up to 64% faster than baselines, leveraging
expert datasets unusable by conventional approaches. Extensive parameter
sensitivity analysis reveals the framework's robustness across varying dataset
sizes and hyperparameter configurations.

</details>


### [89] [Financial Management System for SMEs: Real-World Deployment of Accounts Receivable and Cash Flow Prediction](https://arxiv.org/abs/2511.03631)
*Bartłomiej Małkus,Szymon Bobek,Grzegorz J. Nalepa*

Main category: cs.LG

TL;DR: 开发了一个面向中小企业的集成财务预测系统，包含应收账款预测和现金流预测功能，专门针对资源有限的自由职业者和早期企业设计。


<details>
  <summary>Details</summary>
Motivation: 解决中小企业特别是自由职业者和早期企业面临的财务挑战，这些企业资源有限、客户基础小、数据可用性受限，现有企业级财务工具无法满足其实际需求。

Method: 系统集成两个关键组件：用于预测发票付款延迟的二元分类模型，以及处理不完整和有限历史数据的多模块现金流预测模型。原型系统已作为Web应用程序部署并与Cluee平台集成。

Result: 成功开发并部署了原型系统，证明了该系统在真实世界中小企业财务管理中的实际可行性。

Conclusion: 该集成财务预测系统能够有效解决中小企业特有的财务挑战，填补了企业级财务工具与中小企业实际需求之间的空白。

Abstract: Small and Medium Enterprises (SMEs), particularly freelancers and early-stage
businesses, face unique financial management challenges due to limited
resources, small customer bases, and constrained data availability. This paper
presents the development and deployment of an integrated financial prediction
system that combines accounts receivable prediction and cash flow forecasting
specifically designed for SME operational constraints. Our system addresses the
gap between enterprise-focused financial tools and the practical needs of
freelancers and small businesses. The solution integrates two key components: a
binary classification model for predicting invoice payment delays, and a
multi-module cash flow forecasting model that handles incomplete and limited
historical data. A prototype system has been implemented and deployed as a web
application with integration into Cluee's platform, a startup providing
financial management tools for freelancers, demonstrating practical feasibility
for real-world SME financial management.

</details>


### [90] [nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN](https://arxiv.org/abs/2511.03634)
*Alexander Pfefferle,Johannes Hog,Lennart Purucker,Frank Hutter*

Main category: cs.LG

TL;DR: nanoTabPFN是一个简化的表格基础模型实现，相比TabPFN v2大幅减少了代码复杂度和计算需求，使表格基础模型对学生和研究人员更加可访问。


<details>
  <summary>Details</summary>
Motivation: 现有开源表格基础模型实现过于复杂（超过10,000行代码），缺乏架构文档和代码质量，难以理解和适应新实验。

Method: 简化TabPFN v2架构并实现相应的训练循环，使用预生成的训练数据，在单GPU上1分钟内完成预训练。

Result: 在小数据设置下，性能与传统机器学习基线相当，预训练速度比TabPFN v2快160,000倍。

Conclusion: nanoTabPFN消除了对大型计算资源的需求，使表格基础模型的预训练在教育用途中变得可行。

Abstract: Tabular foundation models such as TabPFN have revolutionized predictive
machine learning for tabular data. At the same time, the driving factors of
this revolution are hard to understand. Existing open-source tabular foundation
models are implemented in complicated pipelines boasting over 10,000 lines of
code, lack architecture documentation or code quality. In short, the
implementations are hard to understand, not beginner-friendly, and complicated
to adapt for new experiments. We introduce nanoTabPFN, a simplified and
lightweight implementation of the TabPFN v2 architecture and a corresponding
training loop that uses pre-generated training data. nanoTabPFN makes tabular
foundation models more accessible to students and researchers alike. For
example, restricted to a small data setting it achieves a performance
comparable to traditional machine learning baselines within one minute of
pre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). This
eliminated requirement of large computational resources makes pre-training
tabular foundation models accessible for educational purposes. Our code is
available at https://github.com/automl/nanoTabPFN.

</details>


### [91] [SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection](https://arxiv.org/abs/2511.03661)
*Mahek Desai,Apoorva Rumale,Marjan Asadinia*

Main category: cs.LG

TL;DR: 该研究提出一个机器学习框架，用于检测医疗物联网中的恶意网络攻击和设备故障异常。评估了8种机器学习模型在三种学习范式下的表现，XGBoost和KNN分别在异常检测和攻击检测中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 医疗物联网设备面临严重的安全性和可靠性挑战，容易受到网络威胁和操作异常的影响，需要有效的检测方法来保护患者数据和设备运行。

Method: 使用包含20万条记录的数据集，评估了8种机器学习模型：监督学习（XGBoost、KNN）、半监督学习（GAN、VAE）和无监督学习（One-Class SVM、Isolation Forest、GNN、LSTM Autoencoders）。

Result: XGBoost在异常检测中达到99%准确率，计算开销仅0.04秒；KNN在攻击检测中实现近乎完美的精度、召回率和F1分数，计算成本最低（0.05秒）；GAN表现最差，计算成本最高且准确率最低。

Conclusion: 该框架通过有效的异常检测策略增强了医疗物联网安全性，能够早期检测网络威胁和设备故障，防止数据泄露，减少系统停机时间，确保医疗设备持续安全运行。

Abstract: The integration of IoT devices in healthcare introduces significant security
and reliability challenges, increasing susceptibility to cyber threats and
operational anomalies. This study proposes a machine learning-driven framework
for (1) detecting malicious cyberattacks and (2) identifying faulty device
anomalies, leveraging a dataset of 200,000 records. Eight machine learning
models are evaluated across three learning approaches: supervised learning
(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (Generative
Adversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervised
learning (One-Class Support Vector Machine (SVM), Isolation Forest, Graph
Neural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). The
comprehensive evaluation was conducted across multiple metrics like F1-score,
precision, recall, accuracy, ROC-AUC, computational efficiency. XGBoost
achieved 99\% accuracy with minimal computational overhead (0.04s) for anomaly
detection, while Isolation Forest balanced precision and recall effectively.
LSTM Autoencoders underperformed with lower accuracy and higher latency. For
attack detection, KNN achieved near-perfect precision, recall, and F1-score
with the lowest computational cost (0.05s), followed by VAE at 97% accuracy.
GAN showed the highest computational cost with lowest accuracy and ROC-AUC.
These findings enhance IoT-enabled healthcare security through effective
anomaly detection strategies. By improving early detection of cyber threats and
device failures, this framework has the potential to prevent data breaches,
minimize system downtime, and ensure the continuous and safe operation of
medical devices, ultimately safeguarding patient health and trust in IoT-driven
healthcare solutions.

</details>


### [92] [DQN Performance with Epsilon Greedy Policies and Prioritized Experience Replay](https://arxiv.org/abs/2511.03670)
*Daniel Perkins,Oscar J. Escobar,Luke Green*

Main category: cs.LG

TL;DR: 对有限环境中深度Q网络的研究，重点分析ε-贪婪探索策略和优先经验回放的影响，通过实验比较不同策略的性能差异。


<details>
  <summary>Details</summary>
Motivation: 研究深度Q网络中探索策略与经验回放机制的相互作用，为资源受限环境下的强化学习提供实用建议。

Method: 通过系统实验评估不同ε衰减策略对学习效率的影响，比较均匀回放、无回放和优先回放三种策略在多个模拟环境中的表现。

Result: 优先经验回放能带来更快的收敛速度和更高的回报，不同探索策略与内存管理机制之间存在权衡关系。

Conclusion: 揭示了DQN训练中探索策略与内存管理之间的相互作用，为资源受限环境下的强化学习提供了实用的配置建议。

Abstract: We present a detailed study of Deep Q-Networks in finite environments,
emphasizing the impact of epsilon-greedy exploration schedules and prioritized
experience replay. Through systematic experimentation, we evaluate how
variations in epsilon decay schedules affect learning efficiency, convergence
behavior, and reward optimization. We investigate how prioritized experience
replay leads to faster convergence and higher returns and show empirical
results comparing uniform, no replay, and prioritized strategies across
multiple simulations. Our findings illuminate the trade-offs and interactions
between exploration strategies and memory management in DQN training, offering
practical recommendations for robust reinforcement learning in
resource-constrained settings.

</details>


### [93] [Structured Matrix Scaling for Multi-Class Calibration](https://arxiv.org/abs/2511.03685)
*Eugène Berta,David Holzmüller,Michael I. Jordan,Francis Bach*

Main category: cs.LG

TL;DR: 本文提出了基于逻辑回归的参数化后验校准方法，通过结构化正则化、鲁棒预处理和高效优化来管理偏差-方差权衡，在多分类校准中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 参数化校准函数基于逻辑回归，可以从简单的理论设置中推导出来，这促使使用比标准温度缩放更具表达力的校准方法。

Method: 使用基于逻辑回归的参数化后验校准方法，通过结构化正则化、鲁棒预处理和高效优化来管理多分类校准中的偏差-方差权衡问题。

Result: 该方法在实验中显著优于现有的基于逻辑回归的校准技术，提供了比常见温度、向量和矩阵缩放实现更好的性能。

Conclusion: 提出的方法通过有效管理偏差-方差权衡，为多分类校准提供了高效且易于使用的开源实现，是现有校准技术的有吸引力的替代方案。

Abstract: Post-hoc recalibration methods are widely used to ensure that classifiers
provide faithful probability estimates. We argue that parametric recalibration
functions based on logistic regression can be motivated from a simple
theoretical setting for both binary and multiclass classification. This insight
motivates the use of more expressive calibration methods beyond standard
temperature scaling. For multi-class calibration however, a key challenge lies
in the increasing number of parameters introduced by more complex models, often
coupled with limited calibration data, which can lead to overfitting. Through
extensive experiments, we demonstrate that the resulting bias-variance tradeoff
can be effectively managed by structured regularization, robust preprocessing
and efficient optimization. The resulting methods lead to substantial gains
over existing logistic-based calibration techniques. We provide efficient and
easy-to-use open-source implementations of our methods, making them an
attractive alternative to common temperature, vector, and matrix scaling
implementations.

</details>


### [94] [Behavior-Adaptive Q-Learning: A Unifying Framework for Offline-to-Online RL](https://arxiv.org/abs/2511.03695)
*Lipeng Zu,Hansong Zhou,Xiaonan Zhang*

Main category: cs.LG

TL;DR: BAQ是一种离线到在线强化学习框架，通过行为一致性信号实现平滑过渡，在不确定性高时对齐离线行为，随着在线经验积累逐渐放松约束。


<details>
  <summary>Details</summary>
Motivation: 解决离线RL策略在动态环境中部署时面临的分布偏移和不可靠价值估计问题，实现从离线到在线的可靠过渡。

Method: 利用离线数据中的隐式行为模型提供行为一致性信号，采用双目标损失函数：在不确定性高时对齐在线策略到离线行为，随着在线经验积累逐步放松约束。

Result: 在标准基准测试中，BAQ始终优于先前的离线到在线RL方法，实现更快的恢复、改进的鲁棒性和更高的整体性能。

Conclusion: 隐式行为适应是现实世界策略部署的原则性和实用解决方案，能够减少分布外估计的错误传播，稳定早期在线更新，并加速对新场景的适应。

Abstract: Offline reinforcement learning (RL) enables training from fixed data without
online interaction, but policies learned offline often struggle when deployed
in dynamic environments due to distributional shift and unreliable value
estimates on unseen state-action pairs. We introduce Behavior-Adaptive
Q-Learning (BAQ), a framework designed to enable a smooth and reliable
transition from offline to online RL. The key idea is to leverage an implicit
behavioral model derived from offline data to provide a behavior-consistency
signal during online fine-tuning. BAQ incorporates a dual-objective loss that
(i) aligns the online policy toward the offline behavior when uncertainty is
high, and (ii) gradually relaxes this constraint as more confident online
experience is accumulated. This adaptive mechanism reduces error propagation
from out-of-distribution estimates, stabilizes early online updates, and
accelerates adaptation to new scenarios. Across standard benchmarks, BAQ
consistently outperforms prior offline-to-online RL approaches, achieving
faster recovery, improved robustness, and higher overall performance. Our
results demonstrate that implicit behavior adaptation is a principled and
practical solution for reliable real-world policy deployment.

</details>


### [95] [AnaFlow: Agentic LLM-based Workflow for Reasoning-Driven Explainable and Sample-Efficient Analog Circuit Sizing](https://arxiv.org/abs/2511.03697)
*Mohsen Ahmadzadeh,Kaichang Chen,Georges Gielen*

Main category: cs.LG

TL;DR: 提出了一种基于多智能体LLM的模拟电路尺寸自动设计框架AnaFlow，解决了传统方法仿真效率低和缺乏可解释性的问题


<details>
  <summary>Details</summary>
Motivation: 模拟/混合信号电路设计仍主要依赖手工过程，存在设计周期长、易出错的问题。现有AI方法需要大量耗时仿真且缺乏可解释性，阻碍了工具的实际应用

Method: 采用多智能体工作流，专门的LLM智能体协作解释电路拓扑、理解设计目标，并通过可解释的推理迭代优化电路设计参数。自适应仿真策略实现高样本效率

Result: AnaFlow框架在两个复杂度不同的电路上成功完成全自动尺寸设计，相比纯贝叶斯优化和强化学习方法表现更优。系统能从优化历史中学习避免错误并加速收敛

Conclusion: 该框架为模拟电路设计空间探索提供了强大工具，代表了模拟EDA的新范式，其中AI智能体作为透明的设计助手

Abstract: Analog/mixed-signal circuits are key for interfacing electronics with the
physical world. Their design, however, remains a largely handcrafted process,
resulting in long and error-prone design cycles. While the recent rise of
AI-based reinforcement learning and generative AI has created new techniques to
automate this task, the need for many time-consuming simulations is a critical
bottleneck hindering the overall efficiency. Furthermore, the lack of
explainability of the resulting design solutions hampers widespread adoption of
the tools. To address these issues, a novel agentic AI framework for
sample-efficient and explainable analog circuit sizing is presented. It employs
a multi-agent workflow where specialized Large Language Model (LLM)-based
agents collaborate to interpret the circuit topology, to understand the design
goals, and to iteratively refine the circuit's design parameters towards the
target goals with human-interpretable reasoning. The adaptive simulation
strategy creates an intelligent control that yields a high sample efficiency.
The AnaFlow framework is demonstrated for two circuits of varying complexity
and is able to complete the sizing task fully automatically, differently from
pure Bayesian optimization and reinforcement learning approaches. The system
learns from its optimization history to avoid past mistakes and to accelerate
convergence. The inherent explainability makes this a powerful tool for analog
design space exploration and a new paradigm in analog EDA, where AI agents
serve as transparent design assistants.

</details>


### [96] [Shrinking the Variance: Shrinkage Baselines for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2511.03710)
*Guanning Zeng,Zhaoyi Zhou,Daman Arora,Andrea Zanette*

Main category: cs.LG

TL;DR: 本文提出了一种基于收缩估计的基线方法，用于改进强化学习与可验证奖励(RLVR)中的策略梯度估计，通过结合每个提示和跨提示的均值来降低方差。


<details>
  <summary>Details</summary>
Motivation: 传统的RLVR方法使用每个提示的经验均值作为基线来稳定训练，但在低生成次数的情况下估计不准确。受Stein悖论启发，作者希望改进基线的估计精度。

Method: 提出收缩估计器，将每个提示的均值和跨提示的均值结合起来，构建一个收缩基线来替代传统的经验均值基线。

Result: 理论上证明收缩基线能产生更低方差的策略梯度估计器；实证结果表明收缩基线始终优于标准经验均值基线，降低了梯度更新的方差并提高了训练稳定性。

Conclusion: 收缩基线可以作为现有每个提示均值基线的即插即用替代方案，无需额外超参数或计算，在RLVR中有效改进训练效果。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for post-training large reasoning models (LRMs) using
policy-gradient methods such as GRPO. To stabilize training, these methods
typically center trajectory rewards by subtracting the empirical mean for each
prompt. Statistically, this centering acts as a control variate (or baseline),
reducing the variance of the policy-gradient estimator.
  Typically, the mean reward is estimated using per-prompt empirical averages
for each prompt in a batch. Drawing inspiration from Stein's paradox, we
propose using shrinkage estimators that combine per-prompt and across-prompt
means to improve the overall per-prompt mean estimation accuracy --
particularly in the low-generation regime typical of RLVR. Theoretically, we
construct a shrinkage-based baseline that provably yields lower-variance
policy-gradient estimators across algorithms. Our proposed baseline serves as a
drop-in replacement for existing per-prompt mean baselines, requiring no
additional hyper-parameters or computation. Empirically, shrinkage baselines
consistently outperform standard empirical-mean baselines, leading to
lower-variance gradient updates and improved training stability.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [97] [Scalable Single-Cell Gene Expression Generation with Latent Diffusion Models](https://arxiv.org/abs/2511.02986)
*Giovanni Palla,Sudarshan Babu,Payam Dibaeinia,James D. Pearce,Donghui Li,Aly A. Khan,Theofanis Karaletsos,Jakub M. Tomczak*

Main category: stat.ML

TL;DR: 提出了scLDM，一种用于单细胞基因表达数据的可扩展潜在扩散模型，该模型尊重数据的可交换性特性，在观察性和扰动性单细胞数据中表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 单细胞基因表达的计算建模对于理解细胞过程至关重要，但由于基因表达数据的计数性质和基因间复杂的潜在依赖性，生成真实的表达谱仍然是一个主要挑战。现有生成模型通常施加人工基因排序或依赖浅层神经网络架构。

Method: 使用具有固定大小潜在变量的VAE，采用统一的Multi-head Cross-Attention Block (MCAB)架构，在编码器中实现置换不变池化，在解码器中实现置换等变解池化。用基于Diffusion Transformers和线性插值的潜在扩散模型替换高斯先验，支持多条件无分类器引导的高质量生成。

Result: 在观察性和扰动性单细胞数据以及细胞级分类等下游任务的各种实验中表现出优越性能。

Conclusion: scLDM是一个尊重单细胞基因表达数据基本可交换性特性的可扩展潜在扩散模型，能够有效生成真实的基因表达谱。

Abstract: Computational modeling of single-cell gene expression is crucial for
understanding cellular processes, but generating realistic expression profiles
remains a major challenge. This difficulty arises from the count nature of gene
expression data and complex latent dependencies among genes. Existing
generative models often impose artificial gene orderings or rely on shallow
neural network architectures. We introduce a scalable latent diffusion model
for single-cell gene expression data, which we refer to as scLDM, that respects
the fundamental exchangeability property of the data. Our VAE uses fixed-size
latent variables leveraging a unified Multi-head Cross-Attention Block (MCAB)
architecture, which serves dual roles: permutation-invariant pooling in the
encoder and permutation-equivariant unpooling in the decoder. We enhance this
framework by replacing the Gaussian prior with a latent diffusion model using
Diffusion Transformers and linear interpolants, enabling high-quality
generation with multi-conditional classifier-free guidance. We show its
superior performance in a variety of experiments for both observational and
perturbational single-cell data, as well as downstream tasks like cell-level
classification.

</details>


### [98] [Unifying Information-Theoretic and Pair-Counting Clustering Similarity](https://arxiv.org/abs/2511.03000)
*Alexander J. Gates*

Main category: stat.ML

TL;DR: 本文提出了一个统一框架，将聚类相似性度量的两个主要家族（配对计数和信息论）联系起来，揭示了它们作为观测与期望共现的加权展开之间的关系，并展示了信息论度量如何系统积累高阶共分配结构。


<details>
  <summary>Details</summary>
Motivation: 现有的聚类相似性度量会产生分歧甚至矛盾的评价结果，两个主要家族之间的深层分析联系尚未完全理解，需要建立一个统一的分析框架来阐明它们的关系。

Method: 开发了一个分析框架，通过两个互补视角：1) 将两个家族表示为观测与期望共现的加权展开；2) 将配对计数推广到k元组一致性，展示信息论度量如何积累高阶共分配结构。

Result: 成功统一了两个家族，揭示了配对计数是二次低阶近似，而信息论度量是高阶频率加权扩展，阐明了它们分歧的原因与权重和近似阶数直接相关。

Conclusion: 该框架为选择、解释和扩展聚类相似性度量提供了原则性基础，明确了两个度量家族何时以及为何会产生分歧。

Abstract: Comparing clusterings is central to evaluating unsupervised models, yet the
many existing similarity measures can produce widely divergent, sometimes
contradictory, evaluations. Clustering similarity measures are typically
organized into two principal families, pair-counting and information-theoretic,
reflecting whether they quantify agreement through element pairs or aggregate
information across full cluster contingency tables. Prior work has uncovered
parallels between these families and applied empirical normalization or
chance-correction schemes, but their deeper analytical connection remains only
partially understood. Here, we develop an analytical framework that unifies
these families through two complementary perspectives. First, both families are
expressed as weighted expansions of observed versus expected co-occurrences,
with pair-counting arising as a quadratic, low-order approximation and
information-theoretic measures as higher-order, frequency-weighted extensions.
Second, we generalize pair-counting to $k$-tuple agreement and show that
information-theoretic measures can be viewed as systematically accumulating
higher-order co-assignment structure beyond the pairwise level. We illustrate
the approaches analytically for the Rand index and Mutual Information, and show
how other indices in each family emerge as natural extensions. Together, these
views clarify when and why the two regimes diverge, relating their
sensitivities directly to weighting and approximation order, and provide a
principled basis for selecting, interpreting, and extending clustering
similarity measures across applications.

</details>


### [99] [Precise asymptotic analysis of Sobolev training for random feature models](https://arxiv.org/abs/2511.03050)
*Katharine E Fisher,Matthew TC Li,Youssef Marzouk,Timo Schorlepp*

Main category: stat.ML

TL;DR: 本文研究了Sobolev训练（同时使用函数和梯度数据进行回归）对高维过参数化随机特征模型泛化误差的影响，发现在某些情况下补充梯度数据并不能改善预测性能，过参数化程度应指导训练方法的选择。


<details>
  <summary>Details</summary>
Motivation: 梯度信息在实际应用中广泛可用且有用，但理论上对Sobolev训练在高度过参数化模型中的泛化误差影响知之甚少，需要系统研究这种训练方式的效果。

Method: 使用统计物理中的复本方法和算子值自由概率理论中的线性化方法，结合随机特征模型，在参数数量、输入维度和训练数据按比例趋于无穷大的极限下进行分析。

Result: 对于单指标模型描述的目标函数，补充梯度数据并不能普遍改善预测性能，过参数化程度应指导训练方法的选择。

Conclusion: 研究确定了模型在插值噪声函数和梯度数据时表现最优的设置，过参数化程度是选择训练方法的重要考虑因素。

Abstract: Gradient information is widely useful and available in applications, and is
therefore natural to include in the training of neural networks. Yet little is
known theoretically about the impact of Sobolev training -- regression with
both function and gradient data -- on the generalization error of highly
overparameterized predictive models in high dimensions. In this paper, we
obtain a precise characterization of this training modality for random feature
(RF) models in the limit where the number of trainable parameters, input
dimensions, and training data tend proportionally to infinity. Our model for
Sobolev training reflects practical implementations by sketching gradient data
onto finite dimensional subspaces. By combining the replica method from
statistical physics with linearizations in operator-valued free probability
theory, we derive a closed-form description for the generalization errors of
the trained RF models. For target functions described by single-index models,
we demonstrate that supplementing function data with additional gradient data
does not universally improve predictive performance. Rather, the degree of
overparameterization should inform the choice of training method. More broadly,
our results identify settings where models perform optimally by interpolating
noisy function and gradient data.

</details>


### [100] [Provable Accelerated Bayesian Optimization with Knowledge Transfer](https://arxiv.org/abs/2511.03125)
*Haitao Lin,Boxin Zhao,Mladen Kolar,Chong Liu*

Main category: stat.ML

TL;DR: DeltaBO算法通过构建源函数与目标函数之间的差异函数δ，在贝叶斯优化中实现知识迁移，显著降低目标任务的遗憾值。


<details>
  <summary>Details</summary>
Motivation: 现有的贝叶斯优化知识迁移方法要么缺乏理论保证，要么与非迁移设置达到相同的遗憾值，无法充分利用历史知识加速优化过程。

Method: 提出DeltaBO算法，基于源函数和目标函数之间的差异函数δ构建新的不确定性量化方法，允许源函数和目标函数属于不同的再生核希尔伯特空间。

Result: 理论证明DeltaBO的遗憾值为Õ(√T(T/N + γ_δ))，其中N为源任务评估次数，通常N≫T。在源任务与目标任务相似的情况下，γ_δ远小于γ_f。实证研究显示DeltaBO优于基线方法。

Conclusion: DeltaBO通过差异函数有效利用源任务知识，在理论和实证上都优于现有方法，为贝叶斯优化的知识迁移提供了有效解决方案。

Abstract: We study how Bayesian optimization (BO) can be accelerated on a target task
with historical knowledge transferred from related source tasks. Existing works
on BO with knowledge transfer either do not have theoretical guarantees or
achieve the same regret as BO in the non-transfer setting,
$\tilde{\mathcal{O}}(\sqrt{T \gamma_f})$, where $T$ is the number of
evaluations of the target function and $\gamma_f$ denotes its information gain.
In this paper, we propose the DeltaBO algorithm, in which a novel
uncertainty-quantification approach is built on the difference function
$\delta$ between the source and target functions, which are allowed to belong
to different reproducing kernel Hilbert spaces (RKHSs). Under mild assumptions,
we prove that the regret of DeltaBO is of order $\tilde{\mathcal{O}}(\sqrt{T
(T/N + \gamma_\delta)})$, where $N$ denotes the number of evaluations from
source tasks and typically $N \gg T$. In many applications, source and target
tasks are similar, which implies that $\gamma_\delta$ can be much smaller than
$\gamma_f$. Empirical studies on both real-world hyperparameter tuning tasks
and synthetic functions show that DeltaBO outperforms other baseline methods
and support our theoretical claims.

</details>


### [101] [Provable Separations between Memorization and Generalization in Diffusion Models](https://arxiv.org/abs/2511.03202)
*Zeqi Ye,Qijie Zhu,Molei Tao,Minshuo Chen*

Main category: stat.ML

TL;DR: 本文通过统计估计和网络逼近两个互补视角，建立了关于扩散模型记忆化的双重分离理论，并基于此开发了一种剪枝方法来减少记忆化同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然在各领域取得显著成功，但容易记忆训练数据而非生成新颖输出，这不仅限制了其创造性潜力，还引发隐私和安全担忧。现有研究主要关注经验性缓解策略，对记忆化的理论理解仍然有限。

Method: 从统计估计角度证明真实评分函数不最小化经验去噪损失，从网络逼近角度证明实现经验评分函数需要网络规模随样本量增长。基于这些理论洞察，开发了一种基于剪枝的方法来减少扩散变换器中的记忆化。

Result: 建立了记忆化的双重分离理论框架，并开发出有效的剪枝方法，能够在减少记忆化的同时维持生成质量。

Conclusion: 通过理论分析揭示了扩散模型记忆化的根本原因，并提供了基于剪枝的实用解决方案，为理解和缓解扩散模型的记忆化问题提供了新的理论视角和方法。

Abstract: Diffusion models have achieved remarkable success across diverse domains, but
they remain vulnerable to memorization -- reproducing training data rather than
generating novel outputs. This not only limits their creative potential but
also raises concerns about privacy and safety. While empirical studies have
explored mitigation strategies, theoretical understanding of memorization
remains limited. We address this gap through developing a dual-separation
result via two complementary perspectives: statistical estimation and network
approximation. From the estimation side, we show that the ground-truth score
function does not minimize the empirical denoising loss, creating a separation
that drives memorization. From the approximation side, we prove that
implementing the empirical score function requires network size to scale with
sample size, spelling a separation compared to the more compact network
representation of the ground-truth score function. Guided by these insights, we
develop a pruning-based method that reduces memorization while maintaining
generation quality in diffusion transformers.

</details>


### [102] [RKUM: An R Package for Robust Kernel Unsupervised Methods](https://arxiv.org/abs/2511.03216)
*Md Ashad Alam*

Main category: stat.ML

TL;DR: RKUM是一个R包，用于实现鲁棒的基于核的无监督方法，包括鲁棒核协方差算子、鲁棒核互协方差算子、鲁棒核典型相关分析和影响函数，能够有效处理污染或噪声数据。


<details>
  <summary>Details</summary>
Motivation: 传统基于核的方法使用二次损失函数，对污染数据敏感。需要开发鲁棒方法，在存在异常值或噪声的情况下仍能可靠分析。

Method: 使用广义损失函数替代传统二次损失来估计鲁棒核协方差算子和互协方差算子，并实现鲁棒核典型相关分析及其影响函数。

Result: 实验表明，标准核CCA的影响函数能有效识别异常值，而RKUM实现的鲁棒核方法对数据污染的敏感性显著降低。

Conclusion: RKUM为高维数据应用提供了一个高效且可扩展的鲁棒核分析平台。

Abstract: RKUM is an R package developed for implementing robust kernel-based
unsupervised methods. It provides functions for estimating the robust kernel
covariance operator (CO) and the robust kernel cross-covariance operator (CCO)
using generalized loss functions instead of the conventional quadratic loss.
These operators form the foundation of robust kernel learning and enable
reliable analysis under contaminated or noisy data conditions. The package
includes implementations of robust kernel canonical correlation analysis
(Kernel CCA), as well as the influence function (IF) for both standard and
multiple kernel CCA frameworks. The influence function quantifies sensitivity
and helps detect influential or outlying observations across two-view and
multi-view datasets. Experiments using synthesized two-view and multi-view data
demonstrate that the IF of the standard kernel CCA effectively identifies
outliers, while the robust kernel methods implemented in RKUM exhibit reduced
sensitivity to contamination. Overall, RKUM provides an efficient and
extensible platform for robust kernel-based analysis in high-dimensional data
applications.

</details>


### [103] [Vector-valued self-normalized concentration inequalities beyond sub-Gaussianity](https://arxiv.org/abs/2511.03606)
*Diego Martinez-Taboada,Tomas Gonzalez,Aaditya Ramdas*

Main category: stat.ML

TL;DR: 本文研究了向量值自归一化过程的浓度界限，特别是针对轻尾分布（如Bennett或Bernstein界限）的情况，并将结果应用于在线线性回归和核化线性赌博机问题。


<details>
  <summary>Details</summary>
Motivation: 虽然标量值自归一化过程的行为已被广泛研究，但向量值过程在亚高斯框架之外的研究相对不足，而这类过程在序列决策和计量经济学中具有重要应用。

Method: 提供了针对轻尾分布（超越亚高斯性）的自归一化过程的浓度界限，包括Bennett和Bernstein类型的界限。

Result: 获得了向量值自归一化过程的浓度界限，这些界限适用于更广泛的轻尾分布情况。

Conclusion: 研究结果在在线线性回归和核化线性赌博机等应用中具有重要价值，扩展了自归一化过程理论的应用范围。

Abstract: The study of self-normalized processes plays a crucial role in a wide range
of applications, from sequential decision-making to econometrics. While the
behavior of self-normalized concentration has been widely investigated for
scalar-valued processes, vector-valued processes remain comparatively
underexplored, especially outside of the sub-Gaussian framework. In this
contribution, we provide concentration bounds for self-normalized processes
with light tails beyond sub-Gaussianity (such as Bennett or Bernstein bounds).
We illustrate the relevance of our results in the context of online linear
regression, with applications in (kernelized) linear bandits.

</details>


### [104] [Colorectal Cancer Histopathological Grading using Multi-Scale Federated Learning](https://arxiv.org/abs/2511.03693)
*Md Ahasanul Arafath,Abhijit Kumar Ghosh,Md Rony Ahmed,Sabrin Afroz,Minhazul Hosen,Md Hasan Moon,Md Tanzim Reza,Md Ashad Alam*

Main category: stat.ML

TL;DR: 提出了一种用于结直肠癌组织病理学分级的可扩展隐私保护联邦学习框架，通过多尺度特征学习和分布式训练，在保护患者隐私的同时提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌分级存在观察者间差异性和多机构数据共享的隐私限制问题，集中式训练模型违反数据治理法规且忽略了多尺度分析的重要性。

Method: 采用双流ResNetRS50骨干网络同时捕获细粒度核细节和更广泛的组织级上下文，集成到使用FedProx稳定的联邦学习系统中，以减轻异构数据分布下的客户端漂移。

Result: 在CRC-HGD数据集上达到83.5%的总体准确率，优于集中式模型(81.6%)，对最具侵袭性的III级肿瘤召回率达到87.5%，在40倍放大下准确率提升至88.0%。

Conclusion: 该联邦多尺度方法不仅保护患者隐私，还增强了模型性能和泛化能力，为可部署的隐私感知临床AI建立了基础。

Abstract: Colorectal cancer (CRC) grading is a critical prognostic factor but remains
hampered by inter-observer variability and the privacy constraints of
multi-institutional data sharing. While deep learning offers a path to
automation, centralized training models conflict with data governance
regulations and neglect the diagnostic importance of multi-scale analysis. In
this work, we propose a scalable, privacy-preserving federated learning (FL)
framework for CRC histopathological grading that integrates multi-scale feature
learning within a distributed training paradigm. Our approach employs a
dual-stream ResNetRS50 backbone to concurrently capture fine-grained nuclear
detail and broader tissue-level context. This architecture is integrated into a
robust FL system stabilized using FedProx to mitigate client drift across
heterogeneous data distributions from multiple hospitals. Extensive evaluation
on the CRC-HGD dataset demonstrates that our framework achieves an overall
accuracy of 83.5%, outperforming a comparable centralized model (81.6%).
Crucially, the system excels in identifying the most aggressive Grade III
tumors with a high recall of 87.5%, a key clinical priority to prevent
dangerous false negatives. Performance further improves with higher
magnification, reaching 88.0% accuracy at 40x. These results validate that our
federated multi-scale approach not only preserves patient privacy but also
enhances model performance and generalization. The proposed modular pipeline,
with built-in preprocessing, checkpointing, and error handling, establishes a
foundational step toward deployable, privacy-aware clinical AI for digital
pathology.

</details>
