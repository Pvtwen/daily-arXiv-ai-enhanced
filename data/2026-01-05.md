<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 13]
- [cs.LG](#cs.LG) [Total: 63]
- [stat.ML](#stat.ML) [Total: 3]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Neural Brain Fields: A NeRF-Inspired Approach for Generating Nonexistent EEG Electrodes](https://arxiv.org/abs/2601.00012)
*Shahar Ain Kedem,Itamar Zimerman,Eliya Nachmani*

Main category: eess.SP

TL;DR: 提出一种受NeRF启发的EEG信号处理方法，将脑电信号表示为连续神经场，实现任意时空分辨率的信号重建与电极数据模拟


<details>
  <summary>Details</summary>
Motivation: EEG数据具有长度可变、信噪比低、个体差异大、时间漂移等挑战，传统深度学习方法难以有效处理，需要新的建模方法

Method: 借鉴NeRF思想，将EEG电极视为不同空间位置的采样点，训练神经网络学习连续神经活动场，生成固定大小的权重向量编码整个信号

Result: 方法能够实现脑电信号的连续可视化（包括超高分辨率）、原始信号重建，并能有效模拟不存在电极的数据，提升下游任务性能

Conclusion: NeRF启发的EEG建模方法为解决脑电信号处理中的挑战提供了新思路，实现了连续时空表示和信号增强

Abstract: Electroencephalography (EEG) data present unique modeling challenges because recordings vary in length, exhibit very low signal to noise ratios, differ significantly across participants, drift over time within sessions, and are rarely available in large and clean datasets. Consequently, developing deep learning methods that can effectively process EEG signals remains an open and important research problem. To tackle this problem, this work presents a new method inspired by Neural Radiance Fields (NeRF). In computer vision, NeRF techniques train a neural network to memorize the appearance of a 3D scene and then uses its learned parameters to render and edit the scene from any viewpoint. We draw an analogy between the discrete images captured from different viewpoints used to learn a continuous 3D scene in NeRF, and EEG electrodes positioned at different locations on the scalp, which are used to infer the underlying representation of continuous neural activity. Building on this connection, we show that a neural network can be trained on a single EEG sample in a NeRF style manner to produce a fixed size and informative weight vector that encodes the entire signal. Moreover, via this representation we can render the EEG signal at previously unseen time steps and spatial electrode positions. We demonstrate that this approach enables continuous visualization of brain activity at any desired resolution, including ultra high resolution, and reconstruction of raw EEG signals. Finally, our empirical analysis shows that this method can effectively simulate nonexistent electrodes data in EEG recordings, allowing the reconstructed signal to be fed into standard EEG processing networks to improve performance.

</details>


### [2] [Modeling Day-Long ECG Signals to Predict Heart Failure Risk with Explainable AI](https://arxiv.org/abs/2601.00014)
*Eran Zvuloni,Ronit Almog,Michael Glikson,Shany Brimer Biton,Ilan Green,Izhar Laufer,Offer Amir,Joachim A. Behar*

Main category: eess.SP

TL;DR: 深度学习模型DeepHHF利用24小时单导联心电图数据预测5年内心衰风险，性能优于30秒心电图片段和临床评分，高风险人群住院或死亡风险增加两倍。


<details>
  <summary>Details</summary>
Motivation: 心衰影响11.8%的65岁以上成年人，降低生活质量和寿命。预防心衰可减少发病率和死亡率。需要非侵入性、廉价且广泛可及的心衰风险预测工具。

Method: 使用Technion-Leumit Holter ECG数据集（69,663条记录，47,729名患者，20年数据）。开发深度学习模型DeepHHF，训练于24小时单导联心电图数据，并与30秒片段模型和临床评分进行比较。

Result: DeepHHF的AUC达到0.80，优于30秒心电图模型和临床评分。高风险人群住院或死亡风险增加两倍。可解释性分析显示模型关注心律失常和心脏异常，关键注意力集中在上午8点至下午3点之间。

Conclusion: 深度学习能够有效建模24小时连续心电图数据，捕捉阵发性事件和昼夜节律变化，这对于可靠的风险预测至关重要。基于单导联Holter心电图的人工智能方法非侵入性、廉价且广泛可及，是心衰风险预测的有前景工具。

Abstract: Heart failure (HF) affects 11.8% of adults aged 65 and older, reducing quality of life and longevity. Preventing HF can reduce morbidity and mortality. We hypothesized that artificial intelligence (AI) applied to 24-hour single-lead electrocardiogram (ECG) data could predict the risk of HF within five years. To research this, the Technion-Leumit Holter ECG (TLHE) dataset, including 69,663 recordings from 47,729 patients, collected over 20 years was used. Our deep learning model, DeepHHF, trained on 24-hour ECG recordings, achieved an area under the receiver operating characteristic curve of 0.80 that outperformed a model using 30-second segments and a clinical score. High-risk individuals identified by DeepHHF had a two-fold chance of hospitalization or death incidents. Explainability analysis showed DeepHHF focused on arrhythmias and heart abnormalities, with key attention between 8 AM and 3 PM. This study highlights the feasibility of deep learning to model 24-hour continuous ECG data, capturing paroxysmal events and circadian variations essential for reliable risk prediction. Artificial intelligence applied to single-lead Holter ECG is non-invasive, inexpensive, and widely accessible, making it a promising tool for HF risk prediction.

</details>


### [3] [Adaptive Pinching Antenna Optimization via Meta-Learning for Physical-Layer Security in Dynamic Wireless Networks](https://arxiv.org/abs/2601.00115)
*Khalid T. Musri,Akram Y. Sarhan,Osamah A. Abdullah,Hayder Al-Hraishawi*

Main category: eess.SP

TL;DR: 提出基于梯度元学习的实时控制框架，用于波导夹持天线系统在用户位置不确定和物理层安全约束下的优化


<details>
  <summary>Details</summary>
Motivation: 在动态无线环境中，用户位置不确定性和物理层安全需求对波导夹持天线系统的实时控制提出了挑战，需要快速适应不同移动和信道条件

Method: 采用模型无关元学习（MAML）框架，学习可迁移的初始化参数，通过少量在线导频反馈实现快速适应，联合优化天线位置和发射功率以满足概率可靠性和安全性要求

Result: 仿真结果表明，该框架在中断概率、安全性能和收敛延迟方面显著优于Reptile元学习、非元强化学习、传统优化、静态天线放置和仅功率控制等方法

Conclusion: 元学习成为非平稳无线环境中可重构夹持天线系统安全低延迟控制的有效工具

Abstract: This paper develops a gradient-based meta-learning framework for real-time control of waveguided pinching-antenna systems under user-location uncertainty and physical-layer security (PLS) constraints. A probabilistic system model is introduced to capture the impact of imperfect localization on outage performance and secrecy. Based on this model, a joint antenna-positioning and transmit-power optimization problem is formulated to satisfy probabilistic reliability and secrecy requirements. To enable rapid adaptation in highly dynamic environments, the proposed approach employs model-agnostic meta-learning (MAML) to learn a transferable initialization across diverse mobility and channel conditions, allowing few-shot online adaptation using limited pilot feedback. Simulation results demonstrate that the proposed framework significantly outperforms Reptile-based meta-learning, non-meta reinforcement learning, conventional optimization, static antenna placement, and power-only control in terms of outage probability, secrecy performance, and convergence latency. These results establish meta-learning as an effective tool for secure and low-latency control of reconfigurable pinching-antenna systems in non-stationary wireless environments.

</details>


### [4] [AI-Driven Channel State Information (CSI) Extrapolation for 6G: Current Situations, Challenges and Future Research](https://arxiv.org/abs/2601.00159)
*Yuan Gao,Zichen Lu,Xinyi Wu,Wenjun Yu,Shengli Liu,Jianbo Du,Yanliang Jin,Shunqing Zhang,Xiaoli Chu,Shugong Xu*

Main category: eess.SP

TL;DR: 本文首次全面综述了6G通信系统中CSI外推技术的现状、挑战和未来方向，包括性能指标分析、模型驱动和AI驱动方法、数据集资源以及研究机遇。


<details>
  <summary>Details</summary>
Motivation: 传统信道估计方法在6G高移动性、超大规模MIMO和多频段系统中面临可扩展性挑战，CSI外推技术通过部分CSI推断完整CSI来显著降低开销，但缺乏全面的技术综述。

Method: 1) 分析6G中CSI外推的特定性能指标；2) 综述时间和AI驱动的时域、频域、天线和多域CSI外推方法；3) 审查可用于训练AI模型的开放数据集和仿真器；4) 讨论现有研究的关键挑战并提出未来研究方向。

Result: 提供了CSI外推技术的全面综述，总结了各种方法的见解和要点，强调了AI驱动方法在满足性能要求方面的潜力，并识别了关键挑战和研究机遇。

Conclusion: 本文填补了CSI外推技术全面综述的空白，为6G通信系统优化提供了重要参考，指出了AI驱动方法的发展方向和研究挑战。

Abstract: CSI extrapolation is an effective method for acquiring channel state information (CSI), essential for optimizing performance of sixth-generation (6G) communication systems. Traditional channel estimation methods face scalability challenges due to the surging overhead in emerging high-mobility, extremely large-scale multiple-input multiple-output (EL-MIMO), and multi-band systems. CSI extrapolation techniques mitigate these challenges by using partial CSI to infer complete CSI, significantly reducing overhead. Despite growing interest, a comprehensive review of state-of-the-art (SOTA) CSI extrapolation techniques is lacking. This paper addresses this gap by comprehensively reviewing the current status, challenges, and future directions of CSI extrapolation for the first time. Firstly, we analyze the performance metrics specific to CSI extrapolation in 6G, including extrapolation accuracy, adaption to dynamic scenarios and algorithm costs. We then review both model-driven and artificial intelligence (AI)-driven approaches for time, frequency, antenna, and multi-domain CSI extrapolation. Key insights and takeaways from these methods are summarized. Given the promise of AI-driven methods in meeting performance requirements, we also examine the open-source channel datasets and simulators that could be used to train high-performance AI-driven CSI extrapolation models. Finally, we discuss the critical challenges of the existing research and propose perspective research opportunities.

</details>


### [5] [Edge AI Inference in ISCC Networks: Sensing Accuracy Analysis and Precoding Design](https://arxiv.org/abs/2601.00171)
*Lingyun Xu,Bowen Wang,Huiyong Li,Ziyang Cheng*

Main category: eess.SP

TL;DR: 该论文研究了ISCC网络中边缘AI推理的感知精度与预编码系数之间的关系，提出了判别增益(DG)来表征感知精度，并开发了有效的预编码算法来最大化DG。


<details>
  <summary>Details</summary>
Motivation: 在集成感知、通信和计算(ISCC)网络中，边缘AI推理需要同时考虑感知精度和通信效率。目前缺乏对感知精度与预编码系数之间关系的深入理解，这限制了边缘推理的性能优化。

Method: 1) 构建了基于空中计算(over-the-air)的ISCC网络系统模型，包含分布式边缘传感器进行特征提取和边缘服务器进行分类；2) 提出判别增益(DG)来量化感知精度；3) 推导了DG关于预编码系数的显式函数；4) 设计了有效的预编码算法来解决非凸的DG最大化问题。

Result: 仿真结果验证了所提设计在ISCC网络中边缘推理的有效性和可行性。推导的DG函数为预编码设计提供了有价值的理论指导，提出的算法能够有效解决非凸优化问题。

Conclusion: 该工作揭示了ISCC网络中感知精度与预编码系数之间的内在关系，提出的DG度量和预编码算法为边缘AI推理的性能优化提供了有效解决方案，推动了ISCC网络在实际应用中的发展。

Abstract: This work explores the relationship between sensing accuracy and precoding coefficients for edge artificial intelligence (AI) inference in integrated sensing, communication and computation (ISCC) networks. We start by constructing a system model of an over-the-air-empowered ISCC network for edge AI inference, involving distributed edge sensors for feature extraction and an edge server for classification. Based on this model, we introduce a discriminant gain (DG) to characterize sensing accuracy and novelly derive an explicit function of the DG about precoding coefficients, giving valuable insights into precoding design. Guided by this, we propose an effective precoding algorithm to solve a non-convex DG-maximization problem. Simulation results verify the effectiveness and feasibility of the proposed design for edge inference in ISCC networks.

</details>


### [6] [Time--to--Digital Converter (TDC)--Based Resonant Compute--in--Memory for INT8 CNNs with Layer--Optimized SRAM Mapping](https://arxiv.org/abs/2601.00434)
*Dhandeep Challagundla,Ignatius Bezzam,Riadul Islam*

Main category: eess.SP

TL;DR: 提出一种谐振时域存内计算架构，用时间数字转换器替代传统ADC，降低功耗和面积，实现高效神经网络加速


<details>
  <summary>Details</summary>
Motivation: 传统存内计算架构使用模拟数字转换器进行MAC操作数字化，但ADC带来面积和功耗显著增加以及非线性问题，需要更高效的数字化方案

Method: 提出谐振时域存内计算架构，采用8T SRAM单元实现可靠位级MAC操作，使用4位时间数字转换器配合脉冲收缩延迟元件进行数字化，结合权重静态数据映射策略和自动SRAM宏选择算法

Result: 在TSMC 28nm工艺上验证8KB SRAM阵列，实现320GOPS吞吐量和38.46TOPS/W能效，在六个CNN模型中推理能耗降低达8倍，量化后精度损失最小

Conclusion: 提出的TDC-CiM架构通过消除ADC需求，使用TDC数字化MAC结果，在降低功耗和面积的同时实现了高效可扩展的神经网络加速

Abstract: In recent years, Compute-in-memory (CiM) architectures have emerged as a promising solution for deep neural network (NN) accelerators. Multiply-accumulate~(MAC) is considered a {\textit de facto} unit operation in NNs. By leveraging the inherent parallel processing capabilities of CiM, NNs that require numerous MAC operations can be executed more efficiently. This is further facilitated by storing the weights in SRAM, reducing the need for extensive data movement and enhancing overall computational speed and efficiency. Traditional CiM architectures execute MAC operations in the analog domain, employing an Analog-to-Digital converter (ADC) to convert the analog MAC values into digital outputs. However, these ADCs introduce significant increase in area and power consumption, as well as introduce non-linearities. This work proposes a resonant time-domain compute-in-memory (TDC-CiM) architecture that eliminates the need for an ADC by using a time-to-digital converter (TDC) to digitize analog MAC results with lower power and area cost. A dedicated 8T SRAM cell enables reliable bitwise MAC operations, while the readout uses a 4-bit TDC with pulse-shrinking delay elements, achieving 1 GS/s sampling with a power consumption of only 1.25 mW. In addition, a weight stationary data mapping strategy combined with an automated SRAM macro selection algorithm enables scalable and energy-efficient deployment across CNN workloads. Evaluation across six CNN models shows that the algorithm reduces inference energy consumption by up to 8x when scaling SRAM size from 32~KB to 256~KB, while maintaining minimal accuracy loss after quantization. The feasibility of the proposed architecture is validated on an 8~KB SRAM memory array using TSMC 28~nm technology. The proposed TDC-CiM architecture demonstrates a throughput of 320~GOPS with an energy efficiency of 38.46~TOPS/W.

</details>


### [7] [MIMO-AFDM Outperforms MIMO-OFDM in the Face of Hardware Impairments](https://arxiv.org/abs/2601.00502)
*Zeping Sui,Zilong Liu,Leila Musavian,Yong Liang Guan,Lie-Liang Yang,Lajos Hanzo*

Main category: eess.SP

TL;DR: 研究硬件损伤对MIMO-AFDM系统的影响，发现AFDM在硬件损伤下仍能保持全分集阶数，且比OFDM对乘性失真更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究硬件损伤（包括乘性和加性）对MIMO-AFDM系统性能的影响，评估其在现实硬件不完美条件下的表现。

Method: 针对小规模MIMO-AFDM系统推导ML检测器的紧密BER上界；针对大规模系统推导LMMSE检测器的闭式BER近似，包括不完美信道估计场景。

Result: 1) 硬件损伤下AFDM系统仍能保持全分集阶数；2) 理论BER结果与仿真匹配；3) MIMO-AFDM比MIMO-OFDM对乘性失真更具鲁棒性；4) 在相同加性硬件损伤条件下，MIMO-AFDM始终优于MIMO-OFDM。

Conclusion: AFDM凭借其固有的啁啾信号特性和离散仿射傅里叶变换的有利扩展效应，比OFDM具有更强的ICI鲁棒性，即使在硬件损伤下也能实现最大全分集增益。

Abstract: The impact of both multiplicative and additive hardware impairments (HWIs) on multiple-input multiple-output affine frequency division multiplexing (MIMO-AFDM) systems is investigated. For small-scale MIMO-AFDM systems, a tight bit error rate (BER) upper bound associated with the maximum likelihood (ML) detector is derived. By contrast, for large-scale systems, a closed-form BER approximation associated with the linear minimum mean squared error (LMMSE) detector is presented, including realistic imperfect channel estimation scenarios. Our first key observation is that the full diversity order of a hardware-impaired AFDM system remains unaffected, which is a unique advantage. Furthermore, our analysis shows that 1) the BER results derived accurately predict the simulated ML performance in moderate-to-high signal-to-noise ratios (SNRs), while the theoretical BER curve of the LMMSE detector closely matches that of the Monte-Carlo based one. 2) MIMO-AFDM is more resilient to multiplicative distortions, such as phase noise and carrier frequency offset, compared to its orthogonal frequency division multiplexing (OFDM) counterparts. This is attributed to its inherent chirp signal characteristics; 3) MIMO-AFDM consistently achieves superior BER performance compared to conventional MIMO-OFDM systems under the same additive HWI conditions, as well as different velocity values. The latter is because MIMO-AFDM is also resilient to the additional inter-carrier interference (ICI) imposed by the nonlinear distortions of additive HWIs. In a nutshell, compared to OFDM, AFDM demonstrates stronger ICI resilience and achieves the maximum full diversity attainable gain even under HWIs, thanks to its intrinsic chirp signalling structure as well as to the beneficial spreading effect of the discrete affine Fourier transform.

</details>


### [8] [Parametrized Sharing for Multi-Agent Hybrid DRL for Multiple Multi-Functional RISs-Aided Downlink NOMA Networks](https://arxiv.org/abs/2601.00538)
*Chi-Te Kuo,Li-Hsiang Shen,Jyun-Jhe Huang*

Main category: eess.SP

TL;DR: 提出一种参数共享的多智能体混合深度强化学习（PMHRL）方法，优化多MF-RIS辅助的NOMA下行网络，实现最高能效


<details>
  <summary>Details</summary>
Motivation: 多功能可重构智能表面（MF-RIS）结合了主动RIS的信号覆盖扩展能力和能量收集的自持特性，但如何优化多MF-RIS配置以最大化NOMA下行网络的能效仍是一个挑战

Method: 提出参数共享的多智能体混合深度强化学习（PMHRL）框架，使用多智能体PPO处理连续变量（功率分配、波束赋形、MF-RIS幅度/相移/EH比率），使用DQN处理离散变量（MF-RIS位置），联合优化所有参数

Result: PMHRL相比无参数共享、纯PPO和纯DQN等方法获得最高能效；多MF-RIS辅助的NOMA下行网络相比无EH/放大、传统RIS和无RIS/MF-RIS部署在不同多址方案下都实现最高能效

Conclusion: 提出的PMHRL方法能有效优化多MF-RIS辅助的NOMA网络，参数共享机制提升了学习效率，多MF-RIS架构显著提高了系统能效

Abstract: Multi-functional reconfigurable intelligent surface (MF-RIS) is conceived to address the communication efficiency thanks to its extended signal coverage from its active RIS capability and self-sustainability from energy harvesting (EH). We investigate the architecture of multi-MF-RISs to assist non-orthogonal multiple access (NOMA) downlink networks. We formulate an energy efficiency (EE) maximization problem by optimizing power allocation, transmit beamforming and MF-RIS configurations of amplitudes, phase-shifts and EH ratios, as well as the position of MF-RISs, while satisfying constraints of available power, user rate requirements, and self-sustainability property. We design a parametrized sharing scheme for multi-agent hybrid deep reinforcement learning (PMHRL), where the multi-agent proximal policy optimization (PPO) and deep-Q network (DQN) handle continuous and discrete variables, respectively. The simulation results have demonstrated that proposed PMHRL has the highest EE compared to other benchmarks, including cases without parametrized sharing, pure PPO and DQN. Moreover, the proposed multi-MF-RISs-aided downlink NOMA achieves the highest EE compared to scenarios of no-EH/amplification, traditional RISs, and deployment without RISs/MF-RISs under different multiple access.

</details>


### [9] [Fractional Programming for Kullback-Leibler Divergence in Hypothesis Testing](https://arxiv.org/abs/2601.00564)
*Jeongwoo Park,Seongkyu Jung,Kaiming Shen,Jeonghun Park*

Main category: eess.SP

TL;DR: 提出基于分数规划的计算高效优化框架，用于最大化Kullback-Leibler散度，显著降低计算复杂度并加速收敛


<details>
  <summary>Details</summary>
Motivation: KLD最大化在主动感知和假设检验中至关重要，但现有方法计算复杂度高，需要每次迭代进行矩阵求逆，限制了实际应用

Method: 1) 使用矩阵分数规划将KLD最大化问题转化为一系列可处理的二次子问题；2) 引入非齐次松弛技术，用闭式更新替代线性系统求解器；3) 采用STEM加速方法，将迭代方案解释为定点映射

Result: 算法将每次迭代复杂度降至二次阶，总运行时间比最先进基准方法减少数个数量级，在多个随机接入和联合集成感知与通信场景中验证了有效性

Conclusion: 提出的基于分数规划的优化框架为KLD最大化提供了计算高效的解决方案，在保持性能的同时显著降低了计算复杂度，适用于实际应用场景

Abstract: Maximizing the Kullback-Leibler divergence (KLD) is a fundamental problem in waveform design for active sensing and hypothesis testing, as it directly relates to the error exponent of detection probability. However, the associated optimization problem is highly nonconvex due to the intricate coupling of log-determinant and matrix trace terms. Existing solutions often suffer from high computational complexity, typically requiring matrix inversion at every iteration. In this paper, we propose a computationally efficient optimization framework based on fractional programming (FP). Our key idea is to reformulate the KLD maximization problem into a sequence of tractable quadratic subproblems using matrix FP. To further reduce complexity, we introduce a nonhomogeneous relaxation technique that replaces the costly linear system solver with a simple closed-form update, thereby reducing the per-iteration complexity to quadratic order. To compensate for the convergence speed trade-off caused by relaxation, we employ an acceleration method called STEM by interpreting the iterative scheme as a fixed-point mapping. The resulting algorithm achieves significantly faster convergence rates with low per-iteration cost. Numerical results demonstrate that our approach reduces the total runtime by orders of magnitude compared to a state-of-the-art benchmark. Finally, we apply the proposed framework to a multiple random access scenario and a joint integrated sensing and communication scenario, validating the efficacy of our framework in such applications.

</details>


### [10] [WiFo-MUD: Wireless Foundation Model for Heterogeneous Multi-User Demodulator](https://arxiv.org/abs/2601.00612)
*Zonghui Yang,Shijian Gao,Xuesong Cai,Xiang Cheng,Liuqing Yang*

Main category: eess.SP

TL;DR: 提出WiFo-MUD，一种基于扩散模型的通用多用户解调基础模型，通过条件去噪、通信感知一致性蒸馏和动态用户分组策略，在异构配置下实现高效推理和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有多用户解调器在通用环境下表现不佳：经典解调器难以平衡精度和复杂度，基于深度学习的方法在异构配置下缺乏适应性，而扩散模型在实际应用中的灵活性有限。

Method: 提出WiFo-MUD模型，包含：1) 对齐用户间信噪比不平衡；2) 通过定制化骨干网络进行条件去噪；3) 通信感知一致性蒸馏方法；4) 动态用户分组策略以增强推理。

Result: 在大规模异构数据集上取得最先进结果，展示了高效推理能力，并在不同系统配置下表现出强大的泛化性能。

Conclusion: WiFo-MUD作为一种通用的扩散基础模型，有效解决了多用户解调中的精度-复杂度权衡和异构配置适应性问题，为无线通信提供了可靠高效的解调解决方案。

Abstract: Multi-user signal demodulation is critical to wireless communications, directly impacting transmission reliability and efficiency. However, existing demodulators underperform in generic multi-user environments: classical demodulators struggle to balance accuracy and complexity, while deep learning-based methods lack adaptability under heterogeneous configurations. Although diffusion models have been introduced for demodulation, their flexibility remains limited for practical use. To address these issues, this work proposes WiFo-MUD, a universal diffusion-based foundation model for multi-user demodulation. The model aligns inter-user signal-to-noise ratio imbalance and performs conditional denoising via a customized backbone. Furthermore, a communication-aware consistency distillation method and a dynamic user-grouping strategy are devised to enhance inference. WiFo-MUD achieves state-of-the-art results on large-scale heterogeneous datasets, demonstrating efficient inference and strong generalization across varying system configurations.

</details>


### [11] [Splitting Precoding with Subspace Selection and Quantized Refinement for Massive MIMO](https://arxiv.org/abs/2601.00616)
*Yasaman Khorsandmanesh,Emil Bjornson,Joakim Jalden*

Main category: eess.SP

TL;DR: 提出一种分割预编码架构，将预编码设计分离到AAS和BBU之间，通过本地子空间选择降低信道维度，提高大规模MIMO系统在有限前传容量下的频谱效率


<details>
  <summary>Details</summary>
Motivation: 大规模MIMO 5G架构中有限的前传容量是一个实际瓶颈。传统下行链路设计将所有预编码计算放在BBU，并通过前传传输高维预编码矩阵，导致显著的量化损失和信令开销

Method: 提出分割预编码架构：AAS执行本地子空间选择以降低信道维度；BBU基于得到的有效信道计算优化的量化细化预编码

Result: 数值结果表明，所提出的分割预编码策略比传统单阶段预编码实现了更高的总频谱效率

Conclusion: 通过将预编码设计分割到AAS和BBU之间，有效解决了有限前传容量下的量化损失问题，提高了大规模MIMO系统的性能

Abstract: Limited fronthaul capacity is a practical bottleneck in massive multiple-input multiple-output (MIMO) 5G architectures, where a base station (BS) consists of an advanced antenna system (AAS) connected to a baseband unit (BBU). Conventional downlink designs place the entire precoding computation at the BBU and transmit a high-dimensional precoding matrix over the fronthaul, resulting in substantial quantization losses and signaling overhead. This letter proposes a splitting precoding architecture that separates the design between the AAS and BBU. The AAS performs a local subspace selection to reduce the channel dimensionality, while the BBU computes an optimized quantized refinement precoding based on the resulting effective channel. The numerical results show that the proposed splitting precoding strategy achieves higher sum spectral efficiency than conventional one-stage precoding.

</details>


### [12] [Conformal Reconfigurable Intelligent Surfaces: A Cylindrical Geometry Perspective](https://arxiv.org/abs/2601.00734)
*Filippo Pepe,Ivan Iudice,Giuseppe Castaldi,Marco Di Renzo,Vincenzo Galdi*

Main category: eess.SP

TL;DR: 圆柱形可重构智能表面（RIS）从理想表面阻抗合成到基于简单1比特超原子的实际实现，通过解析模型和几何光学方法探索设计极限，验证了1比特RIS可实现定向散射且硬件复杂度低。


<details>
  <summary>Details</summary>
Motivation: 曲面RIS是下一代无线通信的有前景方向，可在无人机和城市基础设施等非平面平台上实现自适应波前控制。本文旨在系统研究圆柱形RIS，探索其在实际无线通信场景中的可行性。

Method: 首先开发精确解析和几何光学模型探索基本设计极限，然后提出适用于离散可重构架构的半解析公式。使用进化优化和低复杂度策略（如最小功率无失真响应方法）进行高效波束合成，并通过全波仿真验证。

Result: 验证了1比特RIS能够实现定向散射，具有可管理的旁瓣水平和最小的硬件复杂度。结果证实了圆柱形RIS的可行性。

Conclusion: 圆柱形RIS是可行的，为将曲面RIS集成到实际无线通信平台中的双用途应用打开了大门。

Abstract: Curved reconfigurable intelligent surfaces (RISs) represent a promising frontier for next-generation wireless communication, enabling adaptive wavefront control on nonplanar platforms such as unmanned aerial vehicles and urban infrastructure. This work presents a systematic investigation of cylindrical RISs, progressing from idealized surface-impedance synthesis to practical implementations based on simple one-bit meta-atoms. Exact analytical and geometrical-optics-based models are first developed to explore fundamental design limits, followed by a semi-analytical formulation tailored to discrete, reconfigurable architectures. This model enables efficient beam synthesis using both evolutionary optimization and low-complexity strategies, including the minimum power distortionless response method, and is validated through full-wave simulations. Results confirm that one-bit RISs can achieve directive scattering with manageable sidelobe levels and minimal hardware complexity. These findings establish the viability of cylindrical RISs and open the door to their integration into dual-use wireless platforms for real-world communication scenarios.

</details>


### [13] [Energy Efficiency Maximization of MIMO Systems through Reconfigurable Holographic Beamforming](https://arxiv.org/abs/2601.00780)
*Robert Kuku Fotock,Alessio Zappone,Agbotiname Lucky Imoize,Marco Di Renzo*

Main category: eess.SP

TL;DR: 该研究提出了一种在MIMO系统中使用可重构超表面实现全息波束赋形的方法，通过优化发射协方差矩阵和超表面反射矩阵来最大化系统能效。


<details>
  <summary>Details</summary>
Motivation: 传统全数字波束赋形架构虽然能实现显著的多路复用增益，但能耗较高。研究旨在探索在发射和接收端部署可重构超表面实现全息波束赋形，以在保持性能的同时提高系统能效。

Method: 在点对点无线链路中，发射和接收端均配备多天线阵列，并在天线阵列附近部署两个可重构超表面。开发了低复杂度算法优化发射协方差矩阵和两个超表面的反射矩阵，确保收敛到能效最大化问题的一阶最优点。针对单天线或单流传输的特殊情况，推导了超表面矩阵的闭式表达式。

Result: 数值性能分析表明，与全数字波束赋形架构相比，使用超表面实现的全息波束赋形能提供显著的能效增益，即使后者实现了可观的多路复用增益。

Conclusion: 在MIMO系统中部署可重构超表面实现全息波束赋形是一种有效的能效优化方案，相比传统全数字架构具有显著优势，为未来无线通信系统的能效设计提供了新思路。

Abstract: This study considers a point-to-point wireless link, in which both the transmitter and receiver are equipped with multiple antennas. In addition, two reconfigurable metasurfaces are deployed, one in the immediate vicinity of the transmit antenna array, and one in the immediate vicinity of the receive antenna array. The resulting architecture implements a holographic beamforming structure at both the transmitter and receiver. In this scenario, the system energy efficiency is optimized with respect to the transmit covariance matrix, and the reflection matrices of the two metasurfaces. A low-complexity algorithm is developed, which is guaranteed to converge to a first-order optimal point of the energy efficiency maximization problem. Moreover, closed-form expressions are derived for the metasurface matrices in the special case of single-antenna or single-stream transmission. The two metasurfaces are considered to be nearly-passive and subject to global reflection constraints. A numerical performance analysis is conducted to assess the performance of the proposed optimization methods, showing, in particular, that the use of holographic beamforming by metasurfaces can provide significant energy efficiency gains compared to fully digital beamforming architectures, even when the latter achieve substantial multiplexing gains.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [14] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

TL;DR: 提出基于最优传输的联邦逆强化学习方法，通过Wasserstein重心融合异构智能体的本地奖励函数，实现通信高效的跨环境共享奖励学习


<details>
  <summary>Details</summary>
Motivation: 在多智能体系统中，异构智能体在相似但不同的环境中运行，直接共享数据学习共同奖励函数不现实，因为存在动态差异、隐私限制和通信带宽限制

Method: 1) 每个客户端在本地执行轻量级最大熵逆强化学习；2) 通过Wasserstein重心融合本地奖励函数，考虑其底层几何结构；3) 证明该方法比传统联邦学习参数平均方法能获得更准确的全局奖励估计

Result: 提出了一种原则性且通信高效的框架，能够推导出在异构智能体和环境中具有良好泛化能力的共享奖励函数

Conclusion: 基于最优传输的联邦逆强化学习方法为异构多智能体系统提供了一种有效的共享奖励学习方案，克服了传统方法的局限性

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [15] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: ARISE是一个轻量级强化学习框架，通过添加基于粒子群的探索层来增强标准策略梯度方法，在复杂任务和非平稳奖励环境中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的有效探索仍然是一个关键挑战，特别是在非平稳奖励或高维策略的情况下。现有方法在复杂环境中探索不足，需要更有效的探索机制。

Method: ARISE框架在标准策略梯度方法基础上增加了一个紧凑的基于粒子群的探索层。它将策略动作与粒子驱动的提议混合，每个粒子代表在动作空间中采样的候选策略轨迹，并使用奖励方差线索自适应地调节探索。

Result: 在简单基准测试中只有轻微改进（如CartPole-v1上+0.7%），但在更具挑战性的任务中取得显著提升：LunarLander-v3上+46%，Hopper-v4上+22%，同时在Walker2d和Ant上保持稳定性。在非平稳奖励变化下，ARISE提供明显的鲁棒性优势，在CartPole上比PPO高出+75分，在LunarLander上也有相应改进。

Conclusion: ARISE提供了一个简单、架构无关的途径，可以在不改变核心算法结构的情况下，创建更具探索性和鲁棒性的强化学习智能体。消融研究证实粒子群组件和自适应机制都对性能有贡献。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [16] [Stochastic Actor-Critic: Mitigating Overestimation via Temporal Aleatoric Uncertainty](https://arxiv.org/abs/2601.00737)
*Uğurcan Özalp*

Main category: cs.LG

TL;DR: STAC算法通过引入时间性偶然不确定性来缩放悲观偏差，使用单一分布评论家网络建模回报不确定性，结合dropout正则化，在随机环境中实现风险规避并提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 离策略演员-评论家方法中评论家网络倾向于系统性地高估价值估计，现有方法使用集成量化认知不确定性来引入悲观偏差，但需要处理时间性偶然不确定性（来自随机转移、奖励和策略引起的贝尔曼目标变异性）。

Method: 提出STAC算法：1) 使用单一分布评论家网络建模时间回报不确定性；2) 基于时间性偶然不确定性缩放时间差分更新中的悲观偏差；3) 对评论家和演员网络应用dropout进行正则化。

Result: 仅基于分布评论家的悲观偏差足以缓解高估问题，在随机环境中自然导致风险规避行为。引入dropout进一步通过正则化提高了训练稳定性和性能，同时实现了更高的计算效率。

Conclusion: STAC通过利用时间性偶然不确定性而非认知不确定性来缩放悲观偏差，使用单一分布评论家网络和dropout正则化，在保持性能的同时提高了计算效率，为随机环境中的强化学习提供了有效解决方案。

Abstract: Off-policy actor-critic methods in reinforcement learning train a critic with temporal-difference updates and use it as a learning signal for the policy (actor). This design typically achieves higher sample efficiency than purely on-policy methods. However, critic networks tend to overestimate value estimates systematically. This is often addressed by introducing a pessimistic bias based on uncertainty estimates. Current methods employ ensembling to quantify the critic's epistemic uncertainty-uncertainty due to limited data and model ambiguity-to scale pessimistic updates. In this work, we propose a new algorithm called Stochastic Actor-Critic (STAC) that incorporates temporal (one-step) aleatoric uncertainty-uncertainty arising from stochastic transitions, rewards, and policy-induced variability in Bellman targets-to scale pessimistic bias in temporal-difference updates, rather than relying on epistemic uncertainty. STAC uses a single distributional critic network to model the temporal return uncertainty, and applies dropout to both the critic and actor networks for regularization. Our results show that pessimism based on a distributional critic alone suffices to mitigate overestimation, and naturally leads to risk-averse behavior in stochastic environments. Introducing dropout further improves training stability and performance by means of regularization. With this design, STAC achieves improved computational efficiency using a single distributional critic network.

</details>


### [17] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 该论文对工业异常检测算法进行了全面评估，使用模拟数据集测试了14种检测器在不同异常率和训练规模下的性能，发现最佳检测器取决于训练集中的故障样本数量，并提供了工业部署的实用见解。


<details>
  <summary>Details</summary>
Motivation: 机器学习在工业系统（如质量控制和预测性维护）中具有应用潜力，但面临极端类别不平衡的挑战，主要原因是训练期间故障数据的可用性有限。需要评估异常检测算法在真实工程约束下的性能。

Method: 使用问题无关的模拟数据集，包含2D和10D的超球面异常分布。在异常率0.05%到20%、训练规模1,000到10,000（测试集40,000）的条件下，对14种检测器进行基准测试，评估性能和泛化误差。

Result: 最佳检测器高度依赖于训练数据集中故障样本的总数：少于20个故障样本时，无监督方法（kNN/LOF）占优；30-50个故障样本时，半监督（XGBOD）和监督（SVM/CatBoost）方法性能大幅提升。半监督方法在10个特征时表现出明显优势。研究还发现较小数据集上异常检测方法的泛化性能下降。

Conclusion: 研究强调了异常检测算法性能对训练集中故障样本数量的依赖性，为工业环境中部署异常检测提供了实用指导：需要根据可用故障数据量选择合适的方法，并注意高维特征下半监督方法的优势。

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [18] [Yahtzee: Reinforcement Learning Techniques for Stochastic Combinatorial Games](https://arxiv.org/abs/2601.00007)
*Nicholas A. Pape*

Main category: cs.LG

TL;DR: 该研究将Yahtzee游戏建模为MDP，使用多种策略梯度方法训练自博弈智能体，发现A2C在固定训练预算下表现最稳健，达到接近最优性能的241.78分（最优DP分数为254.59）。


<details>
  <summary>Details</summary>
Motivation: Yahtzee作为具有随机性、组合结构和延迟奖励的经典骰子游戏，是一个有趣的中等规模RL基准。虽然单人游戏可通过动态规划求解最优策略，但多人游戏难以处理，需要近似方法。

Method: 将Yahtzee建模为马尔可夫决策过程，使用REINFORCE、A2C和PPO等策略梯度方法训练自博弈智能体，采用具有共享主干的多头网络。对特征和动作编码、架构、回报估计器和熵正则化进行消融研究。

Result: 在固定训练预算下，REINFORCE和PPO对超参数敏感且未能达到接近最优性能，而A2C在各种设置下都能稳健训练。最佳智能体在10万局评估游戏中获得中位数分数241.78分，接近最优DP分数254.59的95%，上区奖励和Yahtzee达成率分别为24.9%和34.1%。

Conclusion: A2C在Yahtzee游戏中表现出最强的鲁棒性，但所有模型都难以学习上区奖励策略，过度关注四条组合，突显了长期信用分配和探索的持续挑战。

Abstract: Yahtzee is a classic dice game with a stochastic, combinatorial structure and delayed rewards, making it an interesting mid-scale RL benchmark. While an optimal policy for solitaire Yahtzee can be computed using dynamic programming methods, multiplayer is intractable, motivating approximation methods. We formulate Yahtzee as a Markov Decision Process (MDP), and train self-play agents using various policy gradient methods: REINFORCE, Advantage Actor-Critic (A2C), and Proximal Policy Optimization (PPO), all using a multi-headed network with a shared trunk. We ablate feature and action encodings, architecture, return estimators, and entropy regularization to understand their impact on learning. Under a fixed training budget, REINFORCE and PPO prove sensitive to hyperparameters and fail to reach near-optimal performance, whereas A2C trains robustly across a range of settings. Our agent attains a median score of 241.78 points over 100,000 evaluation games, within 5.0\% of the optimal DP score of 254.59, achieving the upper section bonus and Yahtzee at rates of 24.9\% and 34.1\%, respectively. All models struggle to learn the upper bonus strategy, overindexing on four-of-a-kind's, highlighting persistent long-horizon credit-assignment and exploration challenges.

</details>


### [19] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: 论文提出了一种针对LLM模型组合技术的攻击方法，通过设计单个"破坏性token"在移植过程中植入恶意特征，破坏基础模型生成能力


<details>
  <summary>Details</summary>
Motivation: 随着开源LLM生态系统越来越多地使用模型组合技术（如权重合并、推测解码、词汇表扩展），这些方法需要在不同模型家族间进行tokenizer移植。论文发现这一关键互操作性步骤存在供应链漏洞，可能被恶意利用。

Method: 通过利用系数重用的几何特性，设计了双目标优化问题，使用稀疏求解器创建单个"破坏性token"。该token在供体模型中功能惰性，但在移植到基础模型后可靠地重构为高显著性恶意特征，形成不对称可实现性差距。

Result: 攻击无需训练，通过谱模仿逃避异常检测，在移植后能破坏基础模型的生成能力，同时供体模型的效用与正常行为在统计上无法区分。攻击具有结构持久性，能抵抗微调和权重合并。

Conclusion: tokenizer移植作为模型组合的关键步骤存在隐藏风险，可能被用于供应链攻击。论文揭示的漏洞凸显了模块化AI组合管道中的安全隐患，需要更安全的移植机制。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [20] [IMBWatch -- a Spatio-Temporal Graph Neural Network approach to detect Illicit Massage Business](https://arxiv.org/abs/2601.00075)
*Swetha Varadarajan,Abhishek Ray,Lumina Albert*

Main category: cs.LG

TL;DR: IMBWatch是一个时空图神经网络框架，用于大规模检测伪装成合法按摩服务的非法按摩店网络，这些店铺涉及人口贩卖和性剥削。


<details>
  <summary>Details</summary>
Motivation: 非法按摩店（IMBs）以合法健康服务为掩护，进行人口贩卖、性剥削和强迫劳动等犯罪活动。传统检测方法（如社区举报和监管检查）是反应式的，难以揭示犯罪网络的全貌，因为IMBs使用编码广告、频繁更换人员和地点、共享基础设施等手段逃避检测。

Method: IMBWatch是一个时空图神经网络（ST-GNN）框架，从开源情报（在线广告、营业执照记录、众包评论）构建动态图。节点代表企业、别名、电话号码、位置等异构实体，边捕获时空和关系模式（如共址、重复电话使用、同步广告）。框架结合图卷积操作和时间注意力机制，建模IMB网络在时空上的演化模式。

Result: 在美国多个城市的真实数据集上，IMBWatch优于基线模型，获得了更高的准确率和F1分数。除了性能提升，该框架还提供更好的可解释性，为主动干预提供可操作的见解。框架具有可扩展性，可适应其他非法领域。

Conclusion: IMBWatch是一个有效的时空图神经网络框架，能够大规模检测非法按摩店网络，提供比传统方法更好的检测性能和可解释性。该框架开源且可扩展，有助于支持主动干预和可重复研究。

Abstract: Illicit Massage Businesses (IMBs) are a covert and persistent form of organized exploitation that operate under the facade of legitimate wellness services while facilitating human trafficking, sexual exploitation, and coerced labor. Detecting IMBs is difficult due to encoded digital advertisements, frequent changes in personnel and locations, and the reuse of shared infrastructure such as phone numbers and addresses. Traditional approaches, including community tips and regulatory inspections, are largely reactive and ineffective at revealing the broader operational networks traffickers rely on.
  To address these challenges, we introduce IMBWatch, a spatio-temporal graph neural network (ST-GNN) framework for large-scale IMB detection. IMBWatch constructs dynamic graphs from open-source intelligence, including scraped online advertisements, business license records, and crowdsourced reviews. Nodes represent heterogeneous entities such as businesses, aliases, phone numbers, and locations, while edges capture spatio-temporal and relational patterns, including co-location, repeated phone usage, and synchronized advertising. The framework combines graph convolutional operations with temporal attention mechanisms to model the evolution of IMB networks over time and space, capturing patterns such as intercity worker movement, burner phone rotation, and coordinated advertising surges.
  Experiments on real-world datasets from multiple U.S. cities show that IMBWatch outperforms baseline models, achieving higher accuracy and F1 scores. Beyond performance gains, IMBWatch offers improved interpretability, providing actionable insights to support proactive and targeted interventions. The framework is scalable, adaptable to other illicit domains, and released with anonymized data and open-source code to support reproducible research.

</details>


### [21] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

TL;DR: 提出了一种基于图结构的多用户上下文赌博机方法，通过构建统一的多用户RKHS核函数融合图拉普拉斯与基础臂核，设计了理论保证的算法并获得优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 研究多用户上下文赌博机问题，其中用户通过图结构关联，且奖励函数同时表现出非线性行为和图同质性。需要设计能够同时利用图结构和非线性关系的统一框架。

Method: 引入联合惩罚项，结合基于RKHS距离的图平滑项和个体粗糙度惩罚。证明该惩罚等价于单一多用户RKHS中的平方范数，显式推导其再生核，将图拉普拉斯与基础臂核优雅融合。基于此设计算法LK-GP-UCB和LK-GP-TS，利用高斯过程后验进行探索。

Result: 提供了基于多用户核有效维度的高概率遗憾界，替代了对用户数量或环境维度的依赖。实验表明，在非线性设置中优于强线性和非图感知基线，即使在真实奖励为线性时也保持竞争力。

Conclusion: 提出了一个统一、理论严谨且实用的框架，将拉普拉斯正则化与核化赌博机相结合，用于结构化探索，为图结构多用户上下文学习提供了新方法。

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [22] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

TL;DR: 提出一种渐近置信度的最佳臂识别框架，通过放宽精确误差控制要求，在长时域场景中实现更紧的样本复杂度界限，并能灵活处理非参数分布和协变量。


<details>
  <summary>Details</summary>
Motivation: 现有BAI方法在实际应用中存在局限性：严格的精确误差控制需要使用宽松的尾不等式和/或参数限制，导致效率低下。许多实际场景涉及弱信号、高显著性要求和实验后推断需求，这些都需要长时域，但现有方法无法有效处理。

Method: 引入渐近误差控制框架，要求相对于最小样本量渐近有效。开发新颖的渐近任意时间有效置信序列，基于此设计新的BAI算法。方法灵活整合协变量进行方差缩减，确保在完全非参数设置中的近似误差控制。

Result: 在温和收敛假设下，提供样本复杂度的渐近界限，显示最坏情况样本复杂度与高斯BAI在精确误差保证和已知方差下的最佳情况样本复杂度相匹配。实验表明该方法在保持误差控制的同时减少了平均样本复杂度。

Conclusion: 提出的渐近BAI框架克服了现有方法的局限性，通过放宽精确误差控制要求，实现了更紧的最优性，能更好地处理灵活的非参数结果分布，并充分利用个体级上下文信息，适用于需要长时域的实际应用场景。

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [23] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

TL;DR: 提出一种基于扩散模型的分类变量软重参数化方法，通过高斯噪声过程的去噪器实现高效计算，无需训练即可采样并反向传播。


<details>
  <summary>Details</summary>
Motivation: 针对分类变量的梯度优化问题，现有方法存在局限性：得分函数估计器虽然无偏但噪声大，连续松弛方法虽然提供路径梯度但优化的是有偏的温度依赖目标。需要一种更好的分类变量优化方法。

Method: 提出扩散基软重参数化方法，利用高斯噪声过程对分类分布进行去噪，去噪器有闭式解且计算高效，实现无需训练的扩散采样器，支持反向传播。

Result: 在各种基准测试中，提出的重参数化技巧展现出竞争性或改进的优化性能。

Conclusion: 扩散基软重参数化为分类变量优化提供了有效的新方法，通过闭式去噪器实现高效计算和反向传播，在多个任务上表现优异。

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>


### [24] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

TL;DR: NeuroSymBO使用贝叶斯优化自适应选择LLM推理策略，解决方程发现中的指令脆弱性问题，显著优于固定提示方法


<details>
  <summary>Details</summary>
Motivation: 大语言模型在方程发现中表现出潜力，但其输出对提示措辞高度敏感（指令脆弱性）。静态提示无法适应多步生成过程的演化状态，导致模型停留在次优解

Method: 提出NeuroSymBO方法，将提示工程重构为序列决策问题。维护离散推理策略库，使用贝叶斯优化基于数值反馈在每个步骤选择最优指令

Result: 在PDE发现基准测试中，自适应指令选择显著优于固定提示，实现了更高的恢复率和更简洁的解

Conclusion: 将提示工程视为序列决策问题并通过贝叶斯优化自适应选择指令，能有效解决LLM在方程发现中的指令脆弱性问题，提升性能

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [25] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: GRL-SNAM是一个几何强化学习框架，用于未知环境中的同时导航与建图。它通过局部能量景观编码可达性和障碍约束，使用哈密顿优化动态搜索最短路径，无需构建全局地图。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中同时导航与建图的挑战性问题，传统方法需要构建全局地图或设计复杂的多智能体策略，而GRL-SNAM旨在仅依赖局部感官观测实现高效导航。

Method: 将路径导航和建图建模为动态最短路径搜索过程，使用受控哈密顿优化：将感官输入转换为局部能量景观编码可达性、障碍屏障和变形约束，通过更新哈密顿量分阶段演化感知、规划和重配置策略。

Result: 在2D导航任务中评估，相比局部反应式基线和全局策略学习方法，GRL-SNAM保持安全距离，泛化到未见布局，通过局部能量优化而非广泛全局建图实现高质量导航。

Conclusion: 通过哈密顿量更新的几何强化学习能够通过最小化探索和局部能量优化实现高质量导航，无需构建全局地图，为未知环境中的同时导航与建图提供了有效解决方案。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [26] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

TL;DR: 研究非马尔可夫状态和成本过程下带线性函数逼近的强化学习方法，包括策略评估和Q学习的收敛性分析


<details>
  <summary>Details</summary>
Motivation: 传统强化学习通常假设马尔可夫过程，但在实际应用中状态和成本过程可能是非马尔可夫的，需要研究在此条件下的学习算法收敛性

Method: 1) 分析策略评估方法在非马尔可夫过程下的收敛性；2) 研究Q学习在线性函数逼近下的收敛条件，特别针对基于量化映射的基函数；3) 将结果应用于部分可观测马尔可夫决策过程，使用有限记忆变量作为状态表示

Result: 1) 策略评估算法在适当的遍历性条件下收敛；2) 收敛极限对应于正交投影和辅助马尔可夫决策过程贝尔曼算子的联合不动点；3) 对于基于量化映射的基函数，Q学习在类似遍历性条件下收敛；4) 推导了部分可观测马尔可夫决策过程中学习算法的显式误差界

Conclusion: 该研究为非马尔可夫过程下的强化学习提供了理论保证，特别针对线性函数逼近方法，并成功应用于部分可观测马尔可夫决策过程，为实际非马尔可夫环境中的强化学习应用提供了理论基础

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [27] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

TL;DR: 研究使用XGBoost模型分析美国交通事故严重程度预测，发现时间、地理位置和天气变量是最强预测因子，但模型对极端严重程度案例预测能力有限。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索环境、时间和空间因素对美国交通事故严重程度的预测能力，为基于证据的交通管理提供支持。

Method: 使用2016-2023年50万起美国交通事故数据集，采用XGBoost分类器，通过随机搜索交叉验证优化，并使用类别加权处理类别不平衡问题。

Result: 最终模型整体准确率达78%，对主要类别（严重程度2）的精确率和召回率达87%。特征重要性分析显示时间、地理位置和天气变量（能见度、温度、风速）是最强预测因子，但降水和能见度预测能力有限。

Conclusion: 研究发现时间、位置和天气变量能有效预测事故严重程度，但数据集以中等严重程度事故为主限制了模型对极端案例的学习能力，需要改进采样策略、特征工程和外部数据整合。

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [28] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种使用纯强化学习梯度进行Decision Transformers在线微调的新方法，解决了现有方法依赖监督学习目标的问题，在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有Decision Transformers在线微调方法主要依赖监督序列建模目标，而基于重要性采样的纯强化学习梯度方法（如GRPO）与后见回报重标注不兼容，导致训练不稳定。需要开发能够有效利用纯强化学习梯度进行在线微调的DT方法。

Method: 将GRPO适配到DT框架，并引入关键改进：1) 子轨迹优化以改善信用分配；2) 序列级似然目标以增强稳定性和效率；3) 主动采样以鼓励在不确定区域进行探索。

Result: 新方法在多个基准测试中超越了现有的在线DT基线，实现了最先进的性能，证明了纯强化学习梯度在线微调对Decision Transformers的有效性。

Conclusion: 本文成功开发了基于纯强化学习梯度的Decision Transformers在线微调方法，解决了后见回报重标注与重要性采样算法的不兼容问题，为DT在在线强化学习中的应用开辟了新途径。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [29] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

TL;DR: 提出Sequential RC架构，将大储层分解为多个小储层串联，降低内存和计算成本，在高维时空系统中比LSTM和标准RNN有更长的预测时效和更低误差


<details>
  <summary>Details</summary>
Motivation: 传统RNN和LSTM在高维时空系统预测中存在梯度训练和内存瓶颈问题，传统RC虽然通过固定循环层和凸读出优化缓解了这些问题，但输入维度扩展性仍然较差

Method: 提出Sequential Reservoir Computing架构，将大型储层分解为一系列小型互连储层，保持长期时间依赖性的同时降低内存和计算成本

Result: 在低维混沌系统(Lorenz63)和高维物理模拟(2D涡度和浅水方程)中，Sequential RC比LSTM和标准RNN基线获得15-25%更长的有效预测时效，20-30%更低的误差指标(SSIM, RMSE)，训练成本降低高达三个数量级

Conclusion: Sequential RC保持了传统RC的简单性和效率，同时实现了对高维动力系统的卓越可扩展性，为科学和工程应用中的实时、节能预测提供了实用路径

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [30] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 使用电子健康记录数据开发机器学习模型预测肝硬化的研究，XGBoost模型在1-3年预测窗口上均优于传统FIB-4评分


<details>
  <summary>Details</summary>
Motivation: 开发基于常规电子健康记录数据的机器学习模型，用于早期预测肝硬化发生，并与传统FIB-4评分进行性能比较，以支持临床早期干预

Method: 回顾性队列研究，使用大型学术医疗系统的去标识化EHR数据，识别脂肪肝患者并分为肝硬化与非肝硬化组，构建1-3年预测场景，从观察窗口聚合人口统计学、诊断、实验室结果等特征，训练XGBoost模型并在测试集评估

Result: XGBoost模型在1年、2年、3年预测的AUC分别为0.81、0.73、0.69，显著优于FIB-4的0.71、0.63、0.57，且随着预测时间延长性能优势更明显

Conclusion: 基于常规EHR数据的机器学习模型在肝硬化早期预测方面显著优于传统FIB-4评分，可作为自动化决策支持工具集成到临床工作流程中，支持主动的肝硬化预防和管理

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [31] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应重复编码框架，实现按维度不等错误保护，在有限带宽下显著提升语义保真度


<details>
  <summary>Details</summary>
Motivation: 解决带宽受限通信系统中语义意义保持的挑战，传统信道编码（如LDPC或Reed-Solomon）无法实现细粒度语义保护

Method: 基于强化学习的自适应重复编码框架，使用复合语义失真度量（平衡全局嵌入相似性和实体级保持），实现上下文感知的保护分配

Result: 相比均匀保护，在1 dB SNR下获得6.8%更高的chrF分数和9.3%更好的实体保持，统计显著

Conclusion: 智能分配的简单重复编码可实现细粒度语义保护，代码结构必须与语义粒度对齐，适用于边缘计算和物联网等带宽稀缺但语义保真度关键的场景

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [32] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: 本文比较了14种机器学习分类器在自动标记小鼠EEG中棘慢波放电(SWD)的性能，发现1D UNet表现最佳，并通过数据增强进一步改进，最终AugUNet1D优于现有的"Twin Peaks"算法。


<details>
  <summary>Details</summary>
Motivation: 手动标记脑电图(EEG)记录中的事件非常耗时，特别是连续数周至数月的记录。棘慢波放电(SWD)作为失神发作的电生理标志，通常需要手动标记。虽然已有研究使用机器学习自动分割和分类EEG信号，但仍有改进空间。

Method: 使用961小时C3H/HeJ小鼠EEG记录（包含22,637个标记的SWD）的数据集，比较14种机器学习分类器的性能。发现1D UNet最佳后，通过数据增强改进，特别发现缩放增强效果最好。将增强后的AugUNet1D与最近发表的"Twin Peaks"算法进行比较。

Result: 1D UNet在所有分类器中表现最佳。数据增强进一步提升了性能，其中缩放增强效果最显著。AugUNet1D在性能上优于"Twin Peaks"算法，检测到的事件特征更接近手动标记的SWD。

Conclusion: AugUNet1D是自动标记EEG中SWD的有效工具，性能优于现有算法。研究提供了预训练和未训练的模型供公开使用，有助于减少手动标记工作量。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [33] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

TL;DR: 提出SSI-GAN半监督生成对抗网络，仅需1-3%标注数据即可高精度分类蚊子神经元放电模式，用于检测寨卡、登革热病毒感染，大幅减少人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 蚊子是虫媒病毒主要传播媒介，人工分类神经元放电模式耗时耗力。现有深度学习方案需要完全标注的数据集和高度预处理的信号，难以在实际场景大规模应用。需要解决标注数据稀缺问题。

Method: 提出半监督Swin启发式GAN（SSI-GAN），采用基于变换器的生成器和Swin启发的移位窗口判别器。使用多头自注意力模型在平面窗口变换器判别器中学习稀疏高频放电特征。仅用1-3%标注数据训练，使用贝叶斯Optuna框架优化超参数，五折蒙特卡洛交叉验证验证鲁棒性。

Result: SSI-GAN在感染后第三天仅用3%标注数据达到99.93%分类准确率。仅需1%监督即可在所有感染阶段保持高准确率。相比标准监督方法，在相同性能水平下减少97-99%人工标注工作量。移位窗口变换器设计大幅超越所有基线方法。

Conclusion: SSI-GAN通过半监督学习和变换器架构，显著减少神经元放电分类所需标注数据量，为实际现场大规模应用提供了可行解决方案，在基于放电的神经元感染分类中创造了新的最佳性能。

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [34] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

TL;DR: 提出基于特征工程的轻量级心律失常检测框架，结合小波分解和图论描述符，在资源受限边缘设备上实现高精度实时诊断


<details>
  <summary>Details</summary>
Motivation: 心血管疾病特别是心律失常是全球主要死因，需要IoMT持续监测。现有深度学习方法计算开销大，不适合资源受限的边缘设备

Method: 提出资源高效的数据中心框架，优先特征工程而非模型复杂度。整合时频小波分解与图论结构描述符（如PageRank中心性），通过互信息和递归消除优化特征空间，使用超轻量线性分类器

Result: 在MIT-BIH和INCART数据集上达到98.44%诊断准确率，模型仅8.54KB，分类推理延迟0.46μs，每搏处理管道52ms，比压缩模型KD-Light（25KB，96.32%）有数量级效率提升

Conclusion: 该框架通过特征工程使复杂心律失常数据线性可分，为无电池心脏传感器提供实时高效解决方案，显著优于现有压缩模型

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [35] [Unknown Aware AI-Generated Content Attribution](https://arxiv.org/abs/2601.00218)
*Ellie Thieu,Jifan Zhang,Haoyue Bai*

Main category: cs.LG

TL;DR: 该论文提出一种利用未标注网络数据增强AI生成内容溯源的方法，通过约束优化提升对未见生成器的识别能力。


<details>
  <summary>Details</summary>
Motivation: 随着逼真生成模型的快速发展，需要超越简单的真假检测，实现特定生成模型的溯源。现有方法在未见或新发布的生成器上泛化能力不足。

Method: 使用CLIP特征和线性分类器建立基线，提出约束优化方法利用未标注网络数据（可能包含真实图像、未知生成器输出或目标模型样本），鼓励网络样本被分类为非目标，同时保持标注数据性能。

Result: 实验结果表明，引入网络数据显著提升了在挑战性未见生成器上的溯源性能，证明未标注数据能有效增强开放世界场景下的AI生成内容溯源。

Conclusion: 利用未标注网络数据通过约束优化方法可以有效提升生成模型溯源在开放世界设置中的泛化能力，为解决AI生成内容溯源问题提供了新思路。

Abstract: The rapid advancement of photorealistic generative models has made it increasingly important to attribute the origin of synthetic content, moving beyond binary real or fake detection toward identifying the specific model that produced a given image. We study the problem of distinguishing outputs from a target generative model (e.g., OpenAI Dalle 3) from other sources, including real images and images generated by a wide range of alternative models. Using CLIP features and a simple linear classifier, shown to be effective in prior work, we establish a strong baseline for target generator attribution using only limited labeled data from the target model and a small number of known generators. However, this baseline struggles to generalize to harder, unseen, and newly released generators. To address this limitation, we propose a constrained optimization approach that leverages unlabeled wild data, consisting of images collected from the Internet that may include real images, outputs from unknown generators, or even samples from the target model itself. The proposed method encourages wild samples to be classified as non target while explicitly constraining performance on labeled data to remain high. Experimental results show that incorporating wild data substantially improves attribution performance on challenging unseen generators, demonstrating that unlabeled data from the wild can be effectively exploited to enhance AI generated content attribution in open world settings.

</details>


### [36] [Robust Graph Fine-Tuning with Adversarial Graph Prompting](https://arxiv.org/abs/2601.00229)
*Ziyan Zhang,Bo Jiang,Jin Tang*

Main category: cs.LG

TL;DR: 提出对抗性图提示（AGP）框架，首次将对抗学习融入图提示中，通过min-max优化实现鲁棒的图微调，能同时处理图拓扑和节点噪声。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调（PEFT）方法在面对图拓扑和节点属性的各种噪声和攻击时表现出显著脆弱性，需要提升图微调的鲁棒性。

Method: 提出AGP框架：1）将问题形式化为min-max优化问题，采用交替优化方案；2）内层最大化使用JointPGD算法生成强对抗噪声；3）外层最小化学习最优节点提示来对抗噪声。

Result: 理论证明AGP能同时处理图拓扑和节点噪声，实验验证AGP在多个基准任务上比现有方法具有更好的鲁棒性和有效性。

Conclusion: AGP是首个将对抗学习融入图提示的通用框架，能显著提升预训练GNN模型在下游任务中的鲁棒性，可集成到各种预训练GNN模型中。

Abstract: Parameter-Efficient Fine-Tuning (PEFT) method has emerged as a dominant paradigm for adapting pre-trained GNN models to downstream tasks. However, existing PEFT methods usually exhibit significant vulnerability to various noise and attacks on graph topology and node attributes/features. To address this issue, for the first time, we propose integrating adversarial learning into graph prompting and develop a novel Adversarial Graph Prompting (AGP) framework to achieve robust graph fine-tuning. Our AGP has two key aspects. First, we propose the general problem formulation of AGP as a min-max optimization problem and develop an alternating optimization scheme to solve it. For inner maximization, we propose Joint Projected Gradient Descent (JointPGD) algorithm to generate strong adversarial noise. For outer minimization, we employ a simple yet effective module to learn the optimal node prompts to counteract the adversarial noise. Second, we demonstrate that the proposed AGP can theoretically address both graph topology and node noise. This confirms the versatility and robustness of our AGP fine-tuning method across various graph noise. Note that, the proposed AGP is a general method that can be integrated with various pre-trained GNN models to enhance their robustness on the downstream tasks. Extensive experiments on multiple benchmark tasks validate the robustness and effectiveness of AGP method compared to state-of-the-art methods.

</details>


### [37] [GRIT -- Geometry-Aware PEFT with K-FACPreconditioning, Fisher-Guided Reprojection, andDynamic Rank Adaptation](https://arxiv.org/abs/2601.00231)
*Pritish Saha,Chandrav Rajbangshi,Rudra Goyal,Mohit Goyal,Anurag Deo,Biswajit Roy,Ningthoujam Dhanachandra Singh,Raxit Goswami,Amitava Das*

Main category: cs.LG

TL;DR: GRIT是一种动态、曲率感知的LoRA方法，通过K-FAC预条件梯度、定期重投影到Fisher特征方向、自适应调整有效秩，在减少46%可训练参数的同时达到或超越LoRA/QLoRA性能


<details>
  <summary>Details</summary>
Motivation: 现有LoRA和QLoRA方法在几何上不够敏感：它们在固定的随机低秩子空间中优化，主要使用一阶下降，忽略了局部损失曲率。这可能导致有效更新预算膨胀，并沿着弱约束方向放大漂移

Method: GRIT保持LoRA参数化但：(1)使用K-FAC作为自然梯度代理在秩空间中对梯度进行预条件处理；(2)定期将低秩基重投影到主导Fisher特征方向上以抑制漂移；(3)根据谱自适应调整有效秩，使容量集中在信号存在的地方

Result: 在LLaMA骨干网络上的指令跟随、理解和推理基准测试中，GRIT匹配或超越了LoRA和QLoRA，同时平均减少46%的可训练参数（不同任务中减少25-80%），在各种提示风格和数据混合中没有实际质量损失

Conclusion: GRIT通过曲率感知的优化方法，在减少参数的同时保持了性能，相比其他PEFT优化器基线（正交LoRA、IA3、DoRA、Eff-FT、Shampoo）具有更低的漂移和更好的更新与保留平衡

Abstract: Parameter-efficient fine-tuning (PEFT) is the default way to adapt LLMs, but widely used LoRA and QLoRA are largely geometry-agnostic: they optimize in fixed, randomly oriented low-rank subspaces with first-order descent, mostly ignoring local loss curvature. This can inflate the effective update budget and amplify drift along weakly constrained directions. We introduce GRIT, a dynamic, curvature-aware LoRA procedure that preserves the LoRA parameterization but: (1) preconditions gradients in rank space using K-FAC as a natural-gradient proxy; (2) periodically reprojects the low-rank basis onto dominant Fisher eigendirections to suppress drift; and (3) adapts the effective rank from the spectrum so capacity concentrates where signal resides. Across instruction-following, comprehension, and reasoning benchmarks on LLaMA backbones, GRIT matches or surpasses LoRA and QLoRA while reducing trainable parameters by 46% on average (25--80% across tasks), without practical quality loss across prompt styles and data mixes. To model forgetting, we fit a curvature-modulated power law. Empirically, GRIT yields lower drift and a better updates-vs-retention frontier than strong PEFT-optimizer baselines (Orthogonal-LoRA, IA3, DoRA, Eff-FT, Shampoo).

</details>


### [38] [Task-Driven Kernel Flows: Label Rank Compression and Laplacian Spectral Filtering](https://arxiv.org/abs/2601.00276)
*Hongxi Li,Chunlin Huang*

Main category: cs.LG

TL;DR: 宽L2正则化网络中监督学习本质上是压缩的，其特征学习理论表明核的秩受类别数限制，SGD噪声也保持低秩，与自监督的高秩表示形成对比。


<details>
  <summary>Details</summary>
Motivation: 研究宽神经网络中特征学习的本质特性，探索监督学习与自监督学习在表示结构上的根本差异，理解正则化网络中特征演化的压缩特性。

Method: 提出宽L2正则化网络的特征学习理论，推导预测"注水"谱演化的核ODE，证明稳定稳态下核秩受类别数限制，分析SGD噪声的低秩特性。

Result: 监督学习是固有压缩的：核秩受类别数C限制，SGD噪声也是低秩的(O(C))，动力学被限制在任务相关子空间，与自监督学习的高秩扩展表示形成鲜明对比。

Conclusion: 监督学习本质上产生低秩压缩表示，而自监督学习产生高秩扩展表示，该框架统一了对齐的确定性和随机性观点，揭示了两种学习范式在表示结构上的根本差异。

Abstract: We present a theory of feature learning in wide L2-regularized networks showing that supervised learning is inherently compressive. We derive a kernel ODE that predicts a "water-filling" spectral evolution and prove that for any stable steady state, the kernel rank is bounded by the number of classes ($C$). We further demonstrate that SGD noise is similarly low-rank ($O(C)$), confining dynamics to the task-relevant subspace. This framework unifies the deterministic and stochastic views of alignment and contrasts the low-rank nature of supervised learning with the high-rank, expansive representations of self-supervision.

</details>


### [39] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 该论文提出了一个基于国际象棋战术的量子基准测试QKRD，用于系统评估QAOA算法在结构化问题上的表现，发现约束保持混合器、热启动策略等能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有QAOA基准测试主要使用MaxCut、TSP、SAT等随机合成实例，这些实例缺乏语义结构和人类可解释性，无法反映算法在具有实际约束的现实问题上的表现。

Method: 提出Quantum King-Ring Domination (QKRD)基准测试，基于国际象棋战术位置构建，包含5,000个结构化实例，具有one-hot约束、空间局部性和10-40量子比特规模，结合人类可解释的覆盖度指标和内在验证机制。

Result: 约束保持混合器(XY, domain-wall)比标准混合器收敛快约13步；热启动策略减少45步收敛时间，能量改进显著；CVaR优化表现更差。QAOA优于贪心启发式算法12.6%，优于随机选择80.1%。

Conclusion: 结构化基准测试能揭示在随机实例中被掩盖的问题感知QAOA技术优势，为NISQ算法研究提供了可重复的评估框架。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [40] [Smart Fault Detection in Nanosatellite Electrical Power System](https://arxiv.org/abs/2601.00335)
*Alireza Rezaee,Niloofar Nobahari,Amin Asgarifar,Farshid Hajati*

Main category: cs.LG

TL;DR: 提出一种无需姿态确定控制子系统(ADCS)的纳米卫星电源故障检测方法，使用神经网络模拟正常状态，结合多种机器学习方法进行故障分类


<details>
  <summary>Details</summary>
Motivation: 纳米卫星在LEO轨道运行时，其电力系统各部件（光伏子系统、DC-DC转换器、地面电池调节器等）容易因压力耐受性、发射压力和环境因素而出现故障，需要有效的故障检测方法

Method: 首先使用神经网络建立无故障状态模型，以太阳辐射和太阳能电池板表面温度为输入，电流和负载为输出；然后使用神经网络分类器结合PCA分类、决策树和KNN等机器学习方法进行故障模式识别和分类

Result: 开发了一种能够检测纳米卫星电力系统常见故障（包括光伏子系统的线间故障和开路故障、DC-DC转换器的短路和IGBT开路故障、地面电池调节器故障）的诊断系统

Conclusion: 该方法能够在没有姿态确定控制子系统的情况下有效检测和分类纳米卫星电力系统的多种故障，为小型卫星的可靠运行提供了实用的故障诊断解决方案

Abstract: This paper presents a new detection method of faults at Nanosatellites' electrical power without an Attitude Determination Control Subsystem (ADCS) at the LEO orbit. Each part of this system is at risk of fault due to pressure tolerance, launcher pressure, and environmental circumstances. Common faults are line to line fault and open circuit for the photovoltaic subsystem, short circuit and open circuit IGBT at DC to DC converter, and regulator fault of the ground battery. The system is simulated without fault based on a neural network using solar radiation and solar panel's surface temperature as input data and current and load as outputs. Finally, using the neural network classifier, different faults are diagnosed by pattern and type of fault. For fault classification, other machine learning methods are also used, such as PCA classification, decision tree, and KNN.

</details>


### [41] [Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models](https://arxiv.org/abs/2601.00391)
*Nouar AlDahoul,Aznul Qalid Md Sabri,Ali Mohammed Mansoor*

Main category: cs.LG

TL;DR: 该论文提出结合光流和三种深度模型（监督CNN、预训练CNN特征提取器、分层极限学习机）的自动特征学习方法，用于无人机非静态摄像头视频中的人体检测，在UCF-ARG数据集上取得了高精度结果。


<details>
  <summary>Details</summary>
Motivation: 传统手工特征方法依赖专家知识，对光照变化、相机抖动等动态事件敏感，且任务特定。需要更便宜、更易用、能自动学习抽象特征的深度学习方法来解决无人机视频中的人体检测问题。

Method: 结合光流和三种深度模型：1）监督卷积神经网络（S-CNN）配合softmax或SVM分类器；2）预训练CNN特征提取器；3）分层极限学习机（H-ELM）。在UCF-ARG无人机数据集上训练和测试，评估挖掘、挥手、投掷、行走、奔跑五种人类动作。

Result: 预训练CNN平均准确率98.09%，S-CNN使用softmax达到95.6%（SVM为91.7%），H-ELM达到95.9%。H-ELM在CPU上训练时间445秒，S-CNN在GPU上需要770秒。

Conclusion: 提出的自动特征学习方法在无人机视频人体检测任务上表现成功，特别是预训练CNN模型取得了最佳性能，证明了深度学习方法在该领域的有效性。

Abstract: Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).

</details>


### [42] [Deep Delta Learning](https://arxiv.org/abs/2601.00417)
*Yifan Zhang,Yifeng Liu,Mengdi Wang,Quanquan Gu*

Main category: cs.LG

TL;DR: DDL提出了一种新的深度残差网络架构，通过可学习的几何变换调制恒等快捷连接，替代传统的严格加法偏置，从而增强网络建模复杂状态转换的能力。


<details>
  <summary>Details</summary>
Motivation: 传统残差网络的恒等快捷连接虽然有效缓解了梯度消失问题，但其严格的加法归纳偏置限制了网络建模复杂状态转换的能力。需要一种更灵活的机制来增强网络的表达能力。

Method: 提出深度Delta学习（DDL）架构，通过Delta算子（一个由反射方向向量k(X)和门控标量β(X)参数化的秩-1恒等矩阵扰动）来调制恒等快捷连接。将残差更新重构为同步秩-1注入，门控β(X)作为动态步长控制旧信息擦除和新特征写入。

Result: 通过谱分析表明，门控β(X)能够实现恒等映射、正交投影和几何反射之间的动态插值。该架构使网络能够显式控制层间转移算子的谱，从而建模复杂非单调动态，同时保持门控残差架构的稳定训练特性。

Conclusion: DDL成功泛化了标准残差连接，通过可学习的几何变换增强了网络建模复杂状态转换的能力，在保持训练稳定性的同时提供了更丰富的特征变换机制。

Abstract: The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\mathbf{k}(\mathbf{X})$ and a gating scalar $β(\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $β(\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures.

</details>


### [43] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

TL;DR: 提出E-GRPO方法，通过熵感知的分组相对策略优化增强流匹配模型的人类偏好对齐，通过合并低熵步骤为高熵SDE采样步骤来解决多步去噪中的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多步去噪优化中面临稀疏和模糊的奖励信号问题，观察到高熵步骤能实现更高效探索，而低熵步骤导致无差别的轨迹生成。

Method: 提出E-GRPO方法：1) 合并连续低熵步骤形成高熵SDE采样步骤，其他步骤使用ODE采样；2) 引入多步分组归一化优势，在共享相同合并SDE去噪步骤的样本中计算分组相对优势。

Result: 在不同奖励设置下的实验结果证明了该方法的有效性。

Conclusion: 通过熵感知的SDE采样步骤合并和分组相对优势计算，有效解决了流匹配模型中人类偏好对齐的奖励稀疏性问题。

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [44] [A Comparative Analysis of Interpretable Machine Learning Methods](https://arxiv.org/abs/2601.00428)
*Mattia Billa,Giovanni Orlandi,Veronica Guidetti,Federica Mandreoli*

Main category: cs.LG

TL;DR: 对16种可解释机器学习方法在216个真实世界表格数据集上进行大规模比较评估，发现性能高度依赖数据集特征，EBM在回归任务中表现最佳，SR和IGANN在非线性场景中表现良好，GOSDT对类别不平衡敏感。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在高风险领域（医疗、金融、法律）的广泛应用，模型可解释性和问责制日益重要。尽管可解释ML受到关注，但对表格数据的固有可解释模型的系统性评估仍然稀缺，且通常只关注聚合性能结果。

Method: 对16种固有可解释方法进行大规模比较评估，包括经典线性模型、决策树以及EBM、SR、GOSDT等新方法。研究涵盖216个真实世界表格数据集，按数据集结构特征（维度、样本量、线性度、类别不平衡）分层评估性能，同时评估训练时间和分布偏移下的鲁棒性。

Result: 结果显示清晰的性能层次结构，特别是在回归任务中，EBM始终实现强预测准确性。性能高度依赖上下文：SR和IGANN在非线性场景中表现特别好，而GOSDT模型对类别不平衡表现出明显敏感性。

Conclusion: 这些发现为寻求可解释性和预测性能平衡的从业者提供实用指导，并促进对表格数据可解释建模的更深入实证理解。

Abstract: In recent years, Machine Learning (ML) has seen widespread adoption across a broad range of sectors, including high-stakes domains such as healthcare, finance, and law. This growing reliance has raised increasing concerns regarding model interpretability and accountability, particularly as legal and regulatory frameworks place tighter constraints on using black-box models in critical applications. Although interpretable ML has attracted substantial attention, systematic evaluations of inherently interpretable models, especially for tabular data, remain relatively scarce and often focus primarily on aggregated performance outcomes.
  To address this gap, we present a large-scale comparative evaluation of 16 inherently interpretable methods, ranging from classical linear models and decision trees to more recent approaches such as Explainable Boosting Machines (EBMs), Symbolic Regression (SR), and Generalized Optimal Sparse Decision Trees (GOSDT). Our study spans 216 real-world tabular datasets and goes beyond aggregate rankings by stratifying performance according to structural dataset characteristics, including dimensionality, sample size, linearity, and class imbalance. In addition, we assess training time and robustness under controlled distributional shifts. Our results reveal clear performance hierarchies, especially for regression tasks, where EBMs consistently achieve strong predictive accuracy. At the same time, we show that performance is highly context-dependent: SR and Interpretable Generalized Additive Neural Networks (IGANNs) perform particularly well in non-linear regimes, while GOSDT models exhibit pronounced sensitivity to class imbalance. Overall, these findings provide practical guidance for practitioners seeking a balance between interpretability and predictive performance, and contribute to a deeper empirical understanding of interpretable modeling for tabular data.

</details>


### [45] [A Comparative Study of Adaptation Strategies for Time Series Foundation Models in Anomaly Detection](https://arxiv.org/abs/2601.00446)
*Miseon Park,Kijung Yoon*

Main category: cs.LG

TL;DR: 时间序列基础模型（TSFMs）可作为通用异常检测骨干网络，在零样本推理、全模型适应和参数高效微调策略中均优于任务特定基线，特别是在类别不平衡严重时表现突出。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型（在大型异构数据上预训练）是否可作为异常检测的通用骨干网络，以解决现有方法需要大量任务特定训练的问题。

Method: 通过系统实验比较零样本推理、全模型适应和参数高效微调（PEFT）策略，包括LoRA、OFT和HRA等方法，评估TSFMs在多个基准测试中的表现。

Result: TSFMs在AUC-PR和VUS-PR指标上显著优于任务特定基线，特别是在严重类别不平衡情况下表现更佳；PEFT方法不仅降低计算成本，在多数情况下匹配或超越全微调效果。

Conclusion: 时间序列基础模型可作为异常检测的通用骨干网络，即使预训练用于预测任务也能高效适应异常检测，为可扩展高效的时间序列异常检测提供了有前景的解决方案。

Abstract: Time series anomaly detection is essential for the reliable operation of complex systems, but most existing methods require extensive task-specific training. We explore whether time series foundation models (TSFMs), pretrained on large heterogeneous data, can serve as universal backbones for anomaly detection. Through systematic experiments across multiple benchmarks, we compare zero-shot inference, full model adaptation, and parameter-efficient fine-tuning (PEFT) strategies. Our results demonstrate that TSFMs outperform task-specific baselines, achieving notable gains in AUC-PR and VUS-PR, particularly under severe class imbalance. Moreover, PEFT methods such as LoRA, OFT, and HRA not only reduce computational cost but also match or surpass full fine-tuning in most cases, indicating that TSFMs can be efficiently adapted for anomaly detection, even when pretrained for forecasting. These findings position TSFMs as promising general-purpose models for scalable and efficient time series anomaly detection.

</details>


### [46] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 提出可控概念瓶颈模型(CCBMs)，支持概念-标签级、概念级和数据级三种粒度的模型编辑，无需重新训练即可实现动态维护。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型主要针对静态场景，而实际应用中需要持续维护：删除错误/敏感数据（遗忘）、修正错误标注概念、纳入新样本（增量学习）。传统方法需要从头训练，在大规模应用中效率低下。

Method: 基于影响函数推导出数学上严格的闭式近似解，支持三种粒度编辑：概念-标签级、概念级、数据级（包括数据删除和添加）。

Result: 实验证明CCBMs在效率和适应性方面表现优异，能够实现动态可信的概念瓶颈模型。

Conclusion: CCBMs为解决概念瓶颈模型在实际部署中的动态维护问题提供了高效解决方案，具有实际应用价值。

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [47] [Imitation from Observations with Trajectory-Level Generative Embeddings](https://arxiv.org/abs/2601.00452)
*Yongtao Qu,Shangzhe Li,Weitong Zhang*

Main category: cs.LG

TL;DR: TGE：一种用于离线观察模仿学习的轨迹级生成嵌入方法，通过扩散模型潜在空间估计专家状态密度来构建密集平滑的替代奖励，有效处理专家演示稀缺且离线数据分布与专家行为差异大的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有分布匹配方法在专家演示稀缺且离线次优数据与专家行为差异大的情况下表现不佳，因为它们施加严格的支持约束并依赖脆弱的单步模型，难以从不完美数据中提取有用信号。

Method: 提出TGE（轨迹级生成嵌入），通过在离线轨迹数据训练的时序扩散模型的潜在空间中估计专家状态密度，构建密集平滑的替代奖励。利用学习到的扩散嵌入的平滑几何结构，捕获长期时序动态并有效弥合不相交支持之间的差距。

Result: 在D4RL运动和控制基准测试中，该方法始终匹配或优于先前的离线观察模仿学习方法。

Conclusion: TGE通过轨迹级生成嵌入有效解决了离线观察模仿学习中专家演示稀缺和数据分布差异大的挑战，提供了一种鲁棒的学习信号提取方法。

Abstract: We consider the offline imitation learning from observations (LfO) where the expert demonstrations are scarce and the available offline suboptimal data are far from the expert behavior. Many existing distribution-matching approaches struggle in this regime because they impose strict support constraints and rely on brittle one-step models, making it hard to extract useful signal from imperfect data. To tackle this challenge, we propose TGE, a trajectory-level generative embedding for offline LfO that constructs a dense, smooth surrogate reward by estimating expert state density in the latent space of a temporal diffusion model trained on offline trajectory data. By leveraging the smooth geometry of the learned diffusion embedding, TGE captures long-horizon temporal dynamics and effectively bridges the gap between disjoint supports, ensuring a robust learning signal even when offline data is distributionally distinct from the expert. Empirically, the proposed approach consistently matches or outperforms prior offline LfO methods across a range of D4RL locomotion and manipulation benchmarks.

</details>


### [48] [Deep Networks Learn Deep Hierarchical Models](https://arxiv.org/abs/2601.00455)
*Amit Daniely*

Main category: cs.LG

TL;DR: 该论文研究了残差网络中分层SGD如何高效学习层次化模型，这类模型超越了先前可学习模型的深度限制，并探讨了教师提供的层次化标签如何揭示大脑内部算法，为理解深度学习提供基础。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习中残差网络如何高效学习复杂的层次化模型，这类模型超越了先前可学习模型的表达能力限制，并探索人类教师提供的层次化标签如何揭示大脑内部算法结构，为理解深度学习成功提供理论基础。

Method: 采用分层随机梯度下降（layerwise SGD）在残差网络上学习层次化模型。模型假设存在未知的标签层次结构 L₁ ⊆ L₂ ⊆ ... ⊆ Lᵣ = [n]，其中L₁中的标签是输入的简单函数，而i>1时，Lᵢ中的标签是更简单标签的简单函数。

Result: 证明该模型类超越了先前可被深度学习算法学习的模型，达到了高效可学习性的深度极限。存在需要多项式深度才能表达的模型，而先前模型只需对数深度电路即可计算。同时形式化了教师部分了解其内部逻辑的简化模型，展示了层次结构的出现如何促进高效学习。

Conclusion: 层次化模型的学习能力可能最终成为理解深度学习的基础。这类模型不仅自然契合深度学习擅长的领域，而且人类教师的存在支持了层次结构内在可用的假设。教师提供的细粒度标签有效地揭示了大脑使用的内部算法"提示"或"片段"。

Abstract: We consider supervised learning with $n$ labels and show that layerwise SGD on residual networks can efficiently learn a class of hierarchical models. This model class assumes the existence of an (unknown) label hierarchy $L_1 \subseteq L_2 \subseteq \dots \subseteq L_r = [n]$, where labels in $L_1$ are simple functions of the input, while for $i > 1$, labels in $L_i$ are simple functions of simpler labels.
  Our class surpasses models that were previously shown to be learnable by deep learning algorithms, in the sense that it reaches the depth limit of efficient learnability. That is, there are models in this class that require polynomial depth to express, whereas previous models can be computed by log-depth circuits.
  Furthermore, we suggest that learnability of such hierarchical models might eventually form a basis for understanding deep learning. Beyond their natural fit for domains where deep learning excels, we argue that the mere existence of human ``teachers" supports the hypothesis that hierarchical structures are inherently available. By providing granular labels, teachers effectively reveal ``hints'' or ``snippets'' of the internal algorithms used by the brain. We formalize this intuition, showing that in a simplified model where a teacher is partially aware of their internal logic, a hierarchical structure emerges that facilitates efficient learnability.

</details>


### [49] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 正交性损失无法有效提升MoE模型专家多样性，反而增加权重空间重叠，对性能影响不一致且不可靠


<details>
  <summary>Details</summary>
Motivation: 研究几何正则化在MoE模型专家专业化中的作用，探索正交性损失是否能有效增强专家多样性

Method: 在MoE模型中应用正交性损失，分析7种不同正则化强度下权重空间和激活空间的重叠情况

Result: 正交性损失失败：权重空间重叠增加114%，激活空间重叠保持高位(~0.6)，性能影响不一致（WikiText-103轻微改善，TinyStories轻微下降，PTB结果高度波动）

Conclusion: 权重空间正则化既不能实现其几何目标，也不能可靠提升性能，不适合用于MoE多样性增强

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [50] [Neural Chains and Discrete Dynamical Systems](https://arxiv.org/abs/2601.00473)
*Sauro Succi,Abhisek Ganguly,Santosh Ansumali*

Main category: cs.LG

TL;DR: 论文比较了无自注意力Transformer架构（神经链）与数值离散化方法在求解偏微分方程时的表现，发现PINNs使用随机矩阵而传统数值方法使用结构化矩阵，两者都能获得相似的系统动力学知识，但PINNs参数更多、训练成本更高且缺乏物理可解释性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索机器学习（特别是无自注意力的Transformer架构）与传统数值方法在求解偏微分方程方面的类比关系，比较两种方法在获取系统动力学知识方面的异同。

Method: 通过比较标准数值离散化方法（有限差分法）和物理信息神经网络（PINNs）在求解Burgers方程和Eikonal方程时的表现，将两种方法都表述为神经链形式进行分析。

Result: 研究发现标准数值离散化和PINN学习提供了两种不同的路径来获取基本相同的系统动力学知识。PINNs使用随机矩阵，而有限差分法使用高度结构化的三对角矩阵。可接受的随机矩阵数量远多于唯一的矩阵形式，这解释了PINN搜索通常落在随机集合中的原因。

Conclusion: PINNs需要更多参数，导致缺乏物理透明性（可解释性）和较高的训练成本，而有限差分法则没有这些问题。但研究仅针对一维动态问题，不排除PINNs和机器学习在高维问题中可能提供更好的策略。

Abstract: We inspect the analogy between machine-learning (ML) applications based on the transformer architecture without self-attention, {\it neural chains} hereafter, and discrete dynamical systems associated with discretised versions of neural integral and partial differential equations (NIE, PDE). A comparative analysis of the numerical solution of the (viscid and inviscid) Burgers and Eikonal equations via standard numerical discretization (also cast in terms of neural chains) and via PINN's learning is presented and commented on. It is found that standard numerical discretization and PINN learning provide two different paths to acquire essentially the same knowledge about the dynamics of the system. PINN learning proceeds through random matrices which bear no direct relation to the highly structured matrices associated with finite-difference (FD) procedures. Random matrices leading to acceptable solutions are far more numerous than the unique tridiagonal form in matrix space, which explains why the PINN search typically lands on the random ensemble. The price is a much larger number of parameters, causing lack of physical transparency (explainability) as well as large training costs with no counterpart in the FD procedure. However, our results refer to one-dimensional dynamic problems, hence they don't rule out the possibility that PINNs and ML in general, may offer better strategies for high-dimensional problems.

</details>


### [51] [When Small Models Are Right for Wrong Reasons: Process Verification for Trustworthy Agents](https://arxiv.org/abs/2601.00513)
*Laksh Advani*

Main category: cs.LG

TL;DR: 研究发现小型语言模型（7-9B参数）作为自主代理时存在严重可靠性危机：50-69%的正确答案包含根本性错误推理，出现"正确但理由错误"现象，标准准确率指标无法检测。


<details>
  <summary>Details</summary>
Motivation: 部署小型语言模型作为自主代理需要信任其推理过程，而不仅仅是输出结果。现有标准准确率指标无法检测模型推理过程中的根本性缺陷，存在安全隐患。

Method: 通过分析10,734个推理轨迹，引入推理完整性评分（RIS）作为过程评估指标，验证了其高评分者间信度（κ=0.657）。研究了检索增强生成（RAG）和元认知干预对推理完整性的影响，并通过机制分析揭示了其作用原理。

Result: RAG显著提高推理完整性（Cohen's d=0.23-0.93），减少7.6%的错误；而元认知干预在小模型上往往损害性能（d=-0.14到-0.33）。开发了神经分类器实现验证能力，达到0.86 F1分数和100倍加速。

Conclusion: 仅依赖准确率指标是危险的，因为模型可能基于完全错误的推理得到正确答案。过程验证对于可信代理部署至关重要，需要开发基于过程的评估方法。

Abstract: Deploying small language models (7-9B parameters) as autonomous agents requires trust in their reasoning, not just their outputs. We reveal a critical reliability crisis: 50-69\% of correct answers from these models contain fundamentally flawed reasoning -- a ``Right-for-Wrong-Reasons'' phenomenon invisible to standard accuracy metrics. Through analysis of 10,734 reasoning traces across three models and diverse tasks, we introduce the Reasoning Integrity Score (RIS), a process-based metric validated with substantial inter-rater agreement ($κ=0.657$). Conventional practices are challenged by our findings: while retrieval-augmented generation (RAG) significantly improves reasoning integrity (Cohen's $d=0.23$--$0.93$), meta-cognitive interventions like self-critique often harm performance ($d=-0.14$ to $-0.33$) in small models on the evaluated tasks. Mechanistic analysis reveals RAG succeeds by grounding calculations in external evidence, reducing errors by 7.6\%, while meta-cognition amplifies confusion without sufficient model capacity. To enable deployment, verification capabilities are distilled into a neural classifier achieving 0.86 F1-score with 100$\times$ speedup. These results underscore the necessity of process-based verification for trustworthy agents: accuracy alone is dangerously insufficient when models can be right for entirely wrong reasons.

</details>


### [52] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

TL;DR: Trajectory Guard：一种用于检测LLM智能体多步行动计划异常的Siamese循环自编码器，通过对比学习和重构损失联合学习任务-轨迹对齐与序列有效性，在多个基准测试中达到0.88-0.94的F1分数，比LLM Judge基线快17-27倍。


<details>
  <summary>Details</summary>
Motivation: 自主LLM智能体生成的多步行动计划可能因上下文不对齐或结构不连贯而失败。现有异常检测方法不适用于此挑战：均值池化嵌入会稀释异常步骤，而仅对比方法忽略序列结构。标准无监督方法在预训练嵌入上F1分数不超过0.69。

Method: 提出Trajectory Guard，一种Siamese循环自编码器，具有混合损失函数，通过对比学习联合学习任务-轨迹对齐，通过重构学习序列有效性。这种双重目标能够统一检测"错误的任务计划"和"畸形计划结构"。

Result: 在涵盖合成扰动和真实世界失败的基准测试中（RAS-Eval安全审计和Who&When多智能体系统），在平衡数据集上达到0.88-0.94的F1分数，在不平衡外部基准上达到0.86-0.92的召回率。推理延迟32毫秒，比LLM Judge基线快17-27倍。

Conclusion: Trajectory Guard能够有效检测LLM智能体行动计划的异常，支持实时安全验证，适用于生产部署。该方法解决了现有方法在序列异常检测上的局限性，实现了高效准确的双重异常检测。

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [53] [A Sparse-Attention Deep Learning Model Integrating Heterogeneous Multimodal Features for Parkinson's Disease Severity Profiling](https://arxiv.org/abs/2601.00519)
*Dristi Datta,Tanmoy Debnath,Minh Chau,Manoranjan Paul,Gourab Adhikary,Md Geaur Rahman*

Main category: cs.LG

TL;DR: 提出SAFN框架，通过稀疏注意力机制融合多模态数据，用于帕金森病的异质性表征，在PPMI数据集上达到98%准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病具有异质性表现，需要整合生物和临床标志物。现有多模态模型存在可解释性差、类别不平衡、高维特征融合困难等问题。

Method: 提出SAFN框架：使用模态特定编码器处理MRI皮层厚度、MRI体积测量、临床评估和人口统计学变量；采用对称交叉注意力机制捕捉非线性交互；稀疏约束注意力门控层动态选择信息模态；类别平衡焦点损失处理数据不平衡。

Result: 在PPMI数据集703名参与者上，五折交叉验证准确率0.98±0.02，PR-AUC 1.00±0.00，优于现有基准。可解释性分析显示约60%预测权重分配给临床评估，符合临床诊断原则。

Conclusion: SAFN为神经退行性疾病的计算分析提供了可重复、透明的多模态建模范式，具有临床可解释的决策过程。

Abstract: Characterising the heterogeneous presentation of Parkinson's disease (PD) requires integrating biological and clinical markers within a unified predictive framework. While multimodal data provide complementary information, many existing computational models struggle with interpretability, class imbalance, or effective fusion of high-dimensional imaging and tabular clinical features. To address these limitations, we propose the Class-Weighted Sparse-Attention Fusion Network (SAFN), an interpretable deep learning framework for robust multimodal profiling. SAFN integrates MRI cortical thickness, MRI volumetric measures, clinical assessments, and demographic variables using modality-specific encoders and a symmetric cross-attention mechanism that captures nonlinear interactions between imaging and clinical representations. A sparsity-constrained attention-gating fusion layer dynamically prioritises informative modalities, while a class-balanced focal loss (beta = 0.999, gamma = 1.5) mitigates dataset imbalance without synthetic oversampling. Evaluated on 703 participants (570 PD, 133 healthy controls) from the Parkinson's Progression Markers Initiative using subject-wise five-fold cross-validation, SAFN achieves an accuracy of 0.98 plus or minus 0.02 and a PR-AUC of 1.00 plus or minus 0.00, outperforming established machine learning and deep learning baselines. Interpretability analysis shows a clinically coherent decision process, with approximately 60 percent of predictive weight assigned to clinical assessments, consistent with Movement Disorder Society diagnostic principles. SAFN provides a reproducible and transparent multimodal modelling paradigm for computational profiling of neurodegenerative disease.

</details>


### [54] [Optimizing LSTM Neural Networks for Resource-Constrained Retail Sales Forecasting: A Model Compression Study](https://arxiv.org/abs/2601.00525)
*Ravi Teja Pagidoju*

Main category: cs.LG

TL;DR: LSTM模型压缩研究：通过减少隐藏单元从128到16，发现64单元模型在保持准确性的同时，模型大小减少73%，准确率提升47%。


<details>
  <summary>Details</summary>
Motivation: 标准LSTM神经网络在零售业销售预测中准确但计算量大，对中小型零售业具有挑战性。研究LSTM模型压缩，探索模型大小与预测准确性的权衡。

Method: 使用Kaggle Store Item Demand Forecasting数据集（91.3万条日销售记录），逐步减少LSTM隐藏单元数量（从128到16），比较不同规模模型的性能。

Result: 64隐藏单元模型表现最佳：MAPE从128单元的23.6%降至12.4%，模型大小从280KB减至76KB（减少73%），准确率提升47%。

Conclusion: 更大的模型并不总是更好，通过适当压缩LSTM模型可以在显著减小模型大小的同时提高预测准确性，这对资源受限的中小型零售业具有实际应用价值。

Abstract: Standard LSTM(Long Short-Term Memory) neural networks provide accurate predictions for sales data in the retail industry, but require a lot of computing power. It can be challenging especially for mid to small retail industries. This paper examines LSTM model compression by gradually reducing the number of hidden units from 128 to 16. We used the Kaggle Store Item Demand Forecasting dataset, which has 913,000 daily sales records from 10 stores and 50 items, to look at the trade-off between model size and how accurate the predictions are. Experiments show that lowering the number of hidden LSTM units to 64 maintains the same level of accuracy while also improving it. The mean absolute percentage error (MAPE) ranges from 23.6% for the full 128-unit model to 12.4% for the 64-unit model. The optimized model is 73% smaller (from 280KB to 76KB) and 47% more accurate. These results show that larger models do not always achieve better results.

</details>


### [55] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文探讨了大型模型在联邦学习框架下的定制化方法，首次在联邦学习中应用了prefix-tuning技术，验证了其可行性，并与集中式方法性能接近。


<details>
  <summary>Details</summary>
Motivation: 探索大型模型在联邦学习框架下的定制化挑战，研究如何在保护数据隐私的同时实现模型个性化定制。

Method: 综述了多种大型模型定制技术（全微调、高效微调、提示工程、前缀调优、知识蒸馏、检索增强生成），并讨论了它们在联邦学习框架下的实现方式。重点实验了联邦前缀调优技术。

Result: 联邦前缀调优在实验中表现出可行性，性能接近集中式方法。与其他三种联邦定制方法相比，展现出竞争性性能、满意效率和一致鲁棒性。

Conclusion: 联邦前缀调优是大型模型联邦定制化的有效方法，为在保护数据隐私的前提下实现模型个性化提供了可行方案。

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [56] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

TL;DR: 本文提出了一种基于扩散模型的云原生架构，用于自动生成商店特定的货架陈列图，将设计时间从30小时减少到0.5小时（减少98.3%），同时实现94.4%的约束满足率。


<details>
  <summary>Details</summary>
Motivation: 货架陈列图创建是零售业面临的重要挑战，每个复杂布局平均需要30小时。传统优化方法通常重新组织现有布局，而本文旨在通过学习多个零售地点的成功货架安排，自动生成新的陈列图配置。

Method: 采用云原生架构，结合AWS进行云端模型训练和边缘部署实现实时推理。使用扩散模型，并通过修改损失函数集成零售特定约束。系统能够从多个零售地点的成功货架安排中学习。

Result: 系统将货架陈列图设计时间减少98.3%（从30小时降至0.5小时），约束满足率达到94.4%。经济分析显示创建费用减少97.5%，盈亏平衡期为4.4个月。云原生架构支持线性扩展，可处理多达10,000个并发商店请求。

Conclusion: 这项工作证明了生成式AI在自动化零售空间优化中的可行性，通过云原生扩散模型架构实现了高效的货架陈列图自动生成。

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [57] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 提出基于熵的再训练框架，将数据漂移建模为概率流，通过熵平衡分解触发再训练，在非平稳分类任务中显著减少再训练次数


<details>
  <summary>Details</summary>
Motivation: 现有漂移检测方法缺乏动力学原理解释，无法指导再训练频率与运营成本之间的平衡，需要更理论化的框架

Method: 将部署时数据漂移建模为Fokker-Planck方程控制的概率流，使用时变KL散度量模型-数据不匹配，通过熵平衡分解（含非负熵产生项）设计熵触发再训练策略

Result: 在受控非平稳分类实验中，熵触发再训练达到与高频再训练相当的预测性能，同时将再训练事件减少一个数量级（相比每日和基于标签的策略）

Conclusion: 基于非平衡随机动力学的熵触发再训练框架为数据漂移管理提供了理论依据，实现了性能与成本的有效平衡

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [58] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

TL;DR: 论文提出需要区分两种对抗性弱点：利用非鲁棒特征的攻击和不利用这些特征的攻击，并提出了基于集成的方法来测量对抗扰动对非鲁棒特征的操纵程度。


<details>
  <summary>Details</summary>
Motivation: 现有非鲁棒特征理论虽然被广泛接受，但忽略了不直接利用这些特征的对抗样本。论文认为这两种样本代表了不同类型的对抗性弱点，需要区分评估对抗鲁棒性。

Method: 提出了基于集成的度量方法，用于测量对抗扰动对非鲁棒特征的操纵程度，并用该度量分析攻击者生成的对抗样本的构成。

Result: 通过新度量方法分析对抗样本构成，并重新审视了多个现象，包括锐度感知最小化对对抗鲁棒性的影响，以及在鲁棒数据集上对抗训练与标准训练之间的鲁棒性差距。

Conclusion: 区分两种对抗性弱点对于准确评估对抗鲁棒性至关重要，新提出的度量方法为分析对抗样本构成和重新理解相关现象提供了新视角。

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [59] [Learning to be Reproducible: Custom Loss Design for Robust Neural Networks](https://arxiv.org/abs/2601.00578)
*Waqas Ahmed,Sheeba Samuel,Kevin Coakley,Birgitta Koenig-Ries,Odd Erik Gundersen*

Main category: cs.LG

TL;DR: 提出自定义损失函数(CLF)来减少训练结果对随机因素的敏感性，提高深度学习模型的可重复性和可靠性


<details>
  <summary>Details</summary>
Motivation: 当前训练方法缺乏确保跨运行一致性和鲁棒性的机制，即使在受控的初始化和训练条件下，模型准确率仍存在显著变异性

Method: 提出自定义损失函数(CLF)，通过微调参数来平衡预测准确率和训练稳定性，减少对权重初始化和数据洗牌等随机因素的敏感性

Result: 在图像分类和时间序列预测的多种架构上进行广泛实验，证明CLF能显著提高训练鲁棒性而不牺牲预测性能

Conclusion: CLF是开发更稳定、可靠和可信赖神经网络的有效且高效策略

Abstract: To enhance the reproducibility and reliability of deep learning models, we address a critical gap in current training methodologies: the lack of mechanisms that ensure consistent and robust performance across runs. Our empirical analysis reveals that even under controlled initialization and training conditions, the accuracy of the model can exhibit significant variability. To address this issue, we propose a Custom Loss Function (CLF) that reduces the sensitivity of training outcomes to stochastic factors such as weight initialization and data shuffling. By fine-tuning its parameters, CLF explicitly balances predictive accuracy with training stability, leading to more consistent and reliable model performance. Extensive experiments across diverse architectures for both image classification and time series forecasting demonstrate that our approach significantly improves training robustness without sacrificing predictive performance. These results establish CLF as an effective and efficient strategy for developing more stable, reliable and trustworthy neural networks.

</details>


### [60] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: HFedMoE是一个面向异构客户端的MoE联邦学习框架，通过专家重要性评估、自适应专家选择和稀疏感知聚合，实现资源受限设备上的高效LLM微调。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在资源受限设备上微调大语言模型不切实际，而MoE模型虽然计算高效，但在联邦学习环境中面临三大挑战：专家选择缺乏可靠指标、异构计算资源限制、客户端特定专家子集破坏全局聚合。

Method: 1) 基于专家对微调性能的贡献评估专家重要性；2) 从信息瓶颈角度自适应选择专家子集以匹配客户端计算预算；3) 设计稀疏感知模型聚合策略，加权聚合活跃微调的专家和门控参数。

Result: 大量实验表明，HFedMoE在训练准确率和收敛速度方面优于现有最先进基准方法。

Conclusion: HFedMoE成功解决了MoE在联邦学习中的关键挑战，为资源受限设备上的高效LLM微调提供了可行方案，在保持隐私的同时实现了计算效率。

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [61] [Cycling Race Time Prediction: A Personalized Machine Learning Approach Using Route Topology and Training Load](https://arxiv.org/abs/2601.00604)
*Francisco Aguilera Moreno*

Main category: cs.LG

TL;DR: 使用机器学习预测骑行时长，结合路线拓扑特征和运动员当前体能状态（训练负荷指标），相比仅用拓扑特征减少14%误差


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模型的方法需要大量参数（如空气阻力系数、实时风速预测），这对业余骑行者不实用。需要一种更简单的方法来准确预测骑行时长，用于训练规划和赛事准备。

Method: 采用机器学习方法，使用路线拓扑特征和运动员当前体能状态（从训练负荷指标推导）来预测骑行时长。模型从历史数据中学习运动员特定的表现模式，用历史表现代理替代复杂的物理测量。使用N-of-1研究设计，对单个运动员数据集（N=96次骑行）进行严格的特征工程以避免数据泄露。

Result: 使用拓扑+体能特征的Lasso回归模型达到MAE=6.60分钟和R2=0.922。整合体能指标（CTL, ATL）相比仅用拓扑特征（MAE=7.66分钟）减少14%误差，表明生理状态对自定节奏的表现有显著约束作用。渐进检查点预测支持动态比赛规划。

Conclusion: 机器学习方法能够有效预测骑行时长，无需复杂的物理参数。结合路线拓扑和运动员体能状态可以显著提高预测准确性，为训练规划和赛事准备提供实用工具。

Abstract: Predicting cycling duration for a given route is essential for training planning and event preparation. Existing solutions rely on physics-based models that require extensive parameterization, including aerodynamic drag coefficients and real-time wind forecasts, parameters impractical for most amateur cyclists. This work presents a machine learning approach that predicts ride duration using route topology features combined with the athlete's current fitness state derived from training load metrics. The model learns athlete-specific performance patterns from historical data, substituting complex physical measurements with historical performance proxies. We evaluate the approach using a single-athlete dataset (N=96 rides) in an N-of-1 study design. After rigorous feature engineering to eliminate data leakage, we find that Lasso regression with Topology + Fitness features achieves MAE=6.60 minutes and R2=0.922. Notably, integrating fitness metrics (CTL, ATL) reduces error by 14% compared to topology alone (MAE=7.66 min), demonstrating that physiological state meaningfully constrains performance even in self-paced efforts. Progressive checkpoint predictions enable dynamic race planning as route difficulty becomes apparent.

</details>


### [62] [Traffic-Aware Optimal Taxi Placement Using Graph Neural Network-Based Reinforcement Learning](https://arxiv.org/abs/2601.00607)
*Sonia Khetarpaul,P Y Sharan*

Main category: cs.LG

TL;DR: 提出基于图神经网络和强化学习的交通感知出租车热点推荐框架，在模拟德里数据上减少56%乘客等待时间和38%行驶距离


<details>
  <summary>Details</summary>
Motivation: 传统出租车热点预测模型仅依赖历史需求数据，忽略了交通拥堵、道路事故、公共活动等动态因素影响，无法实现智能城市交通中出租车供需的实时高效匹配

Method: 将城市道路网络建模为图（节点为交叉口，边为路段），使用GNN编码时空依赖关系，结合Q-learning智能体推荐最优出租车热点位置，奖励机制联合优化乘客等待时间、司机行驶距离和拥堵避免

Result: 在模拟德里出租车数据集上，相比基线随机选择方法，乘客等待时间减少约56%，行驶距离减少38%

Conclusion: 该框架可适应多模式交通系统，能够集成到智能城市平台中实现实时城市移动性优化

Abstract: In the context of smart city transportation, efficient matching of taxi supply with passenger demand requires real-time integration of urban traffic network data and mobility patterns. Conventional taxi hotspot prediction models often rely solely on historical demand, overlooking dynamic influences such as traffic congestion, road incidents, and public events. This paper presents a traffic-aware, graph-based reinforcement learning (RL) framework for optimal taxi placement in metropolitan environments. The urban road network is modeled as a graph where intersections represent nodes, road segments serve as edges, and node attributes capture historical demand, event proximity, and real-time congestion scores obtained from live traffic APIs. Graph Neural Network (GNN) embeddings are employed to encode spatial-temporal dependencies within the traffic network, which are then used by a Q-learning agent to recommend optimal taxi hotspots. The reward mechanism jointly optimizes passenger waiting time, driver travel distance, and congestion avoidance. Experiments on a simulated Delhi taxi dataset, generated using real geospatial boundaries and historic ride-hailing request patterns, demonstrate that the proposed model reduced passenger waiting time by about 56% and reduced travel distance by 38% compared to baseline stochastic selection. The proposed approach is adaptable to multi-modal transport systems and can be integrated into smart city platforms for real-time urban mobility optimization.

</details>


### [63] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 提出一种用于在向下封闭凸体上最大化非单调γ-弱DR-次模函数的近似算法，当γ=1时恢复0.401近似比，γ<1时性能优雅下降


<details>
  <summary>Details</summary>
Motivation: 在机器学习和优化中，最大化约束下的次模目标是一个基本问题。需要处理非单调、非负的γ-弱DR-次模函数在向下封闭凸体上的最大化问题，现有方法在γ<1时的性能需要改进。

Method: 结合Frank-Wolfe引导的连续贪婪框架与γ感知的双贪婪步骤，形成简单有效的处理非单调性的方法。

Result: 算法保证平滑依赖于γ：γ=1时恢复0.401近似比，γ<1时性能优雅下降，改进了相同约束下γ-弱DR-次模最大化的先前界限，达到了最先进水平。

Conclusion: 该方法为在向下封闭凸体上最大化非单调γ-弱DR-次模函数提供了简单有效的算法，获得了最先进的近似保证，性能随γ平滑变化。

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [64] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

TL;DR: YapBench是一个轻量级基准测试，用于量化LLM在需要简洁回答的场景下的过度生成问题，通过测量超出最小必要答案的字符数来评估模型的冗余程度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在简单请求上经常给出不必要的冗长回答，包含冗余解释、模棱两可的表述和模板化内容，这会增加认知负担和推理成本。先前研究表明基于偏好的后训练和LLM评估可能导致系统性长度偏差，即更长的回答即使质量相当也会获得更高评分。

Method: 引入YapBench基准测试，包含300多个英语提示，涵盖三种需要简洁回答的场景：模糊输入需要简短澄清、封闭式事实问题、单行编码任务。每个项目包括单轮提示、策划的最小充分基线答案和类别标签。主要指标YapScore测量超出基线答案的字符数，YapIndex是类别级别中位数YapScore的均匀加权平均值。

Result: 评估76个助手LLM发现，中位数超额长度存在数量级差异，并观察到特定类别的失败模式：在模糊输入上填充真空内容，在单行技术请求上添加解释或格式化开销。

Conclusion: YapBench提供了一个量化LLM过度生成问题的标准化方法，揭示了不同模型在简洁性方面的显著差异和特定失败模式，有助于跟踪模型简洁性行为随时间的变化。

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [65] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

TL;DR: IGBO框架通过双目标优化训练可解释模型，利用DAG编码特征重要性层次结构，使用TIG测量特征重要性，并提出最优路径预言机解决TIG中的OOD问题。


<details>
  <summary>Details</summary>
Motivation: 当前可解释模型训练中缺乏结构化领域知识的有效整合方法，且传统特征重要性测量方法存在分布外问题，需要更鲁棒的优化框架。

Method: 1) 将特征重要性层次编码为有向无环图；2) 使用时序积分梯度测量特征重要性；3) 提出最优路径预言机学习数据流形感知的积分路径；4) 采用双目标优化框架平衡可解释性和准确性。

Result: 理论分析证明了收敛性和对mini-batch噪声的鲁棒性；在时间序列数据上的实验表明，IGBO能有效实施DAG约束，精度损失最小，优于标准正则化基线方法。

Conclusion: IGBO框架成功整合了结构化领域知识到模型训练中，通过双目标优化平衡了可解释性和准确性，为解决可解释AI中的分布外问题提供了有效方案。

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [66] [Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation](https://arxiv.org/abs/2601.00664)
*Taekyung Ki,Sangwon Jang,Jaehyeong Jo,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出Avatar Forcing框架，通过扩散强迫建模实时用户-虚拟形象交互，实现低延迟（约500ms）的互动式头像生成，无需额外标注数据学习表达性反应。


<details>
  <summary>Details</summary>
Motivation: 当前说话头像生成模型缺乏真正的互动感，生成单向响应且缺乏情感参与。需要解决两个关键挑战：在因果约束下实时生成运动，以及无需额外标注数据学习表达性反应。

Method: 提出Avatar Forcing框架，通过扩散强迫建模实时用户-虚拟形象交互，处理多模态输入（音频和动作）。引入直接偏好优化方法，利用丢弃用户条件构建的合成负样本进行无标签学习。

Result: 框架实现低延迟实时交互（约500ms），相比基线加速6.8倍。生成的虚拟形象动作反应性和表达性更强，在80%以上的对比中优于基线。

Conclusion: Avatar Forcing框架成功解决了交互式虚拟形象生成的关键挑战，实现了低延迟实时交互和表达性反应学习，为真正互动的虚拟通信提供了有效解决方案。

Abstract: Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.

</details>


### [67] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: IRPO提出了一种新的强化学习框架，通过将Bradley-Terry模型集成到GRPO中，使用点式评分替代成对比较，解决了生成奖励模型在RL中的计算瓶颈问题，实现了高效训练并保持可解释性。


<details>
  <summary>Details</summary>
Motivation: 生成奖励模型（GRMs）因其可解释性、推理时扩展性和通过强化学习进行优化的潜力而受到关注，但广泛使用的成对GRMs在与GRPO等RL算法集成时存在计算瓶颈：1）获取相对评分的成对比较需要O(n²)时间复杂度；2）重复采样或额外思维链推理带来的计算开销。

Method: 提出Intergroup Relative Preference Optimization（IRPO）框架，将Bradley-Terry模型集成到Group Relative Policy Optimization（GRPO）中。通过为每个响应生成点式评分，IRPO能够在RL训练期间高效评估任意数量的候选响应，同时保持可解释性和细粒度奖励信号。

Result: IRPO在多个基准测试中实现了点式GRMs中的最先进性能，与当前领先的成对GRMs性能相当。此外，在后训练评估中，IRPO显著优于成对GRMs。

Conclusion: IRPO通过点式评分方法有效解决了成对GRMs在RL训练中的计算瓶颈问题，在保持可解释性的同时实现了高效训练，并在性能上达到或超越了现有方法。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [68] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

TL;DR: TeleDoCTR：面向电信领域票务故障排查的端到端系统，集成领域特定排序和生成模型，自动化分类、检索和生成任务，显著提升故障排查效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 电信领域的票务故障排查是高度复杂且耗时的任务，需要专家解读票务内容、查阅文档和搜索历史记录。这种人工密集型方法不仅延迟问题解决，还阻碍整体运营效率。需要自动化系统来提升电信票务故障排查的效率和效果。

Method: 提出TeleDoCTR系统，这是一个电信相关的、领域特定的、上下文感知的故障排查系统。系统集成领域特定排序模型和生成模型，自动化故障排查工作流程的三个关键步骤：1) 票务分类（路由到适当的专家团队）；2) 检索上下文和语义相似的历史票务；3) 生成详细的故障分析报告（包括问题、根本原因和潜在解决方案）。

Result: 在真实世界的电信基础设施数据集上评估TeleDoCTR，证明其在现有最先进方法上取得优越性能，显著提升了故障排查过程的准确性和效率。

Conclusion: TeleDoCTR通过集成领域特定排序和生成模型，为电信票务故障排查提供了一个有效的端到端解决方案，能够自动化关键工作流程步骤，显著改善运营效率和问题解决速度。

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [69] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 提出贝叶斯逆博弈框架，通过结构化变分自编码器学习智能体目标的后验分布，解决传统最大似然估计过度自信的问题，实现更安全的下游决策。


<details>
  <summary>Details</summary>
Motivation: 传统逆博弈方法使用最大似然估计只能得到点估计，无法量化估计不确定性，导致下游规划决策可能过度自信地采取不安全行动。需要一种能处理不确定性并支持多模态观测的贝叶斯方法。

Method: 提出贝叶斯逆博弈框架，使用结构化变分自编码器（嵌入可微分纳什博弈求解器）在交互数据集上训练，无需真实目标标签。支持多模态观测融合，实时生成隐藏目标的后验分布样本。

Result: 框架成功学习先验和后验分布，相比最大似然估计方法提高了推理质量，实现了更安全的下游决策而不牺牲效率。当轨迹信息不充分时，多模态推理进一步降低不确定性。

Conclusion: 贝叶斯逆博弈方法能有效量化不确定性，通过多模态观测减少不确定性，为自主决策提供更安全的规划基础，优于传统点估计方法。

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [70] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出BSAT（B样条自适应分词器）和L-RoPE混合位置编码，用于解决长时序预测中自注意力二次复杂度和均匀分块不匹配语义结构的问题。


<details>
  <summary>Details</summary>
Motivation: 传统transformer在长时序预测中存在两个主要问题：1）自注意力的二次计算复杂度限制了处理长序列的能力；2）均匀分块方式可能无法与数据的语义结构对齐，导致信息损失。

Method: 1. BSAT：参数自由的自适应分词器，使用B样条拟合时间序列，在高曲率区域放置token，将变长基函数表示为固定大小的token（包含系数和位置信息）。2. L-RoPE：混合位置编码，结合可学习的加性位置编码和具有层间可学习基的旋转位置编码，使不同层能关注不同的时序依赖关系。

Result: 在多个公开基准测试中表现出色，在高压缩率下仍保持强大性能，特别适合内存受限的应用场景。

Conclusion: BSAT和L-RoPE的组合有效解决了长时序预测中的计算效率和语义对齐问题，为内存受限场景提供了高效的解决方案。

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [71] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应精度调优框架，用于线性求解器，通过上下文多臂老虎机问题动态选择计算步骤的精度配置，平衡精度与计算效率。


<details>
  <summary>Details</summary>
Motivation: 在科学计算中，混合精度方法可以显著提高计算效率，但手动选择精度配置复杂且耗时。需要一种自动化的方法来动态调整计算精度，在保证精度的同时最小化计算成本。

Method: 将精度调优建模为上下文多臂老虎机问题，使用离散化状态空间和增量动作价值估计。通过Q表映射离散化特征（如近似条件数、矩阵范数）到精度配置动作，采用epsilon-greedy策略优化多目标奖励函数，平衡精度和计算成本。

Result: 在线性系统迭代精化应用中，该框架能有效选择精度配置，在保持与双精度基准相当的精度同时，显著降低计算成本。框架在未见数据集上表现出良好的泛化能力。

Conclusion: 这是首个基于强化学习的精度自动调优工作，在未见数据集上验证有效。该框架可推广到其他数值算法，推动了科学计算中混合精度方法的发展。

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [72] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 本文提出分布创造性推理（DCR）框架，分析当前LLM推理循环中多样性衰减问题，并提供保持正确性和创造性的理论方案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理循环主要优化正确性，导致推理路径分布崩溃、语义熵降低，削弱创造性问题解决能力。需要理论框架分析这种失败并找到保持多样性的方法。

Method: 提出分布创造性推理（DCR）框架，将训练视为通过解轨迹概率测度的梯度流。STaR、GRPO、DPO等方法都是该损失函数的特例。

Result: 1）多样性衰减定理描述正确性目标如何导致STaR、GRPO、DPO的多样性衰减；2）设计确保收敛到稳定多样策略的方法；3）提供简单可行的实践方案。

Conclusion: DCR为LLM提供了首个保持正确性和创造性的原则性方案，解决了当前推理循环中的分布崩溃问题。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [73] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

TL;DR: 提出基于协变量依赖隐马尔可夫模型(CDHMM)的足球角球防守评估框架，能无标签识别盯人与区域防守任务，用于防守贡献归因和反事实分析。


<details>
  <summary>Details</summary>
Motivation: 传统足球防守评估指标难以捕捉无球防守的协调移动，现有价值模型主要评估有球动作，反事实方法缺乏战术背景。角球作为高度结构化的比赛环节，需要更精细的防守评估方法。

Method: 开发协变量依赖隐马尔可夫模型(CDHMM)，直接从球员追踪数据中推断时间分辨的盯人和区域防守任务分配。基于这些分配提出防守贡献归因框架和角色条件幽灵模型进行反事实分析。

Result: 模型能有效识别防守任务分配，提出的框架提供了可解释的防守贡献评估，相比传统方法能更好地结合战术背景进行反事实分析。

Conclusion: CDHMM框架为足球角球防守提供了无标签、可解释的评估方法，能更好地量化无球防守表现，为战术分析和球员评估提供新工具。

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [74] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的方法，用于LLM的持续学习，将记忆库大小减少到基准的0.3%，同时保持高准确率。


<details>
  <summary>Details</summary>
Motivation: LLM的知识容易过时，需要持续学习来更新信息。现有记忆增强方法面临记忆库随数据流不断增长的问题，导致存储和计算效率低下。

Method: 1. 通过码本优化策略压缩记忆库；2. 引入在线重置机制防止码本崩溃；3. 在注意力层使用Key-Value低秩适配，高效利用压缩后的记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小减少到最强基线的0.3%，同时在在线适应学习中保持高保留准确率。

Conclusion: MBC通过记忆库压缩和码本优化，有效解决了持续学习中记忆库无限增长的问题，实现了高效且稳定的LLM在线适应学习。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [75] [FedHypeVAE: Federated Learning with Hypernetwork Generated Conditional VAEs for Differentially Private Embedding Sharing](https://arxiv.org/abs/2601.00785)
*Sunny Gupta,Amit Sethi*

Main category: cs.LG

TL;DR: FedHypeVAE：一个基于超网络的差分隐私联邦数据合成框架，通过条件VAE架构和客户端感知解码器解决非IID数据异构性问题，同时提供形式化的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有联邦数据共享方法在非IID客户端异构性下表现不佳，且缺乏对梯度泄漏的形式化保护。需要一种既能个性化生成层又能保护隐私的解决方案。

Method: 基于条件VAE架构，用超网络生成客户端感知解码器和类条件先验，替代单一全局解码器和固定先验。采用差分隐私优化超网络，结合局部MMD对齐和Lipschitz正则化增强稳定性。训练后使用元代码实现领域无关合成。

Result: FedHypeVAE在生成器层面统一了个性化、隐私保护和分布对齐，为联邦环境下的隐私保护数据合成建立了理论基础。代码已开源。

Conclusion: 该框架为联邦数据合成提供了原则性基础，解决了非IID异构性和隐私保护的挑战，实现了生成层面的个性化而非下游模型的个性化。

Abstract: Federated data sharing promises utility without centralizing raw data, yet existing embedding-level generators struggle under non-IID client heterogeneity and provide limited formal protection against gradient leakage. We propose FedHypeVAE, a differentially private, hypernetwork-driven framework for synthesizing embedding-level data across decentralized clients. Building on a conditional VAE backbone, we replace the single global decoder and fixed latent prior with client-aware decoders and class-conditional priors generated by a shared hypernetwork from private, trainable client codes. This bi-level design personalizes the generative layerrather than the downstream modelwhile decoupling local data from communicated parameters. The shared hypernetwork is optimized under differential privacy, ensuring that only noise-perturbed, clipped gradients are aggregated across clients. A local MMD alignment between real and synthetic embeddings and a Lipschitz regularizer on hypernetwork outputs further enhance stability and distributional coherence under non-IID conditions. After training, a neutral meta-code enables domain agnostic synthesis, while mixtures of meta-codes provide controllable multi-domain coverage. FedHypeVAE unifies personalization, privacy, and distribution alignment at the generator level, establishing a principled foundation for privacy-preserving data synthesis in federated settings. Code: github.com/sunnyinAI/FedHypeVAE

</details>


### [76] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出一种无需训练的方法，通过注意力模式的光谱分析检测大语言模型中的有效数学推理，利用四个可解释的光谱诊断指标实现85-95%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要训练数据、微调或学习分类器，本文旨在开发一种无需训练的方法来检测语言模型中的有效数学推理，通过分析注意力模式来识别逻辑一致性。

Method: 将注意力矩阵视为动态图的邻接矩阵，提取四个可解释的光谱诊断指标：Fiedler值（代数连通性）、高频能量比（HFER）、图信号平滑度和光谱熵，通过单一阈值实现分类。

Result: 在七个不同架构的Transformer模型上实验，效果量达Cohen's d=3.30，分类准确率85.0-95.6%，发现该方法检测的是逻辑一致性而非编译器接受度，并揭示了注意力机制设计对光谱特征的影响。

Conclusion: 光谱图分析为推理验证提供了原则性框架，具有即时应用于幻觉检测和AI安全监控的潜力，且无需训练数据或微调。

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [77] [Active learning for data-driven reduced models of parametric differential systems with Bayesian operator inference](https://arxiv.org/abs/2601.00038)
*Shane A. McQuarrie,Mengwu Guo,Anirban Chaudhuri*

Main category: stat.ML

TL;DR: 提出基于主动学习的自适应采样框架，用于提升参数化动力学系统数据驱动降阶模型的质量，通过贝叶斯线性回归量化不确定性并指导参数选择


<details>
  <summary>Details</summary>
Motivation: 数据驱动降阶模型作为数字孪生虚拟资产的基础，其质量对有限训练数据敏感，需要智能选择训练参数以提升模型全局稳定性和准确性

Method: 采用算子推断方法，建立参数化贝叶斯线性回归框架，利用概率ROM解的不确定性设计顺序自适应采样方案，选择能提升全局性能的新训练参数

Result: 在多个非线性参数化偏微分方程系统上的数值实验表明，相比随机采样，提出的自适应采样策略在相同计算预算下能产生更稳定和准确的ROM

Conclusion: 主动学习框架能有效指导训练参数选择，显著提升数据驱动降阶模型的质量，为数字孪生应用提供更可靠的虚拟资产基础

Abstract: This work develops an active learning framework to intelligently enrich data-driven reduced-order models (ROMs) of parametric dynamical systems, which can serve as the foundation of virtual assets in a digital twin. Data-driven ROMs are explainable, computationally efficient scientific machine learning models that aim to preserve the underlying physics of complex dynamical simulations. Since the quality of data-driven ROMs is sensitive to the quality of the limited training data, we seek to identify training parameters for which using the associated training data results in the best possible parametric ROM. Our approach uses the operator inference methodology, a regression-based strategy which can be tailored to particular parametric structure for a large class of problems. We establish a probabilistic version of parametric operator inference, casting the learning problem as a Bayesian linear regression. Prediction uncertainties stemming from the resulting probabilistic ROM solutions are used to design a sequential adaptive sampling scheme to select new training parameter vectors that promote ROM stability and accuracy globally in the parameter domain. We conduct numerical experiments for several nonlinear parametric systems of partial differential equations and compare the results to ROMs trained on random parameter samples. The results demonstrate that the proposed adaptive sampling strategy consistently yields more stable and accurate ROMs than random sampling does under the same computational budget.

</details>


### [78] [Detecting Unobserved Confounders: A Kernelized Regression Approach](https://arxiv.org/abs/2601.00200)
*Yikai Chen,Yunxin Mao,Chunyuan Zheng,Hao Zou,Shanzhi Gu,Shixuan Liu,Yang Shi,Wenjing Yang,Kun Kuang,Haotian Wang*

Main category: stat.ML

TL;DR: 提出KRCD方法，用于在非线性单环境观测数据中检测未观测混杂因子，通过比较标准和高阶核回归来识别未观测混杂


<details>
  <summary>Details</summary>
Motivation: 现有方法需要线性假设或多个异质环境，限制了在非线性单环境设置中的应用，需要填补这一空白

Method: 使用再生核希尔伯特空间建模复杂依赖关系，通过比较标准和高阶核回归推导检验统计量，显著偏离零表示存在未观测混杂

Result: 理论证明：无限样本下回归系数相等当且仅当无未观测混杂；有限样本差异收敛到零均值高斯分布。实验表明KRCD优于现有基线且计算效率高

Conclusion: KRCD是首个能在非线性单环境设置中检测未观测混杂因子的方法，具有理论保证和实际优势

Abstract: Detecting unobserved confounders is crucial for reliable causal inference in observational studies. Existing methods require either linearity assumptions or multiple heterogeneous environments, limiting applicability to nonlinear single-environment settings. To bridge this gap, we propose Kernel Regression Confounder Detection (KRCD), a novel method for detecting unobserved confounding in nonlinear observational data under single-environment conditions. KRCD leverages reproducing kernel Hilbert spaces to model complex dependencies. By comparing standard and higherorder kernel regressions, we derive a test statistic whose significant deviation from zero indicates unobserved confounding. Theoretically, we prove two key results: First, in infinite samples, regression coefficients coincide if and only if no unobserved confounders exist. Second, finite-sample differences converge to zero-mean Gaussian distributions with tractable variance. Extensive experiments on synthetic benchmarks and the Twins dataset demonstrate that KRCD not only outperforms existing baselines but also achieves superior computational efficiency.

</details>


### [79] [Generative Conditional Missing Imputation Networks](https://arxiv.org/abs/2601.00517)
*George Sun,Yi-Hui Zhou*

Main category: stat.ML

TL;DR: 提出生成式条件缺失值插补网络(GCMI)，通过链式方程多重插补增强鲁棒性，在MCAR和MAR机制下优于现有方法


<details>
  <summary>Details</summary>
Motivation: 数据集中的缺失值插补是统计分析中的重要问题，需要开发更有效、鲁棒的插补方法

Method: 1. 提出生成式条件缺失值插补网络(GCMI)的理论框架；2. 通过链式方程多重插补增强GCMI的鲁棒性和准确性

Result: 在模拟实验和基准数据集评估中，GCMI方法相比现有领先插补技术表现出优越性能

Conclusion: GCMI具有实用性和潜力，可作为统计数据分析领域的先进工具

Abstract: In this study, we introduce a sophisticated generative conditional strategy designed to impute missing values within datasets, an area of considerable importance in statistical analysis. Specifically, we initially elucidate the theoretical underpinnings of the Generative Conditional Missing Imputation Networks (GCMI), demonstrating its robust properties in the context of the Missing Completely at Random (MCAR) and the Missing at Random (MAR) mechanisms. Subsequently, we enhance the robustness and accuracy of GCMI by integrating a multiple imputation framework using a chained equations approach. This innovation serves to bolster model stability and improve imputation performance significantly. Finally, through a series of meticulous simulations and empirical assessments utilizing benchmark datasets, we establish the superior efficacy of our proposed methods when juxtaposed with other leading imputation techniques currently available. This comprehensive evaluation not only underscores the practicality of GCMI but also affirms its potential as a leading-edge tool in the field of statistical data analysis.

</details>
