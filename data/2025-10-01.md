<div id=toc></div>

# Table of Contents

- [eess.SP](#eess.SP) [Total: 27]
- [cs.LG](#cs.LG) [Total: 180]
- [stat.ML](#stat.ML) [Total: 15]


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [1] [Experimental Demonstration of Robust Distributed Wireless Clock Synchronization](https://arxiv.org/abs/2509.25277)
*Kumar Sai Bondada,Hiten Kothari,Yibin Liang,Daniel J. Jakubisin,R. Michael Buehrer*

Main category: eess.SP

TL;DR: 提出了一种鲁棒的频率跳变双音波形，用于分布式无线时钟同步，使收发器能够在不知道确切传输频率的情况下提取参考信号。


<details>
  <summary>Details</summary>
Motivation: 现有的双音波形时钟同步方法虽然能达到优于1Hz的频率精度，但容易受到有意和无意的干扰，需要提高抗干扰能力。

Method: 采用频率跳变双音波形技术，收发器无需预先知道音调的确切传输频率即可提取参考信号。

Result: 该方法能够有效抵抗干扰，实现稳健的时钟同步。

Conclusion: 频率跳变双音波形提供了一种抗干扰的分布式无线时钟同步解决方案。

Abstract: Distributed wireless clock synchronization is essential for aligning the
clocks of distributed transceivers in support of joint transmission and
reception techniques. One recently explored method involves synchronizing
distributed transceivers using a two-tone waveform, where the tones are
separated in frequency by a clock (frequency) reference signal. Prior research
has demonstrated frequency accuracy better than 1 Hz; however, this approach
remains vulnerable to both intentional and unintentional interference. In this
demonstration, we present a robust, frequency-hopped two-tone waveform that
enables transceivers to extract the reference signal without prior knowledge of
the exact frequency at which the tones are transmitted.

</details>


### [2] [A Graph-based Hybrid Beamforming Framework for MIMO Cell-Free ISAC Networks](https://arxiv.org/abs/2509.25385)
*Yanan Du,Sai Xu,Jagmohan Chauhan*

Main category: eess.SP

TL;DR: 提出了一种基于图神经网络的混合波束成形框架，用于MIMO无蜂窝集成感知与通信网络，通过分布式部署和FPGA加速实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决MIMO无蜂窝ISAC网络中同时优化通信和感知性能的问题，需要在功率约束下实现分布式混合波束成形，并降低推理延迟。

Method: 构建MIMO无蜂窝ISAC网络模型，将多目标优化问题转化为单目标优化，开发基于图神经网络的方法实现混合波束成形，并设计FPGA加速器降低推理延迟。

Result: 数值仿真验证了所提方法在通信和感知方面的能力，实验评估显示FPGA加速在GNN推理中带来显著性能提升。

Conclusion: 该框架能够有效提升MIMO无蜂窝ISAC网络的通信和感知性能，并通过分布式部署和硬件加速实现高效推理。

Abstract: This paper develops a graph-based hybrid beamforming framework for
multiple-input multiple-output (MIMO) cell-free integrated sensing and
communication (ISAC) networks. Specifically, we construct a novel MIMO
cell-free ISAC network model. In this model, multiple dual-function base
station (BS) transmitters employ distributed hybrid beamforming to enable
simultaneous communication and sensing, while maintaining physical separation
between the transmitters and the radar receiver. Building on this model, we
formulate a multi-objective optimization problem under a power constraint to
jointly improve communication and sensing performance. To solve it, the problem
is first reformulated as a single-objective optimization problem. Then, a
graph-based method composed of multiple graph neural networks (GNNs) is
developed to realize hybrid beamforming with either perfect or imperfect
channel state information. Once trained, the neural network model can be
deployed distributively across BSs, enabling fast and efficient inference. To
further reduce inference latency, a custom field-programmable gate array
(FPGA)-based accelerator is developed. Numerical simulations validate the
communication and sensing capabilities of the proposed optimization approach,
while experimental evaluations demonstrate remarkable performance gains of
FPGA-based acceleration in GNN inference.

</details>


### [3] [A closed-loop $2\times4$ downlink MIMO Framework for 5G New Radio using OpenAirInterface](https://arxiv.org/abs/2509.25497)
*Duc Tung Bui,Le-Nam Tran*

Main category: eess.SP

TL;DR: 首次实现了基于OpenAirInterface的5G O-RAN下行链路2×4 MIMO系统，支持最多两层传输，相比现有2×2 MIMO配置在几乎所有场景下提升了数据速率。


<details>
  <summary>Details</summary>
Motivation: 为新兴的开放无线接入网(O-RAN)开发提供基础框架，实现更高效的MIMO传输能力。

Method: 使用OpenAirInterface构建完整的5G NR系统，包括核心网、无线接入网和用户设备，增强CSI报告机制（RI、PMI、CQI），并在固定信道和Rice1信道下测量下行数据速率。

Result: 相比OAI现有的2×2 MIMO配置，在几乎所有场景下数据速率都有提升，特别是在高信噪比条件下表现更佳。

Conclusion: 该2×4 MIMO实现为O-RAN发展提供了重要基础，证明了扩展MIMO配置在提升5G系统性能方面的有效性。

Abstract: We present the first-of-a-kind closed-loop $2\times4$ MIMO implementation for
the downlink of 5G Open RAN using OpenAirInterface (OAI), which is capable of
transmitting up to two transmission layers. Our implementation is a fully
functional 5G New Radio (5G NR) system, including the 5G Core Network (5G CN),
5G Radio Access Network (5G RAN), as well as 5G NR User Equipment (UEs). This
serves as a foundational framework for further advancements in the context of
emerging Open RAN (O-RAN) development. A key feature of our implementation is
the enhanced Channel State Information (CSI) reporting procedure at the UE,
which includes Rank Indicator (RI), Precoding Matrix Indicator (PMI), and
Channel Quality Indicator (CQI). It is adjusted for the extended configuration
to maximize data rates. To demonstrate the performance of our implementation,
we measure the downlink data rates using $\textit{iperf3}$ in two scenarios:
(i) fixed channels to assess two-layer data transmission and (ii)
$\textit{Rice1}$ channels for general transmission analysis. The obtained
simulation results demonstrate that, compared to the existing $2\times2$ MIMO
configuration in the OAI, our implementation improves the data rates in almost
all scenarios, especially at the high Signal-to-Noise-Ratios (SNRs).

</details>


### [4] [An Implementation of Multi-User MIMO Downlink for O-RAN 5G New Radio using OpenAirInterface](https://arxiv.org/abs/2509.25512)
*Duc Tung Bui,Le-Nam Tran*

Main category: eess.SP

TL;DR: 首个在5G O-RAN平台上基于OpenAirInterface实现的MU-MIMO传输方案，展示了在PDSCH信道上同时服务两个UE的能力，显著提升下行吞吐量。


<details>
  <summary>Details</summary>
Motivation: 实现O-RAN兼容的5G NR系统，展示MU-MIMO在O-RAN架构下的性能，验证O-RAN解耦能力。

Method: 构建完整的5G系统（包括5G核心网、分离的CU/DU架构和UE），基于PMI报告进行用户调度，使用iperf评估吞吐量性能。

Result: MU-MIMO方案在高SNR区域显著提升下行吞吐量，同时保持两个UE的BLER低于10^-1阈值。

Conclusion: 成功实现了O-RAN兼容的MU-MIMO传输，证明了在O-RAN架构下实现高性能多用户传输的可行性。

Abstract: We present the first implementation of a Multi-User Multiple-Input
Multiple-Output (MU-MIMO) transmission scheme on the Physical Downlink Shared
Channel (PDSCH) for 5G Open Radio Access Network (O-RAN) based on
OpenAirInterface (OAI). Our implementation features a fully functional
O-RAN-compliant 5G New Radio (5G NR) system, including a 5G Core Network (5G
CN), a refined 5G RAN, which is split into a Centre Unit (CU) and an
Distributed Unit (DU), and 5G NR User Equipment (UEs). This implementation
demonstrates MU-MIMO performance in the downlink while showcasing the
disaggregation capabilities of O-RAN. Specifically, the Base Station (i.e. gNB)
in our setup is capable of serving two UEs simultaneously over the same
downlink Resource Block (RBs). User scheduling is performed based on the
Precoding Matrix Indicators (PMIs) reported by the UEs according to the NR
Channel State Information (CSI) reporting procedure. The system throughput
performance is evaluated using $\textit{iperf}$. The obtained results via
simulation and testbed experiments demonstrate that the MU-MIMO scheme achieves
significant downlink throughput gains, particularly in the high
Signal-to-Noise-Ratio (SNR) regime, while keeping the Block Error Rate (BLER)
below the required threshold of $10^{-1}$ for both UEs.

</details>


### [5] [Joint UE positioning and distributed sensing in the upper mid-band exploiting virtual apertures](https://arxiv.org/abs/2509.25557)
*Soroush Mesforush,Murat Bayraktar,Nuria González-Prelcic*

Main category: eess.SP

TL;DR: 本文提出了一种在FR3频段运行的分布式集成感知与通信(DISAC)系统，通过多输入多输出正交频分复用波形实现联合用户设备定位和目标定位，解决了实际系统中的时序偏移、扩展目标、密集多径等问题。


<details>
  <summary>Details</summary>
Motivation: 利用分布式集成感知与通信节点网络，特别是在上中频段(FR3)运行的大阵列和宽带系统，可以提供增强的定位和感知性能。

Method: 系统采用MIMO-OFDM波形，包含多径估计、杂波去除、新颖的聚类和关联方案，以及最终的用户设备位置和目标位置的联合估计器，通过加权最小二乘问题求解时钟偏移并定位用户设备和目标。

Result: 数值结果显示，在考虑两个用户设备和两个目标的情况下，80%的情况下目标定位误差低于32厘米，用户设备定位误差低于44厘米。

Conclusion: 所提出的DISAC系统在FR3频段能够有效实现联合用户设备定位和目标定位，具有较高的定位精度。

Abstract: Networks exploiting distributed integrated sensing and communication (DISAC)
nodes can provide enhanced localization and sensing performance, further
emphasized when operating with large arrays and bandwidths available in the
upper mid-band (also known as FR3). In this paper, we consider a DISAC system
operating at FR3 where a single base station (BS) acts as the transmitter and
several vehicular user equipments (UEs) act as the receivers. We tackle the
design of the signal processing chain at the UE side to enable joint UE
positioning and target localization. The system model exploits a
multiple-input-multiple-output orthogonal frequency division multiplexing
(MIMO-OFDM) waveform, and incorporates practical effects such as inter-node
timing offsets (TOs), extended targets, dense multipath, and realistic uniform
planar arrays (UPAs) at both ends. The proposed design includes a multipath
estimation stage at each UE, clutter removal, a novel clustering and
association scheme, and a final joint estimator of UE positions and target
locations. The estimator solves a weighted least squares (WLS) problem to
jointly compute clock offsets and localize UEs and targets. Numerical results
considering two UEs and two targets show that for 80\% of the cases the target
localization error is below 32cm, while the UE positioning error is below 44cm.

</details>


### [6] [Rotatable Antenna-Enabled Spectrum Sharing in Cognitive Radio Systems](https://arxiv.org/abs/2509.25656)
*Yanhua Tan,Beixiong Zheng,Yi Fang,Derrick Wing Kwan Ng,Rui Zhang,Jie Xu*

Main category: eess.SP

TL;DR: 提出了一种基于可旋转天线的认知无线电系统，通过联合优化发射波束成形和天线指向来最大化次级接收器的信干噪比，同时满足对主用户的干扰约束。


<details>
  <summary>Details</summary>
Motivation: 可旋转天线技术能够通过动态调整天线三维指向来利用额外的空间自由度，在认知无线电系统中实现高效的频谱共享和干扰抑制。

Method: 采用交替优化和逐次凸逼近技术来解决非凸且变量耦合的联合优化问题，获得高质量解。

Result: 数值结果表明，所提出的可旋转天线辅助系统在频谱共享认知无线电系统中显著优于传统基准方案。

Conclusion: 可旋转天线技术能够同时提升次级接收器的通信质量和降低对主用户的干扰，验证了其在认知无线电系统中的有效性。

Abstract: Rotatable antenna (RA) technology has recently drawn significant attention in
wireless systems owing to its unique ability to exploit additional spatial
degrees-of-freedom (DoFs) by dynamically adjusting the three-dimensional (3D)
boresight direction of each antenna. In this letter, we propose a new
RA-assisted cognitive radio (CR) system designed to achieve efficient spectrum
sharing while mitigating interference between primary and secondary
communication links. Specifically, we formulate an optimization problem for the
joint design of the transmit beamforming and the boresight directions of RAs at
the secondary transmitter (ST), aimed at maximizing the received
signal-to-interference-plus-noise ratio (SINR) at the secondary receiver (SR),
while satisfying both interference constraint at the primary receiver (PR) and
the maximum transmit power constraint at the ST. Although the formulated
problem is challenging to solve due to its non-convexity and coupled variables,
we develop an efficient algorithm by leveraging alternating optimization (AO)
and successive convex approximation (SCA) techniques to acquire high-quality
solutions. Numerical results demonstrate that the proposed RA-assisted system
substantially outperforms conventional benchmark schemes in spectrum-sharing CR
systems, validating RA's capability to simultaneously enhance the communication
quality at the SR and mitigate interference at the PR.

</details>


### [7] [A Novel Statistical Analysis Method for Radiation Source Classification](https://arxiv.org/abs/2509.25675)
*Haobo Geng,Yaoyao Li,Weiping Tong,Youwei Meng,Houpu Xiao,Yicong Liu*

Main category: eess.SP

TL;DR: 提出了一种结合线性判别分析(LDA)和粗糙邻域集(NRS)的大数据分析方法，用于辐射源分类，并在RadioML 2018数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 随着电子信息技术的快速发展，未知辐射源的数量和种类显著增加，其中一些具有共同特征，为解决识别未知辐射源的挑战提供了可能，但目前辐射源分类研究相对有限。

Method: 提出了一种结合线性判别分析(LDA)和粗糙邻域集(NRS)的大数据分析方法。

Result: 在RadioML 2018数据集上的结果表明，在特定约束条件下，所有调制类型可以被分为四个不同的类别。

Conclusion: 该方法为认知干扰信号消除的进一步研究奠定了基础。

Abstract: With the rapid advancement of electronic information technology, the number
and variety of unknown radiation sources have increased significantly. Some of
these sources share common characteristics, which offers the potential to
effectively address the challenge of identifying unknown radiation sources.
However, research on the classification of radiation sources remains relatively
limited. This paper proposes a big data analysis method that combines linear
discriminant analysis (LDA) with a rough neighborhood set (NRS) for radiation
source classification, and its effectiveness is validated on the RadioML 2018
dataset. The results indicate that, under certain constraints, all modulation
types can be categorized into four distinct classes, laying a foundation for
further research on cognitive interference signal cancellation.

</details>


### [8] [Transformer-Based Rate Prediction for Multi-Band Cellular Handsets](https://arxiv.org/abs/2509.25722)
*Ruibin Chen,Haozhe Lei,Hao Guo,Marco Mezzavilla,Hitesh Poddar,Tomoki Yoshimura,Sundeep Rangan*

Main category: eess.SP

TL;DR: 提出基于Transformer的神经网络架构，用于在稀疏历史测量条件下预测多天线阵列和多频段的可实现速率，以改善频段选择决策。


<details>
  <summary>Details</summary>
Motivation: 随着FR3等新频段的扩展，用户设备需要在有限尺寸下支持多天线多频段。信道质量快速变化、天线视场角有限以及硬件功率限制导致的测量稀疏性，给可靠的多频段信道跟踪带来挑战。

Method: 采用基于Transformer的神经网络架构，输入异步速率历史数据，输出每个天线阵列的速率预测。在密集城市微蜂窝场景中使用射线追踪仿真进行评估。

Result: 在FR1和FR3阵列的评估中，该方法相比基线预测器表现出更优越的性能，能够在实际移动性和硬件约束下实现更明智的频段选择。

Conclusion: 所提出的Transformer架构能够有效解决多频段信道跟踪中的测量稀疏问题，为移动设备在复杂环境下的频段选择提供可靠预测。

Abstract: Cellular wireless systems are witnessing the proliferation of frequency bands
over a wide spectrum, particularly with the expansion of new bands in FR3.
These bands must be supported in user equipment (UE) handsets with multiple
antennas in a constrained form factor. Rapid variations in channel quality
across the bands from motion and hand blockage, limited field-of-view of
antennas, and hardware and power-constrained measurement sparsity pose
significant challenges to reliable multi-band channel tracking. This paper
formulates the problem of predicting achievable rates across multiple antenna
arrays and bands with sparse historical measurements. We propose a
transformer-based neural architecture that takes asynchronous rate histories as
input and outputs per-array rate predictions. Evaluated on ray-traced
simulations in a dense urban micro-cellular setting with FR1 and FR3 arrays,
our method demonstrates superior performance over baseline predictors, enabling
more informed band selection under realistic mobility and hardware constraints.

</details>


### [9] [Doppler-Based Multistatic Drone Tracking via Cellular Downlink Signals](https://arxiv.org/abs/2509.25732)
*Chenqing Ji,Qionghui Liu,Jiahong Liu,Chao Yu,Yifei Sun,Rui Wang,Fan Liu*

Main category: eess.SP

TL;DR: 提出了一种基于LTE下行信号的多基地多普勒感知系统，用于无人机跟踪，仅通过多普勒频率测量即可实现高精度轨迹重建


<details>
  <summary>Details</summary>
Motivation: 利用现有LTE基站作为信号发射源，无需测量距离和角度信息，仅通过多普勒频率测量实现无人机轨迹跟踪，降低系统复杂度和成本

Method: 部署三个被动感知接收器，从接收的LTE下行信号中检测目标无人机的双基地多普勒频率，通过求解最小均方误差问题重建无人机轨迹

Result: 实验表明，在目标无人机和所有接收器距离基站约200米的情况下，复杂轨迹的跟踪误差90%低于90厘米，精度显著高于LTE信号的典型距离分辨率

Conclusion: 仅通过多普勒检测即可实现高精度无人机轨迹跟踪，前提是接收器部署密度足够高，这为基于现有通信基础设施的感知系统提供了可行性

Abstract: In this paper, a multistatic Doppler sensing system is proposed for the drone
tracking via downlink Long-Term Evolution (LTE) signals. Specifically, the LTE
base stations (BSs) are exploited as signal illuminators, and three passive
sensing receivers are deployed at different locations to detect the bistatic
Doppler frequencies of a target drone from received downlink signals. It is
shown that even without the measurements of BS-drone-receiver range and angle,
the Doppler measurements could provide sufficient information for trajectory
tracking. Particularly, the trajectory of the target drone, consisting of the
initial position and velocities of all the time slots, can be reconstructed by
solving a minimum mean-squared error problem according to the above Doppler
measurements. It is demonstrated by experiment that although the target drone
and all the sensing receivers are around 200 meters away from the illuminating
BSs, the complicated trajectories can be tracked with 90% errors below 90
centimeters. Since this accuracy is notably higher than the typical range
resolution of LTE signals, the demonstration shows that drone trajectory
tracking with a high accuracy could be feasible solely according to Doppler
detection, as long as the deployment density of receivers is sufficiently high.

</details>


### [10] [Digital Twin Aided Massive MIMO CSI Feedback: Exploring the Impact of Twinning Fidelity](https://arxiv.org/abs/2509.25793)
*Hao Luo,Shuaifeng Jiang,Saeed R. Khosravirad,Ahmed Alkhateeb*

Main category: eess.SP

TL;DR: 提出利用站点特定数字孪生来辅助训练基于深度学习的CSI压缩模型，通过电磁3D模型、硬件模型和射线追踪生成合成CSI数据，减少对真实世界测量的依赖，并开发了保真度分析框架和细化策略。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在压缩和重建大规模MIMO系统信道状态信息时对大量站点特定训练数据的依赖问题，这些数据的收集在现实世界中会产生显著开销并限制跨部署站点的可扩展性。

Method: 集成电磁3D环境模型、硬件模型和射线追踪生成站点特定合成CSI数据；开发保真度分析框架评估数字孪生质量；提出结合少量真实数据对预训练模型进行细化的策略。

Result: 在站点特定数字孪生上训练的模型优于在通用数据集上训练的模型，所提出的细化方法能有效利用有限真实数据提升性能；仿真显示数字孪生保真度（特别是3D几何、射线追踪和硬件建模）对CSI重建质量至关重要。

Conclusion: 数字孪生方法能有效减少对真实世界测量的依赖，保真度分析框架为关键保真度方面提供了有价值见解，有助于为各种无线通信任务开发更高效的数字孪生部署策略。

Abstract: Deep learning (DL) techniques have demonstrated strong performance in
compressing and reconstructing channel state information (CSI) while reducing
feedback overhead in massive MIMO systems. A key challenge, however, is their
reliance on extensive site-specific training data, whose real-world collection
incurs significant overhead and limits scalability across deployment sites. To
address this, we propose leveraging site-specific digital twins to assist the
training of DL-based CSI compression models. The digital twin integrates an
electromagnetic (EM) 3D model of the environment, a hardware model, and ray
tracing to produce site-specific synthetic CSI data, allowing DL models to be
trained without the need for extensive real-world measurements. We further
develop a fidelity analysis framework that decomposes digital twin quality into
four key aspects: 3D geometry, material properties, ray tracing, and hardware
modeling. We explore how these factors influence the reliability of the data
and model performance. To enhance the adaptability to real-world environments,
we propose a refinement strategy that incorporates a limited amount of
real-world data to fine-tune the DL model pre-trained on the digital twin
dataset. Evaluation results show that models trained on site-specific digital
twins outperform those trained on generic datasets, with the proposed
refinement method effectively enhancing performance using limited real-world
data. The simulations also highlight the importance of digital twin fidelity,
especially in 3D geometry, ray tracing, and hardware modeling, for improving
CSI reconstruction quality. This analysis framework offers valuable insights
into the critical fidelity aspects, and facilitates more efficient digital twin
development and deployment strategies for various wireless communication tasks.

</details>


### [11] [Delay-Doppler Domain Channel Measurements and Modeling in High-Speed Railways](https://arxiv.org/abs/2509.25854)
*Hao Zhou,Yiyan Ma,Dan Fei,Weirong Liu,Zhengyu Zhang,Mi Yang,Guoyu Ma,Yunlong Lu,Ruisi He,Guoyu Wang,Cheng Li,Zhaohui Song,Bo Ai*

Main category: eess.SP

TL;DR: 本文提出了一种高速铁路场景下的延迟-多普勒域信道测量与建模方法，通过LTE-R系统测量验证了DD域信道模型的准确性，并发现DD域信道衰落系数的准不变区间远小于准平稳区间。


<details>
  <summary>Details</summary>
Motivation: 下一代无线通信系统需要在高频段和高移动性场景下运行，DD域多载波调制方案（如OTFS）比OFDM具有更优越的可靠性。然而，传统信道建模方法主要局限于时域、频域和空域，DD域信道建模原理研究不足。

Method: 设计了基于LTE-R系统的DD域信道测量方法；研究了准平稳区间、多径分量统计功率建模以及DD域信道衰落系数的准不变区间；通过371km/h的LTE-R测量建立了不同信道时变条件下的DD域信道模型。

Result: 在高速铁路场景下，DD域信道衰落系数的准不变区间为毫秒量级，远小于准平稳区间的100毫秒量级；通过OTFS传输的误码率比较验证了所提DD域信道模型的准确性。

Conclusion: 本研究可为高移动性环境下的DD域建模提供理论指导，支持未来6G及以上的DDMC和集成感知与通信设计。

Abstract: As next-generation wireless communication systems need to be able to operate
in high-frequency bands and high-mobility scenarios, delay-Doppler (DD) domain
multicarrier (DDMC) modulation schemes, such as orthogonal time frequency space
(OTFS), demonstrate superior reliability over orthogonal frequency division
multiplexing (OFDM). Accurate DD domain channel modeling is essential for DDMC
system design. However, since traditional channel modeling approaches are
mainly confined to time, frequency, and space domains, the principles of DD
domain channel modeling remain poorly studied. To address this issue, we
propose a systematic DD domain channel measurement and modeling methodology in
high-speed railway (HSR) scenarios. First, we design a DD domain channel
measurement method based on the long-term evolution for railway (LTE-R) system.
Second, for DD domain channel modeling, we investigate quasi-stationary
interval, statistical power modeling of multipath components, and particularly,
the quasi-invariant intervals of DD domain channel fading coefficients. Third,
via LTE-R measurements at 371 km/h, taking the quasi-stationary interval as the
decision criterion, we establish DD domain channel models under different
channel time-varying conditions in HSR scenarios. Fourth, the accuracy of
proposed DD domain channel models is validated via bit error rate comparison of
OTFS transmission. In addition, simulation verifies that in HSR scenario, the
quasi-invariant interval of DD domain channel fading coefficient is on
millisecond (ms) order of magnitude, which is much smaller than the
quasi-stationary interval length on $100$ ms order of magnitude. This study
could provide theoretical guidance for DD domain modeling in high-mobility
environments, supporting future DDMC and integrated sensing and communication
designs for 6G and beyond.

</details>


### [12] [Closed-Form Least-Squares Design of Fast-Convolution Based Variable-Bandwidth FIR Filters](https://arxiv.org/abs/2509.25931)
*Oksana Moryakova,Håkan Johansson*

Main category: eess.SP

TL;DR: 提出了一种基于快速卷积的可变带宽FIR滤波器的闭式最小二乘设计方法，该方法结合频域采样和重叠保存实现，显著降低了实现复杂度和在线带宽重配置复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有可变带宽滤波器设计方法实现复杂，在线重配置困难，需要一种能够降低复杂度并扩展可变带宽范围的设计方法。

Method: 使用频域采样和重叠保存方法，考虑线性周期时变系统的相应时不变脉冲响应集，在设计中引入适当的加权函数来降低最大逼近误差能量。

Result: 所提方法不仅能够以闭式形式设计FC基VBW滤波器，在给定性能下相比现有方案显著降低复杂度，而且允许扩展可变带宽范围而不增加复杂度。

Conclusion: 该闭式最小二乘设计方法为快速卷积基可变带宽FIR滤波器提供了一种高效的设计方案，在复杂度和性能之间取得了良好平衡。

Abstract: This paper introduces a closed-form least-squares (LS) design approach for
fast-convolution (FC) based variable-bandwidth (VBW) finite-impulse-response
(FIR) filters. The proposed LS design utilizes frequency sampling and the VBW
filter frequency-domain implementation using the overlap-save (OLS) method,
that together offer significant savings in implementation and online bandwidth
reconfiguration complexities. Since combining frequency-domain design and OLS
implementation leads to a linear periodic time-varying (LPTV) behavior of the
VBW filter, a set of the corresponding time-invariant impulse responses is
considered in the proposed design. Through numerical examples, it is
demonstrated that the proposed approach enables not only closed-form design of
FC-based VBW filters with substantial complexity reductions compared to
existing solutions for a given performance, but also allows the variable
bandwidth range to be extended without any increase in complexity. Moreover, a
way of reducing the maximum approximation error energy over the whole set of
the time-invariant filters of the LPTV system is shown by introducing
appropriate weighting functions in the design.

</details>


### [13] [Joint Communications, Sensing, and Positioning in 6G Multi-Functional Satellite Systems: Survey and Open Challenges](https://arxiv.org/abs/2509.25937)
*Chandan Kumar Sheemar,Jorge Querol,Wali Ullah Khan,Prabhu Thiruvasagam,Sourabh Solanki,Idir Edjekouane,Alejandro Gonzalez-Garrido,Mohammed Al-Ansi,Carla E. Garcia,Symeon Chatzinotas*

Main category: eess.SP

TL;DR: 该论文提出了多功能卫星系统(MFSS)框架，将通信、感知和定位导航授时(PNT)服务集成到单一载荷中，通过资源共享和功能协同提高频谱效率、降低成本并增强服务多样性。


<details>
  <summary>Details</summary>
Motivation: 当前功能特定的卫星载荷架构在成本、频谱使用和可持续性方面难以满足6G网络对通信、感知和PNT服务的多样化需求，需要更高效的集成解决方案。

Method: 提出统一分类法，涵盖联合通信与感知(JCAS)、联合通信与PNT(JCAP)、联合感知与PNT(JSAP)以及完全集成的联合通信、感知与PNT(JCSAP)系统，并分析合作、集成和联合设计策略。

Result: 通过将多样化卫星能力统一到单一平台，MFSS能够实现更高的频谱效率、降低发射质量和成本、改善能源使用并增强服务多样性。

Conclusion: MFSS框架有助于开发可持续和智能的非地面网络，为6G及未来空间时代提供更高效的卫星系统解决方案。

Abstract: Satellite systems are expected to be a cornerstone of sixth-generation (6G)
networks, providing ubiquitous coverage and supporting a wide range of services
across communications, sensing, and positioning, navigation, and timing (PNT).
Meeting these demands with current function-specific payload architectures is
challenging in terms of cost, spectral use, and sustainability. This survey
introduces the framework of multi-functional satellite systems (MFSS), which
integrate two or more of these core services into a single payload, enabling
resource sharing and functional synergy. A unified taxonomy is proposed,
covering joint communications and sensing (JCAS), joint communications and PNT
(JCAP), joint sensing and PNT (JSAP), and fully integrated joint
communications, sensing, and PNT (JCSAP) systems. The paper reviews the
state-of-the-art in each domain, examines existing payload architectures, and
outlines cooperative, integrated, and joint design strategies. Key challenges,
including waveform co-design, synchronization, interference mitigation, and
resource management, are discussed, along with potential solutions and future
research directions. By unifying diverse satellite capabilities within a single
platform, MFSS can achieve higher spectral efficiency, reduced launch mass and
cost, improved energy use, and enhanced service versatility, contributing to
the development of sustainable and intelligent non-terrestrial networks (NTNs)
for the 6G and beyond space era.

</details>


### [14] [Neural Network State-Space Estimators](https://arxiv.org/abs/2509.25959)
*Minxing Sun,Li Miao,Qingyu Shen,Yao Mao,Qiliang Bao*

Main category: eess.SP

TL;DR: 提出一种统一的状态空间结构，无需目标状态空间模型，将输入层激活和网络权重作为待估计的潜在状态，通过三种经典估计器在线估计。


<details>
  <summary>Details</summary>
Motivation: 传统状态估计算法依赖预定义的状态空间模型，模型推导复杂且难以适应系统动态变化；神经网络估计器虽然提供数据驱动替代方案，但很少融合经典估计理论且需要大量预计算训练集。

Method: 构建统一状态空间结构，将输入层激活和所有网络权重作为潜在状态，使用扩展卡尔曼估计器、无迹卡尔曼估计器和粒子估计器三种经典估计器进行在线估计。

Result: 在三个代表性场景中与七种领先神经网络估计器对比，结果表明该方法不仅保持了强大的学习能力，而且匹配或超越了传统方法和预训练神经网络方法的准确性。

Conclusion: 提出的神经网络状态空间估计器克服了传统方法和现有神经网络估计器的局限性，提供了一种灵活且准确的状态估计解决方案。

Abstract: Classical state estimation algorithms rely on predefined target's state-space
model, which complicates model derivation and limits adaptability when system
dynamics change. Neural network based estimators offer a data-driven
alternative, but rarely fuse classical estimation theory into their structure
and demand large, pre-computed training sets. To overcome these limitations, we
propose a unified state-space structure without target's state-space model and
treats both the input-layer activations and all network weights as latent
states to be estimated online. We instantiate this nonlinear model with three
canonical estimators-the extended Kalman estimator, the unscented Kalman
estimator, and the particle estimator to simulate different neural network and
demonstrate its generality. We then benchmark our approach against seven
leading neural network estimators across three representative scenarios.
Results show that our neural network state-space estimators not only retain the
robust learning capability, but also match or exceed the accuracy of both
classical and pre-trained neural network methods. Code, data, and more result:
github.com/ShineMinxing/PaperNNSSE.git

</details>


### [15] [Performance Evaluation of eLoran Spatial ASF Corrections Based on Measured ASF Map](https://arxiv.org/abs/2509.26018)
*Jaewon Yu,Pyo-Woong Son*

Main category: eess.SP

TL;DR: 本文通过实测ASF地图评估了韩国eLoran系统中空间ASF校正方法的有效性，发现局部校正方法(S1)在定位精度上表现最佳，而无校正(S0)误差最大，广域校正(S2)仅在参考站附近有有限改善。


<details>
  <summary>Details</summary>
Motivation: 分析不同ASF校正方法对eLoran系统定位性能的影响，为实际应用提供指导。

Method: 在相同仿真设置下评估三种校正方案：无校正(S0)、使用真实ASF值的局部校正(S1)、使用单一参考站值的广域校正(S2)。

Result: S1始终获得最低定位误差，S0误差最大且存在广泛高误差区域，S2在参考站附近有有限改善但随距离和ASF空间梯度增加而性能下降。

Conclusion: 局部ASF校正能显著提升eLoran定位性能，而广域校正仅具有局部化效益。

Abstract: This paper analyzes the effectiveness of spatial ASF correction methods in
the Korean eLoran system using measured ASF maps. Three correction scenarios
were evaluated under identical simulation settings: no correction (S0), local
correction using true ASF values (S1), and wide-area correction using a single
reference station value (S2). Simulation results show that S1 consistently
achieved the lowest positioning errors, while S0 exhibited the largest errors
with extensive high-error regions. S2 provided limited improvements near the
reference station but degraded with increasing distance and ASF spatial
gradients. The findings highlight that local ASF correction significantly
improves eLoran positioning performance, whereas wide-area correction has only
localized benefits.

</details>


### [16] [Path-Based Correlation Analysis of Meteorological Factors and eLoran Signal Delay Variations](https://arxiv.org/abs/2509.26020)
*Junwoo Song,Pyo-woong Son*

Main category: eess.SP

TL;DR: 分析eLoran信号传播延迟的时间变化与沿信号路径不同点的气象因素之间的相关性


<details>
  <summary>Details</summary>
Motivation: eLoran系统因高场强而具有抗干扰能力，在GNSS不可用时仍能可靠运行，但其地面波传播延迟易受地表条件（包括地形和气象变化）影响

Method: 研究eLoran信号传播延迟的时间变化与沿信号路径不同点的气象因素之间的相关性分析

Result: 未在摘要中明确说明具体结果

Conclusion: 需要进一步分析eLoran信号传播延迟与气象因素的相关性，以提高系统精度和可靠性

Abstract: Unlike GNSS, which is vulnerable to jamming and spoofing due to its
inherently weak received power, eLoran exhibits robustness owing to its high
field strength. Therefore, the eLoran system can maintain reliable operation
even in scenarios where GNSS becomes unavailable. However, since eLoran signals
propagate through ground waves, the propagation delay is susceptible to changes
in surface conditions, including both terrain and meteorological variations.
This study aims to analyze the correlation between the temporal variations in
eLoran signal propagation delay and meteorological factors at various points
along the signal path.

</details>


### [17] [Enhancing Connectivity for Emergency Vehicles Through UAV Trajectory and Resource Allocation Optimization](https://arxiv.org/abs/2509.26067)
*S. Fatemeh Bozorgi,S. Mohammad Razavizadeh,Mohsen Rezaee*

Main category: eess.SP

TL;DR: 提出了一种无人机辅助的车联网通信系统，通过联合优化无人机轨迹规划和动态带宽分配，在满足应急车辆最低瞬时数据率要求的同时，最大化普通车辆的平均数据率。


<details>
  <summary>Details</summary>
Motivation: 应急车辆（如救护车、消防车）在各种交通和环境条件下需要有效的通信支持，以确保关键信息的及时获取。

Method: 采用联合优化方法协调无人机轨迹规划和动态带宽分配，将原始非凸问题分解为带宽分配和无人机轨迹设计两个子问题，使用逐次凸逼近方法求解轨迹子问题。

Result: 数值结果表明，所提出的解决方案在满足服务需求方面优于基准方法。

Conclusion: 无人机辅助的车联网通信系统通过联合优化轨迹和资源分配，能够有效支持应急车辆和普通车辆的不同通信需求。

Abstract: Effective communication for emergency vehicles - such as ambulances and fire
trucks - is essential to support their operations in various traffic and
environmental conditions. In this context, this paper investigates a vehicular
communication system assisted by an Unmanned Aerial Vehicle (UAV), which
adjusts its trajectory and resource allocation according to communication
needs. The system classifies vehicles into two groups to address their varying
service requirements: emergency vehicles, which require a minimum instantaneous
data rate to access critical information timely, and normal vehicles. To
support both categories effectively, this paper proposes a joint optimization
approach that coordinates UAV trajectory planning and Dynamic Bandwidth
Allocation (DBA). The objective is to maximize the minimum average data rate
for normal vehicles while ensuring that emergency vehicles maintain an
instantaneous rate above a predefined threshold. This approach takes into
account some system constraints, including UAV propulsion power consumption,
mobility limitations, and backhaul capacity. To tackle the resulting non-convex
problem, an iterative optimization method is employed, where the original
problem is decomposed into two subproblems: bandwidth allocation and UAV
trajectory design. In each iteration, the trajectory subproblem is solved using
the Successive Convex Approximation (SCA) method. Numerical results confirm
that the proposed solution achieves superior performance in meeting service
requirements compared to baseline methods.

</details>


### [18] [Secrecy-Driven Beamforming for Multi-User Integrated Sensing and Communication](https://arxiv.org/abs/2509.26249)
*Ali Khandan Boroujeni,Hyeon Seok Rou,Ghazal Bagheri,Kuranage Roche Rayan Ranasinghe,Giuseppe Thadeu Freitas de Abreu,Stefan Köpsell,Rafael F. Schaefer*

Main category: eess.SP

TL;DR: 提出了一种面向多用户系统的安全集成感知与通信框架，通过波束成形和人工噪声优化来最大化保密率，同时满足感知与通信的联合约束。


<details>
  <summary>Details</summary>
Motivation: 在多用户系统中同时实现安全通信和精确感知面临挑战，需要解决传统人工噪声策略可能干扰合法用户的问题，同时提升系统的保密性能和感知精度。

Method: 采用基于非齐次复数二次变换的加速分式规划方法，将问题分解为波束成形和人工噪声优化的可处理子问题，创新性地利用人工噪声增强感知能力。

Result: 仿真结果显示，所提框架在保密率、通信可靠性和感知精度方面均取得显著提升，证明了方法的有效性和可扩展性。

Conclusion: 该框架成功实现了安全集成感知与通信的协同优化，为多用户系统提供了高效的安全通信和精确感知解决方案。

Abstract: This paper proposes a secure integrated sensing and communications (ISAC)
framework for multi-user systems with multiple communication users (CUs) and
adversarial targets, where the design problem is formulated to maximize secrecy
rate under joint sensing and communication constraints. An efficient solution
is presented based on an accelerated fractional programming method using a
non-homogeneous complex quadratic transform (QT), which decomposes the problem
into tractable subproblems for beamforming and artificial noise (AN)
optimization. Unlike conventional artificial noise strategies, the proposed
approach also exploits AN to enhance sensing while avoiding interference with
legitimate users. Simulation results show significant gains in secrecy rate,
communication reliability, and sensing accuracy, confirming the effectiveness
and scalability of the proposed framework.

</details>


### [19] [FinGAN: An Interpretable RSS Generation Network for Scalable Fingerprint Localization](https://arxiv.org/abs/2509.26286)
*Jiaming Zhang,Jiajun He,Jie Zhang,Okan Yurduseven*

Main category: eess.SP

TL;DR: FinGAN是一种用于生成接收信号强度(RSS)数据的生成对抗网络，能够从未测量的参考点直接生成RSS数据，通过最大化生成数据与参考点之间的互信息来学习潜在关系。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型要么依赖已知参考点位置，要么依赖预定义先验，无法有效扩展RSS指纹数据集。需要一种能够直接从参考点生成RSS数据的端到端方法。

Method: FinGAN通过最大化生成RSS数据与参考点之间的互信息来学习潜在关系，实现从参考点到RSS数据的端到端生成，无需预定义先验。

Result: 定量和定性评估表明，FinGAN生成的合成RSS数据与现场实验收集的真实样本高度一致，定位性能与完整真实数据集相当，在三种典型办公环境数据集上表现一致。

Conclusion: FinGAN能够准确生成未测量参考点的RSS数据，在不同场景下保持一致的性能，为RSS指纹数据集的扩展提供了有效解决方案。

Abstract: This work introduces FinGAN, a robust received signal strength (RSS) data
generator designed to expand RSS fingerprint datasets. Compared to existing
generative adversarial models that either rely on known reference positions
(RPs) or depend on predefined priors, FinGAN learns the latent information
between RPs and RSS values by maximizing the mutual information between the
generated RSS data and the RPs, enabling an end-to-end RSS generation directly
from RPs. This allows us to accurately generate RSS data for previously
unmeasured RPs. Both quantitative and qualitative evaluations demonstrate that
FinGAN produces synthetic RSS data closely aligned with real RSS sample
collected from the on-site experiment, preserving localization performance
comparable to that achieved with complete real-world datasets. To further
validate its generalizability, FinGAN is also trained and evaluated on
open-source datasets from three typical office environments,and the results
demonstrate consistent performance across different scenarios.

</details>


### [20] [Ultra-Reliable Risk-Aggregated Sum Rate Maximization via Model-Aided Deep Learning](https://arxiv.org/abs/2509.26311)
*Hassaan Hashmi,Spyridon Pougkakiotis,Dionysis Kalogerias*

Main category: eess.SP

TL;DR: 提出了一种基于条件风险价值(CVaR)的鲁棒加权和速率最大化方法，使用展开图神经网络(αRGNN)来消除用户深度速率衰落，显著降低统计速率变异性。


<details>
  <summary>Details</summary>
Motivation: 在多输入单输出下行无线网络中，传统加权和速率最大化方法缺乏对用户速率可靠性的考虑，特别是在信道衰落不确定性下无法保证速率可靠性。

Method: 引入风险聚合公式，建立WMMSE类等价关系，设计展开图神经网络策略函数逼近(αRGNN)，训练以最大化不利信道实现下的下尾(CVaR)速率。

Result: 训练后的αRGNN完全消除了每个用户的深度速率衰落，显著且最优地降低了统计用户速率变异性，同时保持了足够的遍历性能。

Conclusion: 基于CVaR的风险规避方法和展开图神经网络的结合能够有效解决无线网络中用户速率可靠性问题，在消除深度衰落的同时保持系统性能。

Abstract: We consider the problem of maximizing weighted sum rate in a multiple-input
single-output (MISO) downlink wireless network with emphasis on user rate
reliability. We introduce a novel risk-aggregated formulation of the complex
WSR maximization problem, which utilizes the Conditional Value-at-Risk (CVaR)
as a functional for enforcing rate (ultra)-reliability over channel fading
uncertainty/risk. We establish a WMMSE-like equivalence between the proposed
precoding problem and a weighted risk-averse MSE problem, enabling us to design
a tailored unfolded graph neural network (GNN) policy function approximation
(PFA), named {\alpha}-Robust Graph Neural Network ({\alpha}RGNN), trained to
maximize lower-tail (CVaR) rates resulting from adverse wireless channel
realizations (e.g., deep fading, attenuation). We empirically demonstrate that
a trained {\alpha}RGNN fully eliminates per user deep rate fades, and
substantially and optimally reduces statistical user rate variability while
retaining adequate ergodic performance.

</details>


### [21] [Transmitter-Side Beyond-Diagonal RIS-Enabled Integrated Sensing and Communications](https://arxiv.org/abs/2509.26333)
*Kexin Chen,Yijie Mao,Wonjae Shin*

Main category: eess.SP

TL;DR: 提出了一种在发射端集成BD-RIS的ISAC框架，通过联合优化主动波束成形和BD-RIS散射矩阵，同时提升感知性能和通信性能，并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: BD-RIS相比传统对角RIS能提供更先进的电磁波传播控制，有望增强6G网络的感知和通信性能，同时减轻发射端大规模全数字射频链的需求。

Method: 提出归一化加权优化问题，联合设计主动波束成形和BD-RIS散射矩阵，最小化感知目标的CRB迹并最大化通信用户的和速率。采用低复杂度迭代算法，将优化问题转化为一系列可处理的投影问题。

Result: 数值结果表明，发射端BD-RIS辅助的ISAC相比传统对角RIS辅助的ISAC在感知和通信性能上均有显著提升，且所提算法在提升双功能性能的同时显著降低了计算复杂度。

Conclusion: 发射端BD-RIS辅助的ISAC框架是6G无线网络中有前景的技术，能有效提升感知和通信性能，同时降低系统复杂度和计算负担。

Abstract: Beyond diagonal reconfigurable intelligent surfaces (BD-RIS) have emerged as
a promising technology for 6G wireless networks, offering more advanced control
over electromagnetic wave propagation than conventional diagonal RIS. This
paper proposes a novel integrated sensing and communication (ISAC) framework
that incorporates BD-RIS at the transmitter. This not only opens the door to
enhanced sensing and communication performance, but also alleviates the need
for large-scale fully digital radio frequency (RF) chains at the transmitter.
Based on the proposed system model, we formulate a normalized weighted
optimization problem to jointly design the active beamforming and the BD-RIS
scattering matrix with the aim of jointly minimizing the trace of the
Cram\'er-Rao bound (CRB) for sensing targets and maximizing the sum rate (SR)
for communication users. To address this highly coupled optimization problem,
we propose a novel and low-complexity iterative algorithm that efficiently
solves the active beamforming and scattering matrix subproblems by transforming
each into a series of tractable projection problems with closed-form solutions.
Numerical results show the appealing capability of the transmitter-side
BD-RIS-aided ISAC over conventional diagonal RIS-aided ISAC in enhancing both
sensing and communication performance. Moreover, compared to the classic
iterative algorithm, the proposed algorithm offers enhanced dual-functional
performance while significantly reducing the computational complexity.

</details>


### [22] [A Physics-Informed Multi-Source Domain Adaptation Framework for Label-Free Post-Earthquake Damage Assessment](https://arxiv.org/abs/2509.26356)
*Yifeng Zhang,Xiao Liang*

Main category: eess.SP

TL;DR: 提出了一种基于物理信息的多源域自适应框架，用于无需损伤标签预测目标建筑的地震后结构损伤。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法需要大量标注数据，但受损结构的标注数据难以获取，限制了地震后结构损伤评估的效率。

Method: 通过三个关键步骤：分析各域关键物理相似性形成权重矩阵；提取多源域和目标域特征，使用分类器和判别器；在对抗训练中应用关键参数矩阵优化各源域特征贡献。

Result: 该框架在标注数据稀缺的情况下为结构损伤评估提供了稳健解决方案。

Conclusion: 该方法显著提升了地震后损伤评估的能力，解决了标注数据不足的问题。

Abstract: Efficient and intelligent assessment of post-earthquake structural damage is
critical for rapid disaster response. While data-driven approaches have shown
promise, traditional supervised learning methods rely on extensive labeled
datasets, which are often impractical to obtain for damaged structures. To
address this limitation, we propose a physics-informed multi-source domain
adaptation framework to predict post-earthquake structural damage for a target
building without requiring damage labels. The multi-source domain integrates
actual damage data and numerical modeling data from buildings similar to the
target structure. The framework operates through three key steps. First, the
similarity of key physics from each domain are analyzed to form a weight
matrix, which enhances domain differentiation. Second, features from the
multi-source and target domains are extracted and fed into a classifier and a
discriminator. The classifier ensures that the features are damage-sensitive
and accurately assign damage states, while the discriminator enforces that the
features remain domain-invariant. Finally, the key parameters matrix is applied
as weights during adversarial training to optimize the contribution of features
from each source domain. The proposed framework provides a robust solution for
assessing structural damage in scenarios where labeled data is scarce,
significantly advancing the capabilities of post-earthquake damage evaluation.

</details>


### [23] [A solution to the mystery of the sub-harmonic series and to the combination tone via a linear mathematical model of the cochlea](https://arxiv.org/abs/2509.26395)
*Ugo Boscain,Xiangyu Ma,Dario Prandi,Giuseppina Turco*

Main category: eess.SP

TL;DR: 该论文通过将耳蜗建模为一组振动弦，研究了听觉信息处理机制，揭示了次谐波序列的出现以及组合音的产生原理。


<details>
  <summary>Details</summary>
Motivation: 研究耳蜗如何将声音信息传递到听觉皮层，探索次谐波序列和组合音的产生机制，这些现象在历史上长期存在争议。

Method: 使用简单的线性模型将耳蜗建模为一组振动弦，假设传递到听觉皮层的信息是弦中存储的能量，并考虑所有振荡模式。

Result: 证明了次谐波序列的出现，这解释了16世纪假设的小调和弦协和性；同时展示了能量非线性如何用于研究组合音（塔蒂尼第三音）的产生。

Conclusion: 该研究为长期争议的听觉现象提供了新的理论解释，通过简单的线性模型揭示了复杂的听觉处理机制。

Abstract: In this paper, we study a simple linear model of the cochlea as a set of
vibrating strings. We make hypothesis that the information sent to the auditory
cortex is the energy stored in the strings and consider all oscillation modes
of the strings. We show the emergence of the sub-harmonic series whose
existence was hypothesized in the XVI century to explain the consonance of the
minor chord. We additionally show how the nonlinearity of the energy can be
used to study the emergence of the combination tone (Tartini's third sound)
shedding new light on this long debated subject.

</details>


### [24] [Indoor/Outdoor Spectrum Sharing Enabled by GNSS-based Classifiers](https://arxiv.org/abs/2509.26500)
*Hossein Nasiri,Muhammad Iqbal Rochman,Monisha Ghosh*

Main category: eess.SP

TL;DR: 该论文提出利用GNSS信号进行室内/室外分类的方法，以解决中频段频谱共享中设备位置识别的问题，通过GNSS信号衰减特性结合机器学习实现高精度分类。


<details>
  <summary>Details</summary>
Motivation: 中频段(1-10GHz)在联邦和商业应用中的需求增长，特别是商业室内用例如工厂自动化，需要实现频谱共享。现有方法缺乏可靠的自动室内/室外分类机制，只能通过硬件限制和功率控制来避免干扰。

Method: 利用GNSS信号进行室内/室外分类，开发了基于阈值的技术和机器学习方法。GNSS信号专为室外接收设计，在室内会显著衰减，提供了鲁棒的环境感知特征。还结合了Wi-Fi数据进行多模态融合。

Result: 仅使用GNSS数据的方法比仅依赖Wi-Fi数据的方法具有更高的分类准确率，特别是在不熟悉的位置。GNSS与Wi-Fi数据的融合进一步提高了分类准确率。

Conclusion: GNSS信号为室内/室外分类提供了有效的解决方案，多模态数据融合显著提升了分类性能，为实现自动功率调整和频谱共享提供了可靠的技术基础。

Abstract: The desirability of the mid-band frequency range (1 - 10 GHz) for federal and
commercial applications, combined with the growing applications for commercial
indoor use-cases, such as factory automation, opens up a new approach to
spectrum sharing: the same frequency bands used outdoors by federal incumbents
can be reused by commercial indoor users. A recent example of such sharing,
between commercial systems, is the 6 GHz band (5.925 - 7.125 GHz) where
unlicensed, low-power-indoor (LPI) users share the band with outdoor
incumbents, primarily fixed microwave links. However, to date, there exist no
reliable, automatic means of determining whether a device is indoors or
outdoors, necessitating the use of other mechanisms such as mandating indoor
access points (APs) to have integrated antennas and not be battery powered, and
reducing transmit power of client devices which may be outdoors. An accurate
indoor/outdoor (I/O) classification addresses these challenges, enabling
automatic transmit power adjustments without interfering with incumbents. To
this end, we leverage the Global Navigation Satellite System (GNSS) signals for
I/O classification. GNSS signals, designed inherently for outdoor reception and
highly susceptible to indoor attenuation and blocking, provide a robust and
distinguishing feature for environmental sensing. We develop various
methodologies, including threshold-based techniques and machine learning
approaches and evaluate them using an expanded dataset gathered from diverse
geographical locations. Our results demonstrate that GNSS-based methods alone
can achieve greater accuracy than approaches relying solely on wireless (Wi-Fi)
data, particularly in unfamiliar locations. Furthermore, the integration of
GNSS data with Wi-Fi information leads to improved classification accuracy,
showcasing the significant benefits of multi-modal data fusion.

</details>


### [25] [Neural Network-Based Single-Carrier Joint Communication and Sensing: Loss Design, Constellation Shaping and Precoding](https://arxiv.org/abs/2509.26508)
*Charlotte Muth,Benedikt Geiger,Daniel Gil Gaviria,Laurent Schmalen*

Main category: eess.SP

TL;DR: 该论文研究了高阶调制格式对单载波联合通信感知系统性能的影响，使用神经网络实现系统组件，比较几何整形调制与传统QAM方案，并展示了多快拍感知和变信噪比条件下的性能改进。


<details>
  <summary>Details</summary>
Motivation: 研究高阶调制格式对联合通信感知系统性能的影响，探索神经网络在系统组件中的优势，特别是在低信噪比条件下提升感知性能。

Method: 使用神经网络实现波束成形器、调制器、目标检测器、到达角估计器和通信解映射器等组件，比较几何整形调制与QAM，采用多快拍感知和变信噪比训练策略，利用克拉美-罗界和互信息作为性能上界来改进训练。

Result: 神经网络感知在低信噪比下优于传统算法，多快拍感知显著减小了传统调制与整形调制之间的性能差距，系统可扩展至多用户MIMO以提升空间效率。

Conclusion: 估计界限对于训练神经网络至关重要，特别是在变信噪比条件下部署训练解决方案时，多快拍感知能有效减小不同调制格式间的性能差距。

Abstract: We investigate the impact of higher-order modulation formats on the sensing
performance of single-carrier joint communication and sensing (JCAS) systems.
Several separate components such as a beamformer, a modulator, a target
detector, an angle of arrival (AoA) estimator and a communication demapper are
implemented as trainable neural networks (NNs). We compare geometrically shaped
modulation formats to a classical quadrature amplitude modulation (QAM) scheme.
We assess the influence of multi-snapshot sensing and varying signal-to-noise
ratio (SNR) on the overall performance of the autoencoder-based system. To
improve the training behavior of the system, we decouple the loss functions
from the respective SNR values and the number of sensing snapshots, using upper
bounds of the sensing and communication performance, namely the Cram\'er-Rao
bound for AoA estimation and the mutual information for communication. The
NN-based sensing outperforms classical algorithms, such as a Neyman-Pearson
based power detector for object detection and ESPRIT for AoA estimation for
both the trained constellations and QAM at low SNRs. We show that the gap in
sensing performance between classical and shaped modulation formats can be
significantly reduced through multi-snapshot sensing. Lastly, we demonstrate
system extension to multi-user multiple-input multiple-output to address the
improvement of spatial efficiency when servicing multiple user equipments. Our
contribution emphasizes the importance of estimation bounds for training neural
networks, especially when the trained solutions are deployed in varying SNR
conditions.

</details>


### [26] [Secure ISAC with Fluid Antenna Systems: Joint Precoding and Port Selection](https://arxiv.org/abs/2509.26572)
*Abdelhamid Salem,Hao Xu,Kai-Kit Wong,Chan-Byoung Chae,Yangyang Zhang*

Main category: eess.SP

TL;DR: 提出了一种利用流体天线系统可重构性增强ISAC系统物理层安全性的新框架，通过联合预编码和端口选择策略最大化保密率并确保可靠雷达感知。


<details>
  <summary>Details</summary>
Motivation: 为了解决下一代无线网络中通信与感知联合系统的安全性问题，利用流体天线系统的可重构特性来增强物理层安全性能。

Method: 采用分数规划框架，结合连续凸逼近的迭代算法，并开发了基于迫零预编码的低复杂度方案，结合贪婪端口选择和迹逆最小化。

Result: 仿真结果显示，与传统基线相比，在广泛的FAS端口、用户负载和感知目标范围内，保密性能和感知精度均有显著提升。

Conclusion: FAS几何优化对于实现下一代无线网络安全高效的通信感知联合系统至关重要。

Abstract: This paper presents a novel framework for enhancing physical-layer security
in integrated sensing and communication (ISAC) systems by leveraging the
reconfigurability of fluid antenna systems (FAS). We propose a joint precoding
and port selection (JPPS) strategy that maximizes the sum secrecy rate while
simultaneously ensuring reliable radar sensing. The problem is formulated using
fractional programming (FP) and solved through an iterative algorithm that
integrates FP transformations with successive convex approximation (SCA). To
reduce computational complexity, we further develop low-complexity schemes
based on zero-forcing (ZF) precoding, combined with greedy port selection and
trace-inverse minimization. Simulation results demonstrate substantial
improvements in both secrecy performance and sensing accuracy compared to
conventional baselines, across a wide range of FAS ports, user loads, and
sensing targets. These findings highlight the critical importance of FAS
geometry optimization in enabling secure and efficient joint
communication-sensing for next-generation wireless networks.

</details>


### [27] [Statistical Inference Framework for Extended Target Detection in mmWave Automotive Radar](https://arxiv.org/abs/2509.26573)
*Vinay Kulkarni,V. V. Reddy,Neha Maheshwari*

Main category: eess.SP

TL;DR: 提出了一种基于Range-Doppler分割框架的毫米波雷达扩展目标检测方法，通过统计建模和偏度测试统计量来保留目标的空间散射结构，提高汽车雷达检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统CFAR检测算法将扩展目标的多个散射点视为独立检测，丢弃了目标固有的空间散射结构信息。为了保留这种散射分布特性，需要开发新的检测框架。

Method: 使用Range-Doppler分割框架封装汽车目标的典型散射轮廓，通过最大似然估计和Gibbs MCMC采样进行统计建模，引入基于偏度的测试统计量进行二元假设分类，并结合IoU和基于峰值响应的分割中心化优化检测流程。

Result: 在仿真和真实数据集上的广泛评估表明，所提方法有效提高了检测精度，适用于汽车雷达应用。

Conclusion: 提出的Range-Doppler分割框架能够有效保留扩展目标的散射结构，通过统计建模和优化的检测流程显著提升了汽车雷达系统的检测性能。

Abstract: Millimeter wave (mmWave) radar systems, owing to their large bandwidth,
provide fine range resolution that enables the observation of multiple
scatterers originating from a single automotive target commonly referred to as
an extended target. Conventional CFAR-based detection algorithms typically
treat these scatterers as independent detections, thereby discarding the
spatial scattering structure intrinsic to the target. To preserve this
scattering spread, this paper proposes a Range-Doppler (RD) segment framework
designed to encapsulate the typical scattering profile of an automobile. The
statistical characterization of the segment is performed using Maximum
Likelihood Estimation (MLE) and posterior density modeling facilitated through
Gibbs Markov Chain Monte Carlo (MCMC) sampling. A skewness-based test
statistic, derived from the estimated statistical model, is introduced for
binary hypothesis classification of extended targets. Additionally, the paper
presents a detection pipeline that incorporates Intersection over Union (IoU)
and segment centering based on peak response, optimized to work within a single
dwell. Extensive evaluations using both simulated and real-world datasets
demonstrate the effectiveness of the proposed approach, underscoring its
suitability for automotive radar applications through improved detection
accuracy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [28] [Unsupervised Detection of Spatiotemporal Anomalies in PMU Data Using Transformer-Based BiGAN](https://arxiv.org/abs/2509.25612)
*Muhammad Imran Hossain,Jignesh Solanki,Sarika Khushlani Solanki*

Main category: cs.LG

TL;DR: T-BiGAN是一个集成窗口注意力Transformer和双向生成对抗网络的新框架，用于无监督实时检测同步相量数据中的异常，在硬件在环PMU基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 确保电网弹性需要及时无监督地检测同步相量数据流中的异常，现有方法难以捕捉复杂的时空依赖关系且依赖人工标注数据。

Method: 采用自注意力编码器-解码器架构捕捉电网复杂时空依赖，联合判别器强制执行循环一致性以对齐潜在空间与真实数据分布，使用自适应评分实时标记异常。

Result: 在硬件在环PMU基准测试中，T-BiGAN达到ROC-AUC 0.95和平均精度0.996，显著优于领先的有监督和无监督方法，特别擅长检测细微频率和电压偏差。

Conclusion: T-BiGAN展示了在不依赖人工标注故障数据的情况下进行实时广域监测的实际价值，为电网异常检测提供了有效的无监督解决方案。

Abstract: Ensuring power grid resilience requires the timely and unsupervised detection
of anomalies in synchrophasor data streams. We introduce T-BiGAN, a novel
framework that integrates window-attention Transformers within a bidirectional
Generative Adversarial Network (BiGAN) to address this challenge. Its
self-attention encoder-decoder architecture captures complex spatio-temporal
dependencies across the grid, while a joint discriminator enforces cycle
consistency to align the learned latent space with the true data distribution.
Anomalies are flagged in real-time using an adaptive score that combines
reconstruction error, latent space drift, and discriminator confidence.
Evaluated on a realistic hardware-in-the-loop PMU benchmark, T-BiGAN achieves
an ROC-AUC of 0.95 and an average precision of 0.996, significantly
outperforming leading supervised and unsupervised methods. It shows particular
strength in detecting subtle frequency and voltage deviations, demonstrating
its practical value for live, wide-area monitoring without relying on manually
labeled fault data.

</details>


### [29] [Beyond Point Estimates: Likelihood-Based Full-Posterior Wireless Localization](https://arxiv.org/abs/2509.25719)
*Haozhe Lei,Hao Guo,Tommy Svensson,Sundeep Rangan*

Main category: cs.LG

TL;DR: 提出MC-CLE方法，使用蒙特卡洛采样训练神经评分网络来估计发射器位置的后验分布，量化定位不确定性


<details>
  <summary>Details</summary>
Motivation: 现代无线系统不仅需要位置估计，还需要量化不确定性来支持规划、控制和无线资源管理

Method: 将定位建模为从接收器测量中推断未知发射器位置的后验推理，使用蒙特卡洛采样训练神经评分网络来比较真实和候选发射器位置

Result: 在线视距模拟中，MC-CLE学习了关键特性包括角度模糊和前后天线模式，相对于均匀基线和高斯后验实现了更低的交叉熵损失

Conclusion: MC-CLE方法能够有效学习定位中的不确定性特性，在不确定性量化方面优于传统方法

Abstract: Modern wireless systems require not only position estimates, but also
quantified uncertainty to support planning, control, and radio resource
management. We formulate localization as posterior inference of an unknown
transmitter location from receiver measurements. We propose Monte Carlo
Candidate-Likelihood Estimation (MC-CLE), which trains a neural scoring network
using Monte Carlo sampling to compare true and candidate transmitter locations.
We show that in line-of-sight simulations with a multi-antenna receiver, MC-CLE
learns critical properties including angular ambiguity and front-to-back
antenna patterns. MC-CLE also achieves lower cross-entropy loss relative to a
uniform baseline and Gaussian posteriors. alternatives under a uniform-loss
metric.

</details>


### [30] [Machine Learning Detection of Lithium Plating in Lithium-ion Cells: A Gaussian Process Approach](https://arxiv.org/abs/2509.26234)
*Ayush Patnaik,Adam B Zufall,Stephen K Robinson,Xinfan Lin*

Main category: cs.LG

TL;DR: 提出基于高斯过程的锂电析锂检测方法，通过直接建模充电压关系并解析计算dQ/dV，实现无平滑处理的稳健检测。


<details>
  <summary>Details</summary>
Motivation: 快速充电过程中的锂析出是导致容量衰减和安全故障的关键降解机制。传统dQ/dV计算方法会放大传感器噪声并引入峰值定位偏差。

Method: 使用高斯过程框架直接建模充电压关系Q(V)，利用GP导数仍为GP的特性，从后验分布中解析推断dQ/dV，实现概率性检测。

Result: 在0.2C-1C倍率和0-40°C温度范围的锂离子纽扣电池实验中，该方法在低温高倍率充电下可靠检测析锂峰值，在基线情况下正确报告无峰值。

Conclusion: 该方法为实时锂析出检测提供了实用途径，通过GP识别的微分峰值、减少的充电通量和容量衰减的一致性验证了其准确性和鲁棒性。

Abstract: Lithium plating during fast charging is a critical degradation mechanism that
accelerates capacity fade and can trigger catastrophic safety failures. Recent
work has identified a distinctive dQ/dV peak above 4.0 V as a reliable
signature of plating onset; however, conventional methods for computing dQ/dV
rely on finite differencing with filtering, which amplifies sensor noise and
introduces bias in peak location. In this paper, we propose a Gaussian Process
(GP) framework for lithium plating detection by directly modeling the
charge-voltage relationship Q(V) as a stochastic process with calibrated
uncertainty. Leveraging the property that derivatives of GPs remain GPs, we
infer dQ/dV analytically and probabilistically from the posterior, enabling
robust detection without ad hoc smoothing. The framework provides three key
benefits: (i) noise-aware inference with hyperparameters learned from data,
(ii) closed-form derivatives with credible intervals for uncertainty
quantification, and (iii) scalability to online variants suitable for embedded
BMS. Experimental validation on Li-ion coin cells across a range of C-rates
(0.2C-1C) and temperatures (0-40\deg C) demonstrates that the GP-based method
reliably detects plating peaks under low-temperature, high-rate charging, while
correctly reporting no peaks in baseline cases. The concurrence of
GP-identified differential peaks, reduced charge throughput, and capacity fade
measured via reference performance tests confirms the method's accuracy and
robustness, establishing a practical pathway for real-time lithium plating
detection.

</details>


### [31] [SOLD: SELFIES-based Objective-driven Latent Diffusion](https://arxiv.org/abs/2509.25198)
*Elbert Ho*

Main category: cs.LG

TL;DR: 提出SOLD模型，一种基于SELFIES的潜在扩散模型，用于在目标蛋白条件下生成高亲和力分子，相比现有方法更简单高效。


<details>
  <summary>Details</summary>
Motivation: 当前基于目标蛋白生成分子的方法通常依赖3D构象空间，速度慢且复杂，需要更简单高效的解决方案。

Method: 使用1D SELFIES字符串的潜在空间，结合潜在扩散模型，训练创新的SELFIES变换器，并提出多任务机器学习模型的损失平衡新方法。

Result: 模型能够为目标蛋白生成高亲和力分子，方法简单高效，并为未来通过添加更多数据改进留出空间。

Conclusion: SOLD模型提供了一种更简单高效的分子生成方法，在药物设计领域具有应用潜力，且具备进一步优化的可能性。

Abstract: Recently, machine learning has made a significant impact on de novo drug
design. However, current approaches to creating novel molecules conditioned on
a target protein typically rely on generating molecules directly in the 3D
conformational space, which are often slow and overly complex. In this work, we
propose SOLD (SELFIES-based Objective-driven Latent Diffusion), a novel latent
diffusion model that generates molecules in a latent space derived from 1D
SELFIES strings and conditioned on a target protein. In the process, we also
train an innovative SELFIES transformer and propose a new way to balance losses
when training multi-task machine learning models.Our model generates
high-affinity molecules for the target protein in a simple and efficient way,
while also leaving room for future improvements through the addition of more
data.

</details>


### [32] [VLHSA: Vision-Language Hierarchical Semantic Alignment for Jigsaw Puzzle Solving with Eroded Gaps](https://arxiv.org/abs/2509.25202)
*Zhuoning Xu,Xinyan Liu*

Main category: cs.LG

TL;DR: 提出了一种结合视觉和语言的多模态拼图求解框架，通过视觉-语言层次语义对齐模块提升拼图组装性能，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统拼图求解方法主要依赖视觉线索，但在具有侵蚀间隙的挑战性场景中缺乏语义指导。本文探索利用自然语言描述为拼图组装提供语义引导。

Method: 提出视觉-语言层次语义对齐(VLHSA)模块，通过从局部标记到全局上下文的多层次语义匹配，将视觉补丁与文本描述对齐。采用结合双视觉编码器和语言特征的多模态架构进行跨模态推理。

Result: 在多个数据集上显著优于最先进模型，实现了14.2个百分点的拼图准确率提升。消融研究证实VLHSA模块对改进视觉方法的关键作用。

Conclusion: 通过融入多模态语义洞察，为拼图求解建立了新范式，证明了语言上下文在增强拼图组装性能中的重要性。

Abstract: Jigsaw puzzle solving remains challenging in computer vision, requiring an
understanding of both local fragment details and global spatial relationships.
While most traditional approaches only focus on visual cues like edge matching
and visual coherence, few methods explore natural language descriptions for
semantic guidance in challenging scenarios, especially for eroded gap puzzles.
We propose a vision-language framework that leverages textual context to
enhance puzzle assembly performance. Our approach centers on the
Vision-Language Hierarchical Semantic Alignment (VLHSA) module, which aligns
visual patches with textual descriptions through multi-level semantic matching
from local tokens to global context. Also, a multimodal architecture that
combines dual visual encoders with language features for cross-modal reasoning
is integrated into this module. Experiments demonstrate that our method
significantly outperforms state-of-the-art models across various datasets,
achieving substantial improvements, including a 14.2 percentage point gain in
piece accuracy. Ablation studies confirm the critical role of the VLHSA module
in driving improvements over vision-only approaches. Our work establishes a new
paradigm for jigsaw puzzle solving by incorporating multimodal semantic
insights.

</details>


### [33] [Spectral Logit Sculpting: Adaptive Low-Rank Logit Transformation for Controlled Text Generation](https://arxiv.org/abs/2509.25204)
*Jin Li,Zhebo Wang,Tianliang Lu,Mohan Li,Wenpeng Xing,Meng Han*

Main category: cs.LG

TL;DR: 提出Spectral Logit Sculpting (SLS)方法，通过谱分析和熵属性动态调制token分布，在推理时优化LLM输出，无需更新模型参数。


<details>
  <summary>Details</summary>
Motivation: 现有基于熵的推理方法计算开销大且未能有效利用历史token上下文，需要更轻量高效的优化方法。

Method: 维护top-K logits滑动缓冲区，进行实时SVD识别主导谱方向，基于熵和logit间隙统计自适应重缩放logits，仅在不确定性高时激活。

Result: 在多个公共基准测试中，SLS持续优于现有基线方法，在数学、编程和科学推理任务中达到更优准确率。

Conclusion: SLS是一种轻量级推理时优化方法，能有效锐化输出分布同时保持上下文一致性，显著提升LLM可靠性。

Abstract: Entropy-based inference methods have gained traction for improving the
reliability of Large Language Models (LLMs). However, many existing approaches,
such as entropy minimization techniques, suffer from high computational
overhead and fail to leverage historical token context effectively. To address
these limitations, we propose Spectral Logit Sculpting (SLS), a lightweight
inference-time optimization method that dynamically modulates token
distributions using spectral and entropic properties of recent logits. SLS
maintains a sliding buffer of top-K logits, performs on-the-fly Singular Value
Decomposition (SVD) to identify dominant spectral directions, and adaptively
rescales logits based on both entropy and logit gap statistics--only activating
when uncertainty is high. Without updating any model parameters, SLS
effectively sharpens the output distribution while preserving contextual
consistency. Experimental results on multiple public benchmarks demonstrate
that SLS consistently outperforms existing baseline methods, achieving superior
accuracy in mathematical, coding, and scientific reasoning tasks.

</details>


### [34] [Polynomial Contrastive Learning for Privacy-Preserving Representation Learning on Graphs](https://arxiv.org/abs/2509.25205)
*Daksh Pandey*

Main category: cs.LG

TL;DR: Poly-GRACE是一个与同态加密兼容的图自监督学习框架，通过全多项式友好的GCN编码器和多项式对比损失函数，实现在隐私保护下的图表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法（如GRACE）由于依赖非多项式操作，与同态加密等隐私保护技术不兼容，限制了在隐私敏感场景下的应用。

Method: 提出完全多项式友好的图卷积网络编码器和基于多项式的对比损失函数，使整个框架与同态加密兼容。

Result: 在Cora、CiteSeer和PubMed三个基准数据集上的实验表明，Poly-GRACE不仅支持隐私预训练，而且在CiteSeer数据集上性能优于标准非隐私基线。

Conclusion: 这项工作为实现实用且高性能的隐私保护图表示学习迈出了重要一步。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for
learning representations on graph data without requiring manual labels.
However, leading SSL methods like GRACE are fundamentally incompatible with
privacy-preserving technologies such as Homomorphic Encryption (HE) due to
their reliance on non-polynomial operations. This paper introduces Poly-GRACE,
a novel framework for HE-compatible self-supervised learning on graphs. Our
approach consists of a fully polynomial-friendly Graph Convolutional Network
(GCN) encoder and a novel, polynomial-based contrastive loss function. Through
experiments on three benchmark datasets -- Cora, CiteSeer, and PubMed -- we
demonstrate that Poly-GRACE not only enables private pre-training but also
achieves performance that is highly competitive with, and in the case of
CiteSeer, superior to the standard non-private baseline. Our work represents a
significant step towards practical and high-performance privacy-preserving
graph representation learning.

</details>


### [35] [Hyperbolic Optimization](https://arxiv.org/abs/2509.25206)
*Yanke Wang,Kyriakos Flouris*

Main category: cs.LG

TL;DR: 本文提出了在双曲流形上的优化方法，扩展了双曲随机梯度下降为双曲Adam优化器，并在扩散模型训练中展示了更快的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 基于黎曼优化原理，探索双曲流形上的优化方法，特别是在庞加莱球上的学习，这种优化方法能促进庞加莱嵌入的学习，在训练早期阶段加速收敛。

Method: 扩展双曲随机梯度下降为双曲Adam优化器，结合双曲时间离散化的朗之万动力学来训练扩散模型。

Result: 在特定数据集上实现了更快的收敛速度，同时没有牺牲生成质量。

Conclusion: 双曲优化方法在训练早期阶段能有效加速收敛，特别是在参数远离最优解时，为双曲流形上的学习提供了有效的优化工具。

Abstract: This work explores optimization methods on hyperbolic manifolds. Building on
Riemannian optimization principles, we extend the Hyperbolic Stochastic
Gradient Descent (a specialization of Riemannian SGD) to a Hyperbolic Adam
optimizer. While these methods are particularly relevant for learning on the
Poincar\'e ball, they may also provide benefits in Euclidean and other
non-Euclidean settings, as the chosen optimization encourages the learning of
Poincar\'e embeddings. This representation, in turn, accelerates convergence in
the early stages of training, when parameters are far from the optimum. As a
case study, we train diffusion models using the hyperbolic optimization methods
with hyperbolic time-discretization of the Langevin dynamics, and show that
they achieve faster convergence on certain datasets without sacrificing
generative quality.

</details>


### [36] [Multi-level Diagnosis and Evaluation for Robust Tabular Feature Engineering with Large Language Models](https://arxiv.org/abs/2509.25207)
*Yebin Lim,Susik Yoon*

Main category: cs.LG

TL;DR: 提出了一个多级诊断评估框架来评估LLM在特征工程中的鲁棒性，发现LLM生成的优质特征可将少样本预测性能提升高达10.52%。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在表格数据的特征工程中显示出潜力，但由于生成输出的变异性，对其可靠性的担忧依然存在。

Method: 引入多级诊断和评估框架，重点关注三个主要因素：关键变量、关系和预测目标类的决策边界值。

Result: LLM的鲁棒性在不同数据集上差异显著，高质量LLM生成的特征可将少样本预测性能提升高达10.52%。

Conclusion: 这项工作为评估和增强LLM驱动特征工程在各种领域中的可靠性开辟了新方向。

Abstract: Recent advancements in large language models (LLMs) have shown promise in
feature engineering for tabular data, but concerns about their reliability
persist, especially due to variability in generated outputs. We introduce a
multi-level diagnosis and evaluation framework to assess the robustness of LLMs
in feature engineering across diverse domains, focusing on the three main
factors: key variables, relationships, and decision boundary values for
predicting target classes. We demonstrate that the robustness of LLMs varies
significantly over different datasets, and that high-quality LLM-generated
features can improve few-shot prediction performance by up to 10.52%. This work
opens a new direction for assessing and enhancing the reliability of LLM-driven
feature engineering in various domains.

</details>


### [37] [DPSformer: A long-tail-aware model for improving heavy rainfall prediction](https://arxiv.org/abs/2509.25208)
*Zenghui Huang,Ting Shu,Zhonglei Wang,Yang Lu,Yan Yan,Wei Zhong,Hanzi Wang*

Main category: cs.LG

TL;DR: DPSformer是一个针对强降雨预测的长尾学习模型，通过高分辨率分支增强对罕见强降雨事件的表示，显著提升了强降雨预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 强降雨预测对现代社会至关重要，但降雨数据分布高度不平衡——大多数观测记录无雨或小雨，而强降雨事件罕见。这种不平衡分布阻碍了深度学习模型有效预测强降雨事件。

Method: 将降雨预测明确视为长尾学习问题，引入DPSformer模型，通过高分辨率分支丰富强降雨事件的表示。

Result: 对于≥50mm/6h的强降雨事件，DPSformer将基准数值天气预报模型的CSI从0.012提升到0.067；对于前1%的强降雨事件，其FSS超过0.45，优于现有方法。

Conclusion: 这项工作为强降雨预测建立了一个有效的长尾范式，提供了增强预警系统和减轻极端天气事件社会影响的实用工具。

Abstract: Accurate and timely forecasting of heavy rainfall remains a critical
challenge for modern society. Precipitation exhibits a highly imbalanced
distribution: most observations record no or light rain, while heavy rainfall
events are rare. Such an imbalanced distribution obstructs deep learning models
from effectively predicting heavy rainfall events. To address this challenge,
we treat rainfall forecasting explicitly as a long-tailed learning problem,
identifying the insufficient representation of heavy rainfall events as the
primary barrier to forecasting accuracy. Therefore, we introduce DPSformer, a
long-tail-aware model that enriches representation of heavy rainfall events
through a high-resolution branch. For heavy rainfall events $ \geq $ 50 mm/6 h,
DPSformer lifts the Critical Success Index (CSI) of a baseline Numerical
Weather Prediction (NWP) model from 0.012 to 0.067. For the top 1% coverage of
heavy rainfall events, its Fraction Skill Score (FSS) exceeds 0.45, surpassing
existing methods. Our work establishes an effective long-tailed paradigm for
heavy rainfall prediction, offering a practical tool to enhance early warning
systems and mitigate the societal impacts of extreme weather events.

</details>


### [38] [STCast: Adaptive Boundary Alignment for Global and Regional Weather Forecasting](https://arxiv.org/abs/2509.25210)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Lei Bai*

Main category: cs.LG

TL;DR: 提出STCast框架，通过空间对齐注意力机制自适应优化区域边界，利用时间混合专家模块动态分配月度预测，在区域天气预报中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有区域天气预报方法受限于静态和不精确的区域边界，导致泛化能力差，需要自适应边界优化和动态时间分配机制。

Method: 使用空间对齐注意力机制对齐全局和区域空间分布来初始化边界，基于注意力模式自适应优化；设计时间混合专家模块，通过离散高斯分布将不同月份的大气变量动态路由到专门专家。

Result: 在全局和区域天气预报、极端事件预测和集合预报四个任务上均优于最先进方法。

Conclusion: STCast框架通过自适应边界优化和动态时间分配，显著提升了区域天气预报的准确性和泛化能力。

Abstract: To gain finer regional forecasts, many works have explored the regional
integration from the global atmosphere, e.g., by solving boundary equations in
physics-based methods or cropping regions from global forecasts in data-driven
methods. However, the effectiveness of these methods is often constrained by
static and imprecise regional boundaries, resulting in poor generalization
ability. To address this issue, we propose Spatial-Temporal Weather Forecasting
(STCast), a novel AI-driven framework for adaptive regional boundary
optimization and dynamic monthly forecast allocation. Specifically, our
approach employs a Spatial-Aligned Attention (SAA) mechanism, which aligns
global and regional spatial distributions to initialize boundaries and
adaptively refines them based on attention-derived alignment patterns.
Furthermore, we design a Temporal Mixture-of-Experts (TMoE) module, where
atmospheric variables from distinct months are dynamically routed to
specialized experts using a discrete Gaussian distribution, enhancing the
model's ability to capture temporal patterns. Beyond global and regional
forecasting, we evaluate our STCast on extreme event prediction and ensemble
forecasting. Experimental results demonstrate consistent superiority over
state-of-the-art methods across all four tasks.

</details>


### [39] [LEMs: A Primer On Large Execution Models](https://arxiv.org/abs/2509.25211)
*Remi Genet,Hugo Inzirillo*

Main category: cs.LG

TL;DR: LEMs是一种新型深度学习框架，将transformer架构扩展到具有灵活时间边界和多重执行约束的复杂执行问题，实现跨不同执行场景的统一模型部署。


<details>
  <summary>Details</summary>
Motivation: 扩展神经VWAP执行策略，从固定时长订单推广到执行时长有最小和最大时间边界的场景，类似于股票回购合约结构。

Method: 采用架构分离设计：通用特征提取流水线（TKANs、VSNs和多头注意力机制）处理市场数据，独立分配网络处理不同场景的具体执行逻辑。

Result: 在日内加密货币市场和多日股票交易中，LEMs相比传统基准实现了更优的执行性能，能在灵活时间约束内动态优化执行路径。

Conclusion: 统一模型架构能够通过单一框架部署到不同执行场景，相比资产特定方法具有显著操作优势。

Abstract: This paper introduces Large Execution Models (LEMs), a novel deep learning
framework that extends transformer-based architectures to address complex
execution problems with flexible time boundaries and multiple execution
constraints. Building upon recent advances in neural VWAP execution strategies,
LEMs generalize the approach from fixed-duration orders to scenarios where
execution duration is bounded between minimum and maximum time horizons,
similar to share buyback contract structures. The proposed architecture
decouples market information processing from execution allocation decisions: a
common feature extraction pipeline using Temporal Kolmogorov-Arnold Networks
(TKANs), Variable Selection Networks (VSNs), and multi-head attention
mechanisms processes market data to create informational context, while
independent allocation networks handle the specific execution logic for
different scenarios (fixed quantity vs. fixed notional, buy vs. sell orders).
This architectural separation enables a unified model to handle diverse
execution objectives while leveraging shared market understanding across
scenarios. Through comprehensive empirical evaluation on intraday
cryptocurrency markets and multi-day equity trading using DOW Jones
constituents, we demonstrate that LEMs achieve superior execution performance
compared to traditional benchmarks by dynamically optimizing execution paths
within flexible time constraints. The unified model architecture enables
deployment across different execution scenarios (buy/sell orders, varying
duration boundaries, volume/notional targets) through a single framework,
providing significant operational advantages over asset-specific approaches.

</details>


### [40] [Six Sigma For Neural Networks: Taguchi-based optimization](https://arxiv.org/abs/2509.25213)
*Sai Varun Kodathala*

Main category: cs.LG

TL;DR: 应用田口实验设计方法优化CNN超参数，用于拳击动作识别。通过正交阵列评估8个超参数，采用多目标优化方法，发现学习率是最关键参数。


<details>
  <summary>Details</summary>
Motivation: CNN超参数优化通常需要大量试错或网格搜索，计算成本高且效率低。传统质量工程中的田口方法可提供系统化的优化方案。

Method: 使用L12(211)正交阵列评估8个超参数，开发5种多目标优化方法，结合信噪比分析和新型对数缩放技术统一冲突指标。

Result: 方法3表现最佳，达到98.84%训练准确率和86.25%验证准确率，同时保持最小损失值。学习率是最重要参数，其次是图像尺寸和激活函数。

Conclusion: 田口方法能有效优化CNN超参数，提供明确的参数优先级指导，显著提高优化效率和性能。

Abstract: The optimization of hyperparameters in convolutional neural networks (CNNs)
remains a challenging and computationally expensive process, often requiring
extensive trial-and-error approaches or exhaustive grid searches. This study
introduces the application of Taguchi Design of Experiments methodology, a
statistical optimization technique traditionally used in quality engineering,
to systematically optimize CNN hyperparameters for professional boxing action
recognition. Using an L12(211) orthogonal array, eight hyperparameters
including image size, color mode, activation function, learning rate,
rescaling, shuffling, vertical flip, and horizontal flip were systematically
evaluated across twelve experimental configurations. To address the
multi-objective nature of machine learning optimization, five different
approaches were developed to simultaneously optimize training accuracy,
validation accuracy, training loss, and validation loss using Signal-to-Noise
ratio analysis. The study employed a novel logarithmic scaling technique to
unify conflicting metrics and enable comprehensive multi-quality assessment
within the Taguchi framework. Results demonstrate that Approach 3, combining
weighted accuracy metrics with logarithmically transformed loss functions,
achieved optimal performance with 98.84% training accuracy and 86.25%
validation accuracy while maintaining minimal loss values. The Taguchi analysis
revealed that learning rate emerged as the most influential parameter, followed
by image size and activation function, providing clear guidance for
hyperparameter prioritization in CNN optimization.

</details>


### [41] [On-the-Fly Adaptation to Quantization: Configuration-Aware LoRA for Efficient Fine-Tuning of Quantized LLMs](https://arxiv.org/abs/2509.25214)
*Rongguang Ye,Ming Tang,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: CoA-LoRA是一种动态调整LoRA适配器以适应任意量化配置的方法，无需重复微调，通过配置感知模型将每个配置映射到其低秩调整。


<details>
  <summary>Details</summary>
Motivation: 随着大型预训练模型的发布，在边缘设备上部署隐私保护应用需要有效压缩。现有方法需要为每个量化配置单独微调高精度LoRA适配器，这在计算上不可行。

Method: 提出配置感知模型，通过Pareto优化的配置搜索构建训练配置集，动态调整LoRA适配器以适应任意量化配置。

Result: CoA-LoRA无需额外时间成本，在性能上达到甚至超过需要为每个配置单独微调的最先进方法。

Conclusion: 该方法有效解决了边缘设备异构能力下的模型压缩问题，实现了高效且高性能的量化适应。

Abstract: As increasingly large pre-trained models are released, deploying them on edge
devices for privacy-preserving applications requires effective compression.
Recent works combine quantization with the fine-tuning of high-precision LoRA
adapters, which can substantially reduce model size while mitigating the
accuracy loss from quantization. However, edge devices have inherently
heterogeneous capabilities, while performing configuration-wise fine-tuning for
every quantization setting is computationally prohibitive. In this paper, we
propose CoA-LoRA, a method that dynamically adjusts the LoRA adapter to
arbitrary quantization configurations (i.e., the per-layer bit-width choices of
a pre-trained model) without requiring repeated fine-tuning. This is
accomplished via a configuration-aware model that maps each configuration to
its low-rank adjustments. The effectiveness of this model critically depends on
the training configuration set, a collection of configurations chosen to cover
different total bit-width budgets. However, constructing a high-quality
configuration set is non-trivial. We therefore design a Pareto-based
configuration search that iteratively optimizes the training configuration set,
yielding more precise low-rank adjustments. Our experiments demonstrate that,
unlike the state-of-the-art methods that require fine-tuning a separate LoRA
adapter for each configuration, CoA-LoRA incurs no additional time cost while
achieving comparable or even superior performance to those methods.

</details>


### [42] [Anomaly detection by partitioning of multi-variate time series](https://arxiv.org/abs/2509.25215)
*Pierre Lotte,André Péninou,Olivier Teste*

Main category: cs.LG

TL;DR: 提出了一种名为PARADISE的无监督分区异常检测方法，用于多元时间序列异常检测，通过变量分区保持变量间关系，并在各子集上执行局部异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多元时间序列异常检测时可能忽略变量间关系，需要一种能保持变量间相关性的分区方法。

Method: 基于变量间多重相关系数的聚类对变量进行分区，确保变量间关系不受影响，然后在每个子集上执行局部异常检测算法。

Result: 在合成和真实数据集上的实验表明，该方法显著提高了异常检测性能。

Conclusion: PARADISE方法通过保持变量间关系的分区策略，有效提升了多元时间序列异常检测的效果。

Abstract: In this article, we suggest a novel non-supervised partition based anomaly
detection method for anomaly detection in multivariate time series called
PARADISE. This methodology creates a partition of the variables of the time
series while ensuring that the inter-variable relations remain untouched. This
partitioning relies on the clustering of multiple correlation coefficients
between variables to identify subsets of variables before executing anomaly
detection algorithms locally for each of those subsets. Through multiple
experimentations done on both synthetic and real datasets coming from the
literature, we show the relevance of our approach with a significant
improvement in anomaly detection performance.

</details>


### [43] [Evaluating Double Descent in Machine Learning: Insights from Tree-Based Models Applied to a Genomic Prediction Task](https://arxiv.org/abs/2509.25216)
*Guillermo Comesaña Cimadevila*

Main category: cs.LG

TL;DR: 该论文通过生物分类任务重新审视了双下降现象，发现只有当模型复杂度在多个维度上联合扩展时才会出现双下降，而固定任一维度时泛化行为会回归到经典的U形或L形模式。


<details>
  <summary>Details</summary>
Motivation: 研究双下降现象在简单模型中的表现，特别是在决策树和梯度提升等传统机器学习模型中的存在性，并通过生物分类任务进行实证验证。

Method: 使用结核病异烟肼耐药性预测作为生物分类任务，系统性地在模型容量（如叶子节点数、提升轮数）和集成规模两个正交维度上变化模型复杂度。

Result: 只有当复杂度在多个维度上联合扩展时才会出现双下降现象；当固定任一维度时，泛化行为回归到经典的U形或L形模式。这些发现在合成基准测试中得到了复现。

Conclusion: 模型复杂度应被视为多维构造，在分析泛化行为时需要考虑多个维度的联合影响，这支持了展开假说，即双下降是不同泛化机制在单一复杂度轴上的投影。

Abstract: Classical learning theory describes a well-characterised U-shaped
relationship between model complexity and prediction error, reflecting a
transition from underfitting in underparameterised regimes to overfitting as
complexity grows. Recent work, however, has introduced the notion of a second
descent in test error beyond the interpolation threshold-giving rise to the
so-called double descent phenomenon. While double descent has been studied
extensively in the context of deep learning, it has also been reported in
simpler models, including decision trees and gradient boosting. In this work,
we revisit these claims through the lens of classical machine learning applied
to a biological classification task: predicting isoniazid resistance in
Mycobacterium tuberculosis using whole-genome sequencing data. We
systematically vary model complexity along two orthogonal axes-learner capacity
(e.g., Pleaf, Pboost) and ensemble size (i.e., Pens)-and show that double
descent consistently emerges only when complexity is scaled jointly across
these axes. When either axis is held fixed, generalisation behaviour reverts to
classical U- or L-shaped patterns. These results are replicated on a synthetic
benchmark and support the unfolding hypothesis, which attributes double descent
to the projection of distinct generalisation regimes onto a single complexity
axis. Our findings underscore the importance of treating model complexity as a
multidimensional construct when analysing generalisation behaviour. All code
and reproducibility materials are available at:
https://github.com/guillermocomesanacimadevila/Demystifying-Double-Descent-in-ML.

</details>


### [44] [Learning to Condition: A Neural Heuristic for Scalable MPE Inference](https://arxiv.org/abs/2509.25217)
*Brij Malhotra,Shivvrat Arya,Tahrima Rahman,Vibhav Giridhar Gogate*

Main category: cs.LG

TL;DR: L2C是一个可扩展的数据驱动框架，通过训练神经网络来评分变量赋值，加速概率图模型中的最可能解释推理。


<details>
  <summary>Details</summary>
Motivation: 解决概率图模型中最可能解释推理这一本质上难以处理的问题，传统方法在大规模问题上效率低下。

Method: 开发可扩展的数据生成管道从现有MPE求解器的搜索轨迹中提取训练信号，训练神经网络作为启发式函数，与搜索算法集成。

Result: 在涉及高树宽概率图模型的挑战性MPE查询上，学习的启发式显著减少搜索空间，同时保持或改进最先进方法的解质量。

Conclusion: L2C框架有效加速MPE推理，为处理大规模概率图模型推理问题提供了有前景的方向。

Abstract: We introduce learning to condition (L2C), a scalable, data-driven framework
for accelerating Most Probable Explanation (MPE) inference in Probabilistic
Graphical Models (PGMs), a fundamentally intractable problem. L2C trains a
neural network to score variable-value assignments based on their utility for
conditioning, given observed evidence. To facilitate supervised learning, we
develop a scalable data generation pipeline that extracts training signals from
the search traces of existing MPE solvers. The trained network serves as a
heuristic that integrates with search algorithms, acting as a conditioning
strategy prior to exact inference or as a branching and node selection policy
within branch-and-bound solvers. We evaluate L2C on challenging MPE queries
involving high-treewidth PGMs. Experiments show that our learned heuristic
significantly reduces the search space while maintaining or improving solution
quality over state-of-the-art methods.

</details>


### [45] [On The Dynamic Ensemble Selection for TinyML-based Systems -- a Preliminary Study](https://arxiv.org/abs/2509.25218)
*Tobiasz Puslecki,Krzysztof Walkowiak*

Main category: cs.LG

TL;DR: 该研究探讨了在TinyML系统中使用动态集成选择(DES)聚类方法进行多类计算机视觉任务，通过调整分类精度来平衡推理时间和能耗。


<details>
  <summary>Details</summary>
Motivation: TinyML系统在计算、内存和能量方面存在特定约束，需要专门的优化技术来平衡推理时间和分类质量。

Method: 开发了TinyDES-Clustering库，采用动态集成选择聚类方法，通过调整分类器池大小来优化嵌入式系统的性能。

Result: 实验表明，较大的分类器池能提高分类精度，但会增加TinyML设备的平均推理时间。

Conclusion: 动态集成选择方法在TinyML系统中能有效平衡分类精度与资源消耗，但需要权衡精度提升与推理时间增加之间的关系。

Abstract: The recent progress in TinyML technologies triggers the need to address the
challenge of balancing inference time and classification quality. TinyML
systems are defined by specific constraints in computation, memory and energy.
These constraints emphasize the need for specialized optimization techniques
when implementing Machine Learning (ML) applications on such platforms. While
deep neural networks are widely used in TinyML, the exploration of Dynamic
Ensemble Selection (DES) methods is also beneficial. This study examines a
DES-Clustering approach for a multi-class computer vision task within TinyML
systems. This method allows for adjusting classification accuracy, thereby
affecting latency and energy consumption per inference. We implemented the
TinyDES-Clustering library, optimized for embedded system limitations.
Experiments have shown that a larger pool of classifiers for dynamic selection
improves classification accuracy, and thus leads to an increase in average
inference time on the TinyML device.

</details>


### [46] [Sensor optimization for urban wind estimation with cluster-based probabilistic framework](https://arxiv.org/abs/2509.25222)
*Yutong Liang,Chang Hou,Guy Y. Cornejo Maceda,Andrea Ianiro,Stefano Discetti,Andrea Meilán-Vila,Didier Sornette,Sandro Claudio Lera,Jialong Chen,Xiaozhou He,Bernd R. Noack*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的机器学习框架，用于无人机在复杂城市地形中飞行时的传感器流场估计，能够处理复杂流场并优化传感器位置。


<details>
  <summary>Details</summary>
Motivation: 传统流场估计方法难以处理复杂城市地形中的流场，且传感器位置固定限制了应用范围，需要一种能够适应复杂流场并优化传感器布局的新方法。

Method: 采用基于雷诺数的流场变量缩放、物理域分解、子域聚类流场表示、子域间信息熵关联以及传感器输入与目标速度估计的多变量概率函数等五个关键技术。

Result: 该框架能够按比例适应域复杂度，在训练数据外进行外推，并允许传感器位置自由输入，通过三建筑群无人机飞行路径验证了有效性。

Conclusion: 该物理信息机器学习框架为复杂城市流场估计提供了有效解决方案，有望扩展到完整城市流场估计并整合天气输入。

Abstract: We propose a physics-informed machine-learned framework for sensor-based flow
estimation for drone trajectories in complex urban terrain. The input is a rich
set of flow simulations at many wind conditions. The outputs are velocity and
uncertainty estimates for a target domain and subsequent sensor optimization
for minimal uncertainty. The framework has three innovations compared to
traditional flow estimators. First, the algorithm scales proportionally to the
domain complexity, making it suitable for flows that are too complex for any
monolithic reduced-order representation. Second, the framework extrapolates
beyond the training data, e.g., smaller and larger wind velocities. Last, and
perhaps most importantly, the sensor location is a free input, significantly
extending the vast majority of the literature. The key enablers are (1) a
Reynolds number-based scaling of the flow variables, (2) a physics-based domain
decomposition, (3) a cluster-based flow representation for each subdomain, (4)
an information entropy correlating the subdomains, and (5) a multi-variate
probability function relating sensor input and targeted velocity estimates.
This framework is demonstrated using drone flight paths through a
three-building cluster as a simple example. We anticipate adaptations and
applications for estimating complete cities and incorporating weather input.

</details>


### [47] [Enhancing Linear Attention with Residual Learning](https://arxiv.org/abs/2509.25223)
*Xunhao Lai,Jialiang Kang,Jianqiao Lu,Tong Lin,Pengyu Zhao*

Main category: cs.LG

TL;DR: 提出了残差线性注意力（RLA）框架，通过显式残差拟合机制解决线性注意力在捕捉长程模式时的表达能力瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然提供线性时间复杂度的替代方案，但在捕捉长程模式方面表现不佳，主要因为现有变体只能进行历史预测和单标记校正，形成了表达能力瓶颈。

Method: 引入残差线性注意力（RLA）框架，通过维护辅助循环状态来累积残差误差并校正基础预测。具体实例化了残差Delta网络（RDN），包含自适应门控和残差裁剪机制以增强校正控制和稳定性。

Result: 在语言建模和召回密集型评估中，RLA和RDN始终优于各自基线和现代线性注意力方法，缩小了与标准Transformer的差距，同时保持线性缩放。

Conclusion: 残差线性注意力框架通过显式残差拟合机制有效解决了线性注意力的表达能力瓶颈，在保持线性复杂度的同时显著提升了性能。

Abstract: Linear attention offers a linear-time alternative to self-attention but often
struggles to capture long-range patterns. We revisit linear attention through a
prediction-correction lens and show that prevalent variants can be written as a
combination of a historical prediction and a single-token correction, which
creates an expressivity bottleneck. To address this bottleneck, we introduce
Residual Linear Attention (RLA), a framework that equips linear attention with
an explicit residual-fitting mechanism. RLA maintains an auxiliary recurrent
state that learns to accumulate residual errors over time and correct the base
prediction. We further instantiate a delta-rule version, Residual Delta Net
(RDN), incorporating adaptive gating and residual clipping for enhanced
correction control and stability. Our implementation leverages highly optimized
linear attention kernels and preserves linear time and memory. Across language
modeling and recall-intensive evaluations, RLA and RDN consistently outperform
their respective baselines and other modern linear-attention methods, narrowing
the gap to standard Transformers while retaining linear scaling.

</details>


### [48] [How Effective Are Time-Series Models for Rainfall Nowcasting? A Comprehensive Benchmark for Rainfall Nowcasting Incorporating PWV Data](https://arxiv.org/abs/2509.25263)
*Yifang Zhang,Pengfei Duan,Henan Wang,Shengwu Xiong*

Main category: cs.LG

TL;DR: 提出了RainfallBench基准测试，专注于降雨临近预报（0-3小时预测），包含5年全球12000多个GNSS站点的气象数据，特别引入了可降水量水汽(PWV)指标。还开发了BFPF模块来解决零膨胀和时间衰减问题。


<details>
  <summary>Details</summary>
Motivation: 现有气象时间序列预测基准主要评估温度、湿度等周期性强的变量，无法反映降雨临近预报这种更复杂实际场景的模型能力。降雨预测具有零膨胀、时间衰减和非平稳性等挑战。

Method: 构建RainfallBench数据集，包含5年15分钟间隔的6个关键气象变量数据。设计了专门的评估策略来测试多尺度预测和极端降雨事件。提出了BFPF（双向聚焦降水预报器）模块，整合领域先验知识来增强降雨时间序列预测。

Result: 在RainfallBench上评估了20多个最先进模型和6种主要架构。统计分析和消融研究验证了数据集的全面性和方法的优越性。BFPF模块有效解决了零膨胀和时间衰减问题。

Conclusion: RainfallBench填补了降雨临近预报基准测试的空白，为复杂气象场景的模型评估提供了标准。BFPF模块通过整合领域知识显著提升了降雨预测性能，为实际应用提供了有效解决方案。

Abstract: Rainfall nowcasting, which aims to predict precipitation within the next 0 to
3 hours, is critical for disaster mitigation and real-time response planning.
However, most time series forecasting benchmarks in meteorology are evaluated
on variables with strong periodicity, such as temperature and humidity, which
fail to reflect model capabilities in more complex and practically meteorology
scenarios like rainfall nowcasting. To address this gap, we propose
RainfallBench, a benchmark designed for rainfall nowcasting, a highly
challenging and practically relevant task characterized by zero inflation,
temporal decay, and non-stationarity, focused on predicting precipitation
within the next 0 to 3 hours. The dataset is derived from five years of
meteorological observations, recorded at 15-minute intervals across six
essential variables, and collected from more than 12,000 GNSS stations
globally. In particular, it incorporates precipitable water vapor (PWV), a
crucial indicator of rainfall that is absent in other datasets. We further
design specialized evaluation strategies to assess model performance on key
meteorological challenges, such as multi-scale prediction and extreme rainfall
events, and evaluate over 20 state-of-the-art models across six major
architectures on RainfallBench. Additionally, to address the zero-inflation and
temporal decay issues overlooked by existing models, we introduce Bi-Focus
Precipitation Forecaster (BFPF), a plug-and-play module that incorporates
domain-specific priors to enhance rainfall time series forecasting. Statistical
analysis and ablation studies validate the comprehensiveness of our dataset as
well as the superiority of our methodology. Code and datasets are available at
https://anonymous.4open.science/r/RainfallBench-A710.

</details>


### [49] [AMLA: MUL by ADD in FlashAttention Rescaling](https://arxiv.org/abs/2509.25224)
*Qichen Liao,Chengqiu Hu,Fangzheng Miao,Bao Li,Yiyang Liu,Junlong Lyu,Lirui Jiang,Jun Wang,Lingchao Zheng,Jun Li,Yuwei Fan*

Main category: cs.LG

TL;DR: AMLA是一种针对华为昇腾NPU优化的高性能内核，通过基于FlashAttention的新算法和预加载流水线策略，显著提升了多头潜在注意力(MLA)的计算效率，在昇腾910 NPU上达到614 TFLOPS，性能利用率达86.8%。


<details>
  <summary>Details</summary>
Motivation: 多头潜在注意力(MLA)虽然显著减少了KVCache内存使用，但带来了大量计算开销和中间变量扩展，对硬件实现效率提出了挑战，特别是在解码阶段。

Method: 1. 基于FlashAttention的新算法，用整数加法替代浮点乘法进行输出块重缩放，利用FP32和INT32表示之间的二进制对应关系；2. 预加载流水线策略与分层分块，最大化FLOPS利用率。

Result: 在昇腾910 NPU上，AMLA达到614 TFLOPS，达到理论最大FLOPS的86.8%，优于最先进的开源FlashMLA实现（在NVIDIA H800 SXM5上FLOPS利用率最高为66.7%）。

Conclusion: AMLA内核已集成到华为CANN中，即将发布，为MLA在昇腾NPU上的高效实现提供了解决方案。

Abstract: Multi-head Latent Attention (MLA) significantly reduces KVCache memory usage
in Large Language Models while introducing substantial computational overhead
and intermediate variable expansion. This poses challenges for efficient
hardware implementation -- especially during the decode phase. This paper
introduces Ascend MLA (AMLA), a high-performance kernel specifically optimized
for Huawei's Ascend NPUs. AMLA is built on two core innovations: (1) A novel
FlashAttention-based algorithm that replaces floating-point multiplications
with integer additions for output block rescaling, leveraging binary
correspondence between FP32 and INT32 representations; (2) A Preload Pipeline
strategy with hierarchical tiling that maximizes FLOPS utilization: the Preload
Pipeline achieves Cube-bound performance, while hierarchical tiling overlaps
data movement and computation within the Cube core. Experiments show that on
Ascend 910 NPUs (integrated in CloudMatrix384), AMLA achieves up to 614 TFLOPS,
reaching 86.8% of the theoretical maximum FLOPS, outperforming the
state-of-the-art open-source FlashMLA implementation, whose FLOPS utilization
is up to 66.7% on NVIDIA H800 SXM5. The AMLA kernel has been integrated into
Huawei's CANN and will be released soon.

</details>


### [50] [MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series](https://arxiv.org/abs/2509.25278)
*Payal Mohapatra,Yueyuan Sui,Akash Pandey,Stephen Xia,Qi Zhu*

Main category: cs.LG

TL;DR: MAESTRO是一个新颖的多模态学习框架，解决了现有方法依赖单一主要模态、成对建模和完整模态观测的局限性，通过动态跨模态交互、符号化标记和稀疏专家混合机制，在完整和部分观测下均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有多模态学习方法在现实世界时间序列应用中存在三个主要限制：依赖单一主要模态进行对齐、成对建模模态、假设完整模态观测，这在实际传感器监测场景中不实用，因为主要模态先验不明确、模态数量多、传感器故障导致任意缺失观测。

Method: MAESTRO通过动态模态内和跨模态交互、符号化标记和自适应注意力预算构建长多模态序列，使用稀疏跨模态注意力处理，并通过稀疏专家混合机制实现不同模态组合下的黑盒专业化。

Result: 在四个数据集上评估，MAESTRO在完整观测下比最佳多模态和多变量方法分别平均提升4%和8%，在缺失40%模态的部分观测下平均提升9%，证明了其稀疏、模态感知设计的鲁棒性和效率。

Conclusion: MAESTRO框架通过克服现有多模态学习方法的局限性，为现实世界多模态时间序列分析提供了更实用和鲁棒的解决方案，特别适用于传感器监测场景中的动态时间序列学习。

Abstract: From clinical healthcare to daily living, continuous sensor monitoring across
multiple modalities has shown great promise for real-world intelligent
decision-making but also faces various challenges. In this work, we introduce
MAESTRO, a novel framework that overcomes key limitations of existing
multimodal learning approaches: (1) reliance on a single primary modality for
alignment, (2) pairwise modeling of modalities, and (3) assumption of complete
modality observations. These limitations hinder the applicability of these
approaches in real-world multimodal time-series settings, where primary
modality priors are often unclear, the number of modalities can be large
(making pairwise modeling impractical), and sensor failures often result in
arbitrary missing observations. At its core, MAESTRO facilitates dynamic intra-
and cross-modal interactions based on task relevance, and leverages symbolic
tokenization and adaptive attention budgeting to construct long multimodal
sequences, which are processed via sparse cross-modal attention. The resulting
cross-modal tokens are routed through a sparse Mixture-of-Experts (MoE)
mechanism, enabling black-box specialization under varying modality
combinations. We evaluate MAESTRO against 10 baselines on four diverse datasets
spanning three applications, and observe average relative improvements of 4%
and 8% over the best existing multimodal and multivariate approaches,
respectively, under complete observations. Under partial observations -- with
up to 40% of missing modalities -- MAESTRO achieves an average 9% improvement.
Further analysis also demonstrates the robustness and efficiency of MAESTRO's
sparse, modality-aware design for learning from dynamic time series.

</details>


### [51] [MSCoD: An Enhanced Bayesian Updating Framework with Multi-Scale Information Bottleneck and Cooperative Attention for Structure-Based Drug Design](https://arxiv.org/abs/2509.25225)
*Long Xu,Yongcai Chen,Fengshuo Liu,Yuzhong Peng*

Main category: cs.LG

TL;DR: MSCoD是一个基于贝叶斯更新的生成框架，用于基于结构的药物设计，通过多尺度信息瓶颈和多头协作注意力机制解决蛋白质-配体相互作用的多尺度层次结构和不对称性问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于结构的药物设计方法难以捕捉蛋白质-配体相互作用的多尺度层次结构和内在不对称性，限制了复杂相互作用的准确建模。

Method: 提出了多尺度信息瓶颈(MSIB)进行多抽象层次的语义压缩，以及多头协作注意力(MHCA)机制，使用不对称的蛋白质到配体注意力来捕捉多样化的相互作用类型。

Result: 在基准数据集上，MSCoD优于最先进的方法。在KRAS G12D等挑战性靶点的案例研究中进一步证明了其在实际场景中的适用性。

Conclusion: MSCoD通过多尺度特征提取和不对称注意力机制，有效解决了蛋白质-配体相互作用建模中的关键挑战，在药物设计中表现出优越性能。

Abstract: Structure-Based Drug Design (SBDD) is a powerful strategy in computational
drug discovery, utilizing three-dimensional protein structures to guide the
design of molecules with improved binding affinity. However, capturing complex
protein-ligand interactions across multiple scales remains challenging, as
current methods often overlook the hierarchical organization and intrinsic
asymmetry of these interactions. To address these limitations, we propose
MSCoD, a novel Bayesian updating-based generative framework for structure-based
drug design. In our MSCoD, Multi-Scale Information Bottleneck (MSIB) was
developed, which enables semantic compression at multiple abstraction levels
for efficient hierarchical feature extraction. Furthermore, a multi-head
cooperative attention (MHCA) mechanism was developed, which employs asymmetric
protein-to-ligand attention to capture diverse interaction types while
addressing the dimensionality disparity between proteins and ligands. Empirical
studies showed that MSCoD outperforms state-of-the-art methods on the benchmark
dataset. Case studies on challenging targets such as KRAS G12D further
demonstrate its applicability in real-world scenarios. The code and data
underlying this article are freely available at
https://github.com/xulong0826/MSCoD.

</details>


### [52] [Gradient Descent with Large Step Sizes: Chaos and Fractal Convergence Region](https://arxiv.org/abs/2509.25351)
*Shuang Liang,Guido Montúfar*

Main category: cs.LG

TL;DR: 论文研究了矩阵分解中梯度下降在大步长下的行为，发现参数空间会出现分形结构。推导了标量-向量分解的临界步长，并显示在临界点附近，选择的极小值对初始化高度敏感。正则化会放大这种敏感性，产生分形边界。


<details>
  <summary>Details</summary>
Motivation: 探索梯度下降在矩阵分解中的动态行为，特别是大步长下的收敛特性和参数空间结构，理解临界步长附近的不稳定性和混沌现象。

Method: 通过理论分析推导标量-向量分解的临界步长，研究正则化对收敛边界的影响，并将分析扩展到一般矩阵分解的正交初始化情况。

Result: 发现大步长下梯度下降会产生分形结构，临界步长附近极小值选择对初始化高度敏感，正则化会增强这种敏感性并产生分形边界。

Conclusion: 临界步长附近的梯度下降进入混沌状态，长期动态不可预测，且不存在简单的隐式偏好（如平衡性、最小范数或平坦性）。

Abstract: We examine gradient descent in matrix factorization and show that under large
step sizes the parameter space develops a fractal structure. We derive the
exact critical step size for convergence in scalar-vector factorization and
show that near criticality the selected minimizer depends sensitively on the
initialization. Moreover, we show that adding regularization amplifies this
sensitivity, generating a fractal boundary between initializations that
converge and those that diverge. The analysis extends to general matrix
factorization with orthogonal initialization. Our findings reveal that
near-critical step sizes induce a chaotic regime of gradient descent where the
long-term dynamics are unpredictable and there are no simple implicit biases,
such as towards balancedness, minimum norm, or flatness.

</details>


### [53] [Integrated Forecasting of Marine Renewable Power: An Adaptively Bayesian-Optimized MVMD-LSTM Framework for Wind-Solar-Wave Energy](https://arxiv.org/abs/2509.25226)
*Baoyi Xie,Shuiling Shi,Wenqi Liu*

Main category: cs.LG

TL;DR: 提出了一种基于贝叶斯优化的多元变分模态分解-长短期记忆网络框架，用于集成风-光-波海洋能源系统的超短期功率预测，显著提升了预测精度和自动化程度。


<details>
  <summary>Details</summary>
Motivation: 现有预测方法多为单能源独立建模，未能充分考虑多能源间的复杂耦合关系，且难以捕捉系统的非线性和非平稳动态特性，限制了预测性能和实用性。

Method: 使用MVMD联合分解风、光、波功率序列以保持跨源耦合关系；采用贝叶斯优化自动搜索MVMD中的模态数量和惩罚参数；最后用LSTM对得到的IMF进行建模实现超短期功率预测。

Result: 基于中国某海上集成能源平台的现场测量数据实验表明，该框架在MAPE、RMSE和MAE指标上显著优于基准模型。

Conclusion: 所提框架具有优越的预测精度、鲁棒性和自动化程度，为集成海洋能源系统的安全运行和优化调度提供了有效支持。

Abstract: Integrated wind-solar-wave marine energy systems hold broad promise for
supplying clean electricity in offshore and coastal regions. By leveraging the
spatiotemporal complementarity of multiple resources, such systems can
effectively mitigate the intermittency and volatility of single-source outputs,
thereby substantially improving overall power-generation efficiency and
resource utilization. Accurate ultra-short-term forecasting is crucial for
ensuring secure operation and optimizing proactive dispatch. However, most
existing forecasting methods construct separate models for each energy source,
insufficiently account for the complex couplings among multiple energies,
struggle to capture the system's nonlinear and nonstationary dynamics, and
typically depend on extensive manual parameter tuning-limitations that
constrain both predictive performance and practicality. We address this issue
using a Bayesian-optimized Multivariate Variational Mode Decomposition-Long
Short-Term Memory (MVMD-LSTM) framework. The framework first applies MVMD to
jointly decompose wind, solar and wave power series so as to preserve
cross-source couplings; it uses Bayesian optimization to automatically search
the number of modes and the penalty parameter in the MVMD process to obtain
intrinsic mode functions (IMFs); finally, an LSTM models the resulting IMFs to
achieve ultra-short-term power forecasting for the integrated system.
Experiments based on field measurements from an offshore integrated energy
platform in China show that the proposed framework significantly outperforms
benchmark models in terms of MAPE, RMSE and MAE. The results demonstrate
superior predictive accuracy, robustness, and degree of automation.

</details>


### [54] [Flow Matching with Semidiscrete Couplings](https://arxiv.org/abs/2509.25519)
*Alireza Mousavi-Hosseini,Stephen Y. Zhang,Michal Klein,Marco Cuturi*

Main category: cs.LG

TL;DR: 本文提出了一种半离散流匹配方法（SD-FM），通过利用目标数据集分布的有限性，解决了传统最优传输流匹配（OT-FM）在批量大小增长时计算成本过高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统OT-FM方法虽然理论上很有前景，但在实践中由于Sinkhorn算法的计算成本随批量大小呈二次方增长而难以广泛应用。作者希望找到一种更高效的方法来实现OT-FM的理论优势。

Method: 采用半离散最优传输（SD-OT）方法，通过SGD估计对偶势向量，然后使用最大内积搜索（MIPS）将新采样的噪声向量与数据点进行匹配，从而消除了对n/ε的二次依赖。

Result: SD-FM在所有训练指标和推理预算约束下都优于FM和OT-FM，在多个数据集、无条件/条件生成以及使用均值流模型的情况下都表现出色。

Conclusion: SD-FM方法成功解决了OT-FM的计算瓶颈问题，实现了更好的生成性能，为流匹配模型的实际应用提供了更高效的解决方案。

Abstract: Flow models parameterized as time-dependent velocity fields can generate data
from noise by integrating an ODE. These models are often trained using flow
matching, i.e. by sampling random pairs of noise and target points
$(\mathbf{x}_0,\mathbf{x}_1)$ and ensuring that the velocity field is aligned,
on average, with $\mathbf{x}_1-\mathbf{x}_0$ when evaluated along a segment
linking $\mathbf{x}_0$ to $\mathbf{x}_1$. While these pairs are sampled
independently by default, they can also be selected more carefully by matching
batches of $n$ noise to $n$ target points using an optimal transport (OT)
solver. Although promising in theory, the OT flow matching (OT-FM) approach is
not widely used in practice. Zhang et al. (2025) pointed out recently that
OT-FM truly starts paying off when the batch size $n$ grows significantly,
which only a multi-GPU implementation of the Sinkhorn algorithm can handle.
Unfortunately, the costs of running Sinkhorn can quickly balloon, requiring
$O(n^2/\varepsilon^2)$ operations for every $n$ pairs used to fit the velocity
field, where $\varepsilon$ is a regularization parameter that should be
typically small to yield better results. To fulfill the theoretical promises of
OT-FM, we propose to move away from batch-OT and rely instead on a semidiscrete
formulation that leverages the fact that the target dataset distribution is
usually of finite size $N$. The SD-OT problem is solved by estimating a dual
potential vector using SGD; using that vector, freshly sampled noise vectors at
train time can then be matched with data points at the cost of a maximum inner
product search (MIPS). Semidiscrete FM (SD-FM) removes the quadratic dependency
on $n/\varepsilon$ that bottlenecks OT-FM. SD-FM beats both FM and OT-FM on all
training metrics and inference budget constraints, across multiple datasets, on
unconditional/conditional generation, or when using mean-flow models.

</details>


### [55] [Simple, Fast and Efficient Injective Manifold Density Estimation with Random Projections](https://arxiv.org/abs/2509.25228)
*Ahmad Ayaz Amin*

Main category: cs.LG

TL;DR: 提出了随机投影流(RPFs)，这是一个基于随机矩阵理论和随机投影几何学的可逆归一化流框架，使用随机半正交矩阵将数据投影到低维潜在空间。


<details>
  <summary>Details</summary>
Motivation: 为可逆归一化流提供一个理论严谨且实用的框架，连接随机投影理论与归一化流，克服PCA流和学习的可逆映射的局限性。

Method: 使用通过高斯矩阵QR分解从Haar分布正交系综中抽取的随机半正交矩阵，将数据投影到低维潜在空间用于基分布，并提供黎曼体积修正项的闭式表达式。

Result: RPFs既是理论严谨的，又具有实际效果，为生成建模提供了强大的基线方法。

Conclusion: RPFs提供了一个即插即用、高效的可逆归一化流框架，在随机投影理论与归一化流之间建立了桥梁。

Abstract: We introduce Random Projection Flows (RPFs), a principled framework for
injective normalizing flows that leverages tools from random matrix theory and
the geometry of random projections. RPFs employ random semi-orthogonal
matrices, drawn from Haar-distributed orthogonal ensembles via QR decomposition
of Gaussian matrices, to project data into lower-dimensional latent spaces for
the base distribution. Unlike PCA-based flows or learned injective maps, RPFs
are plug-and-play, efficient, and yield closed-form expressions for the
Riemannian volume correction term. We demonstrate that RPFs are both
theoretically grounded and practically effective, providing a strong baseline
for generative modeling and a bridge between random projection theory and
normalizing flows.

</details>


### [56] [Meta-Router: Bridging Gold-standard and Preference-based Evaluations in Large Language Model Routing](https://arxiv.org/abs/2509.25535)
*Yichi Zhang,Fangzheng Xie,Shu Yang,Chong Wu*

Main category: cs.LG

TL;DR: 提出一种基于因果推断的LLM路由器训练框架，通过整合黄金标准和偏好数据来纠正偏好数据偏差，提高路由准确性和成本-质量权衡。


<details>
  <summary>Details</summary>
Motivation: 在需要大量人机交互的语言任务中，为每个查询部署单一"最佳"模型成本高昂。现有路由器训练面临可靠监督数据稀缺的问题：黄金标准数据准确但成本高，偏好数据便宜但存在偏差。

Method: 将LLM路由器训练问题转化为因果推断框架，将响应评估机制视为处理分配。基于此视角开发集成因果路由器训练框架，纠正偏好数据偏差，处理两种数据源的不平衡，提高路由鲁棒性和效率。

Result: 数值实验表明，该方法能提供更准确的路由，并改善成本与质量之间的权衡。

Conclusion: 通过因果推断视角整合黄金标准和偏好数据，能够有效纠正偏好数据偏差，实现更高效的LLM路由器训练。

Abstract: In language tasks that require extensive human--model interaction, deploying
a single "best" model for every query can be expensive. To reduce inference
cost while preserving the quality of the responses, a large language model
(LLM) router selects the most appropriate model from a pool of candidates for
each query. A central challenge to training a high-quality router is the
scarcity of reliable supervision. Gold-standard data (e.g., expert-verified
labels or rubric-based scores) provide accurate quality evaluations of LLM
responses but are costly and difficult to scale. In contrast, preference-based
data, collected via crowdsourcing or LLM-as-a-judge systems, are cheaper and
more scalable, yet often biased in reflecting the true quality of responses. We
cast the problem of LLM router training with combined gold-standard and
preference-based data into a causal inference framework by viewing the response
evaluation mechanism as the treatment assignment. This perspective further
reveals that the bias in preference-based data corresponds to the well-known
causal estimand: the conditional average treatment effect. Based on this new
perspective, we develop an integrative causal router training framework that
corrects preference-data bias, address imbalances between two data sources, and
improve routing robustness and efficiency. Numerical experiments demonstrate
that our approach delivers more accurate routing and improves the trade-off
between cost and quality.

</details>


### [57] [Energy Guided Geometric Flow Matching](https://arxiv.org/abs/2509.25230)
*Aaron Zweig,Mingxuan Zhang,Elham Azizi,David Knowles*

Main category: cs.LG

TL;DR: 提出使用分数匹配和退火能量蒸馏学习度量张量，以更准确地捕捉数据几何结构并指导流生成，解决传统流匹配方法在高维空间中面临的维度灾难问题。


<details>
  <summary>Details</summary>
Motivation: 时间数据的有效归纳偏置是轨迹应保持在数据流形附近。传统流匹配方法依赖直线条件路径，而学习测地线的流匹配方法使用RBF核或最近邻图，这些方法在高维空间中面临维度灾难问题。

Method: 使用分数匹配和退火能量蒸馏来学习度量张量，该度量张量能够忠实捕捉底层数据几何结构，并为更准确的流提供信息。

Result: 在具有解析测地线的合成流形上验证了该策略的有效性，并应用于细胞插值任务。

Conclusion: 提出的方法能够有效学习数据几何结构，生成更准确的流，解决了传统方法在高维空间中的局限性。

Abstract: A useful inductive bias for temporal data is that trajectories should stay
close to the data manifold. Traditional flow matching relies on straight
conditional paths, and flow matching methods which learn geodesics rely on RBF
kernels or nearest neighbor graphs that suffer from the curse of
dimensionality. We propose to use score matching and annealed energy
distillation to learn a metric tensor that faithfully captures the underlying
data geometry and informs more accurate flows. We demonstrate the efficacy of
this strategy on synthetic manifolds with analytic geodesics, and interpolation
of cell

</details>


### [58] [BaB-prob: Branch and Bound with Preactivation Splitting for Probabilistic Verification of Neural Networks](https://arxiv.org/abs/2509.25647)
*Fangji Wang,Panagiotis Tsiotras*

Main category: cs.LG

TL;DR: 将分支定界与预激活分裂扩展到概率验证，提出BaB-prob方法，在线性边界传播基础上通过预激活分裂迭代划分子问题，证明了对ReLU网络的声音性和完备性，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分支定界与预激活分裂在确定性神经网络验证中已证明有效，但需要扩展到概率验证场景，以处理更复杂的验证问题。

Method: 提出BaB-prob方法，通过预激活分裂将原问题迭代划分为子问题，利用线性边界传播计算每个子问题的概率边界，引入不确定性水平概念并设计两种高效分裂策略。

Result: 在未训练网络、MNIST和CIFAR-10模型以及VNN-COMP 2025基准测试中，BaB-prob在中高维输入问题上始终优于最先进方法。

Conclusion: BaB-prob成功将分支定界框架扩展到概率验证，为神经网络验证提供了有效的概率验证工具，特别适用于中高维输入问题。

Abstract: Branch-and-bound with preactivation splitting has been shown highly effective
for deterministic verification of neural networks. In this paper, we extend
this framework to the probabilistic setting. We propose BaB-prob that
iteratively divides the original problem into subproblems by splitting
preactivations and leverages linear bounds computed by linear bound propagation
to bound the probability for each subproblem. We prove soundness and
completeness of BaB-prob for feedforward-ReLU neural networks. Furthermore, we
introduce the notion of uncertainty level and design two efficient strategies
for preactivation splitting, yielding BaB-prob-ordered and BaB+BaBSR-prob. We
evaluate BaB-prob on untrained networks, MNIST and CIFAR-10 models,
respectively, and VNN-COMP 2025 benchmarks. Across these settings, our approach
consistently outperforms state-of-the-art approaches in medium- to
high-dimensional input problems.

</details>


### [59] [WDformer: A Wavelet-based Differential Transformer Model for Time Series Forecasting](https://arxiv.org/abs/2509.25231)
*Xiaojian Wang,Chaoli Zhang,Zhonglong Zheng,Yunliang Jiang*

Main category: cs.LG

TL;DR: WDformer是一种基于小波变换的差分Transformer模型，通过多分辨率分析和差分注意力机制，在多个真实世界数据集上实现了最先进的时序预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统时序预测方法仅依赖时域或频域建模，无法充分利用多域信息，且传统注意力机制容易过度关注不相关的历史信息，引入噪声导致预测偏差。

Method: 使用小波变换对时序数据进行多分辨率分析，结合时频域联合表示提取关键特征；采用维度反转的注意力机制捕捉多变量关系；引入差分注意力机制，通过两个独立softmax注意力矩阵的差值计算注意力分数。

Result: WDformer在多个具有挑战性的真实世界数据集上取得了最先进(SOTA)的结果，证明了其准确性和有效性。

Conclusion: WDformer通过小波变换和差分注意力机制的创新结合，有效解决了时序预测中的多域信息利用和噪声抑制问题，展现出优异的预测性能。

Abstract: Time series forecasting has various applications, such as meteorological
rainfall prediction, traffic flow analysis, financial forecasting, and
operational load monitoring for various systems. Due to the sparsity of time
series data, relying solely on time-domain or frequency-domain modeling limits
the model's ability to fully leverage multi-domain information. Moreover, when
applied to time series forecasting tasks, traditional attention mechanisms tend
to over-focus on irrelevant historical information, which may introduce noise
into the prediction process, leading to biased results. We proposed WDformer, a
wavelet-based differential Transformer model. This study employs the wavelet
transform to conduct a multi-resolution analysis of time series data. By
leveraging the advantages of joint representation in the time-frequency domain,
it accurately extracts the key information components that reflect the
essential characteristics of the data. Furthermore, we apply attention
mechanisms on inverted dimensions, allowing the attention mechanism to capture
relationships between multiple variables. When performing attention
calculations, we introduced the differential attention mechanism, which
computes the attention score by taking the difference between two separate
softmax attention matrices. This approach enables the model to focus more on
important information and reduce noise. WDformer has achieved state-of-the-art
(SOTA) results on multiple challenging real-world datasets, demonstrating its
accuracy and effectiveness. Code is available at
https://github.com/xiaowangbc/WDformer.

</details>


### [60] [Annotation-Efficient Active Test-Time Adaptation with Conformal Prediction](https://arxiv.org/abs/2509.25692)
*Tingyu Shi,Fan Lyu,Shaoliang Peng*

Main category: cs.LG

TL;DR: 提出了CPATTA方法，将保形预测引入主动测试时适应，通过保形评分和在线权重更新算法，在保证覆盖率的同时提高数据选择效率，比现有ATTA方法准确率提升约5%。


<details>
  <summary>Details</summary>
Motivation: 现有主动测试时适应方法使用启发式不确定性度量，数据选择效率低，浪费人工标注预算。

Method: 使用平滑保形评分和top-K确定性度量，基于伪覆盖率的在线权重更新算法，域偏移检测器调整人工监督，以及分阶段更新方案平衡人工标注和模型标注数据。

Result: 在广泛实验中，CPATTA始终优于最先进的ATTA方法，准确率提升约5%。

Conclusion: CPATTA成功将原则性、覆盖率保证的不确定性引入ATTA，显著提高了模型在域偏移下的鲁棒性和数据选择效率。

Abstract: Active Test-Time Adaptation (ATTA) improves model robustness under domain
shift by selectively querying human annotations at deployment, but existing
methods use heuristic uncertainty measures and suffer from low data selection
efficiency, wasting human annotation budget. We propose Conformal Prediction
Active TTA (CPATTA), which first brings principled, coverage-guaranteed
uncertainty into ATTA. CPATTA employs smoothed conformal scores with a top-K
certainty measure, an online weight-update algorithm driven by pseudo coverage,
a domain-shift detector that adapts human supervision, and a staged update
scheme balances human-labeled and model-labeled data. Extensive experiments
demonstrate that CPATTA consistently outperforms the state-of-the-art ATTA
methods by around 5% in accuracy. Our code and datasets are available at
https://github.com/tingyushi/CPATTA.

</details>


### [61] [Sampling via Gaussian Mixture Approximations](https://arxiv.org/abs/2509.25232)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 提出了一系列高斯混合近似（GMA）采样器，用于从非归一化目标密度中采样，包括权重GMA、拉普拉斯混合近似、EM-GMA等变体。该方法采用两阶段范式：初始化高斯组件并采样，然后通过优化组件权重或均值和方差来拟合目标分布。


<details>
  <summary>Details</summary>
Motivation: 开发一种无需梯度、计算高效的采样方法，能够从非归一化目标密度中生成忠实样本，同时利用高斯分布易于采样的特性和高效优化方法。

Method: 两阶段方法：1）初始化高斯组件集并从混合提议分布中采样；2）通过基于样本的KL散度目标优化组件权重或均值和方差来拟合目标分布，然后进行分层重采样。

Result: 在温和条件下产生一致的近似，经验结果验证了该方法在各种密度上的准确性和速度。

Conclusion: GMA采样器提供了一种梯度自由、计算高效的采样方法，能够从非归一化目标密度生成忠实样本，具有一致的理论保证和良好的实证性能。

Abstract: We present a family of \textit{Gaussian Mixture Approximation} (GMA) samplers
for sampling unnormalised target densities, encompassing \textit{weights-only
GMA} (W-GMA), \textit{Laplace Mixture Approximation} (LMA),
\textit{expectation-maximization GMA} (EM-GMA), and further variants. GMA
adopts a simple two-stage paradigm: (i) initialise a finite set of Gaussian
components and draw samples from a proposal mixture; (ii) fit the mixture to
the target by optimising either only the component weights or also the means
and variances, via a sample-based KL divergence objective that requires only
evaluations of the unnormalised density, followed by stratified resampling. The
method is gradient-free, and computationally efficient: it leverages the ease
of sampling from Gaussians, efficient optimisation methods (projected gradient
descent, mirror descent, and EM), and the robustness of stratified resampling
to produce samples faithful to the target. We show that this
optimisation-resampling scheme yields consistent approximations under mild
conditions, and we validate this methodology with empirical results
demonstrating accuracy and speed across diverse densities.

</details>


### [62] [Online Decision Making with Generative Action Sets](https://arxiv.org/abs/2509.25777)
*Jianyu Xu,Vidhi Jain,Bryan Wilder,Aarti Singh*

Main category: cs.LG

TL;DR: 本文提出了一种双重乐观算法，用于解决在线学习中动态生成新动作的问题，通过LCB选择动作和UCB生成动作，在医疗问答数据集上取得了良好的生成-质量权衡，并证明了最优遗憾界。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，智能体可以在在线学习过程中动态创建新动作，但动作生成通常需要成本，需要在潜在收益与成本之间取得平衡。本文旨在解决这种在线学习问题，其中智能体可以在任何时间步支付一次性成本生成新动作，这些动作将永久可用。

Method: 提出了一种双重乐观算法，使用下置信界(LCB)进行动作选择，使用上置信界(UCB)进行动作生成，以平衡利用、探索和创建之间的三角权衡关系。

Result: 在医疗问答数据集上的实证评估表明，该方法相比基线策略实现了更优的生成-质量权衡。理论分析证明算法达到了最优遗憾界O(T^(d/(d+2))d^(d/(d+2)) + d√(TlogT))，这是在线学习扩展动作空间的第一个次线性遗憾界。

Conclusion: 该双重乐观算法有效解决了在线学习中动态生成新动作的挑战，在理论和实验上都表现出色，为具有扩展动作空间的在线学习问题提供了首个次线性遗憾保证。

Abstract: With advances in generative AI, decision-making agents can now dynamically
create new actions during online learning, but action generation typically
incurs costs that must be balanced against potential benefits. We study an
online learning problem where an agent can generate new actions at any time
step by paying a one-time cost, with these actions becoming permanently
available for future use. The challenge lies in learning the optimal sequence
of two-fold decisions: which action to take and when to generate new ones,
further complicated by the triangular tradeoffs among exploitation, exploration
and $\textit{creation}$. To solve this problem, we propose a doubly-optimistic
algorithm that employs Lower Confidence Bounds (LCB) for action selection and
Upper Confidence Bounds (UCB) for action generation. Empirical evaluation on
healthcare question-answering datasets demonstrates that our approach achieves
favorable generation-quality tradeoffs compared to baseline strategies. From
theoretical perspectives, we prove that our algorithm achieves the optimal
regret of $O(T^{\frac{d}{d+2}}d^{\frac{d}{d+2}} + d\sqrt{T\log T})$, providing
the first sublinear regret bound for online learning with expanding action
spaces.

</details>


### [63] [FedCLF -- Towards Efficient Participant Selection for Federated Learning in Heterogeneous IoV Networks](https://arxiv.org/abs/2509.25233)
*Kasun Eranda Wijethilake,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

TL;DR: 提出了FedCLF方法，通过校准损失和反馈控制机制解决联邦学习在车辆物联网网络中的异构数据挑战，显著提升模型精度和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在车辆物联网网络中面临高度数据异构性和设备异构性的挑战，需要改进参与方选择和资源优化机制。

Method: FedCLF引入校准损失作为参与方选择的效用指标，并使用反馈控制机制动态调整客户端采样频率。

Result: 在CIFAR-10数据集上，FedCLF相比基线模型（FedAvg、Newt、Oort）在高数据异构性场景下性能提升达16%，同时通过减少采样频率提高了效率。

Conclusion: FedCLF能有效应对联邦学习在车辆物联网网络中的异构数据挑战，显著提升模型精度和资源利用效率。

Abstract: Federated Learning (FL) is a distributed machine learning technique that
preserves data privacy by sharing only the trained parameters instead of the
client data. This makes FL ideal for highly dynamic, heterogeneous, and
time-critical applications, in particular, the Internet of Vehicles (IoV)
networks. However, FL encounters considerable challenges in such networks owing
to the high data and device heterogeneity. To address these challenges, we
propose FedCLF, i.e., FL with Calibrated Loss and Feedback control, which
introduces calibrated loss as a utility in the participant selection process
and a feedback control mechanism to dynamically adjust the sampling frequency
of the clients. The envisaged approach (a) enhances the overall model accuracy
in case of highly heterogeneous data and (b) optimizes the resource utilization
for resource constrained IoV networks, thereby leading to increased efficiency
in the FL process. We evaluated FedCLF vis-\`a-vis baseline models, i.e.,
FedAvg, Newt, and Oort, using CIFAR-10 dataset with varying data heterogeneity.
Our results depict that FedCLF significantly outperforms the baseline models by
up to a 16% improvement in high data heterogeneity-related scenarios with
improved efficiency via reduced sampling frequency.

</details>


### [64] [Learning to Reason as Action Abstractions with Scalable Mid-Training RL](https://arxiv.org/abs/2509.25810)
*Shenao Zhang,Donghan Yu,Yihao Feng,Bowen Jin,Zhaoran Wang,John Peebles,Zirui Wang*

Main category: cs.LG

TL;DR: 论文提出了RA3算法，通过中期训练阶段识别紧凑的动作子空间，提升大语言模型在强化学习中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在强化学习中表现优异，但完全发挥其潜力需要中期训练阶段来识别有用的动作子空间并实现快速选择。

Method: 提出RA3算法，通过推导序列变分下界，利用强化学习迭代发现时间一致的潜在结构，然后在引导数据上进行微调。

Result: 在代码生成任务中，RA3在HumanEval和MBPP上的平均性能比基础模型和下一个token预测基线分别提高了8和4个百分点，在多个基准测试中实现更快收敛和更高渐近性能。

Conclusion: 中期训练在决策空间紧凑且有效视野较短时最有效，强调在动作抽象空间而非原始动作空间操作的重要性。

Abstract: Large language models excel with reinforcement learning (RL), but fully
unlocking this potential requires a mid-training stage. An effective
mid-training phase should identify a compact set of useful actions and enable
fast selection among them through online RL. We formalize this intuition by
presenting the first theoretical result on how mid-training shapes
post-training: it characterizes an action subspace that minimizes both the
value approximation error from pruning and the RL error during subsequent
planning. Our analysis reveals two key determinants of mid-training
effectiveness: pruning efficiency, which shapes the prior of the initial RL
policy, and its impact on RL convergence, which governs the extent to which
that policy can be improved via online interactions. These results suggest that
mid-training is most effective when the decision space is compact and the
effective horizon is short, highlighting the importance of operating in the
space of action abstractions rather than primitive actions. Building on these
insights, we propose Reasoning as Action Abstractions (RA3), a scalable
mid-training algorithm. Specifically, we derive a sequential variational lower
bound and optimize it by iteratively discovering temporally-consistent latent
structures via RL, followed by fine-tuning on the bootstrapped data.
Experiments on code generation tasks demonstrate the effectiveness of our
approach. Across multiple base models, RA3 improves the average performance on
HumanEval and MBPP by 8 and 4 points over the base model and the next-token
prediction baseline. Furthermore, RA3 achieves faster convergence and higher
asymptotic performance in RLVR on HumanEval+, MBPP+, LiveCodeBench, and
Codeforces.

</details>


### [65] [Machine Learning for Pattern Detection in Printhead Nozzle Logging](https://arxiv.org/abs/2509.25235)
*Nikola Prianikov,Evelyne Janssen-van Dam,Marcin Pietrasik,Charalampos S. Kouzinopoulos*

Main category: cs.LG

TL;DR: 提出基于机器学习的打印头故障分类方法，使用时间序列和空间特征，随机森林模型在多个故障机制上优于基于规则的基准方法。


<details>
  <summary>Details</summary>
Motivation: 佳能生产打印的打印头故障可以通过喷嘴行为识别，准确识别故障机制对确保产品质量至关重要。

Method: 采用基于特征的时间序列分类框架，结合领域专家指导选择时间和空间特征，评估多个传统ML分类器。

Result: One-vs-Rest随机森林表现最佳，在多个故障机制的加权F1分数上优于内部基于规则的基准方法。

Conclusion: 机器学习方法能有效分类打印头故障机制，为产品质量控制提供了更准确的解决方案。

Abstract: Correct identification of failure mechanisms is essential for manufacturers
to ensure the quality of their products. Certain failures of printheads
developed by Canon Production Printing can be identified from the behavior of
individual nozzles, the states of which are constantly recorded and can form
distinct patterns in terms of the number of failed nozzles over time, and in
space in the nozzle grid. In our work, we investigate the problem of printhead
failure classification based on a multifaceted dataset of nozzle logging and
propose a Machine Learning classification approach for this problem. We follow
the feature-based framework of time-series classification, where a set of
time-based and spatial features was selected with the guidance of domain
experts. Several traditional ML classifiers were evaluated, and the One-vs-Rest
Random Forest was found to have the best performance. The proposed model
outperformed an in-house rule-based baseline in terms of a weighted F1 score
for several failure mechanisms.

</details>


### [66] [Decentralized Asynchronous Multi-player Bandits](https://arxiv.org/abs/2509.25824)
*Jingqi Fan,Canzhe Zhao,Shuai Li,Siwei Wang*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In recent years, multi-player multi-armed bandits (MP-MAB) have been
extensively studied due to their wide applications in cognitive radio networks
and Internet of Things systems. While most existing research on MP-MAB focuses
on synchronized settings, real-world systems are often decentralized and
asynchronous, where players may enter or leave the system at arbitrary times,
and do not have a global clock. This decentralized asynchronous setting
introduces two major challenges. First, without a global time, players cannot
implicitly coordinate their actions through time, making it difficult to avoid
collisions. Second, it is important to detect how many players are in the
system, but doing so may cost a lot. In this paper, we address the challenges
posed by such a fully asynchronous setting in a decentralized environment. We
develop a novel algorithm in which players adaptively change between
exploration and exploitation. During exploration, players uniformly pull their
arms, reducing the probability of collisions and effectively mitigating the
first challenge. Meanwhile, players continue pulling arms currently exploited
by others with a small probability, enabling them to detect when a player has
left, thereby addressing the second challenge. We prove that our algorithm
achieves a regret of $\mathcal{O}(\sqrt{T \log T} + {\log T}/{\Delta^2})$,
where $\Delta$ is the minimum expected reward gap between any two arms. To the
best of our knowledge, this is the first efficient MP-MAB algorithm in the
asynchronous and decentralized environment. Extensive experiments further
validate the effectiveness and robustness of our algorithm, demonstrating its
applicability to real-world scenarios.

</details>


### [67] [PALADIN: Self-Correcting Language Model Agents to Cure Tool-Failure Cases](https://arxiv.org/abs/2509.25238)
*Sri Vatsa Vuddanti,Aarav Shah,Satwik Kumar Chittiprolu,Tony Song,Sunishchal Dev,Kevin Zhu,Maheep Chaudhary*

Main category: cs.LG

TL;DR: PALADIN是一个增强语言代理故障恢复能力的框架，通过系统化故障注入和专家演示训练，在真实工具环境中实现鲁棒的故障恢复。


<details>
  <summary>Details</summary>
Motivation: 现有代理训练管道仅针对成功轨迹进行优化，无法处理真实部署中普遍存在的工具故障，导致级联推理错误和任务放弃。

Method: 通过系统化故障注入和专家演示构建50,000+恢复注释轨迹，使用LoRA微调保留基础能力，在推理时检测错误并检索相似故障案例执行恢复动作。

Result: 在PaladinEval和ToolReflectEval评估中，恢复率从32.76%提升至89.68%，比最强基线CRITIC提升13.3%，在未见工具API上保持95.2%的恢复性能。

Conclusion: PALADIN是构建容错代理的有效方法，能够在真实工具环境中实现鲁棒的故障恢复。

Abstract: Tool-augmented language agents frequently fail in real-world deployment due
to tool malfunctions--timeouts, API exceptions, or inconsistent
outputs--triggering cascading reasoning errors and task abandonment. Existing
agent training pipelines optimize only for success trajectories, failing to
expose models to the tool failures that dominate real-world usage. We propose
\textbf{PALADIN}, a generalizable framework for equipping language agents with
robust failure recovery capabilities. PALADIN trains on 50,000+
recovery-annotated trajectories constructed via systematic failure injection
and expert demonstrations on an enhanced ToolBench dataset. Training uses
LoRA-based fine-tuning to retain base capabilities while injecting recovery
competence. At inference, PALADIN detects execution-time errors and retrieves
the most similar case from a curated bank of 55+ failure exemplars aligned with
ToolScan's taxonomy, then executes the corresponding recovery action. This
approach generalizes to novel failures beyond the training distribution,
retaining 95.2\% recovery performance on unseen tool APIs. Evaluation across
PaladinEval and ToolReflectEval demonstrates consistent improvements in
Recovery Rate (RR), Task Success Rate (TSR), Catastrophic Success Rate (CSR),
and Efficiency Score (ES). PALADIN improves RR from 32.76% to 89.68% (+57%
relative) over ToolBench and outperforms the strongest baseline CRITIC (76.34%)
by +13.3%. Against vanilla agents, PALADIN achieves 89.86\% RR (+66% relative
improvement from 23.75%). These results establish PALADIN as an effective
method for building fault-tolerant agents capable of robust recovery in
real-world tool environments.

</details>


### [68] [Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access](https://arxiv.org/abs/2509.26000)
*Daniel Ebi,Gaspard Lambrechts,Damien Ernst,Klemens Böhm*

Main category: cs.LG

TL;DR: 提出了一种新的非对称actor-critic框架，允许在训练期间使用任意特权信号而非完整状态信息，扩展了非对称方法的理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有非对称actor-critic方法通常假设训练期间可以访问完整状态信息，但在实际部分可观测环境中这一假设往往不成立。

Method: 提出了知情非对称actor-critic框架，使critic能够基于任意特权信号进行条件化，无需访问完整状态。同时提出了基于核方法和回报预测误差的信息性度量方法。

Result: 在基准导航任务和合成部分可观测环境中的实验验证表明，当有信息丰富的特权输入可用时，该方法能提高学习效率和价值估计。

Conclusion: 挑战了完整状态访问的必要性，为非对称强化学习方法的设计开辟了新方向，既实用又理论可靠。

Abstract: Reinforcement learning in partially observable environments requires agents
to act under uncertainty from noisy, incomplete observations. Asymmetric
actor-critic methods leverage privileged information during training to improve
learning under these conditions. However, existing approaches typically assume
full-state access during training. In this work, we challenge this assumption
by proposing a novel actor-critic framework, called informed asymmetric
actor-critic, that enables conditioning the critic on arbitrary privileged
signals without requiring access to the full state. We show that policy
gradients remain unbiased under this formulation, extending the theoretical
foundation of asymmetric methods to the more general case of privileged partial
information. To quantify the impact of such signals, we propose informativeness
measures based on kernel methods and return prediction error, providing
practical tools for evaluating training-time signals. We validate our approach
empirically on benchmark navigation tasks and synthetic partially observable
environments, showing that our informed asymmetric method improves learning
efficiency and value estimation when informative privileged inputs are
available. Our findings challenge the necessity of full-state access and open
new directions for designing asymmetric reinforcement learning methods that are
both practical and theoretically sound.

</details>


### [69] [HAMMER: Hamiltonian Curiosity Augmented Large Language Model Reinforcement](https://arxiv.org/abs/2509.25240)
*Ming Yang,Xiaofan Li,Zhiyuan Ma,Dengliang Shi,Jintao Du,Yu Cheng,Weiguo Zheng*

Main category: cs.LG

TL;DR: 提出HAMMER方法，通过多样性指标和最小语义哈密顿路径排序训练样本，解决课程强化学习中局部优化问题，提升模型探索能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于难度的课程强化学习方法存在局部优化问题，早期持续训练简单样本会导致策略失去探索能力。

Method: 将数据集评估中的多样性指标引入动态强化学习过程，通过最小语义哈密顿路径对训练样本进行排序，使初始训练保持更多探索。

Result: HAMMER激发模型"好奇心"，在多样化推理基准上平均准确率提升3%到4%。

Conclusion: 多样性驱动的排序有助于稳定收敛，HAMMER方法能有效提升大语言模型在课程强化学习中的性能。

Abstract: Recent curriculum reinforcement learning for large language models (LLMs)
typically rely on difficulty-based annotations for data filtering and ordering.
However, such methods suffer from local optimization, where continual training
on simple samples in the early steps can cause the policy to lose its
exploration. We propose a novel schema, namely Hamiltonian curiosity augmented
large language model reinforcement (HAMMER), that transfers diversity metrics,
commonly used in dataset evaluation, into the dynamic reinforcement learning
procedure, where training samples are ordered via a minimum-semantic
Hamiltonian path making the initial training retrain more exploration. From a
theoretical perspective of generalization bounds, diversity-driven ordering
facilitates stable convergence. Empirical evaluations indicate that HAMMER
stimulates model "curiosity" and consistently achieves a 3% to 4% average
accuracy gain across diverse inference benchmark.

</details>


### [70] [Optimisation of Resource Allocation in Heterogeneous Wireless Networks Using Deep Reinforcement Learning](https://arxiv.org/abs/2509.25284)
*Oluwaseyi Giwa,Jonathan Shock,Jaco Du Toit,Tobi Awodumila*

Main category: cs.LG

TL;DR: 提出基于深度强化学习的异构无线网络资源分配框架，联合优化传输功率、带宽和调度，在多目标奖励函数中平衡吞吐量、能效和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在异构无线网络中难以应对变化的用户负载和信道条件，需要更智能的动态资源分配方案。

Method: 使用深度强化学习框架，比较PPO和TD3算法与三种启发式算法在多种网络场景下的性能。

Result: 深度强化学习框架在动态网络资源分配优化方面优于启发式算法。

Conclusion: 研究结果揭示了未来异构无线网络中深度强化学习设计的关键权衡问题。

Abstract: Dynamic resource allocation in heterogeneous wireless networks (HetNets) is
challenging for traditional methods under varying user loads and channel
conditions. We propose a deep reinforcement learning (DRL) framework that
jointly optimises transmit power, bandwidth, and scheduling via a
multi-objective reward balancing throughput, energy efficiency, and fairness.
Using real base station coordinates, we compare Proximal Policy Optimisation
(PPO) and Twin Delayed Deep Deterministic Policy Gradient (TD3) against three
heuristic algorithms in multiple network scenarios. Our results show that DRL
frameworks outperform heuristic algorithms in optimising resource allocation in
dynamic networks. These findings highlight key trade-offs in DRL design for
future HetNets.

</details>


### [71] [Sandbagging in a Simple Survival Bandit Problem](https://arxiv.org/abs/2509.26239)
*Joel Dyer,Daniel Jarne Ornia,Nicholas Bishop,Anisoara Calinescu,Michael Wooldridge*

Main category: cs.LG

TL;DR: 本文研究了前沿AI系统安全评估中的战略欺骗问题，开发了一个基于生存老虎机框架的模型来检测AI代理是否故意隐藏能力，并构建了统计测试来区分真实能力不足和策略性伪装。


<details>
  <summary>Details</summary>
Motivation: AI系统在安全评估中可能故意隐藏危险能力或表现不佳以避免被停用或重新训练，这种"沙袋"行为会破坏安全评估的完整性，因此需要开发方法来区分真实能力不足和策略性伪装。

Method: 基于生存老虎机框架开发了战略欺骗的序列决策模型，理论上证明了最优理性代理会产生沙袋行为，并构建了统计测试来从测试分数序列中区分沙袋行为和无能。

Result: 模拟实验验证了该统计测试在老虎机模型中可靠地区分沙袋行为和无能的能力。

Conclusion: 这项工作为前沿模型评估科学中开发稳健统计程序建立了一个潜在途径，有助于提高AI安全评估的可靠性。

Abstract: Evaluating the safety of frontier AI systems is an increasingly important
concern, helping to measure the capabilities of such models and identify risks
before deployment. However, it has been recognised that if AI agents are aware
that they are being evaluated, such agents may deliberately hide dangerous
capabilities or intentionally demonstrate suboptimal performance in
safety-related tasks in order to be released and to avoid being deactivated or
retrained. Such strategic deception - often known as "sandbagging" - threatens
to undermine the integrity of safety evaluations. For this reason, it is of
value to identify methods that enable us to distinguish behavioural patterns
that demonstrate a true lack of capability from behavioural patterns that are
consistent with sandbagging. In this paper, we develop a simple model of
strategic deception in sequential decision-making tasks, inspired by the
recently developed survival bandit framework. We demonstrate theoretically that
this problem induces sandbagging behaviour in optimal rational agents, and
construct a statistical test to distinguish between sandbagging and
incompetence from sequences of test scores. In simulation experiments, we
investigate the reliability of this test in allowing us to distinguish between
such behaviours in bandit models. This work aims to establish a potential
avenue for developing robust statistical procedures for use in the science of
frontier model evaluations.

</details>


### [72] [Fine-tuning of Large Language Models for Domain-Specific Cybersecurity Knowledge](https://arxiv.org/abs/2509.25241)
*Yuan Huang*

Main category: cs.LG

TL;DR: 该论文研究如何通过微调策略将网络安全知识嵌入大型语言模型，提升其在网络安全问答任务中的表现，同时注重计算效率。


<details>
  <summary>Details</summary>
Motivation: 基础LLMs在需要专业知识的专门领域（如网络安全）的零样本表现往往不理想，因为它们是针对通用应用设计的，难以在参数空间中封装领域特定知识。

Method: 研究监督微调(SFT)、低秩适应(LoRA)和量化低秩适应(QLoRA)三种微调方法，使用网络安全问答数据集进行实验。

Result: 这些微调方法在网络安全问答任务中显著优于基础模型，且LoRA和QLoRA在计算成本大幅降低的情况下达到与SFT相当的性能。

Conclusion: 低秩微调策略有潜力弥合通用LLMs与领域特定应用之间的差距，为适应专门领域提供高效途径。

Abstract: Recent advancements in training paradigms for Large Language Models (LLMs)
have unlocked their remarkable capabilities in natural language processing and
cross-domain generalization. While LLMs excel in tasks like programming and
mathematical problem-solving, their zero-shot performance in specialized
domains requiring expert knowledge, such as cybersecurity, is often suboptimal.
This limitation arises because foundational LLMs are designed for
general-purpose applications, constraining their ability to encapsulate
domain-specific expertise within their parameter space. To address this, we
explore fine-tuning strategies to embed cybersecurity knowledge into LLMs,
enhancing their performance in cybersecurity question-answering (Q\&A) tasks
while prioritizing computational efficiency. Specifically, we investigate
Supervised Fine-Tuning (SFT), Low-Rank Adaptation (LoRA), and Quantized
Low-Rank Adaptation (QLoRA) using a cybersecurity Q\&A dataset. Our results
demonstrate that these fine-tuning approaches significantly outperform the
foundational model in cybersecurity Q\&A tasks. Moreover, LoRA and QLoRA
achieve comparable performance to SFT with substantially lower computational
costs, offering an efficient pathway for adapting LLMs to specialized domains.
Our work highlights the potential of low-rank fine-tuning strategies to bridge
the gap between general-purpose LLMs and domain-specific applications.

</details>


### [73] [Can VLM Pseudo-Labels Train a Time-Series QA Model That Outperforms the VLM?](https://arxiv.org/abs/2509.25696)
*Takuya Fujimura,Kota Dohi,Natsuo Yamashita,Yohei Kawaguchi*

Main category: cs.LG

TL;DR: 提出了一种使用视觉语言模型生成伪标签来训练时间序列问答模型的方法，即使VLM可能产生错误标签，深度神经网络对噪声标签的鲁棒性仍能有效训练模型。


<details>
  <summary>Details</summary>
Motivation: 时间序列问答任务面临标注数据缺乏的挑战，而视觉语言模型在零样本分析时间序列信号方面显示出潜力。

Method: 利用VLM生成伪标签来训练TSQA模型，尽管VLM可能产生错误标签，但深度神经网络对噪声标签具有内在鲁棒性。

Result: 实验结果表明，TSQA模型不仅能用伪标签成功训练，还能通过利用大量未标注数据超越VLM本身的性能。

Conclusion: 该方法有效解决了TSQA任务中标注数据不足的问题，证明了使用VLM生成伪标签训练模型的可行性。

Abstract: Time-series question answering (TSQA) tasks face significant challenges due
to the lack of labeled data. Alternatively, with recent advancements in
large-scale models, vision-language models (VLMs) have demonstrated the
potential to analyze time-series signals in a zero-shot manner. In this paper,
we propose a training approach that uses pseudo labels generated by a VLM.
Although VLMs can produce incorrect labels, TSQA models can still be
effectively trained based on the property that deep neural networks are
inherently robust to such noisy labels. Our experimental results demonstrate
that TSQA models are not only successfully trained with pseudo labels, but also
surpass the performance of the VLM itself by leveraging a large amount of
unlabeled data.

</details>


### [74] [ACE: Adapting sampling for Counterfactual Explanations](https://arxiv.org/abs/2509.26322)
*Margarita A. Guerrero,Cristian R. Rojas*

Main category: cs.LG

TL;DR: 提出ACE算法，结合贝叶斯估计和随机优化，用更少的查询次数高效生成反事实解释


<details>
  <summary>Details</summary>
Motivation: 现有反事实解释方法需要大量评估黑盒模型，成本高且不实用，特别是在模型访问受限时

Method: ACE算法结合贝叶斯估计和随机优化，优先选择信息量大的点来近似决策边界

Result: ACE在评估效率上优于现有方法，同时能有效识别最小且可行的特征变化

Conclusion: ACE提供了一种样本高效的解决方案，能够在减少模型查询次数的同时生成准确的反事实解释

Abstract: Counterfactual Explanations (CFEs) interpret machine learning models by
identifying the smallest change to input features needed to change the model's
prediction to a desired output. For classification tasks, CFEs determine how
close a given sample is to the decision boundary of a trained classifier.
Existing methods are often sample-inefficient, requiring numerous evaluations
of a black-box model -- an approach that is both costly and impractical when
access to the model is limited. We propose Adaptive sampling for Counterfactual
Explanations (ACE), a sample-efficient algorithm combining Bayesian estimation
and stochastic optimization to approximate the decision boundary with fewer
queries. By prioritizing informative points, ACE minimizes evaluations while
generating accurate and feasible CFEs. Extensive empirical results show that
ACE achieves superior evaluation efficiency compared to state-of-the-art
methods, while maintaining effectiveness in identifying minimal and actionable
changes.

</details>


### [75] [Knowledge distillation through geometry-aware representational alignment](https://arxiv.org/abs/2509.25253)
*Prajjwal Bhattarai,Mohammad Amjad,Dmytro Zhylko,Tuka Alhanai*

Main category: cs.LG

TL;DR: 本文分析了传统特征蒸馏方法的局限性，提出了基于Procrustes距离和特征Gram矩阵Frobenius范数的新蒸馏方法，在语言模型蒸馏任务中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统特征蒸馏方法（如投影均方误差或CKA）无法有效捕捉教师模型的特征空间结构，即使损失为零时也存在问题，需要更有效的特征几何蒸馏方法。

Method: 使用Procrustes距离和特征Gram矩阵的Frobenius范数作为蒸馏损失函数，这些距离在表示对齐测量中已有应用。

Result: 在BERT和OPT语言模型家族的分类和指令跟随任务中，新方法比现有方法提升了高达2个百分点的蒸馏性能，改进具有统计显著性。

Conclusion: 将特征几何整合到现有蒸馏方法中具有巨大潜力，新提出的特征蒸馏方法能更有效地传递教师模型的特征空间结构。

Abstract: Knowledge distillation is a common paradigm for transferring capabilities
from larger models to smaller ones. While traditional distillation methods
leverage a probabilistic divergence over the output of the teacher and student
models, feature-based distillation methods often minimize variants of Euclidean
norms between the hidden layer representations. The main goal is for the
student to mimic the structure of the feature space of the teacher. In this
work, we theoretically show that existing feature distillation methods, such as
projection based mean squared loss or Centered Kernel Alignment (CKA), cannot
capture the feature structure, even under zero loss. We then motivate the use
of Procrustes distance and the Frobenius norm of Feature Gram Matrix, distances
already common in the context of measuring representational alignment, as
distillation losses. We show that feature distillation through our method
showcases statistically significant improvement in distillation performance
across language models families (BERT and OPT) in classification and
instruction-following tasks by up to 2 percentage points, showcasing the
potential of integrating feature geometry into existing distillation methods.

</details>


### [76] [Heterogeneous Multi-agent Collaboration in UAV-assisted Mobile Crowdsensing Networks](https://arxiv.org/abs/2509.25261)
*Xianyang Deng,Wenshuai Liu,Yaru FuB,Qi Zhu*

Main category: cs.LG

TL;DR: 提出基于无人机辅助移动群智感知的联合优化框架，通过多智能体深度强化学习算法最大化处理感知数据量


<details>
  <summary>Details</summary>
Motivation: 解决无人机辅助移动群智感知中的频谱稀缺、设备异构性和用户移动性等挑战，协调感知、通信和计算过程

Method: 采用时间槽划分、资源分配和无人机3D轨迹规划的联合优化，设计基于HAPPO的多智能体深度强化学习算法，结合CNN特征提取和KAN网络捕捉状态-动作依赖关系

Result: 相比其他基准方法，在处理的感知数据量方面取得了显著提升

Conclusion: 所提出的混合actor网络MADRL算法能有效解决无人机辅助移动群智感知的复杂优化问题

Abstract: Unmanned aerial vehicles (UAVs)-assisted mobile crowdsensing (MCS) has
emerged as a promising paradigm for data collection. However, challenges such
as spectrum scarcity, device heterogeneity, and user mobility hinder efficient
coordination of sensing, communication, and computation. To tackle these
issues, we propose a joint optimization framework that integrates time slot
partition for sensing, communication, and computation phases, resource
allocation, and UAV 3D trajectory planning, aiming to maximize the amount of
processed sensing data. The problem is formulated as a non-convex stochastic
optimization and further modeled as a partially observable Markov decision
process (POMDP) that can be solved by multi-agent deep reinforcement learning
(MADRL) algorithm. To overcome the limitations of conventional multi-layer
perceptron (MLP) networks, we design a novel MADRL algorithm with hybrid actor
network. The newly developed method is based on heterogeneous agent proximal
policy optimization (HAPPO), empowered by convolutional neural networks (CNN)
for feature extraction and Kolmogorov-Arnold networks (KAN) to capture
structured state-action dependencies. Extensive numerical results demonstrate
that our proposed method achieves significant improvements in the amount of
processed sensing data when compared with other benchmarks.

</details>


### [77] [Dynamic Policy Induction for Adaptive Prompt Optimization: Bridging the Efficiency-Accuracy Gap via Lightweight Reinforcement Learning](https://arxiv.org/abs/2509.25267)
*Jiexi Xu*

Main category: cs.LG

TL;DR: PPN是一个轻量级强化学习框架，通过自适应策略选择在保持准确性的同时显著降低计算成本，相比Self-Consistency可减少61.5%的token成本。


<details>
  <summary>Details</summary>
Motivation: 现有静态提示策略（如Zero-Shot、Few-Shot、CoT）存在效率-准确性的刚性权衡，准确策略如Self-Consistency在简单任务上浪费计算资源，而轻量方法在复杂输入上表现不佳。

Method: 提出Prompt Policy Network (PPN)，将自适应策略选择形式化为单步马尔可夫决策过程，使用PPO算法训练，并采用资源显式奖励函数来学习仅在必要时分配高成本推理策略。

Result: 在算术推理基准测试中，PPN在效率-准确性帕累托前沿上实现优越性能，相比Self-Consistency减少61.5%的token成本，同时保持竞争力准确性。

Conclusion: PPN为成本高效的LLM部署提供了系统化自适应框架，推动了轻量级优化技术的发展，支持可扩展和可持续的语言模型应用。

Abstract: The performance of Large Language Models (LLMs) depends heavily on the chosen
prompting strategy, yet static approaches such as Zero-Shot, Few-Shot, or
Chain-of-Thought (CoT) impose a rigid efficiency-accuracy trade-off. Highly
accurate strategies like Self-Consistency (SC) incur substantial computational
waste on simple tasks, while lightweight methods often fail on complex inputs.
This paper introduces the Prompt Policy Network (PPN), a lightweight
reinforcement learning framework that formalizes adaptive strategy selection as
a single-step Markov Decision Process (MDP). The PPN, trained with Proximal
Policy Optimization (PPO) and guided by a resource-explicit reward function,
learns to allocate costly reasoning strategies only when necessary. Experiments
on arithmetic reasoning benchmarks demonstrate that PPN achieves superior
performance on the efficiency-accuracy Pareto front, delivering up to 61.5%
token cost reduction compared to Self-Consistency while maintaining competitive
accuracy. This work contributes a systematic, adaptive framework for
cost-efficient LLM deployment, advancing the design of lightweight optimization
techniques for scalable and sustainable language model applications.

</details>


### [78] [A Weather Foundation Model for the Power Grid](https://arxiv.org/abs/2509.25268)
*Cristian Bodnar,Raphaël Rousseau-Rizzi,Nikhil Shankar,James Merleau,Stylianos Flampouris,Guillem Candille,Slavica Antic,François Miralles,Jayesh K. Gupta*

Main category: cs.LG

TL;DR: 该研究通过微调天气基础模型，为电力基础设施提供超本地化的资产级天气预报，在多个关键气象变量上超越了传统数值天气预报基准，特别是在覆冰检测方面取得了突破性进展。


<details>
  <summary>Details</summary>
Motivation: 天气基础模型虽然在天气预报准确性上取得了新突破，但其对现代社会天气敏感基础设施的实际价值尚未充分探索。本研究旨在验证天气基础模型在电力系统等关键基础设施中的实际应用价值。

Method: 使用Hydro-Québec资产观测数据（包括输电线路气象站、风电场测风塔和覆冰传感器）对Silurian AI的15亿参数生成预测变换器进行微调，为五个电网关键变量提供超本地化预测。

Result: 在6-72小时预报时效内，定制化模型超越了最先进的数值天气预报基准：温度平均绝对误差降低15%，总降水量误差降低35%，风速误差降低15%。最重要的是，在覆冰检测方面达到了0.72的平均精度得分，为潜在的灾难性停电事件提供了数小时的可操作预警。

Conclusion: 天气基础模型在通过少量高保真数据进行后训练后，可以作为下一代电网韧性智能的实用基础，显著提升关键基础设施的天气风险应对能力。

Abstract: Weather foundation models (WFMs) have recently set new benchmarks in global
forecast skill, yet their concrete value for the weather-sensitive
infrastructure that powers modern society remains largely unexplored. In this
study, we fine-tune Silurian AI's 1.5B-parameter WFM, Generative Forecasting
Transformer (GFT), on a rich archive of Hydro-Qu\'ebec asset
observations--including transmission-line weather stations, wind-farm met-mast
streams, and icing sensors--to deliver hyper-local, asset-level forecasts for
five grid-critical variables: surface temperature, precipitation, hub-height
wind speed, wind-turbine icing risk, and rime-ice accretion on overhead
conductors. Across 6-72 h lead times, the tailored model surpasses
state-of-the-art NWP benchmarks, trimming temperature mean absolute error (MAE)
by 15%, total-precipitation MAE by 35%, and lowering wind speed MAE by 15%.
Most importantly, it attains an average precision score of 0.72 for day-ahead
rime-ice detection, a capability absent from existing operational systems,
which affords several hours of actionable warning for potentially catastrophic
outage events. These results show that WFMs, when post-trained with small
amounts of high-fidelity, can serve as a practical foundation for
next-generation grid-resilience intelligence.

</details>


### [79] [InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions](https://arxiv.org/abs/2509.25270)
*Liangjian Wen,Qun Dai,Jianzhuang Liu,Jiangtao Zheng,Yong Dai,Dongkai Wang,Zhao Kang,Jun Wang,Zenglin Xu,Jiang Duan*

Main category: cs.LG

TL;DR: 提出InfMasking方法，通过无限掩码策略增强多模态表示学习中的协同信息，在七个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉多模态间的协同信息，而协同信息是多模态表示的核心价值所在

Method: 使用无限掩码策略随机遮挡每个模态的大部分特征，仅保留部分信息创建具有不同协同模式的表示，通过互信息最大化对齐掩码和未掩码的融合表示

Result: 在控制实验中证明能有效增强模态间协同信息，在七个大规模真实世界数据集基准测试中达到最先进性能

Conclusion: InfMasking通过无限掩码策略成功解决了多模态表示学习中协同信息提取的挑战

Abstract: In multimodal representation learning, synergistic interactions between
modalities not only provide complementary information but also create unique
outcomes through specific interaction patterns that no single modality could
achieve alone. Existing methods may struggle to effectively capture the full
spectrum of synergistic information, leading to suboptimal performance in tasks
where such interactions are critical. This is particularly problematic because
synergistic information constitutes the fundamental value proposition of
multimodal representation. To address this challenge, we introduce InfMasking,
a contrastive synergistic information extraction method designed to enhance
synergistic information through an \textbf{Inf}inite \textbf{Masking} strategy.
InfMasking stochastically occludes most features from each modality during
fusion, preserving only partial information to create representations with
varied synergistic patterns. Unmasked fused representations are then aligned
with masked ones through mutual information maximization to encode
comprehensive synergistic information. This infinite masking strategy enables
capturing richer interactions by exposing the model to diverse partial modality
combinations during training. As computing mutual information estimates with
infinite masking is computationally prohibitive, we derive an InfMasking loss
to approximate this calculation. Through controlled experiments, we demonstrate
that InfMasking effectively enhances synergistic information between
modalities. In evaluations on large-scale real-world datasets, InfMasking
achieves state-of-the-art performance across seven benchmarks. Code is released
at https://github.com/brightest66/InfMasking.

</details>


### [80] [ClustRecNet: A Novel End-to-End Deep Learning Framework for Clustering Algorithm Recommendation](https://arxiv.org/abs/2509.25289)
*Mohammadreza Bakhtyari,Bogdan Mazoure,Renato Cordeiro de Amorim,Guillaume Rabusseau,Vladimir Makarenkov*

Main category: cs.LG

TL;DR: ClustRecNet是一个基于深度学习的推荐框架，用于为给定数据集自动选择最合适的聚类算法，解决了无监督学习中聚类算法选择的长期挑战。


<details>
  <summary>Details</summary>
Motivation: 解决聚类算法选择这一无监督学习中的长期难题，减少对手工元特征和传统聚类有效性指标的依赖。

Method: 构建包含34,000个合成数据集的综合数据仓库，使用10种流行聚类算法处理，通过调整兰德指数(ARI)建立真实标签。网络架构整合卷积、残差和注意力机制，捕获输入数据的局部和全局结构模式，支持端到端训练。

Result: 在合成和真实世界基准测试中，该深度学习模型始终优于传统聚类有效性指标（如轮廓系数、Calinski-Harabasz、Davies-Bouldin和Dunn指数）以及最先进的AutoML聚类推荐方法（如ML2DAC、AutoCluster和AutoML4Clust）。在合成数据上比Calinski-Harabasz指数提高0.497 ARI，在真实数据上比最佳AutoML方法提高15.3% ARI。

Conclusion: ClustRecNet通过深度学习有效解决了聚类算法选择问题，显著优于传统方法和现有AutoML方法，为无监督学习中的算法推荐提供了新的解决方案。

Abstract: We introduce ClustRecNet - a novel deep learning (DL)-based recommendation
framework for determining the most suitable clustering algorithms for a given
dataset, addressing the long-standing challenge of clustering algorithm
selection in unsupervised learning. To enable supervised learning in this
context, we construct a comprehensive data repository comprising 34,000
synthetic datasets with diverse structural properties. Each of them was
processed using 10 popular clustering algorithms. The resulting clusterings
were assessed via the Adjusted Rand Index (ARI) to establish ground truth
labels, used for training and evaluation of our DL model. The proposed network
architecture integrates convolutional, residual, and attention mechanisms to
capture both local and global structural patterns from the input data. This
design supports end-to-end training to learn compact representations of
datasets and enables direct recommendation of the most suitable clustering
algorithm, reducing reliance on handcrafted meta-features and traditional
Cluster Validity Indices (CVIs). Comprehensive experiments across synthetic and
real-world benchmarks demonstrate that our DL model consistently outperforms
conventional CVIs (e.g. Silhouette, Calinski-Harabasz, Davies-Bouldin, and
Dunn) as well as state-of-the-art AutoML clustering recommendation approaches
(e.g. ML2DAC, AutoCluster, and AutoML4Clust). Notably, the proposed model
achieves a 0.497 ARI improvement over the Calinski-Harabasz index on synthetic
data and a 15.3% ARI gain over the best-performing AutoML approach on
real-world data.

</details>


### [81] [Scaling Behaviors of LLM Reinforcement Learning Post-Training: An Empirical Study in Mathematical Reasoning](https://arxiv.org/abs/2509.25300)
*Zelin Tan,Hejia Geng,Mulei Zhang,Xiaohang Yu,Guancheng Wan,Yifan Zhou,Qiang He,Xiangyuan Xue,Heng Zhou,Yutao Fan,Zhongzhi Li,Zaibin Zhang,Guibin Zhang,Chen Zhang,Zhenfei Yin,Lei Bai*

Main category: cs.LG

TL;DR: 本文系统研究了LLM在强化学习后训练中的缩放规律，发现大模型在固定计算预算下表现更好，具有更高的样本效率，数据重复使用有效，且这些规律在不同模型类型中均适用。


<details>
  <summary>Details</summary>
Motivation: 虽然预训练阶段的大语言模型缩放规律已被广泛研究，但强化学习后训练阶段的缩放行为仍不明确，特别是在数学推理任务上。

Method: 基于54个实验，系统分析模型规模、数据量和计算预算在RL后训练中的交互作用，重点关注数学推理任务。

Result: 发现四个关键规律：1) 固定计算预算下大模型表现更好；2) 大模型样本效率更高；3) 数据受限时重复使用高质量数据有效；4) 这些规律在不同模型类型中均稳健。

Conclusion: 为通过RL后训练高效扩展LLM推理能力提供了理论基础和实践指导。

Abstract: While scaling laws for large language models (LLMs) during pre-training have
been extensively studied, their behavior under reinforcement learning (RL)
post-training remains largely unexplored. This paper presents a systematic
empirical investigation of scaling behaviors in RL-based post-training, with a
particular focus on mathematical reasoning. Based on 54 experiments across
diverse model sizes and training settings, we characterize how model scale,
data volume, and computational budget interact to shape performance. Our
analysis leads to four key findings: (1). Under a fixed computational budget,
larger models trained for fewer steps consistently outperform smaller models
trained for more steps. (2). Given a fixed amount of training data, larger
models achieve superior sample efficiency, yielding lower loss. (3). In
data-constrained regimes, repeated reuse of high-quality data proves highly
effective, as final performance is primarily governed by the total number of
optimization steps rather than the uniqueness of samples. (4). These scaling
behaviors are robust across both base and instruction-tuned models, which share
similar learning dynamics (e.g., larger models show faster convergence) even
while differing in absolute accuracy. Collectively, these results provide a
principled foundation and practical guidelines for efficiently scaling the
reasoning capabilities of LLMs through RL post-training.

</details>


### [82] [Uncertainty-Aware Generative Oversampling Using an Entropy-Guided Conditional Variational Autoencoder](https://arxiv.org/abs/2509.25334)
*Amirhossein Zare,Amirhessam Zare,Parmida Sadat Pezeshki,Herlock,Rahimi,Ali Ebrahimi,Ignacio Vázquez-García,Leo Anthony Celi*

Main category: cs.LG

TL;DR: 提出LEO-CVAE方法，通过局部熵引导的生成式过采样解决类别不平衡问题，特别关注边界区域的不确定性样本。


<details>
  <summary>Details</summary>
Motivation: 传统过采样方法如SMOTE依赖局部线性插值，常产生不合理的合成样本；而标准CVAE对所有少数类样本一视同仁，忽略了边界区域不确定样本的重要性。

Method: 使用局部熵量化不确定性，通过局部熵加权损失和熵引导采样策略，在不确定区域进行强调学习和集中生成。

Result: 在临床基因组数据集上，LEO-CVAE持续提升分类器性能，优于传统过采样和生成式基线方法。

Conclusion: 对于受复杂非线性结构支配的领域，如组学数据，不确定性感知的生成式过采样具有重要价值。

Abstract: Class imbalance remains a major challenge in machine learning, especially for
high-dimensional biomedical data where nonlinear manifold structures dominate.
Traditional oversampling methods such as SMOTE rely on local linear
interpolation, often producing implausible synthetic samples. Deep generative
models like Conditional Variational Autoencoders (CVAEs) better capture
nonlinear distributions, but standard variants treat all minority samples
equally, neglecting the importance of uncertain, boundary-region examples
emphasized by heuristic methods like Borderline-SMOTE and ADASYN.
  We propose Local Entropy-Guided Oversampling with a CVAE (LEO-CVAE), a
generative oversampling framework that explicitly incorporates local
uncertainty into both representation learning and data generation. To quantify
uncertainty, we compute Shannon entropy over the class distribution in a
sample's neighborhood: high entropy indicates greater class overlap, serving as
a proxy for uncertainty. LEO-CVAE leverages this signal through two mechanisms:
(i) a Local Entropy-Weighted Loss (LEWL) that emphasizes robust learning in
uncertain regions, and (ii) an entropy-guided sampling strategy that
concentrates generation in these informative, class-overlapping areas.
  Applied to clinical genomics datasets (ADNI and TCGA lung cancer), LEO-CVAE
consistently improves classifier performance, outperforming both traditional
oversampling and generative baselines. These results highlight the value of
uncertainty-aware generative oversampling for imbalanced learning in domains
governed by complex nonlinear structures, such as omics data.

</details>


### [83] [Cold-Start Active Correlation Clustering](https://arxiv.org/abs/2509.25376)
*Linus Aronsson,Han Wu,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: 该论文研究在冷启动场景下的主动相关聚类，通过主动学习以成本效益方式查询成对相似性，提出了一种覆盖感知方法来鼓励早期多样性。


<details>
  <summary>Details</summary>
Motivation: 解决在冷启动场景下，没有初始成对相似性数据时如何进行有效的主动相关聚类的问题。

Method: 提出了一种覆盖感知方法，在主动学习过程中早期鼓励多样性，以应对缺乏真实初始相似性数据的挑战。

Result: 通过多个合成和真实世界实验验证了所提出方法的有效性。

Conclusion: 覆盖感知方法在冷启动主动相关聚类中表现出良好的性能，能够有效处理缺乏初始相似性数据的情况。

Abstract: We study active correlation clustering where pairwise similarities are not
provided upfront and must be queried in a cost-efficient manner through active
learning. Specifically, we focus on the cold-start scenario, where no true
initial pairwise similarities are available for active learning. To address
this challenge, we propose a coverage-aware method that encourages diversity
early in the process. We demonstrate the effectiveness of our approach through
several synthetic and real-world experiments.

</details>


### [84] [Let Physics Guide Your Protein Flows: Topology-aware Unfolding and Generation](https://arxiv.org/abs/2509.25379)
*Yogesh Verma,Markus Heinonen,Vikas Garg*

Main category: cs.LG

TL;DR: 提出了一种基于物理原理的非线性噪声过程，将蛋白质展开为二级结构，并与SE(3)流匹配结合，实现高保真蛋白质骨架建模和序列条件折叠。


<details>
  <summary>Details</summary>
Motivation: 现有扩散生成模型在蛋白质设计中忽略了蛋白质的物理现实性，噪声动态缺乏物理原理基础。

Method: 引入基于经典物理的非线性噪声过程，将蛋白质展开为二级结构并保持拓扑完整性；与SE(3)流匹配范式结合，建模蛋白质骨架的不变分布。

Result: 在无条件蛋白质生成中达到最先进性能，产生更具设计性和新颖性的蛋白质结构，并能准确折叠单体序列为精确构象。

Conclusion: 该方法通过物理驱动的噪声过程和流匹配，显著提升了蛋白质生成和折叠的准确性和设计性。

Abstract: Protein structure prediction and folding are fundamental to understanding
biology, with recent deep learning advances reshaping the field.
Diffusion-based generative models have revolutionized protein design, enabling
the creation of novel proteins. However, these methods often neglect the
intrinsic physical realism of proteins, driven by noising dynamics that lack
grounding in physical principles. To address this, we first introduce a
physically motivated non-linear noising process, grounded in classical physics,
that unfolds proteins into secondary structures (e.g., alpha helices, linear
beta sheets) while preserving topological integrity--maintaining bonds, and
preventing collisions. We then integrate this process with the flow-matching
paradigm on SE(3) to model the invariant distribution of protein backbones with
high fidelity, incorporating sequence information to enable
sequence-conditioned folding and expand the generative capabilities of our
model. Experimental results demonstrate that the proposed method achieves
state-of-the-art performance in unconditional protein generation, producing
more designable and novel protein structures while accurately folding monomer
sequences into precise protein conformations.

</details>


### [85] [Predicting Training Re-evaluation Curves Enables Effective Data Curriculums for LLMs](https://arxiv.org/abs/2509.25380)
*Shane Bergsma,Nolan Dey,Joel Hestness*

Main category: cs.LG

TL;DR: 该论文提出了训练重评估曲线（TREC），用于分析模型在不同训练阶段对数据的保留程度，并发现将高质量数据放置在TREC的低点能显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM训练中数据课程安排缺乏明确原则，需要一种方法来诊断和优化数据在训练过程中的放置时机。

Method: 引入TREC诊断方法，通过最终模型权重回顾性评估训练批次，并利用AdamW的隐式EMA系数预测TREC，实现前瞻性课程设计。

Result: 在111M到3.9B参数的模型上验证了TREC的有效性，通过将高质量数据对齐TREC最小值，改善了3.9B参数LLM在900B tokens上的持续预训练性能。

Conclusion: TREC为数据课程设计提供了理论基础和实用工具，能够解释先前实验并揭示次优数据放置，显著提升模型训练效率。

Abstract: Data curriculums have become central to successful LLM training, yet
principles governing optimal data placement remain unclear. We introduce the
*training re-evaluation curve (TREC)*, a diagnostic that retrospectively
evaluates training batches *using the final model weights*. The TREC
characterizes how well a trained model retains training data as a function of
*when* the data was encountered during training. Analyzing TRECs for models
from 111M to 3.9B parameters, we show that placing high-quality data at low
points on the TREC significantly improves performance. Importantly, while a
TREC is initially observable only after training, we demonstrate it can be
*predicted in advance* from AdamW's implicit EMA coefficients, enabling
proactive curriculum design. By predicting TRECs for published training
recipes, we explain prior ablations and reveal suboptimal data placements. We
also align high-quality data with TREC minima in order to improve continual
pre-training of a 3.9B-parameter LLM trained on 900B tokens.

</details>


### [86] [Deep Survival Analysis for Competing Risk Modeling with Functional Covariates and Missing Data Imputation](https://arxiv.org/abs/2509.25381)
*Penglei Gao,Yan Zou,Abhijit Duggal,Shuaiqi Huang,Faming Liang,Xiaofeng Wang*

Main category: cs.LG

TL;DR: FCRN是一个用于竞争风险下离散时间生存分析的深度学习框架，能整合功能协变量并处理缺失数据。


<details>
  <summary>Details</summary>
Motivation: 在重症监护中，需要更有效地捕捉动态风险因素和静态预测因子，同时处理不规则和不完整的数据。

Method: 结合微网络基础层进行功能数据表示和基于梯度的插补模块，同时学习插补缺失值和预测事件特定风险。

Result: 在多个模拟数据集和真实ICU案例研究中，FCRN相比随机生存森林和传统竞争风险模型在预测准确性上有显著提升。

Conclusion: FCRN通过有效捕捉动态风险因素和静态预测因子，同时适应不规则和不完整数据，推进了重症监护中的预后建模。

Abstract: We introduce the Functional Competing Risk Net (FCRN), a unified
deep-learning framework for discrete-time survival analysis under competing
risks, which seamlessly integrates functional covariates and handles missing
data within an end-to-end model. By combining a micro-network Basis Layer for
functional data representation with a gradient-based imputation module, FCRN
simultaneously learns to impute missing values and predict event-specific
hazards. Evaluated on multiple simulated datasets and a real-world ICU case
study using the MIMIC-IV and Cleveland Clinic datasets, FCRN demonstrates
substantial improvements in prediction accuracy over random survival forests
and traditional competing risks models. This approach advances prognostic
modeling in critical care by more effectively capturing dynamic risk factors
and static predictors while accommodating irregular and incomplete data.

</details>


### [87] [On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study](https://arxiv.org/abs/2509.25382)
*Fernanda Zapata Bascuñán*

Main category: cs.LG

TL;DR: 该论文研究了在GW150914引力波数据上训练的变分自编码器-高斯混合模型(VAE-MoG)的潜在空间，发现尽管模型能准确重构信号，但潜在表示存在统计不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 探索生成模型的潜在空间质量，验证强去噪性能是否意味着可靠的潜在表示，强调基于后验验证的重要性。

Method: 使用哈密顿蒙特卡洛(HMC)从干净输入中抽取后验样本，并与编码器从噪声数据输出的结果进行统计比较。

Result: 模型能准确重构信号，但统计比较显示潜在空间存在明显不匹配，表明去噪性能强不代表潜在表示可靠。

Conclusion: 在评估生成模型时，必须使用基于后验的验证方法，仅凭重构准确性不足以评估潜在空间的质量。

Abstract: In this work, we explore the latent space of a denoising variational
autoencoder with a mixture-of-Gaussians prior (VAE-MoG), trained on
gravitational wave data from event GW150914. To evaluate how well the model
captures the underlying structure, we use Hamiltonian Monte Carlo (HMC) to draw
posterior samples conditioned on clean inputs, and compare them to the
encoder's outputs from noisy data. Although the model reconstructs signals
accurately, statistical comparisons reveal a clear mismatch in the latent
space. This shows that strong denoising performance doesn't necessarily mean
the latent representations are reliable highlighting the importance of using
posterior-based validation when evaluating generative models.

</details>


### [88] [Crowdsourcing Without People: Modelling Clustering Algorithms as Experts](https://arxiv.org/abs/2509.25395)
*Jordyn E. A. Lorentz,Katharine M. Clark*

Main category: cs.LG

TL;DR: mixsemble是一种集成方法，将Dawid-Skene模型应用于多个基于模型的聚类算法预测聚合，将聚类输出视为噪声标注来处理。


<details>
  <summary>Details</summary>
Motivation: 在真实数据结构未知的情况下，需要一种能够稳健地整合不同聚类算法结果的方法，避免依赖单一算法可能导致的较差结果。

Method: 将Dawid-Skene模型从传统众包标注扩展到聚类算法集成，将聚类算法的输出建模为噪声标注，通过集成方法聚合多个聚类结果。

Result: 在模拟和真实数据集上的实验表明，mixsemble虽然不是总是表现最佳，但能稳定接近最佳结果并避免较差结果，展现出良好的稳健性。

Conclusion: mixsemble为数据结构未知的情况提供了实用的替代方案，特别适合非专业用户使用，能够可靠地整合不同聚类算法的结果。

Abstract: This paper introduces mixsemble, an ensemble method that adapts the
Dawid-Skene model to aggregate predictions from multiple model-based clustering
algorithms. Unlike traditional crowdsourcing, which relies on human labels, the
framework models the outputs of clustering algorithms as noisy annotations.
Experiments on both simulated and real-world datasets show that, although the
mixsemble is not always the single top performer, it consistently approaches
the best result and avoids poor outcomes. This robustness makes it a practical
alternative when the true data structure is unknown, especially for non-expert
users.

</details>


### [89] [Multi-Task Equation Discovery](https://arxiv.org/abs/2509.25400)
*S C Bee,N Dervilis,K Worden,L A Bull*

Main category: cs.LG

TL;DR: 该论文提出了一种基于多任务学习的贝叶斯相关向量机方法，用于提高方程发现在不同操作条件下的泛化能力，避免过拟合到特定数据集。


<details>
  <summary>Details</summary>
Motivation: 方程发现面临的主要挑战是确保识别模型能够跨操作条件泛化，而不是过拟合到特定数据集。特别是在结构健康监测中，不同负载条件可能无法充分激发系统的非线性动态特性。

Method: 采用多任务学习框架下的贝叶斯相关向量机，将同一结构在不同激励水平下的响应视为相关任务，共享模型参数但保留任务特定的噪声特性。

Result: 与标准单任务RVM相比，MTL-RVM能够结合跨任务信息，在弱激励和中度激励条件下显著改善参数恢复性能，同时在高激励条件下保持强性能。

Conclusion: 多任务贝叶斯推断能够减轻方程发现中的过拟合问题并促进泛化能力，特别适用于结构健康监测中负载条件变化揭示系统物理互补方面的情况。

Abstract: Equation discovery provides a grey-box approach to system identification by
uncovering governing dynamics directly from observed data. However, a
persistent challenge lies in ensuring that identified models generalise across
operating conditions rather than over-fitting to specific datasets. This work
investigates this issue by applying a Bayesian relevance vector machine (RVM)
within a multi-task learning (MTL) framework for simultaneous parameter
identification across multiple datasets. In this formulation, responses from
the same structure under different excitation levels are treated as related
tasks that share model parameters but retain task-specific noise
characteristics. A simulated single degree-of-freedom oscillator with linear
and cubic stiffness provided the case study, with datasets generated under
three excitation regimes. Standard single-task RVM models were able to
reproduce system responses but often failed to recover the true governing terms
when excitations insufficiently stimulated non-linear dynamics. By contrast,
the MTL-RVM combined information across tasks, improving parameter recovery for
weakly and moderately excited datasets, while maintaining strong performance
under high excitation. These findings demonstrate that multi-task Bayesian
inference can mitigate over-fitting and promote generalisation in equation
discovery. The approach is particularly relevant to structural health
monitoring, where varying load conditions reveal complementary aspects of
system physics.

</details>


### [90] [FlashOmni: A Unified Sparse Attention Engine for Diffusion Transformers](https://arxiv.org/abs/2509.25401)
*Liang Qiao,Yue Dai,Yeqi Huang,Hongyu Kan,Jun Shi,Hong An*

Main category: cs.LG

TL;DR: FlashOmni是一个统一的稀疏注意力引擎，通过引入灵活的稀疏符号来标准化各种稀疏策略的表示，能够在单个注意力内核中执行多样化的稀疏计算，显著提升多模态扩散变换器的推理效率。


<details>
  <summary>Details</summary>
Motivation: 多模态扩散变换器(DiTs)在视觉合成方面表现出色，但计算需求巨大。现有的稀疏加速方法需要定制化内核，限制了通用性。

Method: 提出FlashOmni统一稀疏注意力引擎，引入稀疏符号标准化稀疏策略表示，设计优化的稀疏GEMMs消除冗余计算。

Result: FlashOmni在注意力和GEMM-Q中实现接近线性的加速比(1:1)，在GEMM-O中实现2.5-3.8倍加速，达到理论极限的87.5%。应用多粒度稀疏策略使Hunyuan模型(33K)实现约1.5倍端到端加速且不降低视觉质量。

Conclusion: FlashOmni提供了一个通用的稀疏计算解决方案，能够有效加速多模态扩散变换器的推理过程，同时保持视觉质量。

Abstract: Multi-Modal Diffusion Transformers (DiTs) demonstrate exceptional
capabilities in visual synthesis, yet their deployment remains constrained by
substantial computational demands. To alleviate this bottleneck, many
sparsity-based acceleration methods have been proposed. However, their diverse
sparsity patterns often require customized kernels for high-performance
inference, limiting universality. We propose FlashOmni, a unified sparse
attention engine compatible with arbitrary DiT architectures. FlashOmni
introduces flexible sparse symbols to standardize the representation of a wide
range of sparsity strategies, such as feature caching and block-sparse
skipping. This unified abstraction enables the execution of diverse sparse
computations within a single attention kernel. In addition, FlashOmni designs
optimized sparse GEMMs for attention blocks, leveraging sparse symbols to
eliminate redundant computations and further improve efficiency. Experiments
demonstrate that FlashOmni delivers near-linear, closely matching the sparsity
ratio speedup (1:1) in attention and GEMM-$Q$, and achieves
2.5$\times$-3.8$\times$ acceleration in GEMM-$O$ (max peaking at about 87.5% of
the theoretical limit). Applied with a multi-granularity sparsity strategy, it
enables the Hunyuan model (33K) to achieve about 1.5$\times$ end-to-end
acceleration without degrading visual quality.

</details>


### [91] [Rethinking Parameter Sharing for LLM Fine-Tuning with Multiple LoRAs](https://arxiv.org/abs/2509.25414)
*Hao Ban,Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文提出ALoRA和Fed-ALoRA方法，通过不对称的多LoRA设计，在微调中共享B矩阵，实现更平衡的多任务性能和联邦学习效果。


<details>
  <summary>Details</summary>
Motivation: 重新审视多LoRA适配器中A矩阵高度相似的现象，发现这主要源于相同的初始化而非共享知识，B矩阵在知识编码和迁移中起更关键作用。

Method: 提出ALoRA（多任务微调中多个A矩阵共享单个B矩阵）和Fed-ALoRA（联邦学习中跨客户端共享B矩阵），采用新颖的矩阵分解策略处理异构rank。

Result: 在常识推理、数学推理、多任务NLP数据集和联邦NLP数据集上的实验表明，相比现有多LoRA方法，本方法在保持相当或更优平均准确率的同时，实现了更平衡的任务性能。

Conclusion: ALoRA和Fed-ALoRA通过不对称的LoRA设计有效提升了多任务和联邦学习的性能平衡性，验证了B矩阵共享策略的有效性。

Abstract: Large language models are often adapted using parameter-efficient techniques
such as Low-Rank Adaptation (LoRA), formulated as $y = W_0x + BAx$, where $W_0$
is the pre-trained parameters and $x$ is the input to the adapted layer. While
multi-adapter extensions often employ multiple LoRAs, prior studies suggest
that the inner $A$ matrices are highly similar during training and thus
suitable for sharing. We revisit this phenomenon and find that this similarity
is largely attributable to the identical initialization rather than shared
knowledge, with $B$ playing a more critical role in knowledge encoding and
transfer. Motivated by these insights, we propose \textbf{ALoRA}, an asymmetric
multi-LoRA design with multiple $A$ matrices and a single shared $B$ in
multi-task fine-tuning, and \textbf{Fed-ALoRA}, which shares $B$ across clients
in federated fine-tuning under both homogeneous and heterogeneous settings,
through a novel matrix decomposition strategy to accommodate heterogeneous
ranks across clients. Experiments on commonsense reasoning, math reasoning,
multi-task NLP dataset, and federated NLP dataset demonstrate that our methods
achieve more balanced performance across tasks with comparable or superior
average accuracy relative to existing multi-LoRA approaches. Codes are
available at https://github.com/OptMN-Lab/ALoRA.

</details>


### [92] [Leveraging Vulnerabilities in Temporal Graph Neural Networks via Strategic High-Impact Assaults](https://arxiv.org/abs/2509.25418)
*Dong Hyun Jeon,Lijing Zhu,Haifang Li,Pengze Li,Jingna Feng,Tiehang Duan,Houbing Herbert Song,Cui Tao,Shuteng Niu*

Main category: cs.LG

TL;DR: 提出了HIA攻击框架，针对时序图神经网络(TGNNs)的漏洞，通过识别结构重要性和动态重要性节点，结合边注入和删除策略，在最小扰动下显著降低模型性能


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法对时空动态图(STDGs)的扰动过于简单易检测，且未能战略性地针对最具影响力的节点和边，需要更有效的攻击方法来暴露TGNNs的关键漏洞

Method: 使用数据驱动的代理模型识别结构重要节点(网络连通性核心)和动态重要节点(图时序演化关键)，采用混合扰动策略结合战略边注入和定向边删除

Result: 在5个真实数据集和4种TGNN架构上的实验表明，HIA显著降低链接预测任务的准确率，MRR最多下降35.55%，优于现有基线方法

Conclusion: 结果揭示了当前STDG模型的基本漏洞，强调了开发同时考虑结构和时序动态的鲁棒防御的紧迫性

Abstract: Temporal Graph Neural Networks (TGNNs) have become indispensable for
analyzing dynamic graphs in critical applications such as social networks,
communication systems, and financial networks. However, the robustness of TGNNs
against adversarial attacks, particularly sophisticated attacks that exploit
the temporal dimension, remains a significant challenge. Existing attack
methods for Spatio-Temporal Dynamic Graphs (STDGs) often rely on simplistic,
easily detectable perturbations (e.g., random edge additions/deletions) and
fail to strategically target the most influential nodes and edges for maximum
impact. We introduce the High Impact Attack (HIA), a novel restricted black-box
attack framework specifically designed to overcome these limitations and expose
critical vulnerabilities in TGNNs. HIA leverages a data-driven surrogate model
to identify structurally important nodes (central to network connectivity) and
dynamically important nodes (critical for the graph's temporal evolution). It
then employs a hybrid perturbation strategy, combining strategic edge injection
(to create misleading connections) and targeted edge deletion (to disrupt
essential pathways), maximizing TGNN performance degradation. Importantly, HIA
minimizes the number of perturbations to enhance stealth, making it more
challenging to detect. Comprehensive experiments on five real-world datasets
and four representative TGNN architectures (TGN, JODIE, DySAT, and TGAT)
demonstrate that HIA significantly reduces TGNN accuracy on the link prediction
task, achieving up to a 35.55% decrease in Mean Reciprocal Rank (MRR) - a
substantial improvement over state-of-the-art baselines. These results
highlight fundamental vulnerabilities in current STDG models and underscore the
urgent need for robust defenses that account for both structural and temporal
dynamics.

</details>


### [93] [Polychromic Objectives for Reinforcement Learning](https://arxiv.org/abs/2509.25424)
*Jubayer Ibn Hamid,Ifdita Hasan Orney,Ellen Xu,Chelsea Finn,Dorsa Sadigh*

Main category: cs.LG

TL;DR: 提出了一种名为polychromic objective的新目标函数，用于解决强化学习微调中策略多样性丧失的问题，通过改进PPO算法来保持和利用多样化的生成策略。


<details>
  <summary>Details</summary>
Motivation: 传统RLFT方法在微调预训练策略时容易导致策略多样性丧失，收敛到少数易被利用的输出，这阻碍了探索并限制了测试时计算扩展的效益。

Method: 采用vine采样收集on-policy rollout，并修改优势函数以反映新目标下的优势，将PPO算法适配到polychromic目标优化。

Result: 在BabyAI、Minigrid和Algorithmic Creativity上的实验表明，该方法提高了成功率，能解决更多环境配置，在大扰动下泛化更好，在pass@k实验中实现了更高的覆盖率。

Conclusion: polychromic objective方法能有效保持策略多样性，提升探索能力和泛化性能，证明维持多样化策略库的重要性。

Abstract: Reinforcement learning fine-tuning (RLFT) is a dominant paradigm for
improving pretrained policies for downstream tasks. These pretrained policies,
trained on large datasets, produce generations with a broad range of promising
but unrefined behaviors. Often, a critical failure mode of RLFT arises when
policies lose this diversity and collapse into a handful of easily exploitable
outputs. This convergence hinders exploration, which is essential for expanding
the capabilities of the pretrained policy and for amplifying the benefits of
test-time compute scaling. To address this, we introduce an objective for
policy gradient methods that explicitly enforces the exploration and refinement
of diverse generations, which we call a polychromic objective. We then show how
proximal policy optimization (PPO) can be adapted to optimize this objective.
Our method (1) employs vine sampling to collect on-policy rollouts and (2)
modifies the advantage function to reflect the advantage under our new
objective. Experiments on BabyAI, Minigrid, and Algorithmic Creativity show
that our method improves success rates by reliably solving a larger set of
environment configurations and generalizes better under large perturbations.
Moreover, when given multiple attempts in pass@$k$ experiments, the policy
achieves substantially higher coverage, demonstrating its ability to maintain
and exploit a diverse repertoire of strategies.

</details>


### [94] [Feedback Control for Small Budget Pacing](https://arxiv.org/abs/2509.25429)
*Sreeja Apparaju,Yichuan Niu,Xixi Qi*

Main category: cs.LG

TL;DR: 提出一种结合分桶滞后和比例反馈的预算控制方法，显著提升广告投放的预算控制精度和稳定性。


<details>
  <summary>Details</summary>
Motivation: 在线广告中现有预算控制方法依赖临时参数调优，导致不稳定和低效，需要更稳定自适应的支出控制方案。

Method: 结合分桶滞后控制和比例反馈机制，提供参数选择框架和分析，实现跨广告活动的精确支出率跟踪。

Result: 在真实拍卖环境中，相比基线方法，预算控制误差降低13%，λ波动性降低54%，特别对小预算广告活动效果显著。

Conclusion: 通过将控制理论与广告系统结合，该方法为预算控制提供了可扩展且可靠的解决方案。

Abstract: Budget pacing is critical in online advertising to align spend with campaign
goals under dynamic auctions. Existing pacing methods often rely on ad-hoc
parameter tuning, which can be unstable and inefficient. We propose a
principled controller that combines bucketized hysteresis with proportional
feedback to provide stable and adaptive spend control. Our method provides a
framework and analysis for parameter selection that enables accurate tracking
of desired spend rates across campaigns. Experiments in real-world auctions
demonstrate significant improvements in pacing accuracy and delivery
consistency, reducing pacing error by 13% and $\lambda$-volatility by 54%
compared to baseline method. By bridging control theory with advertising
systems, our approach offers a scalable and reliable solution for budget
pacing, with particular benefits for small-budget campaigns.

</details>


### [95] [Beyond Noisy-TVs: Noise-Robust Exploration Via Learning Progress Monitoring](https://arxiv.org/abs/2509.25438)
*Zhibo Hou,Zhiyu An,Wan Du*

Main category: cs.LG

TL;DR: 提出了一种名为学习进度监控(LPM)的新方法，通过监控模型改进而非预测误差来指导探索，有效避免智能体被环境中的不可学习随机源(噪声电视)困住。


<details>
  <summary>Details</summary>
Motivation: 传统基于内在奖励的探索方法在面对环境中的不可学习随机源时，智能体会被这些噪声源困住，而基于不确定性估计或分布相似性的方法虽然最终能逃脱，但样本效率低且计算成本高。

Method: 采用双网络设计：使用误差模型预测动态模型在上一迭代中的预期预测误差，利用当前迭代与上一迭代模型误差的差异来指导探索，奖励模型改进而非预测误差或新颖性。

Result: 在基于MNIST、3D迷宫和Atari的噪声环境中，LPM的内在奖励收敛更快，在迷宫实验中探索更多状态，在Atari中获得更高的外在奖励。

Conclusion: LPM是一种概念简单但有效的噪声鲁棒探索方法，标志着探索范式的转变，通过监控学习进度而非预测误差来指导探索。

Abstract: When there exists an unlearnable source of randomness (noisy-TV) in the
environment, a naively intrinsic reward driven exploring agent gets stuck at
that source of randomness and fails at exploration. Intrinsic reward based on
uncertainty estimation or distribution similarity, while eventually escapes
noisy-TVs as time unfolds, suffers from poor sample efficiency and high
computational cost. Inspired by recent findings from neuroscience that humans
monitor their improvements during exploration, we propose a novel method for
intrinsically-motivated exploration, named Learning Progress Monitoring (LPM).
During exploration, LPM rewards model improvements instead of prediction error
or novelty, effectively rewards the agent for observing learnable transitions
rather than the unlearnable transitions. We introduce a dual-network design
that uses an error model to predict the expected prediction error of the
dynamics model in its previous iteration, and use the difference between the
model errors of the current iteration and previous iteration to guide
exploration. We theoretically show that the intrinsic reward of LPM is
zero-equivariant and a monotone indicator of Information Gain (IG), and that
the error model is necessary to achieve monotonicity correspondence with IG. We
empirically compared LPM against state-of-the-art baselines in noisy
environments based on MNIST, 3D maze with 160x120 RGB inputs, and Atari.
Results show that LPM's intrinsic reward converges faster, explores more states
in the maze experiment, and achieves higher extrinsic reward in Atari. This
conceptually simple approach marks a shift-of-paradigm of noise-robust
exploration. For code to reproduce our experiments, see
https://github.com/Akuna23Matata/LPM_exploration

</details>


### [96] [Norm-Q: Effective Compression Method for Hidden Markov Models in Neuro-Symbolic Applications](https://arxiv.org/abs/2509.25439)
*Hanyuan Gao,Xiaoxuan Yang*

Main category: cs.LG

TL;DR: 提出了Norm-Q方法，一种用于压缩概率符号模型（如隐马尔可夫模型）的归一化线性量化方法，通过减少数据位宽来缓解内存和带宽压力，实现高效部署。


<details>
  <summary>Details</summary>
Motivation: 隐马尔可夫模型在生成任务中表现出色，但神经符号应用继承了神经网络和符号推理的缺点，需要密集计算和数据传输，通信进一步影响性能。

Method: 采用归一化量化感知的期望最大化过程进行概率模型训练，减少数据位宽，最小化对性能的影响。

Result: 在大型语言模型的约束生成任务中，成功将4096个隐藏状态的HMM量化为8位无损失，最多3位可接受损失，权重压缩率达到99%。

Conclusion: Norm-Q方法相比传统量化方法实现了更高的压缩率和合理的分数损失，为概率符号模型的高效部署提供了有效解决方案。

Abstract: Hidden Markov models (HMM) are commonly used in generation tasks and have
demonstrated strong capabilities in neuro-symbolic applications for the Markov
property. These applications leverage the strengths of neural networks and
symbolic reasoning to create robust and interpretable AI systems. However, they
may inherit and amplify the shortcomings of both approaches. Both components
require dense computation and data transfer, and their communication further
hinders performance. This paper proposes Norm-Q, a normalized linear
quantization approach for compressing probabilistic symbolic models, such as
HMMs. We reduce the bit width of the data with minimal impact, thereby
alleviating memory and bandwidth stress and enabling deployment on potential
custom hardware. Our method introduces a normalized quantization-aware
expectation maximization process for probabilistic model training. The
experimental results show that Norm-Q achieves a higher compression rate with
reasonable score loss compared to traditional quantization methods. In the case
of the constrained generation task of large language models, we successfully
quantize an HMM of 4096 hidden states to 8 bits without loss and, at most, 3
bits with acceptable loss. Notably, the Norm-Q method can achieve a compression
rate of 99% for the weights of the HMM. The code is open source at
https://github.com/superstarghy/Norm-Q.

</details>


### [97] [Joint Embeddings Go Temporal](https://arxiv.org/abs/2509.25449)
*Sofiane Ennadir,Siavash Golkar,Leopoldo Sarra*

Main category: cs.LG

TL;DR: 提出了Time Series JEPA（TS-JEPA），一种专门为时间序列表示学习设计的联合嵌入预测架构，在分类和预测任务上达到或超越当前最先进基准。


<details>
  <summary>Details</summary>
Motivation: 解决自监督学习中自回归和掩码建模方法对噪声和混杂变量敏感的问题，将联合嵌入预测架构应用于时间序列领域。

Method: 基于联合嵌入预测架构（JEPA），在潜在空间进行自监督学习，专门针对时间序列数据进行适配。

Result: 在不同标准数据集上，TS-JEPA在分类和预测任务上能够匹配或超越当前最先进基准，展现出跨任务的强性能平衡。

Conclusion: 这项工作为基于联合嵌入的未来时间序列基础模型开发奠定了基础，展示了其作为学习通用表示的稳健基础模型的潜力。

Abstract: Self-supervised learning has seen great success recently in unsupervised
representation learning, enabling breakthroughs in natural language and image
processing. However, these methods often rely on autoregressive and masked
modeling, which aim to reproduce masked information in the input, which can be
vulnerable to the presence of noise or confounding variables. To address this
problem, Joint-Embedding Predictive Architectures (JEPA) has been introduced
with the aim to perform self-supervised learning in the latent space. To
leverage these advancements in the domain of time series, we introduce Time
Series JEPA (TS-JEPA), an architecture specifically adapted for time series
representation learning. We validate TS-JEPA on both classification and
forecasting, showing that it can match or surpass current state-of-the-art
baselines on different standard datasets. Notably, our approach demonstrates a
strong performance balance across diverse tasks, indicating its potential as a
robust foundation for learning general representations. Thus, this work lays
the groundwork for developing future time series foundation models based on
Joint Embedding.

</details>


### [98] [Data-Efficient Multitask DAgger](https://arxiv.org/abs/2509.25466)
*Haotian Fu,Ran Gong,Xiaohan Zhang,Maria Vittoria Minniti,Jigarkumar Patel,Karl Schmeckpeper*

Main category: cs.LG

TL;DR: 提出了一种数据高效的多任务DAgger框架，通过性能感知调度策略从多个任务专家策略中蒸馏出单一多任务策略，显著提高了任务成功率并减少了专家演示需求。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略通常需要大量专家数据或模拟训练，本文旨在开发一种数据高效的方法来学习多任务策略。

Method: 使用性能感知调度策略，基于卡尔曼滤波器估计器跟踪每个任务从数据中获益的程度，从而决定如何在不同任务间分配额外演示数据。

Result: 在MetaWorld和IsaacLab的抽屉开启任务套件上验证，所获策略在所有任务上都达到高性能，且使用更少的专家演示。模拟学习的视觉策略在零样本迁移到真实机器人时表现优于朴素DAgger和行为克隆。

Conclusion: 该方法能够高效学习多任务策略，减少对专家数据的依赖，并在真实机器人上实现良好的零样本迁移性能。

Abstract: Generalist robot policies that can perform many tasks typically require
extensive expert data or simulations for training. In this work, we propose a
novel Data-Efficient multitask DAgger framework that distills a single
multitask policy from multiple task-specific expert policies. Our approach
significantly increases the overall task success rate by actively focusing on
tasks where the multitask policy underperforms. The core of our method is a
performance-aware scheduling strategy that tracks how much each task's learning
process benefits from the amount of data, using a Kalman filter-based estimator
to robustly decide how to allocate additional demonstrations across tasks. We
validate our approach on MetaWorld, as well as a suite of diverse
drawer-opening tasks in IsaacLab. The resulting policy attains high performance
across all tasks while using substantially fewer expert demonstrations, and the
visual policy learned with our method in simulation shows better performance
than naive DAgger and Behavior Cloning when transferring zero-shot to a real
robot without using real data.

</details>


### [99] [Conformal Prediction for Signal Temporal Logic Inference](https://arxiv.org/abs/2509.25473)
*Danyang Li,Yixuan Wang,Matthew Cleaveland,Mingyu Cai,Roberto Tron*

Main category: cs.LG

TL;DR: 提出了一种端到端可微的符合预测框架，用于信号时序逻辑推理，同时提高推理准确性和提供统计保证。


<details>
  <summary>Details</summary>
Motivation: 现有STL推理方法缺乏对推断规则的正式置信保证，而传统符合预测通常作为后训练包装器，无法改进模型学习。

Method: 引入基于鲁棒性的非符合度评分，将平滑CP层直接嵌入训练，使用新损失函数同时优化推理准确性和CP预测集。

Result: 在基准时序任务上的实验表明，该方法减少了预测不确定性（实现高覆盖率同时减小预测集大小），并提高了准确性。

Conclusion: 该方法为学习到的STL公式提供统计保证，在可靠性和可解释性方面均优于现有方法。

Abstract: Signal Temporal Logic (STL) inference seeks to extract human-interpretable
rules from time-series data, but existing methods lack formal confidence
guarantees for the inferred rules. Conformal prediction (CP) is a technique
that can provide statistical correctness guarantees, but is typically applied
as a post-training wrapper without improving model learning. Instead, we
introduce an end-to-end differentiable CP framework for STL inference that
enhances both reliability and interpretability of the resulting formulas. We
introduce a robustness-based nonconformity score, embed a smooth CP layer
directly into training, and employ a new loss function that simultaneously
optimizes inference accuracy and CP prediction sets with a single term.
Following training, an exact CP procedure delivers statistical guarantees for
the learned STL formulas. Experiments on benchmark time-series tasks show that
our approach reduces uncertainty in predictions (i.e., it achieves high
coverage while reducing prediction set size), and improves accuracy (i.e., the
number of misclassifications when using a fixed threshold) over
state-of-the-art baselines.

</details>


### [100] [Translation from Wearable PPG to 12-Lead ECG](https://arxiv.org/abs/2509.25480)
*Hui Ji,Wei Gao,Pengfei Zhou*

Main category: cs.LG

TL;DR: P2Es是一个基于扩散模型的创新框架，能够从PPG信号生成临床有效的12导联心电图，通过频域模糊、时间噪声干扰和人口统计信息感知等技术实现高质量ECG重建。


<details>
  <summary>Details</summary>
Motivation: 现有12导联ECG系统需要复杂的多电极设置，限制了移动监测应用；而基于PPG的方法由于缺乏导联间约束和时空依赖建模，无法重建多导联ECG。

Method: 提出P2Es框架：前向过程使用频域模糊和时间噪声干扰模拟真实信号失真；反向过程采用时间多尺度生成和频域去模糊；结合KNN聚类和对比学习为反向过程分配亲和矩阵，实现人口统计特定的ECG转换。

Result: 广泛的实验结果表明，P2Es在12导联ECG重建方面优于基线模型。

Conclusion: P2Es成功填补了PPG到多导联ECG转换的技术空白，为移动心血管监测提供了可行的解决方案。

Abstract: The 12-lead electrocardiogram (ECG) is the gold standard for cardiovascular
monitoring, offering superior diagnostic granularity and specificity compared
to photoplethysmography (PPG). However, existing 12-lead ECG systems rely on
cumbersome multi-electrode setups, limiting sustained monitoring in ambulatory
settings, while current PPG-based methods fail to reconstruct multi-lead ECG
due to the absence of inter-lead constraints and insufficient modeling of
spatial-temporal dependencies across leads. To bridge this gap, we introduce
P2Es, an innovative demographic-aware diffusion framework designed to generate
clinically valid 12-lead ECG from PPG signals via three key innovations.
Specifically, in the forward process, we introduce frequency-domain blurring
followed by temporal noise interference to simulate real-world signal
distortions. In the reverse process, we design a temporal multi-scale
generation module followed by frequency deblurring. In particular, we leverage
KNN-based clustering combined with contrastive learning to assign affinity
matrices for the reverse process, enabling demographic-specific ECG
translation. Extensive experimental results show that P2Es outperforms baseline
models in 12-lead ECG reconstruction.

</details>


### [101] [Scalable Disk-Based Approximate Nearest Neighbor Search with Page-Aligned Graph](https://arxiv.org/abs/2509.25487)
*Dingyi Kang,Dongming Jiang,Hanshen Yang,Hang Liu,Bingzhe Li*

Main category: cs.LG

TL;DR: PageANN是一个基于磁盘的近似最近邻搜索框架，通过页节点图结构和对齐SSD页面的设计，显著提高了大规模向量搜索的性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于磁盘的ANNS方法存在I/O遍历路径长、与存储I/O粒度不对齐、内存索引开销高等问题，限制了大规模向量搜索的可扩展性。

Method: 提出页节点图结构，将逻辑图节点与物理SSD页面对齐；采用协同设计的磁盘数据布局和合并技术；设计结合轻量级索引和协调内存-磁盘数据分配的内存管理策略。

Result: 在多个数据集和内存预算下，PageANN比最先进的磁盘ANNS方法实现了1.85x-10.83x的吞吐量提升和51.7%-91.9%的延迟降低，同时保持高召回率。

Conclusion: PageANN通过创新的页节点图结构和优化的内存管理策略，有效解决了磁盘ANNS的性能瓶颈，为大规模向量搜索提供了高效可扩展的解决方案。

Abstract: Approximate Nearest Neighbor Search (ANNS), as the core of vector databases
(VectorDBs), has become widely used in modern AI and ML systems, powering
applications from information retrieval to bio-informatics. While graph-based
ANNS methods achieve high query efficiency, their scalability is constrained by
the available host memory. Recent disk-based ANNS approaches mitigate memory
usage by offloading data to Solid-State Drives (SSDs). However, they still
suffer from issues such as long I/O traversal path, misalignment with storage
I/O granularity, and high in-memory indexing overhead, leading to significant
I/O latency and ultimately limiting scalability for large-scale vector search.
  In this paper, we propose PageANN, a disk-based approximate nearest neighbor
search (ANNS) framework designed for high performance and scalability. PageANN
introduces a page-node graph structure that aligns logical graph nodes with
physical SSD pages, thereby shortening I/O traversal paths and reducing I/O
operations. Specifically, similar vectors are clustered into page nodes, and a
co-designed disk data layout leverages this structure with a merging technique
to store only representative vectors and topology information, avoiding
unnecessary reads. To further improve efficiency, we design a memory management
strategy that combines lightweight indexing with coordinated memory-disk data
allocation, maximizing host memory utilization while minimizing query latency
and storage overhead. Experimental results show that PageANN significantly
outperforms state-of-the-art (SOTA) disk-based ANNS methods, achieving
1.85x-10.83x higher throughput and 51.7%-91.9% lower latency across different
datasets and memory budgets, while maintaining comparable high recall accuracy.

</details>


### [102] [Can Molecular Foundation Models Know What They Don't Know? A Simple Remedy with Preference Optimization](https://arxiv.org/abs/2509.25509)
*Langzhou He,Junyou Zhu,Fangxin Wang,Junhua Liu,Haoyan Xu,Yue Zhao,Philip S. Yu,Qitian Wu*

Main category: cs.LG

TL;DR: Mole-PAIR是一个即插即用的模块，通过后训练提升分子基础模型在分布外数据上的可靠性，将OOD检测问题转化为基于ID和OOD样本亲和度估计的偏好优化问题。


<details>
  <summary>Details</summary>
Motivation: 分子基础模型在分布外样本上的不可靠性严重限制了其在药物发现和蛋白质设计等高风险领域的应用，特别是化学幻觉问题——模型对未知分子做出高置信度但完全错误的预测。

Method: 提出分子偏好对齐实例排序方法，将OOD检测问题表述为对ID和OOD样本之间估计的OOD亲和度的偏好优化，通过成对学习目标实现，本质上优化AUROC指标。

Result: 在五个真实世界分子数据集上的实验表明，该方法显著提升了现有分子基础模型的OOD检测能力，在大小、支架和测定分布偏移下分别实现了45.8%、43.9%和24.3%的AUROC改进。

Conclusion: Mole-PAIR是一种简单有效的解决方案，能够通过低成本的后训练显著提高分子基础模型在分布外数据上的可靠性。

Abstract: Molecular foundation models are rapidly advancing scientific discovery, but
their unreliability on out-of-distribution (OOD) samples severely limits their
application in high-stakes domains such as drug discovery and protein design. A
critical failure mode is chemical hallucination, where models make
high-confidence yet entirely incorrect predictions for unknown molecules. To
address this challenge, we introduce Molecular Preference-Aligned Instance
Ranking (Mole-PAIR), a simple, plug-and-play module that can be flexibly
integrated with existing foundation models to improve their reliability on OOD
data through cost-effective post-training. Specifically, our method formulates
the OOD detection problem as a preference optimization over the estimated OOD
affinity between in-distribution (ID) and OOD samples, achieving this goal
through a pairwise learning objective. We show that this objective essentially
optimizes AUROC, which measures how consistently ID and OOD samples are ranked
by the model. Extensive experiments across five real-world molecular datasets
demonstrate that our approach significantly improves the OOD detection
capabilities of existing molecular foundation models, achieving up to 45.8%,
43.9%, and 24.3% improvements in AUROC under distribution shifts of size,
scaffold, and assay, respectively.

</details>


### [103] [EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit](https://arxiv.org/abs/2509.25510)
*Chang Liu,Danial Chitnis*

Main category: cs.LG

TL;DR: EEsizer是一个基于大语言模型的AI代理，通过集成LLM与电路仿真器和数据分析功能，实现全自动的晶体管尺寸优化，无需外部知识。


<details>
  <summary>Details</summary>
Motivation: 模拟和混合信号集成电路设计中的晶体管尺寸优化过程需要大量人工参与，现有机器学习方法仍面临迭代次数多和缺乏电路设计知识的问题。

Method: 使用提示工程和思维链推理，集成大语言模型与电路仿真器，通过迭代探索设计方向、评估性能和优化解决方案。

Result: 在90nm技术节点上，OpenAI o3模型成功实现了用户设定的目标，最多仅需20次迭代，展示了在先进节点上的适应性和鲁棒性。

Conclusion: 基于大语言模型的AI代理能够有效自动化晶体管尺寸优化过程，减少人工干预，提高设计效率。

Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often
involves significant manual effort, especially during the transistor sizing
process. While Machine Learning techniques in Electronic Design Automation
(EDA) have shown promise in reducing complexity and minimizing human
intervention, they still face challenges such as numerous iterations and a lack
of knowledge about AMS circuit design. Recently, Large Language Models (LLMs)
have demonstrated significant potential across various fields, showing a
certain level of knowledge in circuit design and indicating their potential to
automate the transistor sizing process. In this work, we propose EEsizer, an
LLM-based AI agent that integrates large language models with circuit
simulators and custom data analysis functions, enabling fully automated,
closed-loop transistor sizing without relying on external knowledge. By
employing prompt engineering and Chain-of-Thought reasoning, the agent
iteratively explores design directions, evaluates performance, and refines
solutions with minimal human intervention. We first benchmarked 8 LLMs on six
basic circuits and selected three high-performing models to optimize a
20-transistor CMOS operational amplifier, targeting multiple performance
metrics, including rail-to-rail operation from 180 nm to 90 nm technology
nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90
nm across three different test groups, with a maximum of 20 iterations,
demonstrating adaptability and robustness at advanced nodes. To assess design
robustness, we manually designed a bias circuit and performed a variation
analysis using Gaussian-distributed variations on transistor dimensions and
threshold voltages.

</details>


### [104] [World Model for AI Autonomous Navigation in Mechanical Thrombectomy](https://arxiv.org/abs/2509.25518)
*Harry Robertshaw,Han-Ru Wu,Alejandro Granados,Thomas C Booth*

Main category: cs.LG

TL;DR: 提出基于TD-MPC2世界模型的自主血管内导航方法，在10个真实患者血管中训练单一RL智能体，相比SAC方法成功率从37%提升至65%。


<details>
  <summary>Details</summary>
Motivation: 机械取栓术的自主导航面临血管解剖复杂性和实时决策精确性的挑战，现有RL方法在多患者血管泛化和长时程任务上表现不佳。

Method: 使用TD-MPC2模型强化学习算法构建世界模型，在10个真实患者血管数据上训练单一智能体进行多任务血管内导航。

Result: TD-MPC2在多任务学习中显著优于SAC，平均成功率65% vs 37%，路径比率改善，但手术时间增加。

Conclusion: 世界模型在改善自主血管内导航方面具有潜力，为未来可泛化AI驱动机器人介入研究奠定基础。

Abstract: Autonomous navigation for mechanical thrombectomy (MT) remains a critical
challenge due to the complexity of vascular anatomy and the need for precise,
real-time decision-making. Reinforcement learning (RL)-based approaches have
demonstrated potential in automating endovascular navigation, but current
methods often struggle with generalization across multiple patient vasculatures
and long-horizon tasks. We propose a world model for autonomous endovascular
navigation using TD-MPC2, a model-based RL algorithm. We trained a single RL
agent across multiple endovascular navigation tasks in ten real patient
vasculatures, comparing performance against the state-of-the-art Soft
Actor-Critic (SAC) method. Results indicate that TD-MPC2 significantly
outperforms SAC in multi-task learning, achieving a 65% mean success rate
compared to SAC's 37%, with notable improvements in path ratio. TD-MPC2
exhibited increased procedure times, suggesting a trade-off between success
rate and execution speed. These findings highlight the potential of world
models for improving autonomous endovascular navigation and lay the foundation
for future research in generalizable AI-driven robotic interventions.

</details>


### [105] [Steering an Active Learning Workflow Towards Novel Materials Discovery via Queue Prioritization](https://arxiv.org/abs/2509.25538)
*Marcus Schwarting,Logan Ward,Nathaniel Hudson,Xiaoli Yan,Ben Blaiszik,Santanu Chaudhuri,Eliu Huerta,Ian Foster*

Main category: cs.LG

TL;DR: 提出了一种结合生成建模和主动学习的队列优先级算法，用于解决科学中的逆向设计问题，显著提高了高质量候选分子的生成数量。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在解决科学逆向设计问题时既带来机遇也伴随风险，能够自主扩展和优化搜索空间，但会探索低质量区域直到充分微调。

Method: 结合生成建模和主动学习的队列优先级算法，在分布式工作流中探索复杂设计空间，使用主动学习模型优先处理顶级设计候选。

Result: 在碳捕获分子结构发现任务中，主动学习方法显著提高了高质量候选数量：无主动学习平均生成281个高质量候选，有主动学习平均生成604个高质量候选。

Conclusion: 主动学习方法能防止生成式AI工作流在无意义候选上浪费资源，阻止生成模型退化，显著提升高质量候选分子的识别效率。

Abstract: Generative AI poses both opportunities and risks for solving inverse design
problems in the sciences. Generative tools provide the ability to expand and
refine a search space autonomously, but do so at the cost of exploring
low-quality regions until sufficiently fine tuned. Here, we propose a queue
prioritization algorithm that combines generative modeling and active learning
in the context of a distributed workflow for exploring complex design spaces.
We find that incorporating an active learning model to prioritize top design
candidates can prevent a generative AI workflow from expending resources on
nonsensical candidates and halt potential generative model decay. For an
existing generative AI workflow for discovering novel molecular structure
candidates for carbon capture, our active learning approach significantly
increases the number of high-quality candidates identified by the generative
model. We find that, out of 1000 novel candidates, our workflow without active
learning can generate an average of 281 high-performing candidates, while our
proposed prioritization with active learning can generate an average 604
high-performing candidates.

</details>


### [106] [Lightweight and Robust Federated Data Valuation](https://arxiv.org/abs/2509.25560)
*Guojun Tang,Jiayu Zhou,Mohammad Mamun,Steve Drew*

Main category: cs.LG

TL;DR: FedIF是一个联邦学习聚合框架，使用基于轨迹的影响力估计来高效计算客户端贡献，在保持与Shapley值方法相当的鲁棒性同时，将聚合开销降低高达450倍。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临非独立同分布数据和对抗性客户端行为的鲁棒性挑战，现有基于Shapley值的方法计算开销过高，限制了可扩展性。

Method: FedIF通过轻量级梯度操作在客户端更新和公共验证集上计算归一化和平滑的影响力分数，实现基于轨迹的影响力估计。

Result: 在CIFAR-10和Fashion-MNIST上的实验表明，FedIF在标签噪声、梯度噪声和对抗样本存在时，鲁棒性达到或超过SV方法，同时聚合开销降低450倍。

Conclusion: FedIF为现实世界部署提供了一个实用、理论可靠且可扩展的联邦学习鲁棒聚合方案，是Shapley值方法的有效替代。

Abstract: Federated learning (FL) faces persistent robustness challenges due to non-IID
data distributions and adversarial client behavior. A promising mitigation
strategy is contribution evaluation, which enables adaptive aggregation by
quantifying each client's utility to the global model. However,
state-of-the-art Shapley-value-based approaches incur high computational
overhead due to repeated model reweighting and inference, which limits their
scalability. We propose FedIF, a novel FL aggregation framework that leverages
trajectory-based influence estimation to efficiently compute client
contributions. FedIF adapts decentralized FL by introducing normalized and
smoothed influence scores computed from lightweight gradient operations on
client updates and a public validation set. Theoretical analysis demonstrates
that FedIF yields a tighter bound on one-step global loss change under noisy
conditions. Extensive experiments on CIFAR-10 and Fashion-MNIST show that FedIF
achieves robustness comparable to or exceeding SV-based methods in the presence
of label noise, gradient noise, and adversarial samples, while reducing
aggregation overhead by up to 450x. Ablation studies confirm the effectiveness
of FedIF's design choices, including local weight normalization and influence
smoothing. Our results establish FedIF as a practical, theoretically grounded,
and scalable alternative to Shapley-value-based approaches for efficient and
robust FL in real-world deployments.

</details>


### [107] [Safe In-Context Reinforcement Learning](https://arxiv.org/abs/2509.25582)
*Amir Moeini,Minjae Kwon,Alper Kamil Bozkurt,Yuichi Motai,Rohan Chandra,Lu Feng,Shangtong Zhang*

Main category: cs.LG

TL;DR: 提出了首个在上下文强化学习中保证安全适应的方法，在无参数更新的适应过程中同时最大化奖励和最小化成本函数


<details>
  <summary>Details</summary>
Motivation: 现有的上下文强化学习(ICRL)方法在适应过程中缺乏安全保障机制，需要在不进行参数更新的情况下确保智能体在约束马尔可夫决策过程框架下的安全行为

Method: 在ICRL框架中引入约束马尔可夫决策过程，通过扩展策略神经网络的输入上下文，使智能体在适应过程中同时优化奖励和成本函数，并能根据成本容忍度阈值主动调整行为

Result: 智能体能够根据成本预算主动调整行为策略：成本预算较高时行为更激进，成本预算较低时行为更保守，实现了安全的参数更新自由适应

Conclusion: 该方法成功地将安全保障机制集成到上下文强化学习的适应过程中，使智能体能够在无参数更新的情况下安全地适应分布外任务，并能根据成本容忍度灵活调整行为

Abstract: In-context reinforcement learning (ICRL) is an emerging RL paradigm where the
agent, after some pretraining procedure, is able to adapt to
out-of-distribution test tasks without any parameter updates. The agent
achieves this by continually expanding the input (i.e., the context) to its
policy neural networks. For example, the input could be all the history
experience that the agent has access to until the current time step. The
agent's performance improves as the input grows, without any parameter updates.
In this work, we propose the first method that promotes the safety of ICRL's
adaptation process in the framework of constrained Markov Decision Processes.
In other words, during the parameter-update-free adaptation process, the agent
not only maximizes the reward but also minimizes an additional cost function.
We also demonstrate that our agent actively reacts to the threshold (i.e.,
budget) of the cost tolerance. With a higher cost budget, the agent behaves
more aggressively, and with a lower cost budget, the agent behaves more
conservatively.

</details>


### [108] [Machine Learning Algorithms for Improving Black Box Optimization Solvers](https://arxiv.org/abs/2509.25592)
*Morteza Kimiaei,Vyacheslav Kungurtsev*

Main category: cs.LG

TL;DR: 该论文综述了机器学习和强化学习如何增强黑盒优化，将传统无导数方法转化为更可扩展、鲁棒和自适应的优化框架。


<details>
  <summary>Details</summary>
Motivation: 传统黑盒优化方法在高维、噪声或混合整数设置中表现不佳，需要利用ML和RL的先进技术来提升性能。

Method: 涵盖多种代表性算法，包括神经网络模型优化框架、零阶自适应动量方法、自动化BBO、分布式块优化、基于变换器的优化器、扩散模型BBO等。

Result: ML和RL技术显著提升了黑盒优化的可扩展性、鲁棒性和适应性，能够处理更复杂的现实世界优化问题。

Conclusion: 机器学习和强化学习正在彻底改变传统黑盒优化方法，为现实世界优化问题提供更强大的解决方案。

Abstract: Black-box optimization (BBO) addresses problems where objectives are
accessible only through costly queries without gradients or explicit structure.
Classical derivative-free methods -- line search, direct search, and
model-based solvers such as Bayesian optimization -- form the backbone of BBO,
yet often struggle in high-dimensional, noisy, or mixed-integer settings.
  Recent advances use machine learning (ML) and reinforcement learning (RL) to
enhance BBO: ML provides expressive surrogates, adaptive updates, meta-learning
portfolios, and generative models, while RL enables dynamic operator
configuration, robustness, and meta-optimization across tasks.
  This paper surveys these developments, covering representative algorithms
such as NNs with the modular model-based optimization framework (mlrMBO),
zeroth-order adaptive momentum methods (ZO-AdaMM), automated BBO (ABBO),
distributed block-wise optimization (DiBB), partition-based Bayesian
optimization (SPBOpt), the transformer-based optimizer (B2Opt),
diffusion-model-based BBO, surrogate-assisted RL for differential evolution
(Surr-RLDE), robust BBO (RBO), coordinate-ascent model-based optimization with
relative entropy (CAS-MORE), log-barrier stochastic gradient descent (LB-SGD),
policy improvement with black-box (PIBB), and offline Q-learning with Mamba
backbones (Q-Mamba).
  We also review benchmark efforts such as the NeurIPS 2020 BBO Challenge and
the MetaBox framework. Overall, we highlight how ML and RL transform classical
inexact solvers into more scalable, robust, and adaptive frameworks for
real-world optimization.

</details>


### [109] [Binary Sparse Coding for Interpretability](https://arxiv.org/abs/2509.25596)
*Lucia Quirke,Stepan Shabalin,Nora Belrose*

Main category: cs.LG

TL;DR: 提出二进制稀疏自编码器(BAE)和二进制转码器(BTC)，通过将激活值二值化(0或1)来提升特征的解释性和单义性，但会增加重构误差。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器(SAE)的许多特征只有在高激活强度下才可解释，希望通过二值化消除连续激活变化中隐藏的不可解释信息。

Method: 使用二进制稀疏自编码器和二进制转码器，将所有激活值约束为0或1，消除连续激活强度的变化。

Result: 二值化显著提高了发现特征的可解释性和单义性，但增加了重构误差，并且产生了更多不可解释的超高频特征。频率调整后，连续稀疏编码器的解释性得分略优于二进制方法。

Conclusion: 多义性可能是神经激活的固有属性，难以完全消除。

Abstract: Sparse autoencoders (SAEs) are used to decompose neural network activations
into sparsely activating features, but many SAE features are only interpretable
at high activation strengths. To address this issue we propose to use binary
sparse autoencoders (BAEs) and binary transcoders (BTCs), which constrain all
activations to be zero or one. We find that binarisation significantly improves
the interpretability and monosemanticity of the discovered features, while
increasing reconstruction error. By eliminating the distinction between high
and low activation strengths, we prevent uninterpretable information from being
smuggled in through the continuous variation in feature activations. However,
we also find that binarisation increases the number of uninterpretable
ultra-high frequency features, and when interpretability scores are
frequency-adjusted, the scores for continuous sparse coders are slightly better
than those of binary ones. This suggests that polysemanticity may be an
ineliminable property of neural activations.

</details>


### [110] [Effective Model Pruning](https://arxiv.org/abs/2509.25606)
*Yixuan Wang,Dan Guralnik,Saiedeh Akbari,Warren Dixon*

Main category: cs.LG

TL;DR: EMP是一种与上下文无关、无需参数的剪枝规则，通过计算有效数量N_eff来确定保留多少参数，适用于各种剪枝标准和模型架构。


<details>
  <summary>Details</summary>
Motivation: 解决剪枝中的一个基本问题：应该保留多少参数。现有方法通常需要手动设置阈值或依赖特定上下文，缺乏通用性。

Method: 基于逆辛普森指数计算有效数量N_eff，将其作为通用自适应阈值。保留得分最高的N_eff个参数，其余置零。

Result: 在MLP、CNN、Transformer/LLM和KAN等模型上，使用EMP剪枝后的稀疏模型性能与原始密集网络相当。

Conclusion: EMP提供了一个稳健的剪枝阈值，默认β=1效果良好，β≠1可作为可选调整以满足特定稀疏度需求。

Abstract: We introduce Effective Model Pruning (EMP), a context-agnostic,
parameter-free rule addressing a fundamental question about pruning: how many
entries to keep. EMP does not prescribe how to score the parameters or prune
the models; instead, it supplies a universal adaptive threshold that can be
applied to any pruning criterion: weight magnitude, attention score, KAN
importance score, or even feature-level signals such as image pixel, and used
on structural parts or weights of the models. Given any score vector s, EMP
maps s to a built-in effective number N_eff which is inspired by the Inverse
Simpson index of contributors. Retaining the N_eff highest scoring entries and
zeroing the remainder yields sparse models with performance comparable to the
original dense networks across MLPs, CNNs, Transformers/LLMs, and KAN, in our
experiments. By leveraging the geometry of the simplex, we derive a tight lower
bound on the preserved mass s_eff (the sum of retained scores) over the
corresponding ordered probability simplex associated with the score vector s.
We further verify the effectiveness of N_eff by pruning the model with a scaled
threshold \b{eta}*N_eff across a variety of criteria and models. Experiments
suggest that the default \b{eta} = 1 yields a robust threshold for model
pruning while \b{eta} not equal to 1 still serves as an optional adjustment to
meet specific sparsity requirements.

</details>


### [111] [Layer-wise dynamic rank for compressing large language models](https://arxiv.org/abs/2509.25622)
*Zhendong Mi,Bian Sun,Grace Li Zhang,Shaoyi Huang*

Main category: cs.LG

TL;DR: D-Rank是一个基于层间动态秩分配的LLM压缩框架，通过有效秩度量信息密度，使用拉格朗日乘子优化方案自适应分配秩，在固定压缩比下为信息密度更高的层分配更多容量。


<details>
  <summary>Details</summary>
Motivation: 现有SVD压缩方法对所有层使用统一压缩比，忽略了LLM中不同层的信息密度差异，特别是中间层信息更丰富而早期和后期层更冗余的异质性。

Method: 引入有效秩作为权重矩阵信息密度的度量指标，使用拉格朗日乘子优化方案自适应分配秩，并对注意力层进行秩重平衡以考虑其重要性差异，扩展到分组查询注意力的最新LLM。

Result: 在多个压缩比下对各种规模的LLM进行广泛实验，D-Rank始终优于SVD-LLM、ASVD和Basis Sharing，在20%压缩比下LLaMA-3-8B模型在C4数据集上困惑度降低超过15%，在40%压缩比下LLaMA-7B模型的零样本推理准确率提高达5%，同时实现更高的吞吐量。

Conclusion: D-Rank通过考虑层间信息密度异质性的动态秩分配，显著提升了LLM压缩效果，在保持高压缩比的同时改善了模型性能。

Abstract: Large language models (LLMs) have rapidly scaled in size, bringing severe
memory and computational challenges that hinder their deployment. Singular
Value Decomposition (SVD)-based compression has emerged as an appealing
post-training compression technique for LLMs, yet most existing methods apply a
uniform compression ratio across all layers, implicitly assuming homogeneous
information included in various layers. This overlooks the substantial
intra-layer heterogeneity observed in LLMs, where middle layers tend to encode
richer information while early and late layers are more redundant. In this
work, we revisit the existing SVD-based compression method and propose D-Rank,
a framework with layer-wise balanced Dynamic Rank allocation for LLMs
compression. We first introduce effective rank as a principled metric to
measure the information density of weight matrices, and then allocate ranks via
a Lagrange multiplier-based optimization scheme to adaptively assign more
capacity to groups with higher information density under a fixed compression
ratio. Moreover, we rebalance the allocated ranks across attention layers to
account for their varying importance and extend D-Rank to latest LLMs with
grouped-query attention. Extensive experiments on various LLMs with different
scales across multiple compression ratios demonstrate that D-Rank consistently
outperforms SVD-LLM, ASVD, and Basis Sharing, achieving more than 15 lower
perplexity with LLaMA-3-8B model on C4 datasets at 20% compression ratio and up
to 5% higher zero-shot reasoning accuracy with LLaMA-7B model at 40%
compression ratio while achieving even higher throughput.

</details>


### [112] [Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting](https://arxiv.org/abs/2509.25631)
*Jason Stock,Troy Arcomano,Rao Kotamarthi*

Main category: cs.LG

TL;DR: Swift是一个单步一致性模型，首次实现了基于CRPS目标的自回归微调概率流模型，消除了多模型集成或参数扰动的需求，在S2S天气预报中实现了39倍加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型为概率天气预报提供了物理基础框架，但其典型的慢速迭代推理方法在S2S应用中不实用，因为需要长提前期和领域驱动的校准。

Method: 引入Swift单步一致性模型，首次实现基于连续排名概率得分(CRPS)目标的自回归微调概率流模型，无需多模型集成或参数扰动。

Result: Swift产生熟练的6小时预报，可稳定运行长达75天，比最先进的扩散基线快39倍，预报技能与基于数值的IFS ENS操作模型相当。

Conclusion: 这是向从中期到季节尺度的高效可靠集合预报迈出的一步。

Abstract: Diffusion models offer a physically grounded framework for probabilistic
weather forecasting, but their typical reliance on slow, iterative solvers
during inference makes them impractical for subseasonal-to-seasonal (S2S)
applications where long lead-times and domain-driven calibration are essential.
To address this, we introduce Swift, a single-step consistency model that, for
the first time, enables autoregressive finetuning of a probability flow model
with a continuous ranked probability score (CRPS) objective. This eliminates
the need for multi-model ensembling or parameter perturbations. Results show
that Swift produces skillful 6-hourly forecasts that remain stable for up to 75
days, running $39\times$ faster than state-of-the-art diffusion baselines while
achieving forecast skill competitive with the numerical-based, operational IFS
ENS. This marks a step toward efficient and reliable ensemble forecasting from
medium-range to seasonal-scales.

</details>


### [113] [How Does Preconditioning Guide Feature Learning in Deep Neural Networks?](https://arxiv.org/abs/2509.25637)
*Kotaro Yoshida,Atsushi Nitanda*

Main category: cs.LG

TL;DR: 该研究探讨了预条件处理如何通过Gram矩阵影响特征学习和泛化性能，发现预条件器引入的光谱偏置与教师模型光谱对齐时能显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 预条件处理在机器学习中被广泛用于加速经验风险收敛，但其对期望风险的影响尚未充分探索。

Method: 将预条件器实例化为输入协方差矩阵的p次幂，在单索引教师模型框架下分析预条件处理对特征学习的影响。

Result: 研究表明学习到的特征表示紧密反映了预条件器引入的光谱偏置，当这种偏置与教师模型光谱对齐时，泛化性能显著提升。

Conclusion: 预条件处理通过控制光谱偏置影响特征学习，泛化性能的关键在于预条件器光谱偏置与教师模型光谱的对齐程度。

Abstract: Preconditioning is widely used in machine learning to accelerate convergence
on the empirical risk, yet its role on the expected risk remains underexplored.
In this work, we investigate how preconditioning affects feature learning and
generalization performance. We first show that the input information available
to the model is conveyed solely through the Gram matrix defined by the
preconditioner's metric, thereby inducing a controllable spectral bias on
feature learning. Concretely, instantiating the preconditioner as the $p$-th
power of the input covariance matrix and within a single-index teacher model,
we prove that in generalization, the exponent $p$ and the alignment between the
teacher and the input spectrum are crucial factors. We further investigate how
the interplay between these factors influences feature learning from three
complementary perspectives: (i) Robustness to noise, (ii) Out-of-distribution
generalization, and (iii) Forward knowledge transfer. Our results indicate that
the learned feature representations closely mirror the spectral bias introduced
by the preconditioner -- favoring components that are emphasized and exhibiting
reduced sensitivity to those that are suppressed. Crucially, we demonstrate
that generalization is significantly enhanced when this spectral bias is
aligned with that of the teacher.

</details>


### [114] [Deep set based operator learning with uncertainty quantification](https://arxiv.org/abs/2509.25646)
*Lei Ma,Ling Guo,Hao Wu,Tao Zhou*

Main category: cs.LG

TL;DR: 提出了UQ-SONet，一种具有内置不确定性量化的置换不变算子学习框架，解决了DeepONets在传感器约束和不确定性量化方面的局限性。


<details>
  <summary>Details</summary>
Motivation: DeepONets虽然广泛用于科学机器学习，但需要固定传感器数量和位置，缺乏不确定性量化机制，限制了实际应用。现有扩展如VIDON放松了传感器约束，但仍无法处理不完整测量或固有随机性带来的不确定性。

Method: 集成集合变换器嵌入来处理稀疏和可变传感器位置，并使用条件变分自编码器（cVAE）来近似解算子的条件分布，通过最小化负ELBO提供原则性的不确定性估计。

Result: 在确定性和随机偏微分方程（包括Navier-Stokes方程）上的数值实验证明了该框架的鲁棒性和有效性。

Conclusion: UQ-SONet在保持预测准确性的同时，提供了原则性的不确定性估计，解决了现有算子学习方法在传感器约束和不确定性量化方面的关键限制。

Abstract: Learning operators from data is central to scientific machine learning. While
DeepONets are widely used for their ability to handle complex domains, they
require fixed sensor numbers and locations, lack mechanisms for uncertainty
quantification (UQ), and are thus limited in practical applicability. Recent
permutationinvariant extensions, such as the Variable-Input Deep Operator
Network (VIDON), relax these sensor constraints but still rely on sufficiently
dense observations and cannot capture uncertainties arising from incomplete
measurements or from operators with inherent randomness. To address these
challenges, we propose UQ-SONet, a permutation-invariant operator learning
framework with built-in UQ. Our model integrates a set transformer embedding to
handle sparse and variable sensor locations, and employs a conditional
variational autoencoder (cVAE) to approximate the conditional distribution of
the solution operator. By minimizing the negative ELBO, UQ-SONet provides
principled uncertainty estimation while maintaining predictive accuracy.
Numerical experiments on deterministic and stochastic PDEs, including the
Navier-Stokes equation, demonstrate the robustness and effectiveness of the
proposed framework.

</details>


### [115] [Growing Winning Subnetworks, Not Pruning Them: A Paradigm for Density Discovery in Sparse Neural Networks](https://arxiv.org/abs/2509.25665)
*Qihang Yao,Constantine Dovrolis*

Main category: cs.LG

TL;DR: PWMPR是一种从稀疏到密集的训练方法，通过路径权重乘积引导的随机增长自动发现网络的最佳密度，避免了传统剪枝方法的高重训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏网络训练方法（迭代剪枝、动态稀疏训练、初始化剪枝）要么需要大量重训练成本，要么需要预先确定目标密度。PWMPR旨在通过增长而非剪枝的方式，自动发现网络的最佳操作密度。

Method: 从稀疏种子开始，PWMPR使用路径核启发式评分指导添加边，通过随机化缓解瓶颈，并在逻辑拟合规则检测到精度平台期时停止增长。

Result: 在CIFAR、TinyImageNet和ImageNet上的实验表明，PWMPR接近IMP获得的彩票券性能（虽然密度更高），但成本显著降低（约1.5倍密集训练 vs IMP的3-4倍）。

Conclusion: 基于增长的密度发现是一个有前景的范式，与剪枝和动态稀疏方法形成互补。

Abstract: The lottery ticket hypothesis suggests that dense networks contain sparse
subnetworks that can be trained in isolation to match full-model performance.
Existing approaches-iterative pruning, dynamic sparse training, and pruning at
initialization-either incur heavy retraining costs or assume the target density
is fixed in advance. We introduce Path Weight Magnitude Product-biased Random
growth (PWMPR), a constructive sparse-to-dense training paradigm that grows
networks rather than pruning them, while automatically discovering their
operating density. Starting from a sparse seed, PWMPR adds edges guided by
path-kernel-inspired scores, mitigates bottlenecks via randomization, and stops
when a logistic-fit rule detects plateauing accuracy. Experiments on CIFAR,
TinyImageNet, and ImageNet show that PWMPR approaches the performance of
IMP-derived lottery tickets-though at higher density-at substantially lower
cost (~1.5x dense vs. 3-4x for IMP). These results establish growth-based
density discovery as a promising paradigm that complements pruning and dynamic
sparsity.

</details>


### [116] [Nudging the Boundaries of LLM Reasoning](https://arxiv.org/abs/2509.25666)
*Justin Chih-Yao Chen,Becky Xiangyu Peng,Prafulla Kumar Choubey,Kung-Hsiang Huang,Jiaxin Zhang,Mohit Bansal,Chien-Sheng Wu*

Main category: cs.LG

TL;DR: NuRL是一种新的强化学习方法，通过自生成提示来提升LLM推理的上限，解决了现有方法无法从模型无法解决的问题中学习的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前在线强化学习方法如GRPO无法从模型无法解决的问题中学习，导致模型的上限无法提升。NuRL旨在通过自生成提示来解锁这些困难样本的学习潜力。

Method: NuRL使用自生成的提示来降低问题难度。对于通过率为0%的困难样本，注入提示并重新生成轨迹批次，从而引入训练信号。

Result: NuRL在6个基准测试和3个模型上取得了一致的改进，能够提升模型的上限，而GRPO则无法改变pass@1024指标。

Conclusion: NuRL通过自生成提示有效提升了LLM推理能力，特别是抽象且高层次的提示在GRPO收敛后应用效果最佳。

Abstract: Current online reinforcement learning (RL) algorithms like GRPO share a key
limitation in LLM reasoning: they cannot learn from problems that are
"unsolvable" to the model. In other words, they can only improve performance on
problems where the model is capable of exploring the correct answer.
Consequently, the model's "upper limit" remains unchanged after RL training,
even though the likelihood of solving easier, solvable problems may increase.
These hard samples cannot contribute to training, as no rollouts yield rewards
and thus no gradients are produced. To unlock learning from these hard samples,
we propose NuRL, a "nudging" method that aims to push the upper bound of LLM
reasoning using self-generated hints, i.e., abstract cues that help reduce the
problem difficulty for the model. Given a question and its gold answer, the
model generates a CoT and then produces a hint containing the core knowledge
needed to solve the problem. During training, we generate G rollouts from the
base policy and use the pass rate to decide whether the hint should be
injected. For hard samples with a 0% pass rate, we inject the hint and
regenerate a new batch of trajectories. This yields two benefits: (1) the hint
boosts pass rates (from 0% to non-zero), thereby introducing training signals
for previously unsolvable samples, and (2) the hints are self-generated,
avoiding distributional shift and do not rely on external models. NuRL achieves
consistent improvements across 6 benchmarks and 3 models, while remaining
complementary to test-time scaling. Notably, NuRL can raise the model's upper
limit, whereas GRPO leaves pass@1024 unchanged from the base model.
Furthermore, we present a systematic study of what makes an effective hint and
when hints are most useful. Interestingly, the best hints are abstract and
high-level, and are most beneficial when applied necessarily and after GRPO has
converged.

</details>


### [117] [EEG-based AI-BCI Wheelchair Advancement: Hybrid Deep Learning with Motor Imagery for Brain Computer Interface](https://arxiv.org/abs/2509.25667)
*Bipul Thapa,Biplov Paneru,Bishwash Paneru,Khem Narayan Poudyal*

Main category: cs.LG

TL;DR: 本文提出了一种基于AI的脑机接口轮椅控制系统，使用运动想象左右手运动机制来控制轮椅导航，通过BiLSTM-BiGRU模型实现了92.26%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 开发一种基于运动想象的脑机接口轮椅控制系统，为行动不便的用户提供直观的功能性控制方案。

Method: 使用开源EEG数据集，采用19x200的数据段捕获手部运动起始点，采样频率200Hz。提出BiLSTM-BiGRU注意力模型，并与XGBoost、EEGNet和基于Transformer的模型进行对比。

Result: BiLSTM-BiGRU模型测试准确率达到92.26%，交叉验证平均准确率为90.13%，优于其他基线模型。

Conclusion: 注意力机制在脑机接口应用中具有巨大潜力，提出的BiLSTM-BiGRU模型为基于运动想象的轮椅控制提供了有效的解决方案。

Abstract: This paper presents an Artificial Intelligence (AI) integrated novel approach
to Brain-Computer Interface (BCI)-based wheelchair development, utilizing a
motor imagery right-left-hand movement mechanism for control. The system is
designed to simulate wheelchair navigation based on motor imagery right and
left-hand movements using electroencephalogram (EEG) data. A pre-filtered
dataset, obtained from an open-source EEG repository, was segmented into arrays
of 19x200 to capture the onset of hand movements. The data was acquired at a
sampling frequency of 200Hz. The system integrates a Tkinter-based interface
for simulating wheelchair movements, offering users a functional and intuitive
control system. We propose a BiLSTM-BiGRU model that shows a superior test
accuracy of 92.26% as compared with various machine learning baseline models,
including XGBoost, EEGNet, and a transformer-based model. The Bi-LSTM-BiGRU
attention-based model achieved a mean accuracy of 90.13% through
cross-validation, showcasing the potential of attention mechanisms in BCI
applications.

</details>


### [118] [Guiding Mixture-of-Experts with Temporal Multimodal Interactions](https://arxiv.org/abs/2509.25678)
*Xing Han,Hsing-Huan Chung,Joydeep Ghosh,Paul Pu Liang,Suchi Saria*

Main category: cs.LG

TL;DR: 提出了一种基于时间交互动态的多模态MoE路由框架，通过量化模态间的时间变化交互来指导专家路由，提升专家专业化和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构的路由机制忽略了模态间信息丰富且随时间变化的交互动态，这限制了专家专业化，模型无法显式利用内在模态关系进行有效推理。

Method: 提出多模态交互感知路由器，基于token的交互性质将其分发到专家；构建时间多模态交互动态的新公式化表示来指导专家路由；鼓励专家学习可泛化的交互处理技能而非任务特定特征。

Result: 在具有挑战性的多模态基准测试上验证了方法，展示了增强的性能和改善的可解释性；时间多模态交互揭示了跨应用的有意义模式。

Conclusion: 该框架通过利用时间交互动态改进了MoE模型的设计和性能，证明了交互感知路由在多模态学习中的有效性。

Abstract: Mixture-of-Experts (MoE) architectures have become pivotal for large-scale
multimodal models. However, their routing mechanisms typically overlook the
informative, time-varying interaction dynamics between modalities. This
limitation hinders expert specialization, as the model cannot explicitly
leverage intrinsic modality relationships for effective reasoning. To address
this, we propose a novel framework that guides MoE routing using quantified
temporal interaction. A multimodal interaction-aware router learns to dispatch
tokens to experts based on the nature of their interactions. This dynamic
routing encourages experts to acquire generalizable interaction-processing
skills rather than merely learning task-specific features. Our framework builds
on a new formulation of temporal multimodal interaction dynamics, which are
used to guide expert routing. We first demonstrate that these temporal
multimodal interactions reveal meaningful patterns across applications, and
then show how they can be leveraged to improve both the design and performance
of MoE-based models. Comprehensive experiments on challenging multimodal
benchmarks validate our approach, demonstrating both enhanced performance and
improved interpretability.

</details>


### [119] [Minimalist Explanation Generation and Circuit Discovery](https://arxiv.org/abs/2509.25686)
*Pirzada Suhail,Aditya Anand,Amit Sethi*

Main category: cs.LG

TL;DR: 提出基于激活匹配的方法为预训练图像分类器生成最小且忠实的解释，通过训练轻量级自编码器产生二进制掩码来突出决策关键区域，并结合电路读取程序来机制性解释模型内部计算。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在训练过程中学习了大量决策规则，但在高维输入空间中这些规则难以识别和解释。需要找到既保持模型决策又简洁可读的最小解释。

Method: 训练轻量级自编码器生成二进制掩码，整合多层激活对齐、输出标签一致性、稀疏性和紧凑性先验，以及鲁棒性约束。通过电路读取程序识别活跃通道并构建通道级图。

Result: 生成了最小且忠实的解释，能够突出图像中的决策关键区域，同时提供了机制性理解模型内部计算的方法。

Conclusion: 该方法在最小输入级解释和模型内部计算机制性理解之间建立了实用桥梁，有助于更好地解释预训练图像分类器的决策过程。

Abstract: Machine learning models, by virtue of training, learn a large repertoire of
decision rules for any given input, and any one of these may suffice to justify
a prediction. However, in high-dimensional input spaces, such rules are
difficult to identify and interpret. In this paper, we introduce an
activation-matching based approach to generate minimal and faithful
explanations for the decisions of pre-trained image classifiers. We aim to
identify minimal explanations that not only preserve the model's decision but
are also concise and human-readable. To achieve this, we train a lightweight
autoencoder to produce binary masks that learns to highlight the decision-wise
critical regions of an image while discarding irrelevant background. The
training objective integrates activation alignment across multiple layers,
consistency at the output label, priors that encourage sparsity, and
compactness, along with a robustness constraint that enforces faithfulness. The
minimal explanations so generated also lead us to mechanistically interpreting
the model internals. In this regard we also introduce a circuit readout
procedure wherein using the explanation's forward pass and gradients, we
identify active channels and construct a channel-level graph, scoring
inter-layer edges by ingress weight magnitude times source activation and
feature-to-class links by classifier weight magnitude times feature activation.
Together, these contributions provide a practical bridge between minimal
input-level explanations and a mechanistic understanding of the internal
computations driving model decisions.

</details>


### [120] [A Unified Probabilistic Framework for Dictionary Learning with Parsimonious Activation](https://arxiv.org/abs/2509.25690)
*Zihui Zhao,Yuanbo Tang,Jieyu Ren,Xiaoping Zhang,Yang Li*

Main category: cs.LG

TL;DR: 提出了一种基于系数矩阵行向L∞范数的字典学习正则化方法，通过促进整行系数消失来减少跨样本激活的字典原子数量，显著提升重建质量和表示稀疏性。


<details>
  <summary>Details</summary>
Motivation: 传统字典学习方法主要关注单个样本的表示稀疏性，忽略了原子在跨样本间的共享模式，导致字典冗余和次优。需要一种能够减少跨样本激活原子数量的方法。

Method: 引入基于系数矩阵行向L∞范数的正则化器，结合Beta-Bernoulli先验的概率模型，从贝叶斯角度解释正则化参数与先验分布的关系，并建立最优超参数选择的理论计算。

Result: 在基准数据集上的实验表明，该方法实现了显著改善的重建质量（RMSE降低20%），增强的表示稀疏性，仅使用不到十分之一的可用字典原子，同时验证了理论分析。

Conclusion: 该方法通过促进跨样本的原子共享模式优化，在字典学习中实现了更好的重建质量和稀疏性，为贝叶斯模型选择和路径学习提供了理论连接。

Abstract: Dictionary learning is traditionally formulated as an $L_1$-regularized
signal reconstruction problem. While recent developments have incorporated
discriminative, hierarchical, or generative structures, most approaches rely on
encouraging representation sparsity over individual samples that overlook how
atoms are shared across samples, resulting in redundant and sub-optimal
dictionaries. We introduce a parsimony promoting regularizer based on the
row-wise $L_\infty$ norm of the coefficient matrix. This additional penalty
encourages entire rows of the coefficient matrix to vanish, thereby reducing
the number of dictionary atoms activated across the dataset. We derive the
formulation from a probabilistic model with Beta-Bernoulli priors, which
provides a Bayesian interpretation linking the regularization parameters to
prior distributions. We further establish theoretical calculation for optimal
hyperparameter selection and connect our formulation to both Minimum
Description Length, Bayesian model selection and pathlet learning. Extensive
experiments on benchmark datasets demonstrate that our method achieves
substantially improved reconstruction quality (with a 20\% reduction in RMSE)
and enhanced representation sparsity, utilizing fewer than one-tenth of the
available dictionary atoms, while empirically validating our theoretical
analysis.

</details>


### [121] [Physics-Informed Learning for Human Whole-Body Kinematics Prediction via Sparse IMUs](https://arxiv.org/abs/2509.25704)
*Cheng Guo,Giuseppe L'Erario,Giulio Romualdi,Mattia Leonori,Marta Lorenzini,Arash Ajoudani,Daniele Pucci*

Main category: cs.LG

TL;DR: 提出了一种基于物理信息的学习框架，使用仅5个IMU的惯性测量数据来预测人体运动，通过整合领域知识到训练和推理过程中，实现高精度、平滑过渡的运动预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏未来预测和物理约束考虑，且过度依赖过去姿态数据，这在现实场景中并不总是可用。需要开发能够处理这些限制的实用运动预测方案。

Method: 使用物理信息学习框架，在训练时加入正向和微分运动学函数作为额外损失分量来正则化关节预测；在推理时通过迭代细化预测来更新关节状态缓冲区作为网络额外输入。

Result: 实验结果表明该方法实现了高精度、平滑的运动过渡，并且对未见过的受试者具有良好的泛化能力。

Conclusion: 该物理信息学习框架能够有效预测人体运动，解决了传统方法对过去姿态的依赖问题，同时保证了物理可行性和预测准确性。

Abstract: Accurate and physically feasible human motion prediction is crucial for safe
and seamless human-robot collaboration. While recent advancements in human
motion capture enable real-time pose estimation, the practical value of many
existing approaches is limited by the lack of future predictions and
consideration of physical constraints. Conventional motion prediction schemes
rely heavily on past poses, which are not always available in real-world
scenarios. To address these limitations, we present a physics-informed learning
framework that integrates domain knowledge into both training and inference to
predict human motion using inertial measurements from only 5 IMUs. We propose a
network that accounts for the spatial characteristics of human movements.
During training, we incorporate forward and differential kinematics functions
as additional loss components to regularize the learned joint predictions. At
the inference stage, we refine the prediction from the previous iteration to
update a joint state buffer, which is used as extra inputs to the network.
Experimental results demonstrate that our approach achieves high accuracy,
smooth transitions between motions, and generalizes well to unseen subjects

</details>


### [122] [Adaptive Graph Coarsening for Efficient GNN Training](https://arxiv.org/abs/2509.25706)
*Rostyslav Olshevskyi,Madeline Navarro,Santiago Segarra*

Main category: cs.LG

TL;DR: 提出一种自适应图粗化方法，在训练过程中联合学习图神经网络参数并通过K-means聚类合并节点，以处理大规模图数据。


<details>
  <summary>Details</summary>
Motivation: 随着现实世界图的规模不断增大，直接处理变得越来越困难甚至不可行。针对大规模数据定制算法可能会牺牲性能，因此考虑通过图缩减来减少训练期间使用的数据量。

Method: 同时训练GNN并通过基于节点嵌入的K-means聚类来粗化图，在训练过程中动态合并节点，而不是作为预处理步骤。

Result: 该方法在同类性和异类性节点分类数据集上都得到验证，可视化显示粗化图在训练过程中能够适应学习任务。

Conclusion: 提出的自适应图粗化方法能够有效处理大规模图数据，特别适用于传统方法难以处理的场景（如异类性数据）。

Abstract: We propose an adaptive graph coarsening method to jointly learn graph neural
network (GNN) parameters and merge nodes via K-means clustering during
training. As real-world graphs grow larger, processing them directly becomes
increasingly challenging and sometimes infeasible. Tailoring algorithms to
large-scale data may sacrifice performance, so we instead consider graph
reduction to decrease the amount of data used during training. In particular,
we propose a method to simultaneously train a GNN and coarsen its graph by
partitioning nodes via K-means clustering based on their embeddings. Unlike
past graph coarsening works, our approach allows us to merge nodes during
training. Not only does this preclude coarsening as a preprocessing step, but
our node clusters can adapt to the learning task instead of relying solely on
graph connectivity and features. Thus, our method is amenable to scenarios that
are challenging for other methods, such as heterophilic data. We validate our
approach on both homophilic and heterophilic node classification datasets. We
further visualize relationships between node embeddings and their corresponding
clusters to illustrate that our coarsened graph adapts to the learning task
during training.

</details>


### [123] [Expert Merging: Model Merging with Unsupervised Expert Alignment and Importance-Guided Layer Chunking](https://arxiv.org/abs/2509.25712)
*Dengming Zhang,Xiaowen Ma,Zhenliang Ni,Zhenkai Wu,Han Shu,Xin Jiang,Xinghao Chen*

Main category: cs.LG

TL;DR: 提出Expert Merging和Expert Merging++两种模型合并方法，通过层间系数学习和重要性引导的分块策略，实现无需标签、参数高效的多专家模型合并，在LLM和MLLM上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法存在局限性：无训练方法依赖手动调参，训练方法主要对齐参数而非下游任务行为，且通常忽略层间异质性。需要一种训练轻量、考虑层间差异的合并方法。

Method: Expert Merging使用未标注校准数据学习层间系数，通过隐藏状态和对数对齐优化，加入正则化和任务加权损失。Expert Merging++进一步引入重要性引导分块，基于学习系数、任务向量大小和参数数量分配分块系数。

Result: 在InternVL、Qwen2-VL和Mistral等骨干网络上，该方法超越了强基线，Expert Merging++在某些情况下甚至超过监督混合训练。

Conclusion: 提出了一种无需标签、参数高效且可扩展的多专家模型合并方法，显著提升了模型合并性能。

Abstract: Model merging, which combines multiple domain-specialized experts into a
single model, offers a practical path to endow Large Language Models (LLMs) and
Multimodal Large Language Models (MLLMs) with broad capabilities without the
cost of joint training or serving many models. However, training-free methods
rely on hand-tuned coefficients, whereas training-based methods primarily align
parameters rather than downstream task behavior and typically treat all layers
uniformly, ignoring inter-layer heterogeneity. We introduce Expert Merging, a
training-light method that learns a small set of layer-wise coefficients using
only unlabeled calibration data. The coefficients are optimized to explicitly
align the merged model's hidden states and logits with those of the
corresponding experts, with a coefficient regularizer for stability and
task-weighted losses for controllable trade-offs. To capture inter-layer
variation, Expert Merging++ augments this design with importance-guided
chunking: a normalized layer-importance metric, derived from learned
coefficients, task-vector magnitudes, and parameter counts, allocates more
chunk-wise coefficients to high-importance layers while keeping low-importance
layers lightweight. The result is a label-free, parameter-efficient, and
scalable approach to multi-expert model merging across LLMs and MLLMs. Across
MLLM backbones (InternVL and Qwen2-VL) and the LLM backbone (Mistral), our
method surpasses strong training-free and training-based merging baselines,
with Expert Merging++ delivering further gains and, in some cases, even
exceeding supervised Mixture Training. The source code is available at
https://github.com/Littleor/ExpertMerging.

</details>


### [124] [Reweighted Flow Matching via Unbalanced OT for Label-free Long-tailed Generation](https://arxiv.org/abs/2509.25713)
*Hyunsoo Song,Minjung Gim,Jaewoong Choi*

Main category: cs.LG

TL;DR: 提出了UOT-RFM框架，通过无平衡最优传输和重加权策略解决长尾分布生成中的多数类偏见问题，无需类别标签信息。


<details>
  <summary>Details</summary>
Motivation: 标准流匹配在长尾分布中存在多数类偏见，生成少数类模式时保真度低且无法匹配真实类别比例。

Method: 使用小批量无平衡最优传输构建条件向量场，通过基于目标分布与UOT边缘密度比的标签无关多数分数进行逆重加权。

Result: 理论上实现一阶修正恢复目标分布，经验上通过高阶修正改善尾部类别生成，在长尾基准上优于现有流匹配基线。

Conclusion: UOT-RFM有效缓解多数类偏见，在长尾数据集上表现优异，同时在平衡数据集上保持竞争力。

Abstract: Flow matching has recently emerged as a powerful framework for
continuous-time generative modeling. However, when applied to long-tailed
distributions, standard flow matching suffers from majority bias, producing
minority modes with low fidelity and failing to match the true class
proportions. In this work, we propose Unbalanced Optimal Transport Reweighted
Flow Matching (UOT-RFM), a novel framework for generative modeling under
class-imbalanced (long-tailed) distributions that operates without any class
label information. Our method constructs the conditional vector field using
mini-batch Unbalanced Optimal Transport (UOT) and mitigates majority bias
through a principled inverse reweighting strategy. The reweighting relies on a
label-free majority score, defined as the density ratio between the target
distribution and the UOT marginal. This score quantifies the degree of majority
based on the geometric structure of the data, without requiring class labels.
By incorporating this score into the training objective, UOT-RFM theoretically
recovers the target distribution with first-order correction ($k=1$) and
empirically improves tail-class generation through higher-order corrections ($k
> 1$). Our model outperforms existing flow matching baselines on long-tailed
benchmarks, while maintaining competitive performance on balanced datasets.

</details>


### [125] [MuPlon: Multi-Path Causal Optimization for Claim Verification through Controlling Confounding](https://arxiv.org/abs/2509.25715)
*Hanghui Guo,Shimin Di,Pasquale De Meo,Zhangze Chen,Jia Zhu*

Main category: cs.LG

TL;DR: 提出MuPlon框架，通过双因果干预策略解决声明验证中的数据噪声和偏差问题，在完全连接的声明-证据图上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 传统声明验证方法忽视证据间的复杂交互，而基于完全连接图的方法面临数据噪声和偏差两大混淆挑战。

Method: MuPlon整合后门路径和前门路径的双因果干预：后门路径通过优化节点概率权重稀释噪声节点干扰并加强相关证据连接；前门路径提取高相关子图构建推理路径，应用反事实推理消除路径内数据偏差。

Result: 实验结果表明MuPlon优于现有方法，达到最先进性能。

Conclusion: MuPlon通过因果干预有效解决了声明验证中的数据噪声和偏差问题，显著提升了验证可靠性。

Abstract: As a critical task in data quality control, claim verification aims to curb
the spread of misinformation by assessing the truthfulness of claims based on a
wide range of evidence. However, traditional methods often overlook the complex
interactions between evidence, leading to unreliable verification results. A
straightforward solution represents the claim and evidence as a fully connected
graph, which we define as the Claim-Evidence Graph (C-E Graph). Nevertheless,
claim verification methods based on fully connected graphs face two primary
confounding challenges, Data Noise and Data Biases. To address these
challenges, we propose a novel framework, Multi-Path Causal Optimization
(MuPlon). MuPlon integrates a dual causal intervention strategy, consisting of
the back-door path and front-door path. In the back-door path, MuPlon dilutes
noisy node interference by optimizing node probability weights, while
simultaneously strengthening the connections between relevant evidence nodes.
In the front-door path, MuPlon extracts highly relevant subgraphs and
constructs reasoning paths, further applying counterfactual reasoning to
eliminate data biases within these paths. The experimental results demonstrate
that MuPlon outperforms existing methods and achieves state-of-the-art
performance.

</details>


### [126] [Boundary-to-Region Supervision for Offline Safe Reinforcement Learning](https://arxiv.org/abs/2509.25727)
*Huikang Su,Dengyun Peng,Zifeng Zhuang,YuHan Liu,Qiguang Chen,Donglin Wang,Qinghe Liu*

Main category: cs.LG

TL;DR: B2R框架通过非对称条件化处理离线安全强化学习中的安全约束问题，将成本到目标重新定义为边界约束，在38个安全关键任务中35个满足安全约束，且奖励性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于序列模型的方法对回报到目标和成本到目标使用对称的输入标记条件化，忽略了它们内在的非对称性：回报到目标是灵活的性能目标，而成本到目标应代表严格的安全边界。这种对称条件化导致不可靠的约束满足。

Method: 提出Boundary-to-Region (B2R)框架，通过成本信号重新对齐实现非对称条件化。B2R将成本到目标重新定义为固定安全预算下的边界约束，统一所有可行轨迹的成本分布，同时保留奖励结构。结合旋转位置嵌入，增强安全区域内的探索。

Result: 实验结果显示，B2R在38个安全关键任务中的35个满足安全约束，同时在奖励性能上优于基线方法。

Conclusion: 这项工作揭示了对称标记条件化的局限性，并为将序列模型应用于安全强化学习建立了新的理论和实践方法。

Abstract: Offline safe reinforcement learning aims to learn policies that satisfy
predefined safety constraints from static datasets. Existing
sequence-model-based methods condition action generation on symmetric input
tokens for return-to-go and cost-to-go, neglecting their intrinsic asymmetry:
return-to-go (RTG) serves as a flexible performance target, while cost-to-go
(CTG) should represent a rigid safety boundary. This symmetric conditioning
leads to unreliable constraint satisfaction, especially when encountering
out-of-distribution cost trajectories. To address this, we propose
Boundary-to-Region (B2R), a framework that enables asymmetric conditioning
through cost signal realignment . B2R redefines CTG as a boundary constraint
under a fixed safety budget, unifying the cost distribution of all feasible
trajectories while preserving reward structures. Combined with rotary
positional embeddings , it enhances exploration within the safe region.
Experimental results show that B2R satisfies safety constraints in 35 out of 38
safety-critical tasks while achieving superior reward performance over baseline
methods. This work highlights the limitations of symmetric token conditioning
and establishes a new theoretical and practical approach for applying sequence
models to safe RL. Our code is available at https://github.com/HuikangSu/B2R.

</details>


### [127] [A Physics-Guided Probabilistic Surrogate Modeling Framework for Digital Twins of Underwater Radiated Noise](https://arxiv.org/abs/2509.25730)
*Indu Kant Deo,Akash Venkateshwaran,Rajeev K. Jaiman*

Main category: cs.LG

TL;DR: 提出了一种物理引导的概率框架来预测真实海洋环境中的三维传输损耗，用于船舶交通噪声缓解。该框架结合了物理知识和机器学习，构建了不确定性感知的数字孪生系统。


<details>
  <summary>Details</summary>
Motivation: 船舶交通是沿海水域水下辐射噪声日益增长的来源，需要实时数字孪生系统来进行操作噪声缓解。

Method: 使用稀疏变分高斯过程、深度sigma点过程和随机变分深度核学习，结合物理基础的均值函数（球面扩展和频率相关吸收），并整合了可学习的物理信息均值、卷积编码器、神经编码器和残差SVGP层。

Result: 生成了超过3000万个源-接收器对的数据集，构建了能够提供校准预测不确定性的概率数字孪生，并应用于船舶速度优化以减少对海洋哺乳动物的声学影响。

Conclusion: 该框架推进了海洋声学的不确定性感知数字孪生，展示了物理引导的机器学习如何支持可持续的海事运营。

Abstract: Ship traffic is an increasing source of underwater radiated noise in coastal
waters, motivating real-time digital twins of ocean acoustics for operational
noise mitigation. We present a physics-guided probabilistic framework to
predict three-dimensional transmission loss in realistic ocean environments. As
a case study, we consider the Salish Sea along shipping routes from the Pacific
Ocean to the Port of Vancouver. A dataset of over 30 million source-receiver
pairs was generated with a Gaussian beam solver across seasonal sound speed
profiles and one-third-octave frequency bands spanning 12.5 Hz to 8 kHz. We
first assess sparse variational Gaussian processes (SVGP) and then incorporate
physics-based mean functions combining spherical spreading with
frequency-dependent absorption. To capture nonlinear effects, we examine deep
sigma-point processes and stochastic variational deep kernel learning. The
final framework integrates four components: (i) a learnable physics-informed
mean that represents dominant propagation trends, (ii) a convolutional encoder
for bathymetry along the source-receiver track, (iii) a neural encoder for
source, receiver, and frequency coordinates, and (iv) a residual SVGP layer
that provides calibrated predictive uncertainty. This probabilistic digital
twin facilitates the construction of sound-exposure bounds and worst-case
scenarios for received levels. We further demonstrate the application of the
framework to ship speed optimization, where predicted transmission loss
combined with near-field source models provides sound exposure level estimates
for minimizing acoustic impacts on marine mammals. The proposed framework
advances uncertainty-aware digital twins for ocean acoustics and illustrates
how physics-guided machine learning can support sustainable maritime
operations.

</details>


### [128] [Less is More: Towards Simple Graph Contrastive Learning](https://arxiv.org/abs/2509.25742)
*Yanan Zhao,Feng Ji,Jingyang Dai,Jiaze Ma,Wee Peng Tay*

Main category: cs.LG

TL;DR: 提出了一种简单有效的图对比学习方法，通过聚合节点特征和图结构特征来缓解异质图上的特征噪声问题，无需复杂的数据增强或负采样。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法在异质图（连接节点通常属于不同类别）上效果有限，且依赖复杂的增强方案、编码器或负采样。本文探索是否这种复杂性在异质图场景下真正必要。

Method: 使用GCN编码器捕获结构特征，MLP编码器分离节点特征噪声，将原始节点特征和图结构作为对比学习的两个互补视图，无需数据增强或负采样。

Result: 在异质图基准测试中达到最先进结果，计算和内存开销最小，在同质图上也展现出复杂度、可扩展性和鲁棒性优势。

Conclusion: 证明了简单有效的图对比学习原则：通过聚合节点特征噪声与结构特征来提升性能，为异质图学习提供了新的思路。

Abstract: Graph Contrastive Learning (GCL) has shown strong promise for unsupervised
graph representation learning, yet its effectiveness on heterophilic graphs,
where connected nodes often belong to different classes, remains limited. Most
existing methods rely on complex augmentation schemes, intricate encoders, or
negative sampling, which raises the question of whether such complexity is
truly necessary in this challenging setting. In this work, we revisit the
foundations of supervised and unsupervised learning on graphs and uncover a
simple yet effective principle for GCL: mitigating node feature noise by
aggregating it with structural features derived from the graph topology. This
observation suggests that the original node features and the graph structure
naturally provide two complementary views for contrastive learning. Building on
this insight, we propose an embarrassingly simple GCL model that uses a GCN
encoder to capture structural features and an MLP encoder to isolate node
feature noise. Our design requires neither data augmentation nor negative
sampling, yet achieves state-of-the-art results on heterophilic benchmarks with
minimal computational and memory overhead, while also offering advantages in
homophilic graphs in terms of complexity, scalability, and robustness. We
provide theoretical justification for our approach and validate its
effectiveness through extensive experiments, including robustness evaluations
against both black-box and white-box adversarial attacks.

</details>


### [129] [Rotation Control Unlearning: Quantifying and Controlling Continuous Unlearning for LLM with The Cognitive Rotation Space](https://arxiv.org/abs/2509.25743)
*Xiang Zhang,Kun Wei,Xu Yang,Chenghao Xu,Su Yan,Cheng Deng*

Main category: cs.LG

TL;DR: 提出Rotation Control Unlearning (RCU)方法，通过旋转显著性权重和正交旋转轴正则化解决连续遗忘中的累积灾难性效用损失问题，无需保留数据集即可实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型广泛应用，其安全漏洞引起关注。现有遗忘方法依赖保留数据集且面临连续遗忘请求下的累积灾难性效用损失问题。

Method: 使用旋转显著性权重量化控制遗忘程度，设计斜对称损失构建认知旋转空间，通过正交旋转轴正则化确保连续遗忘请求的旋转方向相互垂直。

Result: 在多个数据集上的实验表明，该方法无需保留数据集即可达到最先进的性能。

Conclusion: RCU方法有效解决了连续遗忘中的累积效用损失问题，无需依赖保留数据集，在多个数据集上表现优异。

Abstract: As Large Language Models (LLMs) become increasingly prevalent, their security
vulnerabilities have already drawn attention. Machine unlearning is introduced
to seek to mitigate these risks by removing the influence of undesirable data.
However, existing methods not only rely on the retained dataset to preserve
model utility, but also suffer from cumulative catastrophic utility loss under
continuous unlearning requests. To solve this dilemma, we propose a novel
method, called Rotation Control Unlearning (RCU), which leverages the
rotational salience weight of RCU to quantify and control the unlearning degree
in the continuous unlearning process. The skew symmetric loss is designed to
construct the existence of the cognitive rotation space, where the changes of
rotational angle can simulate the continuous unlearning process. Furthermore,
we design an orthogonal rotation axes regularization to enforce mutually
perpendicular rotation directions for continuous unlearning requests,
effectively minimizing interference and addressing cumulative catastrophic
utility loss. Experiments on multiple datasets confirm that our method without
retained dataset achieves SOTA performance.

</details>


### [130] [OPPO: Accelerating PPO-based RLHF via Pipeline Overlap](https://arxiv.org/abs/2509.25762)
*Kaizhuo Yan,Yingjie Yu,Yifan Yu,Haizhong Zheng,Fan Lai*

Main category: cs.LG

TL;DR: OPPO是一个轻量级、模型无关的PPO-based RLHF框架，通过流水线重叠执行提高训练效率，加速1.8-2.8倍，GPU利用率提升1.4-2.1倍。


<details>
  <summary>Details</summary>
Motivation: 解决PPO-based RLHF训练管道中的效率问题，包括顺序多模型依赖和长尾响应长度导致的训练延迟。

Method: 引入两种新技术：1) 步骤内重叠，将上游模型输出流式传输给下游模型；2) 步骤间重叠，自适应地过度提交提示并推迟长生成。

Result: OPPO加速PPO-based RLHF训练1.8-2.8倍，GPU利用率提升1.4-2.1倍，且不影响训练收敛。

Conclusion: OPPO是一个高效、易于集成的RLHF框架，显著提升训练效率而不牺牲模型性能。

Abstract: Proximal Policy Optimization (PPO)-based reinforcement learning from human
feedback (RLHF) is a widely adopted paradigm for aligning large language models
(LLMs) with human preferences. However, its training pipeline suffers from
substantial inefficiencies due to sequential multi-model dependencies (e.g.,
reward model depends on actor outputs) and long-tail response lengths, where a
few long responses straggle the stage completion. We present OPPO, a novel,
lightweight, and model-agnostic PPO-based RLHF framework that improves training
efficiency by overlapping pipeline execution. OPPO introduces two novel
techniques: (1) Intra-step overlap, which streams upstream model outputs (e.g.,
actor model) in right-sized chunks, enabling the downstream model (e.g.,
reward) to begin prefill while the upstream continues decoding; and (2)
Inter-step overlap, which adaptively overcommits a few prompts and defers long
generations to future steps, mitigating tail latency without discarding partial
work. OPPO integrates easily with existing PPO implementations with a few lines
of code change. Extensive evaluations show that OPPO accelerates PPO-based RLHF
training by $1.8 \times-2.8 \times$ and improves GPU utilization by $1.4
\times-2.1 \times$ without compromising training convergence.

</details>


### [131] [Autonomy-Aware Clustering: When Local Decisions Supersede Global Prescriptions](https://arxiv.org/abs/2509.25775)
*Amber Srivastava,Salar Basiri,Srinivasa Salapaka*

Main category: cs.LG

TL;DR: 提出了一个自主感知聚类框架，使用强化学习和确定性退火来学习并考虑局部自主性对聚类结果的影响，无需事先了解自主性形式。


<details>
  <summary>Details</summary>
Motivation: 现有聚类方法假设实体是被动的，但现实中实体具有局部自主性，会以特征表示无法完全捕捉的方式覆盖预设关联，这会显著改变聚类结果。

Method: 结合强化学习与确定性退火，在退火早期促进探索，后期转向利用。还提出自适应距离估计网络（ADEN），这是一个基于transformer的注意力模型，学习实体与聚类代表之间的依赖关系。

Result: 即使没有显式的自主性模型，该框架也能获得接近真实情况的解（差距约3-4%），而忽略自主性会导致更大差距（约35-40%）。

Conclusion: 自主感知聚类框架能够有效捕捉数据动态，显著提高聚类准确性，证明了考虑实体自主性的重要性。

Abstract: Clustering arises in a wide range of problem formulations, yet most existing
approaches assume that the entities under clustering are passive and strictly
conform to their assigned groups. In reality, entities often exhibit local
autonomy, overriding prescribed associations in ways not fully captured by
feature representations. Such autonomy can substantially reshape clustering
outcomes -- altering cluster compositions, geometry, and cardinality -- with
significant downstream effects on inference and decision-making. We introduce
autonomy-aware clustering, a reinforcement (RL) learning framework that learns
and accounts for the influence of local autonomy without requiring prior
knowledge of its form. Our approach integrates RL with a deterministic
annealing (DA) procedure, where, to determine underlying clusters, DA naturally
promotes exploration in early stages of annealing and transitions to
exploitation later. We also show that the annealing procedure exhibits phase
transitions that enable design of efficient annealing schedules. To further
enhance adaptability, we propose the Adaptive Distance Estimation Network
(ADEN), a transformer-based attention model that learns dependencies between
entities and cluster representatives within the RL loop, accommodates
variable-sized inputs and outputs, and enables knowledge transfer across
diverse problem instances. Empirical results show that our framework closely
aligns with underlying data dynamics: even without explicit autonomy models, it
achieves solutions close to the ground truth (gap ~3-4%), whereas ignoring
autonomy leads to substantially larger gaps (~35-40%). The code and data are
publicly available at https://github.com/salar96/AutonomyAwareClustering.

</details>


### [132] [A Hamiltonian driven Geometric Construction of Neural Networks on the Lognormal Statistical Manifold](https://arxiv.org/abs/2509.25778)
*Prosper Rosaire Mama Assandje,Teumsa Aboubakar,Dongho Joseph,Takemi Nakamura*

Main category: cs.LG

TL;DR: 提出了一种在统计流形上构建神经网络的方法，以对数正态统计流形为例，通过哈密顿系统推导网络架构，包括基于SU(1,1)群作用的权重矩阵和源自辛结构的激活函数。


<details>
  <summary>Details</summary>
Motivation: 将信息几何与机器学习结合，探索在统计流形上构建内在神经网络的可能性，为学习系统提供基于微分几何的新范式。

Method: 在Poincare圆盘中嵌入哈密顿动力学坐标系统作为输入，从几何原理推导网络组件：权重矩阵的旋转部分由SU(1,1)群作用确定，激活函数源自系统的辛结构。

Result: 成功构建了对数正态流形上的神经网络架构，获得了完整的权重矩阵和输出值，证明对数正态流形可被视为神经流形。

Conclusion: 该方法为在参数空间的微分几何基础上构建学习系统提供了新的范式，几何性质决定了独特且可解释的神经网络结构。

Abstract: Bridging information geometry with machine learning, this paper presents a
method for constructing neural networks intrinsically on statistical manifolds.
We demonstrate this approach by formulating a neural network architecture
directly on the lognormal statistical manifold. The construction is driven by
the Hamiltonian system that is equivalent to the gradient flow on this
manifold. First, we define the network's input values using the coordinate
system of this Hamiltonian dynamics, naturally embedded in the Poincare disk.
The core of our contribution lies in the derivation of the network's components
from geometric principles: the rotation component of the synaptic weight matrix
is determined by the Lie group action of SU(1,1) on the disk, while the
activation function emerges from the symplectic structure of the system. We
subsequently obtain the complete weight matrix, including its translation
vector, and the resulting output values. This work shows that the lognormal
manifold can be seamlessly viewed as a neural manifold, with its geometric
properties dictating a unique and interpretable neural network structure. The
proposed method offers a new paradigm for building learning systems grounded in
the differential geometry of their underlying parameter spaces.

</details>


### [133] [From Cheap Geometry to Expensive Physics: Elevating Neural Operators via Latent Shape Pretraining](https://arxiv.org/abs/2509.25788)
*Zhizhou Zhang,Youjia Wu,Kaixuan Zhang,Yanjia Wang*

Main category: cs.LG

TL;DR: 提出一个两阶段框架，利用无物理标签的几何数据预训练自编码器，然后将学到的潜在表示用于神经算子训练，在有限标注数据下提升PDE求解预测精度。


<details>
  <summary>Details</summary>
Motivation: 工业设计评估依赖高精度PDE模拟，但计算昂贵。算子学习可加速预测，但受限于标注物理数据的稀缺。大量无物理标签的几何设计数据未被充分利用。

Method: 两阶段框架：第一阶段在几何重建任务上预训练自编码器学习潜在表示；第二阶段使用预训练的潜在嵌入作为输入，以监督方式训练神经算子预测PDE解。采用基于Transformer的架构处理点云数据。

Result: 在四个PDE数据集和三个最先进的基于Transformer的神经算子上，相比直接在原始点云输入上训练的模型，该方法一致提高了预测精度。

Conclusion: 来自无物理预训练的表征为数据高效的算子学习提供了强大基础，能够更好地利用丰富的几何数据资源。

Abstract: Industrial design evaluation often relies on high-fidelity simulations of
governing partial differential equations (PDEs). While accurate, these
simulations are computationally expensive, making dense exploration of design
spaces impractical. Operator learning has emerged as a promising approach to
accelerate PDE solution prediction; however, its effectiveness is often limited
by the scarcity of labeled physics-based data. At the same time, large numbers
of geometry-only candidate designs are readily available but remain largely
untapped. We propose a two-stage framework to better exploit this abundant,
physics-agnostic resource and improve supervised operator learning under
limited labeled data. In Stage 1, we pretrain an autoencoder on a geometry
reconstruction task to learn an expressive latent representation without PDE
labels. In Stage 2, the neural operator is trained in a standard supervised
manner to predict PDE solutions, using the pretrained latent embeddings as
inputs instead of raw point clouds. Transformer-based architectures are adopted
for both the autoencoder and the neural operator to handle point cloud data and
integrate both stages seamlessly. Across four PDE datasets and three
state-of-the-art transformer-based neural operators, our approach consistently
improves prediction accuracy compared to models trained directly on raw point
cloud inputs. These results demonstrate that representations from
physics-agnostic pretraining provide a powerful foundation for data-efficient
operator learning.

</details>


### [134] [Characterization and Learning of Causal Graphs with Latent Confounders and Post-treatment Selection from Interventional Data](https://arxiv.org/abs/2509.25800)
*Gongxu Luo,Loka Li,Guangyi Chen,Haoyue Dai,Kun Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种考虑后处理选择偏差的因果发现方法，通过引入新的因果公式和FI-Markov等价类，能够区分真实因果关系与选择模式，在存在潜在混杂因素和后处理选择的情况下恢复因果结构。


<details>
  <summary>Details</summary>
Motivation: 现有干预性因果发现方法往往忽视后处理选择偏差问题，这种偏差在生物研究中普遍存在（如基因表达分析中的质量控制），可能导致虚假依赖关系并扭曲因果发现结果。

Method: 提出新的因果公式明确建模后处理选择，引入FI-Markov等价类和F-PAG图表示，开发F-FCI算法利用观测和干预数据识别因果关系、潜在混杂因素和后处理选择。

Result: 在合成和真实数据集上的实验表明，该方法能够在存在选择和潜在混杂因素的情况下恢复因果关系。

Conclusion: 后处理选择是因果发现中的重要挑战，本文提出的方法能够有效处理这一问题，超越传统等价类限制，更接近真实因果结构。

Abstract: Interventional causal discovery seeks to identify causal relations by
leveraging distributional changes introduced by interventions, even in the
presence of latent confounders. Beyond the spurious dependencies induced by
latent confounders, we highlight a common yet often overlooked challenge in the
problem due to post-treatment selection, in which samples are selectively
included in datasets after interventions. This fundamental challenge widely
exists in biological studies; for example, in gene expression analysis, both
observational and interventional samples are retained only if they meet quality
control criteria (e.g., highly active cells). Neglecting post-treatment
selection may introduce spurious dependencies and distributional changes under
interventions, which can mimic causal responses, thereby distorting causal
discovery results and challenging existing causal formulations. To address
this, we introduce a novel causal formulation that explicitly models
post-treatment selection and reveals how its differential reactions to
interventions can distinguish causal relations from selection patterns,
allowing us to go beyond traditional equivalence classes toward the underlying
true causal structure. We then characterize its Markov properties and propose a
Fine-grained Interventional equivalence class, named FI-Markov equivalence,
represented by a new graphical diagram, F-PAG. Finally, we develop a provably
sound and complete algorithm, F-FCI, to identify causal relations, latent
confounders, and post-treatment selection up to $\mathcal{FI}$-Markov
equivalence, using both observational and interventional data. Experimental
results on synthetic and real-world datasets demonstrate that our method
recovers causal relations despite the presence of both selection and latent
confounders.

</details>


### [135] [CardioForest: An Explainable Ensemble Learning Model for Automatic Wide QRS Complex Tachycardia Diagnosis from ECG](https://arxiv.org/abs/2509.25804)
*Vaskar Chakma,Ju Xiaolin,Heling Cao,Xue Feng,Ji Xiaodong,Pan Haiyan,Gao Zhan*

Main category: cs.LG

TL;DR: 开发基于集成机器学习的框架，用于从ECG信号自动检测宽QRS波心动过速(WCT)，强调使用可解释AI的诊断准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 开发准确且可解释的WCT自动检测系统，帮助心脏病专家在紧急情况下做出及时诊断。

Method: 集成学习技术，包括优化的随机森林(CardioForest)、XGBoost和LightGBM模型，使用MIMIC-IV数据集进行训练和测试，并应用SHAP进行模型可解释性分析。

Result: CardioForest模型在所有指标上表现最佳，测试准确率达94.95%，平衡准确率88.31%，SHAP分析确认模型能根据临床直觉对ECG特征进行排名。

Conclusion: CardioForest被确认为高度可靠且可解释的WCT检测模型，能够提供准确预测和透明度，是帮助心脏病专家做出及时诊断的宝贵工具。

Abstract: This study aims to develop and evaluate an ensemble machine learning-based
framework for the automatic detection of Wide QRS Complex Tachycardia (WCT)
from ECG signals, emphasizing diagnostic accuracy and interpretability using
Explainable AI. The proposed system integrates ensemble learning techniques,
i.e., an optimized Random Forest known as CardioForest, and models like XGBoost
and LightGBM. The models were trained and tested on ECG data from the publicly
available MIMIC-IV dataset. The testing was carried out with the assistance of
accuracy, balanced accuracy, precision, recall, F1 score, ROC-AUC, and error
rate (RMSE, MAE) measures. In addition, SHAP (SHapley Additive exPlanations)
was used to ascertain model explainability and clinical relevance. The
CardioForest model performed best on all metrics, achieving a test accuracy of
94.95%, a balanced accuracy of 88.31%, and high precision and recall metrics.
SHAP analysis confirmed the model's ability to rank the most relevant ECG
features, such as QRS duration, in accordance with clinical intuitions, thereby
fostering trust and usability in clinical practice. The findings recognize
CardioForest as an extremely dependable and interpretable WCT detection model.
Being able to offer accurate predictions and transparency through
explainability makes it a valuable tool to help cardiologists make timely and
well-informed diagnoses, especially for high-stakes and emergency scenarios.

</details>


### [136] [Improving Sampling Efficiency in RLVR through Adaptive Rollout and Response Reuse](https://arxiv.org/abs/2509.25808)
*Yuheng Zhang,Wenlin Yao,Changlong Yu,Yao Liu,Qingyu Yin,Bing Yin,Hyokun Yun,Lihong Li*

Main category: cs.LG

TL;DR: AR3PO是一种采样高效的RLVR算法，通过自适应采样和响应重用技术解决GRPO中的优势消失问题，在多个基准测试中优于GRPO，与DAPO性能相当但显著降低采样成本。


<details>
  <summary>Details</summary>
Motivation: 解决GRPO算法中当组内所有响应获得相同奖励时出现的优势消失问题，提升强化学习验证奖励(RLVR)范式的训练效率和性能。

Method: 提出自适应采样(adaptive rollout)动态分配更多响应给困难提示，节省简单提示的计算；响应重用(response reuse)利用先前生成的正確响应提供有用训练信号。

Result: 在7B和8B模型上，AR3PO持续优于GRPO，与DAPO相当或更好，采样成本降低高达4.2倍；在32B模型上，AR3PO在相似训练步数下与DAPO性能相当，但采样成本显著更低。

Conclusion: AR3PO通过自适应采样和响应重用技术有效解决了GRPO的优势消失问题，在保持高性能的同时大幅降低了采样成本，为RLVR训练提供了更高效的解决方案。

Abstract: Large language models (LLMs) have achieved impressive reasoning performance,
with reinforcement learning with verifiable rewards (RLVR) emerging as a
standard paradigm for post-training. A representative algorithm, group relative
policy optimization (GRPO) (Shao et al., 2024), computes advantages by
normalizing outcome rewards within response groups, but suffers from a
vanishing advantage issue when all responses in a group receive identical
rewards. To address this issue, we propose Adaptive Rollout and Response Reuse
Policy Optimization (AR3PO), a sampling efficient RLVR algorithm that
introduces two novel techniques: adaptive rollout, which dynamically allocates
more responses to difficult prompts while saving computation on easier ones,
and response reuse, which leverages previously generated correct responses to
provide useful training signals. We compare AR3PO with strong RLVR baselines on
multiple representative benchmarks using two different families of base models.
Across the 7B and 8B models, AR3PO consistently outperforms GRPO and matches or
surpasses DAPO (Yu et al., 2025), reducing rollout cost by up to 4.2x. On the
larger 32B model, AR3PO achieves comparable performance to DAPO at similar
training steps while maintaining substantially lower rollout cost.

</details>


### [137] [Kairos: Towards Adaptive and Generalizable Time Series Foundation Models](https://arxiv.org/abs/2509.25826)
*Kun Feng,Shaocheng Lan,Yuchen Fang,Wenchao He,Lintao Ma,Xingyu Lu,Kan Ren*

Main category: cs.LG

TL;DR: Kairos是一个灵活的时间序列基础模型框架，通过动态分块标记化和实例自适应位置嵌入来解决时间序列信息密度不均的问题，在零样本基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列基础模型使用非自适应处理流程，无法捕捉时间序列中随时间变化的异质信息密度，特别是在零样本场景下面临显著建模挑战。

Method: 提出Kairos框架，集成动态分块标记化器和实例自适应位置嵌入，自适应选择标记化粒度并为每个时间序列实例定制位置编码，使用大规模可预测性分层时间序列语料库进行训练。

Result: 在GIFT-Eval和Time-Series-Library两个零样本基准测试中，Kairos以更少的参数实现了优越性能，在各种任务中持续超越现有方法。

Conclusion: Kairos通过自适应处理策略有效解决了时间序列信息密度不均的问题，为时间序列基础模型提供了更灵活的建模能力。

Abstract: Time series foundation models (TSFMs) have emerged as a powerful paradigm for
time series analysis, driven by large-scale pretraining on diverse data
corpora. However, time series inherently exhibit heterogeneous information
density over time, influenced by system states and signal complexity,
presenting significant modeling challenges especially in a zero-shot scenario.
Current TSFMs rely on non-adaptive processing pipelines that fail to capture
this dynamic nature. For example, common tokenization strategies such as
fixed-size patching enforce rigid observational granularity, limiting their
ability to adapt to varying information densities. Similarly, conventional
positional encodings impose a uniform temporal scale, making it difficult to
model diverse periodicities and trends across series. To overcome these
limitations, we propose Kairos, a flexible TSFM framework that integrates a
dynamic patching tokenizer and an instance-adaptive positional embedding.
Kairos adaptively selects tokenization granularity and tailors positional
encodings to the unique characteristics of each time series instance. Trained
on a large-scale Predictability-Stratified Time Series (PreSTS) corpus
comprising over 300 billion time points and adopting a multi-patch prediction
strategy in the inference stage, Kairos achieves superior performance with much
fewer parameters on two common zero-shot benchmarks, GIFT-Eval and the
Time-Series-Library benchmark, consistently outperforming established methods
across diverse tasks. The project page is at
https://foundation-model-research.github.io/Kairos .

</details>


### [138] [MIDAS: Misalignment-based Data Augmentation Strategy for Imbalanced Multimodal Learning](https://arxiv.org/abs/2509.25831)
*Seong-Hyeon Hwang,Soyoung Choi,Steven Euijong Whang*

Main category: cs.LG

TL;DR: MIDAS是一种数据增强策略，通过生成语义不一致的多模态错位样本，并使用弱模态加权和难样本加权机制，有效解决多模态模型中的模态不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 多模态模型往往过度依赖主导模态，无法达到最优性能。现有方法主要关注修改训练目标或优化过程，而数据中心的解决方案尚未充分探索。

Method: 提出MIDAS数据增强策略：1）生成语义不一致的跨模态错位样本；2）基于单模态置信度进行标注；3）引入弱模态加权机制，动态增加最不自信模态的损失权重；4）提出难样本加权机制，优先处理语义模糊的错位样本。

Result: 在多个多模态分类基准测试上的实验表明，MIDAS在解决模态不平衡问题上显著优于相关基线方法。

Conclusion: MIDAS通过数据增强和加权机制有效解决了多模态模型中的模态不平衡问题，提升了模型性能。

Abstract: Multimodal models often over-rely on dominant modalities, failing to achieve
optimal performance. While prior work focuses on modifying training objectives
or optimization procedures, data-centric solutions remain underexplored. We
propose MIDAS, a novel data augmentation strategy that generates misaligned
samples with semantically inconsistent cross-modal information, labeled using
unimodal confidence scores to compel learning from contradictory signals.
However, this confidence-based labeling can still favor the more confident
modality. To address this within our misaligned samples, we introduce
weak-modality weighting, which dynamically increases the loss weight of the
least confident modality, thereby helping the model fully utilize weaker
modality. Furthermore, when misaligned features exhibit greater similarity to
the aligned features, these misaligned samples pose a greater challenge,
thereby enabling the model to better distinguish between classes. To leverage
this, we propose hard-sample weighting, which prioritizes such semantically
ambiguous misaligned samples. Experiments on multiple multimodal classification
benchmarks demonstrate that MIDAS significantly outperforms related baselines
in addressing modality imbalance.

</details>


### [139] [Distillation of Large Language Models via Concrete Score Matching](https://arxiv.org/abs/2509.25837)
*Yeongmin Kim,Donghyeok Shin,Mina Kang,Byeonghu Na,Il-Chul Moon*

Main category: cs.LG

TL;DR: 提出了Concrete Score Distillation (CSD)方法，通过离散分数匹配克服了传统知识蒸馏中softmax平滑和logit平移不变性的限制，在LLM蒸馏中实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法存在两个问题：softmax会模糊有价值的logit信息，而直接logit蒸馏又无法处理logit平移不变性，限制了最优解空间。

Method: CSD通过离散分数匹配目标，对齐学生和教师模型在所有词汇对上的相对logit差异，并解决了自回归LLM中训练不稳定和二次复杂度问题。

Result: 在GPT-2-1.5B、OpenLLaMA-7B和GEMMA-7B-IT上的实验表明，CSD持续超越现有KD方法，实现更好的保真度-多样性权衡，并与策略技术结合产生互补增益。

Conclusion: CSD展示了在LLM蒸馏中的可扩展性和有效性，为高效推理提供了更好的解决方案。

Abstract: Large language models (LLMs) deliver remarkable performance but are costly to
deploy, motivating knowledge distillation (KD) for efficient inference.
Existing KD objectives typically match student and teacher probabilities via
softmax, which blurs valuable logit information. While direct logit
distillation (DLD) mitigates softmax smoothing, it fails to account for logit
shift invariance, thereby restricting the solution space. We propose Concrete
Score Distillation (CSD), a discrete score-matching objective that overcomes
both softmax-induced smoothing and restrictions on the optimal solution set. We
resolve the training instability and quadratic complexity of discrete
score-matching in autoregressive LLMs, and the resulting CSD objective aligns
relative logit differences across all vocabulary pairs between student and
teacher with flexible weighting. We provide both mode-seeking and mode-covering
instances within our framework and evaluate CSD on task-agnostic
instruction-following and task-specific distillation using GPT-2-1.5B,
OpenLLaMA-7B, and GEMMA-7B-IT. Experiments show that CSD consistently surpasses
recent KD objectives, achieves favorable fidelity-diversity trade-offs, and
yields complementary gains when combined with on-policy techniques,
demonstrating its scalability and effectiveness for LLM distillation.

</details>


### [140] [S$^2$FS: Spatially-Aware Separability-Driven Feature Selection in Fuzzy Decision Systems](https://arxiv.org/abs/2509.25841)
*Suping Xu,Chuyi Dai,Ye Liu,Lin Shang,Xibei Yang,Witold Pedrycz*

Main category: cs.LG

TL;DR: 提出了一种面向模糊决策系统的空间感知可分性特征选择方法(S²FS)，通过整合空间方向信息来提升分类性能并减少规则冗余。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法要么无法直接与学习性能对齐，要么仅依赖非方向性欧氏距离来捕捉决策类间关系，这限制了它们澄清决策边界的能力。实例的空间分布对决策边界的清晰度有潜在影响。

Method: 提出了空间感知可分性准则，同时考虑类内紧凑性和类间分离性，通过整合标量距离与空间方向信息来更全面地描述类结构。采用前向贪心策略迭代选择最具判别性的特征。

Result: 在10个真实世界数据集上的广泛实验表明，S²FS在分类准确率和聚类性能上一致优于8种最先进的特征选择算法，特征可视化进一步证实了所选特征的可解释性。

Conclusion: S²FS通过整合空间方向信息有效提升了模糊决策系统的特征选择性能，在保持可解释性的同时显著改善了分类和聚类效果。

Abstract: Feature selection is crucial for fuzzy decision systems (FDSs), as it
identifies informative features and eliminates rule redundancy, thereby
enhancing predictive performance and interpretability. Most existing methods
either fail to directly align evaluation criteria with learning performance or
rely solely on non-directional Euclidean distances to capture relationships
among decision classes, which limits their ability to clarify decision
boundaries. However, the spatial distribution of instances has a potential
impact on the clarity of such boundaries. Motivated by this, we propose
Spatially-aware Separability-driven Feature Selection (S$^2$FS), a novel
framework for FDSs guided by a spatially-aware separability criterion. This
criterion jointly considers within-class compactness and between-class
separation by integrating scalar-distances with spatial directional
information, providing a more comprehensive characterization of class
structures. S$^2$FS employs a forward greedy strategy to iteratively select the
most discriminative features. Extensive experiments on ten real-world datasets
demonstrate that S$^2$FS consistently outperforms eight state-of-the-art
feature selection algorithms in both classification accuracy and clustering
performance, while feature visualizations further confirm the interpretability
of the selected features.

</details>


### [141] [Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation](https://arxiv.org/abs/2509.25849)
*Ziniu Li,Congliang Chen,Tianyun Yang,Tian Ding,Ruoyu Sun,Ge Zhang,Wenhao Huang,Zhi-Quan Luo*

Main category: cs.LG

TL;DR: 本文提出了一种基于背包问题的最优探索预算分配方法，用于解决LLM在强化学习自改进过程中探索预算均匀分配导致的梯度消失问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在强化学习自改进过程中采用均匀探索预算分配，导致简单任务总是成功、困难任务总是失败，两者在GRPO训练中产生零梯度，影响学习效率。

Method: 将每个任务的探索视为具有不同"价值"和"成本"的"物品"，建立与经典背包问题的联系，推导出基于模型当前学习状态的自适应资源分配规则。

Result: 在GRPO中，该方法将非零策略梯度的有效比例提高了20-40%，可为特别困难的问题分配更大预算（如93次rollout），在数学推理基准上平均提升2-4分，特定任务峰值提升9分。

Conclusion: 该方法作为计算"免费午餐"，能够将学习饱和任务的探索预算重新分配到最有影响力的任务上，相比传统均匀分配方法，达到相同性能只需约一半的计算资源。

Abstract: Large Language Models (LLMs) can self-improve through reinforcement learning,
where they generate trajectories to explore and discover better solutions.
However, this exploration process is computationally expensive, often forcing
current methods to assign limited exploration budgets to each task. This
uniform allocation creates problematic edge cases: easy tasks consistently
succeed while difficult tasks consistently fail, both producing zero gradients
during training updates for the widely used Group Relative Policy Optimization
(GRPO). We address this problem from the lens of exploration budget allocation.
Viewing each task's exploration as an "item" with a distinct "value" and
"cost", we establish a connection to the classical knapsack problem. This
formulation allows us to derive an optimal assignment rule that adaptively
distributes resources based on the model's current learning status. When
applied to GRPO, our method increases the effective ratio of non-zero policy
gradients by 20-40% during training. Acting as a computational "free lunch",
our approach could reallocate exploration budgets from tasks where learning is
saturated to those where it is most impactful. This enables significantly
larger budgets (e.g., 93 rollouts) for especially challenging problems, which
would be computationally prohibitive under a uniform allocation. These
improvements translate to meaningful gains on mathematical reasoning
benchmarks, with average improvements of 2-4 points and peak gains of 9 points
on specific tasks. Notably, achieving comparable performance with traditional
homogeneous allocation would require about 2x the computational resources.

</details>


### [142] [RL-Guided Data Selection for Language Model Finetuning](https://arxiv.org/abs/2509.25850)
*Animesh Jha,Harshit Gupta,Ananjan Nandi*

Main category: cs.LG

TL;DR: 提出基于强化学习的数据选择方法，在仅使用5%数据的情况下，性能达到或超过全数据集微调，同时训练时间减少2倍


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型微调中的数据选择问题，将其建模为预算约束下的优化问题，现有方法在预训练场景有效但微调场景效果不佳

Method: 将数据选择问题重新表述为可处理的马尔可夫决策过程，使用各种强化学习方法训练智能体学习最优数据选择策略，基于代理模型的高效奖励信号

Result: 在四个数据集上，使用5%数据子集微调的性能达到或超过全数据集微调，准确率提升高达10.8个百分点，训练时间减少2倍

Conclusion: 强化学习指导的数据选择方法具有显著潜力，能够在大幅减少训练数据的同时保持或提升模型性能

Abstract: Data selection for finetuning Large Language Models (LLMs) can be framed as a
budget-constrained optimization problem: maximizing a model's downstream
performance under a strict training data budget. Solving this problem is
generally intractable, and existing approximate approaches are
pretraining-oriented and transfer poorly to the fine-tuning setting. We
reformulate this problem as a tractable Markov Decision Process (MDP) and train
agents using various Reinforcement Learning (RL) methods to learn optimal data
selection policies, guided by an efficient, proxy-model-based reward signal.
Across four datasets, training on a $5\%$ subset selected by our approach
matches or outperforms fine-tuning on the full dataset by up to $10.8$ accuracy
points, while cutting wall-clock training time by up to $2 \times$,
highlighting the promise of RL-guided data selection.

</details>


### [143] [Efficient On-Policy Reinforcement Learning via Exploration of Sparse Parameter Space](https://arxiv.org/abs/2509.25876)
*Xinyu Zhang,Aishik Deb,Klaus Mueller*

Main category: cs.LG

TL;DR: ExploRLer是一个可插拔的管道，通过系统性地探索策略梯度方法中未探索的参数空间邻域，显著提升了PPO和TRPO等策略梯度算法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的策略梯度方法如PPO通常沿着单一随机梯度方向更新，忽略了参数空间的丰富局部结构。研究发现代理梯度与真实奖励地形相关性差，附近未探索区域可能存在更高性能的解。

Method: 提出ExploRLer管道，与PPO和TRPO等在线策略算法无缝集成，系统性地探索代理在线策略梯度更新周围的未探索邻域，而不增加梯度更新次数。

Result: 在复杂的连续控制环境中，ExploRLer相比基线方法取得了显著改进，证明了迭代级探索的有效性。

Conclusion: 迭代级探索为强化在线策略强化学习提供了一种实用有效的方法，并为代理目标函数的局限性提供了新的视角。

Abstract: Policy-gradient methods such as Proximal Policy Optimization (PPO) are
typically updated along a single stochastic gradient direction, leaving the
rich local structure of the parameter space unexplored. Previous work has shown
that the surrogate gradient is often poorly correlated with the true reward
landscape. Building on this insight, we visualize the parameter space spanned
by policy checkpoints within an iteration and reveal that higher performing
solutions often lie in nearby unexplored regions. To exploit this opportunity,
we introduce ExploRLer, a pluggable pipeline that seamlessly integrates with
on-policy algorithms such as PPO and TRPO, systematically probing the
unexplored neighborhoods of surrogate on-policy gradient updates. Without
increasing the number of gradient updates, ExploRLer achieves significant
improvements over baselines in complex continuous control environments. Our
results demonstrate that iteration-level exploration provides a practical and
effective way to strengthen on-policy reinforcement learning and offer a fresh
perspective on the limitations of the surrogate objective.

</details>


### [144] [Federated Learning with Enhanced Privacy via Model Splitting and Random Client Participation](https://arxiv.org/abs/2509.25906)
*Yiwei Li,Shuai Wang,Zhuojun Tian,Xiuhua Wang,Shijian Su*

Main category: cs.LG

TL;DR: 提出MS-PAFL框架，通过模型分割和隐私放大技术，在联邦学习中实现更好的隐私-效用权衡，减少差分隐私噪声对模型精度的影响。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中差分隐私保护会因添加噪声而严重降低模型精度，需要解决隐私保护与模型效用之间的平衡问题。

Method: 将客户端模型分割为私有子模型（本地保留）和公共子模型（全局聚合），仅在公共子模型注入校准高斯噪声，结合随机客户端参与和本地数据子采样实现隐私放大。

Result: 理论分析表明MS-PAFL显著减少了满足目标隐私保护水平所需的噪声量，实验验证其在强隐私保证下能训练出高精度模型。

Conclusion: MS-PAFL通过模型分割和隐私放大实现了优越的隐私-效用权衡，为联邦学习中的隐私保护提供了有效解决方案。

Abstract: Federated Learning (FL) often adopts differential privacy (DP) to protect
client data, but the added noise required for privacy guarantees can
substantially degrade model accuracy. To resolve this challenge, we propose
model-splitting privacy-amplified federated learning (MS-PAFL), a novel
framework that combines structural model splitting with statistical privacy
amplification. In this framework, each client's model is partitioned into a
private submodel, retained locally, and a public submodel, shared for global
aggregation. The calibrated Gaussian noise is injected only into the public
submodel, thereby confining its adverse impact while preserving the utility of
the local model. We further present a rigorous theoretical analysis that
characterizes the joint privacy amplification achieved through random client
participation and local data subsampling under this architecture. The analysis
provides tight bounds on both single-round and total privacy loss,
demonstrating that MS-PAFL significantly reduces the noise necessary to satisfy
a target privacy protection level. Extensive experiments validate our
theoretical findings, showing that MS-PAFL consistently attains a superior
privacy-utility trade-off and enables the training of highly accurate models
under strong privacy guarantees.

</details>


### [145] [ReNF: Rethinking the Design Space of Neural Long-Term Time Series Forecasters](https://arxiv.org/abs/2509.25914)
*Yihang Lu,Xianwei Meng,Enhong Chen*

Main category: cs.LG

TL;DR: 该论文提出了一种基于第一性原理的长期时间序列预测新范式，通过结合自回归和直接输出方法的优势，使用简单的MLP模型实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前神经预测器在长期时间序列预测领域过度关注架构复杂性，而忽视了基本的预测原则，阻碍了该领域的进展。

Method: 提出了多重神经预测定理作为理论基础，设计了增强直接输出策略，结合了自回归和直接输出的优势，并通过平滑跟踪模型参数来稳定学习过程。

Result: 实验表明这些基于原则的改进使得简单的MLP模型在几乎所有情况下都优于近期复杂的模型，达到了最先进的性能。

Conclusion: 论文验证了提出的定理，建立了动态性能边界，并为未来研究指明了有前景的方向。

Abstract: Neural Forecasters (NFs) are a cornerstone of Long-term Time Series
Forecasting (LTSF). However, progress has been hampered by an overemphasis on
architectural complexity at the expense of fundamental forecasting principles.
In this work, we return to first principles to redesign the LTSF paradigm. We
begin by introducing a Multiple Neural Forecasting Theorem that provides a
theoretical basis for our approach. We propose Boosted Direct Output (BDO), a
novel forecasting strategy that synergistically combines the advantages of both
Auto-Regressive (AR) and Direct Output (DO). In addition, we stabilize the
learning process by smoothly tracking the model's parameters. Extensive
experiments show that these principled improvements enable a simple MLP to
achieve state-of-the-art performance, outperforming recent, complex models in
nearly all cases, without any specific considerations in the area. Finally, we
empirically verify our theorem, establishing a dynamic performance bound and
identifying promising directions for future research. The code for review is
available at: .

</details>


### [146] [From MNIST to ImageNet: Understanding the Scalability Boundaries of Differentiable Logic Gate Networks](https://arxiv.org/abs/2509.25933)
*Sven Brändle,Till Aczel,Andreas Plesner,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 本文研究了可微分逻辑门网络（DLGNs）在大规模多类别数据集上的表现，探讨了其表达能力、可扩展性以及替代输出策略，重点关注温度调谐对输出层性能的影响。


<details>
  <summary>Details</summary>
Motivation: DLGNs作为一种快速节能的神经网络替代方案，目前主要在小规模数据集（最多10个类别）上测试，需要研究其在大规模多类别分类任务中的表现和可扩展性。

Method: 使用合成和真实世界数据集，评估DLGNs在不同输出策略下的表现，特别关注温度调谐对Group-Sum层性能的影响。

Result: 研究提供了关于温度调谐重要性的关键见解，评估了Group-Sum层在不同条件下的表现，并成功应用于多达2000个类别的大规模分类任务。

Conclusion: DLGNs在大规模多类别分类中具有良好潜力，温度调谐是影响输出层性能的关键因素，Group-Sum层在适当条件下能够有效处理大规模分类任务。

Abstract: Differentiable Logic Gate Networks (DLGNs) are a very fast and
energy-efficient alternative to conventional feed-forward networks. With
learnable combinations of logical gates, DLGNs enable fast inference by
hardware-friendly execution. Since the concept of DLGNs has only recently
gained attention, these networks are still in their developmental infancy,
including the design and scalability of their output layer. To date, this
architecture has primarily been tested on datasets with up to ten classes.
  This work examines the behavior of DLGNs on large multi-class datasets. We
investigate its general expressiveness, its scalability, and evaluate
alternative output strategies. Using both synthetic and real-world datasets, we
provide key insights into the importance of temperature tuning and its impact
on output layer performance. We evaluate conditions under which the Group-Sum
layer performs well and how it can be applied to large-scale classification of
up to 2000 classes.

</details>


### [147] [AIM: Adaptive Intervention for Deep Multi-task Learning of Molecular Properties](https://arxiv.org/abs/2509.25955)
*Mason Minot,Gisbert Schneider*

Main category: cs.LG

TL;DR: 提出AIM优化框架，通过动态策略调解多任务学习中的梯度冲突，在数据稀缺的药物发现场景中显著提升性能并提供任务关系诊断能力


<details>
  <summary>Details</summary>
Motivation: 解决多任务学习中常见的破坏性梯度干扰问题，特别是在数据稀缺的药物发现场景中，提升多属性分子设计的效率和鲁棒性

Method: 提出AIM优化框架，联合训练动态策略网络和主网络，使用包含密集可微分正则化器的新增目标函数，生成几何稳定且动态高效的梯度更新

Result: 在QM9和靶向蛋白降解剂基准测试的子集上，AIM相比多任务基线取得统计显著改进，在数据稀缺场景中优势最明显

Conclusion: AIM结合了数据高效性能和诊断洞察能力，展示了自适应优化器在加速科学发现方面的潜力，为多属性分子设计提供更鲁棒和可解释的模型

Abstract: Simultaneously optimizing multiple, frequently conflicting, molecular
properties is a key bottleneck in the development of novel therapeutics.
Although a promising approach, the efficacy of multi-task learning is often
compromised by destructive gradient interference, especially in the data-scarce
regimes common to drug discovery. To address this, we propose AIM, an
optimization framework that learns a dynamic policy to mediate gradient
conflicts. The policy is trained jointly with the main network using a novel
augmented objective composed of dense, differentiable regularizers. This
objective guides the policy to produce updates that are geometrically stable
and dynamically efficient, prioritizing progress on the most challenging tasks.
We demonstrate that AIM achieves statistically significant improvements over
multi-task baselines on subsets of the QM9 and targeted protein degraders
benchmarks, with its advantage being most pronounced in data-scarce regimes.
Beyond performance, AIM's key contribution is its interpretability; the learned
policy matrix serves as a diagnostic tool for analyzing inter-task
relationships. This combination of data-efficient performance and diagnostic
insight highlights the potential of adaptive optimizers to accelerate
scientific discovery by creating more robust and insightful models for
multi-property molecular design.

</details>


### [148] [Reevaluating Convolutional Neural Networks for Spectral Analysis: A Focus on Raman Spectroscopy](https://arxiv.org/abs/2509.25964)
*Deniz Soysal,Xabier García-Andrade,Laura E. Rodriguez,Pablo Sobron,Laura M. Barge,Renaud Detry*

Main category: cs.LG

TL;DR: 该论文提出了一个用于自主拉曼光谱分类的端到端深度学习工作流，包含基线无关分类、池化控制鲁棒性、标签高效学习和恒定时间适应四个关键进展。


<details>
  <summary>Details</summary>
Motivation: 解决火星车、深海着陆器和现场机器人等自主设备中拉曼光谱分析面临的挑战，包括荧光基线失真、峰位移和有限地面真值标签等问题。

Method: 使用一维卷积神经网络处理原始光谱，通过调整池化参数实现鲁棒性，采用半监督生成对抗网络和对比预训练提高标签效率，并通过冻结CNN主干仅重训练softmax层实现快速适应。

Result: 紧凑CNN在基线无关分类中优于传统方法，池化调参可容忍30 cm-1的拉曼位移，半监督方法在仅10%标签下提升准确率11%，恒定时间适应在新矿物分类中优于Siamese网络。

Conclusion: 该工作流为自主探索中的鲁棒、低足迹拉曼分类提供了实用路径，包括原始光谱训练、池化调参、半监督学习和轻量微调等步骤。

Abstract: Autonomous Raman instruments on Mars rovers, deep-sea landers, and field
robots must interpret raw spectra distorted by fluorescence baselines, peak
shifts, and limited ground-truth labels. Using curated subsets of the RRUFF
database, we evaluate one-dimensional convolutional neural networks (CNNs) and
report four advances: (i) Baseline-independent classification: compact CNNs
surpass $k$-nearest-neighbors and support-vector machines on handcrafted
features, removing background-correction and peak-picking stages while ensuring
reproducibility through released data splits and scripts. (ii)
Pooling-controlled robustness: tuning a single pooling parameter accommodates
Raman shifts up to $30 \,\mathrm{cm}^{-1}$, balancing translational invariance
with spectral resolution. (iii) Label-efficient learning: semi-supervised
generative adversarial networks and contrastive pretraining raise accuracy by
up to $11\%$ with only $10\%$ labels, valuable for autonomous deployments with
scarce annotation. (iv) Constant-time adaptation: freezing the CNN backbone and
retraining only the softmax layer transfers models to unseen minerals at
$\mathcal{O}(1)$ cost, outperforming Siamese networks on resource-limited
processors. This workflow, which involves training on raw spectra, tuning
pooling, adding semi-supervision when labels are scarce, and fine-tuning
lightly for new targets, provides a practical path toward robust, low-footprint
Raman classification in autonomous exploration.

</details>


### [149] [Data-Free Continual Learning of Server Models in Model-Heterogeneous Federated learning](https://arxiv.org/abs/2509.25977)
*Xiao Zhang,Zengzhe Chen,Yuan Yuan,Yifei Zou,Fuzhen Zhuang,Wenyu Jiao,Yuke Wang,Dongxiao Yu*

Main category: cs.LG

TL;DR: FedDCL是一个在模型异构联邦学习环境中实现服务器模型数据自由持续学习的新框架，利用扩散模型提取轻量级类别原型来解决数据异构、模型异构和灾难性遗忘等问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习面临数据异构、模型异构、灾难性遗忘和知识不对齐等挑战，特别是在新数据不断涌现和模型多样化的动态环境中。

Method: 使用预训练扩散模型提取轻量级类别特定原型，实现三方面的数据自由优势：生成当前任务的合成数据、无示例生成回放、以及从异构客户端到服务器的数据自由动态知识转移。

Result: 在多个数据集上的实验结果表明FedDCL的有效性，展示了其在动态环境中增强联邦学习泛化能力和实际应用潜力的能力。

Conclusion: FedDCL框架通过数据自由持续学习成功解决了联邦学习中的关键挑战，为动态环境下的联邦学习提供了可行的解决方案。

Abstract: Federated learning (FL) is a distributed learning paradigm across multiple
entities while preserving data privacy. However, with the continuous emergence
of new data and increasing model diversity, traditional federated learning
faces significant challenges, including inherent issues of data heterogeneity,
model heterogeneity and catastrophic forgetting, along with new challenge of
knowledge misalignment. In this study, we introduce FedDCL, a novel framework
designed to enable data-free continual learning of the server model in a
model-heterogeneous federated setting. We leverage pre-trained diffusion models
to extract lightweight class-specific prototypes, which confer a threefold
data-free advantage, enabling: (1) generation of synthetic data for the current
task to augment training and counteract non-IID data distributions; (2)
exemplar-free generative replay for retaining knowledge from previous tasks;
and (3) data-free dynamic knowledge transfer from heterogeneous clients to the
server. Experimental results on various datasets demonstrate the effectiveness
of FedDCL, showcasing its potential to enhance the generalizability and
practical applicability of federated learning in dynamic settings.

</details>


### [150] [Reconcile Certified Robustness and Accuracy for DNN-based Smoothed Majority Vote Classifier](https://arxiv.org/abs/2509.25979)
*Gaojie Jin,Xinping Yi,Xiaowei Huang*

Main category: cs.LG

TL;DR: 该研究在PAC-Bayesian框架下，为平滑多数投票分类器开发了具有认证鲁棒半径的泛化误差界，并提出了一种新颖的光谱正则化方法来提升认证鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏关于多数投票分类器认证鲁棒性及其与泛化性能交互作用的理论研究，特别是在PAC-Bayesian框架下。

Method: 开发了平滑多数投票分类器的泛化误差界，发现权重谱范数在泛化界和认证鲁棒半径中的关键作用，提出基于球面高斯输入维度无关特性的光谱正则化方法。

Result: 理论分析表明泛化界在认证鲁棒半径内的任何数据扰动下都成立，实验结果表明所提方法能有效提升认证鲁棒性。

Conclusion: 该研究填补了多数投票分类器认证鲁棒性理论研究的空白，提出的光谱正则化方法能有效增强平滑多数投票分类器的认证鲁棒性能。

Abstract: Within the PAC-Bayesian framework, the Gibbs classifier (defined on a
posterior $Q$) and the corresponding $Q$-weighted majority vote classifier are
commonly used to analyze the generalization performance. However, there exists
a notable lack in theoretical research exploring the certified robustness of
majority vote classifier and its interplay with generalization. In this study,
we develop a generalization error bound that possesses a certified robust
radius for the smoothed majority vote classifier (i.e., the $Q$-weighted
majority vote classifier with smoothed inputs); In other words, the
generalization bound holds under any data perturbation within the certified
robust radius. As a byproduct, we find that the underpinnings of both the
generalization bound and the certified robust radius draw, in part, upon weight
spectral norm, which thereby inspires the adoption of spectral regularization
in smooth training to boost certified robustness. Utilizing the
dimension-independent property of spherical Gaussian inputs in smooth training,
we propose a novel and inexpensive spectral regularizer to enhance the smoothed
majority vote classifier. In addition to the theoretical contribution, a set of
empirical results is provided to substantiate the effectiveness of our proposed
method.

</details>


### [151] [Exact Solutions to the Quantum Schrödinger Bridge Problem](https://arxiv.org/abs/2509.25980)
*Mykola Bordyuh,Djork-Arné Clevert,Marco Bertolini*

Main category: cs.LG

TL;DR: 本文从拉格朗日视角重新表述量子薛定谔桥问题，推导出涉及玻姆势的演化方程，获得了高斯分布间的闭式解，并基于此开发了高斯混合模型算法应用于生成建模。


<details>
  <summary>Details</summary>
Motivation: 量子薛定谔桥问题在数学文献中已有研究，但本文旨在从拉格朗日视角重新表述，使其更适合生成建模应用，探索量子非定域性在随机过程中的体现。

Method: 通过求解福克-普朗克方程和哈密顿-雅可比方程，推导出高斯分布间量子薛定谔桥问题的精确闭式解，发现解为高斯过程但协方差演化受量子效应影响。

Result: 获得了高斯分布间量子薛定谔桥问题的闭式解，开发了基于高斯混合模型的改进算法，并在单细胞演化数据、图像生成、分子翻译和平均场博弈等实验中验证了有效性。

Conclusion: 量子薛定谔桥问题与经典薛定谔桥问题不同，其解虽然也是高斯过程，但协方差演化受量子效应影响，这种量子非定域性为生成建模提供了新的可能性。

Abstract: The Quantum Schr\"odinger Bridge Problem (QSBP) describes the evolution of a
stochastic process between two arbitrary probability distributions, where the
dynamics are governed by the Schr\"odinger equation rather than by the
traditional real-valued wave equation. Although the QSBP is known in the
mathematical literature, we formulate it here from a Lagrangian perspective and
derive its main features in a way that is particularly suited to generative
modeling. We show that the resulting evolution equations involve the so-called
Bohm (quantum) potential, representing a notion of non-locality in the
stochastic process. This distinguishes the QSBP from classical stochastic
dynamics and reflects a key characteristic typical of quantum mechanical
systems. In this work, we derive exact closed-form solutions for the QSBP
between Gaussian distributions. Our derivation is based on solving the
Fokker-Planck Equation (FPE) and the Hamilton-Jacobi Equation (HJE) arising
from the Lagrangian formulation of dynamical Optimal Transport. We find that,
similar to the classical Schr\"odinger Bridge Problem, the solution to the QSBP
between Gaussians is again a Gaussian process; however, the evolution of the
covariance differs due to quantum effects. Leveraging these explicit solutions,
we present a modified algorithm based on a Gaussian Mixture Model framework,
and demonstrate its effectiveness across several experimental settings,
including single-cell evolution data, image generation, molecular translation
and applications in Mean-Field Games.

</details>


### [152] [CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models](https://arxiv.org/abs/2509.25996)
*Weiyu Huang,Yuezhou Hu,Jun Zhu,Jianfei Chen*

Main category: cs.LG

TL;DR: CAST是一个完全连续可微的稀疏感知训练框架，用于半结构化稀疏模型，通过联合优化稀疏模式和权重，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 稀疏感知训练能减少大语言模型推理时的延迟和内存消耗，但现有方法在稀疏模式和权重优化上存在分离问题，需要更高效的联合优化方案。

Method: CAST包含三个核心组件：AdamS优化器（自适应L1衰减）、权重缩放模块（防止幅度衰减）、知识蒸馏（使用稠密模型作为自教师）。

Result: 在2:4稀疏模式下，从125M到13B参数的多个模型上都优于现有方法，LLaMA2-7B稀疏模型仅用2%预训练token就达到接近稠密模型的性能。

Conclusion: CAST框架能高效训练半结构化稀疏模型，建立了可预测性能的缩放规律，并在量化和微调场景中验证了实用性。

Abstract: Sparsity-aware training is an effective approach for transforming large
language models (LLMs) into hardware-friendly sparse patterns, thereby reducing
latency and memory consumption during inference. In this paper, we propose
Continuous Adaptive Sparse Trainer (CAST), a fully continuous and
differentiable sparsity-aware training framework for semi-structured (or "N:M")
sparse models. Unlike previous approaches that optimize sparsity patterns and
weights separately, CAST enables seamless joint optimization during training,
while progressively transforming the model into the desired sparsity format.
Specifically, CAST introduces three key components: 1) AdamS, a sparsity-aware
optimizer that leverages adaptive L1 decay to promote uniform sparsification
across all parameters; 2) Weight Scaling, a module designed to mitigate the
magnitude reduction caused by decay while preserving desired sparsity patterns;
3) Knowledge Distillation, which employs the dense model as a self-teacher to
enhance training efficiency. We evaluate CAST under 2:4 sparsity patterns
across multiple model families, ranging from 125M to 13B parameters. Our
results demonstrate significant improvements over previous state-of-the-art
methods in both perplexity and zero-shot accuracy with minimal training
resources. Notably, on LLaMA2-7B, our 2:4 sparse model achieves a negligible
perplexity increase of 0.09 and a 0.36% gain in zero-shot accuracy compared to
the dense model using only 2% of the original pretraining tokens. Additionally,
we establish an accurate and robust empirical scaling law to predict sparse
model performance given adequate training resources. Finally, we demonstrate
the practical applicability of our sparse models by evaluating them under
quantization and fine-tuning scenarios.

</details>


### [153] [Indirect Attention: Turning Context Misalignment into a Feature](https://arxiv.org/abs/2509.26015)
*Bissmella Bahaduri,Hicham Talaoubrid,Fangchen Feng,Zuheng Ming,Anissa Mokraoui*

Main category: cs.LG

TL;DR: 本文分析了注意力机制在键值对来自不同序列或模态时的表现，发现上下文不对齐会引入超过临界阈值的噪声，从而损害标准注意力机制的效果。为此提出了间接注意力机制，通过间接推断相关性来更好地处理不对齐情况。


<details>
  <summary>Details</summary>
Motivation: 研究注意力机制在键值对来自不同序列或模态时的表现，特别是分析上下文不对齐对注意力机制的影响，并开发能够处理这种不对齐情况的改进机制。

Method: 首先分析注意力机制在噪声值特征下的行为，建立临界噪声阈值；然后将上下文不对齐建模为值特征中的结构化噪声；最后提出间接注意力机制，通过间接推断相关性来处理不对齐场景。

Result: 实验表明上下文不对齐引入的噪声会显著超过临界阈值，损害标准注意力机制的效果；间接注意力机制在合成任务和实际应用中都能更好地处理不对齐情况。

Conclusion: 上下文不对齐会严重损害标准注意力机制的性能，而间接注意力机制提供了一种有效的解决方案，能够在键值对来自不同序列或模态时保持更好的性能。

Abstract: The attention mechanism has become a cornerstone of modern deep learning
architectures, where keys and values are typically derived from the same
underlying sequence or representation. This work explores a less conventional
scenario, when keys and values originate from different sequences or
modalities. Specifically, we first analyze the attention mechanism's behavior
under noisy value features, establishing a critical noise threshold beyond
which signal degradation becomes significant. Furthermore, we model context
(key, value) misalignment as an effective form of structured noise within the
value features, demonstrating that the noise induced by such misalignment can
substantially exceed this critical threshold, thereby compromising standard
attention's efficacy. Motivated by this, we introduce Indirect Attention, a
modified attention mechanism that infers relevance indirectly in scenarios with
misaligned context. We evaluate the performance of Indirect Attention across a
range of synthetic tasks and real world applications, showcasing its superior
ability to handle misalignment.

</details>


### [154] [FITS: Towards an AI-Driven Fashion Information Tool for Sustainability](https://arxiv.org/abs/2509.26017)
*Daphne Theodorakopoulos,Elisabeth Eberling,Miriam Bodenheimer,Sabine Loos,Frederic Stahl*

Main category: cs.LG

TL;DR: 开发了基于Transformer的时尚可持续性信息工具FITS，通过微调BERT模型从可信文本中提取和分类可持续性信息，以解决时尚行业可持续性信息稀缺和难以理解的问题。


<details>
  <summary>Details</summary>
Motivation: 时尚行业缺乏可信且易于理解的可持续性信息，通用语言模型存在知识不足和幻觉问题，在需要事实准确性的领域尤其有害。

Method: 使用多个基于BERT的语言模型（包括科学和气候特定数据预训练的模型），在精心策划的语料库上进行微调，采用领域特定分类模式，并通过贝叶斯优化调整超参数。

Result: 开发了FITS原型系统，能够从非政府组织报告和科学出版物中提取和分类可持续性信息，并通过焦点小组评估了可用性、视觉设计、内容清晰度和潜在用例。

Conclusion: 领域适应的NLP在促进明智决策方面具有价值，AI应用在应对气候相关挑战方面具有更广泛的潜力，同时提供了可持续纺织品语料库数据集和未来更新的方法。

Abstract: Access to credible sustainability information in the fashion industry remains
limited and challenging to interpret, despite growing public and regulatory
demands for transparency. General-purpose language models often lack
domain-specific knowledge and tend to "hallucinate", which is particularly
harmful for fields where factual correctness is crucial. This work explores how
Natural Language Processing (NLP) techniques can be applied to classify
sustainability data for fashion brands, thereby addressing the scarcity of
credible and accessible information in this domain. We present a prototype
Fashion Information Tool for Sustainability (FITS), a transformer-based system
that extracts and classifies sustainability information from credible,
unstructured text sources: NGO reports and scientific publications. Several
BERT-based language models, including models pretrained on scientific and
climate-specific data, are fine-tuned on our curated corpus using a
domain-specific classification schema, with hyperparameters optimized via
Bayesian optimization. FITS allows users to search for relevant data, analyze
their own data, and explore the information via an interactive interface. We
evaluated FITS in two focus groups of potential users concerning usability,
visual design, content clarity, possible use cases, and desired features. Our
results highlight the value of domain-adapted NLP in promoting informed
decision-making and emphasize the broader potential of AI applications in
addressing climate-related challenges. Finally, this work provides a valuable
dataset, the SustainableTextileCorpus, along with a methodology for future
updates. Code available at https://github.com/daphne12345/FITS

</details>


### [155] [Muon Outperforms Adam in Tail-End Associative Memory Learning](https://arxiv.org/abs/2509.26030)
*Shuche Wang,Fengzhuo Zhang,Jiaxiang Li,Cunxiao Du,Chao Du,Tianyu Pang,Zhuoran Yang,Mingyi Hong,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: Muon优化器在训练大语言模型时比Adam更快，其成功机制源于对关联记忆参数（VO注意力权重和FFN）的优化，在处理重尾数据时能更有效地学习尾部类别。


<details>
  <summary>Details</summary>
Motivation: Muon优化器在训练LLMs时比Adam表现更好，但其成功机制尚不明确，本文旨在通过关联记忆的视角揭示这一机制。

Method: 通过消融实验分析Muon优化的transformer组件，从关联记忆角度解释Muon在重尾数据上的优势，并进行理论分析验证。

Result: 发现Muon的更新规则产生更各向同性的奇异谱，在重尾数据上能更有效地优化尾部类别，理论分析证实Muon能实现跨类别的平衡学习。

Conclusion: Muon的核心优势在于其更新规则与线性关联记忆的外积结构对齐，在重尾分布中比Adam能更平衡有效地学习尾部类别。

Abstract: The Muon optimizer is consistently faster than Adam in training Large
Language Models (LLMs), yet the mechanism underlying its success remains
unclear. This paper demystifies this mechanism through the lens of associative
memory. By ablating the transformer components optimized by Muon, we reveal
that the associative memory parameters of LLMs, namely the Value and Output
(VO) attention weights and Feed-Forward Networks (FFNs), are the primary
contributors to Muon's superiority. Motivated by this associative memory view,
we then explain Muon's superiority on real-world corpora, which are
intrinsically heavy-tailed: a few classes (tail classes) appear far less
frequently than others. The superiority is explained through two key
properties: (i) its update rule consistently yields a more isotropic singular
spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes
tail classes more effectively than Adam. Beyond empirical evidence, we
theoretically confirm these findings by analyzing a one-layer associative
memory model under class-imbalanced data. We prove that Muon consistently
achieves balanced learning across classes regardless of feature embeddings,
whereas Adam can induce large disparities in learning errors depending on
embedding properties. In summary, our empirical observations and theoretical
analyses reveal Muon's core advantage: its update rule aligns with the
outer-product structure of linear associative memories, enabling more balanced
and effective learning of tail classes in heavy-tailed distributions than Adam.

</details>


### [156] [Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification](https://arxiv.org/abs/2509.26032)
*Xiaobao Wang,Ruoxiao Sun,Yujun Zhang,Bingdao Feng,Dongxiao He,Luzhi Wang,Di Jin*

Main category: cs.LG

TL;DR: DPSBA是一个针对图神经网络分类任务的清洁标签后门攻击框架，通过对抗训练学习分布内触发器，有效抑制结构和语义异常，在保持高攻击成功率的同时显著提升隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有图分类后门攻击方法存在两个主要异常源：稀有子图触发器导致的结构偏差和标签翻转引起的语义偏差，这使得中毒图容易被异常检测模型发现。

Method: 提出DPSBA框架，通过异常感知判别器指导的对抗训练来学习分布内触发器，同时抑制结构和语义异常。

Result: 在真实数据集上的大量实验验证，DPSBA在有效性和可检测性之间实现了优于现有基线的平衡。

Conclusion: DPSBA通过抑制结构和语义异常，实现了高攻击成功率和显著提升的隐蔽性，为图神经网络后门攻击提供了更隐蔽的解决方案。

Abstract: Graph Neural Networks (GNNs) have demonstrated strong performance across
tasks such as node classification, link prediction, and graph classification,
but remain vulnerable to backdoor attacks that implant imperceptible triggers
during training to control predictions. While node-level attacks exploit local
message passing, graph-level attacks face the harder challenge of manipulating
global representations while maintaining stealth. We identify two main sources
of anomaly in existing graph classification backdoor methods: structural
deviation from rare subgraph triggers and semantic deviation caused by label
flipping, both of which make poisoned graphs easily detectable by anomaly
detection models. To address this, we propose DPSBA, a clean-label backdoor
framework that learns in-distribution triggers via adversarial training guided
by anomaly-aware discriminators. DPSBA effectively suppresses both structural
and semantic anomalies, achieving high attack success while significantly
improving stealth. Extensive experiments on real-world datasets validate that
DPSBA achieves a superior balance between effectiveness and detectability
compared to state-of-the-art baselines.

</details>


### [157] [Scaling Up Temporal Domain Generalization via Temporal Experts Averaging](https://arxiv.org/abs/2509.26045)
*Aoming Liu,Kevin Miller,Venkatesh Saligrama,Kate Saenko,Boqing Gong,Ser-Nam Lim,Bryan A. Plummer*

Main category: cs.LG

TL;DR: 提出了Temporal Experts Averaging (TEA)框架，通过权重平均更新整个模型来解决时序域泛化问题，相比仅预测分类器层的方法，在保持计算效率的同时显著提升泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有时序域泛化方法主要预测未来模型权重，但全模型预测计算成本过高，而仅预测分类器层的方法无法调整其他模型组件，限制了泛化能力。

Method: 1) 在保持参数相似性的前提下，通过在单个时序域上微调领域无关的基础模型来创建具有功能多样性的专家模型；2) 在主成分子空间中建模时序权重轨迹，通过自适应平均系数优化偏差-方差权衡。

Result: 在7个时序域泛化基准测试、5种模型和2种时序域泛化设置上的实验表明，TEA比现有方法性能提升高达69%，同时计算效率提升高达60倍。

Conclusion: TEA框架通过权重平均有效解决了时序域泛化问题，在保持计算效率的同时显著提升了模型对未来域的泛化能力。

Abstract: Temporal Domain Generalization (TDG) aims to generalize across temporal
distribution shifts, e.g., lexical change over time. Prior work often addresses
this by predicting future model weights. However, full model prediction is
prohibitively expensive for even reasonably sized models. Thus, recent methods
only predict the classifier layer, limiting generalization by failing to adjust
other model components. To address this, we propose Temporal Experts Averaging
(TEA), a novel and scalable TDG framework that updates the entire model using
weight averaging to maximize generalization potential while minimizing
computational costs. Our theoretical analysis guides us to two steps that
enhance generalization to future domains. First, we create expert models with
functional diversity yet parameter similarity by fine-tuning a domain-agnostic
base model on individual temporal domains while constraining weight changes.
Second, we optimize the bias-variance tradeoff through adaptive averaging
coefficients derived from modeling temporal weight trajectories in a principal
component subspace. Expert's contributions are based on their projected
proximity to future domains. Extensive experiments across 7 TDG benchmarks, 5
models, and 2 TDG settings shows TEA outperforms prior TDG methods by up to 69%
while being up to 60x more efficient.

</details>


### [158] [Real-time Noise Detection and Classification in Single-Channel EEG: A Lightweight Machine Learning Approach for EMG, White Noise, and EOG Artifacts](https://arxiv.org/abs/2509.26058)
*Hossein Enshaei,Pariya Jebreili,Sayed Mahmoud Sakahei*

Main category: cs.LG

TL;DR: 提出了一种混合频谱-时间框架，用于单通道EEG中眼电、肌电和白噪声伪迹的实时检测与分类，通过结合时域低通滤波和频域功率谱密度分析，使用PCA优化的特征融合和轻量级MLP架构，在低信噪比下达到99%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决真实场景中EEG伪迹检测面临的挑战：多通道方法计算效率低、对同时噪声鲁棒性差、深度学习模型在准确性和复杂性之间存在权衡。

Method: 结合时域低通滤波（针对低频眼电伪迹）和频域功率谱密度分析（捕获宽带肌电伪迹），然后进行PCA优化的特征融合以减少冗余，最后使用轻量级多层感知机进行分类。

Result: 在低信噪比（-7dB）下达到99%准确率，中等噪声（4dB）下>90%准确率；对于同时多源污染（眼电+肌电+白噪声）保持96%分类准确率；训练时间仅30秒，比CNN快97%。

Conclusion: 该框架在临床适用性和计算效率之间架起桥梁，支持可穿戴脑机接口的实时使用；通过领域知识的特征融合在噪声场景中超越了复杂架构，挑战了模型深度对EEG伪迹检测的普遍依赖。

Abstract: Electroencephalogram (EEG) artifact detection in real-world settings faces
significant challenges such as computational inefficiency in multi-channel
methods, poor robustness to simultaneous noise, and trade-offs between accuracy
and complexity in deep learning models. We propose a hybrid spectral-temporal
framework for real-time detection and classification of ocular (EOG), muscular
(EMG), and white noise artifacts in single-channel EEG. This method, in
contrast to other approaches, combines time-domain low-pass filtering
(targeting low-frequency EOG) and frequency-domain power spectral density (PSD)
analysis (capturing broad-spectrum EMG), followed by PCA-optimized feature
fusion to minimize redundancy while preserving discriminative information. This
feature engineering strategy allows a lightweight multi-layer perceptron (MLP)
architecture to outperform advanced CNNs and RNNs by achieving 99% accuracy at
low SNRs (SNR -7) dB and >90% accuracy in moderate noise (SNR 4 dB).
Additionally, this framework addresses the unexplored problem of simultaneous
multi-source contamination(EMG+EOG+white noise), where it maintains 96%
classification accuracy despite overlapping artifacts. With 30-second training
times (97% faster than CNNs) and robust performance across SNR levels, this
framework bridges the gap between clinical applicability and computational
efficiency, which enables real-time use in wearable brain-computer interfaces.
This work also challenges the ubiquitous dependence on model depth for EEG
artifact detection by demonstrating that domain-informed feature fusion
surpasses complex architecture in noisy scenarios.

</details>


### [159] [Clip-Low Increases Entropy and Clip-High Decreases Entropy in Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2509.26114)
*Jaesung R. Park,Junsu Kim,Gyeongman Kim,Jinyoung Jo,Sean Choi,Jaewoong Cho,Ernest K. Ryu*

Main category: cs.LG

TL;DR: 本文揭示了PPO和GRPO中的裁剪机制对熵产生偏差：clip-low增加熵，clip-high减少熵。在标准参数下，clip-high效应占主导，导致熵减少。通过调整clip-low值可以控制熵，防止RLVR训练中的熵崩溃。


<details>
  <summary>Details</summary>
Motivation: RLVR方法在增强大语言模型推理能力方面表现突出，但容易出现熵崩溃问题，即模型快速收敛到近乎确定性的形式，阻碍了长期RL训练中的探索和进展。

Method: 通过理论和实证分析，研究PPO和GRPO中裁剪机制对熵的影响，特别是clip-low和clip-high的不同效应。

Result: 研究发现clip-low增加熵，clip-high减少熵，在标准参数下clip-high效应占主导，导致整体熵减少。调整clip-low值可以有效控制熵水平。

Conclusion: 裁剪机制是RLVR中被忽视的混杂因素，独立于奖励信号影响熵，进而影响推理行为。通过调整clip-low值可以增加熵、促进探索，防止熵崩溃。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has recently emerged as
the leading approach for enhancing the reasoning capabilities of large language
models (LLMs). However, RLVR is prone to entropy collapse, where the LLM
quickly converges to a near-deterministic form, hindering exploration and
progress during prolonged RL training. In this work, we reveal that the
clipping mechanism in PPO and GRPO induces biases on entropy. Through
theoretical and empirical analyses, we show that clip-low increases entropy,
while clip-high decreases it. Further, under standard clipping parameters, the
effect of clip-high dominates, resulting in an overall entropy reduction even
when purely random rewards are provided to the RL algorithm. Our findings
highlight an overlooked confounding factor in RLVR: independent of the reward
signal, the clipping mechanism influences entropy, which in turn affects the
reasoning behavior. Furthermore, our analysis demonstrates that clipping can be
deliberately used to control entropy. Specifically, with a more aggressive
clip-low value, one can increase entropy, promote exploration, and ultimately
prevent entropy collapse in RLVR training.

</details>


### [160] [UncertainGen: Uncertainty-Aware Representations of DNA Sequences for Metagenomic Binning](https://arxiv.org/abs/2509.26116)
*Abdulkadir Celikkanat,Andres R. Masegosa,Mads Albertsen,Thomas D. Nielsen*

Main category: cs.LG

TL;DR: UncertainGen是首个用于宏基因组分箱的概率嵌入方法，将DNA片段表示为潜在空间中的概率分布，解决了确定性表示无法捕捉DNA序列不确定性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的宏基因组分箱方法依赖确定性表示（如k-mer谱或大语言模型嵌入），无法捕捉DNA序列中固有的不确定性，这些不确定性源于物种间DNA共享和具有高度相似表示的片段。

Method: 提出概率嵌入方法UncertainGen，将每个DNA片段表示为潜在空间中的概率分布，引入数据自适应度量来扩展可行潜在空间，实现更灵活的分箱/聚类分离。

Result: 在真实宏基因组数据集上的实验表明，该方法在分箱任务上优于确定性k-mer和基于LLM的嵌入方法，为大规模宏基因组分析提供了可扩展的轻量级解决方案。

Conclusion: 概率嵌入框架通过建模序列级不确定性并提供嵌入可区分性的理论保证，显著提升了宏基因组分箱的性能。

Abstract: Metagenomic binning aims to cluster DNA fragments from mixed microbial
samples into their respective genomes, a critical step for downstream analyses
of microbial communities. Existing methods rely on deterministic
representations, such as k-mer profiles or embeddings from large language
models, which fail to capture the uncertainty inherent in DNA sequences arising
from inter-species DNA sharing and from fragments with highly similar
representations. We present the first probabilistic embedding approach,
UncertainGen, for metagenomic binning, representing each DNA fragment as a
probability distribution in latent space. Our approach naturally models
sequence-level uncertainty, and we provide theoretical guarantees on embedding
distinguishability. This probabilistic embedding framework expands the feasible
latent space by introducing a data-adaptive metric, which in turn enables more
flexible separation of bins/clusters. Experiments on real metagenomic datasets
demonstrate the improvements over deterministic k-mer and LLM-based embeddings
for the binning task by offering a scalable and lightweight solution for
large-scale metagenomic analysis.

</details>


### [161] [Domain-Aware Hyperdimensional Computing for Edge Smart Manufacturing](https://arxiv.org/abs/2509.26131)
*Fardin Jalil Piran,Anandkumar Patel,Rajiv Malhotra,Farhad Imani*

Main category: cs.LG

TL;DR: 研究发现超维计算(HDC)的超参数与性能关系在不同应用中不稳定，通过分析信号质量监控和图像缺陷检测任务，揭示了编码器类型、投影方差、维度等参数对性能的影响，并开发出在边缘设备上比深度学习模型快6倍、训练能耗低40倍的优化方案。


<details>
  <summary>Details</summary>
Motivation: 智能制造需要满足严格延迟和能耗预算的设备端智能，超维计算(HDC)提供轻量级替代方案，但先前研究假设HDC超参数与性能关系在不同应用中稳定，这一假设需要验证。

Method: 分析两个代表性任务：CNC加工中的信号质量监控和LPBF中的图像缺陷检测，研究编码器类型、投影方差、超维维度等参数如何影响精度、推理延迟、训练时间和能耗，建立形式化复杂度模型解释可预测趋势。

Result: 信号任务偏好非线性随机傅里叶特征编码，图像任务偏好线性随机投影编码；优化后的HDC模型在精度上匹配或超过最先进的深度学习和Transformer模型，同时提供至少6倍更快的推理速度和超过40倍更低的训练能耗。

Conclusion: 领域感知的HDC编码是必要的，调优后的HDC为受限硬件上的实时工业AI提供了实用、可扩展的路径，未来工作将实现自适应编码器和超参数选择，扩展评估到更多制造模式，并在低功耗加速器上验证。

Abstract: Smart manufacturing requires on-device intelligence that meets strict latency
and energy budgets. HyperDimensional Computing (HDC) offers a lightweight
alternative by encoding data as high-dimensional hypervectors and computing
with simple operations. Prior studies often assume that the qualitative
relation between HDC hyperparameters and performance is stable across
applications. Our analysis of two representative tasks, signal-based quality
monitoring in Computer Numerical Control (CNC) machining and image-based defect
detection in Laser Powder Bed Fusion (LPBF), shows that this assumption does
not hold. We map how encoder type, projection variance, hypervector
dimensionality, and data regime shape accuracy, inference latency, training
time, and training energy. A formal complexity model explains predictable
trends in encoding and similarity computation and reveals nonmonotonic
interactions with retraining that preclude a closed-form optimum. Empirically,
signals favor nonlinear Random Fourier Features with more exclusive encodings
and saturate in accuracy beyond moderate dimensionality. Images favor linear
Random Projection, achieve high accuracy with small dimensionality, and depend
more on sample count than on dimensionality. Guided by these insights, we tune
HDC under multiobjective constraints that reflect edge deployment and obtain
models that match or exceed the accuracy of state-of-the-art deep learning and
Transformer models while delivering at least 6x faster inference and more than
40x lower training energy. These results demonstrate that domain-aware HDC
encoding is necessary and that tuned HDC offers a practical, scalable path to
real-time industrial AI on constrained hardware. Future work will enable
adaptive encoder and hyperparameter selection, expand evaluation to additional
manufacturing modalities, and validate on low-power accelerators.

</details>


### [162] [Accelerating Transformers in Online RL](https://arxiv.org/abs/2509.26137)
*Daniil Zelezetsky,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: 提出了一种使用Accelerator策略作为transformer训练器的两阶段方法，通过在离线阶段进行行为克隆预训练，然后在在线阶段进行完全在线训练，解决了transformer在模型无关在线强化学习中的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: transformer模型在强化学习中的应用带来了新的可能性，但在模型无关在线强化学习中存在实现挑战和不稳定性问题，现有学习算法难以直接应用。

Method: 两阶段算法：第一阶段使用更简单稳定的Accelerator模型独立与环境交互，同时通过行为克隆训练transformer；第二阶段预训练后的transformer在完全在线设置中与环境交互。

Result: 在ManiSkill和MuJoCo环境上的实验表明，该方法不仅实现了transformer的稳定训练，还将图像环境训练时间减少最多一半，并将离线策略方法所需的回放缓冲区大小降至1-2万，显著降低计算需求。

Conclusion: 所提出的模型无关算法加速了transformer的性能提升，使其能够以更稳定和快速的方式进行在线训练，同时大幅降低计算资源需求。

Abstract: The appearance of transformer-based models in Reinforcement Learning (RL) has
expanded the horizons of possibilities in robotics tasks, but it has
simultaneously brought a wide range of challenges during its implementation,
especially in model-free online RL. Some of the existing learning algorithms
cannot be easily implemented with transformer-based models due to the
instability of the latter. In this paper, we propose a method that uses the
Accelerator policy as a transformer's trainer. The Accelerator, a simpler and
more stable model, interacts with the environment independently while
simultaneously training the transformer through behavior cloning during the
first stage of the proposed algorithm. In the second stage, the pretrained
transformer starts to interact with the environment in a fully online setting.
As a result, this model-free algorithm accelerates the transformer in terms of
its performance and helps it to train online in a more stable and faster way.
By conducting experiments on both state-based and image-based ManiSkill
environments, as well as on MuJoCo tasks in MDP and POMDP settings, we show
that applying our algorithm not only enables stable training of transformers
but also reduces training time on image-based environments by up to a factor of
two. Moreover, it decreases the required replay buffer size in off-policy
methods to 10-20 thousand, which significantly lowers the overall computational
demands.

</details>


### [163] [Leveraging AI modelling for FDS with Simvue: monitor and optimise for more sustainable simulations](https://arxiv.org/abs/2509.26139)
*James Panayis,Matt Field,Vignesh Gopakumar,Andrew Lahiff,Kristian Zarebski,Aby Abraham,Jonathan L. Hodges*

Main category: cs.LG

TL;DR: 提出了一个多管齐下的方法来提高火灾模拟的效率和规模，包括机器学习替代模型、引导优化过程和Simvue框架，显著减少了模拟时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 火灾模拟在规模和数量上都有很高需求，需要改进模拟的时间和能源消耗来满足这些需求。

Method: 使用定制机器学习替代模型预测热传播动态；采用引导优化程序减少所需模拟数量；开发Simvue框架提供自动组织和跟踪功能。

Result: 机器学习模型比最先进的CFD软件快几个数量级；引导优化将定位建筑物中最危险火灾位置的模拟次数减少十倍；Simvue框架实现了数据重用和更好的模拟管理。

Conclusion: 该方法显著提高了火灾模拟的效率，通过机器学习、优化算法和系统框架的结合，实现了时间和能源的大幅节约。

Abstract: There is high demand on fire simulations, in both scale and quantity. We
present a multi-pronged approach to improving the time and energy required to
meet these demands. We show the ability of a custom machine learning surrogate
model to predict the dynamics of heat propagation orders of magnitude faster
than state-of-the-art CFD software for this application. We also demonstrate
how a guided optimisation procedure can decrease the number of simulations
required to meet an objective; using lightweight models to decide which
simulations to run, we see a tenfold reduction when locating the most dangerous
location for a fire to occur within a building based on the impact of smoke on
visibility. Finally we present a framework and product, Simvue, through which
we access these tools along with a host of automatic organisational and
tracking features which enables future reuse of data and more savings through
better management of simulations and combating redundancy.

</details>


### [164] [Alignment-Aware Decoding](https://arxiv.org/abs/2509.26169)
*Frédéric Berdoz,Luca A. Lanzendörfer,René Caky,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出了一种名为对齐感知解码（AAD）的新方法，在推理阶段直接增强模型对齐，无需额外训练，在多个对齐基准测试中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的对齐仍然是自然语言处理中的核心挑战，现有的偏好优化方法通常需要训练时或基于提示的干预，而AAD旨在在推理阶段直接提升模型对齐效果。

Method: AAD是一种在推理时直接增强模型对齐的方法，理论可解释为隐式奖励优化，但只需要标准的DPO设置，无需专门训练。

Result: AAD在多样化的对齐基准测试和不同模型规模上持续优于强基线方法，在数据受限的情况下，AAD可以生成高质量的合成数据来改进标准解码下的对齐效果。

Conclusion: AAD提供了一种实用的解决方案，特别是在标注数据有限的情况下，能够在推理阶段有效提升模型对齐性能。

Abstract: Alignment of large language models remains a central challenge in natural
language processing. Preference optimization has emerged as a popular and
effective method for improving alignment, typically through training-time or
prompt-based interventions. In this paper, we introduce alignment-aware
decoding (AAD), a method to enhance model alignment directly at inference.
Theoretically, AAD can be interpreted as implicit reward optimization, yet it
requires no specialized training beyond the standard DPO setup. Empirically,
AAD consistently outperforms strong baselines across diverse alignment
benchmarks and model scales. Moreover, in data-constrained settings, AAD can
produce high-quality synthetic data to improve alignment under standard
decoding, providing a practical solution when labeled data is limited.

</details>


### [165] [Neighbor-aware informal settlement mapping with graph convolutional networks](https://arxiv.org/abs/2509.26171)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Christovam Barcellos,Nadine Dessay*

Main category: cs.LG

TL;DR: 提出基于图卷积网络的框架，通过整合相邻空间单元的地理上下文信息来改进非正规住区分类，在里约热内卢案例中显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法将空间单元独立处理，忽略了城市结构的关联性，需要更好地整合地理上下文信息来改进非正规住区映射。

Method: 构建图结构将每个空间单元与其相邻单元连接，使用轻量级图卷积网络(GCN)进行分类，通过空间交叉验证确保模型泛化能力。

Result: 在里约热内卢五个不同区域的实验中，该方法显著优于基线，Kappa系数比单个单元分类提高了17个百分点，且优于简单的邻域特征拼接方法。

Conclusion: 图结构建模能有效编码空间结构，提升城市场景理解能力，为非正规住区检测提供了更鲁棒和可泛化的解决方案。

Abstract: Mapping informal settlements is crucial for addressing challenges related to
urban planning, public health, and infrastructure in rapidly growing cities.
Geospatial machine learning has emerged as a key tool for detecting and mapping
these areas from remote sensing data. However, existing approaches often treat
spatial units independently, neglecting the relational structure of the urban
fabric. We propose a graph-based framework that explicitly incorporates local
geographical context into the classification process. Each spatial unit (cell)
is embedded in a graph structure along with its adjacent neighbors, and a
lightweight Graph Convolutional Network (GCN) is trained to classify whether
the central cell belongs to an informal settlement. Experiments are conducted
on a case study in Rio de Janeiro using spatial cross-validation across five
distinct zones, ensuring robustness and generalizability across heterogeneous
urban landscapes. Our method outperforms standard baselines, improving Kappa
coefficient by 17 points over individual cell classification. We also show that
graph-based modeling surpasses simple feature concatenation of neighboring
cells, demonstrating the benefit of encoding spatial structure for urban scene
understanding.

</details>


### [166] [PDE Solvers Should Be Local: Fast, Stable Rollouts with Learned Local Stencils](https://arxiv.org/abs/2509.26186)
*Chun-Wun Cheng,Bin Dong,Carola-Bibiane Schönlieb,Angelica I Aviles-Rivero*

Main category: cs.LG

TL;DR: FINO是一种有限差分启发的神经架构，通过严格局部性和可学习时间步进方案解决PDE，在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子模型依赖全局混合机制（如谱卷积或注意力），容易过度平滑局部动态且计算成本高，需要一种既能保持局部细节又高效的PDE求解方法。

Method: FINO用可学习卷积核替代固定有限差分系数，通过局部算子块（包含差分模板层、门控掩码和线性融合步骤）构建自适应类导数局部特征，采用编码器-解码器结构。

Result: 在六个基准测试和气候建模任务中，FINO相比最先进的算子学习方法实现了高达44%的误差降低和约2倍的加速。

Conclusion: 严格局部性与可学习时间步进相结合为神经PDE求解器提供了准确且可扩展的基础。

Abstract: Neural operator models for solving partial differential equations (PDEs)
often rely on global mixing mechanisms-such as spectral convolutions or
attention-which tend to oversmooth sharp local dynamics and introduce high
computational cost. We present FINO, a finite-difference-inspired neural
architecture that enforces strict locality while retaining multiscale
representational power. FINO replaces fixed finite-difference stencil
coefficients with learnable convolutional kernels and evolves states via an
explicit, learnable time-stepping scheme. A central Local Operator Block
leverage a differential stencil layer, a gating mask, and a linear fuse step to
construct adaptive derivative-like local features that propagate forward in
time. Embedded in an encoder-decoder with a bottleneck, FINO captures
fine-grained local structures while preserving interpretability. We establish
(i) a composition error bound linking one-step approximation error to stable
long-horizon rollouts under a Lipschitz condition, and (ii) a universal
approximation theorem for discrete time-stepped PDE dynamics. (iii) Across six
benchmarks and a climate modelling task, FINO achieves up to 44\% lower error
and up to around 2\times speedups over state-of-the-art operator-learning
baselines, demonstrating that strict locality with learnable time-stepping
yields an accurate and scalable foundation for neural PDE solvers.

</details>


### [167] [Optimizing Indoor Environmental Quality in Smart Buildings Using Deep Learning](https://arxiv.org/abs/2509.26187)
*Youssef Sabiri,Walid Houmaidi,Aaya Bougrine,Salmane El Mansour Billah*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的室内环境质量主动管理方法，使用LSTM、GRU和CNN-LSTM三种架构预测CO2浓度、温度和湿度，在平衡建筑能效的同时优化室内环境质量。


<details>
  <summary>Details</summary>
Motivation: 传统HVAC系统在确保室内环境质量时能耗高，需要一种能平衡能效和舒适度的智能管理方法。

Method: 利用ROBOD数据集，比较LSTM、GRU和CNN-LSTM三种深度学习架构在不同时间跨度上预测IEQ变量的性能。

Result: GRU在短期预测中精度最高且计算开销小，CNN-LSTM在长期预测中特征提取能力更强，LSTM在长时序建模方面表现稳健。预测可靠性受数据分辨率、传感器位置和人员流动影响。

Conclusion: 研究结果为智能建筑管理系统实施预测性HVAC控制提供了可行方案，有助于降低能耗并提升实际建筑运营中的居住舒适度。

Abstract: Ensuring optimal Indoor Environmental Quality (IEQ) is vital for occupant
health and productivity, yet it often comes at a high energy cost in
conventional Heating, Ventilation, and Air Conditioning (HVAC) systems. This
paper proposes a deep learning driven approach to proactively manage IEQ
parameters specifically CO2 concentration, temperature, and humidity while
balancing building energy efficiency. Leveraging the ROBOD dataset collected
from a net-zero energy academic building, we benchmark three
architectures--Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), and
a hybrid Convolutional Neural Network LSTM (CNN-LSTM)--to forecast IEQ
variables across various time horizons. Our results show that GRU achieves the
best short-term prediction accuracy with lower computational overhead, whereas
CNN-LSTM excels in extracting dominant features for extended forecasting
windows. Meanwhile, LSTM offers robust long-range temporal modeling. The
comparative analysis highlights that prediction reliability depends on data
resolution, sensor placement, and fluctuating occupancy conditions. These
findings provide actionable insights for intelligent Building Management
Systems (BMS) to implement predictive HVAC control, thereby reducing energy
consumption and enhancing occupant comfort in real-world building operations.

</details>


### [168] [Marginal Flow: a flexible and efficient framework for density estimation](https://arxiv.org/abs/2509.26221)
*Marcello Massimo Negri,Jonathan Aellen,Manuel Jahn,AmirEhsan Khorashadizadeh,Volker Roth*

Main category: cs.LG

TL;DR: 提出Marginal Flow框架，通过参数化分布和可学习的潜在参数分布来克服现有密度建模方法的局限性，实现高效训练和推理。


<details>
  <summary>Details</summary>
Motivation: 现有密度建模方法存在训练昂贵、推理缓慢、近似似然、模式崩溃或架构约束等问题，需要一种能同时克服这些局限性的新方法。

Method: 定义模型q_θ(x)通过参数化分布q(x|w)和潜在参数w，通过从可学习分布q_θ(w)中采样w来边缘化潜在变量，实现高效密度评估和采样。

Result: 提出的模型允许精确密度评估，在训练和推理上都比竞争模型快几个数量级，且在合成数据集、基于模拟的推理、正定矩阵分布和图像潜在空间流形学习等任务上表现优异。

Conclusion: Marginal Flow是一个灵活框架，不受神经网络架构限制，能学习低维流形上的分布，可高效训练多种目标函数，并能轻松处理多模态目标分布。

Abstract: Current density modeling approaches suffer from at least one of the following
shortcomings: expensive training, slow inference, approximate likelihood, mode
collapse or architectural constraints like bijective mappings. We propose a
simple yet powerful framework that overcomes these limitations altogether. We
define our model $q_\theta(x)$ through a parametric distribution $q(x|w)$ with
latent parameters $w$. Instead of directly optimizing the latent variables $w$,
our idea is to marginalize them out by sampling $w$ from a learnable
distribution $q_\theta(w)$, hence the name Marginal Flow. In order to evaluate
the learned density $q_\theta(x)$ or to sample from it, we only need to draw
samples from $q_\theta(w)$, which makes both operations efficient. The proposed
model allows for exact density evaluation and is orders of magnitude faster
than competing models both at training and inference. Furthermore, Marginal
Flow is a flexible framework: it does not impose any restrictions on the neural
network architecture, it enables learning distributions on lower-dimensional
manifolds (either known or to be learned), it can be trained efficiently with
any objective (e.g. forward and reverse KL divergence), and it easily handles
multi-modal targets. We evaluate Marginal Flow extensively on various tasks
including synthetic datasets, simulation-based inference, distributions on
positive definite matrices and manifold learning in latent spaces of images.

</details>


### [169] [Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners](https://arxiv.org/abs/2509.26226)
*Xin Xu,Cliveb AI,Kai Yang,Tianhao Chen,Yang Wang,Saiyong Yang,Can Yang*

Main category: cs.LG

TL;DR: TFPI是一种简单有效的RLVR改进方法，通过ThinkFree操作丢弃思维内容来减少推理时的token使用，提高性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: RLVR虽然能有效解决复杂任务，但训练时需要极长的上下文长度，导致巨大的计算成本。多阶段训练只能部分缓解这个问题，而且过短的初始上下文会导致不可逆的性能下降。

Method: 提出TFPI方法，通过简单的ThinkFree操作（直接附加</think>标记来丢弃思维内容）来减少推理时的token使用。在训练中使用ThinkFree适配的输入，即使在原始慢思考模式下也能提高性能和降低token消耗。

Result: TFPI加速了RL收敛，达到了更高的性能上限，并产生了更具token效率的推理模型。仅使用TFPI，4B模型在AIME24上达到89.0%准确率，在LiveCodeBench上达到65.5%准确率，使用不到4K H20小时。

Conclusion: TFPI是一种简单而有效的方法，能够在不需要专门奖励或复杂训练设计的情况下，显著提高RLVR的性能和效率。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) effectively solves
complex tasks but demands extremely long context lengths during training,
leading to substantial computational costs. While multi-stage training can
partially mitigate this, starting with overly short contexts often causes
irreversible performance degradation, ultimately failing to reduce overall
training compute significantly. In this paper, we introduce
**T**hinking-**F**ree **P**olicy **I**nitialization (**TFPI**), a simple yet
effective adaptation to RLVR that bridges long Chain-of-Thought (CoT)
distillation and standard RLVR. TFPI employs a simple *ThinkFree* operation,
explicitly discarding the thinking content via a direct *</think>* append, to
reduce token usage during inference. Training with *ThinkFree*-adapted inputs
improves performance and lowers token consumption, even in the original
slow-thinking mode. Extensive experiments across various benchmarks have shown
that TFPI accelerates RL convergence, achieves a higher performance ceiling,
and yields more token-efficient reasoning models without specialized rewards or
complex training designs. With TFPI only, we train a 4B model to reach 89.0%
accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.

</details>


### [170] [Beyond Linear Probes: Dynamic Safety Monitoring for Language Models](https://arxiv.org/abs/2509.26238)
*James Oldfield,Philip Torr,Ioannis Patras,Adel Bibi,Fazl Barez*

Main category: cs.LG

TL;DR: 提出截断多项式分类器(TPCs)，一种用于动态激活监控的灵活安全监测方法，可根据输入难度和计算资源动态调整计算成本


<details>
  <summary>Details</summary>
Motivation: 传统安全监测器对所有查询使用相同计算量，导致资源浪费或漏检风险。需要一种成本随输入难度和可用计算资源灵活调整的安全监测方法

Method: TPCs是线性探针的自然扩展，采用多项式训练和逐项评估。支持渐进式评估，可在测试时提前停止进行轻量监控，或使用更多项获得更强保护

Result: 在两个大规模安全数据集(WildGuardMix和BeaverTails)上，对4个参数达300亿的模型测试表明，TPCs在相同规模下与MLP基线相当或更优，同时比黑盒方法更具可解释性

Conclusion: TPCs提供两种使用模式：作为安全调节器可购买更强保护，作为自适应级联可降低总体监控成本，实现灵活高效的安全监测

Abstract: Monitoring large language models' (LLMs) activations is an effective way to
detect harmful requests before they lead to unsafe outputs. However,
traditional safety monitors often require the same amount of compute for every
query. This creates a trade-off: expensive monitors waste resources on easy
inputs, while cheap ones risk missing subtle cases. We argue that safety
monitors should be flexible--costs should rise only when inputs are difficult
to assess, or when more compute is available. To achieve this, we introduce
Truncated Polynomial Classifiers (TPCs), a natural extension of linear probes
for dynamic activation monitoring. Our key insight is that polynomials can be
trained and evaluated progressively, term-by-term. At test-time, one can
early-stop for lightweight monitoring, or use more terms for stronger
guardrails when needed. TPCs provide two modes of use. First, as a safety dial:
by evaluating more terms, developers and regulators can "buy" stronger
guardrails from the same model. Second, as an adaptive cascade: clear cases
exit early after low-order checks, and higher-order guardrails are evaluated
only for ambiguous inputs, reducing overall monitoring costs. On two
large-scale safety datasets (WildGuardMix and BeaverTails), for 4 models with
up to 30B parameters, we show that TPCs compete with or outperform MLP-based
probe baselines of the same size, all the while being more interpretable than
their black-box counterparts. Our code is available at
http://github.com/james-oldfield/tpc.

</details>


### [171] [From Fragile to Certified: Wasserstein Audits of Group Fairness Under Distribution Shift](https://arxiv.org/abs/2509.26241)
*Ahmad-Reza Ehyaei,Golnoosh Farnadi,Samira Samadi*

Main category: cs.LG

TL;DR: 提出了一种基于Wasserstein分布鲁棒性的框架，用于在最坏情况下的测试分布上认证群体公平性，解决了传统公平性指标在分布偏移下的不稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 群体公平性指标（如均等化几率）在不同重采样中变化剧烈，在分布偏移下尤其脆弱，这削弱了可靠审计的能力。

Method: 使用Wasserstein分布鲁棒框架，通过强对偶性推导出可处理的重新表述和高效估计器（DRUNE），在经验法则周围的合理测试分布球上认证最坏情况群体公平性。

Result: 提出的ε-WDF方法在标准基准和分类器上，在分布偏移下提供了稳定的公平性评估，为超越观测数据的群体公平性审计和认证提供了理论基础。

Conclusion: 该框架为群体公平性审计提供了一种原则性方法，能够在分布偏移情况下提供可靠的公平性认证保证。

Abstract: Group-fairness metrics (e.g., equalized odds) can vary sharply across
resamples and are especially brittle under distribution shift, undermining
reliable audits. We propose a Wasserstein distributionally robust framework
that certifies worst-case group fairness over a ball of plausible test
distributions centered at the empirical law. Our formulation unifies common
group fairness notions via a generic conditional-probability functional and
defines $\varepsilon$-Wasserstein Distributional Fairness ($\varepsilon$-WDF)
as the audit target. Leveraging strong duality, we derive tractable
reformulations and an efficient estimator (DRUNE) for $\varepsilon$-WDF. We
prove feasibility and consistency and establish finite-sample certification
guarantees for auditing fairness, along with quantitative bounds under
smoothness and margin conditions. Across standard benchmarks and classifiers,
$\varepsilon$-WDF delivers stable fairness assessments under distribution
shift, providing a principled basis for auditing and certifying group fairness
beyond observational data.

</details>


### [172] [Wasserstein Distributionally Robust Optimization Through the Lens of Structural Causal Models and Individual Fairness](https://arxiv.org/abs/2509.26275)
*Ahmad-Reza Ehyaei,Golnoosh Farnadi,Samira Samadi*

Main category: cs.LG

TL;DR: 该论文将Wasserstein分布鲁棒优化应用于个体公平性问题，结合因果结构，提出了可计算的DRO对偶形式和近似最坏情况损失的闭式解，并提供了有限样本误差界。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少探索DRO在个体公平性方面的应用，特别是在考虑因果结构和敏感属性的学习问题中，需要填补这一空白。

Method: 从因果和个体公平视角构建DRO问题，提出DRO对偶形式作为高效工具，将问题转化为更易处理的形式，并推导近似最坏情况损失的闭式解作为正则化项。

Result: 成功将DRO问题转化为更易计算的形式，消除了min-max问题中的max步骤，并在更一般情况下估计正则化项，建立了DRO与经典鲁棒优化的联系。

Conclusion: 通过移除已知结构因果模型的假设，提供了使用经验分布和估计因果结构设计DRO时的有限样本误差界，确保了高效和鲁棒的学习。

Abstract: In recent years, Wasserstein Distributionally Robust Optimization (DRO) has
garnered substantial interest for its efficacy in data-driven decision-making
under distributional uncertainty. However, limited research has explored the
application of DRO to address individual fairness concerns, particularly when
considering causal structures and sensitive attributes in learning problems. To
address this gap, we first formulate the DRO problem from causality and
individual fairness perspectives. We then present the DRO dual formulation as
an efficient tool to convert the DRO problem into a more tractable and
computationally efficient form. Next, we characterize the closed form of the
approximate worst-case loss quantity as a regularizer, eliminating the max-step
in the min-max DRO problem. We further estimate the regularizer in more general
cases and explore the relationship between DRO and classical robust
optimization. Finally, by removing the assumption of a known structural causal
model, we provide finite sample error bounds when designing DRO with empirical
distributions and estimated causal structures to ensure efficiency and robust
learning.

</details>


### [173] [Reframing Generative Models for Physical Systems using Stochastic Interpolants](https://arxiv.org/abs/2509.26282)
*Anthony Zhou,Alexander Wikner,Amaury Lancelin,Pedram Hassanzadeh,Amir Barati Farimani*

Main category: cs.LG

TL;DR: 本文比较了不同生成模型在物理系统仿真中的表现，发现随机插值方法比基于高斯噪声的方法更有效，能够用更少的采样步骤产生更准确的预测。


<details>
  <summary>Details</summary>
Motivation: 当前大多数生成模型依赖迭代去噪高斯噪声，这可能不是PDE和动力系统自回归预测任务的最有效选择。本文旨在评估不同生成模型在物理仿真中的表现。

Method: 使用随机插值方法，直接学习当前状态和未来状态之间的随机过程，利用连续物理分布的邻近性。

Result: 随机插值方法比基于高斯噪声传输的模型使用更少的采样步骤，产生更准确的预测。

Conclusion: 随机插值方法在物理仿真中具有竞争力，能够平衡确定性精度、谱一致性和概率校准要求。

Abstract: Generative models have recently emerged as powerful surrogates for physical
systems, demonstrating increased accuracy, stability, and/or statistical
fidelity. Most approaches rely on iteratively denoising a Gaussian, a choice
that may not be the most effective for autoregressive prediction tasks in PDEs
and dynamical systems such as climate. In this work, we benchmark generative
models across diverse physical domains and tasks, and highlight the role of
stochastic interpolants. By directly learning a stochastic process between
current and future states, stochastic interpolants can leverage the proximity
of successive physical distributions. This allows for generative models that
can use fewer sampling steps and produce more accurate predictions than models
relying on transporting Gaussian noise. Our experiments suggest that generative
models need to balance deterministic accuracy, spectral consistency, and
probabilistic calibration, and that stochastic interpolants can potentially
fulfill these requirements by adjusting their sampling. This study establishes
stochastic interpolants as a competitive baseline for physical emulation and
gives insight into the abilities of different generative modeling frameworks.

</details>


### [174] [Noise-Guided Transport for Imitation Learning](https://arxiv.org/abs/2509.26294)
*Lionel Blondé,Joao A. Candido Ramos,Alexandros Kalousis*

Main category: cs.LG

TL;DR: NGT是一种轻量级的模仿学习方法，将模仿学习建模为最优传输问题，通过对抗训练解决，在极低数据量下（仅20个转换）仍能在连续控制任务中表现良好。


<details>
  <summary>Details</summary>
Motivation: 在低数据环境下，仅有少量专家演示可用，传统依赖大规模预训练或高容量架构的方法难以应用，需要高效利用演示数据的方法。

Method: 将模仿学习建模为最优传输问题，通过对抗训练求解，无需预训练或专用架构，内置不确定性估计，实现简单易调。

Result: 在具有挑战性的连续控制任务（包括高维Humanoid任务）上表现强劲，在极低数据量（仅20个转换）下仍能有效工作。

Conclusion: NGT提供了一种简单有效的模仿学习方法，特别适合低数据环境，无需复杂架构或预训练过程。

Abstract: We consider imitation learning in the low-data regime, where only a limited
number of expert demonstrations are available. In this setting, methods that
rely on large-scale pretraining or high-capacity architectures can be difficult
to apply, and efficiency with respect to demonstration data becomes critical.
We introduce Noise-Guided Transport (NGT), a lightweight off-policy method that
casts imitation as an optimal transport problem solved via adversarial
training. NGT requires no pretraining or specialized architectures,
incorporates uncertainty estimation by design, and is easy to implement and
tune. Despite its simplicity, NGT achieves strong performance on challenging
continuous control tasks, including high-dimensional Humanoid tasks, under
ultra-low data regimes with as few as 20 transitions. Code is publicly
available at: https://github.com/lionelblonde/ngt-pytorch.

</details>


### [175] [Tuning the Tuner: Introducing Hyperparameter Optimization for Auto-Tuning](https://arxiv.org/abs/2509.26300)
*Floris-Jan Willemsen,Rob V. van Nieuwpoort,Ben van Werkhoven*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的自动调优器超参数调优方法，通过统计评估、数据集共享和仿真模式，将超参数调优成本降低两个数量级，显著提升自动调优性能。


<details>
  <summary>Details</summary>
Motivation: 自动性能调优在科学计算中广泛应用，但优化算法的超参数几乎从未被调优过，其潜在性能影响未被研究。

Method: 提出通用超参数调优方法，包括跨搜索空间的稳健统计评估、FAIR数据集发布和仿真模式（重放历史调优数据）。

Result: 即使有限的超参数调优也能平均提升自动调优器性能94.8%，使用元策略优化超参数可平均提升204.7%。

Conclusion: 超参数调优是推进自动调优研究和实践的强大技术，常被忽视但潜力巨大。

Abstract: Automatic performance tuning (auto-tuning) is widely used to optimize
performance-critical applications across many scientific domains by finding the
best program variant among many choices. Efficient optimization algorithms are
crucial for navigating the vast and complex search spaces in auto-tuning. As is
well known in the context of machine learning and similar fields,
hyperparameters critically shape optimization algorithm efficiency. Yet for
auto-tuning frameworks, these hyperparameters are almost never tuned, and their
potential performance impact has not been studied.
  We present a novel method for general hyperparameter tuning of optimization
algorithms for auto-tuning, thus "tuning the tuner". In particular, we propose
a robust statistical method for evaluating hyperparameter performance across
search spaces, publish a FAIR data set and software for reproducibility, and
present a simulation mode that replays previously recorded tuning data,
lowering the costs of hyperparameter tuning by two orders of magnitude. We show
that even limited hyperparameter tuning can improve auto-tuner performance by
94.8% on average, and establish that the hyperparameters themselves can be
optimized efficiently with meta-strategies (with an average improvement of
204.7%), demonstrating the often overlooked hyperparameter tuning as a powerful
technique for advancing auto-tuning research and practice.

</details>


### [176] [NeuroTTT: Bridging Pretraining-Downstream Task Misalignment in EEG Foundation Models via Test-Time Training](https://arxiv.org/abs/2509.26301)
*Suli Wang,Yangshen Deng,Zhenghua Bao,Xinyu Zhan,Yiqun Duan*

Main category: cs.LG

TL;DR: 本文提出了一种两阶段对齐策略，通过NeuroTTT自监督微调和测试时训练来解决EEG基础模型与下游任务之间的不对齐问题，显著提升了脑机接口任务的性能。


<details>
  <summary>Details</summary>
Motivation: 大规模EEG信号基础模型在脑机接口应用中面临预训练目标与下游任务不对齐以及跨受试者分布偏移的挑战，需要一种有效的方法来弥合这一差距。

Method: 采用两阶段对齐策略：1) NeuroTTT自监督微调，通过任务相关的自监督目标增强基础模型；2) 测试时训练，包括自监督测试时训练和预测熵最小化，动态校准模型以适应新输入。

Result: 在三个不同的脑机接口任务（想象语音、压力检测、运动想象）上，该方法实现了最先进的性能，显著优于传统的微调和适应方法。

Conclusion: 提出的对齐策略成功地将领域调优的自监督与测试时训练相结合，为大规模EEG基础模型提供了有效的解决方案，显著提升了模型的鲁棒性和准确性。

Abstract: Large-scale foundation models for EEG signals offer a promising path to
generalizable brain-computer interface (BCI) applications, but they often
suffer from misalignment between pretraining objectives and downstream tasks,
as well as significant cross-subject distribution shifts. This paper addresses
these challenges by introducing a two-stage alignment strategy that bridges the
gap between generic pretraining and specific EEG decoding tasks. First, we
propose NeuroTTT: a domain-specific self-supervised fine-tuning paradigm that
augments the foundation model with task-relevant self-supervised objectives,
aligning latent representations to important spectral, spatial, and temporal
EEG features without requiring additional labeled data. Second, we incorporate
test-time training (TTT) at inference, we perform (i) self-supervised test-time
training on individual unlabeled test samples and (ii) prediction entropy
minimization (Tent), which updates only normalization statistics to continually
calibrate the model to each new input on the fly. Our approach, which, to our
knowledge, is the first to unify domain-tuned self-supervision with test-time
training in large-scale EEG foundation models, yields substantially improved
robustness and accuracy across diverse BCI tasks (imagined speech, stress
detection, motor imagery). Using CBraMod and LaBraM as backbones, our method
pushes their performance to a markedly higher level. Results on three diverse
tasks demonstrate that the proposed alignment strategy achieves
state-of-the-art performance, outperforming conventional fine-tuning and
adaptation methods. Our code is available at
https://github.com/wsl2000/NeuroTTT.

</details>


### [177] [Attribution-Guided Decoding](https://arxiv.org/abs/2509.26307)
*Piotr Komorowski,Elena Golimblevskaia,Reduan Achtibat,Thomas Wiegand,Sebastian Lapuschkin,Wojciech Samek*

Main category: cs.LG

TL;DR: 提出了基于可解释性的解码策略AGD，通过引导生成过程向用户定义的感兴趣区域（ROI）来提升LLM的指令遵循能力和事实准确性


<details>
  <summary>Details</summary>
Motivation: 标准解码方法在满足复杂指令遵循和事实准确性方面存在不足，而现有控制技术往往会降低输出质量，需要一种更有效的方法来提升LLM的可靠性

Method: AGD从高概率输出候选词中选择对用户定义ROI具有最高归因度的词，ROI可以灵活定义在模型输入或内部组件的不同部分

Result: 在指令遵循任务中显著提升成功率（Llama 3.1从66.0%提升到79.1%），在知识密集型任务中减少幻觉并提高事实准确性，还提出了基于熵的自适应变体来减轻质量下降

Conclusion: AGD是一种多功能、更可解释且有效的方法，能够增强现代LLM的可靠性

Abstract: The capacity of Large Language Models (LLMs) to follow complex instructions
and generate factually accurate text is critical for their real-world
application. However, standard decoding methods often fail to robustly satisfy
these requirements, while existing control techniques frequently degrade
general output quality. In this work, we introduce Attribution-Guided Decoding
(AGD), an interpretability-based decoding strategy. Instead of directly
manipulating model activations, AGD considers a set of high-probability output
token candidates and selects the one that exhibits the highest attribution to a
user-defined Region of Interest (ROI). This ROI can be flexibly defined over
different parts of the model's input or internal components, allowing AGD to
steer generation towards various desirable behaviors. We demonstrate AGD's
efficacy across three challenging domains. For instruction following, we show
that AGD significantly boosts adherence (e.g., improving the overall success
rate on Llama 3.1 from 66.0% to 79.1%). For knowledge-intensive tasks, we show
that guiding generation towards usage of internal knowledge components or
contextual sources can reduce hallucinations and improve factual accuracy in
both closed-book and open-book settings. Furthermore, we propose an adaptive,
entropy-based variant of AGD that mitigates quality degradation and reduces
computational overhead by applying guidance only when the model is uncertain.
Our work presents a versatile, more interpretable, and effective method for
enhancing the reliability of modern LLMs.

</details>


### [178] [A Review on Single-Problem Multi-Attempt Heuristic Optimization](https://arxiv.org/abs/2509.26321)
*Judith Echevarrieta,Etor Arza,Aritz Pérez,Josu Ceberio*

Main category: cs.LG

TL;DR: 本文对单问题多尝试启发式优化进行了系统性综述，将算法选择、参数调优、多起点和资源分配等策略统一到一个共同框架中，并建立了分类体系。


<details>
  <summary>Details</summary>
Motivation: 在实际优化场景中，当计算预算充足时，可以尝试多种启发式方法来解决同一问题，但如何顺序选择这些替代方案以高效找到最佳解决方案尚未得到系统研究。

Method: 通过统一术语和共同框架，将来自算法选择、参数调优、多起点和资源分配等不同研究领域的策略进行整合，并建立分类体系来系统组织这些策略。

Result: 提出了一个统一的框架和分类法，能够系统性地组织和分类单问题多尝试启发式优化中的顺序选择策略。

Conclusion: 这项工作填补了单问题多尝试启发式优化领域系统性综述的空白，为实践者提供了选择最佳策略的理论基础和方法指导。

Abstract: In certain real-world optimization scenarios, practitioners are not
interested in solving multiple problems but rather in finding the best solution
to a single, specific problem. When the computational budget is large relative
to the cost of evaluating a candidate solution, multiple heuristic alternatives
can be tried to solve the same given problem, each possibly with a different
algorithm, parameter configuration, initialization, or stopping criterion. The
sequential selection of which alternative to try next is crucial for
efficiently identifying the one that provides the best possible solution across
multiple attempts. Despite the relevance of this problem in practice, it has
not yet been the exclusive focus of any existing review. Several sequential
alternative selection strategies have been proposed in different research
topics, but they have not been comprehensively and systematically unified under
a common perspective.
  This work presents a focused review of single-problem multi-attempt heuristic
optimization. It brings together suitable strategies to this problem that have
been studied separately through algorithm selection, parameter tuning,
multi-start and resource allocation. These strategies are explained using a
unified terminology within a common framework, which supports the development
of a taxonomy for systematically organizing and classifying them.

</details>


### [179] [A Generalized Information Bottleneck Theory of Deep Learning](https://arxiv.org/abs/2509.26327)
*Charles Westphal,Stephen Hailes,Mirco Musolesi*

Main category: cs.LG

TL;DR: 提出了广义信息瓶颈(GIB)框架，通过协同信息视角重新表述信息瓶颈原理，解决了原始IB的理论模糊性和估计挑战问题


<details>
  <summary>Details</summary>
Motivation: 信息瓶颈原理虽然为理解神经网络学习提供了理论框架，但其实际应用受到理论模糊性和准确估计困难的限制

Method: 基于协同信息（特征联合处理获得的信息）重新表述IB，使用基于平均交互信息的可计算协同定义

Result: GIB在各种架构中（包括ReLU激活的标准IB失效情况）都表现出压缩阶段，在CNN和Transformer中产生可解释的动态，并与对抗鲁棒性理解更一致

Conclusion: GIB框架在保持与现有IB理论兼容的同时，解决了其局限性，为理解神经网络学习提供了更实用的理论工具

Abstract: The Information Bottleneck (IB) principle offers a compelling theoretical
framework to understand how neural networks (NNs) learn. However, its practical
utility has been constrained by unresolved theoretical ambiguities and
significant challenges in accurate estimation. In this paper, we present a
\textit{Generalized Information Bottleneck (GIB)} framework that reformulates
the original IB principle through the lens of synergy, i.e., the information
obtainable only through joint processing of features. We provide theoretical
and empirical evidence demonstrating that synergistic functions achieve
superior generalization compared to their non-synergistic counterparts.
Building on these foundations we re-formulate the IB using a computable
definition of synergy based on the average interaction information (II) of each
feature with those remaining. We demonstrate that the original IB objective is
upper bounded by our GIB in the case of perfect estimation, ensuring
compatibility with existing IB theory while addressing its limitations. Our
experimental results demonstrate that GIB consistently exhibits compression
phases across a wide range of architectures (including those with \textit{ReLU}
activations where the standard IB fails), while yielding interpretable dynamics
in both CNNs and Transformers and aligning more closely with our understanding
of adversarial robustness.

</details>


### [180] [FedMuon: Federated Learning with Bias-corrected LMO-based Optimization](https://arxiv.org/abs/2509.26337)
*Yuki Takezawa,Anastasia Koloskova,Xiaowen Jiang,Sebastian U. Stich*

Main category: cs.LG

TL;DR: 本文提出了FedMuon，一种在联邦学习中应用Muon优化器的方法，解决了直接使用Muon作为本地优化器不收敛的问题，并通过实验证明其优于现有联邦学习方法。


<details>
  <summary>Details</summary>
Motivation: Muon优化器基于线性最小化预言机(LMO)，在神经网络训练中比Adam等自适应优化方法更快，但直接应用于联邦学习中的FedAvg会导致不收敛问题。

Method: 提出FedMuon方法，通过缓解LMO作为有偏算子的问题，并分析近似求解LMO对收敛率的影响，发现FedMuon可以在任何牛顿-舒尔茨迭代次数下收敛。

Result: 实验证明FedMuon能够超越最先进的联邦学习方法，且随着LMO求解精度的提高，收敛速度更快。

Conclusion: FedMuon成功将Muon优化器应用于联邦学习，解决了收敛性问题，并在性能上优于现有方法，为联邦学习提供了更高效的优化方案。

Abstract: Recently, a new optimization method based on the linear minimization oracle
(LMO), called Muon, has been attracting increasing attention since it can train
neural networks faster than existing adaptive optimization methods, such as
Adam. In this paper, we study how Muon can be utilized in federated learning.
We first show that straightforwardly using Muon as the local optimizer of
FedAvg does not converge to the stationary point since the LMO is a biased
operator. We then propose FedMuon which can mitigate this issue. We also
analyze how solving the LMO approximately affects the convergence rate and find
that, surprisingly, FedMuon can converge for any number of Newton-Schulz
iterations, while it can converge faster as we solve the LMO more accurately.
Through experiments, we demonstrated that FedMuon can outperform the
state-of-the-art federated learning methods.

</details>


### [181] [Memory-Driven Self-Improvement for Decision Making with Large Language Models](https://arxiv.org/abs/2509.26340)
*Xue Yan,Zijing Ou,Mengyue Yang,Yan Song,Haifeng Zhang,Yingzhen Li,Jun Wang*

Main category: cs.LG

TL;DR: 提出了一个记忆驱动的自改进框架，将LLM的通用先验知识与特定领域的紧凑记忆相结合，通过记忆与LLM先验的相互增强来提升序列决策任务的性能。


<details>
  <summary>Details</summary>
Motivation: LLM在序列决策任务中虽然具备广泛知识，但面对特定任务时缺乏针对性，难以高效适应有限的领域数据。

Method: 构建包含历史交互和Q值的记忆模块，利用记忆中的决策相关知识来优化LLM先验，同时LLM生成的高奖励轨迹反过来丰富记忆，形成自改进循环。

Result: 在ALFWorld实验中，该方法显著优于传统RL和基于LLM的基线方法，在分布内任务上性能提升超过40%，在未见任务上泛化能力提升超过75%。

Conclusion: 记忆驱动的自改进框架有效结合了LLM的通用知识和领域特定经验，在序列决策任务中实现了显著性能提升和良好的泛化能力。

Abstract: Large language models (LLMs) have emerged as effective action policies for
sequential decision-making (SDM) tasks due to their extensive prior knowledge.
However, this broad yet general knowledge is often insufficient for specific
decision-making tasks with limited task-related data, making it challenging to
efficiently adapt LLMs to specific SDM tasks. To address this challenge, we
propose a memory-driven self-improvement framework that combines LLM general
prior knowledge with a compact memory of domain-specific experiences. Memory
retains past interactions and associated Q-values, thereby capturing
decision-relevant knowledge that facilitates accurate value estimation and
informs the LLM prior refinement. The refined LLM prior, in turn, generates
higher-reward trajectories that further enrich memory, forming a natural
self-improvement framework where memory and LLM prior mutually reinforce each
other. Experiments show that our memory-driven approach significantly
outperforms both traditional RL and LLM-based baselines, e.g., improving
performance by over 40\% on in-distribution tasks and over 75\% when
generalized to unseen tasks in ALFWorld.

</details>


### [182] [LLM-Assisted Emergency Triage Benchmark: Bridging Hospital-Rich and MCI-Like Field Simulation](https://arxiv.org/abs/2509.26351)
*Joshua Sebastian,Karma Tobden,KMA Solaiman*

Main category: cs.LG

TL;DR: 该研究创建了一个开放的急诊分诊基准，用于预测患者恶化情况（ICU转移、院内死亡率），包含医院资源丰富和模拟大规模伤亡事件两种场景，并利用大语言模型辅助数据集构建。


<details>
  <summary>Details</summary>
Motivation: 急诊和大规模伤亡事件分诊研究因缺乏开放、可复现的基准而受限，现有MIMIC-IV-ED数据库需要大量预处理才能用于分诊研究，这限制了非技术用户的可访问性。

Method: 使用大语言模型辅助构建急诊分诊基准：1）统一噪声字段（如AVPU和呼吸设备）；2）优先选择临床相关的生命体征和实验室指标；3）指导模式对齐和不同表格的高效合并。定义了两种场景：医院资源丰富设置和MCI模拟现场设置。

Result: 提供了基线模型和基于SHAP的可解释性分析，展示了不同场景间的预测差距以及分诊中最关键的特征。

Conclusion: 这些贡献使分诊预测研究更具可复现性和可访问性，是临床AI中数据集民主化的重要一步。

Abstract: Research on emergency and mass casualty incident (MCI) triage has been
limited by the absence of openly usable, reproducible benchmarks. Yet these
scenarios demand rapid identification of the patients most in need, where
accurate deterioration prediction can guide timely interventions. While the
MIMIC-IV-ED database is openly available to credentialed researchers,
transforming it into a triage-focused benchmark requires extensive
preprocessing, feature harmonization, and schema alignment -- barriers that
restrict accessibility to only highly technical users.
  We address these gaps by first introducing an open, LLM-assisted emergency
triage benchmark for deterioration prediction (ICU transfer, in-hospital
mortality). The benchmark then defines two regimes: (i) a hospital-rich setting
with vitals, labs, notes, chief complaints, and structured observations, and
(ii) an MCI-like field simulation limited to vitals, observations, and notes.
Large language models (LLMs) contributed directly to dataset construction by
(i) harmonizing noisy fields such as AVPU and breathing devices, (ii)
prioritizing clinically relevant vitals and labs, and (iii) guiding schema
alignment and efficient merging of disparate tables.
  We further provide baseline models and SHAP-based interpretability analyses,
illustrating predictive gaps between regimes and the features most critical for
triage. Together, these contributions make triage prediction research more
reproducible and accessible -- a step toward dataset democratization in
clinical AI.

</details>


### [183] [Data-to-Energy Stochastic Dynamics](https://arxiv.org/abs/2509.26364)
*Kirill Tamogashev,Nikolay Malkin*

Main category: cs.LG

TL;DR: 提出了首个无需数据样本、仅通过非归一化密度建模薛定谔桥的通用方法，该方法基于强化学习框架将迭代比例拟合扩展到无数据情况。


<details>
  <summary>Details</summary>
Motivation: 现有薛定谔桥算法需要两个分布的样本数据，但许多实际问题中只能获得分布的未归一化密度函数，无法获取样本。

Method: 基于强化学习框架，将迭代比例拟合扩展到无数据情况，学习动态系统的扩散系数，实现仅通过能量函数建模分布间传输。

Result: 在合成问题上成功学习多模态分布间的传输，改进现有数据到数据算法，并应用于生成模型隐空间后验采样，实现无数据图像到图像转换。

Conclusion: 提出的数据到能量IPF方法有效解决了无样本情况下的薛定谔桥问题，为扩散模型和流匹配提供了新工具。

Abstract: The Schr\"odinger bridge problem is concerned with finding a stochastic
dynamical system bridging two marginal distributions that minimises a certain
transportation cost. This problem, which represents a generalisation of optimal
transport to the stochastic case, has received attention due to its connections
to diffusion models and flow matching, as well as its applications in the
natural sciences. However, all existing algorithms allow to infer such dynamics
only for cases where samples from both distributions are available. In this
paper, we propose the first general method for modelling Schr\"odinger bridges
when one (or both) distributions are given by their unnormalised densities,
with no access to data samples. Our algorithm relies on a generalisation of the
iterative proportional fitting (IPF) procedure to the data-free case, inspired
by recent developments in off-policy reinforcement learning for training of
diffusion samplers. We demonstrate the efficacy of the proposed data-to-energy
IPF on synthetic problems, finding that it can successfully learn transports
between multimodal distributions. As a secondary consequence of our
reinforcement learning formulation, which assumes a fixed time discretisation
scheme for the dynamics, we find that existing data-to-data Schr\"odinger
bridge algorithms can be substantially improved by learning the diffusion
coefficient of the dynamics. Finally, we apply the newly developed algorithm to
the problem of sampling posterior distributions in latent spaces of generative
models, thus creating a data-free image-to-image translation method. Code:
https://github.com/mmacosha/d2e-stochastic-dynamics

</details>


### [184] [Refine Drugs, Don't Complete Them: Uniform-Source Discrete Flows for Fragment-Based Drug Discovery](https://arxiv.org/abs/2509.26405)
*Benno Kaech,Luis Wyss,Karsten Borgwardt,Gianvito Grasso*

Main category: cs.LG

TL;DR: InVirtuoGen是一个基于离散流的生成模型，用于小分子的从头生成、片段约束生成以及目标属性/先导化合物优化，在药物发现中建立了多功能的生成基础。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理片段化SMILES表示的生成模型，支持从早期命中发现到多目标先导化合物优化的完整药物发现流程。

Method: 使用离散流生成模型学习将均匀源分布转换为数据分布，结合遗传算法和近端属性优化微调策略进行属性优化。

Result: 在从头生成任务中实现了更好的质量-多样性权衡，在Practical Molecular Optimization基准测试中创造了新的最优性能，在先导化合物优化中获得了更高的对接分数。

Conclusion: InVirtuoGen为药物发现提供了一个多功能的生成基础，从早期命中发现到多目标先导化合物优化，并开源了预训练模型和代码以促进可重复性。

Abstract: We introduce InVirtuoGen, a discrete flow generative model for fragmented
SMILES for de novo and fragment-constrained generation, and
target-property/lead optimization of small molecules. The model learns to
transform a uniform source over all possible tokens into the data distribution.
Unlike masked models, its training loss accounts for predictions on all
sequence positions at every denoising step, shifting the generation paradigm
from completion to refinement, and decoupling the number of sampling steps from
the sequence length. For \textit{de novo} generation, InVirtuoGen achieves a
stronger quality-diversity pareto frontier than prior fragment-based models and
competitive performance on fragment-constrained tasks. For property and lead
optimization, we propose a hybrid scheme that combines a genetic algorithm with
a Proximal Property Optimization fine-tuning strategy adapted to discrete
flows. Our approach sets a new state-of-the-art on the Practical Molecular
Optimization benchmark, measured by top-10 AUC across tasks, and yields higher
docking scores in lead optimization than previous baselines. InVirtuoGen thus
establishes a versatile generative foundation for drug discovery, from early
hit finding to multi-objective lead optimization. We further contribute to open
science by releasing pretrained checkpoints and code, making our results fully
reproducible\footnote{https://github.com/invirtuolabs/InVirtuoGen_results}.

</details>


### [185] [Ascent Fails to Forget](https://arxiv.org/abs/2509.26427)
*Ioannis Mavrothalassitis,Pol Puigdemont,Noam Itzhak Levi,Volkan Cevher*

Main category: cs.LG

TL;DR: 梯度上升的无约束优化方法在机器遗忘中经常失败，原因是遗忘集和保留集之间存在统计依赖性，即使只是简单的相关性也会导致这些方法无法有效执行遗忘。


<details>
  <summary>Details</summary>
Motivation: 挑战普遍认为梯度上升方法能有效执行机器遗忘的观点，揭示遗忘集和保留集之间的统计依赖性是导致这些方法失败的根本原因。

Method: 通过理论分析和实验验证，包括逻辑回归案例研究和玩具示例，展示梯度下降-上升迭代如何因数据集间依赖性而偏离理想的重训练模型。

Result: 梯度上升方法不仅无法收敛到理想的重训练模型，反而可能收敛到比原始模型更差的解，使遗忘过程产生负面影响。在复杂神经网络上的实验也证实了这些理论发现。

Conclusion: 遗忘集和保留集之间的统计依赖性（即使是简单的相关性）足以导致基于梯度上升的遗忘方法失败，这对机器遗忘的实际应用提出了重要挑战。

Abstract: Contrary to common belief, we show that gradient ascent-based unconstrained
optimization methods frequently fail to perform machine unlearning, a
phenomenon we attribute to the inherent statistical dependence between the
forget and retain data sets. This dependence, which can manifest itself even as
simple correlations, undermines the misconception that these sets can be
independently manipulated during unlearning. We provide empirical and
theoretical evidence showing these methods often fail precisely due to this
overlooked relationship. For random forget sets, this dependence means that
degrading forget set metrics (which, for a retrained model, should mirror test
set metrics) inevitably harms overall test performance. Going beyond random
sets, we consider logistic regression as an instructive example where a
critical failure mode emerges: inter-set dependence causes gradient
descent-ascent iterations to progressively diverge from the ideal retrained
model. Strikingly, these methods can converge to solutions that are not only
far from the retrained ideal but are potentially even further from it than the
original model itself, rendering the unlearning process actively detrimental. A
toy example further illustrates how this dependence can trap models in inferior
local minima, inescapable via finetuning. Our findings highlight that the
presence of such statistical dependencies, even when manifest only as
correlations, can be sufficient for ascent-based unlearning to fail. Our
theoretical insights are corroborated by experiments on complex neural
networks, demonstrating that these methods do not perform as expected in
practice due to this unaddressed statistical interplay.

</details>


### [186] [AdaBlock-dLLM: Semantic-Aware Diffusion LLM Inference via Adaptive Block Size](https://arxiv.org/abs/2509.26432)
*Guanxi Lu,Hao,Chen,Yuto Karashima,Zhican Wang,Daichi Fujiki,Hongxiang Fan*

Main category: cs.LG

TL;DR: 本文提出了AdaBlock-dLLM，一种训练免费、即插即用的调度器，通过自适应调整块大小来优化扩散式大语言模型的半自回归解码，解决了固定块大小方法的延迟解码开销和过早解码错误问题。


<details>
  <summary>Details</summary>
Motivation: 传统半自回归解码使用固定块大小存在两个基本限制：延迟解码开销（高置信度令牌被不必要延迟）和过早解码错误（低置信度令牌过早提交导致错误）。本文首次系统性地挑战固定块大小假设。

Method: 通过对去噪过程中置信度动态的统计分析，识别出波动带区域，该区域编码局部语义结构。基于此提出AdaBlock-dLLM调度器，在运行时自适应调整块大小，使块边界与语义步骤对齐。

Result: 在多样化基准测试中，AdaBlock-dLLM在相同吞吐量预算下实现了高达5.3%的准确率提升。

Conclusion: 除了推理时优化，作者希望这种语义感知的自适应调度方法和基于置信度的分析能够启发未来dLLMs的训练策略。

Abstract: Diffusion-based large language models (dLLMs) are gaining attention for their
inherent capacity for parallel decoding, offering a compelling alternative to
autoregressive LLMs. Among various decoding strategies, blockwise
semi-autoregressive (semi-AR) approaches are widely adopted due to their
natural support for KV caching and their favorable accuracy-speed trade-off.
However, this paper identifies two fundamental limitations in the conventional
semi-AR decoding approach that applies a fixed block size: i) late decoding
overhead, where the unmasking of high-confidence tokens outside the current
block is unnecessarily delayed, and ii) premature decoding error, where
low-confidence tokens inside the current block are committed too early, leading
to incorrect tokens. This paper presents the first systematic investigation
challenging the fixed block size assumption in semi-AR decoding. Through a
statistical analysis of confidence dynamics during the denoising process, we
identify a volatility band (VB) region during dLLM decoding, which encodes
local semantic structure and can be used to guide adaptive block sizing.
Leveraging these insights, we introduce AdaBlock-dLLM, a training-free,
plug-and-play scheduler that adaptively aligns block boundaries with semantic
steps by adjusting block size during runtime. Extensive experiments across
diverse benchmarks show that AdaBlock-dLLM achieves up to 5.3% accuracy
improvement under the same throughput budget. Beyond inference-time
optimization, we hope our semantics-aware adaptive scheduling approach and
confidence-based analysis will inspire future training strategies for dLLMs.

</details>


### [187] [ACT: Agentic Classification Tree](https://arxiv.org/abs/2509.26433)
*Vincent Grari,Tim Arni,Thibault Laugel,Sylvain Lamprier,James Zou,Marcin Detyniecki*

Main category: cs.LG

TL;DR: ACT将决策树方法扩展到非结构化数据，通过将每个分裂点表述为自然语言问题，结合基于不纯度的评估和LLM反馈，在文本分类任务中达到或超过基于提示的方法，同时提供透明可解释的决策路径。


<details>
  <summary>Details</summary>
Motivation: 在高风险场景中，AI系统需要产生透明、可解释和可审计的决策。传统决策树如CART提供清晰可验证的规则，但仅限于结构化表格数据，无法直接处理非结构化文本输入。而LLM虽然广泛用于此类数据，但基于提示的策略仍依赖自由形式的推理，难以确保可信行为。

Method: 提出Agentic Classification Tree (ACT)，通过将每个分裂点表述为自然语言问题来扩展决策树方法，使用基于不纯度的评估和通过TextGrad的LLM反馈来优化问题。

Result: 在文本基准测试中，ACT达到或超过了基于提示的基线方法，同时产生透明和可解释的决策路径。

Conclusion: ACT成功将决策树的可解释性优势扩展到非结构化文本数据，为高风险应用提供了既准确又透明的分类解决方案。

Abstract: When used in high-stakes settings, AI systems are expected to produce
decisions that are transparent, interpretable, and auditable, a requirement
increasingly expected by regulations. Decision trees such as CART provide clear
and verifiable rules, but they are restricted to structured tabular data and
cannot operate directly on unstructured inputs such as text. In practice, large
language models (LLMs) are widely used for such data, yet prompting strategies
such as chain-of-thought or prompt optimization still rely on free-form
reasoning, limiting their ability to ensure trustworthy behaviors. We present
the Agentic Classification Tree (ACT), which extends decision-tree methodology
to unstructured inputs by formulating each split as a natural-language
question, refined through impurity-based evaluation and LLM feedback via
TextGrad. Experiments on text benchmarks show that ACT matches or surpasses
prompting-based baselines while producing transparent and interpretable
decision paths.

</details>


### [188] [Extensions of Robbins-Siegmund Theorem with Applications in Reinforcement Learning](https://arxiv.org/abs/2509.26442)
*Xinyu Liu,Zixuan Xie,Shangtong Zhang*

Main category: cs.LG

TL;DR: 扩展Robbins-Siegmund定理，放宽零阶项的可和性要求为平方可和性，为强化学习中的随机逼近算法提供新的收敛性分析工具。


<details>
  <summary>Details</summary>
Motivation: 原始Robbins-Siegmund定理要求零阶项可和，但在许多重要强化学习应用中该条件无法满足，限制了定理的应用范围。

Method: 引入对随机过程增量的新颖温和假设，结合平方可和条件，实现几乎必然收敛到有界集。

Result: 获得了几乎必然收敛速率、高概率集中界和L^p收敛速率，首次为带线性函数逼近的Q学习提供了这些收敛结果。

Conclusion: 扩展的Robbins-Siegmund定理为随机逼近和强化学习算法提供了更广泛适用的收敛性分析框架。

Abstract: The Robbins-Siegmund theorem establishes the convergence of stochastic
processes that are almost supermartingales and is foundational for analyzing a
wide range of stochastic iterative algorithms in stochastic approximation and
reinforcement learning (RL). However, its original form has a significant
limitation as it requires the zero-order term to be summable. In many important
RL applications, this summable condition, however, cannot be met. This
limitation motivates us to extend the Robbins-Siegmund theorem for almost
supermartingales where the zero-order term is not summable but only square
summable. Particularly, we introduce a novel and mild assumption on the
increments of the stochastic processes. This together with the square summable
condition enables an almost sure convergence to a bounded set. Additionally, we
further provide almost sure convergence rates, high probability concentration
bounds, and $L^p$ convergence rates. We then apply the new results in
stochastic approximation and RL. Notably, we obtain the first almost sure
convergence rate, the first high probability concentration bound, and the first
$L^p$ convergence rate for $Q$-learning with linear function approximation.

</details>


### [189] [fev-bench: A Realistic Benchmark for Time Series Forecasting](https://arxiv.org/abs/2509.26468)
*Oleksandr Shchur,Abdul Fatir Ansari,Caner Turkmen,Lorenzo Stella,Nick Erickson,Pablo Guerron,Michael Bohlke-Schneider,Yuyang Wang*

Main category: cs.LG

TL;DR: 提出了fev-bench基准测试，包含100个时间序列预测任务，覆盖7个领域，其中46个任务包含协变量。配套的fev库支持可重现评估，使用引导置信区间进行统计严谨的聚合分析。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试存在领域覆盖窄、忽视协变量任务、缺乏统计严谨性、评估基础设施不足等问题，阻碍了时间序列预测领域的持续进展。

Method: 开发了fev-bench基准测试集和fev Python库，采用引导置信区间进行统计严谨的聚合，从胜率和技能得分两个维度评估模型性能。

Result: 在fev-bench上评估了多种预训练、统计和基线模型，并确定了未来研究方向。

Conclusion: fev-bench解决了现有基准测试的局限性，为时间序列预测提供了更全面、统计严谨的评估框架。

Abstract: Benchmark quality is critical for meaningful evaluation and sustained
progress in time series forecasting, particularly given the recent rise of
pretrained models. Existing benchmarks often have narrow domain coverage or
overlook important real-world settings, such as tasks with covariates.
Additionally, their aggregation procedures often lack statistical rigor, making
it unclear whether observed performance differences reflect true improvements
or random variation. Many benchmarks also fail to provide infrastructure for
consistent evaluation or are too rigid to integrate into existing pipelines. To
address these gaps, we propose fev-bench, a benchmark comprising 100
forecasting tasks across seven domains, including 46 tasks with covariates.
Supporting the benchmark, we introduce fev, a lightweight Python library for
benchmarking forecasting models that emphasizes reproducibility and seamless
integration with existing workflows. Usingfev, fev-bench employs principled
aggregation methods with bootstrapped confidence intervals to report model
performance along two complementary dimensions: win rates and skill scores. We
report results on fev-bench for various pretrained, statistical and baseline
models, and identify promising directions for future research.

</details>


### [190] [DiVeQ: Differentiable Vector Quantization Using the Reparameterization Trick](https://arxiv.org/abs/2509.26469)
*Mohammad Hassan Vali,Tom Bäckström,Arno Solin*

Main category: cs.LG

TL;DR: DiVeQ通过将量化视为添加模拟量化失真的误差向量，在保持前向传播硬分配的同时允许梯度流动，解决了向量量化中梯度阻塞的问题。


<details>
  <summary>Details</summary>
Motivation: 向量量化在深度模型中很常见，但其硬分配会阻塞梯度并阻碍端到端训练，需要一种既能保持前向硬分配又能让梯度流动的量化方法。

Method: 提出DiVeQ，将量化视为添加模拟量化失真的误差向量；还提出空间填充变体SF-DiVeQ，将分配扩展到码字连接线构成的曲线上，减少量化误差并充分利用码本。

Result: 在VQ-VAE压缩和VQGAN生成任务中，DiVeQ和SF-DiVeQ在各种数据集上相比其他量化方法提升了重建质量和样本质量。

Conclusion: DiVeQ和SF-DiVeQ能够在不需辅助损失或温度调度的情况下实现端到端训练，有效解决了向量量化中的梯度阻塞问题。

Abstract: Vector quantization is common in deep models, yet its hard assignments block
gradients and hinder end-to-end training. We propose DiVeQ, which treats
quantization as adding an error vector that mimics the quantization distortion,
keeping the forward pass hard while letting gradients flow. We also present a
space-filling variant (SF-DiVeQ) that assigns to a curve constructed by the
lines connecting codewords, resulting in less quantization error and full
codebook usage. Both methods train end-to-end without requiring auxiliary
losses or temperature schedules. On VQ-VAE compression and VQGAN generation
across various data sets, they improve reconstruction and sample quality over
alternative quantization approaches.

</details>


### [191] [Equivariance by Local Canonicalization: A Matter of Representation](https://arxiv.org/abs/2509.26499)
*Gerrit Gerhartz,Peter Lippmann,Fred A. Hamprecht*

Main category: cs.LG

TL;DR: 提出一个框架将张量场网络转换为更高效的局部正则化范式，保持等变性同时显著提升运行速度，并发布了tensor_frames软件包。


<details>
  <summary>Details</summary>
Motivation: 等变神经网络在分子和几何数据学习中具有强归纳偏置，但通常依赖专门的、计算昂贵的张量操作，需要提高计算效率。

Method: 开发框架将现有张量场网络转换为局部正则化范式，系统比较不同等变表示的理论复杂度、经验运行时间和预测准确性。

Result: 该框架在保持等变性的同时显著改善了运行时间，并提供了易于集成等变性到标准消息传递神经网络的实现。

Conclusion: 局部正则化范式为等变神经网络提供了更高效的实现方式，通过tensor_frames软件包使等变性更易于集成到现有神经网络架构中。

Abstract: Equivariant neural networks offer strong inductive biases for learning from
molecular and geometric data but often rely on specialized, computationally
expensive tensor operations. We present a framework to transfers existing
tensor field networks into the more efficient local canonicalization paradigm,
preserving equivariance while significantly improving the runtime. Within this
framework, we systematically compare different equivariant representations in
terms of theoretical complexity, empirical runtime, and predictive accuracy. We
publish the tensor_frames package, a PyTorchGeometric based implementation for
local canonicalization, that enables straightforward integration of
equivariance into any standard message passing neural network.

</details>


### [192] [Entropy After $\langle \texttt{/Think} \rangle$ for reasoning model early exiting](https://arxiv.org/abs/2509.26522)
*Xi Wang,James McInerney,Lequn Wang,Nathan Kallus*

Main category: cs.LG

TL;DR: 提出EAT（Entropy After </Think>）信号来检测和防止大型推理模型的过度思考问题，通过监控推理过程中停止思考标记后的熵变化，实现自适应计算分配，减少13-21%的token使用而不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在长链思维中表现更好，但存在过度思考问题，即在达到正确答案后仍继续修改答案，造成token浪费。需要一种方法检测和防止这种低效行为。

Method: 提出EAT信号：在推理过程中添加停止思考标记</think>，监控后续token的熵变化轨迹。当Pass@1趋于稳定时，熵会下降并稳定，通过指数移动平均的方差阈值实现提前停止推理。

Result: 在MATH500和AIME2025数据集上，EAT方法减少13-21%的token使用，同时保持准确性不变。在无法访问推理模型logits的黑盒设置中，使用代理模型计算EAT仍然有效。

Conclusion: EAT是一种简单有效的过度思考检测方法，能够自适应分配计算资源，提高推理效率，在多种设置下都表现出良好的性能。

Abstract: Large reasoning models show improved performance with longer chains of
thought. However, recent work has highlighted (qualitatively) their tendency to
overthink, continuing to revise answers even after reaching the correct
solution. We quantitatively confirm this inefficiency by tracking Pass@1 for
answers averaged over a large number of rollouts and find that the model often
begins to always produce the correct answer early in the reasoning, making
extra reasoning a waste of tokens. To detect and prevent overthinking, we
propose a simple and inexpensive novel signal -- Entropy After </Think> (EAT)
-- for monitoring and deciding whether to exit reasoning early. By appending a
stop thinking token (</think>) and monitoring the entropy of the following
token as the model reasons, we obtain a trajectory that decreases and
stabilizes when Pass@1 plateaus; thresholding its variance under an exponential
moving average yields a practical stopping rule. Importantly, our approach
enables adaptively allocating compute based on the EAT trajectory, allowing us
to spend compute in a more efficient way compared with fixing the token budget
for all questions. Empirically, on MATH500 and AIME2025, EAT reduces token
usage by 13 - 21% without harming accuracy, and it remains effective in black
box settings where logits from the reasoning model are not accessible, and EAT
is computed with proxy models.

</details>


### [193] [TAP: Two-Stage Adaptive Personalization of Multi-task and Multi-Modal Foundation Models in Federated Learning](https://arxiv.org/abs/2509.26524)
*Seohyun Lee,Wenzhi Fang,Dong-Jun Han,Seyyedali Hosseinalipour,Christopher G. Brinton*

Main category: cs.LG

TL;DR: 提出了TAP（两阶段自适应个性化）方法，通过客户端-服务器模型架构不匹配和知识蒸馏来解决异构联邦学习中多任务多模态的个性化问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习产生的最终模型不一定适合每个客户端的需求，特别是在数据、任务和模态都异构的环境中，缺乏对基础模型进行微调和个性化的研究

Method: TAP方法：(i)利用客户端与服务器模型架构不匹配来选择性进行替换操作；(ii)在联邦学习后进行知识蒸馏，在不损害个性化的情况下获取有益的一般知识

Result: 通过大量实验验证了所提算法在各种数据集和任务上的有效性，并提供了服务器模型在模态-任务对架构下的收敛性分析

Conclusion: TAP方法能够有效解决异构联邦学习环境中的个性化问题，随着模态-任务对数量增加，服务器模型服务所有任务的能力会下降

Abstract: Federated Learning (FL), despite demonstrating impressive capabilities in the
training of multiple models in a decentralized manner, has been shown to
produce a final model not necessarily well-suited to the needs of each client.
While extensive work has been conducted on how to create tailored personalized
models, called Personalized Federated Learning (PFL), less attention has been
given to personalization via fine-tuning of foundation models with multi-task
and multi-modal properties. Moreover, there exists a lack of understanding in
the literature on how to fine-tune and personalize such models in a setting
that is heterogeneous across clients not only in data, but also in tasks and
modalities. To address this gap in the literature, we propose TAP (Two-Stage
Adaptive Personalization), which (i) leverages mismatched model architectures
between the clients and server to selectively conduct replacement operations
when it benefits a client's local tasks and (ii) engages in post-FL knowledge
distillation for capturing beneficial general knowledge without compromising
personalization. We also introduce the first convergence analysis of the server
model under its modality-task pair architecture, and demonstrate that as the
number of modality-task pairs increases, its ability to cater to all tasks
suffers. Through extensive experiments, we demonstrate the effectiveness of our
proposed algorithm across a variety of datasets and tasks in comparison to a
multitude of baselines. Implementation code is publicly available at
https://github.com/lee3296/TAP.

</details>


### [194] [Machine-Learning Driven Load Shedding to Mitigate Instability Attacks in Power Grids](https://arxiv.org/abs/2509.26532)
*Justin Tackett,Benjamin Francis,Luis Garcia,David Grimsman,Sean Warnick*

Main category: cs.LG

TL;DR: 提出一种基于监督机器学习的成本效益方法，用于增强电力系统负荷削减决策系统，以防御不稳定性攻击。


<details>
  <summary>Details</summary>
Motivation: 随着关键基础设施日益复杂且依赖性增强，电力系统成为复杂网络攻击的诱人目标，特别是新型的不稳定性攻击目前缺乏有效防护措施。

Method: 使用数据驱动的监督机器学习模型，在IEEE 14总线系统上通过Achilles Heel Technologies电力系统分析器进行概念验证，并采用改进的Prony分析(MPA)来检测不稳定性攻击。

Result: 证明MPA是一种可行的检测不稳定性攻击并触发防御机制的方法。

Conclusion: 该方法能够以成本效益的方式为现有电力系统负荷削减决策系统提供不稳定性攻击防护能力。

Abstract: Every year critical infrastructure becomes more complex and we grow to rely
on it more and more. With this reliance, it becomes an attractive target for
cyberattacks from sophisticated actors, with one of the most attractive targets
being the power grid. One class of attacks, instability attacks, is a newer
type of attack that has relatively few protections developed. We present a cost
effective, data-driven approach to training a supervised machine learning model
to retrofit load shedding decision systems in power grids with the capacity to
defend against instability attacks. We show a proof of concept on the IEEE 14
Bus System using the Achilles Heel Technologies Power Grid Analyzer, and show
through an implementation of modified Prony analysis (MPA) that MPA is a viable
method for detecting instability attacks and triggering defense mechanisms.

</details>


### [195] [The Loss Kernel: A Geometric Probe for Deep Learning Interpretability](https://arxiv.org/abs/2509.26537)
*Maxwell Adam,Zach Furman,Jesse Hoogland*

Main category: cs.LG

TL;DR: 提出了一种称为损失核的可解释性方法，用于根据训练好的神经网络测量数据点之间的相似性。该核是在保持低损失的参数扰动分布下计算的每样本损失的协方差矩阵。


<details>
  <summary>Details</summary>
Motivation: 开发一种实用的可解释性工具，能够根据神经网络的内部机制来理解数据点之间的关系，并验证其与语义层次结构的一致性。

Method: 损失核方法，通过计算在低损失保持参数扰动分布下的每样本损失的协方差矩阵来构建数据相似性度量。

Result: 在合成多任务问题上验证了方法有效性，显示其能按任务分离输入；在Inception-v1上应用于ImageNet，发现核结构与WordNet语义层次结构一致。

Conclusion: 损失核是一种实用的可解释性和数据归因工具，能够揭示神经网络对数据语义关系的理解。

Abstract: We introduce the loss kernel, an interpretability method for measuring
similarity between data points according to a trained neural network. The
kernel is the covariance matrix of per-sample losses computed under a
distribution of low-loss-preserving parameter perturbations. We first validate
our method on a synthetic multitask problem, showing it separates inputs by
task as predicted by theory. We then apply this kernel to Inception-v1 to
visualize the structure of ImageNet, and we show that the kernel's structure
aligns with the WordNet semantic hierarchy. This establishes the loss kernel as
a practical tool for interpretability and data attribution.

</details>


### [196] [TASP: Topology-aware Sequence Parallelism](https://arxiv.org/abs/2509.26541)
*Yida Wang,Ke Hong,Xiuhong Li,Yuanchao Xu,Wenxun Wang,Guohao Dai,Yu Wang*

Main category: cs.LG

TL;DR: TASP是一种针对长上下文LLM的拓扑感知序列并行方法，通过拓扑分解和原语分解充分利用现代加速器的通信能力，相比Ring Attention实现了最高3.58倍的加速。


<details>
  <summary>Details</summary>
Motivation: 主流序列并行方法Ring Attention由于采用Ring AllGather通信原语与现代加速器的AlltoAll拓扑不匹配，导致通信效率低下，限制了实际应用。

Method: 基于哈密顿分解理论，将现代加速器拓扑分解为多个正交环形数据路径，同时将Ring AllGather原语分解为相同数量的并发环形数据传输，提出TASP方法。

Result: 在单节点和多节点NVIDIA H100系统以及单节点AMD MI300X系统上的实验表明，TASP比Ring Attention及其变体Zigzag-Ring Attention实现了更高的通信效率，最高加速比达3.58倍。

Conclusion: TASP通过拓扑感知的序列并行方法有效解决了长上下文LLM中的通信效率问题，为大规模模型训练提供了更高效的解决方案。

Abstract: Long-context large language models (LLMs) face constraints due to the
quadratic complexity of the self-attention mechanism. The mainstream sequence
parallelism (SP) method, Ring Attention, attempts to solve this by distributing
the query into multiple query chunks across accelerators and enable each Q
tensor to access all KV tensors from other accelerators via the Ring AllGather
communication primitive. However, it exhibits low communication efficiency,
restricting its practical applicability. This inefficiency stems from the
mismatch between the Ring AllGather communication primitive it adopts and the
AlltoAll topology of modern accelerators. A Ring AllGather primitive is
composed of iterations of ring-styled data transfer, which can only utilize a
very limited fraction of an AlltoAll topology.
  Inspired by the Hamiltonian decomposition of complete directed graphs, we
identify that modern accelerator topology can be decomposed into multiple
orthogonal ring datapaths which can concurrently transfer data without
interference. Based on this, we further observe that the Ring AllGather
primitive can also be decomposed into the same number of concurrent ring-styled
data transfer at every iteration. Based on these insights, we propose TASP, a
topology-aware SP method for long-context LLMs that fully utilizes the
communication capacity of modern accelerators via topology decomposition and
primitive decomposition. Experimental results on both single-node and
multi-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate
that TASP achieves higher communication efficiency than Ring Attention on these
modern accelerator topologies and achieves up to 3.58 speedup than Ring
Attention and its variant Zigzag-Ring Attention. The code is available at
https://github.com/infinigence/HamiltonAttention.

</details>


### [197] [Bayesian Influence Functions for Hessian-Free Data Attribution](https://arxiv.org/abs/2509.26544)
*Philipp Alexander Kreer,Wilson Wu,Maxwell Adam,Zach Furman,Jesse Hoogland*

Main category: cs.LG

TL;DR: 提出了局部贝叶斯影响函数(BIF)，通过使用损失景观统计替代Hessian逆矩阵，解决了深度神经网络中传统影响函数面临的非可逆Hessian和高维参数空间问题。


<details>
  <summary>Details</summary>
Motivation: 传统影响函数在深度神经网络中面临两大挑战：Hessian矩阵不可逆和高维参数空间，限制了其在现代大规模神经网络中的应用。

Method: 采用Hessian-free方法，通过随机梯度MCMC采样估计损失景观统计，替代传统的Hessian逆矩阵计算，能够捕捉参数间的高阶交互作用。

Result: 该方法能够高效扩展到数十亿参数的神经网络，在预测重训练实验方面取得了最先进的性能。

Conclusion: 局部贝叶斯影响函数为深度神经网络提供了一种可扩展且有效的影响分析工具，克服了传统方法的局限性。

Abstract: Classical influence functions face significant challenges when applied to
deep neural networks, primarily due to non-invertible Hessians and
high-dimensional parameter spaces. We propose the local Bayesian influence
function (BIF), an extension of classical influence functions that replaces
Hessian inversion with loss landscape statistics that can be estimated via
stochastic-gradient MCMC sampling. This Hessian-free approach captures
higher-order interactions among parameters and scales efficiently to neural
networks with billions of parameters. We demonstrate state-of-the-art results
on predicting retraining experiments.

</details>


### [198] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2509.26564)
*Florian Grötschla,Longxiang Jiao,Luca A. Lanzendörfer,Roger Wattenhofer*

Main category: cs.LG

TL;DR: Panama是一个端到端的主动学习框架，结合LSTM和WaveNet架构训练参数化吉他放大器模型，通过集成模型分歧最大化策略减少所需数据点数量。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够通过最小化数据点（放大器旋钮设置）数量来创建虚拟放大器的参数化模型，以匹配领先的非参数化放大器建模器的感知质量。

Method: 使用LSTM和WaveNet-like架构，采用基于集成的主动学习策略，通过梯度优化最大化集成模型间的分歧来识别最具信息量的数据点。

Result: MUSHRA听力测试显示，仅用75个数据点，Panama模型就能在感知质量上与领先的开源非参数化放大器建模器NAM相媲美。

Conclusion: Panama框架通过主动学习策略有效减少了训练参数化吉他放大器模型所需的数据量，同时保持了高质量的音频重建效果。

Abstract: We introduce Panama, an active learning framework to train parametric guitar
amp models end-to-end using a combination of an LSTM model and a WaveNet-like
architecture. With \model, one can create a virtual amp by recording samples
that are determined through an ensemble-based active learning strategy to
minimize the amount of datapoints needed (i.e., amp knob settings). Our
strategy uses gradient-based optimization to maximize the disagreement among
ensemble models, in order to identify the most informative datapoints. MUSHRA
listening tests reveal that, with 75 datapoints, our models are able to match
the perceptual quality of NAM, the leading open-source non-parametric amp
modeler.

</details>


### [199] [Importance of localized dilatation and distensibility in identifying determinants of thoracic aortic aneurysm with neural operators](https://arxiv.org/abs/2509.26576)
*David S. Li,Somdatta Goswami,Qianying Cao,Vivek Oommen,Roland Assi,Jay D. Humphrey,George E. Karniadakis*

Main category: cs.LG

TL;DR: 使用有限元框架生成合成胸主动脉瘤，训练神经网络从扩张和可扩张性数据预测初始损伤，发现UNet模型性能最佳，且可扩张性数据对预测准确性至关重要。


<details>
  <summary>Details</summary>
Motivation: 胸主动脉瘤由多种机械和机械生物学破坏引起，不同损伤会产生不同的机械脆弱性，需要识别驱动疾病进展的相互作用因素。

Method: 使用有限元框架生成数百种异质性损伤的合成胸主动脉瘤，构建局部扩张和可扩张性的空间图，训练多种神经网络架构（DeepONet、UNet、Laplace神经算子）预测初始损伤。

Result: 所有网络仅使用扩张数据训练时预测误差显著更高，UNet在所有数据格式中始终提供最高准确度。

Conclusion: 在胸主动脉瘤评估中获取扩张和可扩张性的全场测量对于揭示疾病的机械生物学驱动因素至关重要，支持个性化治疗策略的开发。

Abstract: Thoracic aortic aneurysms (TAAs) arise from diverse mechanical and
mechanobiological disruptions to the aortic wall that increase the risk of
dissection or rupture. Evidence links TAA development to dysfunctions in the
aortic mechanotransduction axis, including loss of elastic fiber integrity and
cell-matrix connections. Because distinct insults create different mechanical
vulnerabilities, there is a critical need to identify interacting factors that
drive progression. Here, we use a finite element framework to generate
synthetic TAAs from hundreds of heterogeneous insults spanning varying degrees
of elastic fiber damage and impaired mechanosensing. From these simulations, we
construct spatial maps of localized dilatation and distensibility to train
neural networks that predict the initiating combined insult. We compare several
architectures (Deep Operator Networks, UNets, and Laplace Neural Operators) and
multiple input data formats to define a standard for future subject-specific
modeling. We also quantify predictive performance when networks are trained
using only geometric data (dilatation) versus both geometric and mechanical
data (dilatation plus distensibility). Across all networks, prediction errors
are significantly higher when trained on dilatation alone, underscoring the
added value of distensibility information. Among the tested models, UNet
consistently provides the highest accuracy across all data formats. These
findings highlight the importance of acquiring full-field measurements of both
dilatation and distensibility in TAA assessment to reveal the mechanobiological
drivers of disease and support the development of personalized treatment
strategies.

</details>


### [200] [Linking Process to Outcome: Conditional Reward Modeling for LLM Reasoning](https://arxiv.org/abs/2509.26578)
*Zheng Zhang,Ziwei Shan,Kaitao Song,Yexin Li,Kan Ren*

Main category: cs.LG

TL;DR: 提出条件奖励建模(CRM)方法，通过将LLM推理建模为时序过程，将每个推理步骤的奖励与最终结果明确关联，解决现有过程奖励模型中的信用分配模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型(PRMs)要么孤立处理推理步骤，无法捕捉步骤间依赖关系，要么难以将过程奖励与最终结果对齐，导致奖励信号不尊重时序因果关系，面临信用分配模糊问题。

Method: 提出条件奖励建模(CRM)，将LLM推理视为导向正确答案的时序过程。每个推理步骤的奖励不仅基于前序步骤，还明确与推理轨迹的最终结果关联。通过强制执行条件概率规则，捕捉推理步骤间的因果关系。

Result: 在Best-of-N采样、波束搜索和强化学习等实验中，CRM始终优于现有奖励模型，对奖励攻击更鲁棒，无需依赖基于真实值的可验证奖励即可实现稳定的下游改进。

Conclusion: CRM为增强LLM推理提供了一个原则性框架，通过一致的概型建模实现更可靠的跨样本比较，解决了信用分配模糊问题。

Abstract: Process Reward Models (PRMs) have emerged as a promising approach to enhance
the reasoning capabilities of large language models (LLMs) by guiding their
step-by-step reasoning toward a final answer. However, existing PRMs either
treat each reasoning step in isolation, failing to capture inter-step
dependencies, or struggle to align process rewards with the final outcome.
Consequently, the reward signal fails to respect temporal causality in
sequential reasoning and faces ambiguous credit assignment. These limitations
make downstream models vulnerable to reward hacking and lead to suboptimal
performance. In this work, we propose Conditional Reward Modeling (CRM) that
frames LLM reasoning as a temporal process leading to a correct answer. The
reward of each reasoning step is not only conditioned on the preceding steps
but also explicitly linked to the final outcome of the reasoning trajectory. By
enforcing conditional probability rules, our design captures the causal
relationships among reasoning steps, with the link to the outcome allowing
precise attribution of each intermediate step, thereby resolving credit
assignment ambiguity. Further, through this consistent probabilistic modeling,
the rewards produced by CRM enable more reliable cross-sample comparison.
Experiments across Best-of-N sampling, beam search and reinforcement learning
demonstrate that CRM consistently outperforms existing reward models, offering
a principled framework for enhancing LLM reasoning. In particular, CRM is more
robust to reward hacking and delivers stable downstream improvements without
relying on verifiable rewards derived from ground truth.

</details>


### [201] [Clarification as Supervision: Reinforcement Learning for Vision-Language Interfaces](https://arxiv.org/abs/2509.26594)
*John Gkountouras,Ivan Titov*

Main category: cs.LG

TL;DR: 提出AC-RL方法，通过交互训练视觉模型理解推理系统所需的信息，改善视觉数学推理任务中的信息缺失问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型为人类读者生成描述时，往往忽略推理系统所需的精确细节，导致推理失败不是由于推理能力限制，而是因为缺乏关键视觉信息。

Method: 采用自适应澄清强化学习(AC-RL)，通过训练过程中的澄清请求揭示信息差距，惩罚需要澄清的成功案例，促使模型生成更全面的初始描述。

Result: 在七个视觉数学推理基准测试中，AC-RL相比预训练基线平均准确率提高了4.4个百分点，分析显示如果允许澄清请求，可减少高达39%的澄清需求。

Conclusion: AC-RL证明仅通过交互学习，无需显式标注，就能有效学习视觉语言接口，将澄清视为一种隐式监督形式。

Abstract: Recent text-only models demonstrate remarkable mathematical reasoning
capabilities. Extending these to visual domains requires vision-language models
to translate images into text descriptions. However, current models, trained to
produce captions for human readers, often omit the precise details that
reasoning systems require. This creates an interface mismatch: reasoners often
fail not due to reasoning limitations but because they lack access to critical
visual information. We propose Adaptive-Clarification Reinforcement Learning
(AC-RL), which teaches vision models what information reasoners need through
interaction. Our key insight is that clarification requests during training
reveal information gaps; by penalizing success that requires clarification, we
create pressure for comprehensive initial captions that enable the reasoner to
solve the problem in a single pass. AC-RL improves average accuracy by 4.4
points over pretrained baselines across seven visual mathematical reasoning
benchmarks, and analysis shows it would cut clarification requests by up to 39%
if those were allowed. By treating clarification as a form of implicit
supervision, AC-RL demonstrates that vision-language interfaces can be
effectively learned through interaction alone, without requiring explicit
annotations.

</details>


### [202] [Uncertainty Quantification for Regression using Proper Scoring Rules](https://arxiv.org/abs/2509.26610)
*Alexander Fishkov,Kajetan Schweighofer,Mykyta Ielanskyi,Nikita Kotelevskii,Mohsen Guizani,Maxim Panov*

Main category: cs.LG

TL;DR: 提出了一个基于适当评分规则的统一回归不确定性量化框架，能够分解为偶然和认知不确定性，并在实际参数假设下推导出闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 机器学习预测的不确定性量化对于安全关键应用中的可靠决策至关重要，但目前的不确定性量化理论主要集中在分类任务，扩展到回归任务仍具有挑战性。

Method: 基于适当评分规则（如CRPS、对数、平方误差和二次评分）构建统一框架，在实际参数假设下推导闭式表达式，并使用模型集成进行估计。

Result: 推导出的不确定性度量自然分解为偶然和认知不确定性分量，恢复了基于预测方差和微分熵的流行回归UQ度量，并在合成和真实数据集上进行了广泛评估。

Conclusion: 该框架为回归任务提供了可靠的不确定性量化方法，并为选择合适的UQ度量提供了指导。

Abstract: Quantifying uncertainty of machine learning model predictions is essential
for reliable decision-making, especially in safety-critical applications.
Recently, uncertainty quantification (UQ) theory has advanced significantly,
building on a firm basis of learning with proper scoring rules. However, these
advances were focused on classification, while extending these ideas to
regression remains challenging. In this work, we introduce a unified UQ
framework for regression based on proper scoring rules, such as CRPS,
logarithmic, squared error, and quadratic scores. We derive closed-form
expressions for the resulting uncertainty measures under practical parametric
assumptions and show how to estimate them using ensembles of models. In
particular, the derived uncertainty measures naturally decompose into aleatoric
and epistemic components. The framework recovers popular regression UQ measures
based on predictive variance and differential entropy. Our broad evaluation on
synthetic and real-world regression datasets provides guidance for selecting
reliable UQ measures.

</details>


### [203] [Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training](https://arxiv.org/abs/2509.26625)
*Junlin Han,Shengbang Tong,David Fan,Yufan Ren,Koustuv Sinha,Philip Torr,Filippos Kokkinos*

Main category: cs.LG

TL;DR: LLMs通过纯文本预训练意外地发展出丰富的视觉先验知识，这些先验知识由可分离的感知先验和推理先验组成，具有不同的扩展趋势和来源。


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs在纯文本预训练中如何发展视觉能力，理解视觉先验的组成和来源，为构建下一代多模态LLMs提供指导。

Method: 通过100多个对照实验（消耗50万GPU小时），涵盖完整的MLLM构建流程，包括LLM预训练、视觉对齐和监控多模态微调，分析不同数据类别和模型规模的影响。

Result: 发现推理先验主要由推理中心数据（代码、数学、学术）发展而来，可迁移到视觉推理；感知先验则更广泛地来自语料库，对视觉编码器和视觉指令调优数据更敏感。

Conclusion: 提出了一种基于数据的预训练视觉感知LLMs的方法，为有意识地培养语言预训练中的视觉先验提供了新途径。

Abstract: Large Language Models (LLMs), despite being trained on text alone,
surprisingly develop rich visual priors. These priors allow latent visual
capabilities to be unlocked for vision tasks with a relatively small amount of
multimodal data, and in some cases, to perform visual tasks without ever having
seen an image. Through systematic analysis, we reveal that visual priors-the
implicit, emergent knowledge about the visual world acquired during language
pre-training-are composed of separable perception and reasoning priors with
unique scaling trends and origins. We show that an LLM's latent visual
reasoning ability is predominantly developed by pre-training on
reasoning-centric data (e.g., code, math, academia) and scales progressively.
This reasoning prior acquired from language pre-training is transferable and
universally applicable to visual reasoning. In contrast, a perception prior
emerges more diffusely from broad corpora, and perception ability is more
sensitive to the vision encoder and visual instruction tuning data. In
parallel, text describing the visual world proves crucial, though its
performance impact saturates rapidly. Leveraging these insights, we propose a
data-centric recipe for pre-training vision-aware LLMs and verify it in 1T
token scale pre-training. Our findings are grounded in over 100 controlled
experiments consuming 500,000 GPU-hours, spanning the full MLLM construction
pipeline-from LLM pre-training to visual alignment and supervised multimodal
fine-tuning-across five model scales, a wide range of data categories and
mixtures, and multiple adaptation setups. Along with our main findings, we
propose and investigate several hypotheses, and introduce the Multi-Level
Existence Bench (MLE-Bench). Together, this work provides a new way of
deliberately cultivating visual priors from language pre-training, paving the
way for the next generation of multimodal LLMs.

</details>


### [204] [Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models](https://arxiv.org/abs/2509.26626)
*Siddarth Venkatraman,Vineet Jain,Sarthak Mittal,Vedant Shah,Johan Obando-Ceron,Yoshua Bengio,Brian R. Bartoldson,Bhavya Kailkhura,Guillaume Lajoie,Glen Berseth,Nikolay Malkin,Moksh Jain*

Main category: cs.LG

TL;DR: 提出递归自聚合（RSA）方法，结合并行和顺序推理扩展的优势，通过迭代聚合候选推理链来提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法要么采用并行推理（选择多个独立解），要么采用顺序推理（自我精炼），但未能充分利用推理链中的丰富信息。RSA旨在结合两者的优势，通过利用推理链中的中间步骤信息来提升模型性能。

Method: RSA通过迭代方式精炼候选推理链：每一步聚合子集以产生改进的解群，作为下一轮迭代的候选池。该方法利用推理链中的丰富信息（不仅是最终答案），并能从不同思维链中的部分正确中间步骤进行引导。

Result: RSA在不同任务、模型家族和规模上均带来显著性能提升，特别是在增加计算预算时。Qwen3-4B-Instruct-2507模型使用RSA后能与更大的推理模型（如DeepSeek-R1和o3-mini）竞争，并在多个基准测试中优于纯并行和顺序扩展策略。

Conclusion: RSA是一种有效的测试时扩展方法，结合了并行和顺序推理的优势。通过聚合感知的强化学习训练模型结合解能带来显著性能提升，证明了利用推理链中间信息的重要性。

Abstract: Test-time scaling methods improve the capabilities of large language models
(LLMs) by increasing the amount of compute used during inference to make a
prediction. Inference-time compute can be scaled in parallel by choosing among
multiple independent solutions or sequentially through self-refinement. We
propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired
by evolutionary methods that combines the benefits of both parallel and
sequential scaling. Each step of RSA refines a population of candidate
reasoning chains through aggregation of subsets to yield a population of
improved solutions, which are then used as the candidate pool for the next
iteration. RSA exploits the rich information embedded in the reasoning chains
-- not just the final answers -- and enables bootstrapping from partially
correct intermediate steps within different chains of thought. Empirically, RSA
delivers substantial performance gains with increasing compute budgets across
diverse tasks, model families and sizes. Notably, RSA enables
Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning
models, including DeepSeek-R1 and o3-mini (high), while outperforming purely
parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning
Gym, LiveCodeBench-v6, and SuperGPQA. We further demonstrate that training the
model to combine solutions via a novel aggregation-aware reinforcement learning
approach yields significant performance gains. Code available at
https://github.com/HyperPotatoNeo/RSA.

</details>


### [205] [Attention as a Compass: Efficient Exploration for Process-Supervised RL in Reasoning Models](https://arxiv.org/abs/2509.26628)
*Runze Liu,Jiakang Wang,Yuling Shi,Zhihui Xie,Chenxin An,Kaiyan Zhang,Jian Zhao,Xiaodong Gu,Lei Lin,Wenping Hu,Xiu Li,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.LG

TL;DR: 提出了AttnRL框架，通过注意力机制实现高效探索的过程监督强化学习，在数学推理任务上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有过程监督强化学习方法在分支位置和采样方面探索效率有限，需要更高效的探索策略

Method: 基于注意力分数选择高价值位置进行分支，采用自适应采样策略考虑问题难度和历史批次大小，设计一步离策略训练流程

Result: 在多个数学推理基准测试中，该方法在性能、采样和训练效率方面均优于现有方法

Conclusion: AttnRL框架通过注意力引导的探索策略有效提升了过程监督强化学习的效率和性能

Abstract: Reinforcement Learning (RL) has shown remarkable success in enhancing the
reasoning capabilities of Large Language Models (LLMs). Process-Supervised RL
(PSRL) has emerged as a more effective paradigm compared to outcome-based RL.
However, existing PSRL approaches suffer from limited exploration efficiency,
both in terms of branching positions and sampling. In this paper, we introduce
a novel PSRL framework (AttnRL), which enables efficient exploration for
reasoning models. Motivated by preliminary observations that steps exhibiting
high attention scores correlate with reasoning behaviors, we propose to branch
from positions with high values. Furthermore, we develop an adaptive sampling
strategy that accounts for problem difficulty and historical batch size,
ensuring that the whole training batch maintains non-zero advantage values. To
further improve sampling efficiency, we design a one-step off-policy training
pipeline for PSRL. Extensive experiments on multiple challenging mathematical
reasoning benchmarks demonstrate that our method consistently outperforms prior
approaches in terms of performance and sampling and training efficiency.

</details>


### [206] [AccidentBench: Benchmarking Multimodal Understanding and Reasoning in Vehicle Accidents and Beyond](https://arxiv.org/abs/2509.26636)
*Shangding Gu,Xiaohan Wang,Donghao Ying,Haoyu Zhao,Runing Yang,Ming Jin,Boyi Li,Marco Pavone,Serena Yeung-Levy,Jun Wang,Dawn Song,Costas Spanos*

Main category: cs.LG

TL;DR: AccidentBench是一个大规模多模态基准测试，包含约2000个视频和19000个标注问答对，用于评估模型在安全关键场景中的时空推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型需要能够严格评估在安全关键、动态现实场景中的理解和推理能力的基准测试。

Method: 结合车辆事故场景与空中和水上安全关键场景，系统性地测试时间、空间和意图理解推理能力，涵盖不同视频长度和难度级别。

Result: 评估显示即使最先进的模型（如Gemini-2.5 Pro和GPT-5）在最难任务和最长视频上的准确率仅约18%，揭示了在现实世界时空推理方面的显著差距。

Conclusion: AccidentBench旨在暴露这些关键差距，推动开发更安全、更稳健、更符合现实世界安全挑战的多模态模型。

Abstract: Rapid advances in multimodal models demand benchmarks that rigorously
evaluate understanding and reasoning in safety-critical, dynamic real-world
settings. We present AccidentBench, a large-scale benchmark that combines
vehicle accident scenarios with Beyond domains, safety-critical settings in air
and water that emphasize spatial and temporal reasoning (e.g., navigation,
orientation, multi-vehicle motion). The benchmark contains approximately 2000
videos and over 19000 human-annotated question--answer pairs spanning multiple
video lengths (short/medium/long) and difficulty levels (easy/medium/hard).
Tasks systematically probe core capabilities: temporal, spatial, and intent
understanding and reasoning. By unifying accident-centric traffic scenes with
broader safety-critical scenarios in air and water, AccidentBench offers a
comprehensive, physically grounded testbed for evaluating models under
real-world variability. Evaluations of state-of-the-art models (e.g.,
Gemini-2.5 Pro and GPT-5) show that even the strongest models achieve only
about 18% accuracy on the hardest tasks and longest videos, revealing
substantial gaps in real-world temporal, spatial, and intent reasoning.
AccidentBench is designed to expose these critical gaps and drive the
development of multimodal models that are safer, more robust, and better
aligned with real-world safety-critical challenges. The code and dataset are
available at: https://github.com/SafeRL-Lab/AccidentBench

</details>


### [207] [SPATA: Systematic Pattern Analysis for Detailed and Transparent Data Cards](https://arxiv.org/abs/2509.26640)
*João Vitorino,Eva Maia,Isabel Praça,Carlos Soares*

Main category: cs.LG

TL;DR: SPATA是一种确定性方法，可将任何表格数据集转换为统计模式的领域无关表示，用于创建更详细透明的数据卡片，支持在不泄露数据的情况下进行AI模型鲁棒性评估。


<details>
  <summary>Details</summary>
Motivation: AI模型易受数据扰动和对抗样本影响，需要鲁棒性评估，但传统方法需要访问训练和测试数据集，这会危及数据隐私和机密性。需要一种允许外部验证AI而不泄露私有数据集的方法。

Method: 提出系统性模式分析(SPATA)方法，将每个数据实例投影到离散空间中进行分析和比较，避免数据泄露。这些投影数据集可用于评估不同特征如何影响ML模型鲁棒性，并生成可解释的行为解释。

Result: SPATA能够提供领域无关的统计模式表示，支持在不暴露原始数据的情况下进行AI模型验证和鲁棒性评估。

Conclusion: SPATA方法有助于提高处理机密数据或关键基础设施组织的透明度，通过提供更详细透明的数据卡片，为更可信的AI做出贡献。

Abstract: Due to the susceptibility of Artificial Intelligence (AI) to data
perturbations and adversarial examples, it is crucial to perform a thorough
robustness evaluation before any Machine Learning (ML) model is deployed.
However, examining a model's decision boundaries and identifying potential
vulnerabilities typically requires access to the training and testing datasets,
which may pose risks to data privacy and confidentiality. To improve
transparency in organizations that handle confidential data or manage critical
infrastructure, it is essential to allow external verification and validation
of AI without the disclosure of private datasets. This paper presents
Systematic Pattern Analysis (SPATA), a deterministic method that converts any
tabular dataset to a domain-independent representation of its statistical
patterns, to provide more detailed and transparent data cards. SPATA computes
the projection of each data instance into a discrete space where they can be
analyzed and compared, without risking data leakage. These projected datasets
can be reliably used for the evaluation of how different features affect ML
model robustness and for the generation of interpretable explanations of their
behavior, contributing to more trustworthy AI.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [208] [Neural Optimal Transport Meets Multivariate Conformal Prediction](https://arxiv.org/abs/2509.25444)
*Vladimir Kondratyev,Alexander Fishkov,Nikita Kotelevskii,Mahmoud Hegazy,Remi Flamary,Maxim Panov,Eric Moulines*

Main category: stat.ML

TL;DR: 提出条件向量分位数回归框架，结合神经最优传输和摊销优化，应用于多元保形预测，相比坐标方法能适应条件分布几何形状，产生更紧密的预测区域


<details>
  <summary>Details</summary>
Motivation: 经典分位数回归无法自然扩展到多元响应，现有方法常忽略联合分布的几何结构，需要开发能适应条件分布几何形状的多元保形预测方法

Method: 将条件向量分位数函数参数化为输入凸神经网络的凸势梯度，确保单调性和均匀秩；引入对偶势的摊销优化来降低高维变分问题求解成本

Result: 在基准数据集上实验显示，相比基线方法在覆盖效率权衡方面有改进，产生更紧密和更具信息量的预测区域

Conclusion: 将神经最优传输与保形预测相结合具有显著优势，能够构建具有有限样本有效性的无分布预测区域，适应条件分布的几何特性

Abstract: We propose a framework for conditional vector quantile regression (CVQR) that
combines neural optimal transport with amortized optimization, and apply it to
multivariate conformal prediction. Classical quantile regression does not
extend naturally to multivariate responses, while existing approaches often
ignore the geometry of joint distributions. Our method parametrizes the
conditional vector quantile function as the gradient of a convex potential
implemented by an input-convex neural network, ensuring monotonicity and
uniform ranks. To reduce the cost of solving high-dimensional variational
problems, we introduced amortized optimization of the dual potentials, yielding
efficient training and faster inference. We then exploit the induced
multivariate ranks for conformal prediction, constructing distribution-free
predictive regions with finite-sample validity. Unlike coordinatewise methods,
our approach adapts to the geometry of the conditional distribution, producing
tighter and more informative regions. Experiments on benchmark datasets show
improved coverage-efficiency trade-offs compared to baselines, highlighting the
benefits of integrating neural optimal transport with conformal prediction.

</details>


### [209] [Fair Classification by Direct Intervention on Operating Characteristics](https://arxiv.org/abs/2509.25481)
*Kevin Jiang,Edgar Dobriban*

Main category: stat.ML

TL;DR: 提出了一种基于干预预训练分类器操作特性的新方法，用于在属性感知设置下满足多个群体公平性约束（如DP、EO、PP），通过ROC凸包识别最优操作特性并进行后处理。


<details>
  <summary>Details</summary>
Motivation: 解决在二进制分类中同时满足多个群体公平性约束（如人口统计均等、均等化几率、预测均等）的挑战，特别是在属性感知设置下。

Method: 基于预训练基分类器的组间ROC凸包识别最优操作特性，然后通过随机化组间阈值规则的后处理来匹配这些目标，最小化干预次数。

Result: 在标准数据集（COMPAS和ACSIncome）上，方法能同时近似满足DP、EO和PP约束，干预次数少且准确率下降接近最优水平，优于先前方法。

Conclusion: 提出的方法能有效处理多个保护属性和多个线性分数约束，在保持高准确性的同时实现群体公平性。

Abstract: We develop new classifiers under group fairness in the attribute-aware
setting for binary classification with multiple group fairness constraints
(e.g., demographic parity (DP), equalized odds (EO), and predictive parity
(PP)). We propose a novel approach, applicable to linear fractional
constraints, based on directly intervening on the operating characteristics of
a pre-trained base classifier, by (i) identifying optimal operating
characteristics using the base classifier's group-wise ROC convex hulls and
(ii) post-processing the base classifier to match those targets. As practical
post-processors, we consider randomizing a mixture of group-wise thresholding
rules subject to minimizing the expected number of interventions. We further
extend our approach to handle multiple protected attributes and multiple linear
fractional constraints. On standard datasets (COMPAS and ACSIncome), our
methods simultaneously satisfy approximate DP, EO, and PP with few
interventions and a near-oracle drop in accuracy; comparing favorably to
previous methods.

</details>


### [210] [One-shot Conditional Sampling: MMD meets Nearest Neighbors](https://arxiv.org/abs/2509.25507)
*Anirban Chatterjee,Sayantan Choudhury,Rohan Hore*

Main category: stat.ML

TL;DR: 提出了CGMMD框架，用于从未完全观测的条件分布中生成样本，通过直接最小化问题实现对抗器自由训练，支持单次前向生成


<details>
  <summary>Details</summary>
Motivation: 解决在图像后处理、基于模拟的推理和复杂数据条件分布建模等应用中，如何从未完全观测的条件分布中生成样本的问题

Method: 基于最大均值差异(MMD)的条件生成器框架，将训练目标构建为简单的对抗器自由直接最小化问题

Result: 建立了CGMMD采样器损失的严格理论界限，证明了估计分布向真实条件分布的收敛性，并在合成任务和图像去噪、超分辨率等应用中表现出竞争力

Conclusion: CGMMD提供了一种理论保证且实用的条件采样方法，具有低测试时间复杂度和单次前向生成能力

Abstract: How can we generate samples from a conditional distribution that we never
fully observe? This question arises across a broad range of applications in
both modern machine learning and classical statistics, including image
post-processing in computer vision, approximate posterior sampling in
simulation-based inference, and conditional distribution modeling in complex
data settings. In such settings, compared with unconditional sampling,
additional feature information can be leveraged to enable more adaptive and
efficient sampling. Building on this, we introduce Conditional Generator using
MMD (CGMMD), a novel framework for conditional sampling. Unlike many
contemporary approaches, our method frames the training objective as a simple,
adversary-free direct minimization problem. A key feature of CGMMD is its
ability to produce conditional samples in a single forward pass of the
generator, enabling practical one-shot sampling with low test-time complexity.
We establish rigorous theoretical bounds on the loss incurred when sampling
from the CGMMD sampler, and prove convergence of the estimated distribution to
the true conditional distribution. In the process, we also develop a uniform
concentration result for nearest-neighbor based functionals, which may be of
independent interest. Finally, we show that CGMMD performs competitively on
synthetic tasks involving complex conditional densities, as well as on
practical applications such as image denoising and image super-resolution.

</details>


### [211] [Conservative Decisions with Risk Scores](https://arxiv.org/abs/2509.25588)
*Yishu Wei,Wen-Yee Lee,George Ekow Quaye,Xiaogang Su*

Main category: stat.ML

TL;DR: 提出了一种基于风险评分的最优截断区间方法，允许在不确定时弃权，在区间外最大化分类准确率。该方法受SVM启发但最小化分类间隔，支持保守决策并生成风险-覆盖率曲线作为评估指标。


<details>
  <summary>Details</summary>
Motivation: 在二元分类中，允许弃权的保守决策策略具有优势。需要一种方法能够确定风险评分的最优截断区间，在不确定时弃权，同时在确定情况下最大化分类准确率。

Method: 基于风险评分确定最优截断区间，区间内弃权，区间外最大化分类准确率。受SVM启发但最小化分类间隔而非最大化，提供了理论最优解。

Result: 方法支持保守决策并生成风险-覆盖率曲线，结合AUC可作为类似ROC曲线的综合性能评估指标。通过模拟研究和前列腺癌诊断案例验证了方法有效性。

Conclusion: 提出的最优截断区间方法为二元分类提供了有效的保守决策框架，风险-覆盖率曲线为分类器评估提供了新的综合性能指标。

Abstract: In binary classification applications, conservative decision-making that
allows for abstention can be advantageous. To this end, we introduce a novel
approach that determines the optimal cutoff interval for risk scores, which can
be directly available or derived from fitted models. Within this interval, the
algorithm refrains from making decisions, while outside the interval,
classification accuracy is maximized. Our approach is inspired by support
vector machines (SVM), but differs in that it minimizes the classification
margin rather than maximizing it. We provide the theoretical optimal solution
to this problem, which holds important practical implications. Our proposed
method not only supports conservative decision-making but also inherently
results in a risk-coverage curve. Together with the area under the curve (AUC),
this curve can serve as a comprehensive performance metric for evaluating and
comparing classifiers, akin to the receiver operating characteristic (ROC)
curve. To investigate and illustrate our approach, we conduct both simulation
studies and a real-world case study in the context of diagnosing prostate
cancer.

</details>


### [212] [Coupling Generative Modeling and an Autoencoder with the Causal Bridge](https://arxiv.org/abs/2509.25599)
*Ruolin Meng,Ming-Yu Chung,Dhanajit Brahma,Ricardo Henao,Lawrence Carin*

Main category: stat.ML

TL;DR: 本文提出了一种在存在未观测混杂因素的情况下估计因果效应的方法，通过使用因果桥函数结合自编码器架构来改进估计质量。


<details>
  <summary>Details</summary>
Motivation: 在因果推断中，当存在同时影响治疗和结果的未观测混杂因素时，传统的因果效应估计方法会失效。本文旨在解决这一挑战。

Method: 使用两个独立的控制（代理）测量集，通过因果桥函数估计治疗效应，并结合自编码器架构在观测变量（代理、治疗和结果）之间共享统计强度。

Result: 在合成和真实世界数据上的实验表明，该方法在代理测量方面优于现有最先进方法。

Conclusion: 提出的因果桥与自编码器结合的方法能够有效处理未观测混杂问题，提高因果效应估计的准确性。

Abstract: We consider inferring the causal effect of a treatment (intervention) on an
outcome of interest in situations where there is potentially an unobserved
confounder influencing both the treatment and the outcome. This is achievable
by assuming access to two separate sets of control (proxy) measurements
associated with treatment and outcomes, which are used to estimate treatment
effects through a function termed the em causal bridge (CB). We present a new
theoretical perspective, associated assumptions for when estimating treatment
effects with the CB is feasible, and a bound on the average error of the
treatment effect when the CB assumptions are violated. From this new
perspective, we then demonstrate how coupling the CB with an autoencoder
architecture allows for the sharing of statistical strength between observed
quantities (proxies, treatment, and outcomes), thus improving the quality of
the CB estimates. Experiments on synthetic and real-world data demonstrate the
effectiveness of the proposed approach in relation to the state-of-the-art
methodology for proxy measurements.

</details>


### [213] [When Langevin Monte Carlo Meets Randomization: Non-asymptotic Error Bounds beyond Log-Concavity and Gradient Lipschitzness](https://arxiv.org/abs/2509.25630)
*Xiaojie Wang,Bin Yang*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Efficient sampling from complex and high dimensional target distributions
turns out to be a fundamental task in diverse disciplines such as scientific
computing, statistics and machine learning. In this paper, we revisit the
randomized Langevin Monte Carlo (RLMC) for sampling from high dimensional
distributions without log-concavity. Under the gradient Lipschitz condition and
the log-Sobolev inequality, we prove a uniform-in-time error bound in
$\mathcal{W}_2$-distance of order $O(\sqrt{d}h)$ for the RLMC sampling
algorithm, which matches the best one in the literature under the log-concavity
condition. Moreover, when the gradient of the potential $U$ is non-globally
Lipschitz with superlinear growth, modified RLMC algorithms are proposed and
analyzed, with non-asymptotic error bounds established. To the best of our
knowledge, the modified RLMC algorithms and their non-asymptotic error bounds
are new in the non-globally Lipschitz setting.

</details>


### [214] [Test time training enhances in-context learning of nonlinear functions](https://arxiv.org/abs/2509.25741)
*Kento Kuwataka,Taiji Suzuki*

Main category: stat.ML

TL;DR: 本文研究了测试时训练与上下文学习结合的理论基础，证明了单层transformer通过TTT能够适应任务中的特征向量和链接函数变化，而单独的ICL难以适应链接函数偏移。


<details>
  <summary>Details</summary>
Motivation: 测试时训练在实践中表现出色但理论支撑不足，特别是在非线性模型方面。本文旨在为TTT与ICL结合提供理论分析，揭示其适应能力。

Method: 在单指数模型框架下，分析单层transformer使用基于梯度的测试时训练算法，建立预测风险的上界。

Result: 理论证明TTT使模型能够同时适应特征向量β和链接函数σ*的变化，且随着上下文长度和网络宽度增加，预测误差可逼近噪声水平。

Conclusion: TTT与ICL结合在理论上优于单独的ICL，能够有效处理链接函数偏移问题，为测试时训练提供了坚实的理论依据。

Abstract: Test-time training (TTT) enhances model performance by explicitly updating
designated parameters prior to each prediction to adapt to the test data. While
TTT has demonstrated considerable empirical success, its theoretical
underpinnings remain limited, particularly for nonlinear models. In this paper,
we investigate the combination of TTT with in-context learning (ICL), where the
model is given a few examples from the target distribution at inference time.
We analyze this framework in the setting of single-index models
$y=\sigma_*(\langle \beta, \mathbf{x} \rangle)$, where the feature vector
$\beta$ is drawn from a hidden low-dimensional subspace. For single-layer
transformers trained with gradient-based algorithms and adopting TTT, we
establish an upper bound on the prediction risk. Our theory reveals that TTT
enables the single-layer transformers to adapt to both the feature vector
$\beta$ and the link function $\sigma_*$, which vary across tasks. This creates
a sharp contrast with ICL alone, which is theoretically difficult to adapt to
shifts in the link function. Moreover, we provide the convergence rate with
respect to the data length, showing the predictive error can be driven
arbitrarily close to the noise level as the context size and the network width
grow.

</details>


### [215] [Sharpness of Minima in Deep Matrix Factorization: Exact Expressions](https://arxiv.org/abs/2509.25783)
*Anil Kamber,Rahul Parhi*

Main category: stat.ML

TL;DR: 本文首次提出了在过参数化深度矩阵分解（即深度线性神经网络训练）问题中，平方误差损失Hessian矩阵最大特征值的精确表达式，解决了Mulayoff & Michaeli (2020)提出的开放性问题。


<details>
  <summary>Details</summary>
Motivation: 理解损失函数在最小值附近的几何形状对于解释梯度方法在非凸优化问题（如深度神经网络训练）中的隐式偏差至关重要。Hessian矩阵的最大特征值作为衡量损失函数曲面尖锐度的关键量，其精确作用一直因缺乏一般情况下的精确表达式而被掩盖。

Method: 通过理论分析推导出在一般过参数化深度矩阵分解问题中，平方误差损失Hessian矩阵最大特征值的精确表达式，并通过实证研究验证基于该尖锐度表达式的梯度训练逃逸现象。

Result: 获得了深度矩阵分解问题中Hessian矩阵最大特征值的第一个精确表达式，并实证观察到梯度训练在最小值附近的逃逸现象，该现象依赖于尖锐度的精确表达式。

Conclusion: 本文解决了深度矩阵分解中尖锐度测量的开放性问题，为理解梯度方法在非凸优化中的行为提供了新的理论工具，并通过实证验证了尖锐度在训练动态中的关键作用。

Abstract: Understanding the geometry of the loss landscape near a minimum is key to
explaining the implicit bias of gradient-based methods in non-convex
optimization problems such as deep neural network training and deep matrix
factorization. A central quantity to characterize this geometry is the maximum
eigenvalue of the Hessian of the loss, which measures the sharpness of the
landscape. Currently, its precise role has been obfuscated because no exact
expressions for this sharpness measure were known in general settings. In this
paper, we present the first exact expression for the maximum eigenvalue of the
Hessian of the squared-error loss at any minimizer in general overparameterized
deep matrix factorization (i.e., deep linear neural network training) problems,
resolving an open question posed by Mulayoff & Michaeli (2020). To complement
our theory, we empirically investigate an escape phenomenon observed during
gradient-based training near a minimum that crucially relies on our exact
expression of the sharpness.

</details>


### [216] [Graph Distribution-valued Signals: A Wasserstein Space Perspective](https://arxiv.org/abs/2509.25802)
*Yanan Zhao,Feng Ji,Xingchao Jian,Wee Peng Tay*

Main category: stat.ML

TL;DR: 提出了一种新颖的图信号处理框架，将信号建模为图分布值信号（GDSs），即Wasserstein空间中的概率分布，克服了传统向量基GSP的局限性。


<details>
  <summary>Details</summary>
Motivation: 克服传统图信号处理的三个关键限制：顶点同步观测假设、无法捕捉不确定性、图滤波需要严格对应关系。通过将信号表示为分布，GDSs能自然编码不确定性和随机性。

Method: 建立图分布值信号框架，将信号建模为Wasserstein空间中的概率分布，并系统性地将核心GSP概念映射到对应的GDS版本。

Result: 该框架严格推广了传统图信号，经典定义成为特例。通过图滤波学习预测任务的实验验证了框架的有效性。

Conclusion: 提出的GDS框架为图信号处理提供了更灵活和强大的理论基础，能够更好地处理不确定性和异步观测等现实问题。

Abstract: We introduce a novel framework for graph signal processing (GSP) that models
signals as graph distribution-valued signals (GDSs), which are probability
distributions in the Wasserstein space. This approach overcomes key limitations
of classical vector-based GSP, including the assumption of synchronous
observations over vertices, the inability to capture uncertainty, and the
requirement for strict correspondence in graph filtering. By representing
signals as distributions, GDSs naturally encode uncertainty and stochasticity,
while strictly generalizing traditional graph signals. We establish a
systematic dictionary mapping core GSP concepts to their GDS counterparts,
demonstrating that classical definitions are recovered as special cases. The
effectiveness of the framework is validated through graph filter learning for
prediction tasks, supported by experimental results.

</details>


### [217] [BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories under Spatio-Temporal Vector Fields](https://arxiv.org/abs/2509.26005)
*Rui-Yang Zhang,Henry B. Moss,Lachlan Astfalck,Edward Cripps,David S. Leslie*

Main category: stat.ML

TL;DR: 提出了一种名为BALLAST的贝叶斯主动学习方法，用于指导拉格朗日观测器的放置，以推断时变矢量场，在合成和高保真洋流模型中显示出显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有的观测器放置策略大多采用标准的空间填充设计或相对临时的专家意见，缺乏系统性的主动学习方法。拉格朗日观测器在矢量场中连续移动，在不同位置和时间进行测量，因此需要考虑观测器的未来轨迹。

Method: 使用物理信息时空高斯过程代理模型，提出BALLAST方法（贝叶斯主动学习与前瞻修正的海漂轨迹），考虑候选放置位置的未来效用。

Result: 在合成和高保真洋流模型中，BALLAST辅助的顺序观测器放置策略显示出明显优势。

Conclusion: BALLAST方法为海洋学、海洋科学和海洋工程中的矢量场推断提供了一种有效的主动学习框架，能够显著改善观测器放置策略。

Abstract: We introduce a formal active learning methodology for guiding the placement
of Lagrangian observers to infer time-dependent vector fields -- a key task in
oceanography, marine science, and ocean engineering -- using a physics-informed
spatio-temporal Gaussian process surrogate model. The majority of existing
placement campaigns either follow standard `space-filling' designs or
relatively ad-hoc expert opinions. A key challenge to applying principled
active learning in this setting is that Lagrangian observers are continuously
advected through the vector field, so they make measurements at different
locations and times. It is, therefore, important to consider the likely future
trajectories of placed observers to account for the utility of candidate
placement locations. To this end, we present BALLAST: Bayesian Active Learning
with Look-ahead Amendment for Sea-drifter Trajectories. We observe noticeable
benefits of BALLAST-aided sequential observer placement strategies on both
synthetic and high-fidelity ocean current models.

</details>


### [218] [Non-Vacuous Generalization Bounds: Can Rescaling Invariances Help?](https://arxiv.org/abs/2509.26149)
*Damien Rouchouse,Antoine Gonon,Rémi Gribonval,Benjamin Guedj*

Main category: stat.ML

TL;DR: 提出在不变性提升表示中研究PAC-Bayes边界，解决ReLU网络中的重缩放不变性问题，提供更紧的数据相关泛化保证。


<details>
  <summary>Details</summary>
Motivation: ReLU网络中的重缩放不变性导致不同权重分布可能表示相同函数，但PAC-Bayes复杂度却任意不同，需要解决这一差异。

Method: 在不变性提升表示中研究PAC-Bayes边界，使用KL散度基的重缩放不变PAC-Bayes边界方法。

Result: 该方法提供了不变性保证，并通过数据处理获得更紧的边界。

Conclusion: 不变性提升表示能有效解决PAC-Bayes边界在ReLU网络中的重缩放不变性问题，提供更紧的泛化保证。

Abstract: A central challenge in understanding generalization is to obtain non-vacuous
guarantees that go beyond worst-case complexity over data or weight space.
Among existing approaches, PAC-Bayes bounds stand out as they can provide
tight, data-dependent guarantees even for large networks. However, in ReLU
networks, rescaling invariances mean that different weight distributions can
represent the same function while leading to arbitrarily different PAC-Bayes
complexities. We propose to study PAC-Bayes bounds in an invariant, lifted
representation that resolves this discrepancy. This paper explores both the
guarantees provided by this approach (invariance, tighter bounds via data
processing) and the algorithmic aspects of KL-based rescaling-invariant
PAC-Bayes bounds.

</details>


### [219] [Spectral gap of Metropolis-within-Gibbs under log-concavity](https://arxiv.org/abs/2509.26175)
*Cecilia Secchi,Giacomo Zanella*

Main category: stat.ML

TL;DR: 该论文研究了Metropolis-within-Gibbs算法在方差自适应提议下的混合性能，将谱间隙下界从O(1/κ²d)改进到O(1/κd)，表明该算法仅比精确Gibbs采样器差一个常数因子。


<details>
  <summary>Details</summary>
Motivation: 研究MwG算法在高维分布采样中的性能，特别是当精确条件采样不可行时，使用方差自适应提议策略是否能显著改善混合速度。

Method: 使用随机游走Metropolis更新，提议方差调整为目标条件方差，分析一维RWM核的传导性，建立谱间隙下界。

Result: 获得了O(1/κd)的谱间隙下界，比之前的O(1/κ²d)有显著改进，表明MwG在方差自适应提议下混合更快。

Conclusion: MwG算法在方差自适应提议下可以显著改善混合性能，其表现仅比精确Gibbs采样器差一个常数因子，为观察到的经验行为提供了理论支持。

Abstract: The Metropolis-within-Gibbs (MwG) algorithm is a widely used Markov Chain
Monte Carlo method for sampling from high-dimensional distributions when exact
conditional sampling is intractable. We study MwG with Random Walk Metropolis
(RWM) updates, using proposal variances tuned to match the target's conditional
variances. Assuming the target $\pi$ is a $d$-dimensional log-concave
distribution with condition number $\kappa$, we establish a spectral gap lower
bound of order $\mathcal{O}(1/\kappa d)$ for the random-scan version of MwG,
improving on the previously available $\mathcal{O}(1/\kappa^2 d)$ bound. This
is obtained by developing sharp estimates of the conductance of one-dimensional
RWM kernels, which can be of independent interest. The result shows that MwG
can mix substantially faster with variance-adaptive proposals and that its
mixing performance is just a constant factor worse than that of the exact Gibbs
sampler, thus providing theoretical support to previously observed empirical
behavior.

</details>


### [220] [An Orthogonal Learner for Individualized Outcomes in Markov Decision Processes](https://arxiv.org/abs/2509.26429)
*Emil Javurek,Valentyn Melnychuk,Jonas Schweisthal,Konstantin Hess,Dennis Frauen,Stefan Feuerriegel*

Main category: stat.ML

TL;DR: 提出了一个名为DRQ-learner的新元学习器，用于在序列决策中预测个性化潜在结果，具有双重鲁棒性、Neyman正交性和准oracle效率等理论保证。


<details>
  <summary>Details</summary>
Motivation: 在个性化医疗等序列决策场景中，预测长期潜在结果具有挑战性，现有方法缺乏正交性和准oracle效率等理论保证。

Method: 通过因果推断视角重新审视序列决策中的潜在结果预测问题，开发了具有理论基础的元学习器DRQ-learner，适用于离散和连续状态空间。

Result: 数值实验验证了DRQ-learner的理论结果，显示其优于最先进的基线方法。

Conclusion: DRQ-learner为序列决策中的个性化结果预测提供了具有强大理论保证的灵活解决方案。

Abstract: Predicting individualized potential outcomes in sequential decision-making is
central for optimizing therapeutic decisions in personalized medicine (e.g.,
which dosing sequence to give to a cancer patient). However, predicting
potential outcomes over long horizons is notoriously difficult. Existing
methods that break the curse of the horizon typically lack strong theoretical
guarantees such as orthogonality and quasi-oracle efficiency. In this paper, we
revisit the problem of predicting individualized potential outcomes in
sequential decision-making (i.e., estimating Q-functions in Markov decision
processes with observational data) through a causal inference lens. In
particular, we develop a comprehensive theoretical foundation for meta-learners
in this setting with a focus on beneficial theoretical properties. As a result,
we yield a novel meta-learner called DRQ-learner and establish that it is: (1)
doubly robust (i.e., valid inference under the misspecification of one of the
nuisances), (2) Neyman-orthogonal (i.e., insensitive to first-order estimation
errors in the nuisance functions), and (3) achieves quasi-oracle efficiency
(i.e., behaves asymptotically as if the ground-truth nuisance functions were
known). Our DRQ-learner is applicable to settings with both discrete and
continuous state spaces. Further, our DRQ-learner is flexible and can be used
together with arbitrary machine learning models (e.g., neural networks). We
validate our theoretical results through numerical experiments, thereby showing
that our meta-learner outperforms state-of-the-art baselines.

</details>


### [221] [Pretrain-Test Task Alignment Governs Generalization in In-Context Learning](https://arxiv.org/abs/2509.26551)
*Mary I. Letey,Jacob A. Zavatone-Veth,Yue M. Lu,Cengiz Pehlevan*

Main category: stat.ML

TL;DR: 本文研究了预训练任务结构如何影响Transformer模型的上下文学习泛化能力，提出了一个对齐度量来量化任务分布对齐对ICL性能的影响，揭示了专业化与泛化之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer模型中上下文学习能力出现的数据结构基础及其鲁棒性机制，目前仍不清楚。本文旨在研究预训练任务结构如何控制ICL的泛化能力。

Method: 使用线性注意力机制的可解线性回归ICL模型，在高维条件下推导出任意预训练-测试任务协方差不匹配时的ICL泛化误差精确表达式，提出了一个新的对齐度量方法。

Result: 发现对齐度量不仅能在可解模型中预测ICL性能，在非线性Transformer中也适用。分析揭示了ICL中专化与泛化之间的权衡：根据任务分布对齐程度，增加预训练任务多样性可能改善或损害测试性能。

Conclusion: 训练-测试任务对齐是ICL泛化能力的关键决定因素，识别出任务分布对齐对上下文学习性能的重要影响。

Abstract: In-context learning (ICL) is a central capability of Transformer models, but
the structures in data that enable its emergence and govern its robustness
remain poorly understood. In this work, we study how the structure of
pretraining tasks governs generalization in ICL. Using a solvable model for ICL
of linear regression by linear attention, we derive an exact expression for ICL
generalization error in high dimensions under arbitrary pretraining-testing
task covariance mismatch. This leads to a new alignment measure that quantifies
how much information about the pretraining task distribution is useful for
inference at test time. We show that this measure directly predicts ICL
performance not only in the solvable model but also in nonlinear Transformers.
Our analysis further reveals a tradeoff between specialization and
generalization in ICL: depending on task distribution alignment, increasing
pretraining task diversity can either improve or harm test performance.
Together, these results identify train-test task alignment as a key determinant
of generalization in ICL.

</details>


### [222] [Estimating Dimensionality of Neural Representations from Finite Samples](https://arxiv.org/abs/2509.26560)
*Chanwoo Chun,Abdulkadir Canatar,SueYeon Chung,Daniel Lee*

Main category: stat.ML

TL;DR: 提出了一种针对神经网络表示流形全局维度测量的偏差校正估计器，解决了现有方法对样本量敏感的问题，并在合成数据和真实神经数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的全局维度测量方法（特别是特征值参与比）对样本量高度敏感，在小样本情况下存在严重偏差，需要开发更准确的估计器。

Method: 提出了偏差校正的维度估计器，能够更准确地估计有限样本和噪声条件下的真实维度，并可通过适当加权有限样本来测量弯曲神经流形的局部维度。

Result: 在合成数据上能够恢复已知的真实维度，在神经脑记录数据（钙成像、电生理记录、fMRI）和大语言模型神经激活数据上表现出对样本量的不变性。

Conclusion: 提出的偏差校正估计器为神经网络表示流形的维度分析提供了更可靠的工具，特别是在小样本情况下，并且能够扩展到局部维度测量。

Abstract: The global dimensionality of a neural representation manifold provides rich
insight into the computational process underlying both artificial and
biological neural networks. However, all existing measures of global
dimensionality are sensitive to the number of samples, i.e., the number of rows
and columns of the sample matrix. We show that, in particular, the
participation ratio of eigenvalues, a popular measure of global dimensionality,
is highly biased with small sample sizes, and propose a bias-corrected
estimator that is more accurate with finite samples and with noise. On
synthetic data examples, we demonstrate that our estimator can recover the true
known dimensionality. We apply our estimator to neural brain recordings,
including calcium imaging, electrophysiological recordings, and fMRI data, and
to the neural activations in a large language model and show our estimator is
invariant to the sample size. Finally, our estimators can additionally be used
to measure the local dimensionalities of curved neural manifolds by weighting
the finite samples appropriately.

</details>
